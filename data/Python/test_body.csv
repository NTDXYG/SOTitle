"In the tensorflow examples , feed_dict is used to send either training or validation input into the same model graph . Unfortunately , you can not feed tensors : I 've been using an input pipeline and TFRecordReader , so my data never really enters python . Having to call run to get the data into python just to feed it back to tensorflow seems silly and is definitely slow.Does anyone have a good solution for this ? Currently , I just create two identical copies of the model subgraph that use the same parameters . This works , but forces me to organize my code in an odd way.EDITFor example , I 'm currently doing something like : so that the test model uses the parameters learned by training . The nice thing about feed_dict is that I only need to define the model once and I do not have to separate the model 's parameters from its structure ."
I am trying to set up a Django Model that works as a base class for other models . The Base model has two ForeignKey fields to other objects of the same class ( TestForeign ) . I can get the models working using multitable inheritance but I want to use abstract model inheritance because I have read that there are some performance issues when using multitable inheritance.The following example works when I use multitable inheritance ( abstract = False ) but fails when I run it with abstract inheritance.When I synchronize the database I get the following error : Is there any way I can get the references of the base class not to clash in the derived models ?
"How can I ( hermetically ) include python as an ( executable ) input to my genrule ? Conceptually , I 'm aware of the following approaches : include a python interpreter in the source repo at third_party/pythonhave Bazel fetch python ala rules-goThere also seem to be a couple methods for doing so : py_runtimepy_toolchain ( does n't seem to be available yet ) py_distribution_package ( does n't seem to be available yet ) genrules.toolsgenrules.toolchainsExample : I have a python library : I 'd like to produce a pip install-able distribution with Bazel so that I can do : I added a ( empty ) WORKSPACE file at myproject/WORKSPACE and followed the example in this medium article : This works ( ie . produces bazel-genfiles/mylibrary.tar.gz ) , but I 'm shelling out to python . How can I ( hermetically ) include python as an ( executable ) input to my genrule ?"
I have this code : I expected to get this : But instead I 'm getting this : Is there an attribute to remove the div tag wrapper ?
"With a single-indexed dataframe , the columns are available in the group by object : But in a MultiIndex dataframe when not grouping by level , the columns are no longer accessible in the group by objectAs a workaround , this works but it 's not efficient since it does n't use the cpythonized aggregator , not to mention it 's awkward looking . Is there a way to access MultiIndex column in groupby object that I missed ?"
"So the question is pretty simple : If we have a random class , let 's say an int and we try to access a non defined attribute : We will get this error : But if we try to access an index of it ( in which case Python will do a lookup for a __getitem__ attribute ) : We get : What is the logic behind the change in exception type ? It seems weird to me that a TypeError is raised , complaining about a missing attribute ( AttributeError seems like a much better candidate for that )"
"I have a flask app running under uWSGI behind nginx.The permissions on the socket are okay ( 666 , and set to the same user as nginx ) , in fact , even when I run nginx as root I still get this error . The flask app/uwsgi is sending the request properly . But it 's just not being read by Nginx . This is on Ubuntu Utopic Unicorn . Any idea where the permission might be getting denied if the nginx process has full access to the socket ? As a complicating factor this server is running in a container that has Ubuntu 14.04 installed in it . And this setup used to work ... but I recently upgraded the host to 14.10 ... I can fully understand that this could be the cause of the problem . But before I downgrade the host or upgrade the container I want to understand why . When I run strace on a worker that 's generating this error I see the call it 's making is something like this:14 seems to be the file descriptor created by this system callSo it ca n't read from a local socket that it has just created ?"
How I can use pagination from list_route method ? I have view :
"I just wrapped a Fortran 90 subroutine to python using F2PY . The subtlety here is that the Fortran subroutine aslo takes a python call-back function as one of its arguments : The pyfunc is a python function defined elsewhere in my python code . The wrapper works fine , but running the wrapped version above , I got an elapsed time about factor of 5 times longer than what I can get using pure python as follows , So , the question is , what is the overhead coming from ? I really want to leave pyfunc as is because it is extremely time-consuming to re-code it into pure fortran function , so is there any way to improve the speed of the wrapper module ?"
"Is it possible to put ListBoxes inside of SimpleListWalkers ? I 'm trying to make nested ListBoxes , but I have this error : AttributeError : 'MyListBox ' object has no attribute 'rows '"
if df1 is : and df2 is : I want the result as : To do the intersection I want to consider only Non-Nan values of df2- where ever there is a NaN in df2 that column value should be ignored to perform the intersection .
"Problem : split a string into a list of words by a delimiter characters passed in as a list.String : `` After the flood ... all the colors came out . `` Desired output : [ 'After ' , 'the ' , 'flood ' , 'all ' , 'the ' , 'colors ' , 'came ' , 'out ' ] I have written the following function - note I am aware that there are better ways to split a string using some of pythons built in functions but for sake of learning I thought I would proceed this way : I ca n't figure out why `` came out '' is not split into `` came '' and `` out '' as two separate words . Its like as if the whitespace character between the two words is being ignored . I think the remainder of the output is junk that stems from the problem associated with the `` came out '' problem . EDIT : I followed @ Ivc 's suggestion and came up with the following code :"
"I 'm using Django Oauth Library.I want to have different Auth and Resource Server.On Auth Server , following is my setting.On my Resource Server } Question 1 ) How do I obtain RESOURCE_SERVER_AUTH_TOKEN ? Question 2 ) Upon introspecting the token , Auth Server returns 403 Forbidden Error in the console logs.Following is the flow to obtain the access token.I get the client_id , client_secret , grant_type and scopes from the client POST request onto the Resource Server . I call the AuthServer from the Resource Server and return the response back to the client.What exactly am I missing over here ?"
"Say I have a classification problem that is multiclass and characteristically hierarchical , e.g . 'edible ' , 'nutritious ' and '~nutritious ' - so it can be represented like soWhile one can get reasonable performance with classifiers that support multiclass classification or using one-vs-one/all schemes for those that do n't , it may also be beneficial to separately train classifiers at each level and concatenate them so the instances classified as 'edible ' can be classified as either nutritious or not.I would like to use scikit-lean estimators as building blocks and I am wondering if I can make the Pipeline support this or if I would need to write my own estimator that implements the base estimator and possibly BaseEnsemble to do this.It has been mentioned before by @ ogrisel on the mailing list http : //sourceforge.net/mailarchive/message.php ? msg_id=31417048 and I 'm wondering if anyone has insights or suggestions on how to go about doing this ."
I create a pandas dataframe with a DatetimeIndex like so : Which givesI can easily plot the A colum with pandas by doing : which plots a line of the A column but leaves out the weekend column as it does not hold numerical data.How can I put a `` marker '' on each spot of the A column where the weekend column has the value yes ?
"I have a very large database - I 'm working with a subset that is 350m rows , but ultimately it will be around 3b rows . My entire goal here is to optimize a particular type of query on this database , at the expense of pretty much everything but memory . The db file I 'm working with right now is compressed with blosc at level 1 , on PyTables version 2.3.1 ( I could update , if that would help ) . Each row has thirteen entries - a typical entry looks like this : They 're all numerical , but not necessarily the same type . I 'm currently storing them in a PyTables Table , with this definition : I do n't really care how long it takes to set up this database - I 'll ultimately be creating it once and then just accessing it . My queries are of the form : Basically I 'm searching for entries that match a particular row around a given tolerance . On my smaller database ( 350m rows ) , that query takes 38 seconds if I 've indexed all four columns that I 'm searching on : and 10 seconds if I do not index beforehand . I 'm not sure I understand why the query is slower on the indexed database.. maybe the indexing creates overhead that is n't necessary ? Like I said , my goal is to optimize this type of query as much as possible - at the expense of basically everything but memory usage ( I want to use about 2G , and really do n't want to use more than about 5G ) . I 've tried indexing and it does n't seem to work . All of my queries are on a single value of mh_sites , and there are only about 100 possible values , so I thought about splitting it up into multiple tables , so I 'm only searching a subset of the data at any one time ( although I 'm not entirely sure how to do that , other than mydata.root.table_1 , mydata.root.table_2 , etc ) . I 've also thought about trying to store it as an array instead - maybe a float array , and then convert everything else to ints when I need to use them ? If it makes a difference , my queries usually return between 20k and 500k results . Any suggestions on optimizing this query ?"
"I was taking a look at some commit of a project , and I see the following change in a file : The coder replaced import dataFile by dataFile = __import__ ( dataFile ) .What exactly is the difference between them ?"
"I have the following code : just to generate a exemple for run a Tensorboard.When I run the tensorboard -- logdir=./logs to generate the graph , the error is : So what is getting wrong here ? I 'm on Windows 10"
plotly.express is very convenient to produce nice interactive plots . The code below generates a line chart colored by country . Now what I need is to add points to the plot . Does anyone know how I can add points to the line chart ?
"With SQLAlchemy , it is possible to add a default value to every function . As I understand it , this may also be a callable ( either without any arguments or with an optional ExecutionContext argument ) .Now in a declarative scenario , I wonder if it is somehow possible to have a default function which is called with the object that is being stored . I.e . possibly like so : Is something like this possible ? Or do I have to somehow set up an before-insertion hook for this ( how ? ) ?"
"I have been following a few tutorials on bufferoverflow exploitation . But my problem is , that I am not able to open a root shell , I will always get a normal user shell instead . I have checked the following pointsI re-verified the following items but still I could not achieve an actual root shell : I properly set the owner of the binary to root and also set the s flag ( check ) I have verified that the exploit I use is working properly , the correct adresses for system @ plt and exit @ plt are used and the values are proper loaded into rdi via pop rdi ; ret ; segments ; I do get a shell after all but not a root shell as expected ; ( check ) I heard that nowadays dash and bash do drop privileges and that linking /bin/sh to /bin/zsh will help , but this didnt help me ; still getting non root shell ( check , approach did not work for me ) i also tried to call setuid ( 0 ) and seteuid ( 0 ) for testing in the binary . still no root shell ; ( check , did n't work for me ) I also saw that some people set /proc/sys/kernel/yama/ptrace_scope to 0 ( see post here ) see post here , but that 's not the case for me ( value is set to 1 and I never touched that ) ( check , my value is set to 1 and that should be ok ) I am using linux mint 18.1 serena , maybe there is an additional security feature here that drops privileges and prevents a root-shell ? see my c code and exploit python script below for reference ( vulnerability is in function vuln ( ) ) ; the function shell ( ) is just to have the addresses to the corresponding @ plt functions available ( this is just for exercising and playing around ) I use 'gcc -fno-stack-protector -o ghost ghost.c ' to compile the binary to avoid stack canariesDoes someone have an idea what the problem might be ? Why I still wont get a root shell ? Thanks in advance for any advice and hints.Best Zaphoxxthe vulnerable c code : python exploit script to create payload : EDIT UPDATE : so I retried as suggested to call execve ( ) in a small binary directly instead of using the vulnerable binary , just to check if that would open a root shell : So it does not open a root shell ; I did link /bin/sh to /bin/zsh as has been suggested in other posts , see here : As suggested by Peter I did use '/usr/bin/id ' instead as argument for system in my exploit to check but the result was the same as expected : UPDATE : I got a good hint from K.A.Buhr to check /proc/mounts for nosuid entries and : So this seems to be the cause of my issues . How would I change that the right way or how can I temporarily deactivate nosuid so I can test the exploit ( s ) ?"
"I am trying to update my serializers in my drf project to be shown in a nested way . The two models in question are Image and Gallery , images are related to Galleries.I tried following https : //www.django-rest-framework.org/api-guide/relations/ # nested-relationships , but i am not entirely sure why it is not working.Below is models.pyserializers.pyExpecting outcome would be"
"I 'm trying to merge a series of dataframes in pandas . I have a list of dfs , dfs and a list of their corresponding labels labels and I want to merge all the dfs into 1 df in such that the common labels from a df get the suffix from its label in the labels list . i.e . : When I try this , I get the error : I 'm trying to make a series of merges that at each merge grows at most by number of columns N , where N is the number of columns in the `` next '' df in the list . The final DF should have as many columns as all the df columns added together , so it grow additively and not be combinatorial.The behavior I 'm looking for is : Join dfs on the column names that are specified ( e.g . specified by on= ) or that the dfs are indexed by . Unionize the non-common column names ( as in outer join ) . If a column appears in multiple dfs , optionally overwrite it . Looking more at the docs , it sounds like update might be the best way to do this . Though when I try join='outer ' it raises an exception signaling that it 's not implemented.EDIT : Here is my attempt at an implementation of this , which does not handle suffixes but illustrates the kind of merge I 'm looking for : This assumes that the merging happens on the indices of each of the dfs . New columns are added in an outer-join style , but columns that are common ( and not part of the index ) are used in the join via the on= keyword.Example : The twist on this would be one where you arbitrarily tag a suffix to each df based on a set of labels for columns that are common , but that is less important . Is the above merge operation something that can be done more elegantly in pandas or that already exists as a builtin ?"
"I would like to construct list x from two lists y and z. I want all elements from y be placed where ypos elements point . For example : So , x must be [ 11 , 12 , 13 , 14 , 15 ] Another example : So , x must be [ 35 , 58 , 77 , 74 ] I 've written function that does what I want but it looks ugly : How to write it in pythonic way ?"
I was messing around with classes in python and wrote 2 little ones : And my test-main : This leads to the following output : Now my question is why is the output of my first print the memory address even though I provided a functioning string-method for my class ClaCluster ? Is there a way to get the method invoked when I am printing the dictionary or do I have to iterate by hand ?
"I want to revert my last migration ( 0157 ) by running its Migration.backwards ( ) method . Since I am reverting the migration in production server I want to run it automatically during code deployment . Deployment script executes these steps : Pull code changesRun migrations : manage.py migrate < app > Refresh Apache to use newest code : touch django.wsgiIf I could , I would create new migration file which would tell South to backward migrate to 0156 : This commited migration would be deployed to production and executed during manage.py migrate < app > command . In this case I would n't have to execute backward migration by hand , like suggested in these answers.Lets say , I have created two data migrations , first for user 's Payment , second for User model . I have implemented backwards ( ) methods for both migrations in case I 'd have to revert these data migrations . I 've deployed these two migrations to production . And suddenly find out that Payment migration contains an error . I want to revert my two last data migrations as fast as possible . What is the fastest safe way to do it ?"
"I have data array , with shape 100x100 . I want to divide it into 5x5 blocks , and each block has 20x20 grids . The value of each block I want is the sum of all values in it.Is there a more elegant way to accomplish it ? This is based on index , how if based on x ? Z_new ?"
"Sorry if this is a stupid question , but I 'm a bit of a Django newbie and ca n't find the answer.I have an Order model with a self-referencing field : While this works fine if I have an Order and I want to know its source , I also need to get a list of an Order 's `` children '' - that is , a list of Orders for which this Order is the source . Does this need to be done through filter ( ) s or is there a nice , Django-y way to do it ?"
"I have a dataframe , df with 646585 rows and 3 columns which looks like : I tried to pivot the dataframe using the code : but I got"
"I 'm looking for a way to install a bunch of python modules in .exe format like : I can install other modules by installing distribute and call pip from command-line , but I want to know if I can automate installs of .exe files - so users do n't have to click buttons of 'next ' 'okay ' ."
"I am getting the following error message when attempting to write GML file after merging two graphs using compose ( ) : The background is that I import two GML files using : Each node ( in both GML files ) file is structured thus : After importing each file , I can check all the node attributes , see that nodes are unique within each graph . I also see that NetworkX by default discards the 'id ' field , und uses the 'label ' as the identifier of the node . It retains the user_id attribute ( which happens to be a Twitter user_id and suits my purposes well ) . Running I can see that the data for the node above is : There is ( in this test case ) one common node shared by Graph g and Graph h , - the one shown above . All others are unique by user_id and label.I then merge the two graphs using : This works ok . I then go to write out a new GML from the graph , f , using : At this point I get the error , above : I have checked the uniqueness of all user_id 's ( in case I had duplicated one ) : Which outputs : ( formatted for readability ) .So , all user_id 's are unique , as far as I can tell . So , if it is not a question of uniqueness of keys , what is the error telling me ? I 've exhausted my thinking on this ! Any pointers , please , would be very much appreciated !"
"I 'm trying to model an entity that as one or more one-to-many relationships , such that it 's last_modified attribute is updated , whena child is added or removeda child is modifiedthe entity itself is modifiedI 've put together the following minimal example : This seems to work , but I 'm wondering if there 's a better way to do this ? Specifically , this becomes very verbose since my actual ConfigParam implementation has more attributes and I 'm having multiple one-to-many relations configured on the parent Config class ."
"similar to the matplotlib ax.set_navigate ( False ) command possible ? Here is a minimal example using ipython notebook.Is is possible to hold the second y axis in place , while zooming and panning in the other y axis ? Using PanTool dimensions did not help me with this problem.Edit : Screenshot inserted : Blue line is drawn on the first axis , red on the secondIf I zoom in , pan around the x axis , I want the red line to keep in place ."
"I have following project hierarchy : There is the SomeException class definition in lib/agent/erros.pyI use following code to import them inside lib/agent/some_agent_script.py : Also I use following code to import in lib/some_script.pyThe problem is when I raise a SomeException in lib/agent/some_agent_script.py then lib/some_script.py can not catch it in an except block : I can see in sys.modules that erros module was imported twice : the first is with 'agent.errors ' key and the second is with 'lib.agent.errors ' keyThe following code goes right , but it 's not a beautiful solution : What should I do to make this module not to import twice ?"
I am in the beginning phases of learning NumPy . I have a Numpy array of 3x3 matrices . I would like to create a new array where each of those matrices is rotated 90 degrees . I 've studied this answer but I still ca n't figure out what I am doing wrong.Note : I realize I could do the following but I 'd like to understand the vectorized option .
"I am trying to find the unique differences between 5 different lists.I have seen multiple examples of how to find differences between two lists but have not been able to apply this to multiple lists.It has been easy to find the similarities between 5 lists.Example : However , I want to figure out how to determine the unique features for each set.Does anyone know how to do this ?"
I 've encountered this code from Most pythonic way of counting matching elements in something iterabler is iterated once . and then it 's iterated again . I thought if an iterator is once consumed then it 's over and it should not be iterated again.Generator expressions can be iterated only once : enumerate ( L ) too : and file object too : Why does xrange behave differently ?
"I was trying to do the first problem in codeforces just to be familiar with it.It gives results when I try it on my Ipython notebook , but always gives runtime error when I upload it on codeforces . Can anyone help ? Problem : Theatre Square in the capital city of Berland has a rectangular shape with the size n × m meters . On the occasion of the city 's anniversary , a decision was taken to pave the Square with square granite flagstones . Each flagstone is of the size a × a.What is the least number of flagstones needed to pave the Square ? It 's allowed to cover the surface larger than the Theatre Square , but the Square has to be covered . It 's not allowed to break the flagstones . The sides of flagstones should be parallel to the sides of the Square.Input : The input contains three positive integer numbers in the first line : n , m and a ( 1 ≤ n , m , a ≤ 109 ) .Output : Write the needed number of flagstones.Sample testcase : My attempt : EDIT : There is not much explanation about the error other than `` Runtime Error on test 1 '' . Also , if it helps , time used was 92 ms , and memory used was 0 KB ."
"I 'm trying to figure out the best way to accomplish the following : Download a large XML ( 1GB ) file on daily basis from a third-party websiteConvert that XML file to relational database on my serverAdd functionality to search the databaseFor the first part , is this something that would need to be done manually , or could it be accomplished with a cron ? Most of the questions and answers related to XML and relational databases refer to Python or PHP . Could this be done with javascript/nodejs as well ? If this question is better suited for a different StackExchange forum , please let me know and I will move it there instead.Below is a sample of the xml code : Here 's some more information about how these files will be used : All XML files will be in the same format . There are probably a few dozen elements within each record . The files are updated by a third party on a daily basis ( and are available as zipped files on the third-party website ) . Each day 's file represents new case files as well as updated case files.The goal is to allow a user to search for information and organize those search results on the page ( or in a generated pdf/excel file ) . For example , a user might want to see all case files that include a particular word within the < text > element . Or a user might want to see all case files that include primary code 025 ( < primary-code > element ) and that were filed after a particular date ( < filing-date > element ) . The only data entered into the database will be from the XML files -- users wo n't be adding any of their own information to the database ."
"I need to match two very large Numpy arrays ( one is 20000 rows , another about 100000 rows ) and I am trying to build a script to do it efficiently . Simple looping over the arrays is incredibly slow , can someone suggest a better way ? Here is what I am trying to do : array datesSecondDict and array pwfs2Dates contain datetime values , I need to take each datetime value from array pwfs2Dates ( smaller array ) and see if there is a datetime value like that ( plus minus 5 minutes ) in array datesSecondDict ( there might be more than 1 ) . If there is one ( or more ) I populate a new array ( of the same size as array pwfs2Dates ) with the value ( one of the values ) from array valsSecondDict ( which is just the array with the corresponding numerical values to datesSecondDict ) . Here is a solution by @ unutbu and @ joaquin that worked for me ( thanks guys ! ) : Thanks , Aina ."
"I am developing a simple type system for my own interpreter.I am writing something like this : My problem is that obviously if I do something like this : I have summed f1 and f2 , which are Float type , but I have obtained f3 , which is Number type , but I would like f3 to be a Float type . How could I define my add operator just one time in my Number superclass returning a type which is the same of f1 and f2 ? Have I to use isinstance ? Is there a cleaner way to do this ? Thank you !"
"I 'm wrapping a C library that contains a struct : and a function that creates such a struct : SWIG generates a python class SCIP and a function SCIPcreate ( *args ) from that.When I now try calling SCIPcreate ( ) in python , it obviously expects a parameter of type SCIP** , how am I supposed to create such a thing ? Or should I try and extend the SCIP class with a constructor that calls SCIPcreate ( ) automatically ? If so , how would I go about that ?"
"For my telegram bot ( python-telegram-bot ) i generated a PIL.Image.Image and i want to send it directly to a user.What works is to send an image as bufferedReader from a file , but i do n't want to safe the image . I do n't need it again afterwards and i might generate a lot of different images at the same time , so saving is kind of messy.Because i generated it myself , i cant use an URL or file_id . I thought it might be possible to convert the image to a bufferedReader , but i only managed to get a bytes object from it , which did n't work.The image is generated like : Thanks in advance : ) merry x-mas"
"I recently came across some surprising behaviour in Python generators : Which gives the output : I was ( pleasantly ) surprised that *Excepted Successfully* got printed , as this was what I wanted , but also surprised that the Exception still got propagated up to the top level . I was expecting to have to use the ( commented in this example ) raise keyword to get the observed behaviour.Can anyone explain why this functionality works as it does , and why the except in the generator does n't swallow the exception ? Is this the only instance in Python where an except does n't swallow an exception ?"
"I want to assign values to the diagonal of a dataframe . The fastest way I can think of is to use numpy 's np.diag_indices and do a slice assignment on the values array . However , the values array is only a view and ready to accept assignment when a dataframe is of a single dtype Consider the dataframes d1 and d2 Then let 's get our indicesd1 is of a single dtype and therefore , this works But not on d2 I need to write a function and make it fail when df is of mixed dtype . How do I test that it is ? Should I trust that if it is , this assignment via the view will always work ?"
"I would like to write my spark dataframe as a set of JSON files and in particular each of which as an array of JSON.Let 's me explain with a simple ( reproducible ) code . We have : Saving the dataframe as : each file just created has one JSON object per line , something like : but I would like to have an array of those JSON per file :"
"I am familiarizing myself with Tkinter , and I am attempting to write a very simple program , which displays a button in a window , using the pack geometry manager.I was experimenting with various configuration options for pack ( ) , such as expand , fill , and side , and I 've run into a peculiar problem . I have written the following code : The problem is that the button expands to fill the window in the horizontal direction , but not the vertical direction . This is the same result that I get if instead of specifying fill=BOTH I use fill=X . In addition , if I specify instead fill=Y the button does not expand in either direction . Something seems to be going wrong with the fill in the vertical direction , and I can not figure out what it might be.I attempted to Google this problem and surprisingly found no mention of this happening to anyone else . I am using a Mac with OS X Yosemite and running python 2.7.5 . I also attempted to compile with python 3.4.1 and saw no change.Edit : Based off of the answer and comments below , it is clear that there is nothing wrong with my code , because it seems to work on other machines . If not an error in the code , does anyone know what could possibly be causing the button to not stretch vertically when I run the above code ?"
"Imagine I have a big CLI application with many different commands ( think , for example image-magick ) . I wanted to organize this application into modules and etc . So , there would be a master click.group somewhere : that can be imported in each module that define a command : The problem is that I run into a circular import problem , since the main.py file knows nothing about command_x.py and I would have to import it before calling the main section.This happens in Flask too and is usually dealt with the app factory pattern . Usually you would have the app being created before the views : In the app factory pattern you postpone the `` registration '' of the blueprints : And make a factory that knows how to build the app and register the blueprints : And you can then create a script to run the app : Can a similar pattern be used with Click ? Could I just create a `` click app '' ( maybe extending click.Group ) where I register the `` controllers '' which are the individual commands ?"
"I 'm writing a spider to crawl web pages . I know asyncio maybe my best choice . So I use coroutines to process the work asynchronously . Now I scratch my head about how to quit the program by keyboard interrupt . The program could shut down well after all the works have been done . The source code could be run in python 3.5 and is attatched below.But if I press 'Ctrl+C ' while it 's running , some strange errors may occur . I mean sometimes the program could be shut down by 'Ctrl+C ' gracefully . No error message . However , in some cases the program will be still running after pressing 'Ctrl+C ' and would n't stop until all the works have been done . If I press 'Ctrl+C ' at that moment , 'Task was destroyed but it is pending ! ' would be there.I have read some topics about asyncio and add some code in main ( ) to close coroutines gracefully . But it not work . Is someone else has the similar problems ?"
"I want to check the stationary of a time series data saved in TS.csv.However , R 's tseries : :adf.test ( ) and Python 's statsmodels.tsa.stattools.adfuller ( ) give completely different results.adf.test ( ) shows it 's stationary ( p < 0.05 ) , while adfuller ( ) shows it 's non-stationary ( p > 0.05 ) .Is there any problems in the following codes ? What 's the right process to test stationary of a time series in R and Python ? Thanks.R codes : Python codes ( from Time_Series.ipynb ) : Update @ gfgm give exellent explanations why results of R and Python are different , and how to make them the same by changing the parameters.For the second quetsion above : `` What 's the right process to test stationary of a time series in R and Python ? `` .I 'd like to provide some details : When forecast a time series , ARIMA model needs the input time series to be stationary.If the input is n't stationary , it should be log ( ) ed or diff ( ) ed to make it stationary , then fit it into the model.So the problem is : should I think the input is stationary ( with R 's default parameters ) and fit it directly into ARIMA model , or think it 's non-stationary ( with Python 's default parameters ) , and make it stationary with extra functions ( like log ( ) or diff ( ) ) ?"
"I came across this problem Unlucky number 13 ! recently but could not think of efficient solution this.Problem statement : N is taken as input.N can be very large 0 < = N < = 1000000009Find total number of such strings that are made of exactly N characters which do n't include `` 13 '' . The strings may contain any integer from 0-9 , repeated any number of times.My solution : OutputThe output file should contain answer to each query in a new line modulo 1000000009.Any other efficient way to solve this ? My solution gives time limit exceeded on coding site ."
"Say I have a function func ( i ) that creates an object for an integer i , and N is some nonnegative integer . Then what 's the fastest way to create a list ( not a range ) equal to this listwithout resorting to advanced methods like creating a function in C ? My main concern with the above list comprehension is that I 'm not sure if python knows beforehand the length of range ( N ) to preallocate mylist , and therefore has to incrementally reallocate the list . Is that the case or is python clever enough to allocate mylist to length N first and then compute it 's elements ? If not , what 's the best way to create mylist ? Maybe this ? RE-EDIT : Removed misleading information from a previous edit ."
"I 'm trying to understand the numpy fft function , because my data reduction is acting weirdly . But now that I 've transformed a simple sum of two sines , I get weird results . The peaks I have is extremely high and several points wide around zero , flattening the rest . Does anybody have a clue of what I might be doing wrong ? What I get is this : While it should have two sidepeaks on both sides , one of em 2x higher than the other"
Is there an equivalent of boot and boot.ci in python ? In R I would do
"In my Python code I have this class : And there are two lists , initialPointsList and burnedPointsList : I want to calculate the difference between initialPointsList and burnedPointsListI have executed : And get the following output : But I expected another result , without burned point coordinates : What is the best way to do that in Python ? What is incorrect with my code ?"
Simple question : Is there a shorthand for checking the existence of several keys in a dictionary ?
Given an object it is easy to get the object type in python : How can I do this without first creating an instance of the class ? Background : Once you have the type you easily can create instances of that type . Using this mechanism I want to pass a type to an object which will later create instances of that type . It would be very reasonable not to have to create an object first which will immediately be thrown away again .
"Let A be a non-empty set of integers . Write a function find that outputs a non-empty subset of A that has the maximum product . For example , find ( [ -1 , -2 , -3 , 0 , 2 ] ) = 12 = ( -2 ) * ( -3 ) *2Here 's what I think : divide the list into a list of positive integers and a list of negative integers : If we have an even number of negative integers , multiply everything in both list and we have the answer . If we have an odd number of negative integers , find the largest and remove it from the list . Then multiply everything in both lists . If the list has only one element , return this element.Here 's my code in Python : Am I missing anything ? P.S . This is a problem from Google 's foobar challenge , I am apparently missing one case but I do n't know which.Now here 's actual problem :"
"When trying a selfwritten module , it is likely to end with errors at the first few times.But when fixing these errors , ipython does not seem to notice that.Is there an ipython command to reload the new module ? 'clear ' does not do the trick . So far , the only thing that works is to 'exit ' and start a new session . But this also means redoing everything I have done so far.Or do I need to add something to the module that it kills all its internal variables after it has been run ? Example : OK , that 's easy . I made a typo in line 198 , and wrote Cons_NaNs instead of Cons_NaNs_str , and thus I get the obvious error of trying to join a dataframe with a string.But after fixing it in the mymodule.py file , I get the following ( shortened ) error : Looking at the traceback , ipython is well aware of the changes is made in the source file , it shows that I fixed the typo with the missing _str , but it still gives an error , that at the first look seems to be impossible . After running clear and reimporting everything , it shows the same behaviour.So just to make sure that I did not make a stupid mistake somewhere along the way , I went trough my whole module step by step in ipython . And every variable that leads me to that point behaves as expected.Cons_NaNs is a dataframe , Cons_NaNs_count is an integer and Cons_NaNs_str is a string.So I exited ipython , restarted it and reimported everything and now it works.But having to exit ipython sucks . Most of the times this means having to reimport dozens of things and doing a few dozen commands , to get to the point where I can actually test what I am currently working on ."
"I met this problem when I want to try a python version of : https : //leetcode.com/problems/first-missing-positive/discuss/17071/My-short-c++-solution-O ( 1 ) -space-and-O ( n ) -timeI am not sure why a [ 0 ] , a [ a [ 0 ] ] = a [ a [ 0 ] ] , a [ 0 ] this one does not do the swap ? My guess is that the implementation of a , b = b , a syntax is something like : Then I checked the implementation of swap function in C++ . I know nothing about C++ , but it looks like the idea is the same : http : //www.cplusplus.com/reference/algorithm/swap/we have c = a , then a = b and b = aSo why C++ swap function does not have this problem ? And how to write this kind of swap function in a pythonic way ?"
"I was looking at the answer to this question : Is it possible to define a class constant inside an Enum ? What interested me most was the Constant class in Ethan Furman 's answer . The question was about Python 3.4 but I 'm using 2.7 . In the answer Ethan sets the gravitational constant as an instance variable of the class Planet like so : My testing of this class in 2.7 shows that typing just G gives me this : ( I set it to 3 for ease of use while testing . This looks like the __repr__ output to me , please correct me if I '' m wrong . ) The value is available via G.value . However , in Ethan 's answer he usesThis obviously only works if the value is returned vs the __repr__ output . Now if I change class Constant : to class Constant ( int ) : then type G I still get the __repr__ output but if I type G * 4 I get 12 not the error I was getting . ( TypeError : unsupported operand type ( s ) for * : 'instance ' and 'int ' ) So clearly something like the int object can output a number when called . Is there a magic method I 'm missing that would allow me to do this for the Constant class ? Since constants could be strings , integers , or floats I 'd prefer to have 1 class that handles them all vs 3 separate classes that extend those objects.The value is also settable via G.value . Can I lock this down so the Constant class behaves likes an actual constant ? ( I suspect the answer is no . )"
"I am using sympy to generate different expressions for cfd-simulations.Mostly these expressions are of the kind exp = f ( x , y , z ) for example f ( x , y , z ) = sin ( x ) *cos ( y ) *sin ( z ) . To get values on a grid I use simpy.lambdify . For example : This seems to work pretty good but unfortunately my expressions are getting more and more complex and the grid computation takes a lot of time . My Idea was that it is maybe possible to use the uFuncify method ( see http : //docs.sympy.org/latest/modules/numeric-computation.html ) to speed the computations up , im not sure if this is the right way ? And im also not sure how to get ufunctify to work for 3d grids ? Thanks for any Suggestions"
"My package has the following structure : signal.py contains : plot.py contains : I then have my script that makes use of this package that contains something like : When I run this I get an attribute error : module 'myPackage ' has no attribute 'plot ' I 'm not sure why its referring to plot as an attribute when its a module in my package , or why its referring to myPackage as a module.I then tried importing my package and calling the function a different way : However , now I get an import error : can not import name 'build_filter ' and the traceback is referring to plot.py where its trying to import the build_filter ( ) function from a sibling module.I 'm thinking this has something to do with the fact that the 2 modules are using functions from one another , and recursively importing the other module.What 's the correct way to organize this package so that sibling modules can import functions from each other ?"
"I 'm trying to learn Spark following some hello-word level example such as below , using pyspark . I got a `` Method isBarrier ( [ ] ) does not exist '' error , full error included below the code . Although , when I start a pyspark session in command line directly and type in the same code , it works fine : My setup : windows 10 Pro x64python 3.7.2spark 2.3.3 hadoop 2.7pyspark 2.4.0"
I need code for counting the number of cells in the image and only the cells that are in pink color should be counted .I have used thresholding and watershed method.I am not able to segment the pink cells properly.At some places two pink cells are attached together those also should be separated.output :
"Given an array of integers size N , how can you efficiently find a subset of size K with elements that are closest to each other ? Let the closeness for a subset ( x1 , x2 , x3 , ..xk ) be defined as : constraints : Array may contain duplicates and is not guaranteed to be sorted.My brute force solution is very slow for large N , and it does n't check if there 's more than 1 solution : Examples :"
"I desperatly try to set parameters in a in python.This doc says the parameters need to be of type Struct.I read here that the parameters needs to be a google.protobuf.Struct.But it does not work for me.Is there another Struct type equivalent in python ? If i send the EventInput without parameters , the intent is detected correctly.I tried this so far : Anybody having experience with this usecase ? Things i also tried : Pass a class named p yields : Parameter to MergeFrom ( ) must be instance of same class : expected Struct got p. for field EventInput.parametersPass a dict : yields : Protocol message Struct has no `` given-name '' field.Generate Struct with constructor : yields sometimes : Exception in thread ptvsd.stopping ( most likely raised during interpreter shutdown ) : /EventInput"
"I am building a model with multiple inputs as shown in pyimagesearch , however I ca n't load all images into RAM and I am trying to create a generator that uses flow_from_directory and get from a CSV file all the extra attributes for each image being processed.Question : How do I get the attributes from the CSV to correspond with the images in each batch from the image generator ?"
"I 'm running some basic code from the documentationBut for some reason the window open then closes straight away , i just see a flicker of it . Is there some sort of function in which i can keep the window open ? Thanks"
"This came up in a recent PyCon talk.The statementdoes nothing meaningful , but it does not throw an exception either . I have the feeling this must be due to unpacking rules . You can do tuple unpacking with lists too , e.g. , does what you would expect . As logical consequence , this also should work , when the number of elements to unpack is 0 , which would explain why assigning to an empty list is valid . This theory is further supported by what happens when you try to assign a non-empty list to an empty list : I would be happy with this explanation , if the same would also be true for tuples . If we can unpack to a list with 0 elements , we should also be able to unpack to a tuple with 0 elements , no ? However : It seems like unpacking rules are not applied for tuples as they are for lists . I can not think of any explanation for this inconsistency . Is there a reason for this behavior ?"
"I am using PyCharm to write some python code and notice that I run into the following problem quite often : I write a line of code like thisLater , I realize that I would like the index of item as well , so I try to turn that line into this : In order to turn the first line into the second , I put the cursor to the left of item and type i , . Then , I put the cursor to the left of myList and type enu ; by this time , the code-completer suggests that I might want to type enumerate , which is exactly the behavior that I 'm after . When I hit tab to implement the suggested enumerate , I notice that my line turns intoThe myList has been overwritten ! The behavior that I expect is this : with the cursor immediately to the right of either the myList or the : .Is there any way that I could make Pycharm behave according to my expectations ? Just in case it matters , my dev environment is Mac OSX 10.7.5 ( Lion )"
"why does py.test run the TestFoo.test_foo ( ) test there ? I understand it runs TestBar.test_foo ( ) .Contents of test_foo.py : Contents of test_bar.py : Output : If TestBar is put in the same file as TestFoo , the TestFoo.test_foo ( ) test get ran only once : Output : Should n't py.test ignore the tests that are found behind imports ?"
"Some simple theano code that works perfectly , stop working when I import pymc3Here some snipets in order to reproduce the error : And I get the following error for each of the previous snippets : Any Ideas ? Thanks in advance"
"I have a library ( subx ) which depends on subprocess32 . The subprocess32 library is a backport for Python2.7 and provides the timeout kwarg.My library needs the timeout kwarg.I need subprocess32 only if the target platform is Python2.x . How should I define the dependency in my project ? I get this error message , if I define a dependency to subprocess32 via `` install_requires '' ( setup.py ) and I am inside a python3 virtualenv :"
"I want to know how you get the months by the giving day , week number and year . For example if you have something like this"
"I am new to gimp python-fu programmingI have spent my 4 days WITH NO LUCK to figure out which is the best appropriate functions to use in order to get the coordinates of drawn path and show the output on gtk message box like the following image.Please consider that I develop on windows machineI have tried my code like this : Sometimes the plugin itself does not show in the menu , sometimes no output at all ."
"I have a very simple machine learning code here : My X values are 59 features with the 60th column being my Y value , a simple 1 or 0 classification label.Considering that I am using financial data , I would like to lookback the past 20 X values in order to predict the Y value.So how could I make my algorithm use the past 20 rows as an input for X for each Y value ? I 'm relatively new to machine learning and spent much time looking online for a solution to my problem yet I could not find anything simple as my case.Any ideas ?"
"What is the use of typename associated with a particular class ? For example , Where would you normally use typename ' P ' ? Thank you !"
"I have a Pandas Dataframe consisting of multiple .fits files , each one containing multiple columns with individual labels . I 'd like to extract one column and create variables that contain the first and last rows of said column but I 'm having a hard time accomplishing that for the individual .fits files and not just the entire Dataframe . Any help would be appreciated ! : ) Here is how I read in my files : ^^^ This recursively searches through my directory containing multiple .fits files in many subfolders.I tried using the .head and .tail commands for Pandas Dataframes but I am unsure how to properly use it for what I desire . For how I read in my fits files , the following code gives me the very first few rows and the very last few rows ( 5 to be exact with the default value for head and tail being 5 ) as seen here : What I want to do is try to get the first and last row for each .fits file for the specific column I want and not just for the Dataframe containing the .fits files . With the way I am reading in my .fits files , the Dataframe seems to sort of concatenate all the files together . Any tips on how I can accomplish this goal ?"
"I am trying to learn how to ( idiomatically ) use Python 3.4 's asyncio . My biggest stumbling block is how to `` chain '' coroutines that continually consume data , update state with it , and allow that state to be used by another coroutine.The observable behaviour I expect from this example program is simply to periodically report on the sum of numbers received from a subprocess . The reporting should happen at roughly the same rate the Source object recieves numbers from the subprocess . IO blocking in the reporting function should not block reading from the subprocess . If the reporting function blocks for longer than an iteration of reading from the subprocess , I do n't care if it skips ahead or reports a bunch at once ; but there should be about as many iterations of reporter ( ) as there are of expect_exact ( ) over a long enough timeframe.This example requires pexpect to be installed from git ; you could just as easily replace run ( ) with : But the real subprocess I 'm interested in needs to be run in a pty , which I think means the supplied subprocess transport/protocol framework in asyncio wo n't be sufficient for this . The point is that the source of the asynchronous activity is a coroutine that can be used with yield from.Note that the reporter ( ) function in this example is not valid code ; my problem is that I do n't know what should go in there . Ideally I 'd like to keep the reporter ( ) code separate from run ( ) ; the point of this exersise is to see how to factor out more complex programs into smaller units of code using the components in asyncio.Is there a way to structure this kind of behaviour with the asyncio module ?"
"I would like to visualize the strategy of Canadian stores on plotly map . I have done this for American stores . I just want to replicate it for Canada . I think location mode , scope and projection should change , but I do not know with which value . I would appreciate any help ."
I want to extend `` Image '' class in PIL.RESULT : Error.myOriginal has no attribute `` open '' .How can I extend Image class without rewriting open ( ) method ?
"UPDATE 1Both sets contain strings of maximum length 20 and can only take values from 'abcdefghijklmnopqrstuvwxyz'UPDATE 2I constructed the sets by reading 2 files from disk using a library called ujson ( similar to simplejson ) and then converting the returned lists into sets . I am trying to take the difference of 2 sets containing 100 million elements each.This code executes in 2 minutes : This code executes in 6 hours : All I did was add membership check which , if I am not mistaken is done in O ( 1 ) ? Where is this massive reduction coming from ? I know about set ( a ) - set ( b ) , it is practically doing exactly what my second block of code is doing , takes 6 hours to complete as well , I just wanted to write the whole procedure to demonstrate my point of confusion.Do you think there is a better solution for what I am trying to do ?"
"I am hoping to make a Django query by comparing two values within a JSONField class . I ran across Django F ( ) Objects for references fields on the model , but it does n't appear to work with JSONField as it tries to do a JOIN with the later section . So , for example : Let 's assume the data field looks something like this : I was hoping to query it like such : However , the error is something like this : Also have tried : But am given the error : Also ; Django 1.10 , Python 2.7.11 , PG Version : 9.4.9Any idea how to filter based on a comparison of value_1 and value_2 ?"
"Given two arrays , A ( shape : M X C ) and B ( shape : N X C ) , is there a way to subtract each row of A from each row of B without using loops ? The final output would be of shape ( M N X C ) .ExampleDesired result ( can have some other shape ) ( edited ) : ( Loop is too slow , and `` outer '' subtracts each element instead of each row )"
"I was reading Python Multiple Inheritance ( on Programiz ) and then I found this StackOverflow question , Method Resolution Order ( MRO ) in new-style classes ? but in this question some programmers like Alex Martelli said it uses the depth-first approach , and I have a doubt.Example : So if I build a graph based on the MRO then according to depth-first it should follow this : and path should be : A -- > B -- > C -- > E -- > F -- > G -- > D -- > HBut if you run above code you will get : Because it is following this path : A -- > B -- > C -- > E -- > D -- > F -- > G -- > HNow I have confusion about node `` D '' or class `` D '' in depth first it comes when earlier and in MRO it comes later.What 's going on here ?"
I can not launch a python script from a .desktop launcher created on Linux Mint 17.1 Cinnamon.The problem is that the script will be launched in the wrong path - namely the home folder instead of the directory it is placed in . Thereby it can not find other vital files accompanying it in its folder and hence does not work.To examine that misbehaviour I created a short script to check the folder a python script is executing in : Executing it from its own folder gives the output : I am setting a desktop launcher via Nemo 's menu . Now executing the same script yields : I do not understand this behaviour . How could I create a working desktop launcher for my python script ?
"Looking at this blog - 5 . Create Dockerfile . It seems I had to create a new Dockerfile pointing to my private image on Docker.io.And since the last command shall be starting an executable or the docker image would end up in nirvana , there is the supervisrd at the end : This is a bit confusing to me , because I have a fully tested custom Docker image that ends with supervisord , see below : So how do I serve my custom image ( CMD ? ) instead of using supervisord ? Unless I 'm overlooking something ... .UPDATEI have applied the suggested updates , but it fails to authenticate to private repo on DockerHub.The dockercfg inside a folder called docker inside the S3 bucket isThe Dockerrun.aws.json is :"
"In Python , is there an option to create a custom string class , that could be created by typing something like : Just like python has its u '' '' and r '' '' strings ?"
"I wish to print the elements of an array , without commas , and on separate lines . I am writing a function for insertion sort . Though it works correctly , I am finding it tricky to print them properly . The code I 've written is : The output I get is : I should get the following output for the code to pass the test case : i.e without the extra space between lines . Click here for the detailed problem statement ."
"I have been trying to port invRegex.py to a node.js implementation for a while , but I 'm still struggling with it . I already have the regular expression parse tree thanks to the ret.js tokenizer and it works pretty well , but the actual generation and concatenation of all the distinct elements in a way that is memory-efficient is revealing very challenging to me . To keep it simple , lets say I have the following regex : Feeding that to invRegex.py produces the following output ( tabbified to take less space ) : Considering I 'm able to get each individual token and produce an array of all the valid individual outputs : I can compute the cartesian product of all the arrays and get the same expected output : The problem with this is that it holds all the 36 values in memory , if I had a slightly more complicated regular expression , such as [ a-z ] { 0,10 } it would hold 146813779479511 values in memory , which is totally unfeasible . I would like to process this huge list in an asynchronous fashion , passing each generated combination to a callback and allowing me to interrupt the process at any sensible point I see fit , much like invRegex.py or this Haskell package - unfortunately I ca n't understand Haskell and I do n't know how to mimic the generator behavior in Python to Javascript either.I tried running a couple of simple generator experiments in node 0.11.9 ( with -- harmony ) like this one : Needless to say the above does n't work . =/Banging my head against the wall here , so any help tackling this problem would be highly appreciated.UPDATE : Here is a sample ret.js parse tree for b [ a-z ] { 3 } : The SET / RANGE type should yield 26 distinct values , and the parent REPETITION type should take that previous value to the power of 3 , yielding 17576 distinct combinations . If I was to generate a flattened out tokens array like I did before for cartesianProductOf , the intermediate flattened values would take as much space as the actual cartesian product itself.I hope this example explains better the problem I am facing ."
"There is a page i want to parse in lxml，the table data will change into different form when you click on.if i write , i get the table data of the corresponding of what i get is : what i want to get the table data is the corresponding of what i want to get is : how can i write my xpath expression root.xpath properly ? more info : when you click on 按年度 , the table will change into other one .onclick= '' ChangeRptF10AssetStatement ( '30005902 ' , ' 8 ' , 'Year ' , this , '' ) I have tried with selenium : Open /tmp/test.html , no data in it , how can i get my expect data ?"
Is there any efficient way of changing between Cartesian coordinate system and n-spherical one ? The transformation is as follows : The following is my code but I want to get rid of the loop :
"I 'm trying to run one of these tutorials and this is what I get : It installed successfully , not sure why it ca n't load it 's own modules ?"
"I have georeferenced tiff , gdalinfo output : There is second file containing a map marker image - called marker1.png ( 36x60 pixels ) .I want to overlay marker1.png on top of the above generated.tiff - so that its top left corner is located at coordinates 0.037,0.025 of the geotiff file . Visually the result should look like a google map with a single marker on top of it.How would I go about achieving that ? I have managed to partially implement this , but I am not sure whether this is the right path.This uses pixel coordinates instead of geographical ones ( but that conversion is easy ) , however the marker images show up as black blobs ( but with right dimensions ) - looks like the palette might be wrong ?"
"I have one form on my site , that other forms uses as base ( and are on different urls ) but they all do the same job , creating users.I was thinking python/django could push a specific event to google analytics when the form is valid.Is this possible ? Would there be any caveats of doing it server side apposed to client side ( javascript )"
How would I go about renaming the index on a dask dataframe ? I tried it like sobut rechecking df.index.name shows it still being whatever it was previously .
"So I 'm writing some code and have recently come across the need to implement a few mixins . My question is , what is the proper way to design a mix-in ? I 'll use the example code below to illustrate my exact query.Basically , I 'm wondering , are mix-ins sort of like Java interfaces where there is a sort of ( in Python 's case implicit ) contract that if one wishes to use the code one must define certain variables / functions ( not unlike a framework ) , or is it more like the code I 've written above , where each mix-in must be initialized explicitly ?"
"I 'm having major problems with PyCrypto , as the code below demonstrates . One issue is that the test case does not fail in a repeatable way , but has different causes on different platforms using different keys.Note that the test case provides two sets of keys for Alice and Bob , the first generated by OpenSSL and the second generated by PyCrypto ( uncomment the section 'Alternate keys ' ) .The test case is a simple round trip : Alice generates a symmetric key and encrypts the dataAlice encrypts the symmetric key with Bob 's public key , then signs the encrypted key with her private key ( hashes are not used in this simple test case ) .Bob verifies the signature with Alice 's public key and decrypts thesymmetric key with his private key.Bob decrypts the data with the symmetric key.Here are the results of some some sample runs : On Linux with OpenSSL keysOn Linux with PyCrypto keysOn Windows with OpenSSL keysOn Windows with PyCrypto keysHere is the test case : Is there any reason why PyCrypto seems to be such a basket case ?"
"I have a script that is iterating through images of different forms . When parsing the Google Vision Text detection response , I use the XY coordinates in the 'boundingPoly ' for each text item to specifically look for data in different parts of the form . The problem I 'm having is that some of the responses come back with only an X coordinate . Example : I 've set a try/except ( using python 2.7 ) to catch this issue , but it 's always the same issue : KeyError : ' y ' . I 'm iterating through thousands of forms ; so far it has happened to 10 rows out of 1000 . Has anyone had this issue before ? Is there a fix other than attempting to re-submit the request if it reaches this error ?"
"As example , I have the following code which creates a dataframe with an index containing a single value - the date '2018-03-06 ' ( a Tuesday ) . Note that this date falls in the week of 2018-03-05 ( a Monday ) : which produces : Why does pandas roll the date forward one week ? I would have expected the result to have been resampled to 2018-03-05 since that is the week during which the values were generated and I 'm using freq= ' W-MON'.UPDATEAs was pointed out , I needed to add the label argument to resample which defines which bin edge to use . Using label='left ' solves the problem of bucketing the dates in the correct week except when the date falls on the start of the week ( in this case , Monday ) . For example , if I apply resample to the date 2018-03-05 using label='left ' then the resampled value is 2018-02-26 when it should be 2018-03-05 ."
"The usage of { } in Python f-strings is well known to execute pieces of code and give the result in string format ( some tutorials here ) . However , what does the '= ' at the end of the expression mean ?"
I 'm using ndb.Model . The Search API has the following field classes : Suppose I have a 'tags ' field which is a list field : How am I supposed to treat this field with search.Document ? Right now I 'm turning tags list into a string : And then : Any suggestions ?
"I need sort my dictionary by another element who determine your order.This sorting can come as a list or a dictionary , whichever is easier.Dictionary after sort :"
"With the following example list : L = [ ' a ' , ' b ' , ' c ' , 'd ' ] I 'd like to achieve the following output : Pseudo-code would be :"
butWhy get same id ( ) result only when assign string ? Edited : I replace `` ascii string '' with just `` string '' . Thanks for feedback
"I need Django 0.96 . I have a huge Django project which I need to run , but it 's very very tied to 0.96 , I could take a looot of time to port it 1.xWhen I do pip install django==0.96 I get thisHow do I Install Django 0.96 ?"
"So I am currently learning python the hard way . In one of the exercises we print string literals with variables between them . I noticed that unlike other languages python automatically adds a space between a string literal and variables . I was just curious as to how it does that . Below is an example of what I mean . example.py : terminal : This is silly and not very important but I am curious , and I am hoping that SO can enlighten me !"
My question is very similar to Cumsum within group and reset on condition in pandas and Pandas : cumsum per category based on additional condition but they do n't quite get me there due to my conditional requirements . I have a data frame that looks like this : I want to create another column `` Cumulative '' that calculates the cumsum of Delta for each TransactionId . So the result would look like this : I have the condition for checking TransactionId equality setup : But I ca n't figure out how to add the Delta value to the previous Cumulative row .
"Consider the following sample code : There is nothing syntactically wrong ( using Python2.7 ) with the code except that if you run python with warnings turned on , you would see a DeprecationWarning : FYI , this is because .message was deprecated since python2.6 and removed in python3.Now , I 'd like to find all places in the project where .message is called on any exception instance by using static code analysis tools . As an end goal , I 'm planning to have this check running as a part of a daily build & test & code quality check task and raise an error if the syntax is still used.Is it possible ? Is it something that pylint , pyflakes or other code analysis tools are capable of ? I found that pep8 tool has several similar checks implemented , for instance , has_key ( ) usage check : As an alternative solution , I can treat all warnings as errors ( like suggested here ) and make my tests fail but this has its disadvantages : there are other deprecation warnings coming from third-party packages that I can not fixstrictly speaking , this requires 100 % coverage , which is difficult to maintain"
"I have a file that was written with the following Delphi declaration ... I want to analyse the data in the files ( many MB in size ) using Python if possible - is there an easy way to read in the data and cast the data into Python objects similar in form to the Delphi records ? Does anyone know of a library perhaps that does this ? This is compiled on Delphi 7 with the following options which may ( or may not ) be pertinent , Record Field Alignment : 8Pentium Safe FDIV : FalseStack Frames : FalseOptimization : True"
I am writing some functions to do things like format dates and text in my templates.template : output : I am not sure what the real difference between functions and filters are . It seems to me that filters just look cleaner ?
"I want to plot the learning curves of a K Nearest Neighbors classifier . I have the following code : This code worked perfectly before with e.g . a Decision Tree or a Random Forest , but here I get the following weird error : Any idea what this means and how I can fix this ? EDIT : After updating scikit-learn to version 0.16 , I got the following error :"
"I 'm writing a command-line directory navigator for Windows in Python and struggling a bit with os.path.join . Here 's , in essence , what I 'm trying to do : The problem is that os.path.join is not inserting a '/ ' after ' C : ' and I ca n't figure out why . Any help ? Edit : In case anyone in the future comes here looking for a solution , I just added os.sep after `` C : '' instead of hardcoding a backslash and that worked ."
"I am trying to figure out how to pass the following conditional statement into the python interpreter 's command option ( -c ) .However , I continually get syntax errors , such as the following : I found it surprisingly difficult to find a good example of this usage . I must be missing something big here ..."
"Is it possible to have a Python interpreter open in a Vim buffer ? Something like : Right now I have Vim open and a separate interpreter open . I saw the answer to this and was blown away.Anyway , thanks for the help and if there is something I can just google then point me in that direction ."
"Is there any way in a panel of NxM subplots to just have the axes being shown for the left column and bottom row.Where A = axis and N = no axisSometimes my subplots are 10x8 , 3x4 , 4x9 etc . and they all have the same x and y axis . I just want it to appear on the very left and the very bottom of that subset . At the moment I have to know which axis it is plotting to and doI want to generalise this for any NxM panel arrangement without having to know before hand.Thanks.EDIT : I have found a way to do the y-axis . I setup the number of panels wide using : Which means I can just do Any ideas for bottom ? EDIT : Worked it out . x-axis is as follows :"
"I am trying to write a simple proof-of-work nonce-finder in python.Now I am trying to do this multiprocessed , so it can use all CPU cores and find the nonce faster . My idea is to use multiprocessing.Pool and execute the function proof_of_work multiple times , passing two params num_of_cpus_running and this_cpu_id like so : So , if there are 4 cores , every one will calculate nonces like this : So , I have to rewrite proof_of_work so when anyone of the processes finds a nonce , everyone else stops looking for nonces , taking into account that the found nonce has to be the lowest value possible for which the required bytes are 0 . If a CPU speeds up for some reason , and returns a valid nonce higher than the lowest valid nonce , then the proof of work is not valid.The only thing I do n't know how to do is the part in which a process A will only stop if process B found a nonce that is lower than the nonce that is being calculated right now by process A . If its higher , A keeps calculating ( just in case ) until it arrives to the nonce provided by B.I hope I explained myself correctly . Also , if there is a faster implementation of anything I wrote , I would love to hear about it . Thank you very much !"
I have an Html document that look like this : I want to scrape only links that immediately follows the code tag . If I do soup.findAll ( ' a ' ) it returns all hyperlinks.How can I make BS4 to start scraping after that specific code element ?
"Warning , this is a sheer-laziness query ! As my project develops , so also do the number of functions within a class . I also have a number of classes per .py file . So what I would like to do is re-sort them to that the function names are organised [ sorry , UK here , I 've already compromised hugely with the ' z ' in Alphabetizing ; - ) ] alphabetically . Eg currently : ..and for ease of lookup , I 'd like to rearrange to : purely for cosmetic reasons , as it makes searching for the relevant functions more intuitive . I 'd obviously want to keep the init ( ) etc at the top of the class . Now , I can ( but have n't yet ) do this with a python script that reads in the .py the class ( es ) reside in as a text file , and do a tonne of string manipulation . But I wonder is there a better way to do this ? Perhaps there 's a tool I can use somewhere ? I 've had a good hunt , but I ca n't really find anything I could use.I 'm using emacs , so perhaps there 's a big 5-minute key combination sequence that will do the magic ( eg ) , but I ca n't see anything in python-mode ! Finally , I would like some way to be able to visualise and publicise the class structure/docstrings . When I say visualise structure , I mean , for example , where myClass.a ( ) calls myOtherClass.g ( ) , that would be a link , so I 'd be able to see what calls what , either on a class-by-class basis ( ie , what functions/classes does myClass use ? ) or overall ( here are a bunch of files , how do they 'connect ' ? ) When I say publicise the class , I guess I mean something like the API documentation you see in eg1 , eg2 ."
"I am developing an interactive app with bokeh ( 0.12.2 ) that updates plots based on specific interactions.For now I use sliders to change positions of a glyph in a plot , but I actually want to access the position of my mouse within a specific plot.The dataset is a multidimensional matrix ( tensor ) , dense data , and each plot displays one dimension at a specific location . If I change the position of the marker glyph on one plot , the other plots need to be updated , which means I have to slice my dataset according to the updated position.Here 's a simple example I tried to get the mouse data in my bokeh server update function using the hover tool : Unfortunately , the server only outputs : { ' y ' : [ 0 , 1 ] , ' x ' : [ 0 , 1 ] } { ' y ' : [ 0 , 1 ] , ' x ' : [ 0 , 1 ] } { ' y ' : [ 0 , 1 ] , ' x ' : [ 0 , 1 ] } { ' y ' : [ 0 , 1 ] , ' x ' : [ 0 , 1 ] } Is there a way to access the mouse position ( in python code ) ? Even accessing the position of a glyph would be sufficient ( because I can change the position of the glyph with some javascript code ) .EDIT : So I recently found out that there is this tool_events.on_change ( ) that I could use for this purpose . Unfortunately it does only work for TapTool , LassoSelectTool and BoxSelectTool , not for HoverTool : Based on an answer I found here : How can I get data from a ColumnDataSource object which is synchronized with local variables of Bokeh 's CustomJS function ? . The problem with this solution is that I can not use pan and trigger the tool_events callback . I can only click ( TapTool ) or pan and trigger a callback only once ( Lasso/BoxSelectTool ) . I actually wish to trigger such a callback on every mouse move.."
"I 'm witnessing the logging module behaving in a funny way . Am I missing something ? I 'm doing the usual thing of having two handlers : a StreamHandler for logging only INFO and higher to the console , and a FileHandler that will also handle all the DEBUG information.It worked fine until I decided to have a different format for the exeptions . I wanted a full stacktrace in the file , but just the exception type and value on the console . Since handlers have a setFormatter function , and since it seems easy to write a subclass of logging.Formatter , I thought it would work.The console handler and the file handler both have their own formatter . The print statements in the code proves it . However , the call to logger.exception will use only the formatException of the first handler added = > the exception is logged in the file with the format it should have in the console . Change the order of the logger.addHandler lines and then it 's the formatException of the file handler that 's used everywhere.What 's going on ? EDIT : I 'm using python 2.6 by the way.EDIT : Typo in code about `` console_formatter '' variable name corrected ."
"Is it possible to shuffle only a ( continuous ) part of a given list ( or array in numpy ) ? If this is not generally possible , how about the special case where the first element is fixed while the rest of the list/array need to be shuffled ? For example , I have a list/array : where the first element should always stay , while the rest are going to be shuffled repeatedly.One possible way is to shuffle the whole list first , and then check the first element , if it is not the special fixed element ( e.g . None ) , then swap its position with that of the special element ( which would then require a lookup ) .Is there any better way for doing this ?"
I am trying to make a simple function to download file in pythonThe code is something likeMy issue is that if I want to cancel the download process in the middle of downloading how do I approach ? ? ? This function runs in the background of app and is triggered by a button . Now I am trying to trigger it off with another button.The platform is XBMC .
"So i have a function based view in my Django rest framework and i do authentication in it as follows : Now in this view file , I have quite a few views and inside each function , i use the if else to check the authentication.So in order to reduce the lines of code , i decided to make this a function and then call it inside each function view as follows : However , this does not work.This could be really silly but i am clueless as to what is amiss here.."
"Similarly to this question , I 'm interested in creating time series spirals . The solution does n't necessarily have to be implemented in R or using ggplot , but it seems the majority of solutions have been implemented in R with ggplot , with a handful in Python and one in d3 . My attempts so far have all used R. Unlike this question , I 'm interested in displaying specific ranges of data without quantizing/binning the data . That is , I 'd like to display a spiral timeline showing when particular events start and stop , where theta-min and theta-max of every event represent specific points in time.Consider this travel data : I 'd like to plot these discrete events the same way the below plots do , but where 2pi is one week rather than 24 hours in order to illuminate the periodicity of these events , where color represents distance . I 've attempted modifying the solution linked at the beginning of this question , but it has n't gotten me anywhere . My new approach is to modify this solution , but I 'm having a difficult time getting anything but horizontal and vertical lines scattered about a spiral . Making them curve and display in the correct locations is tough.I 'm open to any approach that successfully displays the data in a spiral plot without quantizing/binning it into specific intervals but rather allows the intervals themselves to describe discrete events along a continuous spiralling timeline . Likewise , I 'm not interested in converting this to a raw single-point time series format where I 'd have a great deal of data representing the time between trips . I 'd like to achieve this in a temporal format ( one that describes a time window rather than an event at a particular time ) ."
"For a Django application that I 'm working on , I wanted to allow group membership to be determined by Active Directory group . After a while of digging through the pywin32 documentation , I came up with this : I spent a while googling before I figured this out though , and the examples I found almost exclusively used LDAP for this kind of thing . Is there any reason why that 's to be preferred over this method ? Bear a couple things in mind : I 'm not using Active Directory to actually perform authentication , only permissions . Authentication is performed by another server.While it would be nice to have some cross-platform capabilities , this will probably run almost exclusively on Windows ."
"myscript.pyBash example : If it is not being piped to , it would hang.What is the way to know if the script should or should not read from stdin ; I 'm hoping there 's something obvious other than a command line argument for the script ."
"I 've been experimenting with Seaborn 's lmplot ( ) and Statsmodels .ols ( ) functions for simple linear regression plots and their associated p-values , r-squared , etc . I 've noticed that when I specify which columns I want to use for lmplot , I can specify a column even if it has multiple words for it : However , if I try to use ols , I 'm getting an error for inputting in `` Count of Specific Strands '' as my dependent variable ( I 've only listed out the last couple of lines in the error ) : Conversely , if I specify the `` Counts of Specific Strand '' as shown below , the regression works : Does anyone know why this is ? Is it just because of how Statsmodels was written ? Is there an alternative to specify the dependent variable for regression analysis that does n't involve iloc or loc ?"
"Using the django dev server ( 1.7.4 ) , I want to add some headers to all the static files it serves.It looks like I can pass a custom view to django.conf.urls.static.static , like so : And common.views.static.serve looks like this : However , simply having django.contrib.staticfiles in INSTALLED_APPS adds the static urls automatically , and there does n't seem to be a way to override them . Removing django.contrib.staticfiles from INSTALLED_APPS makes this work , however , if I do that , the staticfiles templatetags are no longer available.How can I override the headers that are served for static files using the django development server ?"
"This question is maybe related to another SO question.I 'm running MacOS 10.11.2 El Capitan . Wishing rich GUI featuresaround my OpenGL applications I decided to give PyQT5 a shot to create the OpenGL context so I can integrate my OpenGL as a QtWidget int a GUI application . QT5 provides several API methods for QGlWidget which I summarize shortly here : initializeGL : gets invoked once before paintGL paintGL : place to draw stuff to active frame bufferI 'm able to create the widget and initialize shaders etc . But when it comes to framebuffer related operations like glClear an error appears : I found a website reporting about related issue . It seems that there is no framebuffer configured when the API methods get invoked . As I feel it should be the task of QT I did not try to configure window framebuffer myself . But I found that after some calls of the API methods the framebuffer was created magically . Thus I build a little hack which will wait until paintGL was invoked NSKIP_PAINTGL=3 times . Then I configure my object so the normal paintGL process starts to work . This seems to work . But sometimes it needs more than NSKIP_PAINTGL times , so I included a little sleep within the workaround . QT seems to create the frame buffer a little after it should . Maybe QT does it in a separate thread ? The QOpenGLWidget confirm that the framebuffer may not be created at some time : Returns The frame buffer object handle or 0 if not yet initialized.I do n't like work arounds like this , since I am afraid of raised conditions here . Also I do not have a lot of control here ( I need to rely on the fact that QT invokes paintGL often enough in first place so the hack can work ) . I 'm not familiar with QT framework at the moment so here is my question : How can I create some kinda loop which , when QGLControllerWidget was created , runs updateGL methods covered by a try/catch and retries as long as the GlError appears ? Alternatively the loop may listen to QOpenGLWidget : :defaultFramebufferObject ( ) and waits for an object handle.Of course I want to integrate this hack as elegant as possible into QT application flow - doing it the cute way . Or did I miss something here ? Is it possible to setup PyQT in some way so it wo n't invoke the OpenGL API methods before a valid framebuffer exists ? Here is an isolated code with the hack which runs on my Mac : Note that also C++ snippets are welcome as I will try convert then into python ."
"I am using Python version 2.6 and am learning NumPy version 1.3.I have tried out several NumPy array initialization and column splicing examples below , and added some inline questions as comments and a list of findings in the end . Hopefully someone can explain to me what is behind the differences in behaviors . Lots of inter-related questions and a rather long post , but each example is small , feel free to just answer one or a couple.a ) Initialize from a list of tuplesb ) A normal list of tuplesQuestion 1 : a ) looks like a list of tuples from print , except withoutthe comma between the tuples . If I print it with repr ( a ) , it even hasthe commas . Even so , it should no longer be considered the same as b ) , correct ? c ) Fail : Try to initialize array returned from np.zeroes as a list of listQuestion 2 : Is the below failing because the dtype does not match thelist that I passed in ? d ) Fail : Same as c ) but try to set the dtype as a listQuestion 3 : Is the below failing , because I am not allowed to specify a dtype that is a list ? e ) Try to initialize array using np.array from a list of a listQuestion 4 : Why would e ) below which is also a list of list work , but d ) fail ? f ) Try to initialize array using np.array from a list of a tuplesQuestion 5 : Same example as e ) , but this time initializing withlist of tuples he print out of f ) is identical as e ) , soinitializing with list of tuples and list of list are reallyidentical then ? g ) Try to initialize array using np.array from a CSV fileQuestion 6 : Same example as e and f , but this time initializingfrom file Minor difference in quoting for the print out . Thereshould be no difference # between the array generated like thisand e ) and f ) right ? h ) Splicing the NumPy arrays by columnQuestion 7 Why would splicing e below work , but a fail above with Index error with the same syntax ? Finding 1 : Initializing numpy.ndarray by using nd.array and a list of tuples , list of list , or CSV file are identical . This is maybe contrary to what this other answer that I viewed that says np.array expects a list of a tuples , Stack Overflow question Define dtypes in NumPy using a list ? .Finding 2 : Initializing numpy.ndarray by using np.zeroes , I am unable to initialize the ndarray from a list of a list.Finding 3 : For column splicing , initializing numpy.ndarray by using nd.array , I could do a column splice ( that is , e [ : ,2 ] , but the syntax of splicing , using the np.zeroes initialization method is different a [ 'f2 ' ] . A normal list of tuples can not be spliced ."
"Sometimes the printed numpy array is provide to share the data such as this post . So far , I converted that manually . But the array in the post is too big to convert by hand.I want to convert a string representation of a numpy array back to an array . ( Thanks , @ LevLevitsky . I reference your expression . ) I tried this codeHowever , this could not retain the data type . Also if ndim > 3 , It does not work properly.is interpreted as"
"I have two pandas dataframes , say df1 and df2 , of some size each but with different indexes and I would like to sum up the two dataframes element by element . I provide you an easy example to better understand the problem : so df1 will beand df2 will benow if typewhat I get isHow can I make pandas understand that I want to sum up the two dataframe just element by element ?"
"Is there an equivalent to pandas.cut ( ) in Dask ? I try to bin and group a large dataset in Python . It is a list of measured electrons with the properties ( positionX , positionY , energy , time ) . I need to group it along positionX , positionY and do binning in energy classes.So far I could do it with pandas , but I would like to run it in parallel . So , I try to use dask.The groupby method works very well , but unfortunately , I run into difficulties when trying to bin the data in energy . I found a solution using pandas.cut ( ) , but it requires to call compute ( ) on the raw dataset ( turning it essentialy into non-parallel code ) . Is there an equivalent to pandas.cut ( ) in dask , or is there another ( elegant ) way to achieve the same functionality ? Thanks a lot !"
"In my dataframe I have some categorical columns with over 100 different categories . I want to rank the categories by the most frequent . I keep the first 9 most frequent categories and the less frequent categories rename them automatically by : OTHERExample : Here my df : I keep the first 3 categories then I rename the rest by `` OTHER '' , how should I proceed ? Thanks ."
"I upgraded from Qt4 to Qt5 ( PyQt , to be specific ) and QWheelEvent broke for me - it returns empty pixelDelta ( ) , but phase is at 2 ( default ) . I am on Win 7 , so the warning about phases should n't apply to me . When I run this code : scrolling prints 'PyQt5.QtCore.QPoint ( ) ' without coordinates . What can I do ?"
"I 'm trying to make my Travis CI send test coverage data to Code Climate service , but documentation on Code Climate and Travis CI do not describe in detail how to do this using Python . Still its supported feature according Code Climate and Travis documentations . I 've tried to find any working examples on this without luck and ca n't make it work on my own.Code Climate documentation : Setting Up Test Coverage , Readme : codeclimate-test-reporter Travis CI documentation : Using Code Climate with Travis CII 've set the CODECLIMATE_REPO_TOKEN in Travis CI as described in this answer : https : //stackoverflow.com/a/31281481/1754089My .travis.yml file : as the after_success line is excecuted in Travis it gives me this in log-view :"
"I have a Django template where I 'm trying to display a list as an unordered list in html . Currently I have it done it in a pretty messy way , using |length and |slice : If list_tasks has 253 elements , the output is something like this : Is there a neater and cleaner way of doing this ?"
"The program asks the user for a number N.The program is supposed to displays all numbers in range 0-N that are `` super numbers '' . Super number : is a number such that the sum of the factorials of its digits equals the number.Examples:12 ! = 1 ! + 2 ! = 1 + 2 = 3 ( it 's not super ) 145 = 1 ! + 4 ! + 5 ! = 1 + 24 + 120 ( is super ) The part I seem to be stuck at is when the program displays all numbers in range 0-N that are `` super numbers '' . I have concluded I need a loop in order to solve this , but I do not know how to go about it . So , for example , the program is supposed to read all the numbers from 0-50 and whenever the number is super it displays it . So it only displays 1 and 2 since they are considered superI have written two functions ; the first is a regular factorial program , and the second is a program that sums the factorials of the digits :"
"I am translating a code from perl to python.Even if it works exactly the same , there is a part of the code that is 5x slower in python than in perl and I can not figure out why.Both perl and python are in the same machine , as well as the mysql database.The code queries a db to download all columns of a table and then process each row.There are more than 5 million rows to process and the big issue is in retrieving the data from the database to the python processing.Here I attach the two code samples : Python : While here comes the Perl equivalent script : The runtime for these two scripts is : ~40s for the Perl script~200s for the Python scriptI can not figure out why this happens [ I tried using fetchone ( ) or fetchmany ( ) to see if there are memory issues but the runtime at most reduces 10 % from the 200s ] .My main problem is understanding why there is such a relevant performance difference between the two functionally equivalent code blocks.Any idea about how can I verify what is happening would be greatly appreciated.Thanks ! UPDATE ABOUT SOLUTIONPeeyush'comment could be an answer and I 'd like him to post it because it allowed me to find a solution.The problem is the python connector . I just changed that for mySqlDb module which is a C compiled module . That made the python code slightly faster than the perl code.I added the changes in the python code with a < -- -- `` '' to show how easy it has been to gain performance ."
"I 'm running a function in another thread that is supposed to fill out a dialog and then show it but it just seg faults as soon as I tried to alter the dialog in any way . I 've read that this is a common issue with WxPython and that devs are not intended to directly alter dialogs in another thread.How do I get around this ? I can just call the function in my main thread but that will block my GUI and it is a lengthy operation to initialize the dialog - I would like to avoid this.My code is similar to the below.In the main threadThe function I am callingEven with a blank dialog and just a simple call to show inside the function I get a segmentation fault . Any help would be greatly appreciated , thanks ."
"I 'm writing Python that targets versions 3.2 and higher . It looks like using the built-in function callable is the most straightforward and efficient way to do this . I 've seen recommendations for hasattr ( x , `` __call__ '' ) , collections.Callable ( x ) , and just using try/except around an attempted call.I 've tested items that are callable ( a class and a function ) , using timeit with 100,000 iterations ; in both cases using callable takes only about 75 % of the time of checking for the attribute . When the item is not callable ( an integer and a string ) using callable stays at the same cost as a class or function while checking for the attribute is about 2.3 times more expensive than for a class or function . I did n't expect that difference , but it also favors the clear and concise callable ( x ) approach.But I 'm relatively new to Python and no expert , so are there reasons I 'm not aware of that I should use the hasattr approach or another approach ? FWIW , the results of the various timeits follow . The first character is just t for timeit , the second indicates what the type of the object being tested ( c = class , f = function , i = integer , s = string ) , and the rest indicates the method ( attr - check attribute , call - use callable , try - use try/except ) ."
"I 'm trying to implement SVG path calculations in Python , but I 'm running into problems with Arc curves.I think the problem is in the conversion from end point to center parameterization , but I ca n't find the problem . You can find notes on how to implement it in section F6.5 of the SVG specifications . I 've also looked at implementations in other languages and I ca n't see what they do different either.My Arc object implementation is here : You can test this with the following code that will draw the curves with the Turtle module . ( The raw_input ( ) at the end is just to that the screen does n't disappear when the program exits ) .The issue : Each of these four arcs drawn should draw from the Start point to the End point . However , they are drawn from the wrong points . Two curves go from end to start , and two goes from 100 , -50 to 0,0 instead of from 0,0 to 100 , 50.Part of the problem is that the implementation notes give you the formula from how to do the conversion form endpoints to center , but does n't explain what it does geometrically , so I 'm not all clear on what each step does . An explanation of that would also be helpful ."
"I 'm totally new to Python ( as of half an hour ago ) and trying to write a simple script to enumerate users on an SMTP server.The users file is a simple list ( one per line ) of usernames.The script runs fine but with each iteration of the loop it slows until , around loop 14 , it seems to hang completely . No error - I have to ^c . Can anyone shed some light on the problem please ? TIA , Tom"
"Can I eliminate all Python loops in this computation : where x [ i ] , y [ j ] , z [ k ] are vectors of length N and x , y , z have first dimensions with length A , B , C s.t . output is shape ( A , B , C ) and each element isthe sum of a triple-product ( element-wise ) .I can get it down from 3 to 1 loops ( code below ) , but am stuck trying toeliminate the last loop.If necessary I could make A=B=C ( via small amount of padding ) ."
"BackgroundI write small python packages for a system that uses modules ( https : //luarocks.org/ ) to manage packages . For those of you who do n't know it , you can run module load x and a small script is run that modifies various environmental variables to make software ' x ' work , you can then undo this with module unload x.This method of software management is nearly ubiquitous in scientific computing and has a lot of value in that arena : you can run ancient unmaintained software alongside packages that that software would interfere with , you can run multiple versions of software , which allows you to reproduce your data exactly ( you can go back to old versions ) , and you can run frankly poorly written non updated software with outdated dependencies.These features are great , but they create an issue with the python 2/3 split : What if you want to write a package that works with both python 2 and 3 and use it alongside software that requires either python 2 or 3 ? The way you make old python2 dependent software work on these large systems is that you make a python/2.7.x module and a python/3.5 module . When you want to run a script that uses python 2 , you load that module , etc.However , I want to write a single python package that can work in either environment , because I want that software to be active regardless of which python interpreter is being used.This is fundamentally extremely easy : just use a # ! /usr/bin/env python shebang line , done . That works . I write all my software to work with either , so no problem.QuestionThe issue is : I want to use setuptools to distribute my package to other scientists in the same situation , and setup tools mangles the shebang line.I do n't want to get into a debate about whether mangling the shebang line is a good idea or not , I am sure it is since it has existed for years now in the same state . I honestly do n't care , it does n't work for me . The default setuptools install causes the software not to run because when a python interpreter 's module is not loaded , that python interpreter does not function , the PYTHONPATH is totally wrong for it.If all of my users had root access , I could use the data_files option to just copy the scripts to /usr/bin , but this is a bad idea for compatibility , and my users do n't have root access anyway so it is a moot point.Things I tried so far : I tried setting the sys.executable to /usr/bin/env python in the setup.py file , but that does n't work , because then the shebang is : # ! `` /usr/bin/env python '' , which obviously does n't work . I tried the Do n't touch my shebang class idea in this question : Do n't touch my shebang ! ( it is the bottom answer with 0 votes ) . That did n't work either , probably because it is written for distutils and not setuptools . Plus that question is 6 years old.I also looked at these questions : Setuptools entry_points/console_scripts have specific Python version in shebangChanging console_script entry point interpreter for packagingThe methods described there do not work , the shebang line is still altered.Creating a setup.cfg file with the contents : :also does not change the shebang line mangling behavior.There is an open issue on the setuptools github page that discusses something similar : https : //github.com/pypa/setuptools/issues/494So I assume this is n't possible to do natively , but I wonder if there is a workaround ? Finally , I do n't like any solution that involves asking the user to modify their install flags , e.g . with -e.Is there anyway to modify this behavior , or is there another distribution system I can use instead ? Or is this too much of an edge case and I just need to write some kind of custom installation script ? Thanks all.UpdateI think I was not clear enough in my original question , what I want the user to be able to do is : Install the package in both python2 and python3 ( the modules will go into lib/pythonx/site-lib.Be able to run the scripts irrespective of which python environment is active.If there is a way to accomplish this without preventing shebang munging , that would be great.All my code is already compatible with python 2.7 and python 3.3+ out of the box , the main thing is just making the scripts run irrespective of active python environment ."
"I have been developing feedforward neural networks ( FNNs ) and recurrent neural networks ( RNNs ) in Keras with structured data of the shape [ instances , time , features ] , and the performance of FNNs and RNNs has been the same ( except that RNNs require more computation time ) .I have also simulated tabular data ( code below ) where I expected a RNN to outperform a FNN because the next value in the series is dependent on the previous value in the series ; however , both architectures predict correctly . With NLP data , I have seen RNNs outperform FNNs , but not with tabular data . Generally , when would one expect a RNN to outperform a FNN with tabular data ? Specifically , could someone post simulation code with tabular data demonstrating a RNN outperforming a FNN ? Thank you ! If my simulation code is not ideal for my question , please adapt it or share a more ideal one ! Two features were simulated over 10 time steps , where the value of the second feature is dependent on the value of both features in the prior time step.FNN : LSTM :"
"I can do append value to a tuplesBut how can i append a tuples to a tuplesHow can i make it ( ( 1 , 2 ) , ( 3 , 4 ) , ( 5 , 6 ) , ( 8 , 9 ) , ( 0 , 0 ) )"
"I have been working on this for a few weeks now and I 've read many questions about python memory leak but I just ca n't figure it out.I have a file that contains about 7 million lines . For each line , I need to create a dictionary . So this is a list of dictionary that looks like : What I am doing is ... The problem is that when I run this it always gets killed around the 6000000th line . Originally I was just doing dict = { } but changed it so I do dict.clear ( ) after reading similar posts , but it did n't improve anything . I know some posts mentioned about circular references and I looked into my code but I did n't think I have that problem . I doubt that storing 7 million dictionaries in a list ca n't be handled in Python ? I would appreciate any advice on how I can run the whole things without getting killed . ( The version is 2.7.4 )"
"I have two different timeseries with partially overlapping timestamps : which represents following data : I would like to calculate a weighted average on every day with coefficients a ( 0.3 ) and b ( 0.7 ) , while ignoring missing values : when I first try to align these timeseries : I get correctly masked timeseries : but when I do a1 * 0.3 + b1 * 0.7 , it ignores values , that are present in one timeseries only : What should I do to receive the awaited ? EDIT : The answer should be applicable also to more than two initial timeseries with different weights and differently missing values.So if we have four timeseries with weights T1 ( 0.1 ) , T2 ( 0.2 ) , T3 ( 0.3 ) and T4 ( 0.4 ) , their weights at a given timestamp will be :"
Should n't the results be the same ? I do not understand .
"I am working with a cluster system over linux ( www.mosix.org ) that allows me to run jobs and have the system run them on different computers . Jobs are run like so : This will naturally create the process and run it on the background , returning the process id , like so : Later it will return . I am writing a Python infrastructure that would run jobs and control them . For that I want to run jobs using the mosrun program as above , and save the process ID of the spawned process ( 29199 in this case ) . This naturally can not be done using os.system or commands.getoutput , as the printed ID is not what the process prints to output ... Any clues ? Edit : Since the python script is only meant to initially run the script , the scripts need to run longer than the python shell . I guess it means the mosrun process can not be the script 's child process . Any suggestions ? Thanks"
I 'm using Class Based Views for the first time . I 'm having trouble understating how using class based views I would implement django-endless-pagination twitter styling paging.Could I have an example of how one would go about this ? This is my view :
"I have two dataframes which are datetimeindexed . One is missing a few of these datetimes ( df1 ) while the other is complete ( has regular timestamps without any gaps in this series ) and is full of NaN 's ( df2 ) . I 'm trying to match the values from df1 to the index of df2 , filling with NaN 's where such a datetimeindex does n't exist in df1.Example : Using df2.combine_first ( df1 ) returns the same data as df1.reindex ( index= df2.index ) , which fills any gaps where there should n't be data with some value , instead of NaN.This is what I was hoping to get : Could someone shed some light on why this is happening , and how to set how these values are filled ?"
"I 've been biting my nails on this one for quite some time . In my Flask-app I currently have a product-database , for which in the app I have a page that queries each product column into a table . For example I 'd have product 1234 for which I could view the details ( i.e . database columns ) in example.com/items/1234 which would give me the following : What I 'm struggling with is the following : I 'd like to be able to add new products with same table-style . For this I have created a form like the following : I am now at a total loss on how to customize the form appearance in the template beyond just { { wtf.quick_form ( form ) } } . What I tried is the following : The table looks good ( at least that ) but the request is not being sent correctly I presume . The page loads correctly with `` POST /url HTTP/1.1 '' but it does n't seem to come through correctly . What I mean by that is that though the request is being sent correctly , as I can see if I run the app via the Flask server . However it seems that nothing is transmitted to the database . The page simply reloads with the entered data still in the fields and nothing transmitted to the database . If I simply use the wtf.quick_form then the data is correctly sent to the database . So how can I correctly customize the form appearance and/or crucial step did I miss ?"
"I was going through the code for SVM loss and derivative , I did understand the loss but I can not understand how the gradient is being computed in a vectorized mannerUnderstood up to here , After here I can not understand why we are summing up row-wise in binary matrix and subtracting by its sum"
"I want to get low level access to webcam properties using DirectShow 's IAMVideoProcAmp.There are several Python modules ) pywin32 , pywintypes , comtypes , win32com , pythoncom ) that are used in this context and they seem to be related somehow . But I have no clue where to start.I found some examples ( here , here , here ) but I could not figure out how to get a IID / CLSID to use likeor with a clear name likeor Can someone help me with this ? I found another example ( dshow.py ) , but it has some dependencies that I could not find ( interfaces , uuids ) .This page from Microsoft lists the procedures as Call QueryInterface on the capture filter for the IAMVideoProcAmp interface.or Query the capture filter for the IAMCameraControl.and states some C++ code for this : Edit : I finally found some code that looks good so far : jaracoIt seem to do exactly what I am trying to write and uses some elements fromDirectShow ( see here ) : jaraco.video claims to be `` a port of the VideoCapture module in pure Python using ctypes and comtypes . `` It is using a DirectShow.tlb file ( whatever that is ) to get the definitions into comtypes A type library ( .tlb ) is a binary file that stores information about a COM or DCOM object 's properties and methods in a form that is accessible to other applications at runtime ."
"I have a large csv with two strings per row in this form : I read in the first two columns and recode the strings to integers as follows : This code is from https : //stackoverflow.com/a/39419342/2179021.The code works very well but is slow when df is large . I timed each step and the result was surprising to me.pd.read_csv takes about 40 seconds . le.fit ( df.values.flat ) takes about 30 secondsdf = df.apply ( le.transform ) takes about 250 seconds.Is there any way to speed up this last step ? It feels like it should be the fastest step of them all ! More timings for the recoding step on a computer with 4GB of RAMThe answer below by maxymoo is fast but does n't give the right answer . Taking the example csv from the top of the question , it translates it to : Notice that 'd ' is mapped to 3 in the first column but 2 in the second.I tried the solution from https : //stackoverflow.com/a/39356398/2179021 and get the following.Then I increased the dataframe size by a factor of 10.This method appears to use so much RAM trying to translate this relatively small dataframe that it crashes.I also timed LabelEncoder with the larger dataset with 10 millions rows . It runs without crashing but the fit line alone took 50 seconds . The df.apply ( le.transform ) step took about 80 seconds.How can I : Get something of roughly the speed of maxymoo 's answer and roughly the memory usage of LabelEncoder but that gives the right answer when the dataframe has two columns . Store the mapping so that I can reuse it for different data ( as in the way LabelEncoder allows me to do ) ?"
"I have written pytest unit tests for a package I 've written with a structure like below : packagedirsetup.pymypackage__init__.pyfunctionpackage__init__.pyfunctionmodule.pytest__init__.pyconftest.pyunit_tests__init__.pyfunctionmodule_test.pyNow packagedir/mypackage/test/conftest.py establishes two required command line parameters to be created as fixtures for the tests . When I run something like : All the tests run and the command line parameters are read in correctly . However , when I pip install the package ( which installs the test and unit_tests packages correctly ) and then try something like this : I get an error like this : If I run : the tests run but fail because the command line arguments are not present.Can someone explain to me what might be going wrong here ? I could guess and say this is because the -- pyargs argument changes the interpretation of the command line parameters for pytest , but it 's just a guess . I 'd like to know if there is an actual way I could fix this . The overarching goal would be to allow someone to pip install mypackage and then be able to easily test it without navigating to the site-packages location of the install ."
"I 'm using scipy 's convolve2d : After convolution all values are in magnitudes of 10000s , but considering I 'm working with images , I need them to be in the range of 0-255 . How do I normalize it ?"
"I have three lists as follows.For each concept in myconcepts , I want to get the count every other concept connected to it using mylist . Then remove the hatedconcepts from it . So , my output should looks like as follows.I was using this code to do it.The output of the code is : However now I have a huge mylist with a size of 3GB and nearly 9000 myconcepts . The hatedconcepts count is only 20 . It looks like it takes about two weeks to run using my current code . The main reason for this could be that my current program is O^3 which is not very efficient . Therefore , I am looking for ways to make my current program more efficient . I am even fine with pythonic solutions that even take 5-6 days to run . Please let me know your thoughts.I have added a portion of mylist in : https : //drive.google.com/file/d/1M3EhIRwwKwD3Kv4zDsmXaH1D73tx0eF3/view ? usp=sharing just to get some idea how it looks like.I am happy to provide more details if needed ."
"I have a python script I 'm trying to debug and I 'm using Pycharm Community Edition version 2016.3.2.What I 'd like to do is make some plots in the debug console ( which I activate by setting a breakpoint and starting the debugger ) , but the problem is that the plot simply does n't show up.Some code to get a reproducible example of my problem is provided on the official matplotlib documentation here , in particular this bit of code : What I find strange is that if I open a new python console from inside pycharm , when executing this code pycharm pops up a new window showing the plot , but this does n't happen if I paste the same code in the `` debug '' console.In both cases , I get the following output in the consoleI found a potentially related post here , but frankly I ca n't tell if the two problems reduce to the same issue ."
"It is few hours I am stuck with this : I have a DataFrame containing a list of email addresses , from those email addresses I want to check whether in the mail is contained or not a number I.E . roberto123 @ example.com , if yes I want this number to be appended to an array : I have tried both with a DataFrame , and also a ndarray woth numpy , but it does not work . This is what i am trying to do : I think my problem is that I do n't know how I can convert all the elements in this array to string so that the method isdigit ( ) would work and iterate through all the elements inside ( 825 mail addresses ) .When running the code above this is the error i get : Meanwhile , if i try with the numpy array ( mail_addresses_toArray ) this is the error :"
"I know that methods __repr__ and __str__ exist to give a formal and informal representation of class instances . But does an equivalent exist for class objects too , so that when the class object is printed , a nice representation of it could be shown ?"
"I am new to python and have looked at other answers on merging dictionaries but was still a bit confused . I am looking to merge together two dictionaries in python by a common value within a specific key to output that common key with the other keys from both dictionaries in a new dictionary . Here is sample data : I would like to merge on the key value pair of 'career_business ' : 'operations / logistics'and output a dictionary that looks like this : An additional problem is that I do not know if the names will match , and I am looping through a list of add_sal and a list of add_perc . Any advice would be appreciated ! Thank you in advance !"
"Mysql allows duplicate column names in the results of a query . So , in the terminal , none of the column names are prefixed using the above query.However , I 'm using mysqldb in python with the DictCursor . Results are a list of dictionaries where the column names are the keys . Sometimes , the dict cursor automatically prefixes the column name with the table name . As far as I can tell , it does this for the second of two ambiguous column names , but only if the second value is unique . Anyways , I 'd like to force the cursor to prefix ALL keys with the table name.From the mysqldb docs on the fetch.row ( ) function ... The second parameter ( how ) tells it how the row should be represented . By default , it is zero which means , return as a tuple . how=1 means , return it as a dictionary , where the keys are the column names , or table.column if there are two columns with the same name ( say , from a join ) . how=2 means the same as how=1 except that the keys are always table.column ; this is for compatibility with the old Mysqldb module.So , it seems doable , but I 'm not using the fetch.row ( ) function directly ... so the question is , how can I cause the mysqldb dict cursor to always use how=2 when it fetches rows ?"
I want to export data to separate text files ; I can do it with this hack : What is the right way to do it with Spark 1.3.1/Python dataframes ? I want to do it in a single job as opposed to N ( or N + 1 ) jobs . May be : saveAsTextFileByKey ( )
"I have a class with a dictionary that is used to cache response from server for a particular input . Since this is used for caching purpose , this is kept as a class variable.And when writing unit tests , since the dictionary is a class variable , the values are cached across test cases.Test CasesIf test_caching executes first , the cached value will be some mock object . If test_do_something executes first , then the assertion that the test case is called exactly once will fail.How do I make the tests independent of each other , besides manipulating the dictionary directly ( since this is like requiring intimate knowledge of the inner working of the code . what if the inner working were to change later . All I need to verify is the API itself , and not rely on the inner workings ) ?"
"I recently ran into the need to disable transaction requests in one of my views , in order to be able to call db.connection.close ( ) and connect ( ) during requests in an effort to improve performance.I have a DRF ViewSet , and used the following very simple view to verify that the non_atomic_requests decoractor seems to have no effect . ATOMIC_REQUESTS=True is enabled in settings.py , and DEBUG=False.After calling the view , I open Django shell , and verify that the amount of rows in the db has not grown , even though it should have . Also opening a debugger during the request to halt execution after the line m.save ( ) , I can observe in Django shell that a new row is not visible yet.If I set ATOMIC_REQUESTS=False in settings.py , the code works as expected , and the number of rows in the db is grown by one , even if an error is raised before returning from the view.When ATOMIC_REQUESTS=False , using @ transaction.atomic decorator does work as expected though . So as a workaround , I could use it to set every other view as atomic instead ... I am currently thinking this is a bug in the framework . Can anybody verify my findings , or point out if I am misunderstanding how this decorator is meant to function ? I am using Python 3.6 , Django 2.0 and DRF 3.7.7 ."
"Is there any equivalent of strict mocks in python ? Some mechanism to report unintended call of mocked methods ( action.step2 ( ) in this example ) , just like this in GoogleMock framework ."
"I 'm attempting to run tests on a GIS Django application running PostGIS as a database backend.When I attempt to run tests , I get the following error : The error makes sense . Only admin database users can install extensions since this privilege allows the execution of arbitrary external code . BUT since the test runner has to re-create the database each time the tests are run , Django 's database user ca n't proceed . Here is my database configuration ."
"I am writing a script to launch a load generation experiment on several hosts . I could write a bash script to start multiple ssh sessions , but I was hoping to use something more structured . Since I use Python for most of my scripting , I thought Fabric looked like a good option . The only problem is that I need to pass a small amount of host specific data with each command ( really just an id or counter ) , and I would like to run them in parallel . In other words , I would like to do something like the following , where host_num is different ( possibly just incremented ) for each host . Is this possible in Fabric ? If not , is there another tool I could use to accomplish the same thing ?"
"I have a Python web application in which the client ( Ember.js ) communicates with the server via WebSocket ( I am using Flask-SocketIO ) . Apart from the WebSocket server the backend does two more things that are worth to be mentioned : Doing some image conversion ( using graphicsmagick ) OCR incoming images from the client ( using tesseract ) When the client submits an image its entity is created in the database and the id is put in an image conversion queue . The worker grabs it and does image conversion . After that the worker puts it in the OCR queue where it will be handled by the OCR queue worker.So far so good . The WS requests are handled synchronously in separate threads ( Flask-SocketIO uses Eventlet for that ) and the heavy computational action happens asynchronously ( in separate threads as well ) .Now the problem : the whole application runs on a Raspberry Pi 3 . If I do not make use of the 4 cores it has I only have one ARMv8 core clocked at 1.2 GHz . This is very little power for OCR . So I decided to find out how to use multiple cores with Python . Although I read about the problems with the GIL ) I found out about multiprocessing where it says The multiprocessing package offers both local and remote concurrency , effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads.. Exactly what I wanted . So I instantly replaced the byThe queue needed to be handled by the multiple cores as well So i had to changetoas well . Problematic : the queue and the Thread libraries are monkey patched by Eventlet . If I stop using the monkey patched version of Thread and Queue and use the one from multiprocsssing instead then the request thread started by Eventlet blocks forever when accessing the queue.Now my question : Is there any way I can make this application do the OCR and image conversion on a separate core ? I would like to keep using WebSocket and Eventlet if that 's possible . The advantage I have is that the only communication interface between the processes would be the queue . Ideas that I already had : - Not using a Python implementation of a queue but rather using I/O . For example a dedicated Redis which the different subprocesses would access - Going a step further : starting every queue worker as a separate Python process ( e.g . python3 wsserver | python3 ocrqueue | python3 imgconvqueue ) . Then I would have to make sure myself that the access on the queue and on the database would be non-blockingThe best thing would be to keep the single process and make it work with multiprocessing , though.Thank you very much in advance"
"I 'd like to know if it 's possible to apply a function ( or juste an operation , such as replacing values ) to column in a python 2d array , without using for loops . I 'm sorry if the question has already been asked , but I could n't find anything specific about my problem.I 'd like to do something like : Which would mean put 1 for each value at the third column , orWhich would mean apply func ( ) to the third column of array.Is there any magic python-way to do it ? EDIT : The truth has been spoken . I forgot to say that I did n't want to avoid for ( ) statement to improve performance , but just because I do n't wan to add multiples lines for this precise instance . We got 2 answers here , one in a native way , and two more with the help of Numpy . Thanks a lot for your answers !"
"I create my python virtual environment using : to activate , I source venv3/bin/activate.venv3/bin/activate does n't appear to be all that complex : I can see it modifying $ PATH , and $ PS1 , creating a deactivate function , and even backing up old variables that it modifies so it can restore them when the user runs the deactivate function . All this makes sense.The one thing I do n't see is where python 's sys.path is modified . On my system , this is what I see : sys.path outside of virtual environment : sys.path inside of virtual environment : Clearly , sys.path gets modified at some point , somehow . This makes sense , since that 's how python knows where to find the third-party python libraries that are installed . I would think that this is the main feature of the virtual environment , but I ca n't see where it gets set.I 'm not trying to accomplish anything - mostly just curious ."
"I have a list of state-level data with numbers for each state , e.g . : and I want to make it into a weighted map , with darkest where the number is highest and lightest here it is lowest . Is there any software , or a java or python library that can generate such an image ?"
Assume I have some Celery tasks which are based on one abstract task.I want to do something on start of every task execution . Are there any possibilities to achieve this result ? Thanks .
"I need to get matrix of TF-IDF features from the text stored in columns of a huge dataframe , loaded from a CSV file ( which can not fit in memory ) . I am trying to iterate over dataframe using chunks but it is returning generator objects which is not an expected variable type for the method TfidfVectorizer . I guess I am doing something wrong while writing a generator method ChunkIteratorshown below.Can anybody please advise how to modify the ChunkIterator method above , or any other approach using dataframe . I would like to avoid creating separate text files for each row in the dataframe . Following is some dummy csv file data for recreating the scenario ."
"I 've just encountered the following issues . Starting from two arrays , and performing a boolean comparison like : returns a Boolean value instead of an array of booleans , whereas : works as expected . Is this an issue related to the sizes of the arrays ? Performing a simple comparison on the single dimension of the array works , no matter the size.Anyone is able to reproduce the problem ? I 'm on Numpy 1.9.2 and Python 2.7.3.EDIT : just update to Numpy 1.11 but the issue persists ."
"I 'm trying to access the Twitter stream which I had working previously while improperly using Tweepy . Now that I understand how Tweepy is intended to be used I wrote the following Stream.py module . When I run it , I get error code 401 which tells me my auth has been rejected . But I had it working earlier with the same consumer token and secret . Any ideas ?"
"I have this structure : In module.submodule.__init__.py I have this : In module.submodule.foo.py I have this : I would like to import bar only , but I got slowed down by foo ( let 's imagine there is an ugly time.sleep ( 3 ) in both foo and module/__init__.py ) . So my goal is to write this below without getting slowed down by other parts of my module : How can I just import saybar located in my submodule bar ?"
"I 'd like to mock timing so that be able to set certain time to a field of type DateTimeField with auto_now_add=True during my tests e.g : I 'm aware the current date is always used for this type of fields , that is why I 'm looking for an alternative to mock somehow the system timing , but just in a context.I 've tried some approaches , for instance , creating a context with freeze_time but did n't work : Ofc I guess , this is due to the machinery behind the scene to create the object when auto_now_add=True.I do n't want to remove auto_now_add=True and/or use default values.Is there a way we can mock the timing so that we can make this type of field to get the time that I want in certain context ? I 'm using Django 1.9.6 and Python 3.4"
Except that python-redis-lock module provides contextmanager for the lock object - what are the differences when compared to the lock object you get from redispy module ? what is so special about python-redis-lock ? Using redispy : Using python-redis-lock :
"I 've got a website written in bottle and I 'd like to deploy it via Amazon 's Elastic Beanstalk . I followed the tutorial for deploying flask which I hoped would be similar.I tried to adapt the instructions to bottle by making the requirements.txt this : and replaced the basic flask version of the application.py file with this : I updated to this version as instructed in the tutorial , and when I wrote eb status it says it 's green , but when I go to the URL nothing loads . It just hangs there . I tried the run ( ) method at the end as it is shown above and also how it is written in the bottle hello world application ( ie run ( host='localhost ' , port=8080 , debug=True ) ) and neither seemed to work . I also tried both @ route ( '/hello ' ) as well as the @ route ( '/ ' ) .I went and did it with flask instead ( ie exactly like the Amazon tutorial says ) and it worked fine . Does that mean I ca n't use bottle with elastic beanstalk ? Or is there something I can do to make it work ? Thanks a lot , AlexEDIT : With aychedee 's help , I eventually got it to work using the following application file :"
I am trying to run a specific test method with pytest on Python 3.6 with the following command : ( as mentioned in pytest docs ) and it fails with the following error message : when I run the whole test file the tests are running properly with : When I tried running a specific method in isolated project it works perfectly.Any reason why pytest ca n't find specifically my test methods on my project ? The file is very simple and looks like this :
"I am using sympy from time to time , but am not very good at it . At the moment I am stuck with defining a list of indexed variables , i.e . n1 to nmax and performing a summation on it . Then I want to be able to take the derivative : So far I tried the following : However , if i try to take the derivative with respect to one variable , this fails : I also tried to avoid working with IndexedBase.However , here already the summation fails because mixing python tuples and the sympy summation.How I can perform indexedbase derivatives or some kind of workaround ?"
"I am developing a Tensorflow network based on their MNIST for beginners template . Basically , I am trying to implement a simple logistic regression in which 10 continuous variables predict a binary outcome , so my inputs are 10 values between 0 and 1 , and my target variable ( Y_train and Y_test in the code ) is a 1 or 0.My main problem is that there is no change in accuracy no matter how many training sets I run -- it is 0.276667 whether I run 100 or 31240 steps . Additionally , when I switch from the softmax to simply matmul to generate my Y values , I get 0.0 accuracy , which suggests there may be something wrong with my x*W + b calculation . The inputs read out just fine.What I 'm wondering is a ) whether I 'm not calculating Y values properly because of an error in my code and b ) if that 's not the case , is it possible that I need to implement the one_hot vectors -- even though my output already takes the form of 0 or 1 . If the latter is the case , where do I include the one_hot=TRUE function in my generation of the target values vector ? Thanks !"
"Assume my data is daily counts and has as its index a DateTimeIndex column . Is there a way to get the average of the past n weekdays ? For instance , if the date is Sunday August 15th , I 'd like to get mean of counts on ( sunday august 8th , sunday august 1st , ... ) .I started using pandas yesterday , so here 's what I 've brute forced.There has to be a one liner ? Is there a way to use apply ( ) ( without passing a function that has a for loop ? since n is variable ) or some form of groupby ? For instance , the way to find the mean of all data on each weekday is :"
"I have a question similar to the question asked here : simple way of fusing a few close points . I want to replace points that are located close to each other with the average of their coordinates . The closeness in cells is specified by the user ( I am talking about euclidean distance ) . In my case I have a lot of points ( about 1-million ) . This method is working , but is time consuming as it uses a double for loop.Is there a faster way to detect and fuse close points in a numpy 2d array ? To be complete I added an example : A scatterplot of the points is visible below . The red circle indicates the points located close to each other ( in this case a distance of 27.91 between the last two points in the array ) . So if the user would specify a minimum distance of 30 these points should be fused.In the output of the fuse function the last to points are fused . This will look like :"
I have used PIL I need to add a border to the image with a width on all 4 sides of image
"I 'm attempting to get this PyTorch person detection example running : https : //pytorch.org/tutorials/intermediate/torchvision_tutorial.htmlI 'm using Ubuntu 18.04 . Here is a summary of the steps I 've performed:1 ) Stock Ubuntu 18.04 install on a Lenovo ThinkPad X1 Extreme Gen 2 with a GTX 1650 GPU.2 ) Perform a standard CUDA 10.0 / cuDNN 7.4 install . I 'd rather not restate all the steps as this post is going to be more than long enough already . This is a standard procedure , pretty much any link found via googling is what I followed.3 ) Install torch and torchvision4 ) From this link on the PyTorch site : https : //pytorch.org/tutorials/intermediate/torchvision_tutorial.htmlI saved the source available from the link at the bottom : https : //pytorch.org/tutorials/_static/tv-training-code.pyTo a directory I made , PennFudanExample5 ) I did the following ( found at the top of the above linked notebook ) : Install the CoCo API into Python : open Makefile in gedit , change the two instances of `` python '' to `` python3 '' , then : Get the necessary files the above linked files need to run : from ~/vision/references/detection , copy coco_eval.py , coco_utils.py , engine.py , transforms.py , and utils.py to directory PennFudanExample.6 ) Download the Penn Fudan Pedestrian dataset from the link on the above page : https : //www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zipthen unzip and put in directory PennFudanExample7 ) The only change I made to tv-training-code.py was to change the training batch size from 2 to 1 to prevent a GPU out of memory crash , see this other post I made here : PyTorch Object Detection with GPU on Ubuntu 18.04 - RuntimeError : CUDA out of memory . Tried to allocate xx.xx MiBHere is tv-training-code.py as I 'm running it with the slight batch size edit I mentioned : Here is the full text output including error I 'm getting currently : The really strange thing is after I resolved the above-mentioned GPU error this was working for about 1/2 a day and now I 'm getting this error , and I could swear I did n't change anything.I 've tried uninstalling and reinstalling torch , torchvision , pycocotools , and for copying the files coco_eval.py , coco_utils.py , engine.py , transforms.py , and utils.py , I 've tried checking out torchvision v0.5.0 , v0.4.2 , and using the latest commit , all produce the same error.Also , I was working from home yesterday ( Christmas ) and this error does not happen on my home computer , which is also Ubuntu 18.04 with an NVIDIA GPU.In Googling for this error one suggestion that is relatively common is to backdate numpy to 1.11.0 , but that version is really old now and therefore this would likely cause problems with other packages.Also in Googleing for this error it seems the general fix is to add a cast to int somewhere or to change a divide by / to // but I 'm really hesitant to make changes internal to pycocotools or worse yet inside numpy . Also since error was not occurring previously and is not occurring on another computer I do n't suspect this is a good idea anyway.Fortunately I can comment out the lineFor now and the training will complete , although I do n't get the evaluation data ( Mean Average Precision , etc . ) About the only thing left I can think of at this point is to format the HD and reinstall Ubuntu 18.04 and everything else , but this will take at least a day , and if this ever happens again I 'd really like to know what may be causing it.Ideas ? Suggestions ? Additional stuff I should check ? -- EDIT -- After re-testing on the same computer experiencing the concern , I found this same error occurs with the evaluation step when using the TensorFlow object detection API ."
"I was poking around at the source of the requests module , and noticed this code : My understanding of super ( ) suggests this call would n't do anything at all . I 've found quite a few questions about superclass calling , but all work from subclasses of other classes , not object itself . The python docs , too , do n't mention this construction.It occurred to me this might simply be a mistake , and if you git blame that file to the commit that introduced that line , you 'll see that at the time of authorship , Response was a subclass of BaseResponse . Is the line just a holdover from a class refactoring , or does it this super ( ) call do anything at all ?"
Script : Output : BS Docs : Question : Why is there a syntax error ?
"I have an array : and list of indices with repetitions : and another array i would like to add to A using indices above : The operation : Gives the result : array ( [ 1 , 1 , 1 ] ) , so obviously values from B were not summed up . What is the best way to get as a result array ( [ 2 , 2 , 2 ] ) ? Do I have to iterate over indices ?"
"I am unable to visualize the Latex table generated using dataframe.to_latex ( ) from pandas in my IPython notebook . It shows the exact string with the `` \begin.. '' line in a box.I am also curious why the table is formatted { lrrrrr } and how I can change it columns with lines separating the values like { l|c|c|c|c } . I 'm not quite sure if my setup is the issue and I am wondering if there are further documentation for formatting Latex rendered tables using pandas.dataframe.to_latex ( ) . I use IPython notebook ( 0.132 ) and Pandas ( 0.13 ) . I 'm running Ubuntu 13.04 , Texlive2012 . IPython Notebook code : IPython notebook output even after copying and running as markdown , only a box around the text is added . I would appreciate any help as I am still very new to pandas and the wonderful things it can do with the SciPy suite !"
Possible Duplicate : Timeout on a Python function call I want to implement that when the function took more than 90 seconds to complete it should return immediately when timeout . Is there any way to achieve that ? EditedPlease download this test file . I have created a thread class and raise an exception within thread if timeout error occur . But thread is still alive because it prints i am still alive : ) even after exception . Why an exception does not force the thread to stop ? ?
"I 've seen two different Python objects used to group arbitrary data together : empty classes and functions.Even if the class is n't empty , it seems to work so long as it 's defined at runtime.But when I got cocky and tried to do this with built-in functions or classes , it did n't work.Is there a fundamental difference between built-in and `` custom '' classes and functions that accounts for this behavior ?"
"I have a DataFrame , and I want to select certain rows and columns from it . I know how to do this using loc . However , I want to be able to specify each criteria individually , rather than in one go . RequirementI want to be able to achieve the same result with something like the following bit of code , where I specify each criteria one by one . It 's also important that I 'm able to use a slice_list to allow dynamic behaviour [ i.e . the syntax should work whether there are two , three or ten different criteria in the slice_list ] ."
"I 'm sure this concept has come up before but I ca n't find a good , simple answer . Is using try/finally a bad way to handle functions with multiple returns ? For example I haveThis just seems nicer than storing update ( ) commands in a temporary variable and returning that ."
"I have the following python function that allows me to run shell commands from within a python script : This allows me to do things like : My question is : with the definition of run_shell_command above , which type of shell is started ? ( e.g . login vs interactive ) . Knowing which shell is started would help know which which dot files ( e.g . .bashrc , .profile , etc . ) are executed prior to my shell command ."
"So I have datetime objects in UTC time and I want to convert them to UTC timestamps . The problem is , time.mktime makes adjustments for localtime . So here is some code : Here is some output : So obviously if the system is in UTC time no problem , but when it 's not , it is a problem . Setting the environment variable and calling time.tzset works but is that safe ? I do n't want to adjust it for the whole system.Is there another way to do this ? Or is it safe to call time.tzset this way ."
Assuming I have a column called df.Text which contains text ( more that 1 sentence ) and I want to use polyglot Detector to detect the language and store the value in a new column df [ 'Text-Lang ' ] how do I ensure I also capture the other details like code and confidence returns name : English code : en confidence : 94.0 read bytes : 1920but ends with AttributeError : 'float ' object has no attribute 'encode ' and Detector is not able to detect the language reliably.Am I applying the Detector function incorrectly or storing the output incorrectly or something else ?
"On a Centos machine , Python v2.6.6 and Apache Spark v1.2.1Getting the following error when trying to run ./pysparkSeems some issue with python but not able to figure out"
JSON is very similar to Python syntax . Can all JSON objects directly convert to Python without error ? ExampleThe following is a valid JSON object : This object will directly translate to a Python dictionary with key `` foo '' and value `` bar '' :
"i 've been trying the last couple days to convert this js script to python code.My implementation ( blindfull cp mostly , some minor fixes here and there ) so far : Issues : I have absolutely no knowledge of javascript.When i try to `` learn '' some text to a `` markov '' class object [ e.g . : a=markov ( ) ; a.learn ( `` sdfg '' ) ; ] i get the following error : `` TypeError : unhashable type : 'list ' '' , for the `` mem '' dictionary at the `` learnPart '' function , member of the `` learn '' function.So my question so far is why does this exception [ TypeError for a list object , falsely referring to a dictionary object ( which is hashable ) ] occur ? thanks in advance for any suggestions , directions , points , help in general : D"
"I have a signal in my MessageFolder model which works fine , however in some special ocassions I do n't want the post_save signal action to occur . How can I deactivate it in this case ? I have tried the following but it 's not workign.Views.pyModels.py"
"Is there a way in Falcon framework to respond with HTTP 500 status on any unspecific exception that is not handled in resource handler ? I 've tried to add following handler for Exception : But this makes impossible to throw , for example , falcon.HTTPNotFound — it is handled by the handler above and I receive 500 instead of 404 ."
"Today I upgraded from pip 7.1.0 to 7.1.2 , and now it does n't work.So I tried reinstalling : The reinstall ran without error , but when I tried to search , I got the same error . So , I tried reinstalling the old version : Again , the reinstall worked , but searching was still broken after the reinstall . In addition to the error , I did get the version upgrade message : Disabling the cache also gives the same error : The problem seems to only be with the search function , as pip still functions well enough to reinstall itself and such.I believe I have only installed one other package today , which was docker-compose . The problem occurs when I search for packages other than docker-compose , as in my examples.Any ideas ?"
"I am extending Python with some C++ code.One of the functions I 'm using has the following signature : ( link : http : //docs.python.org/release/1.5.2p2/ext/parseTupleAndKeywords.html ) The parameter of interest is kwlist . In the link above , examples on how to use this function are given . In the examples , kwlist looks like : When I compile this using g++ , I get the warning : So , I can change the static char* to a static const char* . Unfortunately , I ca n't change the Python code . So with this change , I get a different compilation error ( ca n't convert char** to const char** ) . Based on what I 've read here , I can turn on compiler flags to ignore the warning or I can cast each of the constant strings in the definition of kwlist to char * . Currently , I 'm doing the latter . What are other solutions ? Sorry if this question has been asked before . I 'm new ."
"I am calculating a sum using lambda like this : and its output is 10.But I want a lambda function that takes random arguments and sums all of them . Suppose this is a lambda function : someone should be able to call the add function as : That is , one should be able to supply arbitrary number of parenthesis.Is this possible in Python ?"
"I 'm looking to develop some code , that creates Bitcoin private and public keys from a mnemonic . My current understanding of this process is : I am using Trezor 's nmemonic library and moneywagon in my code.If you comment out the above entropy line , and run the code , you get : My problem is none of this is reflected in iancoleman.io/bip39 or bip32jp.github.io for generating mnemonic codes and public/private keys.Where am I going wrong ?"
"I have something like this : If I 'm looking at a match object I 'm not really interested which specific text was matched , I just want to know if it was group1 or group2groupdict ( ) gives me something like this : Now , of course , I could find out that it 's group2 by just iterating over the dict , but that seems slow if I have a lot of matches to check.Is there a more direct way to get the group name ? ( Python 2.7 )"
"I am trying to create a python package ( not anywhere at the moment ) and I want to use 3.6 python variable annotations , that iswhile still providing support for Python 3.5.Is there any way to provide these style of variable annotations inside Python 3.5 , either through a or similar . I know it is possible to use comment type annotations , but I would like to be able to use this style ."
"I have the following file ( df_SOF1.csv ) , it is 1 million records longI am using pandas to analyse it I have been been trying for at least 40 hoursto find a way to group the data in a way that I can aggregate the time column D_TimeI have loaded the required modules I create a dataframe see below using DateOccured as an index I can group by any column or iterate through any row e.g.However I have not found a way to sum up and take the mean of the D_Time column using pandas . I have read over 20 articles on timedeltas etc but am still not the wiser how I do this in pandas.Any solution that can allow me do arithmetic on the D_Time column would be appreciated . ( even if it has to be done outside of pandas ) .I thought one possible solution would be to change the D_Time column into seconds.__________________________________2012/11/01I ran the following command on the 30 items abovedf_SOF1.groupby ( 'Transport ' ) .agg ( { 'D_Time ' : sum } ) TransportLorry 0:00:000:00:000:00:000:07:000:29:003:27:003:28 ... Train 7:49:007:50:007:52:007:48:007:55:007:49:007:52..It seems to sum the values together physically rather than give a numerical sum ( like adding strings ) Cheers"
"So now that k8s is integrated directly with spark in 2.3 my spark submit from the console executes correctly on a kuberenetes master without any spark master pods running , spark handles all the k8s details : What I am trying to do is do a spark-submit via AWS lambda to my k8s cluster . Previously I used the command via the spark master REST API directly ( without kubernetes ) : And it worked . Now I want to integrate Kubernetes and do it similarly where I submit an API request to my kubernetes cluster from python and have spark handle all the k8s details , ideally something like : Is it possible in the Spark 2.3/Kubernetes integration ?"
"I am a little bit confused about the object and type classes in Python 3 . Maybe someone can clear up my confusion or provide some additional information.My current understanding is that every class ( except object ) inherits from a base class called object . But every class ( including object ) is also an instance of the class type , which is an instance of itself and object and also inherits from object.My questions are : Is there a reason/design decision why object is an instance of type and type inherits from object ? Has the type/class of an object also to be an object itself ? How can a class ( type ) be an instance of itself ? Which one is the real base class object or type ? I always thought object would be the most `` fundamental '' class , but it seems to be an instance of type , which is an instance of object , which is an instance of type , ... Where does this recursion end ? Is there a possibility to illustrate the relation between the object and the type class ? I tried looking up the entries of object and type in the Documentation of the Python Standard Library.Every class ( except object ) inherits from object.Every class is an instance of the class type.type is an instance of itself.type also inherits from object.Also"
"I 'm seeking for an efficient way to combine a number range like ( 20,24 ) with another object like { ' a ' : ' b ' } to get , If I had a list of numbers like [ 20 , 21 , 22 , 23 ] , I know iterating through the list is the way to go . But here I have a range of numbers . So may be there is a way to wrap range elements in tuples more elegantly . Here is my attempt to achieve this : I just want to know whether there is any better way to do this ."
"In looking at some python strings and functions , I discovered this strange quirk of python : which then prints : ollehHowever , print s [ len ( s ) -1 : -1 : -1 ] does n't work . My understanding is that it should iterate from the last element s [ len ( s ) -1 ] to the first element s [ 0 ] . However , it just prints an empty string `` , which I believe is because in strings of some given length ( say , 5 ) , s [ 4 ] == s [ -1 ] . But , I do n't understand why python decides to use -1 instead of 4 , which is the actual len ( s ) .Furthermore , s [ len ( s ) :0 : -1 ] + s [ 0 ] works . Why is len ( s ) a valid index ? Does python just convert len ( s ) to 0 arbitrarily ? P.S . This is in Python 2.7.8 , I 'm not sure if it also works in 3.x.xEDIT : Confirmed to be the same in Python 3"
"I have a function within a Python ( 2.7 ) class that should retrieve the values of the 'cells ' around it in a 2 dimensional numpy array . If the index is out of range , I would the value should be set as None.I 'm struggling to find a way to do this without writing 8 try/catch statements , or without using multiple if x else None statements as is in my code below . While they would both work , they do n't seem very well structured , and I 'm thinking there must be a simpler way to do this - I 'm probably caught thinking about this in entirely the wrong way . Any help would be much appreciated ."
"While shortening my code I was cutting down a few variable declarations onto one line-However , when I tried doing the same thing to this code-This throws the errorI have read the relevant Python documentation , but I still ca n't find a way to shorten this particular bit of code ."
"I am trying to optimize some code , and by profiling i noticed that this particular loop takes a lot of time . Can you help me write it faster ? Is there any numpy function that can do this efficiently ?"
"I noticed that in the code : the first print function throws a ZeroDivisionError , but the second one outputs nan . I 'm aware that type ( b [ 0 ] ) is numpy.float64 , while type ( a ) is float . I have two questions:1 ) Why was it implemented this way ? 2 ) Is there anyway to have it throw a ZeroDivisionError ?"
"I 've got little problem with testing my Flask app . My view looks like this : and my test function like this : If I try to print mock objects in test function , they return correct values . Yet in view I 'm still getting : mongo_photo variable is returned as < MagicMock name='pymongo.db.photos.find_one ( ) ' id='59485392 ' > .Do I use mock bad way , patching on wrong place ?"
"Well hello , this is the most interesting bug/conflict I 've ever faced.In python shell , I can not type lowercase `` b '' . At first I thought something was under the key , but no , in everywhere else it functions very well . Also ctrl+b shift+b even with capslock b works.Yet more , when I run the shell with sudo ( ie . sudo python ) , my little lowercase `` b '' works well.My last move was installing pyexiv2 ( using aptitude ) , I can import it without problems in both with and without sudo . I have removed it but the result did n't change.What the hell might be wrong ? I am using Ubuntu 10.04 LTS x86 with Python 2.6.5 Further note : I have installed a vim modifier script which might be the trouble.Using this : This scripts initiates more git clones , so it might be hard to follow . But it does many changes including the terminal 's look.UPDATE:1 ) I even can not copy/paste `` b '' character . ctrl+c/v select & middle click both doesnt work . 2 ) When I open the shell with python -E , the modifiers from the mentioned so called vim script does not appear . And b works well . When I open it with python or python -S the modifications exists and i can not type b . 3 ) Good news : I have managed to locate the fault , it is the so called vim script . I have renamed its folder and it worked fine . In couple of hours , I will examine deeply what exactly causes the problem and post it here with detailed results ."
"I have an instance of a python class.For good reasons that I do n't want to get into , I want to monkey patch this object so that one of its attributes is off-limits in a certain use case . I 'd prefer that if I or another developer down the road tries to use the attribute on the monkey-patched object , a useful exception is raised which explains the situation . I tried to implement this with a property , but am not having luck . e.g. , What am I missing ?"
"While searching for some numpy stuff , I came across a question discussing the rounding accuracy of numpy.dot ( ) : Numpy : Difference between dot ( a , b ) and ( a*b ) .sum ( ) Since I happen to have two ( different ) Computers with Haswell-CPUs sitting on my desk , that should provide FMAand everything , I thought I 'd test the example given by Ophion in the first answer , and I got a result that somewhat surprised me : After updating/installing/fixing lapack/blas/atlas/numpy , I get the following on both machines : So the standard multiplication + sum ( ) is more precise than np.dot ( ) . timeit however confirmed that the .dot ( ) version is faster ( but not much ) for both float64 and float128.Can anyone provide an explanation for this ? edit : I accidentally deleted the info on numpy versions : same results for 1.9.0 and 1.9.3 with python 3.4.0 and 3.4.1 ."
"I need to reorder a sorted list so the `` middle '' element is the highest number . The numbers leading up to the middle are incremental , the numbers past the middle are in decreasing order.I have the following working solution , but have a feeling that it can be done simpler :"
"I 've retrieved a datetime from a bigquery record ( using the google.cloud.bigquery library ) and need to send it to the google admin sdk reports API in rfc 3339 format according to the 'startTime ' parameter of this api method . The API is expecting the datetime to look like this:2010-10-28T10:26:35.000ZWhich is normally possible by creating a python datetime without tzinfo and calling isoformat like this : The problem I 'm having is that the timestamp coming from BigQuery includes a tzinfo object , and that 's causing isoformat to return text that the Google API ca n't process.Specifically , the +00:00 is not accepted by Google 's API as startTime . If I manually remove +00:00 from the string the API call works , but I 'm not sure how to do this in python without an ugly string hack . Is there some clean way to remove this from the datetime object ? I 've also tried this , but results in the same :"
"For visual effect purpose , I wish I could remove the grids outside the circle and only keep those are within the circle . Btw , how to fulfill the cell ( [ 8,9 ] , [ 9,10 ] ) with red color , I mean , the cell on the right of x=8 and down y=9.My code is below and current image is also attached ."
"Does anybody know of a tool to handle module dependencies + deployment in Python ? Details : By handle , I mean : list , keep track of and bundle up a zip/installable file for me . Make it trivial to redeploy on another system ( ie : includes all modules at the correct version in a deploy file , and does not have to go somewhere to get them * ) . Alerts me if I am about to do something which changes the environment . It must follow module dependencies all the way , not just one level deep.Plus some stuff I probably have n't thought of . I 'm not talking about Virtualenv , Fabric , pip freeze** and ( I do n't think ) Paver.This evening I tried to count the modules that Pylons depends on . After a detour into Snakefood and Graphviz , the answer is A LOT . 100+ ( and Snakefood did not get them all ) . As I 'm getting more and more into Python , handling this problem manually is starting to take up more of my time than I would like , and it 's unreliable.If it matters , I use Python 2.7 on Windows 7 ."
"I have a pandas dataframe that contains a list in column split_categories : I would like to select all the rows where the at least one category in a specific list [ 480 , 9 , 104 ] .Expected output : I manage to do it using apply : But this code runs on production and this way takes too long ( I run it for multiple columns containing lists ) Is there a way to run something like : Thanks"
I wrote some Python code which works but Pylint does n't like the star . It keeps telling me : Is it possible to write my code without the star ? Some info : I 'm using lxml ; self.xml is an objectified XML file .
"I am using a technique discussed here before , to turn a dictionary into an object , so that I can access the elements of the dictionary with the dot ( . ) notion , as instance variables.This is what I am doing : So now , I can do : and the result is : This works , however the issues start if I try to add some other methods to the `` Struct '' class . For example lets say that I am adding a simple method that just creates an variable : If I do : My dictionary object is broken , `` myVariable '' is inserted as a key with the value `` 67 '' . So If I do : I am getting : Is there a way to fix this ? I kind of understand what is happening , but not sure If I need to entirely change my approach and build a class with internal methods to handle the dictionary object or is there a simpler way to fix this problem ? Here is the original link : Convert Python dict to object ? Thanks ."
"I made a Matlab function and I would like to convert it to Python to use with my web application.I converted ( .m file to .py file ) almost everything using OMPC.However , I ca n't make the solve ( ) function to work ( I am using the sympy library ) .This is the Matlab line : And this is the Python line where x and y are symbols ( with x = Symbol ( ' x ' ) and y = Symbol ( ' y ' ) ) : With this Python code , I am getting False instead of a result ( it works great with the Matlab code ) .Am I missing something ? EDIT : And with this , I am getting [ ] :"
"Say I have an iterable ( in my case a list ) : I know that the easiest and fastest way to check if at least one of those elements is True is simply to use any ( l ) , which will return True.But what if I want to check that at least two elements are True ? My goal is to process it in the fastest way possible.My code right now looks like this ( for two elements ) : This is about 10 times slower than any ( ) , and does not seem very pythonic to me ."
"I am trying to build a color palette around 2 colors : teal and roseI found this website : https : //learnui.design/tools/data-color-picker.html # paletteWhich could do half of what I was looking for so I wanted to try to do this in python using matplotlib , seaborn , palettable , and/or colorsys.Is there a way to interpolate what the next colors would be given a series of colors in a gradient ? For example , from the website I gave the start_color and end_color . It gave me 6 colors ranging from start_color to end_color . Is there a way to do this but to make the end_color the middle_color and continue to gradient ? I would like to make the teal start_color remain the first color , make the rose end_color the middle_color ( in between 3 and 4 ) , and then have the color palette finish to make 6 total colors . I was going to try and get the RGB values and then do some type of modeling to figure out where it would go but I think there is probably an easier way to do this ."
"I know that you can use the Python shell in Vi mode on Unix-like operating systems . For example , I have this line in my ~/.inputrc : This lets me use Vi-style editing inside the Python shell.But can this be made to work when using Python on a Windows XP box ? I 'm using the pre-built Python for Windows downloaded directly from python.org.I 'm guessing that the Windows version does not use the GNU Readline library , but I 'd be happy to be proven wrong . : )"
"When using a dictionary in Python , the following is impossible : since 'list ' is an unhashable type . However , the id function in Python returns an integer for an object that is guaranteed to be unique for the object 's lifetime.Why does n't Python use id to hash a dictionary ? Are there drawbacks ?"
"I 'm trying to follow the Wikipedia Article on latent semantic indexing in Python using the following code : How the math says it should work : What does work , with math that looks incorrect : ( from here ) Why does route work , and the first not , when everything I can find about the math of LSA shows the first as correct ? I feel like I 'm missing something obvious ..."
"I 'm using Git to track some matlab code . A toy example best illustrates the problem . The project so far looks like this.Contents of A are x=5We make commit C , where the line is changed to x=6We then make commit B , where our content becomes as belowIf we attempt a merge with the goal of the project looking likewith the merge result in D , we 'll get a conflict , because the main line has been changed in both ( indentation added in B , 5 changed to 6 in C ) . Is there a best practice way for integrating indentation changes from one branch , and content changes from another branch , to get a merge result ? I 've read about one strategy in https : //stackoverflow.com/a/5262473/288545 , and while that would avoid the conflict , it would discard the indent in favor of the content change ( which is an improvement , but still makes for harder to read code ) .I suppose I could just suck it up and not change indentation when writing my code . This makes it less readable , but is n't a huge deal in matlab . However , in python , indentation really matters , so how do python folks deal with it ? This gets much uglier if there are large blocks of code that we later change to be inside of control structures , so the diff touches many lines and makes merge conflicts a huge headache.Is there a merge strategy that will handle spacing changes and content changes separately , and then integrate them ? I 'd like the result of the merge to be"
"I need to remove white spaces between xml tags , e.g . if the original xml looks like : I 'd like the end-result to be crunched down to single line : Please note that I will not have control over the xml structure , so the solution should be generic enough to be able to handle any valid xml . Also the xml might contain CDATA blocks , which I 'd need to exclude from this crunching and leave them as-is.I have couple of ideas so far : ( 1 ) parse the xml as text and look for start and end of tags < and > ( 2 ) another approach is to load the xml document and go node-by-node and print out a new document by concatenating the tags.I think either method would work , but I 'd rather not reinvent the wheel here , so may be there is a python library that already does something like this ? If not , then any issues/pitfalls to be aware of when rolling out my own cruncher ? Any recommendations ? EDITThank you all for answers/suggestions , both Triptych 's and Van Gale 's solutions work for me and do exactly what I want . Wish I could accept both answers ."
"I 'd like to get a nice neat list comprehension for this code or something similar ! Thanks ! Edit* The indices are a list of integers . A list of indexes of another array.For example if indices is [ 1 , 52 , 150 ] then the goal ( here , this is the second time I 've wanted two separate actions on continuously indexed outputs in a list comprehension ) Then extra_indices would be [ 1 , 2 , 3 , 52 , 53 , 54 , 150 , 151 , 152 ]"
"I have a script that randomly generates a set of data and trains several classifiers to compare them against each other ( it 's very similar to http : //scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html ) : And to run it : The script outputs something like : but the clf.fit ( ) is now single-threaded . Assuming that I have enough threads to run all classifiers for each iteration , How would I be able to train the classifiers using different threads for every iteration of for num_samples , num_feats , num_feats_to_remove in product ( _samples , _feats , _feats_to_rm ) ? And if I am restricted to 4 or 8 threads but I need to train > 4 or > 8 classifiers for each iteration , how is it done ?"
"Currently I override the class ' __setattr__ ( ) towards the end of the class ' __init__ ( ) method to prevent new attribute creation -Is there a way to avoid manually overriding __setattr__ ( ) and do this automatically with the help of metaclasses ? The closest I came was -Is there a more generic way of doing this such that apriori knowledge of the subclass attributes is not required ? Basically , how to avoid the line dctry.update ( { ' x ' : 0 , ' y ' : 0 } ) and make this work irrespective of what the names of class attributes are ? P.S . - FWIW I have already evaluated the __slots__ and namedtuple options and found them lacking for my needs . Please do n't narrow your focus to the pared down Points ( ) example that I have used to illustrate the question ; the actual use case involves a far more complex class ."
There is this code : Why object2.variable is not also 1 after assignment to class variable ?
"How do i find the actual numerical values held in an MXNet symbol.Suppose I have , if x = [ 100,200 ] and y= [ 300,400 ] , I want to print : z = [ 400,600 ] , sort of like tensorflow 's eval ( ) method"
"How can you iterate over all 2^ ( n^2 ) binary n by n matrices ( or 2d arrays ) in numpy ? I would something like : Do you have to use itertools.product ( [ 0,1 ] , repeat = n**2 ) and then convert to a 2d numpy array ? This code will give me a random 2d binary matrix but that is n't what I need ."
"I was going over some distribution functions at python : Uniform , Binomial , Bernoulli , normal distributionsI found that pretty much the same functions are present in both scipy and numpy.Going over the code I found scipy uses numpy internally : https : //github.com/scipy/scipy/blob/master/scipy/stats/_discrete_distns.pyhttps : //github.com/numpy/numpy/blob/master/numpy/random/mtrand/distributions.cSo , my question is what is the primary motive to have 2 copies of the same distribution functions ? what additional functionality is provided by scipy library that is not there in numpy ? Complete list of methods in each module is here : Numpy Random module : https : //docs.scipy.org/doc/numpy/reference/routines.random.htmlScipy stats module : https : //docs.scipy.org/doc/scipy/reference/stats.htmlI found reference to some basic difference between the 2 modules : Difference between random draws from scipy.stats ... .rvs and numpy.random"
"I have AWS API gateway setup for a public endpoint with no auth . It connects to a websocket that triggers a Lambda.I was creating connections with Python 's websocket-client lib at https : //pypi.org/project/websocket_client/.I noticed that connections would fail ~10 % of the time , and get worse as I increased load . I ca n't find anywhere that would be throttling me seeing as my general API Gateway settings say Your current account level throttling rate is 10000 requests per second with a burst of 5000 requests.. That ’ s beside the point that just 2-3 requests per second would trigger issue fairly often . Meanwhile the failure response would be like { u'message ' : u'Forbidden ' , u'connectionId ' : u'Z2Jp-dR5vHcCJkg= ' , u'requestId ' : u'Z2JqAEJRvHcFzvg= ' } I went into my CloudWatch log insights and searched for the connection ID and request ID . The log group for the API gateway would find no results with either ID . Yet a search on my Lambda that fires on websocket connect , would have a log with that connection ID . The log showed everything running as expected on our side . The lambda simply runs a MySQL query that fires . Why would I get a response of forbidden , despite the lambda working as expected ? The existing question over at getting message : forbidden reply from AWS API gateway , seems to address if it 's ALWAYS returning forbidden for some private endpoints . Nothing lined up with my use case.UPDATEI think this may be related to locust.io , or python , which I 'm using to connect every second . I installed https : //www.npmjs.com/package/wscat on my machine and am connecting and closing as fast as possible repeatedly . I am not getting a Forbidden message . It 's just extra confusing since I 'm not sure how the way I connect would randomly spit back a Forbidden message some of the time.Update 2I have enabled access logs , the one type of log that was n't there before . I can now see that my lambdas are always getting a 200 with no issue . The 403 is coming from some MESSAGE eventType that does n't hit an actual routeKey . Not sure where it comes from , but pretty sure finding that answer will solve this.I was also able to confirm there are no ENI issues ."
"My data frame contains 10,000,000 rows ! After group by , ~ 9,000,000 sub-frames remain to loop through.The code is : This is super inefficient , and the code has been running for 10+ hours now . Is there a way to speed it up ? Full Code : To get the data : Note : Most of id 's will only have 1 date . This indicates only 1 visit . For id 's with more visits , I would like to structure them in a 3d format e.g . store all of their visits in the 2nd dimension out of 3 . The output is ( id , visits , features )"
"I 'm trying to make a basic MLP example in keras . My input data has the shape train_data.shape = ( 2000,75,75 ) and my testing data has the shape test_data.shape = ( 500,75,75 ) . 2000 and 500 are the numbers of samples of training and test data ( in other words , the shape of the data is ( 75,75 ) , but there are 2000 and 500 pieces of training and testing data ) . The output should have two classes.I 'm unsure what value to use for the input_shape parameter on the first layer of the network . Using the code from the mnist example in the keras repository , I have ( updated ) : where 5625 is 75 * 75 ( emulating the MNIST example ) . The error I get is : Any ideas ?"
"is python exception slow ? I 'm kind using python exceptions to structure programm follow in my web application , and I 'm wondering how throwing exceptions will affect performance of my application . what is your thoughts ? which one of the following statements is less expensive in terms of memory and cpu ?"
"I would like to customize the output name of my pytest to include the name of my fixturesSo I haveand I 'd like the displayed test name to be some version ofHow can I do this ? I tried creatingbut no dice , it did n't change the test display name"
"I have tried to use New Times New Roman font instead of Bitstream vera sans font which is a default for matplotlib in the school network server.I get the following error with Times or Helvetica , or Arial . not found errorTo resolve this , I inquired to technical help to upload those fonts to server . I verified that they were uploaded.Now after I removed fontList.cache and re-run the code as below : The good thing is that I do n't see the error message anymore , but bad thing is that after adding fontdict= { `` name '' : `` Times New Roman '' } , the label has disappeared.I ca n't find the reason of this without any error ."
"Let 's say I have a list like below : I want sort it to : So first sort it in descending order by the score and then sort it in ascending order by the name.I 've tried : It 's working , so now I just need reverse it : Ah , reverse=True simply reversed the list but did n't give the expect output . So I just want reverse the output of int ( x [ 1 ] ) , but not x [ 0 ] . How can I do that ?"
"I 'm trying to deploy one of my Flask apps to mod_wsgi on apache , but I 'm having trouble because apache tries to solve SOME of the routes on the filesystem : apache 's error_log : I 'm saying `` SOME of the routes '' because the authentication ( on `` / '' ) and redirection to `` /chat '' works . The route `` _publish_message '' is accessed via AJAX like this ( using jQuery ) : The route `` _sse_stream '' is used as the URL for an EventSource.These two are n't working ! The virtual host configuration : The webchat.wsgi file : A basic `` hello world '' app deployed to mod_wsgi runs OK.My flask app , when run using the development server integrated into flask , behaves well ."
"Quesion and code is at the endI hope this question belongs here and not to the TCS 's stack.I am trying to go through algorithms in Turk and Pentland 's `` Eigenfaces for Recognition '' .On page 74 one can read ( last paragraph of left column ) : Let the training ( ... ) The average face of the set is defined by [ * ] Where [ * ] is an equation saying that average face is equal to sum of images divided by it 's count . In order to use this equantion I created python script using OpenCV and numpy.On page 75 there is Figure 1. which should represent average face from Figure 1 . ( pg . 74 ) and this is what I am trying to achieve . As a face set I am using all faces from Faces94 . When I calculate traditionall average ( 1/M*sum ) the result looks like this : which is far away from expected , mostly because of those wierd 'spots'.However , when I calculate average like there was more faces than actually is ( e.g . 1/ ( 2*M ) * sum ) result looks more accurate : I think there is some problem in converting int8 < - > int but I can not prove it.If anyone can spot any problem with the code please let me know even if it is not solution.Question : what am I doing wrong / what to do to get better results.Here is the code :"
"Can we use standard multiprocessing.Queue class for communicating between independent Python3 scripts likeIf no , Is there another library like standard multiprocessing module for communicating with independent scripts ."
"I was slightly surprised by this example given by Eli Bendersky ( http : //eli.thegreenplace.net/2015/the-scope-of-index-variables-in-pythons-for-loops/ ) But when I thought about it , it made some sense — the lambda is capturing a reference to i rather than i 's value.So a way to get around this is the following : It appears that the reason that this works is that when i is provided to the outer lambda , the outer lambda creates a scope and dereferences i , setting a to i . Then , the inner lambda , which is returned , holds a reference to a.Is this a correct explanation ?"
I 'm fairly new to both Python and Google App Engine . I want to organize my script files by creating a folder structure . However when I do that I can no longer figure out how to import them.For example : How do I import spam.py in main.py ?
"I 'm writing tests for a Django application and using a attribute on my test class to store which view it 's supposed to be testing , like this : My guess is that the problem I 'm having is caused because since I 'm calling an attribute of the OrderTests class , python assumes I wan na send self and then order get the wrong arguments . Easy to solve ... just not use it as a class attribute , but I was wondering if there 's a way to tell python to not send self in this case.Thanks ."
"I have a Pylons app where I would like to move some of the logic to a separate batch process . I 've been running it under the main app for testing , but it is going to be doing a lot of work in the database , and I 'd like it to be a separate process that will be running in the background constantly . The main pylons app will submit jobs into the database , and the new process will do the work requested in each job.How can I launch a controller as a stand alone script ? I currently have : and in the controller file , but not part of the controller class : But starting ImportServer.py on the command line results in :"
"What 's the difference between using File.write ( ) and print > > File , ? Which is the pythonic way to write to file ? Is there an advantage when using print > > File , ?"
"I 'm trying to get my head around what is the most efficient and less memory consuming way to share the same data source between different process.Imagine the following code , that simplify my problem.In the first method will use the global variable ( wo n't work on Windows , only on Linux/OSX ) which will then access by the function . In the second method I 'm passing `` data '' as part of the arguments . In terms of memory used during the process , there will be a difference between the two methods ? A third method , rather than passing all the `` data '' , since we know we 'll be using only the first records , I 'm only passing the first 1000 records . Will this make any difference ? BackgroundThe problem I 'm facing I have a big dataset of about 2 millions rows ( 4GB in memory ) which will then by four subprocess to do some elaboration . Each elaboration only affect a small portion of the data ( 20000 rows ) and I 'd like to minimize the memory use by each concurrent process ."
"What I learnt from python None : None is frequently used to represent the absence of a valueWhen i put in a list and sorted with numbers and string . I got the following result , which means it is the smallest number ? Reverse : Normal sort : How python sorted function is working with None ?"
Suppose I have a Dataframe df : This works when we have 2 columns..Expected Output : Can I use to_dict on Dataframe df to have a key with 2 other columns as values in form of list ? ?
"I 've recently read an article/code snippet that shows an example of replacing conditionals with polymorphism . Here is the code : Before : After : Here is where I got it from . Now my question is , in which significant way is the second method better than the first ? For as far I know , if I want to use the second method , I have to do something like this every time I want to log something : Am I missing the point or something very obvious ?"
"I am using piston and I would like to spit out a custom format for my response.My model is something like this : Now when I issue a GET request to something like /api/cars/1/ I want to get a response like this : However piston only outputs this : In other words I want to customize the representation of a particular resource.My piston Resource handler currently looks like this : So I do n't really get where I have the chance to customize the data . Unless I have to overwrite the JSON emitter , but that seems like a stretch ."
"In the past when using PyCrypto I was able to do the following to generate a fingerprint of a RSA public key : How can I achieve the same without PyCrypto ? EDITWhat I provide in pub_rsa_key is a content of a .perm file , i.e . : PyCrypto is deemed unsafe and is not maintained anymore so I switched to Python 's Cryptography but it seems that it does not have an adequate feature.Is there similar functionality that I missed in the Pythons Cryptography API ? Is PyCryptoDome possible is a worthy ( stable and safe ) replacement for PyCrypto to use to implement this functionality ? If none of the above is it possible to export that key in a DER format by a self written function ? Any documentation or search terms to perform the export would be helpful.EDIT 2Maarten Bodewes ' comments ( thank you ) took me to a place that seems to be the thing I was looking for . But the results of the DER export differ : whereThis is an effort to move from Py2 to Py3 - please notice that the two examples use different Python versions . Could encoding be an issue here ?"
"I used mysql_config_editor to create a .mylogin.cnf file with a password . I know it worked correctly because I can use it to connect through both the command line utility mysql and the R package RMySQL without a problem.However , when trying to connect using Mysql-Connector/Python : or with PyMySQL : I get the same error : Skimming over the source code , it looks like these are trying to read the files in cleartext . However , mysql_config_editor encrypts the login file it generates . Both modules work fine when entering the password manually in the code.How can I connect to Python using one of these generated config files ? I 'm using Python 3 , so MySQLdb is n't an option.update : for now , I 'm using RPy2 to run queries in R and pipe the results back into Python . The code is a little ugly but the workflow is n't so bad ."
"Well , let me introduce the problem first.I 've got some data via POST/GET requests . The data were UTF-8 encoded string . Little did I know that , and converted it just by str ( ) method . And now I have full database of `` nonsense data '' and could n't find a way back.Example code : unicode_str - this is the string I should obtainencoded_str - this is the string I got with POST/GET requests - initial databad_str - the data I have in the Database at the moment and I need to get unicode from.So apparently I know how to convert : unicode_str = ( encode ) = > encoded_str = ( str ) = > bad_strBut I could n't come up with solution back : bad_str = ( ? ? ? ) = > encoded_str = ( decode ) = > unicode_str"
"I want to create GridLayout in Kivy with many rectangle buttons with some custom images with different size . To do so , I want to scale image ( the way I wan to do this is shown below ) and then crop/hide the parts that overflow the widget borders . The resizing works nice , but I can not figure out how to hide/crop that parts which are outside the rectangle . For example : if widget have dimensions of 10px x 10px and my image is 100px x 200px I will rezise it to 10px x 20px and center it in widget , but the 5px below and abowe will be visible . I do not want that . : ) Cloud somebody help me with this problem ?"
This is my google address api script which contains the api keyI have used .env file in project directory to hide sensitive info from settings.py file . How can I use .env file to hide my api key from my template ?
"I 've been playing around with a function that intakes CSV data and uses pandas to_dict function as one of the steps towards an end goal of converting the data to JSON . The problem is it is modifying the numbers ( e.g . 1.6 becomes 1.6000000000000001 ) . I am not concerned about the loss of accuracy , but because users will see the change in the numbers , it looks ... amateurish . I am aware this is something that has come up before here , but it was 2 years ago , was not really answered in a great way , and I have an additional complication - the data frames I am looking to convert to dictionaries could be any combination of datatypes . As such the issue with the previous solutions are : Converting all the numbers to objects only works if you do n't need to use the numbers - I want the option to calculate sums and averages which reintroduces the addition decimal issueForce rounding of numbers to x decimals will either reduce accuracy or add additional unnecessary 0s depending on the data the user providesSo , at a high-level , my question is : Is there a better way to ensure the numbers are not being modified , but are kept in a numeric datatype ? Is it a question of changing how I import the CSV data in the first place ? Surely there is a simple solution I am overlooking ? Here is a simple script that will reproduce this bug :"
"I have 4 2D numpy arrays , called a , b , c , d , each of them made of n rows and m columns . What I need to do is giving to each element of b and d a value calculated as follows ( pseudo-code ) : Where min_of_neighbors_coords is a function that , given the coordinates of an element of the array , returns the coordinates of the 'neighbor ' element that has the lower value . I.e. , considering the array : min_of_neighbors_coords ( 1 , 1 ) will refer to the central element with the value of 7 , and will return the tuple ( 0 , 0 ) : the coordinates of the number 1.I managed to do this using for loops ( element per element ) , but the algorithm is VERY slow and I 'm searching a way to improve it , avoiding loops and demanding the calculations to numpy.Is it possible ?"
I am trying to make a function that rounds other functions for my university degree . For example I would like to call the round_sqrt = round ( sqrt ) and when i call the round_sqrt ( 5 ) it has to shows me 2 instead of 2.23606797749979 . What I am trying is this : but this does n't work.EDIT : The function should have only one parameter . For examplethe start of the function should be so in this function the funct function needs to be rounded . so when I call rounding ( abs ) ( 3.2 ) it shows me 3 .
"I already have a facial landmark detector and can already save the image using opencv and dlib with the code below : I have Arnold 's face and I save part of his face using opencv imwrite.What I 'm trying to achieve is to get the image of the jaw only and I do n't want to save the neck part . See the image below : Does anyone has an idea on how I can remove the other parts , except the jaw detected by dlib . Something like this is the expected output :"
"I have a list of strings that is already sorted in case-insensitive order . I would like to insert a new string into the list . One way to do this is to append the item and then sort the list , like so : But I was wondering if there is a way to just insert the item in the correct position without sorting the whole thing again.I found this question : Insert an item into a sorted list in Python . It points towards Python 's bisect module . But that module does n't look like it can support case-insensitivity.Edit : I tested out several of the answers listed here.Appending the item to the end and sorting the whole list ( as proposed in the original question ) was the slowest.Moinuddin Quadri 's answer was faster than sorting the whole list , but it was still pretty slow , due to running lower ( ) on every item on the list.Stefan Pochmann 's answer was an order of magnitude faster than sorting the whole list.Jared Goguen 's answer was the fastest for repeated insertions . The first time , though , it runs lower ( ) on every element.It was a close call to accept an answer . In the end , I went with Stefan Pochmann 's answer because it was the best for a one-time insertion , and accessing the resulting list does not require accessing a member variable . However , use cases vary , so be sure to examine all the answers ."
"I have a legacy database which contains simple data structures ( no CODE refs thank goodness ) that have been written using the nfreeze method of the Storable module in Perl.Now I have a need to load this data into a Python application . Does anyone know of a Python implementation of Storable 's thaw ? Google has n't helped me.If it comes to it , I can reverse engineer the data format from the Storable source , but I 'd prefer to avoid that fun if it 's been done already.To express in code : Given a Perl program like this : I 'm after a magic_function such that this Python : prints : when run against the output of the Perl program ."
"I have this simple Flask app : The prolog_handler module starts a session with a triplestore and loads some rules . It also has an atexit function that ends the session and prints a message like `` Closing ... '' . I start the server from the bash prompt with python myapp.py . Whenever I hit CTRL-C to stop the server , nothing happens . I do n't get returned back to the bash prompt , and I do n't see the `` Closing ... '' message printed . I also tried to do this with Web.py with the same results.The that prolog_handler does is literally as simple as this : So why is it so difficult to just perform an atexit task ? PS : if I comment out all the stuff about opening the Prolog Session and ending it , and just leave the part that prints the message `` Closing ... '' then I do see the `` Closing ... '' message when I hit CTRL-C and I do get returned to the bash prompt . That works as expected . But what 's the point of atexit if I ca n't do anything useful with it ?"
So far what I have is and the output ends up beings something along the lines ofHow do I make it so a number is only used once and the letters do not repeat ? ( ex . something like ) Thanks in advance
Given : I want to add another column which is the max at each index from the beginning . For example the desired column would be : I tried with a pandas rolling function but the window can not be dynamic it seems
"I am trying to create a JPEG compressed DICOM image using pydicom . A nice source material about colorful DICOM images can be found here , but it 's mostly theory and C++ . In the code example below I create a pale blue ellipsis inside output-raw.dcm ( uncompressed ) which looks fine like this : At the very end I am trying to create compressed DICOM : I tried setting various transfer syntaxes , compressions with PIL , but no luck . I believe the generated DICOM file is corrupt . If I were to convert the raw DICOM file to JPEG compressed with gdcm-tools : By doing a dcmdump on this converted file we could see an interesting structure , which I do n't know how to reproduce using pydicom : I tried to use pydicom 's encaps module , but I think it 's mostly for reading data , not writing . Anyone else have any ideas how to deal with this issue , how to create/encode these PixelSequences ? Would love to create JPEG compressed DICOMs in plain Python without running external tools ."
"Possible Duplicate : Why do I have to specify my own class when using super ( ) , and is there a way to get around it ? I was reading this book - `` Core Python Programming '' , and I really found it very nice I would say . But I got confused at a point while I was studying the topic Inheritance..The book somewhere said that- we can use super ( ) to invoke super class method which will find the base class method for us and also we do n't need to pass self explicitly , like we do without super..Here 's the sample code : - My Question is - why we have to pass the classname to super ( ) call.. As the classname can be inferred from the class from which super is invoked.. So , since super ( ) is invoked from class Child , classname should be implicit there.. So why do we need that ? ? *EDIT : - Giving Child as a parameter to super ( ) does n't make sense , because it does n't give any information.. We could have used super ( self ) .. And the method would have been searched in the super classes in the order they are given inside parenthesis.."
"I am trying to build a docker image containing Python 2 on my mac ( High Sierra ) . Here is the docker file . ( build command : docker build -t integration_test . ) Here are the contents of the requirements.txtWhen I try to build the docker image , I get the following error.I have seen a similar problem here It talks about installing the dependent library on host OS . I wanted to know how this can be done on docker image/container.Has anyone faced this problem before ? Any suggestions on how to solve this ?"
"This is my directory structure : Goals : Have an organized project structureBe able to independently run each .py file when necessaryBe able to reference/import both sibling and cousin modulesKeep all import/from statements at the beginning of each file.I Achieved # 1 by using the above structureI 've mostly achieved 2 , 3 , and 4 by doing the following ( as recommended by this excellent guide ) In any package that needs to access parent or cousin modules ( such as the Tests directory above ) I include a file called setpath.py which has the following code : Then , in each module that needs parent/cousin access , such as foo_tests.py , I can write a nice clean list of imports like so : Inside setpath.py , the second and third inserts are not strictly necessary for this example , but are included as a troubleshooting step.My problem is that this only works for imports that reference the module name directly , and not for imports that reference the package . For example , inside bar_tests.py , neither of the two statements below work when running bar_tests.py directly.I receive the error `` ImportError : No module named 'Project_3 ' '' . What is odd is that I can run the file directly from within PyCharm and it works fine . I know that PyCharm is doing some behind the scenes magic with the Python Path variable to make everything work , but I ca n't figure out what it is . As PyCharm simply runs python.exe and sets some environmental variables , it should be possible to clone this behavior from within a Python script itself . For reasons not really germane to this question , I have to reference bar using the Project_3 qualifier.I 'm open to any solution that accomplishes the above while still meeting my earlier goals . I 'm also open to an alternate directory structure if there is one that works better . I 've read the Python doc on imports and packages but am still at a loss . I think one possible avenue might be manually setting the __path__ variable , but I 'm not sure which one needs to be changed or what to set it to ."
"Tensorflow has a great deal of transformations that can be applied to 3D-tensors representing images ( [ height , width , depth ] ) like tf.image.rot90 ( ) or tf.image.random_flip_left_right ( ) for example . I know that they are meant to be used with queues hence the fact that they operate on only one image . But would there be a way to vectorize the ops to transform 4D-tensor ( [ batch_size , height , width , depth ] ) to same size tensor with op applied image-wise along the first dimension without explicitely looping through them with tf.while_loop ( ) ? ( EDIT : Regarding rot90 ( ) a clever hack taken from numpy rot90 would be to do : EDIT 2 : It turns out this question has already been answered quite a few times ( one example ) it seems map_fn is the way to go if you want an optimized version . I had already seen it but I had forgotten . I guess this makes this question a duplicate ... However for random op or more complex op it would be nice to have a generic method to vectorize existing functions ... )"
"I have a list of complex numbers for which I want to find the closest value in another list of complex numbers.My current approach with numpy : Unfortunately , this takes ages for a large amount of values.Is there a faster or more `` pythonian '' way of matching each value in myArray to the closest value in refArray ? FYI : I do n't necessarily need numpy in my script.Important : the order of both myArray as well as refArray is important and should not be changed . If sorting is to be applied , the original index should be retained in some way ."
"I have a list of dictionaries of the following form : I want to be able to sum all values for a name for a particular hour , e.g . if Name == Nick and Hour == 0 , I want value to give me the sum of all values meeting the condition . 2.75 + 2.21 , according to the piece above . I have already tried the following but it does n't help me out with both conditions . This sums up all the values for a particular Name , not checking if the Hour was the same . How can I incorporate that condition into my code as well ? My expected output :"
"I am new to Sphinx and would like to show the same figure in two different *.rst files.The first *.rst file `` lower.rst '' is on the same level as the `` figures '' folder and I am including a figure in it like this : The second *.rst file `` higher.rst '' , is several levels higher than lower.rst . In fact , I am including lower.rst in higher.rst like this : Unfortunately , in the higher.rst , the figures from lower.rst are not displayed : higher.rst looks in the current directory for the figure instead of pointing to the original lower directory.This question is sort-of addressed here : Can sphinx link to documents that are not located in directories below the root document ? , but I still do not understand how to resolve my problem with the information given there ."
"I am new to Django.Every time I install new library using pip , I have to run pip freeze -l > requirements.txt and sometimes I forget this ( and error happens at my production environment ) .What 's the best way to run this command automatically when I install new packages ... ? I am using :"
I want to iteratively fit a curve to data in python with the following approach : Fit a polynomial curve ( or any non-linear approach ) Discard values > 2 standard deviation from mean of the curverepeat steps 1 and 2 till all values are within confidence interval of the curveI can fit a polynomial curve as follows : How do I do steps 2 and 3 ?
I have a device from which I read images . Every image is an array of 8b grayscale pixels row by row . How can I display the image sequence as a video ? Something like this : Specifications : must be implemented using PyQt
"I 'm learning python recently , and is doing many practice with the language.One thing I found interesting is that , when I read from an array , it 's almost half of the time slower than list . Does somebody know why ? here 's my code : the output is : which indicates that reading array is slower than list.I think array is a fixed size memory , while list is a dynamic structure.So I assumed that array would be faster than list.Does anyone has any explanation ?"
"I 'm trying to parse a test file . the file has username , address and phone in the following format : Only for almost 10k users : ) what I would like to do is convert those rows to columns , for example : I would prefer to do it in bash but if you know how to do it in python that would be great too , the file that has this information is in /root/docs/information . Any tips or help would be much appreciated ."
"My understanding was that concurrent.futures relied on pickling arguments to get them running in different processes ( or threads ) . Should n't pickling create a copy of the argument ? On Linux it does not seem to be doing so , i.e. , I have to explicitly pass a copy.I 'm trying to make sense of the following results : Here 's the code : Where on earth is [ 97 , 32 , 17 , 15 , 57 , 97 , 63 , 72 , 60 , 8 ] coming from ? That 's not even one of the sequences passed to submit.The results differ slightly under Python 2 ."
"I need some help with get pass on the if statement . I have a problem with if statement as I am trying to get pass when I am trying to compare on the values using three variables.I can get pass on each if statement with no problem , but my problem is when I have the variable start_time with the value 22:35 which it is not equal to current_time value 23:48 . So how I can compare with the values between the start_time and current_time as I want to compare to see if it is less than the value 00:00 from the variable stop_time ? I want to check if the value from the variable stop_time which is less than the current_time and start_time.Just imagine you are watching the tv program which it start at 22:35 in your current time . You are watching which it is less than before the program finish at 00:00 , so you check the time again later before the finish time 00:00 , it say it is still half way through . Then you check it later at your current time which it is after than 00:00 so it say the program has finished.Well on my code , I will always get pass on the elif current_time > stop_time : as i always keep getting the print `` program has finished '' which I should have print `` program is half way '' instead.How can you compare three values between the variables start_time and current_time to see if it is less than and check to see if it is less the value 00:00 from the variable stop_time ? EDIT : Here is what I use them as a string when i am getting the hours and minutes from the date format especially year , month , day , hours , minutes and seconds"
"I have a triangular tessellation like the one shown in the figure.Given N number of triangles in the tessellation , I have a N X 3 X 3 array which stores ( x , y , z ) coordinates of all three vertices of each triangle . My goal is to find for each triangle the neighbouring triangle sharing the same edge . The is an intricate part is the whole setup that I do not repeat the neighbour count . That is if triangle j was already counted as a neighbour of triangle i , then triangle i should not be again counted as neighbour of triangle j . This way , I would like to have a map storing list of neighbours for each index triangle . If I start with a triangle in index i , then index i will have three neighbours , and all others will have two or less . As an illustration suppose I have an array which stores vertices of the triangle : Suppose I start my count from vertex index 2 , i.e . the one with the vertices [ [ 1.0 , 2.0 , 3.0 ] , [ 2.0 , 1.0 , 3.0 ] , [ 3.0 , 1.0 , 2.0 ] ] , then , I would like my output to be something like : Update : Following the answer from @ Ajax1234 , I think a good way of storing the output is just like how @ Ajax1234 has demonstrated . However , there is ambiguity in that output , in a sense that it is not possible to know whose neighbour is which . Although the example array are not good , I have an actual vertices from icosahedron , then if I start with a given triangle , I am guaranteed to have 3 neighbours for the first one , and two neighbours for rest ( until all the triangle counts deplete ) . In this regard , suppose I have a following array : The BFS algorithm shown in the answer below by @ Ajax1234 gives the output of while if I just swap the position of the last element such thatwhich gives an output of This is kind of ambiguous , as the positions in the gird have not been changed at all , they were just swapped . Therefore , I would like to have a consistent way the search is carried . For example , first time search of neighbours at index 2 gives [ 0 , 1 , 3 ] for both vertices1 and vertices2 , now I would like the search to be at index 0 , which finds nothing and thus go to next element 1 should find index 7 for vertices1 and index 5 for vertices2 . Thus the current output should be [ 0 , 1 , 3 , 7 ] , [ 0 , 1 , 3 , 5 ] for vertices1 and vertices2 respectively . Next we go to index 3 , and so on . After we have exhausted all the search , the final output for the first should be and that for the second shouldWhat would be the efficient way to achieve this ?"
"I was interested in comparing ruby speed vs python so I took the simplest recursive calculation , namely print the fibonacci sequance.This is the python codeand here is the ruby codeover several runs , time reports this averagethats for ruby , now python2.7 givesWhats the deal ?"
"I need to unit test a Django REST framework mixin . so i go for a test that looks like this : However when i run this test i do get the : It seems that django REST framework would have some different approach to unit test viewsets , mixins and views.But I can not figure out what should I do instead . The official docs page suggests to use real urls , but it suits more for acceptance tests rather than for unit tests ."
How can I call the same method one time to set multiple fields with a Django Rest Framework serializer ? This is what I do now but this clearly calls the method two times . How can I limit it to only be called once ?
I 'm new to MongoDB and am trying to design a simple schema for a set of python objects . I 'm having a tough time working with the concept of polymorphism.Below is some pseudo-code . How would you represent this inheritance hierarchy in MongoDB schema : The problem I 'm facing is that the schema of A.data depends on A.content . How can A be represented in a mongodb schema ?
"What is a best way in Python to hint a filename , so that it 's acceptable to pass anything into a function that you can open as a file ? Especially both strings and files found via Pathlib ."
"Having trouble getting scheduled tasks to run at a specified future time while using Celery and RabbitMQ . Using Django on a Heroku server , with the RabbitMQ add-on . The Problem : Sometimes the tasks do n't run at all , sometimes they do run , but the times that they run at are off by a significant margin ( like an hour ) .Example task that did not run : When I try to run a task with a countdown or ETA , it never actually executes . This is an example ETA task that did not run : Resulting Log : One minute later nothing happens . The unacknowledged message count in my Heroku RabbitMQ management console increases by one and stays there . This works : I 've made sure that the celery task is properly registered and RabbitMQ is configured to accept tasks by verifying that I can run the task using the delay ( ) method.Resulting Log : Any help on this would be greatly appreciated . Thanks a lot !"
"In Python , I can raise an Exception in two waysapart from the fact that you can supply exception message in latter case , is there any fundamental difference between these two styles ? Should I choose one over the other ?"
"I need to download image object from vary object storage buckets , and send that to a user through Django Rest Framework.I have something like that : data variable contains bytes type.While testing I 'm receiving error : 'utf-8 ' codec ca n't decode byte 0x89 in position 0 : invalid start byteWhat can be solution for this problem ?"
"I would like to make C directly from A and B , are there any simply ways to construct a diagonal array C ? Thanks ."
"This is part of a bigger problem where we needed to port an old application to a new device ( and of course to another programming language/framework ) . After a lot of effort ( sniffing the comm lines , reverse engineering the transmitted data for about a 2-3 weeks ) I managed to narrow in on what I think are the 4 bytes that contains 2 numbers . One of the number is a temperature reading like 30.51 , 30.46 etc as shown in the first column ( of Table 01 ) . The other value is a float that can be in the range of something like 3.9 to 3.6 ( and even lower , with 4 decimals ) . The 4 hex bytes ( col 2 of Table 01 ) should contain both these values . I had to do some reverse engineering because no documentation is available nor the source code . I managed to narrow in on the part of java code that I think decodes the hex strings into 2 number as stated . Will someone be able to check if the code is what I think that it is ? I am not a java programmer and I mostly deal with other programming languages . So I will need couple of thingsIs the attached code responsible for decoding the hex into the 3 float numbers ? This is the most importantIf possible refactor a bit of that code so that it can run on ( https : //www.compilejava.net/ ) . This is so that I can try test the algo with different sets of hex numbers . `` If possible '' add some commentsTable 01If there is any information needed please let me know because I have spent a lot of time trying to figure this out . I can record more numbers if needed.Btw the number on the left is a temperature reading ( in celsius ) . So that `` may '' involve a multiplication in the end to arrive at the number ? I am not so sure about that but I thought I 'd mention what I know about this.I do n't have the time to learn java to fix this ( it is super rare for us to deal with java ) and I have already spent close to a month on this . Really appreciate any help to just clear this hurdle . Here is the java code that decodes the hex to 2 numbers . I got this by reverse engineering the legacy app . Please note that I deleted the decompiled code that I posted earlier because I was just made aware of the fact that it is covered under an NDA.Oops , I made a mistake of not mentioning that finally this needed to be plugged into a Python program - as I was mentioning it was part of a much bigger Python project , nearly all of which I 've coded and works great . And a huge apology for not mentioning that ( and forgetting to add the Python tag ) . I will have to rewrite this in Python ."
I 've got 2 questions : How can I convert a Spanish datetime 'ago122010 ' into 2010-08-12 using pandas . Is the format used in strptime correct ? I 've tried the following : but I 'm getting the following error : how can I convert this '20100812 ' to a datetime using pandas .
I 'm following a couple of Pythone exercises and I 'm stumped at this one.What is a Tuple ? Do they mean a List of Lists ?
"results is a nested list , and looks like this : pr is a function , with definition like this : Normal iteration on results does behaves like this : But implicit iteration with map , results in this behaviour : My question : Why does map function produce the additional iteration ?"
"I have the following image : Converted to base64 , it looks like this : Download the output file ( base64 ) here : https : //file.io/NXV7v4Now , my question is : How can I retrieve the converted image and show it in jupyter notebook , without having to store it ? Based on [ this ] [ 2 ] question , I tried : but I got :"
"I have a problem to upload an image trought the Flickr API.I use OAuthLib-requests ( https : //github.com/requests/requests-oauthlib ) The Flickr doc : https : //secure.flickr.com/services/api/upload.api.htmlMy code : But in the content I can find this error : 'No photo specified'.What is the way to upload a photo ? I 'm authenticated , and others api calls work perfectly ( like 'flickr.photosets.create ' ) Thank you in advance"
"Take this code : On Python 2.7 it prints [ ' b ' , ' c ' , ] as expected.On Python 3.6 it throws an exception : The same goes for np.int32 , and other methods of the itertools package throw similar errors , e.g . when you use permutations you get TypeError : Expected int as r.I could n't find much on this apart from this numpy issue and related ones , but that one was closed 3 years ago implying it was solved.And basic things like indexing with numpy ints data [ dw [ 0 ] ] or boolean comparisons like dw [ 0 ] == 1 work just fine.Am I missing something ? Could this be a Python 3 bug ?"
"I want to use pytest to do unit testing for the scripts in another folder called src . Here is my directory structure : However , when I try to run pytest in the test folder through command line , I saw the following error from script2 import fun2 E ModuleNotFoundError : No module named 'script2'In each script , I have the following contentsscript2.py : script1.py : test_fun.py : How can I run pytest with the directory provided above ?"
"Suppose I have those two lists I want to replace elements in list a that are between 0 and 4 with items from list b.In other words , my output should be Note that essentially , we strip list a of elements satisfying a certain condition and simply replacing it with an item from list b onto this . ( The ordering in b is maintained ) EDITI actually know that exactly N number of items in list a satisfy the condition imposed . Then I need to `` replace '' these in-place with another list b of size N ."
"Depending on which installation of Python I am using I have some problems to load a module . When I type in the terminal using Python 3.4.0 , the following error message is returned : In the folder /usr/ ... as referred to above , I do however find a module called asset_pricing . ( I have to admit that I additionally do not understand why the module asset_pricing interferes . ) I installed quantecon with : I suspect that the problems are related to the Python version I am using . I also installed and when I call the module approx_markov form the terminal , using Python 2.7.6 ( I think this is the standard Python version of the OS I am using ) I do not receive any error message . To solve the problem I already followed the instruction in the following discussion but to no avail Python3 has no acces to python2 modules ( ubuntu ) ."
"I am using an adjacency matrix to represent a network of friends which can be visually interpreted as Using this matrix , I want to compile a list of all possible friendship triangles with the condition that user 1 is friends with user 2 , and user 2 is friends with user 3 . For my list , it is not required that user 1 is friends with user 3.I have a bit of code that works well with small triangles , but I need it to scale for very large sparse matrices.I was able to run the code on a csr_matrix in approximately 21 minutes . The matrix was 1032570 x 1032570 and contained 88910 stored elements . There were a total of 2178893 triplets generated.I need to be able to do something similar with a 1968654 x 1968654 sparse matrix with 9428596 stored elements . I 'm very new to python ( little less than a month of experience ) and not the greatest at linear algebra , which is why my code does not take advantage of matrices operations.Can anyone make any suggestions for improvement or let me know if my objective is even realistic ?"
"I 'm trying to product a plot in which the y-axis has both major and minor gridlines , while the x-axis has only major gridlines.It 's straightforward enough to disable minor grid lines on both axes , like so : But I ca n't find a way of applying this to only one axis . Am I missing something ?"
"I upgraded my outdated packages with brew upgrade , but now I find that the pip3 command ( pip for Python 3 ) that I previously had is gone . My Python 3.6 installation is still there : > pip points to pip for Python 2.7 : pip3.5 seems to be a leftover from an older Python 3 version : I tried using get-pip.py to get the command back , but that did n't work : What can I do now to get the command back in a clean way ? Reinstalling with brew reinstall python did not install pip . Also , note the error message : Prepending to the PATH acoording to @ Mark Setchell 's answer does not seem to change things :"
"I would like to know how i can handle it , that i get a variable/value from the subprocess to the parent.I am running the subprocess as an script . The parent looks like : The child looks like : I am totally new to python . I think the problem is , that i can not store variables this way and make them `` public '' to the parent ? Do i have to write them in a *.txt or something like that and get them with numpy.loadtxt ( ) ?"
"Having a weird problem with emails I am sending out via Python email / smtplib.I am attempting to compose an email with : Alternatives of plain-text and HTML message bodiesAn image embedded inline in the HTML bodyA separate non-inline attachmentThe MIME structure is setup like this : This seems to work fine on every mail client I 've tested { BlueMail on Android , iOS mail client , Roundcube } except for the Windows 10 mail client . For some reason , the Windows 10 built-in mail client seems to show the inline image just fine , but shows no trace of the other attachment . The limited information I have been able to find on the internet points to this being a bug with the Windows 10 mail client , but I have personally received other emails in this client with both inline and attached attachments , which are displayed just fine - so there obviously is some sort of workaround / alternative message structure that works . My question is thus : How can I format this message differently so that it will show up properly in all relevant mail clients ? I am composing the email like this , in Python : Update : Here 's the message data from Windows 10 mail ( as output via the `` save '' feature - there 's no way to view the original message raw data that I can find ... ) I 'm not sure if this is a result of saving the email from the app , or this is what the app is actually storing , but it seems that the Windows 10 Mail app is cutting out everything outside the multipart/related stanza - that is , it 's only taking the chosen alternative and not storing anything else.For comparison , I 've found and exported an email that displayed properly , with an image , html , and attachment , but the format seems to be a lot simpler - that email consisted only of a multipart/mixed layer with text/html and an application/pdf attachment . That email used an external image referenced in the HTML , instead of embedding it in the message - I would like to avoid hosting the images in each email externally ."
"I have a problem with getting nearest values for some rows in pandas dataframe and fill another column with values from those rows.data sample I have : The thing is , wherever match_v is equal to 100 , I need to replace that 100 with a value from the row where r_value is the closest to r_value from origin row ( where match_v is equal to 100 ) , but just withing group ( grouped by id ) Expected outputI have tried with creating lead and leg with shift and then finding differences . But does n't work well and it somehow messed up already good values . I have n't tried anything else cause I really do n't have any idea.Any help or hint is welcomed and I if you need any additional info , I 'm here.Thanks in advance ."
I 'm having trouble debugging unit tests in pycharm . I can run them fine with my configuration but when I run the debugger I get the following error output : My directory structure is : My unittest configuration is : tests/test_setup_script.py looks like : What does pydevd_attach_to_process do and how can I make sure it 's found during debugging ? Is the problem not actually related to that file/directory being found ?
Is there a better way of simultaneously inserting and extending a list ? Here is an ugly example of how I 'm currently doing it . ( lets say I want to insert ' 2.4 ' and ' 2.6 ' after the ' 2 ' element ) :
"I 'm trying hard to figure out how I can process a directed acyclic graph in parallel . Each node should only be able to `` execute '' when all its input nodes have been processed beforehand . Imagine a class Task with the following interface : I can not think of a way to process the graph that could be representedby this structure asynchronously with a maximum number of workers at thesame time , except for one method.I think the optimal processing would be achieved by creating a threadfor each task , waiting for all inputs to be processed . But , spawninga thread for each task immediately instead of consecutively ( i.e . when thetask is ready to be processed ) does not sound like a good idea to me.Is there a way to mimic this behaviour more ideally ? Also , this approachdoes currently not implement a way to limit the number of running tasks ata time ."
"Is it possible to use multiple concurrent transactions within one Django process ? Specifically , I 've got two functions which should each execute concurrently ( yielding between each other ) , but should each be in their own transaction . For example : I 'd like each function to be operating in an independent database transaction , so in this example , the ThingProcessingLog records will be created and visible immediately , but the Thing objects wo n't be visible until they have all been processed.How can I do this with Django 's ORM ?"
Hi I 'm sitting in a Greyhound Bus with Wifi and want to connect a second Device to the Network . But I have to accept an onscreen contract and the device does not have a browser . To accept the contract the following form has to be accepted . The device has no CURL but all the standard python 2.6. libraries . How would I write a quick python script to accept the contract ?
"I stumbled across the following warning when I was reading Code Like a Pythonista : Idiomatic Python by David Goodger . Excerpt from the article ... This is very powerful . With this , you can do all the string formatting you want without having to worry about matching the interpolation values to the template . But power can be dangerous . `` With great power comes great responsibility . '' If you use the locals ( ) from with an externally-supplied template string , you expose your entire local namespace to the caller . This is just something to keep in mind.I am trying to understand the specific scenarios in which using locals ( ) can be dangerous . Any examples of how the presence of locals ( ) in a code can be exploited are appreciated . Thanks !"
"I 'm using Nick Johnson 's Bulk Update library on google appengine ( http : //blog.notdot.net/2010/03/Announcing-a-robust-datastore-bulk-update-utility-for-App-Engine ) . It works wonderfully for other tasks , but for some reason with the following code : I get the following error in my logs : It seems quite bizarre to me . Clearly that class is right above the job , they 're in the same file and clearly the job.start is being called . Why ca n't it see my Migrate class ? EDIT : I added this update job in a newer version of the code , which is n't the default . I invoke the job with the correct URL ( http : //version.myapp.appspot.com/migrate ) . Is it possible this is related to the fact that it is n't the 'default ' version served by App Engine ?"
"I am comparing a large set of networkx graphs for isomorphism , where most of the graphs should not be isomorphic ( Lets say 0-20 % are isomorphic to something in the list , for example ) .I have tried the following approach.This let me get a much faster reduced set , but I still find it too slow for ideal use . Is there some faster algorithm to handle this type of problem ( comparing pairs of transitive commutative properties ) or a way to extend this algorithm to a multicore setup ( running on a 20 core machine ) .I am already filtering these sets of data based on the number of nodes / edges , we can assume that the nx.is_isomorphic function can not be made faster by any filtering types of operations . I also can not change tools easily right now , so using a compiled package is not an option.Additional Information : Graphs tend to be roughly 16-20 nodes with 24-48 edges total , there is a lot of interconnection so each node has roughly 8 edges . Each edge is labeled as well , but there are only 2-3 types of edges ever used ."
"I 'd like to do something like this : In one cell , and then redraw the exact same plot in another cell , like so : I 've messed around with some combinations of this stuff as well but nothing works.This question is similar to IPython : How to show the same plot in different cells ? but I 'm not particularly looking to update my plot , I just want to redraw it ."
I 'm using this tutorialhttp : //pythonvision.org/basic-tutorialHowever when I pass a png image : I get an error : TypeError : mahotas.otsu : This function only accepts integer types ( passed array of type float32 ) Does anyone have exp . w/this issue ? Thanks !
How can I tell the file ( or tty ) that is attached to my stdios ? Something like : I could look in proc : But seems like there should be a builtin way ?
"I am getting JIRA data using the following python code , how do I store the response for more than one key ( my example shows only one KEY but in general I get lot of data ) and print only the values corresponding to total , key , customfield_12830 , summaryresponse.json ( ) OUTPUT : -http : //pastebin.com/h8R4QMgB"
"I 'm writing a program in Python , and nearly every method im my class is written like this : As the class grows , it is a little bit annoying to write the same try-except block over and over . Is it possible to create some sort of 'global ' exception for the whole class ? What 's the recommended way in Python to deal with this ?"
"I am trying to optimize the following loop : I tried different solutions and found using numba to calculate the sum of the product leads to better performances : This lead toTime numpy : 4.1595Time numba1 : 0.6993Time numba2 : 1.0135Using the numba version of the sum function ( sum_opt ) performs very well . But I am wondering why the numba version of the double loop function ( numba2 ) leads to slower execution times . I tried to use jit instead of autojit , specifying the argument types , but it was worse.I also noticed that looping first on the smallest loop is slower than looping first on the biggest loop . Is there any explanation ? Whether it is , I am sure this double loop function can be improved a lot vectorizing the problem ( like this ) or using another method ( map ? ) but I am a little bit confused about these methods.In the other parts of my code , I used numba and numpy slicing methods to replace all explicit loops but in this particular case , I do n't how to set it up.Any ideas ? EDITThanks for all your comments . I worked a little on this problem : Your solution is very elegant Divakar , but I have to use this function a large number of time in my code . So , for 1000 iterations , this lead toTime numba1 : 3.2487Time numba2 : 3.7012Time numba3 : 3.2088Time convol : 22.7696autojit and jit are very close.However , when using jit , it seems important to specify all argument types.I do not know if there is a way to specify argument types in the jit decorator when the function has multiple outputs . Someone ? For now I did not find other solution than using numba . New ideas are welcomed !"
"I want to find the pct_change of Dew_P Temp ( C ) from the initial value of -3.9 . I want the pct_change in a new column.Source here : I have tried variations of this for loop ( even going as far as adding an index shown here ) but to no avail : I think the problem is the weather [ 'pct_diff ' ] , it does n't take the new it takes the last value of the data frame and subtracts it from old So its always ( 2.1-3.9 ) /3.9*100 thus my percent change is always -46 % .The end result I want is this : Any ideas ? Thanks !"
I can now post data to the viewset that looks like this : The only thing that 's bugging me is that the frontend would have to be aware of the contenttype id 's Instead I want to be able to post the content_types name like 'User ' as content_type and have the backend determine the id .
"When installing packages in Python using pip , I came across the following error : Clearly , pip is trying to use gcc-4.2 which is not in any of the binary directories , namely /usr/bin . After looking around this site and others , I have found that many others are suffering from the same issue . However , gcc is updated to version 4.2 in OS X Lion and this is not a version problem , rather , gcc is just called gcc , not gcc-4.2 , under /usr/bin , and is symlinked to llvm-gcc . After adding gcc-4.2 in /usr/bin as a symlink to gcc , pip was able to install everything successfully . Why is it then , that pip is looking for gcc-4.2 and not gcc ? Is this the fault of pip , OS X Lion , or the packages within pip that are trying to install themselves with gcc-4.2 ? Finally , is my symlink hack the right thing to do in this situation , or should I have fixed this some other way ?"
"I 've started using RQ / Redis to build out some asynchronous execution of some long running jobs for my django site . I 'm hoping to do something like the following : I want one queue for each instance of a model . You can think of this model like an api user account . ( there will not be a lot of these . 15 - 20 at most ) I will be distributing batches of tasks ( anywhere from 10 - 500 ) evenly across the queues . Multiple batches may be added before the first batch completes.With each batch , I would like to start up a worker for each queue that is not actively being worked on , and I would like to run these workers in batch mode , so that once they run out of tasks they will shut down.I realize I could just not run them in batch mode , and then I would always be working on / listening for work on all of the queues . The problem with this is that I would like to be able to add and remove queues dynamically , so it 's better if it starts up the available queues with each batch.I realize it might seem odd that I 'm distributing tasks across the queues , but the reason for this is that each task in the same queue must be rate limited / throttled according to a service I 'm using ( think of it as an API rate limit , but where each queue represents a different account ) . But for my purposes it makes no difference which account the task is running on , so I might as well parallelize across all the accounts.The problem that I am facing is that if I start a worker and give it a queue that is already being worked on , I now have two workers operating independently on that queue and so my expected throttling rate gets cut in half . How can I only start a worker if there is not already a worker operating on that queue ? I could probably find a hacky solution to this , but I would prefer to handle it the `` right '' way and since I do n't have much experience with queues I thought I should ask . I am already implementing my own worker class so that I can dynamically control the queues , so I just need a way to add logic where if that queue is already being worked on , it will not be given a new worker . A simple version of my worker is here :"
"I have a numpy array like thisand I want to create an array that looks like this : Thereby , each row corresponds to ar which is shifted by the row index + 1.A straightforward implementation could look like this : which gives me the desired output.My question is whether there is a smarter way of doing this which avoids the loop ."
"I have a raster with a set of unique ID patches/regions which I 've converted into a two-dimensional Python numpy array . I would like to calculate pairwise Euclidean distances between all regions to obtain the minimum distance separating the nearest edges of each raster patch . As the array was originally a raster , a solution needs to account for diagonal distances across cells ( I can always convert any distances measured in cells back to metres by multiplying by the raster resolution ) . I 've experimented with the cdist function from scipy.spatial.distance as suggested in this answer to a related question , but so far I 've been unable to solve my problem using the available documentation . As an end result I would ideally have a 3 by X array in the form of `` from ID , to ID , distance '' , including distances between all possible combinations of regions . Here 's a sample dataset resembling my input data :"
"What I have : What I 'd like ( using Django 1.11 ) : I 've tried : which is close , but __dict__ only gets a small subset of the settings , which I assume are hard coded ones as opposed to added attributes . Thanks for any help !"
"I 'm getting an error when importing my code_parsing package with Python 3.2.Directory code_parsing is within PYTHONPATH and contains the following files ( some others too , but irrelvant here ) __init__.py contains : When importing my module I get this error : With python 2.7 on the same machine with the same environment it works fine.Some precisions : Others import in the same directory using the same directory/__init__.py structure work fine.ada.py is a pure python file , no special compiled/cython/.pyd stuff.Any ideas ?"
"I am having problem with executing celery task from another celery task.Here is the problematic snippet ( data object already exists in database , its attributes are just updated inside finalize_data function ) : Get call in optimize_data function fails with `` Data matching query does not exist . `` If I call the retrieve by pk function in finalize_data function it works fine . It also works fine if I delay the celery task call for some time.This line : instead of works fine . But I do n't want to use hacks in my code . Is it possible that .save ( ) call is asynchronously blocking access to that row/object ?"
"My data has a datetime index like this 2016-11-05 23:40:00.I want to extract the datetime elements into three new columns of the year , month , and day . I use the following But the rusults are in float I want Help is appreciated ."
"I had a working IMAP client for Google mail , however it recently stopped working . I believe the problem is that gmail no longer allows TTL username/password logins , but now requires OAuth2.0.I would like to know the best way to alter my example below such that my twisted IMAP client authenticates using OAuth2.0 . ( And doing so without Google API packages , if that 's possible . ) Example using username/password login ( no longer works ) I have a trivial factory for this client . It gets started by using reactor.connectSSL with Google mail 's host url and port.I have followed the directions at https : //developers.google.com/gmail/api/quickstart/quickstart-python for an `` installed app '' ( but I do n't know if this was the right choice ) . I can run their `` quickstart.py '' example successfully.My quick and dirty attempt ( does not work ) I basically just copied over `` quickstart.py '' into serverGreeting and then tried to set the client state to `` auth '' .This authenticates just fine , but then twisted is unable to select the inbox : [ AriSBDGmailImap4Client ( TLSMemoryBIOProtocol ) , client ] FAIL : Unknown command { random gibberish } The random gibberish has letters and numbers and is different each time the select inbox command fails.Thanks for your help !"
"I am trying to write a memoization library that uses shelve to store the return values persistently . If I have memoized functions calling other memoized functions , I am wondering about how to correctly open the shelf file.Except this does n't workWhat you recommend for a solution to this sort of problem ."
"I 'm writing a client for a service that provides a signed url for uploads . This works just fine for smaller uploads , but fails for larger uploads that would be benefit from using a multipart upload.The authorization docs suggest that I can use the provided signature and access key id in both the URL or via the Authorization header . I '' ve tried using the header approach to start the multipart upload , but I get an access denied . When I use the query string approach , I get a method not allowed ( POST in this case ) . I 'm using boto to generate the URL . For example : Then when trying to start the multipart upload using the signed URL , I 'm doing the following : This returns a method not allowed . Is this strategy possible ? Is there a better way to provide limited credentials to a specific resource in order to allow large multipart uploads ? UpdateI 've managed to find a slightly more specific error that makes me think this is n't possible . Unfortunately , I get a 403 saying that the signature does n't match the request.This makes me think that I wo n't be able to use the signed URL because the signature wo n't match.UPDATEI 've decided that it is not reasonable to use a signed URL for a multipart upload . While I suspect it is technical possible , it is not practical . The reason being is that a signed URL requires the URL , headers and request method all match exactly in order to work as expected . As a multipart upload needs to initialize the upload , upload each part and finalize ( or cancel ) the upload , it would be some what painful to generate URLs for each step . Instead , I found can create a federated token to provide read / write access to a specific key in a bucket . This ends up being more practical and simple because I can immediately use boto as though I had credentials ."
"We have two lists , A and B : Is there a pythonic way to build the set of all maps between A and B containing 2^n ( here 2^3=8 ) ? That is : Using itertools.product , it 's possible to get all the tuples : Which gives :"
"I 'm curious about what goes on behind the scenes when using argparse . I 've checked hereand here , as apparently Namespace presently only exists in the argparse library.It 's possible I 'm using the wrong keywords to search SO/Google . It 's also possible that I 'm asking a senseless or obvious question , but here we go.When capturing a string of input in Python via argparse as such : When running the code below , I would expect that by specifying parser.add_argument ( 'string ' ... the resulting Namespace acts as a buffer for the single input string.The next line where I assign the string to `` args '' must be the first time we actually parse the input , incurring a workload proportionate to the length of the input string . At this point , `` args '' actually contains a Namespace object , which can not be parsed via for loop or otherwise ( that I know of ) .Finally , in order to parse the input using `` for '' or some other loop , I use the Namespace object to populate a string . I 'm curious how many times this process incurs a compute time proportionate to the original string length ? Which of these steps copy by address or copy by value behind the scenes ? Looks like the optimistic worse case would be 2x . Once to create the Namespace object , then once again to assign its content to `` arg_str '' Thanks for looking ! !"
"I am testing a piece of Python code that contains the line : When I run the script , I get the error : A quick google search shows the code for linsolve.py ( hosted on Koders.com ) . My question is : Is linsolve a part of scilab - or do I need to download linsolve.py separately ?"
"I have a python dictionary with string keys , integer values . I want to find the sum of values if the key is not ' ? ' . I can find the sum using a for loop like this . For ambition 's sake only , I would really like to refactor that into a reduce ( ) function . I took a stab at it : but I do n't really know what I 'm doing . I 'm sure someone with better functional chops than I can do this in minutes . Help me out ?"
"According to the Python 2.7 docs , Queue.qsize is n't dependable , and help ( Queue.Queue.qsize ) says that it is n't reliable . Is there a particular implementation issue I am not aware of ? P.S . I am aware that Queue.Queue.qsize uses mutexes , and that the size of the Queue may change between when I call the method and when I get the result , but for single-threaded applications , are Queues safe ? Message from help ( Queue.Queue.qsize ) :"
"I made a little generator function for character ranges : And then I can do this : Yay ! But this does n't work : And this works , but is O ( n ) , unlike range 's O ( 1 ) : That means it takes about 0.35 seconds to search for character number 109,999 out of the maximum of 110,000 . 109999 in range ( 110000 ) is , of course , fast.At that point , my first thought was to simply subclass range . Unfortunately : So I guess I would have to mimic it in some way that allows me to pass characters as arguments , works like range internally , and produces characters . Unfortunately , I 'm not sure how to proceed . I tried a dir ( ) : which lets me see what functions are in there , but I 'm not sure what they 're doing , or how range uses them . I looked for the source for range , but it 's in C , and I do n't know where to find its Python wrapper ( it does have one , right ? ) .Where do I go from here , and should I even go there ?"
"I have 2 functions that need to be executed and the first takes about 4 hours to execute . Both use SQLAlchemy : And here is how I define DBSession : first ( ) finishes fine ( takes about 4 hrs ) and I see `` going onto second function '' printed then it immediately gives me an error : From reading the docs I thought assigning session=DBSession would get two different session instances and so that second ( ) would n't timeout . I 've also tried playing with pool_recycle and that does n't seem to have any effect here . In the real world , I ca n't split first ( ) and second ( ) into 2 scripts : second ( ) has to execute immediately after first ( )"
"I have a class that has __eq__ and __hash__ overridden , to make its objects act as dictionary keys . Each object also carries a dictionary , keyed by other objects of the same class . I get a weird AttributeError when I try to deepcopy the whole structure . I am using Python 3.6.0 on OsX.From Python docs it looks as if deepcopy uses a memo dictionary to cache the objects it has already copied , so nested structures should not be a problem . What am I doing wrong then ? Should I code up my own __deepcopy__ method to work around this ? How ?"
"I have the following problem . Having a list of integers , I want to split it , into a list of lists , whenever the step between two elements of the original input list is not 1.For example : input = [ 0 , 1 , 3 , 5 , 6 , 7 ] , output = [ [ 0 , 1 ] , [ 3 ] , [ 5 , 6 , 7 ] ] I wrote the following function , but it 's uggly as hell , and I was wondering if anyone of you guys would help me get a nicer solution . I tried to use itertools , but could n't solve it.Here 's my solution : Thanks a lot !"
"My program for creating a Mandelbrot set has a bug : whenever the pen changes colors , and every 42nd pixel after that , is lighter . This is , rather coincidentally , a mandelbug ( yes , I just learned that term ) , as it is inconsistent for many pixels near an `` edge '' ( it might actually be blurred between the color it 's supposed to be and the color the last , or next , pixel is supposed to be ) , but it 's always the 42nd pixel after that one until the next color change . I am using OSX 10.6.8 , PYTHON 2.7 . When I wrote this program at school , it worked perfectly ( Windows ) , and then I sent it to myself , and worked on it a little more ( mostly just making the sample size and therefore image larger ) , and ran it , I got this bug . EDIT : My bad , I forgot to mention that this only happens with my Mandelbrot program , the few other turtle programs I have at home are fine.Parts of screenshots ( so that you do n't have to wait forever while the program runs to see what I 'm talking about ) : From my first version from home : From the current version ( sideways ) : Heres the code : EDIT : A fix has been suggested by DSM , who likes this bug . However , I have no experience editing Python source code , and all the underscores are making me nervous . Can someone tell me specifically what to edit and/or how ?"
"So , I am developing this class that has an indexer . I 'd like to throw ( or `` raise '' , in Python lingo ) an IndexError exception . Well , that 's pretty trivial , However , when this code is run in console and error happens , the stack trace includes also the line where the error is raised : I find this kind of odd , I 'd like to hide the inner workings of my class from the implementer , not give out an information about the file , row and code extract from an external module.Is there some way to manage this ? All the point of raising error is to provide enough information to describe why the function call went wrong , not where inside the external code the error was raised ."
"I 've created a set of demos of a TCP server however my gevent examples are noticely slower . I 'm really not testing performance but at the moment something is making the gevent version 5 times slower.I 'm sure must be how I compiled gevent but ca n't work out the problem . I 'm using OSX leopard using fink compiled python 2.6 and 2.7 . I 've tried both the stable gevent and gevent 1.0b1 and it acts the same . The echo takes 5 seconds to respond , where the other examples take < 1sec . If I remove the urllib call then the problem goes away.I put all the code in https : //github.com/djay/geventechodemoTo run the examples I 'm using zc.buildout so to buildTo run the gevent example : This will take 3-4 seconds to respond on my system.However the threaded example or the twisted exampleIs less than 1s . Any idea what I 'm doing wrong ?"
"Is there a fast numpy function for returning a list of indices in a larger array where it matches values from a smaller array ? Smaller array is ~ 30M values and larger is 800M so I want to avoid a for-loop of numpy.where calls.The problem with searchsorted is that it will return results even when their is not an exact match , it just gives the closest index , but I only want indices where there are exact matchesinstead of this : I would want this : EDIT : the set of values in both the smaller and larger arrays are always unique and sorted ."
"I 'm trying to fit some data to a lognormal distribution and from this generate random lognormal distribution using optimized parameters.After some search I found some solutions , but none convincing : solution1 using the fit function : or Solution 2 using mu and sigma from original data : None of those solutions are fitting well : I am not sure if i understand well the way to use distributions , or if I am missing something else ... I though finding the solution here : Does anyone have example code of using scipy.stats.distributions ? but I am not able to get the shape from my data ... am I missing something in the use of the fit function ? thanksEDIT : this is an example in order to understand better my problem : the result is : while , if I do the same in R : i get : that is much more closer , so I guess I am doing something wrong when using scipy ..."
"I have a daemon and it logs its operations : After a while , the log size got really big and there was no space left on the device.So I had to manually remove the logs and it was fine.What 's the best way to keep the log size not to go over certain limit ? Can it be done via logging module or do i need to write external script to just remove the logs every once in a while ?"
"I am new to Python and decorators , so apologies if this seems to be a trivial question.I am trying to apply decorators to multiple imported functions using a loop in Python as shown belowIdeally , I expect the output to be in this form : But , I am only getting the result of the choice function as the output.The decorator has not been applied to any of the functions and it also looks like the random ( ) and the randint ( 3,8 ) calls are not getting executed.I would like to know , what is going wrong here and what can be done to decorate multiple imported functions using loops ? Thanks for the help"
"I am using the python2.6 and lxml , I want to add the top level comments into the xml like thisI google this addprevious ( ) method to do that , Here is my code : But , the addprevious ( ) seems to be not very logical , you have to add the second line and then you add the first line , is there any better logical way to do that ? Thanks ."
"i have the following tablewhat i am looking to do is add a column to this table that basically enumerates the unique combinations of ui , mw , maxw , tC and HL and enumerates so for example in the above table unique combinations of ui , mw , maxw , tC and HL areThere are total 3 so the output should be something like"
"Assuming I 've the following pandas.Series : I can use the in operator to find any of the integers or Booleans . Examples , the following all yield True : However , this fails when I do : My workaround is to use pandas.Series.str or to first convert the Series to a list and then use the in operator : Any idea why I ca n't directly use the in operator to find a string in a Series ?"
"I am looking to perform element-wise mpmath operations on Python arrays . For example , Alternatively , using mpmath matricesDoes mpmath have any capibilities in this area , or is it necessary to loop through the entries ?"
"I have a large time-series set of data at 30 minute intervals and trying to do a sliding window on this set of data but separately for each point of the day using pandas.I 'm no statistician and not great at thinking or coding for this sort of work but here is my clumsy attempt at doing what I want . I 'm really looking for help improving it as I know there will be a better way of doing this , possibly using MultiIndexes and some proper iteration ? But I have struggled to do this across the 'time-axes'.run.START , run.END and run.WINDOW are two points within data and 45 ( days ) . I 've been staring at this code a lot so I 'm not sure what ( if any ) of it make sense to anyone else , please ask so that I can clarify anything else.SOLVED : ( Solution courtesy of crewbum ) The modified function which as expected goes stupidly fast : The unpivot function : The center=True on sliding_mean appears to be broken at the moment , will file it in github if I get the chance ."
"pandas to_latex ( ) methode seems to be a convenient way to convert bigger tabulars into into latex code . However , writing math mode , the $ seems to get escaped . An example : output : Is there a way to prevent this from happening ?"
"I have some data like this : I will attempt a representation to make it clearer : So in the example case , 8-9 is the critical period if the second scheme is used because all the points are active . What is a fast and good way to solving this problem in python ? I am thinking of using dynamic programming but are there other approaches that are suggested ? My approach until now : I was thinking more from a real-time perspective . So , whenever I get a new point , I do this : Assume I already got 2-10 and I get 3-15 then I pick the max of start and min of end so this case it is 3-10 and increment this interval 's count to 2 . Then the third point comes in 4-9 , pick the max which is 4 and the min is 9 and update the value 3-10 to 4-9 and update count to 3 . Now when 8-14 comes in , I pick the start of this interval is greater than 4-9 and the end of this interval is less than 4-9 . In this case , it is not true so I will create a new bucket 8-14 and I put the count to 1 . This is not the entire algorithm but should give a high-level idea of what I am doing here . I will see if I can sketch the pseudo-code ."
I have the following list : Which I would like to sort in the following order : Using l.sort ( ) yields : Any cool tricks to sort these lists easily in Python ?
"mod1.pymod2.pyComing from a C++ background I had the feeling it was necessary to import the module containing the Universe class definition before the show_answer function would work . I.e . everything had to be declared before it could be used.Am I right in thinking this is n't necessary ? This is duck typing , right ? So if an import is n't required to see the methods of a class , I 'd at least need it for the class definition itself and the top level functions of a module ? In one script I 've written , I even went as far as writing a base class to declare an interface with a set of methods , and then deriving concrete classes to inherit that interface , but I think I get it now - that 's just wrong in Python , and whether an object has a particular method is checked at runtime at the point where the call is made ? I realise Python is so much more dynamic than C++ , it 's taken me a while to see how little code you actually need to write ! I think I know the answer to this question , but I just wanted to get clarification and make sure I was on the right track.UPDATE : Thanks for all the answers , I think I should clarify my question now : Does mod2.show_answer ( ) need an import ( of any description ) to know that thing has a method called answer ( ) , or is that determined dynamically at runtime ?"
"I am trying to follow tutorial at http : //conda.pydata.org/docs/build_tutorials/pkgs.html to create conda skeleton from pypi pyinstrument package.when I call command conda skeleton pypi pyinstrument , it throws following error : This error is also thrown when trying to install py-stackexchange package , what am I doing wrong ?"
"I have two ordered numpy arrays and I want to interleave them so that I take one item from the first array , then another from the second , then back to the first - taking the next item that is larger than the one I just took from the second and so on.Those are actually arrays of indices to other arrays , and I 'll be ok with operating on the original arrays as long as the operation is vectorized ( but of course working on the index array as a vector operation will be awesome ) .Example ( ok to assume that the intersection of arrays is empty ) I would like to get [ 1,5,7,13,17,19 ]"
"I believe I am having a memory issue using numpy arrays . The following code is being run for hours on end : where new_x , new_y1 , new_y2 , new_y3 are floats.After about 5 hours of recording this data every second ( more than 72000 floats ) , the program becomes unresponsive . What I think is happening is some kind of realloc and copy operation that is swamping the process . Does anyone know if this is what is happening ? I need a way to record this data without encountering this slowdown issue . There is no way to know even approximately the size of this array beforehand . It does not necessarily need to use a numpy array , but it needs to be something similar . Does anyone know of a good method ?"
I want to be able to define what the contents of a subclass of list have to be . The class would look like the following.I want to include typing such that the following would happen .
So I 'm attempting to implement something similar to how unittesting frameworks do the following thing : A thing I find peculiar : Which would indicate to me that it is n't using a metaclass to set __test__ to True on construction of the type.I tried grepping through the python library find . -name `` *.py '' | xargs grep '__test__ ' but did not seem to find anything related to this.My `` guess '' approach at solving this problem is to do the following : However this feels fragile to me ... Is there a cleaner / nicer way to do this that works in all cases ? Is there ever a chance that a class will not have a __dict__ property ?
"I am trying to code a lazy version of Sieve of Eratosthenes in Python 3.2 . Here 's the code : However , when I iterate over primes ( ) , I only get consecutive numbers . E.g. , prints the listTo my surprise , the following tiny modification of primes ( ) makes it work : I am guessing I am missing something about the scope of the parameters of the generatorbut I can not see how to fix the code without adding this random-looking new line . Does anybody know what I am doing wrong ? Thanks ."
"I 'm doing some parsing that requires one token of lookahead . What I 'd like is a fast function ( or class ? ) that would take an iterator and turn it into a list of tuples in the form ( token , lookahead ) , such that : basically , this would be handy for looking ahead in iterators like this : Though , I 'm not sure if there 's a name for this technique or function in itertools that already will do this . Any ideas ? Thanks !"
There is this code : Why __instancecheck__ is called for [ ] argument but not for a argument ?
"This is in reference to the answer for this question to `` Use abc module of python to create abstract classes . '' ( by @ alexvassel and accepted as an answer ) . I tried the suggestions , but strangely enough , in spite of following the suggestions to use the abc way , it does n't work for me . Hence I am posting it as a question here : Here is my Python code : When I execute this module , here is the output on my console : as opposed to that accepted answerSo what am I doing right or wrong ? Why does work and not fail ? Appreciate any expert insight into this ."
"I 'm trying to write a Python script that finds all integers ( N ) where a certain power of the sum of the digits of N is equal to N. For example , N=81 qualifies , because 8 + 1 = 9 , and a certain power of 9 ( namely 2 ) = 81 . The ranges I chose are arbitrary . My script works but it is very , very slow . Ideally , I 'd want to find the first 30 such integers in about 6000ms.My first solution : In my second solution , I tried storing all the powers for each sumOfDigits , but that did not improve the performance much.I have n't studied data structures and algorithms yet , so I 'd appreciate any pointers on making this script more efficient ."
"So I 'm running the following code using the psycopg2 driver in Python 3.5 to Pandas 19.x.The read_csv blows chunks when reading from the memory buffer : Uh..wot path ? buf is in memory . What am I missing here ? Just FYI , the Copy to seems to be working as expected.SOLUTION CODE BELOWThanks to the answer below , My query speed doubled using this method and my memory use dropped by 500 % . Here is my final test code included to help others resolve their performance issues . I 'd love to see any code that improves this ! Be sure to link back to this question in your question.Speed is ~4 minutes to copy the data from postgres and less than 30 seconds to load it into the the pandas dataframe . Note the copy command is a feature of the psycopg2 driver and may not work in other drivers ."
"I need to implement GPR ( Gaussian process regression ) in Python using the scikit-learn library.My input X has two features . Ex . X= [ x1 , x2 ] . And output is one dimension y= [ y1 ] I want to use two Kernels ; RBF and Matern , such that RBF uses the 'x1 ' feature while Matern use the 'x2 ' feature . I tried the following : But this gives an error ValueError : Found input variables with inconsistent numbers of samples : [ 2 , 18 ] I tried several methods but could not find a solution . Really appreciate if someone can help ."
"For example : When I check a type/annotation of bar pycharm tells me that it is Optional [ int ] .bar : int = None looks much cleaner rather then bar : Optional [ int ] = None , especially when you have 10+ parameters.So can I simply omit Optional ? Will tools like mypy or other linters highlight this case as en error ? Looks like python itself does n't like the idea :"
"I have a certain SQLAlchemy declarative Base that I create on a sqlite memory DB : I 'm using this for unit testing logic.With this I have my tables in the DB . But now I wish to migrate certain things using alembic.AFAIK alembic migrations use the env.py run_migrations_online and there uses a SQLAlchemy function called engine_from_config creating a new engine here.The problem I wish to solve is to have a way to use the previously created connection , which holds the recently created tables , for the alembic migrations.I used this on my test scripts : Using Alembic API from inside application code , so that my script does the following after the previous create_all call : [ Please mind , I would just create my schemas with the Base.metadata.create_all ( engine ) call but my alembic versions not only hold schema changes , they also have some filling of catalog tables data , that 's why I intend to use alembic here . In fact , if my alembic migrations hold some `` create tables '' logic , these two would conflict . So I can safely remove the create_all call and depend on alembic alone to create my schemas here . ] Having already modified my alembic 's env.py : As far as I can tell the connectable=engine_from_config creates a connection to a new engine on a new sqlite : /// : memory : database , and that 's why I ca n't upgrade via alembic the previously created DB on my script with create_all ( engine ) .So ... TLDR ; is there a way to pass my previously existing engine connection ( with my created tables ) to alembic so that it can migrate it ? ( I 'm pretty sure that the dbPath arg I created is useless here , in fact , I 'm just copying what the other post I referenced uses ) ."
"I want to do something like this , but I have n't had much success so far . I would like to make each attr a property that computes _lazy_eval only when accessed : ** UPDATE **This does n't work either : ** UPDATE 2 **Used the __metaclass__ solution , but stuck it in Base.__new__ instead . It looks like it needed a better defined closure -- `` prop ( ) '' -- to form the property correctly :"
"I am trying to create a generator ( iterator which supports doing a next , perhaps using yield in python ) which gives all combinations of r elements from { 1,2 , ... n } ( n and r are parameters ) such that in the selected r elements , no two are consecutive.For example , for r = 2 and n= 4The generated combinations are { 1,3 } , { 1,4 } , { 2 , 4 } .I could generate all combinations ( as an iterator ) and filter those which do n't satisfy the criteria , but we will be doing unnecessary work.Is there some generation algorithm such that the next is O ( 1 ) ( and if that is not possible , O ( r ) or O ( n ) ) .The order in which the sets are returned is not relevant ( and hopefully will allow an O ( 1 ) algorithm ) .Note : I have tagged it python , but a language-agnostic algorithm will help too.Update : I have found a way to map it to generating pure combinations ! A web search reveals that O ( 1 ) is possible for combinations ( though it seems complicated ) .Here is the mapping.Suppose we have a combination x_1 , x_2 , ... , x_r with x_1 + 1 < x_2 , x_2 + 1 < x_3 , ... We map to y_1 , y_2 , ... , y_r as followsThis way we have that y_1 < y_2 < y_3 ... without the non-consecutive constraint ! This basically amounts to choosing r elements out of n-r+1 . Thus all I need to do is run the generation for ( n-r+1 choose r ) .For our purposes , using the mapping after things are generated is good enough.Reasons for choosing svkcr 's answerAll great answers , but I have chosen svkcr 's answer.Here are some reasons whyIt is effectively stateless ( or `` Markovian '' to be more precise ) . The next permutation can be generated from the previous one . It is in a way almost optimal : O ( r ) space and time.It is predictable . We know exactly the order ( lexicographic ) in which the combinations are generated.These two properties make it easy to parallelise the generation ( split at predictable points and delegate ) , with fault tolerance thrown in ( can pick off from the last generated combination if a CPU/machine fails ) ! Sorry , parallelisation was not mentioned earlier , because it did n't occur to me when I wrote the question and I got that idea only later ."
"Apart from PyYAML , are there any safe Python data serialization libraries which correctly handle unicode/str ? For example : Note that I want the serializers to be safe ( so pickle and marshel are out ) , and PyYAML is an option , but I dislike the complexity of YAML , so I 'd like to know if there are other options.Edit : it appears that there is some confusion about the nature of my data . Some of them are Unicode ( ex , names ) and some of them are binary ( ex , images ) … So a serialization library which confuses unicode and str is just as useless to me as a library which confuses `` 42 '' and 42 ."
"Example : Ref : https : //docs.python.org/3/library/enum.html # planetWhy do I want to do this ? If there are a few primitive types ( int , bool ) in the constructor list , it would be nice to used named arguments ."
"This may be a bit of a weird question , but is there any reliable way to serialize IronPython objects whose classes extend CLR types ? For instance : System.Collections.Generic.List < string > is serializable with Pickle , as it implements the ISerializable interface , but emitted subclasses of serializable CLR types seem to not work , and i get ImportError : No module named Generic in mscorlib , Version=4 when running pickle.dumps ( Foo ( ) ) .Additionally , running the usual Formatter.Serialize ( stream , object ) gives me : How can I implement serialization of IronPython objects when running in an embedded C # environment ?"
"I have an automatically generated regular expression , which basically is one big `` or '' group like so : I 've noticed that in case ofIt would match `` hat '' only , not `` hat . '' as I want . Is there a way to make it more greedy ? UPDATE : forgot about word boundaries , sorry for that ."
"I spent almost an hour googling for the solution , but the documentation for numpy.distutils is very sparse.I have a f2py-wrapped module . It consists basically of 3 files : The module is well compiled with the following shell-script command.As a result , I have the python module a.so ( the name is specified in the .pyf file ) .How do I do that with numpy.distutils ( or some other python-oriented building tools ) ? A less important question is , can I also include the dependence from lib.a ( and rebuild it when necessary ? )"
"I have a DataFrame like this : I 'm trying to understand how to apply a custom rolling function to it . I 've tried doing this : But this gives me the original DataFrame back : If I have a different DataFrame , like this : The same rolling apply seems to work : Why is this not working for the first DataFrame ? Pandas version : 0.20.2Python version : 2.7.10UpdateSo , I 've realized that df2 's columns are object-type , whereas the output of my lambda function is an integer . df3 's columns are both integer columns . I 'm assuming that this is why the apply is n't working . The following does n't work : Furthermore , say I want to concatenate the characters in the value column on a rolling basis , so that the output of the lambda function is a string , rather than an integer . The following also does n't work : What 's going on here ? Can rolling operations be applied to object-type columns in pandas ?"
"I was playing around with PIL and transformation matrices to understand what 's behind simple 2D image manipulation.In my attempt to rotate an image as `` low level '' as possible ( that is , not using any rotate ( degrees ) function , but doing the math ) I decided to rotate every pixel of the image using a clockwise rotation matrix : The rotation went fine , but the image now looks like it 's missing some pixels.Original image , painted on a 435x353 black background : Rotated 45° clockwise and moved to the right by 300 pixels : Oddly enough , the issue does n't occur when rotating the picture 90° clockwise ( and moving 400 px to the right ) : What could be causing this ? Using Image.Image.rotate works just fine , so I guess the issue resides on my code . It 's worth mentioning the original picture has a transparent background , which was lost in the compression when uploaded here . However , I 've done the exact same operation to a jpeg ( non-transparent ) image and the result was the same.Code used to do the rotation :"
"I create a custom legend , but he does not show the hatches of the patches . Whats wrong ? Thanks"
"I want to be able to create an instance of a parent class X , with a string `` Q '' as an extra argument.This string is to be a name being an identifier for a subclass Q of the parent class X.I want the instance of the parent class to become ( or be replaced with ) an instance of the subclass . I am aware that this is probably a classic problem ( error ? ) . After some searching I have n't found a suitable solution though.I came up with the following solution myself ; I added a dictionary of possible identifiers as keys for their baseclass-instances to the init-method of the parent class.Then assigned the class-attribute of the corresponding subclass to the current instances class-attribute.I required the argument of the init-method not to be the default value to prevent infinite looping.Following is an example of what the code looks like in practice ; output : Is there a more pythonic solution and are there foresee-able problems with mine ? ( And am I mistaken that its a good programming practice to explicitly call the .__init__ ( self ) of the parent class in .__init__ of the subclass ? ) .My solution feels a bit ... wrong ... Quick recap so far ; Thanks for the quick answers @ Mark Tolonen 's solutionI 've been looking into the __new__-method , but when I try to make A , B and C in Mark Tolonen 's example subclasses of Z , I get the error that class Z is n't defined yet . Also I 'm not sure if instantiating class A the normal way ( with variable=A ( ) outside of Z 's scope ) is possible , unless you already have an instance of a subclass made and call the class as an attribute of an instance of a subclass of Z ... which does n't seem very straightforward . __new__ is quite interesting so I 'll fool around with it a bit more , your example is easier to grasp than what I got from the pythondocs . @ Greg Hewgill 's solutionI tried the staticmethod-solution and it seems to work fine . I looked into using a seperate function as a factory before but I guessed it would get hard to manage a large program with a list of loose strands of constructor code in the main block , so I 'm very happy to integrate it in the class.I did experiment a bit seeing if I could turn the create-method into a decorated .__call__ ( ) but it got quite messy so I 'll leave it at that ."
"I am trying to save GeoPoint in my Firebase Firestore using Python , and I did n't have any success.This is giving me syntax error : Is there any solution for this ? Maybe I 'm missing something to import , but I ca n't find what . Thanks in advance ."
"Does the Django ORM provide a way to conditionally create an object ? For example , let 's say you want to use some sort of optimistic concurrency control for inserting new objects.At a certain point , you know the latest object to be inserted in that table , and you want to only create a new object only if no new objects have been inserted since then.If it 's an update , you could filter based on a revision number : However , I ca n't find any documented way to provide conditions for a create ( ) or save ( ) call . I 'm looking for something that will apply these conditions at the SQL query level , so as to avoid `` read-modify-write '' problems ."
"Setup : Camera : Blackfly S Mono 20.0 MPLens : Opto telecentric lens TC23080Lights : 16 green LEDSPython : 3.7.3openCV : 4.0+Sorry for the image links , but one image is around 20MB , also did not want to loose any qualityImage samples : https : //drive.google.com/file/d/11PU-5fzvSJt1lKlmP-lQXhdsuCJPGKbN/view ? usp=sharinghttps : //drive.google.com/file/d/1B3lSFx8YvTYv3hzuuuYtphoHBuyEdc4o/viewCase : There will be metal parts with different shapes from 5x5 to 10x10 size ( cm ) . Inside these metal parts there are plenty of circular holes from 2 to 10~ that have to be detected very accurately . The actual size of holes are unknown , as there are huge variety of possible parts . The goal is to write a generic algorithm with OpenCV , that could work with any metal parts and detect circular holes.What we have tried : We have tried to detect the holes with HoughCircles algorithm with little to no success . The algorithm is either too sensitive , or it does not detect the holes at all . We have experimented with different param1 and param2 values with no success . We have also tried blurring the image and passing it through Canny before using HoughCircles , but such an approach did not produce better results . The very same algorithm works significantly better with lower resolution pictures . However , resolution can not be sacrificed as accuracy is extremely important in this project.https : //drive.google.com/file/d/1TRdDbperi37bha0uJVALS4C2dBuaNz6u/view ? usp=sharingThe above circles were detected with the following parameters : By playing around with the above parameters , we can get almost the results that we want . The problem arises when we use the same parameters with different pictures . The end result we want to get is the diameter of a given circle with great accuracy , and we want the same algorithm to be usable on different part picturesWhat makes this problem different from the other ones posted is that we do not know the approximate radius of a given circle ( so we can not manipulate minradius , maxradius , param1 , param2 or any other values ) ."
Yeah I know there are a lot of similar questions up there . But I just can not find what I was looking for.My confusion is about the backward slicing.Now I have found that the result will be So I thought that maybe I will test it out by changing the ends in that string slicing.I was really sure that Python would give me something like Instead it gave me this which made me totally lost ... Can someone explain what is going on here ? I am new to Python and find this very confusing.. Thanks for any help .
"I 'm trying to make a basic app on Google App Engine with two modules using Google App Engine Modules ( https : //developers.google.com/appengine/docs/python/modules/ ) and They share session information between the modules : Modules : Module 1 - Login Page : a basic page with a login form where if is a valid user I create a session and then the user is redirected to the dashboard page ( Module 2 ) Module 2 - Dashboard Page : a page that show a message if the module can read the data in the session variableThe problem is that in the dashboard module the session data created in the Login Page ( module 1 ) does not exist . Is it possible access the sessions data between two o more modules in Google App Engine ? Source : baseHandler.pyModule 1 ( Signin ) main.pyModule 2 ( Dashboard ) main.pyAny help is welcome , sorry if something is misspelled"
"I am running a jupyter-notebook on a Spark cluster ( with yarn ) . I am using the `` findspark '' package to set up the notebook and it works perfectly fine ( I connect to the cluster master through a SSH tunnel ) .When I write a `` self-contained '' notebook , it works perfectly , e.g . the following code runs with no problem : The Spark job is perfectly distributed on the workers . However , when I want to use a python package that I wrote , the files are missing on the workers . When I am not using Jupyter-notebook and when I use spark-submit -- master yarn -- py-files myPackageSrcFiles.zip , my Spark job works fine , e.g . the following code runs correctly : main.pyThenThe question is : How to run main.py from a jupyter notebook ? I tried specifying the .zip package in the SparkContext with the pyfiles keyword but I got an error ..."
"With the new release of GAE 1.5.0 , we now have an easy way to do async datastore calls . Are we required to call get_result ( ) after calling 'put_async ' ? For example , if I have an model called MyLogData , can I just call : right before my handler returns without calling the matching get_result ( ) ? Does GAE automatically block on any pending calls before sending the result to the client ? Note that I do n't really care to handle error conditions . i.e . I do n't mind if some of these puts fail ."
"You can specify types of parameters in Python docstrings like this : With Sphinx ' autodoc feature , this yields the parameters list , and each parameter is properly tagged with their types.But how do I do this with instance attributes ? Something like thisdoes not work . One can put a single-word type after the : ivar but here , it consists of three words , so that would not work ."
"Here are my simple viewset and serializer classes : Suppose I want to update only my user 's first name . In that case , I should use PATCH { `` first_name '' : `` New First Name '' } . But at the same time , it looks like that PUT { `` first_name '' : `` New First Name '' } also works the same way , though it should n't , because it has to validate that all the fields are specified . At least I thought so . Am I wrong ? And if I 'm , then what is the difference between update and partial_update in Django Rest Framework and is there any reason to keep them both ( since any additional method implies additional testing , so the latter question is a bit philosophical , because looks like people find this PUT/PATCH pair really confusing ) . By the way , I 'm using djangorestframework==3.8.2 . Thank you ."
When I run Celery from the command line I can see only the tasks that are in the same file as the Celery object but not those in other files.The structure of the project is the following : The content of the files is as followsAnd any of the tasks.py files has something like thisWhen I run Celery as followsI get the followingthat is just the task that is in the same file as the application.Any suggestions on how to solve it ? I really need to separate the tasks by topics because they are many so that they are in a single file .
"This is my first post and I 'm still a Python and Scipy newcomer , so go easy on me ! I 'm trying to convert an Nx1 matrix into a python list . Say I have some 3x1 matrixx = scipy.matrix ( [ 1,2,3 ] ) .transpose ( ) My aim is to create a list , y , from x so that y = [ 1 , 2 , 3 ] I 've tried using the tolist ( ) method , but it returns [ [ 1 ] , [ 2 ] , [ 3 ] ] , which is n't the result that I 'm after . The best i can do is thisbut it 's a bit cumbersome , and I 'm not sure if there 's an easier way to achieve the same result . Like I said , I 'm still coming to grips with Python and Scipy ... Thanks"
Does anyone knows how I can get the URL name from the request.get_full_path ( ) ? For example : I have this URL in urls.pyIn my context_processor.py : How can I return `` evaluation '' instead of `` /evaluation-active/ '' ? Thanks
"I 'm really confused : The standard approach in Java is to throw exceptions only in `` abnormal '' conditions and not to use them to signal end-of-iterator.examples : Effective Java , item 57 ( `` Use exceptions only for exceptional conditions '' ) and JavaSpecialists newsletter 162 : Flow control We should never cause an exception that is otherwise preventable . I have seen code where instead of checking bounds , it is assumed that the data will be correct and then RuntimeExceptions are caught : Here is an example of bad code ( please do n't code like this ) : whereas it is standard to use this idiom in Python , e.g . StopIteration : exception StopIteration Raised by an iterator ‘ s next ( ) method to signal that there are no further values . This is derived from Exception rather than StandardError , since this is not considered an error in its normal application.Why is it bad for Java but good for Python ?"
"Consider the following functionThe call to tape.watch ( x ) is necessary if the function is called say as foo ( tf.constant ( 3.14 ) ) , but is not when it is passed in a variable directly , such as foo ( tf.Variable ( 3.14 ) ) .Now my question is , is the call to tape.watch ( x ) safe even in the case when tf.Variable is passed in directly ? Or will some strangness happen due to the variable already being auto-watched and then watched manually again ? What is the correct way to write general functions like this that can accept both tf.Tensor and tf.Variable ?"
"I have the following code : Pylint reports error : ID : E1101 A.random_function : Instance of ' A ' has no 'name ' memberIt 's true , but I do n't care because A is abstract . Is there a way to get rid of this warning without just suppressing it ? Thanks"
"I use this python code to output the number of Things every 5 seconds : If another process generates a new Thing while my_count ( ) is running , my_count ( ) will keep printing the same number , even though it now has changed in the database . ( But if I kill my_count ( ) and restart it , it will display the new Thing count . ) Things are stored in a MYSQL innodb database , and this code runs on ubuntu.Why wo n't my_count ( ) display the new Thing.objects.count ( ) without being restarted ?"
"I 'm trying to switch our application from python mail to Mailgun but am having trouble with emails that have attachments . Specifically PDF 's that are generated by the application ( not stored in the file system ) .Have no problems sending emails without attachments.Currently we generate the PDF as such : And then attach and mail it as such : Works great ... how can we convert to a Mailgun call ? I 've tried various things including just passing it as file as is ( unsuccessfully ) : The above works fine without the attachment . data contains to , from , o : tags ... etc.Any help would be appreciated . Thanks ! EDITI was able to get it to work by changing my PDF code and getting the requests.post structured properly :"
"I noticed that Pandas knows how to smartly format a timedelta object into a string.When I try to do this manually I keep getting the string in nanoseconds.I would rather not reinvent the wheel , so is there any way to access the string formatting method ( or the string itself ) that Pandas uses to show a timedelta object in x days , hh : mm : ss ?"
I am having trouble creating a simple mixin that i plan to use on a bunch of sqlalchemy declarative classes . The basic idea is I want to have a create/modify time stamp and a create/modify user stored on multiple tables . The mixin is in its own file ( global_mixins.py ) and the class is imported in each model file that needs the mixin . When I run an import of data i get the error below the code .
"I hit the block wall again . I am a total newbie so I have to rely on your mighty knowledge again.I was starting with a dataset , looking like this : And my code looking like this : Which produced an error : The error and data is shortened.Obviously , I have no idea what does it mean or how to fix it..I am trying to make a k-means model from the example on different dataset . K-means model is required.Thank you ! Merry Christmas !"
I recently upgraded to python2.7 and django1.3 and since thenYour help would be greatly appreciated .
"I want to transfer my vector to array , so I usewhere the column features is vector dtype . But Spark tells me that I know the reason must be the type I use in the UDF so I tried get_array = udf ( lambda x : x.toArray ( ) , ArrayType ( FloatType ( ) ) ) , which also can not work.I know it is numpy.narray after transfer , but how can I show it correctly ? Here is the code how I get my dataframe result2 : Here is what indexed looks like : In result2 , I have some columns with type double , and then I use VectorAssembler assemble those double columns into a vector features , which is the column that I want to transfer to array ."
I know how to write the txt file using numpy.savetxt ( ) but I ca n't get it to write the file using just integers . I have the following code : This is how the matrix that I 'm getting looks like : What I 'm looking for is something like this :
I have recently noticed that if I mock out a method using mock.patch it does n't list the instance object passed to the mocked method in the call_args field . Is this by design ? Code/output below may better explain what I mean : Output is : You can see that the instance object is passed to new_method when it replaces bark . But you can not see it in the call_args of the mocked out method . Is n't this strange ? I am using version 1.01 of the python mock library .
"Okay I know this has been asked before with a limited example for scaling [ -1 , 1 ] intervals [ a , b ] Different intervals for Gauss-Legendre quadrature in numpy BUT no one has posted how to generalize this for [ -a , Infinity ] ( as is done below , but not ( yet ) fast ) . Also this shows how to call a complex function ( in quantitative option pricing anyhow ) with several implementations . There is the benchmark quad code , followed by leggauss , with links to code examples on how to implement an adaptive algorithm . I have worked through most of the linked adaptive algorithm difficulties - it currently prints the sum of the divided integral to show it works correctly . Here you will find functions to convert a range from [ -1 , 1 ] to [ 0 , 1 ] to [ a , Infinity ] ( thanks @ AlexisClarembeau ) . To use the adaptive algorithm I had to create another function to convert from [ -1 , 1 ] to [ a , b ] which is fed back into the [ a , Infinity ] function.Still need to increase the speed and accuracy with less dimensions as I ca n't manually adjust the degrees range for -a to get convergence . To illustrate why this is a problem , put in these values instead : a=-20 , F=50 , then run . You can increase degrees=1000 and see that there is no benefit to this Gauss-Legendre algorithm if it is not applied intelligently . My requirement for speed is to get to 0.0004s per loop , whereas the last algorithm I Cythonized took about 0.75s , which is why I am trying to use a low degree , high accuracy algorithm with Gauss-Legendre . With Cython and multi-threading this requirement from a completely optimized Python implementation is roughly 0.007s per loop ( a non-vectorized , loop ridden , inefficient routine could be 0.1s per loop , with degrees=20 , i.e . % timeit adaptive_integration ( x , w ) .A possible solution which I 've half implemented is here http : //online.sfsu.edu/meredith/Numerical_Analysis/improper_integrals on pages 5/6 , adaptive integration whereas the interval a-b ( in this case , I wrote the transform_integral_negative1_1_to_a_b function ) where the interval is divided in 2 ( @ 0.5 ) , the function is then evaluated on these 1/2 intervals , and the sum of the two 0- > 0.5 + 0.5- > 1 are compared to the function results for the whole range 0- > 1 . If accuracy is not within tolerance , the range is further subdivided at 0.25 and 0.75 , the function is again evaluated for each subinterval , and compared to the prior 1/2 interval sums @ 0.5 . If 1 side is within tolerance ( e.g . abs ( 0- > 0.5 - ( 0- > 0.25 + 0.25- > 0.5 ) ) < precision ) , but the other side is not , splitting stops on the side within tolerance , but continues on the other side until precision is reached . At this point the results for each slice of the interval are summed to obtain the full integral with higher accuracy.There are likely faster and better ways of approaching this problem . I do n't care as long as it is fast and accurate . Here is the best description of integration routines I 've come across for reference http : //orion.math.iastate.edu/keinert/computation_notes/chapter5.pdf Award is 100pts bounty + 15pts for answer acceptance . Thank you for assisting in making this code FAST and ACCURATE ! EDIT : Here is my change to the adaptive_integration code - if someone can make this work fast I can accept an answer and award bounty . This Mathematica code on page 7 http : //online.sfsu.edu/meredith/Numerical_Analysis/improper_integrals does the routine I attempted . It has work on a routine that does n't converge well , see the variables below . Right now my code errors out : RecursionError : maximum recursion depth exceeded in comparison on some inputs , or if the degrees are set too high , or does n't get close to the quad result when it does work , so something is apparently wrong here.The output with degrees=100 ( after calculating GL with degrees=10000 for a better initial estimate , otherwise , the algorithm always agrees with its own accuracy apparently and does n't invoke the adaptive path which fails every time ) :"
"I 'd like to enumerate those items in an iterable that satisfy a certain condition . I 've tried something like ( that tries to enumerate the numbers between 4 and 7 just for the sake of an example ) . From this , I get the resultWhat I 'd like to get isIs there a pythonic way to achieve the desired result ? Note that in the actual problem I 'm working on , I do n't know in advance how many items satisfy the condition ."
"This is my Python program : If I run this using python myfile.py it prints an empty string . If I run it using myfile.py , it prints the correct path . Why is this ? I 'm using Windows Vista and Python 2.6.2 ."
"I 've written an LSTM network with Keras ( following code ) : The monitored metrics are loss , accuracy , precision , recall and f1 score.I 've noticed that the validation loss metric start to climb around 300 epochs , so I 've figured overfitting ! however , recall is still climbing and precision is slightly improving . Why is that ? is my model overfitted ?"
I was wondering if there is an easy way to build an indexable weak ordered set in Python . I tried to build one myself . Here 's what I came up with : Is there an easier way to do this ?
"I have two lists list1 and list2 . I 've found on stackoverflow a very simple method to get the common elements in this two lists as follows result = list ( set ( list1 ) & set ( list2 ) ) . Unfortunately , with this , the order of elements in the resulting list , is not preserved.For instance : I want the result ( common elements ) to be [ ' e ' , ' a ' , ' b ' , ' c ' ] in this order . Because , for instance , ' e ' is in list1 and in list2 and is in position 2 in list1 and position 1 in list2 , while ' a ' is in list1 and in list2 and is in position 1 in list1 and position 3 in list2 , so ' e ' is before ' a ' since 2+1 < 1+3.So , is there any simple way to have the common elements between two lists and preserving the order of elements ?"
"I have a decorator ( call it deco ) that I would like to apply to every view in my Flask app , in order to modify the response headers to avoid IE 's compatibility mode ( res.headers.add ( `` X-UA-Compatible '' , `` IE=Edge '' ) . I use it likeI currently use a subclass of Flask to create the app ( to modify jinja behavior ) Is there a way I can modify CustomFlask to apply deco decorator to all the responses ?"
"I 've been trying to import a p4 depot path to git using git-p4 python script.After configuring my environment ( git 1.7.1 , python 2.7 , Windwos XP , p4 env variables ) I tried to runthe git-p4 script , geetting the following ouptut : Does anybody know what 's going on here ? If I try to run the git command that line # 332 states ( git rev-parse origin ) from the command line shell , the command is correctly executed.Thanks.Update : Seems that the script is unable to launch any process whose exec file is not in the execution path.I think it 's an initialization issue with python on windows ..."
"As the following piece of code shows , the tensorflow tf.nn.dilation2D function does n't behave as a conventional dilation operator . Returns the following tensor : I do n't understand neither why it behaves like that , neither how I should use tf.nn.dilation2d to retrieve the expected output : Can someone enlighten the succinct documentation of tensorflow and give an explanation of what the the tf.nn.dilation2D function does ?"
"I need to build a string in python and pass it to the wrapped C lib . The C lib defines the function : In the .pyx file : But I get : TypeError : expected bytes , str foundI have tried encoding the python string but get the same error ."
"I have a string abccddde I need to find substrings like : a , b , c , cc , d , dd , ddd , esubstrings ab or cd are not valid.I tried finding all the substrings from a string but its not efficientThis is outputting : [ ' a ' , 'ab ' , 'abc ' , 'abcc ' , 'abccd ' , 'abccdd ' , 'abccddd ' , 'abccddde ' , ' b ' , 'bc ' , 'bcc ' , 'bccd ' , 'bccdd ' , 'bccddd ' , 'bccddde ' , ' c ' , 'cc ' , 'ccd ' , 'ccdd ' , 'ccddd ' , 'ccddde ' , ' c ' , 'cd ' , 'cdd ' , 'cddd ' , 'cddde ' , 'd ' , 'dd ' , 'ddd ' , 'ddde ' , 'd ' , 'dd ' , 'dde ' , 'd ' , 'de ' , ' e ' ] This was the method i followed to find the substrings but it gives all the possiblities but that is what makes it inefficientPlease Help !"
In the code below the astuple function is carrying out a deep copy of a class attribute of the dataclass . Why is it not producing the same result as the function my_tuple ? FootnoteAs Anthony Sottile 's excellent response makes clear this is the behavior coded into Python 3.7 . Anyone expecting astuple to unpack the same way as collections.namedtuple will need to replace it with a method similar to Demo.my_tuple . The following code is less fragile than my_tuple because it will not need modification if the fields of the dataclass are changed . On the other hand it wo n't work if __slots__ are in use.Both versions of the code pose a threat whenever a __hash__ method is present in the class or its superclasses . See the Python 3.7 documentation for unsafe_hash in particular the two paragraphs beginning 'Here are the rules governing implicit creation of a __hash__ ( ) method ' .
"I am trying to get the To Russia With Love tutoial from the Stem project working . I have tweaked it a bit from the original to get it working with python 3.4 and I am also using pysocks instead of socksipy . I started with urllib instead of urllib3 and I had the same issue . Currently I am getting : I have had similar code work outside of tor . I can connect my tor browser to this site and I can browse to it with no problems . I have tried changing the port numbers , but this is the one that is set up in Tor 's proxy settings . One thought I had is that this may be a timing issue . Is it possible that the code is not waiting long enough to the site to respond ? Any help in getting this working would be greatly appreciated ."
While processing a file with pdfminer ( pdf2txt.py ) I received empty output : Can anybody say what wrong with this file and what I can do to get data from it ? Here 's dumppdf.py docs/homericaeast.pdf output :
"So far I have done this . I am stuck on recursion . I have no idea how to move forward , joining and reversing etc.EXPECTED OUTPUT : `` apmnolkjihgfedcbq ''"
"I am trying to use Haystack and Whoosh with my Django app . I followed the steps on Haystack docs , but i am getting this error when i do a searchsearch_indexes.py -I could n't find help on this error anywhere , what am i doing wrong ? Stacktrace - Python 2.7.6Django 1.9.1Haystack 2.4.1Whoosh 2.7.0"
"I have a very similar setup to the person in this question : How do I update a list data_relation in Python Evewith a users resource and a friends sub-resource of list type.However , when I try to add a new value to the friends list , the other values in the list get replaced by the new value . How do I add a single value to the list and keep the old values ? I have also tried PUT , but it replaces the list with the new value as well.EDIT : I just tried using POST.AND"
"I am using py.test to test some modules of mine that contains quite a bit of stdlib logging . I would of course like for the logging to log to stdout , which is captured by py.test , so that I will get all relevant logging messages if a test fails . The problem with this is that the logging module ends up trying to log messages to the 'stdout'-object provided by py.test after this object has been discarded by py.test . That is , I get : If I turn off capturing with -s , I do n't have any problems , but of course that makes the test output unreadable with irrelevant logging.Can anyone tell me the proper way to integrate stdlib logging with py.test ? ( I tried looking at this , where it looks like it should just work without issues , so it did n't help me much )"
"I 'm having trouble working with Python3 's tempfile library in general . I need to write a file in a temporary directory , and make sure it 's there . The third party software tool I use sometimes fails so I ca n't just open the file , I need to verify it 's there first using a 'while loop ' or other method before just opening it . So I need to search the tmp_dir ( using os.listdir ( ) or equivalent ) .Specific help/solution and general help would be appreciated in comments . Thank you . Small sample code :"
"I have a tiny class that extends a namedtuple , but the __dict__ property of its instances is always returning empty.p2 has the attributes , but its __dict__ is empty . They are listed correctly with dir ( ) , though , which is strange . Note this work correctly when SubPoint extends a vanilla class.What is happening , and how do I list the attributes in my subclass instance ?"
"I have a single pointand I wish to rotate that point around the origin in increments of d degrees to get N = 360/d points . For example I imagine the following function.The shape of points should be ( 2 , N ) I can do the code in a loop using a new rotation matrix for each rotation and then concatenating the results but I was hoping for some kind of vectorized solution ."
"I 'm trying to get a list of the CSV files in a directory with python . This is really easy within unix : And , predictably , I get a list of the files that end with .csv in my directory . However , when I attempt the Python equivalent using the Subprocess module : Can somebody please explain what 's going on ? Edit : Adding shell = True removes the error , but instead of getting a list of just CSV files , I get a list of all the files in the directory ."
"I want to learn Data Science and so have used some really popular Python modules likes Pandas , Matplotlib , Numpy , etc . So I clean installed Anaconda and am now using it as my default Python interpreter and also using Conda for installing packages and making virtual environments . I use VS Code as my daily text editor . But I have run into some issues when using the integrated Git terminal in VS Code with the Anaconda Python interpreter.There are a couple of issues that I am facing . One of the first issues that I see is when I am using CMD to run Python . If I type and enter python in cmd , the Python interpreter provided by anaconda comes up . But I also get a warning : Warning : This Python interpreter is in a conda environment , but the environment has not been activated . Libraries may fail to load . To activate this environment please see https : //conda.io/activationI did n't expect to get this output . Anyway , there 's another problem in VS code . But first I would like to mention that I have checked `` Add to PATH '' when installing Anaconda so no issues there . Now , when I open a new Terminal in VS Code , automatically C : /Users/User/Anaconda3/Scripts/activate is run and then conda activate base is run . But when conda activate base is run , automatically , as mentioned , I get a CommandNotFoundError . It states Your shell has not been properly configured to use 'conda activate'.If using 'conda activate ' from a batch script , change yourinvocation to 'CALL conda.bat activate'And then I am told to initialize my shell , so I did conda init bash but still no luck . And this brings me to talk about .bash_profile . I think it has to do something with this bash profile . Anyway , this is what is in my bash profileJust a summary of the problem : Unexpected warning in CMD when running Anaconda Python interpreterAutomatically run Anaconda Scripts and conda activate base when opening new Terminal in VS CodeConda init bash not helpingP.S I have tried using conda activate [ env_name ] in CMD and also in Git Bash and they work without any issues . In other words , Anaconda and Conda work perfectly outside of VS Code terminal ."
"I am currently learning Python . Since I am a big fan of OO ( object-oriented ) programming , obviously it 's not hard to apply it in Python . But when I tried it , it seems very different to C # .As you can see below , I am trying to create a character class , with three attributes Id , Hp , and Mana . The score is calculated by adding up Hp and Mana and then times 10.As you can see , after defining MyChar where id=10 hp=100 mana=100 , I was expecting MyChar.Score is ( 100+100 ) *10 , which is 2000 , but weirdly , it says : bound method Character.Score of < __main__.Character object at 0x0000021B17DD1F60 > as the result of print ( MyChar.Score ) .How can I fix this problem ? Here is my code :"
"I 've got the following snippet of codeWhich of course works just fine , but I wonder sometimes if there is a `` better '' way to write that construct where you only act on non-blank lines . The challenge is this should use the iterative nature of the for the 'fd ' read and be able to handle files in the 100+ MB range.UPDATE - In your haste to get points for this question you 're ignoring an import part , which is memory usage . For instance the expression : Is going to buffer the whole file into memory , not to mention performing a strip ( ) action twice . Which will work for small files , but fails when you 've got 100+MB of data ( or once in a while a 100GB ) .Part of the challenge is the following works , but is soup to read : Look for magic folks ! FINAL UPDATE : PEP-289 is very useful for my own better understanding of the difference between [ ] and ( ) with iterators involved ."
"Is it possible to perform min/max in-place assignment with NumPy multi-dimensional arrays without an extra copy ? Say , a and b are two 2D numpy arrays and I would like to have a [ i , j ] = min ( a [ i , j ] , b [ i , j ] ) for all i and j.One way to do this is : But according to the documentation , numpy.minimum creates and returns a new array : numpy.minimum ( x1 , x2 [ , out ] ) Element-wise minimum of array elements . Compare two arrays and returns a new array containing the element-wise minima.So in the code above , it will create a new temporary array ( min of a and b ) , then assign it to a and dispose it , right ? Is there any way to do something like a.min_with ( b ) so that the min-result is assigned back to a in-place ?"
"In my GAE app I have the following handler in app.yaml : So a call to /lang/strings.js will actually map to the js_lang.py request handler which populates the response as application/javascript . I want this response to be cached in the browser so that the request handler only gets called once in a while ( for example when I `` invalidate '' the cache by importing /lang/strings.js ? v=xxxx when I deploy a new version of the app.For normal static content , there is the default_expiration element , which is very handy . And results in http response headers like this : Ok , the question : is there an easy way for me to return headers such as this , without having to explicitly set them ? Alternatively , is there a code snippet out there that accepts a few basic parameters such as `` days '' and produces the expected http-headers ? Edit 12 April 2011I solved this very by simply setting the two headers Expires and Cache-Control like this :"
"I 'm new to Python . While reading , please mention any other suggestions regarding ways to improve my Python code.Question : How do I generate a 8xN dimensional array in Python containing random numbers ? The constraint is that each column of this array must contain 8 draws without replacement from the integer set [ 1,8 ] . More specifically , when N = 10 , I want something like this.To do this I use the following approach : In practice N will be ~1e7 . The algorithm above is O ( n ) in time and it takes roughly .38 secs when N=1e3 . The time therefore when N = 1e7 is ~1hr ( i.e . 3800 secs ) . There has to be a much more efficient way . Timing the function"
It 's quite easy to run into import problems with Django and apps which interact with each other . My question is simple : What is the accepted process for minimizing circular imports or has anyone come up with an accepted coding standard to reduce these which they are willing to share . ? I 'm looking for good principles that can be standardized on.ModelsvsViews : vs ( which tends to cause the issues..The problem with this is that you tend to do a lot of imports all over the place..
"I 've got two systems that need to talk . The systems are setup likeso : System A , running Django ( Python 2.5 ) on Google App Engine ( GAE ) System B , running Django ( Python 2.6 ) on Ubuntu/Linux over Lighttpd ( maybe nginx , later ) System A will periodically make requests ( 'requisitions ' ) of System B using Url Fetch.System B has a Django app setup to listen for these requests with a urls.py with something like : And a corresponding views.py with something like : It would be a valuable addition to security of the system if System B responded to requisitions only from System A.I 'd like to know what options are available for System B to verify that requests have come from System A. I 've considered the following : Check that the IP address is from GAE ( however I do n't know the GAE IP addresses , they may change , and they may be spoofed ) Check that the reverse DNS of the IP is from GAE ( however I do n't know what GAE 's DNS entries are , if they will change , and they may be spoofed ) Use a TLS client certificate from System A - but I do n't know how to do this with GAEDo a challenge/response based on something shared , like a salt , with pycryptoIdeally I want to end up with a views.py with something likeso : Where verify_request_origin ( ) returns true when the request made to System B was from System A on GAE.Thank you and I look forward to hearing your thoughts ."
"In SQLAlchemy , a hybrid attribute is either a property or method applied to an ORM-mapped class , This allows for distinct behaviors at the class and instance levels , thus making it simpler to evaluate SQL statements using the same code , Now in Django , if I have a property on a model , It is my understanding that I can not do the following , Is there an equivalent of SQLAlchemy 's hybrid attribute in Django ? If not , is there perhaps a common workaround ?"
"I noticed an inconsistent behavior in numpy.dot when nans and zeros are involved . Can anybody make sense of it ? Is this a bug ? Is this specific to the dot function ? I 'm using numpy v1.6.1 , 64bit , running on linux ( also tested on v1.6.2 ) . I also tested on v1.8.0 on windows 32bit ( so I ca n't tell if the differences are due to the version or OS or arch ) .Case # 4 seems to be working correctly in v1.6.2 and v1.8.0 , but not case # 2 ... EDIT : @ seberg pointed out this is a blas issue , so here is the info about the blas installation I found by running from numpy.distutils.system_info import get_info ; get_info ( 'blas_opt ' ) : ( I personally do n't know what to make of it )"
"I need to find the unit digit of x1 ^ ( x2 ^ ( x3 ^ ( ... ^ xn ) ) ) from integers passed into the function as a list.For example the input [ 3 , 4 , 2 ] would return 1 because 3 ^ ( 4 ^ 2 ) = 3 ^ 16 = 43046721 the last digit of which is 1 . The function needs to be efficient as possible because obviously trying to calculate 767456 ^ 981242 is not very quick . I have tried a few methods but I think the best way to solve this is using sequences . For example any number ending in a 1 , when raised to a power , will always end in 1 . For 2 , the resulting number will end in either 2 , 4 , 6 or 8 . If a number is raised to a power , the last digit of the resulting number will follow a pattern based on the last digit of the exponent:1 : Sequence is 12 : Sequence is 2 , 4 , 8 , 63 : Sequence is 3 , 9 , 7 , 14 : Sequence is 4 , 65 : Sequence is 56 : Sequence is 67 : Sequence is 7 , 9 , 3 , 18 : Sequence is 8 , 4 , 2 , 69 : Sequence is 9 , 10 : Sequence is 0I think the easiest way to calculate the overall last digit is to work backwards through the list and calculate the last digit of each calculation one at a time until I get back to the start but I am not sure how to do this ? If anyone could help or suggest another method that is equally or more efficient than that would be appreciated . I have this code so far but it does not work for very large numbersEdit : 0 ^ 0 should be assumed to be 1"
"Is there a way to calculate many histograms along an axis of an nD-array ? The method I currently have uses a for loop to iterate over all other axes and calculate a numpy.histogram ( ) for each resulting 1D array : Needless to say this is very slow , however I could n't find a way to solve this using numpy.histogram , numpy.histogram2d or numpy.histogramdd ."
THIS is an updated version of the question providing a handy function pd_read_printed ( str_printed_df ) designed to create a pandas DataFrame out of the string written previously using print ( some_pandas_DataFrame ) : I put it together for own use after I have got here the answers to the following question : I see in Internet often the content of a pandas DataFrame in its printed version like for example : : The question is : How to obtain a variable holding the DataFrame from a string variable in a style like : ? NOW let 's use the provided function to create a DataFrame from df1_as_string : and check if it worked as expected : gives :
"Maybe it is a dumb question , but I do n't understand the error that the function cross_val_score in the code below give me . Perhaps the answer is in the format of X sample , seeing that this is exactly what was shown in the crash message , but I do n't know how to fix . This is a piece of code from my project with some random values.Gives me the error :"
"I have played around a bit and ca n't get saving a plot rendered with seaborn correctly . When using plt.savefig I lose the grid . However , using plt.show and then saving the figure manually works . This happens with eps and png as well . I need to render large amount of plots so this is a problem.Automatic saveManual save"
"I 'm using Python version : 2.7.3.In Python , we use the magic methods __str__ and __unicode__ to define the behavior of str and unicode on our custom classes : The behavior suggests that the return value from __str__ and __unicode__ is coerced to either str or unicode depending on which magic method is run.However , if we do this : Calling str.mro ( ) and unicode.mro ( ) says that both are subclasses of basestring . However , __unicode__ also allows returning of buffer objects , which directly inherits from object and does n't inherit from basestring.So , my question is , what actually happens when str and unicode are called ? What are the return value requirements on __str__ and __unicode__ for use in str and unicode ?"
"Is there a way to loop in while if you start the script with python -c ? This does n't seem to be related to platform or python version ... LinuxWindowsI have tried removing parenthesis in the while statement , but nothing seems to make this run ."
"I have a sequence of numbers in a list and I 'm looking for an elegant solution , preferably list comprehension , to get the individual sequences ( including single values ) . I have solved this small problem but it is not very pythonic.The following list defines an input sequence : The desired output should be :"
"I 've a simplest test case here : Here 's the setup.py script : When I build it and test it as follows , I get : The traceback I got wasEnvironment : Win 7 64 bit , Python 2.7.3 ( default , Aug 15 2012 , 18:18:52 ) [ MSC v.1500 64 bit ( AMD64 ) ] on win32Swig 2.0.7"
"When I open an entry of `` Placerating '' in Admin , and try to save it after making a change to any field , Django admin displays `` This field is required '' above the field `` Pic '' .I created the entry without problem with a form , so I do n't understand why admin would now require this field to be completed ."
"I have the following Python program running in a Docker container . Basically , if the Python process exits gracefully ( ex . when I manually stop the container ) or if the Python process crashes ( while inside some_other_module.do_work ( ) ) then I need to do some cleanup and ping my DB telling it that process has exited . What 's the best way to accomplish this ? I saw one answer where they did a try catch on main ( ) , but that seems a bit odd.My code :"
"I am trying to plot histograms of a couple of series from a dataframe . Series have different maximum values : returns : When I tried to plot histograms I use sharex=False , subplots=True but it looks like sharex property is ignored : I can clearly plot each of them separately , but this is less desirable . Also I would like to know what I am doing wrong.The data I have is too big too be included , but it is easy to create something similar : Now I have : Check the x axis . I want it to be this way ( but using one command with subplots ) ."
"I am looking for some analogue of decltype in C++ . What I am trying to accomplish is the following : So the idea is to use type annotation of another function . The solution I found looks somehow clumsy : Basically , the question is whether there exists something like decltype ( perhaps it should be called `` return_type '' ) or whether it 's planned in further versions . I have also written a small function that illustrates possible use of this functionality : UPD As was suggested by Jim Fasarakis-Hilliard we can also use get_type_hints instead of annotations"
"After seeing a conversation in a forum from many years ago that was never resolved , it caused me to wonder how one would correctly create a tuple that referenced itself . Technically , this is a very bad idea since tuples are supposed to be immutable . How could an immutable object possibly contain itself ? However , this question is not about best practices but is a query regarding what is possible in Python.The previous function was designed to access the C API of Python while keeping internal structures and datatypes in mind . However , the following error is usually generated when running the function . Through unknown processes , it has been possible to create a self-referencing tuple via similar techniques before . Question : How should the function self_reference be modified to consistently work all of the time ? Edit : Here are two different conversations with the interpreter that are somewhat confusing . The code up above appears to be correct if I understand the documentation correctly . However , the conversations down below appear to both conflict with each other and the self_reference function up above.Conversation 1 : Conversation 2 :"
"I am doing some timezone conversions , and I get really weird results . Basically converting between timezones that differ only by whole hours , I still get non-whole results . For example : gives me ( time difference between Bucharest and Berlin is 1 hour , so I should get 19:00 - instead I get 19:16 ) I 'm probably missing something really obvious , but I ca n't figure it out . What am I doing wrong ?"
"I wrote the following decorator to be used in some Django views where I do n't want the user to be logged in ( like register and forgot-password ) : Once I have it , I can easily write : I have written unit tests for the views that are using it , and it is working without problems , but I 'm wondering what would be the best way of unit testing the not_logged_in function alone ?"
"I want to do something like this : but it 's just painful to type these all the time , is there anyway to have alias for this ? I know in C++ it 's possible to have a helper function which return a reference so you can just type : with F as a helper function . Thanks very much !"
"I 'm aware that the sys.exc_info documentation says to take care when dealing with traceback objects , but am still uncertain of how safe or unsafe some cases are . Additionally , the documentation says `` Warning : Do n't do this ! `` , followed immediately by `` Note : Actually , its ok '' , which further confuses me.In any case , the docs and `` Why is there a need to explicitly delete the sys.exc_info ( ) traceback in Python ? '' ( Alex Martelli 's answer ) , seem to imply its only local variables that reference the traceback value assigned to them that cause a problem.This leaves me with a few questions : What , exactly , does `` local variable '' mean in this context ? I 'm struggling for terminology , but : does this mean only variables created in the function , or also variable created by the function parameters ? What about other variables in scope , e.g , globals or self ? How do closures affect the potential circular references of the traceback ? The general thought being : a closure can reference everything its enclosing function can , so a traceback with a reference to a closure can end up referencing quite a bit . I 'm struggling to come up with a more concrete example , but some combination of : an inner function , code that returns sys.exc_info ( ) , with expensive-short-lived-objects in scope somewhere.Feel free to tell me where my conclusions or assumptions are wrong , as I 've reasoned myself into belief and non-belief of my own statements several times as I 've written this : ) .While I 'd like answers to my specific examples , I 'm also asking for general advice , knowledge , or war stories on how to safely deal with tracebacks in more esoteric situations ( e.g , you have to run a loop and want to accumulate any raised exceptions , you have to spawn a new thread and need to report any of its raised exceptions , you have to create closures and callbacks and have to communicate back raised exceptions , etc ) .Example 1 : an inner function that does error handlingDoes the Handled ( ) closure cause any raised exception to reference error_queue and result in a circular reference because error_queue also contains the traceback ? Is removing the traceback from error_queue ( i.e. , calling .get ( ) ) enough to eliminate the circular-reference ? Example 2 : a long lived object in scope of exc_info , or returning exc_infoDoes the raised exception of AlphaSub ( ) have a reference to expensive_object , and , because expensive_object is cached , the traceback never goes away ? If so , how does one break such a cycle ? Alternatively , exc_info contains the Alpha stack frame , and the Alpha stack frame contains the reference to exc_info , resulting in a circular reference . If so , how does one break such a cycle ?"
With QTableView set as editable using QAbstractTableModel 's flag ( ) method : double-clicking the QTableView 's item puts this item into the editing mode . By default the pre-existing string disappears from the field and an entire item is blank . I wonder if this behavior can be avoided or overridden ? Here is the QTableView field before the user double-clicks it : And here is how it looks on double-click : EDITED WORKING CODE ( Many thanks to M4rtini ) :
"I am using python to go through a file and remove any comments . A comment is defined as a hash and anything to the right of it as long as the hash is n't inside double quotes . I currently have a solution , but it seems sub-optimal : Is there a way to find the first hash not within quotes without for loops ( i.e . through regular expressions ? ) Examples : Edit : Here is a pure regex solution created by user2357112 . I tested it , and it works great : See his reply for more details on how this regex works.Edit2 : Here 's a version of user2357112 's code that I modified to account for escape characters ( \ '' ) . This code also eliminates the 'if ' by including a check for end of string ( $ ) :"
"I got a list of objects which look like strings , but are not real strings ( think about mmap'ed files ) . Like this : What i want is x to be directly indexable like it was a big string , i.e . : ( Of course I do n't want to do `` '' .join ( x ) which would merge all strings , because reading a string is too expensive in my case . Remember it 's mmap'ed files . ) .This is easy if you iterate over the entire list , but it seems to be O ( n ) . So I 've implemented __getitem__ more efficiently by creating such a list : Therefore I can do a binary search in __getitem__ to quickly find the tuple with the right data and then indexing its string . This works quite well.I see how to implement __setitem__ , but it seems so boring , I 'm wondering if there 's not something that already does that . To be more precise , this is how the data structure should honor __setitem__ : I 'd take any idea about such a data structure implementation , name or any hint ."
"I 'm using sympy v1.0 in a Jupyter Notebook . I 'm having trouble getting expression to simplify how I 'd like . Here 's a toy example ; it does the same thing my more complicated expressions do ... gives me ... But I would prefer ... How can I get sympy to group things in this way ? Ive tried some of the other simplify functions , but they all give me the same result . Or am I missing something else ?"
"I learned how to create a playlist in a previous question , but now I ca n't figure out how to add tracks to it . Right now I have : My tracks are in a list of tuples tracks , where the first element of the tuple is a score and the second is the actual track object . generatePlaylists is an iterator which splits all library tracks into 10 lists . The above code runs without error , but in iTunes the playlists are empty ."
"I define a class in a given python module . From a few other python files I will create instances of said class . The instances register themselves at object creation , ie during __init__ ( ) , in a singleton registry object . From a third type of python file I would like to access the registry , look at the objects therein and be able to figure out in which files these objects were created beforehand.A code sample might look as follows : Python module file : '/Users/myself/code/myobjectmodule.py ' : singleton decorator according to http : //www.python.org/dev/peps/pep-0318/ # examples Instance creation python files : '/Users/myself/code/instance_creation_python_file.py ' : Third python file : '/Users/myself/code/registry_access.py ' : Now , I would like to have a method foo.get_file_of_object_creation ( ) .How can I implement this method ? Edit : The reason for this approach is the following scenario:1 . A framework defines a set of objects that shall specify data sources and contain the data after loading ( MyObject ) .2 . Apps making use of this framework shall specify these objects and make use of them . Each app is saved in a .py file or a folder that also specifies the name of the app via its name.3 . An engine provides functionality for all apps , but needs to know , for some features , which of the objects originate from which app / file ."
"Since i 'm planning on working with large arrays , is there a more numpy-like version of this ? I feel like the answer is right under my nose and I 'm thinking it has something to do with reduce , but numpy 's version only works with ufuncs , not functions . Even a hint would be greatly appreciated.Thanks in advance ."
"I 'm working with a Dyson fan and am trying to write a function that increases the speed of the fan . The enum object FanSpeed has the following members . When I execute [ print ( i ) for i in FanSpeed ] I get : So the first enum object ( above ) has the name FAN_SPEED_1 ( FanSpeed.FAN_SPEED_1.name ) and has a value of `` 0001 '' ( FanSpeed.FAN_SPEED_1.value ) , and so on.The function to set speed ( to 5 in this example is ) : I ca n't figure out how to write a function that gets the current speed and then sets it to one level higher . I 've tried:1/ Treating the enum object as a a string and replacing the last character with a number ( character type ) that 's higher . This approach does n't work for enum objects though.2/ Looking for some kind of increment or next function but those do n't seem to exist for enum objects ."
"I have been running some code , a part of which loads in a large 1D numpy array from a binary file , and then alters the array using the numpy.where ( ) method . Here is an example of the operations performed in the code : I have run this many times ( on a free machine , i.e . no other inhibiting CPU or memory usage ) with no issue . But recently I have noticed that sometimes the code hangs for an extended period of time , making the runtime an order of magnitude longer . On these occasions I have been monitoring % CPU and memory usage ( using gnome system monitor ) , and found that python 's CPU usage drops to 0 % . Using basic prints in between the above operations to debug , it seems to be arbitrary as to which operation causes the pausing ( i.e . open ( ) , np.fromfile ( ) , np.where ( ) have each separately caused a hang on a random run ) . It is as if I am being throttled randomly , because on other runs there are no hangs.I have considered things like garbage collection or this question , but I can not see any obvious relation to my problem ( for example keystrokes have no effect ) .Further notes : the binary file is 32GB , the machine ( running Linux ) has 256GB memory . I am running this code remotely , via an ssh session.EDIT : This may be incidental , but I have noticed that there are no hang ups if I run the code after the machine has just been rebooted . It seems they begin to happen after a couple of runs , or at least other usage of the system ."
"Very often I use the following construction : Sometimes , instread of ' ? ' I use 0 or None . I do not like this construction . It is too verbose . Is there a shorter way to do what I do ( just in one line ) . Something like ."
"I am using Django 1.6 and model inheritance in Django . What I wan na do is , hooking new class extending . It would be done in Python like , When this part of code is initialized , custom __new__ method will be invoked.How can I do that in Django with model inheritance . When you try to do this with Django Models , an error such that , is given : Thank you ."
"I would really like to run my Win7 ipython interactively inside a proper terminal and shell provided by cygwin ( mintty or rxvt would be great ) .I have had some success with python , but IPython is not really cooperating . If we run this from rxvt bash prompt : I end up with a fairly successful python interactive session . But vanilla python is just not as wonderful as IPython . So then I try this : and the program just blocks the bash prompt , and seems to spawn an entirely separate process ( which does not produce any windows , or show signs of stdin/stdout ) .If instead I use this : it is exciting at first , but you quickly realize that ipython is not properly engaging with the terminal . There is no readline support , poor cursor control , stdin seems to handle typical python , but there are no [ out ] prompts ( although `` prints '' do spit out text ) , simple things like backspace/enter/tab seem completely broken ( the cursor is very defiant ) , there is nothing like an ncurse buffer being maintained ( you can just type over the ipy command prompts ) .Once I have given up on the session , exiting becomes another problem . I can return to the bash prompt , but I have no standard input . It turns out that there is always a python.exe process hanging that must be killed from the Windows side ( and it doesnt release stdin until it dies ) .Is there a quick fix or alternate method to run Ipython in this manner ? I can do most of my development using the cygwin binaries , but being able to run win32 binaries interactive would help tremendously when debugging/testing win32 specific python libraries.P.S . : : : : I really need tab-completion and clean output . I am trying to piece my way through a bunch of COM interfaces , and the only way I can get anywhere is with an interactive ipy session.P.S . : : : : I am using a 64-bit Cygwin and a 32-bit win32 python . Could this be simple mismatch ?"
"I 'm trying to get buildout to use a specific , forked & tweaked package from my github account , however it seems to be completely ignoring the reference and instead opting for the standard PyPi module.Here 's my buildout config : I 'm using the latest zc.buildout from pypi , version 1.5.2.I 've tried with both http and https for the link ( because of the recent github change ) . The link is active and works directly , so I 'm guessing it 's my configuration . Am I missing something ?"
"I implemented the Madhava–Leibniz series to calculate pi in Python , and then in Cython to improve the speed . The Python version : The Cython version : When I stopped the Python version it had correctly calculated pi to 3.141592 . The Cython version eventually ended up at 3.141597 with some more digits that I do n't remember ( my terminal crashed ) but were incorrect . Why are the Cython version 's calculations incorrect ?"
"In Mercurial , many of the extensions wrap their help/syntax string in a call to an underscore function , like so : This confuses me , because it does not seem necessary ( the Writing Extensions instructions do n't mention it ) and there does n't seem to be a _ defined in the class , so I 'm wondering if this is some special syntax that I do n't understand , perhaps another way to say lambda , or maybe the identity function ? Additionally I 'm wondering what the benefit of this methodology ( whatever it is ) is over just the raw string like the documentation suggests.Nothing I 've seen in the Python documentation mentions such a function , so I 'm not sure if this is really a Python question , or a Mercurial question.Here are two examples that use this structure ( look at the cmdtable dictionary near the bottom of the file ) https : //www.mercurial-scm.org/repo/hg/file/42408cd43f55/hgext/mq.pyhttps : //www.mercurial-scm.org/repo/hg/file/42408cd43f55/hgext/graphlog.py"
I want to create a finalDic which contains common keys and sum of their values First find common keysThen Sum and sort by their valuesI 've tried this and not even close what i wantThanks
I have a dataframe where the index and column are both numbers -- i.e.I want the values of the dataframe to be a function of the index and column -- so for instance box [ 2 ] [ 2 ] should equal 4.Currently I have it in but I feel like there should be a more elegant solution . Is there a way to do this purely with slicing/dataframe assignment methods ?
"How do you write a Python code to check if the operation ∗ on the set { 0,1 , .. , n−1 } defined by a Cayley table is associative or not.My attempted code is :"
"I 'm looking to find out how to use Python to get rid of needless newlines in text like what you get from Project Gutenberg , where their plain-text files are formatted with newlines every 70 characters or so . In Tcl , I could do a simple string map , like this : This would keep paragraphs separated by two newlines ( or a newline and a tab ) separate , but run together the lines that ended with a single newline ( substituting a space ) , and drop superfluous CR 's . Since Python does n't have string map , I have n't yet been able to find out the most efficient way to dump all the needless newlines , although I 'm pretty sure it 's not just to search for each newline in order and replace it with a space . I could just evaluate the Tcl expression in Python , if all else fails , but I 'd like to find out the best Pythonic way to do the same thing . Can some Python connoisseur here help me out ?"
"I want to change a python function to return two values . How do I achieve that without affecting any of the previous function calls which only expect one return value ? For eg.Original Definition : sum = foo ( ) Ne Definition : sum , diff = foo ( ) I want to do this in a way that the previous call to foo also remains valid ? Is this possible ?"
Is there a C # error exception for a value error ? the equivalent of the python `` value error '' below ?
"I have a Flask App in which my plots are created using Bokeh in the controller python code with below commands : and I pass `` script '' & `` div '' elements to my HTML page using : I want to add a interactive slider bar on top of my plot . Based on the Bokeh website with the following command , I should be able to do it.So my first question is , how can I put slider information to the components function to generate correct `` script '' and `` div '' elements that I could pass it to my HTML file ? My second question is : Having a value on the slider , how can I post it back to my controller to update my plots and send new `` div '' and `` script '' elements to the HTML file to update my plots ? I really appreciate if you could explain necessary steps to achieve this solution ."
"I am trying to implement the following equation using scipy 's sparse package : where x & y are a nxm csc_matrix . Basically I 'm trying to multiply each col of x by each col of y and sum the resulting nxn matrices together . I then want to make all non-zero elements 1.This is my current implementation : This implementation has the following problems : Memory usage seems to explode . As I understand it , memory should only increase as c becomes less sparse , but I am seeing that the loop starts eating up > 20GB of memory with a n=10,000 , m=100,000 ( each row of x & y only has around 60 non-zero elements ) .I 'm using a python loop which is not very efficient.My question : Is there a better way to do this ? Controlling memory usage is my first concern , but it would be great to make it faster ! Thank you !"
"I 'm looking for ways to speed up ( or replace ) my algorithm for grouping data.I have a list of numpy arrays . I want to generate a new numpy array , such that each element of this array is the same for each index where the original arrays are the same as well . ( And different where this is not the case . ) This sounds kind of awkward , so have an example : Note that elements I marked ( indices 0 and 4 ) of the expected outcome have the same value ( 0 ) because the original two arrays were also the same ( namely 10 and 21 ) . Similar for elements with indices 3 and 5 ( 3 ) .The algorithm has to deal with an arbitrary number of ( equally-size ) input arrays , and also return , for each resulting number , what values of the original arrays they correspond to . ( So for this example , `` 3 '' refers to ( 11 , 22 ) . ) Here is my current algorithm : Note that the expression value_array [ matching ] == needed_value is evaluated many times for each individual index , which is where the slowness comes from.I 'm not sure if my algorithm can be sped up much more , but I 'm also not sure if it 's the optimal algorithm to begin with . Is there a better way of doing this ?"
"I have following setup : I 'm using Django 1.8 and Python 2.7.What I want to achieve is to be able to use # option 1 but all methods in BaseInfoQuerySet should use modified get_queryset ( ) from BUManager . BaseInfoQuerySet is used as base class on multiple querysets for other models , so I do n't want to get rid of it and use only models.Manager . And I also want to have ability to chain QuerySet filters ( for example BU.objects.public ( ) .not_grouped ( ) ) .The way I see it the solution would be to modify somehow method as_manager ( ) to return modified Manager with overriden get_queryset method ."
"I have two 1D arrays , one for measured data and the other one for location . For example , the measured data could be temperature and the other array the heights of the measurement : As you can see , the height of the measurements is not regularly spaced.I want to compute the mean temperature in regularly spaced height intervals . This is some kind of moving average , but the window size is variable , because the data points inside the interval of interest is not always the same.This could be done with a for loop in the following way : I do n't like this approach that much and I was wondering if there would be a more `` numpy-style '' solution ."
"So , can I somehow stop django rendering specific template variables ? Background is , that I wanted to try vuejs in a django app , which kind of worked.Problem is , both share the same syntax for variables.So in vuejs you declare them like And djangos template engine interprete it as a django template variable and try to render it . Because it doenst exist , it disappear and vuejs isnt working anymore ."
I 'm trying to do some image processing with matplotlib . However the y-axis is decreasing bottom up . I want it to be increasing bottom up without flipping the image upside downI have the following codeit produces the following image : The images can be obtained thereI tried plt.gca ( ) .invert_yaxis ( ) without successWhat shall I do
"I am using sklearn to carry out recursive feature elimination with cross-validation , using the RFECV module . RFE involves repeatedly training an estimator on the full set of features , then removing the least informative features , until converging on the optimal number of features . In order to obtain optimal performance by the estimator , I want to select the best hyperparameters for the estimator for each number of features ( edited for clarity ) . The estimator is a linear SVM so I am only looking into the C parameter.Initially , my code was as follows . However , this just did one grid search for C at the beginning , and then used the same C for each iteration.The documentation for RFECV gives the parameter `` estimator_params : Parameters for the external estimator . Useful for doing grid searches when an RFE object is passed as an argument to , e.g. , a sklearn.grid_search.GridSearchCV object . `` Therefore I want to try to pass my object 'rfecv ' to the grid search object , as follows : But this returns the error : So my question is : how can I pass the rfe object to the grid search in order to do cross-validation for each iteration of recursive feature elimination ? Thanks"
"I have two sql tables . Table 1 ( id , name ) and Table 2 with ( id , name , table1_id ) If the result of the sql query is Then everything is OK and I have [ ( < Table1 > , < Table2 > ) ] for table1_table2_tuple_list But If the result of the sql query is : ( also all other code is for this result ) Then instead of receiving [ ( < Table1 > , None ) ] I receive [ ( None , None ) ] If I change my code a little bit : Then I receive : But in this case I 'm not even sure if this is correct because I match two columns from the sql to model Table2 that has three columns . Not sure at all why this is working , but everything seems on place . Still is not what I want , because I do n't want to return to this query and to specify again and again new columns if there are such for Table2What I need is a way to select from two tables with pure sql , and to match the result to my models . Also I want to have result even in the cases when one of the tables does n't have value in it 's result columns . Hope I am clear . My goal is to receive when the query is for all colums ( SELECT t1 . * , t2 . * ) and there is LEFT JOIN with null added ."
"I want to use the multiprocessing.Manager ( ) object so I can asynchronously send information from a worker to the manager to send information to a server . What I have is roughly 10 instances writing PDFs to disk . I then wanted to use the manager object in the multiprocessing package to send that data to my S3 bucket because I do not want to hold up the local content generation.So I was wondering if I create a custom manager object , is this the proper way to do this ? Will each process submitted to the manager object get queued ? or if I call multiple uploads , will the manager drop some of the calls ? Below is a sample code of what I am thinking of doing :"
"I 've the following adjacency matrix : Which can be drawn like that : My goal is to identify the connected graph ABC and DEFG . It 's seems that Depth-First Search algorithm is what I need and that Scipy implemented it . So here is my code : But I do n't get the result : what 's that array ( [ -9999 , 0 , 1 , -9999 , -9999 , -9999 , -9999 ] ) ? Also , in the documentation , they talk about a sparse matrix not about an adjacency one . But an adjacency matrix seems to be a sparse matrix by definition so it 's not clear for me ."
"I have a list of dictionaries list_of_dict , a set of keys set_of_keys and another dictionary dict_to_compare . I need to filter the list of dicts if the values of any two of the three possible keys matches the values from dict_to_compare.Input : Output : All elements in list_of_dicts have same keys.dict_to_compare also have same keys as elements of list_of_dicts.Multiple elements from list_of_dicts can be matched.Values against any combination of two keys should be matched not all three.I tried doing this by explicitly specifying bunch of if elif conditions . But the problem is set of keys is really huge . Is there a better way to solve this ? Thanks"
"I have a script I wrote in python and it works fine but I was curious to see if I could speed it up . It basically is recursive script.If I run it within normal python 2.7 , it takes about 30 seconds . When I run the same thing using pypy than I get the following error : I 'm not sure what pypy is doing differently because I 'm not modifying the script.Can anyone help me understand what is going on ? Update : ok I figured it out . Increasing the limit helped but I think I was running the wrong file . I found a file under the bin directory called py.py and was using that . I 'm not sure what the file does but its slower than normal python . I had to search and find 'pypy-c ' seems to work now ."
"What is a pythonic way to remap each dictionary key in a list of identically-keyed dictionaries to different key names ? E.g. , must transform into ( I need to do the transformation in order to match an output specification for an API I 'm working on . )"
"On a fresh 64-bit install of Anconda2 ( 4.3.1 ) with Python 2 ( 2.7.13 ) on Windows 10 , I get the following error : I realise there are a few postings about SSL problems with Python in Anaconda that could be related : I 've already tried running things like conda update openssl , I do n't have certifi installed , and pip install ssl does n't work ( since ssl is bundled with python now , I guess ) .Does anyone have any advice ? I 'm trying to get BioPython to work , in case there 's some sort of BioPython-specific fix.Thanks !"
Imagine I have these python lists : Is there a direct or at least a simple way to produce the following list of dictionaries ?
"I have made a custom form for my django app which allows users to register themselves for the website.This is the form for my django app : This is the relevant part of the view : When I press submit , the user is created , however no password is set for the user"
"I 'm looking for the best way to rename my header using dictreader / dictwriter to add to my other steps already done.This is what I am trying to do to the Source data example below.Remove the first 2 linesReorder the columns ( header & data ) to 2 , 1 , 3 vs the source fileRename the header to ASXCode , CompanyName , GISCWhen I 'm atIf I use 'reader = csv.reader.inf ' the first lines are removed and columns reordered but as expected no header renameAlternately when I run the dictreader line 'reader = csv.DictReader ( inf , fieldnames= ( 'ASXCode ' , 'CompanyName ' , 'GICS ' ) ) ' I receive the error 'dict contains fields not in fieldnames : ' and shows the first row of data rather than the header.I 'm a bit stuck on how I get around this so any tips appreciated.Source Data exampleMy Code"
"Short summary : How do I quickly calculate the finite convolution of two arrays ? Problem descriptionI am trying to obtain the finite convolution of two functions f ( x ) , g ( x ) defined byTo achieve this , I have taken discrete samples of the functions and turned them into arrays of length steps : I then tried to calculate the convolution using the scipy.signal.convolve function . This function gives the same results as the algorithm conv suggested here . However , the results differ considerably from analytical solutions . Modifying the algorithm conv to use the trapezoidal rule gives the desired results.To illustrate this , I letthe results are : Here Riemann represents a simple Riemann sum , trapezoidal is a modified version of the Riemann algorithm to use the trapezoidal rule , scipy.signal.convolve is the scipy function and analytical is the analytical convolution.Now let g ( x ) = x^2 * exp ( -x ) and the results become : Here 'ratio ' is the ratio of the values obtained from scipy to the analytical values . The above demonstrates that the problem can not be solved by renormalising the integral.The questionIs it possible to use the speed of scipy but retain the better results of a trapezoidal rule or do I have to write a C extension to achieve the desired results ? An exampleJust copy and paste the code below to see the problem I am encountering . The two results can be brought to closer agreement by increasing the steps variable . I believe that the problem is due to artefacts from right hand Riemann sums because the integral is overestimated when it is increasing and approaches the analytical solution again as it is decreasing . EDIT : I have now included the original algorithm 2 as a comparison which gives the same results as the scipy.signal.convolve function.Thank you for your time !"
"I have a 30MB .txt file , with one line of data ( 30 Million Digit Number ) Unfortunately , every method I 've tried ( mmap.read ( ) , readline ( ) , allocating 1GB of RAM , for loops ) takes 45+ minutes to completely read the file.Every method I found on the internet seems to work on the fact that each line is small , therefore the memory consumption is only as big as the biggest line in the file . Here 's the code I 've been using.Other than splitting the number from one line to various lines ; which I 'd rather not do , is there a cleaner method which wo n't require the better part of an hour ? By the way , I do n't necessarily have to use text files.I have : Windows 8.1 64-Bit , 16GB RAM , Python 3.5.1"
"Recently I 've been struggling with the following problem : Given an array of integers , find a minimal ( shortest length ) subarray that sums to at least k.Obviously this can easily be done in O ( n^2 ) . I was able to write an algorithm that solves it in linear time for natural numbers , but I ca n't figure it out for integers.My latest attempt was this : For example : However , it fails for : where the correct result is ( 1 , 2 ) but the algorithm does n't find it.Can this at all be done in linear time ( with preferably constant space complexity ) ?"
"I have this simple code in Python : Then , I run it this way : python path/crawler.py 10From my understanding , it should loop 10 times and stop , right ? Why it does n't ?"
"I have a model that looks like this : And a manager that looks like this : And a view that looks like this : The idea being that I can call Item.objects.published ( ) and get a queryset of all published Items.The problem is that Django seems to be executing the datetime.now ( ) call in the manager when the server is started and then caching that value . So if today is May 24th , and I created an Item with a publish date of May 23rd , and started the server on May 22nd , that May 23rd item will not show up in the ItemArchive view . As soon as I restart Apache , the May 23rd item shows up in the view correctly.How can I force Django to execute datetime.now ( ) every time the manager is called ?"
"When using setuptools/distutils to build C libraries in Pythonthe *.so/*.pyd files are placed in build/lib.win32-2.7 ( or equivalent ) . I 'd like to test these files in my test suite , but I 'd rather not hard code the build/lib* path . Does anyone know how to pull this path from distutils so I can sys.path.append ( build_path ) - or is there an even better way to get hold of these files ? ( without having installed them first )"
"Trying to run this line on my computer results in a string formatting error : ValueError : unsupported format character ' , ' ( 0x2c ) at index 36It seems to be concerning the , but I have checked and all the parenthesis are properly nested ( none enclosing an errant , )"
"I 'm trying to create a bot and this bot have to click some elements that does n't recognize the mouse click but recognize the touch , i searched a bit on the web and i found a way for simulate touch events.I wrote thisand not error are raised but it does n't work , nothing change on the screen . According to the docs https : //seleniumhq.github.io/selenium/docs/api/java/org/openqa/selenium/interactions/touch/TouchActions.htmlthe tap methods simulates mouse clicks and not touchscreen events , so i was wondering if there is a way for simulate touchscreen events on selenium , or this is the correct way and i 'm doing it wrong.I tried too by writing touchactions.tap ( element ) .perform ( ) instead of touchactions.tap ( element ) but it raised this errorand do n't understand why.I 'm using gekodriver , python 3 and windows 10"
I am trying to develop a code to identify filled circle between number of empty circles.I have already identified each circle in center ordinates.How to detect which circle is empty & Which circle is filled ? I have already develop this codeThese are my input & output images for above code..Thanks in advance..
"I am trying to read a datatable from using db2 . It seems , however , that decimals are simply ignored and floats are somehow multiplied by 100 . For instance 100.50 becomes 10050.0 when read into a pandas dataframe . BTW I am from Norway , so decimals are denoted by a , rather than a .. I dont know if this matters.I use the following SQL ( in WinSQL Lite ) : , which gives : I know this is correct , so the issue is not the data.In Spyder , using Python , I have What is happening here , and how can I fix it ? The column SAVINGS_AMOUNT is DECIMAL with Size=9 and Scale=2 . Surely the issue is somehow related to this . I guess I have to set a parameter in pd.read_sql to interpret the column.I know I can simply divide by 100 to correct this . But I dont want to do that . I want to read the correct numbers.Also I 'm using Windows ."
"I was trying to read a changing file in Python , where a script can process newly appended lines . I have the script below which prints out the lines in a file and does not terminate.Where 'tmp.txt ' consists of some lines , e.g . : If I appended to the 'tmp.txt ' file , such as using : The script will print out the new line in if the script is run with Python 3 , but not with Python 2 . Is there an equivalent in Python 2 ? And what 's different between the two versions of Python in this case ?"
"I 'm asking this in the context of Google Dataflow , but also generally . Using PyTorch , I can reference a local directory containing multiple files that comprise a pretrained model . I happen to be working with a Roberta model , but the interface is the same for others.However , my pretrained model is stored in a GCS bucket . Let 's call it gs : //my-bucket/roberta/ . In the context of loading this model in Google Dataflow , I 'm trying to remain stateless and avoid persisting to disk , so my preference would be to get this model straight from GCS . As I understand it , the PyTorch general interface method from_pretrained ( ) can take the string representation of a local dir OR a URL . However , I ca n't seem to load the model from a GCS URL.If I try to use the public https URL of the directory blob , it will also fail , although that is likely due to lack of authentication since the credentials referenced in the python environment that can create clients do n't translate to public requests to https : //storage.googleapisI understand that GCS does n't actually have subdirectories and it 's actually just being a flat namespace under the bucket name . However , it seems like I 'm blocked by the necessity of authentication and a PyTorch not speaking gs : //.I can get around this by persisting the files locally first.But this seems like such a hack , and I keep thinking I must be missing something . Surely there 's a way to stay stateless and not have to rely on disk persistence ! So is there a way to load a pretrained model stored in GCS ? Is there a way to authenticate when doing the public URL request in this context ? Even if there is a way to authenticate , will the non-existence of subdirectories still be an issue ? Thanks for the help ! I 'm also happy to be pointed to any duplicate questions 'cause I sure could n't find any.Edits and ClarificationsMy Python session is already authenticated to GCS , which is why I 'm able to download the blob files locally and then point to that local directory with load_frompretrained ( ) load_frompretrained ( ) requires a directory reference because it needs all the files listed at the top of the question , not just pytorch-model.binTo clarify question # 2 , I was wondering if there 's some way of giving the PyTorch method a request URL that had encrypted credentials embedded or something like that . Kind of a longshot , but I wanted to make sure I had n't missed anything.To clarify question # 3 ( in addition to the comment on one answer below ) , even if there 's a way to embed credentials in the URL that I do n't know about , I still need to reference a directory rather than a single blob , and I do n't know if the GCS subdirectory would be recognized as such because ( as the Google docs state ) subdirectories in GCS are an illusion and they do n't represent a real directory structure . So I think this question is irrelevant or at least blocked by question # 2 , but it 's a thread I chased for a bit so I 'm still curious ."
"I 'm pretty new to Python and programming in general , and I was wondering if it is a good programming practice to write long statements with many logic operators - for example , in a for loop.For example , here 's a function I made that gets all the vowels from a word and returns a list containing those vowels . As you can see , the if statement has gotten very long . Is it considered good programming ? If it 's not , is there a better way to code this function ?"
"I would like to find the last occurrence of a number of characters in a string . str.rfind ( ) will give the index of the last occurrence of a single character in a string , but I need the index of the last occurrence of any of a number of characters . For example if I had a string : I would want a function that returns the index of the last occurence of { , [ , or { similar toWhich would ideally return 8 . What is the best way to do this ? seems clunky and not Pythonic ."
"I 'm trying to convert a Math Formula into PHP code.You can see the formula in the accepted answer here : Applying a Math Formula in a more elegant way ( maybe a recursive call would do the trick ) .I 'm not a professional coder so I 'm trying my best to translate it but my skills are limited and I 've encountered several problems.Let 's start.There 's a vector containing players ' stacks : I think a bidimensional array should do the work here . I 'd add a key to identify each player.Now he wants to create a Matrix of values , I did my researches and found a PEAR package called Math_Matrix , installed it but I 'm wondering how to create that sort of matrix.I 'm worried that I wo n't be able to translate the entire code because he uses advances methods like recursive calls etc.Could you help me ? EDIT : OLD BOUNTY REWARDI tried what you 've suggested but I feel like wasting my time because of my poor-programming skills.I 'VE DECIDED TO OFFER A 50 BOUNTY IF SOMEONE WANTS TO HELP ME BY TRANSLATING THAT FORMULA IN PHP.Note that if you think that translating in Python is easier/more suitable/other , please provide me a way to include the Python script inside a PHP script since I 'm planning to use this formula in a website ."
"I implemented the Last Value Caching ( LVC ) example of ZMQ ( http : //zguide.zeromq.org/php : chapter5 # Last-Value-Caching ) , but ca n't get a 2nd subscriber to register at the backend.The first time a subscriber comes on board , the event [ 0 ] == b'\x01 ' condition is met and the cached value is sent , but the second subscriber ( same topic ) does n't even register ( if backend in events : is never true ) . Everything else works fine . Data gets passed from the publisher to the subscribers ( all ) .What could be the reason for this ? Is the way the backend is connected correct ? Is this pattern only supposed to work with the first subscriber ? UpdateWhen I subscribe the 2nd subscriber to another topic , I get the right behaviour ( i.e . \x01 when subscribing ) . This really seems to work for the first subscriber onlt . Is is a bug in ZeroMQ ? Update 2Here 's a minimal working example that shows that the LVC pattern is not working ( at least not the way it 's implemented here ) .And here 's the broker ( as in the example , but with a bit more verbosity and an integrated publisher ) .Running this code ( 1 x broker.py , 2 x subscriber.py ) shows that the first subscriber registers at the broker as expected ( \x01 and cache lookup ) , but the 2nd subscriber does not get registered the same way . Interestingly , the 2nd subscriber is hooked up to the pub/sub channel , as after a while ( 10 sec ) both subscribers receive data from the publisher.This is very strange . Perhaps some of my libraries are outdated . Here 's what I got : Update 3This behaviour can also be observed in zeromq 4.1.2 and pyzmq-14.7.0 ( with or without libpgm and libsodium installed ) .Update 4Another observation suggests that the first subscriber is somehow handled differently : The first subscriber is the only one unsubscribing in the expected way from the XPUB socket ( backend ) by preceding its subscription topic with \x00 . The other subscribers ( I tried more than 2 ) stayed mute on the backend channel ( although receiving messages ) .Update 5I hope I 'm not going down a rabbit hole , but I 've looked into the czmq bindings and ran my Python example in C. The results are the same , so I guess it 's not a problem with the bindings , but with libzmq.I also verified that the 2nd subscriber is sending a subscribe message and indeed I can see this on the wire : First subscribe:2nd subscribe message with difference ( to above ) marked and explained . The same data is sent in the subscribe frame ."
"when programming a kind of backup application , I did an evaluation of file copying performance on Windows.I have several questions and I wonder about your opinions.Thank you ! Lucas.Questions : Why is the performance so much slower when copying the 10 GiB file compared to the 1 GiB file ? Why is shutil.copyfile so slow ? Why is win32file.CopyFileEx so slow ? Could this be because of the flag win32file.COPY_FILE_RESTARTABLE ? However , it does n't accept the int 1000 as flag ( COPY_FILE_NO_BUFFERING ) , which is recommended for large files : http : //msdn.microsoft.com/en-us/library/aa363852 % 28VS.85 % 29.aspxUsing an empty ProgressRoutine seems to have no impact over using no ProgressRoutine at all.Is there an alternative , better-performing way of copying the files but also getting progress updates ? Results for a 1 GiB and a 10 GiB file : Test Environment : Notes : win32file.CopyFileEx A ( using no ProgressRoutine ) : win32file.CopyFileEx B ( using empty ProgressRoutine ) :"
"This question is more of a curiosity.To change the default fig size to a custom one in matplotlib , one doesafter that , figure appears with chosen size . Now , I 'm finding something new ( never happened/noticed before just now ) : in a Jupyter notebook , when inlining matplotlib as this apparently overwrites the rcParams dictionary restoring the default value for the figure size . Hence in oder to be able to set the size , I have to inline matplotlib before changing the values of the rcParams dictionary . I am on a Mac OS 10.11.6 , matplotlib version 1.5.1 , Python 2.7.10 , Jupyter 4.1 ."
"I 'm training an LSTM cell on batches of sequences that have different lengths . The tf.nn.rnn has the very convenient parameter sequence_length , but after calling it , I do n't know how to pick the output rows corresponding the last time step of each item in the batch.My code is basically as follows : lstm_outputs is a list with the LSTM output at each time step . However , each item in my batch has a different length , and so I would like to create a tensor containing the last LSTM output valid for each item in my batch.If I could use numpy indexing , I would just do something like this : But it turns out that for the time begin tensorflow does n't support it ( I 'm aware of the feature request ) .So , how could I get these values ?"
"I recently had to construct a module that required a tensor to be included . While back propagation worked perfectly using torch.nn.Parameter , it did not show up when printing the net object . Why is n't this parameter included in contrast to other modules like layer ? ( Should n't it behave just like layer ? ) Output :"
"I 'm trying to plot several surfaces , each of a different color , in Plotly for Python . Specifically , a surface shows the predicted reward function for taking an action at different points in phase space . Since I have several possible actions at each point , each is a different surface . I 'd like to color each surface uniquely , but independent of the x , y , or z coordinate.I 've tried to follow answer in R , but I ca n't figure out what I 've done wrong . I always get the same blue color . Since I 'm using PyPlot in other parts of my code , I 'm choosing colors from the default matplotlib tableau.Here 's a basic example with toy data . Produces the following : I should see a blue saddle and an orange paraboloid , but I do n't . Note that even if I change the argument to cmap , I always get the same blue color . Thanks for your help !"
Why is the following simple loop not saving the value of i at the end of the loop ? The above prints : But it should print :
"I ca n't understand why my m2m_changed signal is not triggered.Here is the code : models.pysignals.pyapps.py ( this post advised to change this file ) When going to a shell : m2m_changed.receivers returns an empty list ( it does not work , and the signal can never be received ) I found similar post but did not found the answer in it.Why m2m_changed signal does not work ? EDITWhen opening a shell and importing badges.signals , it works.It means the problem is in apps.py :"
"I have a `` bytes '' object and an `` int '' mask , I want to do a xor over all the bytes with my mask.I do this action repeatedly over big `` bytes '' objects ( ~ 4096 KB ) .This is the code I have which does the work well , only it is very CPU intensive and slows down my script : The best I could come up with is this , which is about 20 times faster : My questions are : Is there a more efficient way to do this ( CPU-wize ) ? Is there a `` cleaner '' way to do this ( without hurting performance ) ? P.S . if it is of any importance , I 'm using Python 3.5"
"Suppose I have a function that raises unexpected exceptions , so I wrap it in ipdb : I can move up the stack to find out what values x and y have : However , when debugging , I want to just put a debugger at the top level : I can display the traceback , but I ca n't view the variables inside the function called : The exception object clearly still has references to the stack when the exception occurred . Can I access x and y here , even though the stack has unwound ?"
"Can I call a global function from a function that has the same name ? For example : By { sorted } I mean the global sorted function.Is there a way to do this ? I then want to call my function with the module name : service.sorted ( services ) I want to use the same name , because it does the same thing as the global function , except that it adds a default argument ."
"I am trying , using Qt classes QWebEngineView , and QWebChannel to make a simple connection between HTML page and Python script . The goal is simply to execute foo ( ) when the header < h2 > is clicked . The problem seems to be that the variable backend is not visible outside of the QWebChannel constructor . I tried to make backend an attribute of widnow to make it global but it did not work ."
"I 'm working through Allen Downey 's How To Think Like A Computer Scientist , and I 've written what I believe to be a functionally correct solution to Exercise 10.10 . But it took just over 10 hours ( ! ) to run , so I 'm wondering if I 'm missing some really obvious and helpful optimization.Here 's the Exercise : '' Two words 'interlock ' if taking alternating letters from each forms a new word . For example , 'shoe ' and 'cold ' interlock to form 'schooled ' . Write a program that finds all pairs of words that interlock . Hint : Do n't enumerate all pairs ! `` ( For these word-list problems , Downey has supplied a file with 113809 words . We may assume that these words are in a list , one word per item in the list . ) Here 's my solution : The print statements are not the problem ; my program found only 652 such pairs . The problem is the nested loops , right ? I mean , even though I 'm looping over lists that contain only words of the same length , there are ( for example ) 21727 words of length 7 , which means my program has to check over 400 million `` interlockings '' to see if they 're actual words -- -and that 's just for the length-7 words.So again , this code took 10 hours to run ( and found no pairs involving words of length 5 or greater , in case you were curious ) . Is there a better way to solve this problem ? Thanks in advance for any and all insight . I 'm aware that `` premature optimization is the root of all evil '' -- -and perhaps I 've fallen into that trap already -- -but in general , while I can usually write code that runs correctly , I often struggle with writing code that runs well ."
"My ultimate goal is as follows : I have a huge data set of points , representing how a part will be 3D printed layer by layer . I need to create a line through these points and extrude a circle along this line ( so rebuild the part as it will be printed later ) . I initially tried doing a spline , however this attempts to create a smooth line and does not follow the points at all . I attempted changing the minDeg and maxDeg options but this still did n't help enough to create the actual curve I need . See this result for the splineSee here the actual path ( the above spline is one of the infill parts ) So I have attempted creating a spline between just two points at a time , then when creating a wire add them all together . This looks promising as now I do get actual sharp corners and the lines going through the exact points . However , now when I try to extrude along it the normal of the extruded profile does not change with the angle of wire . This is what happens with the last thing I triedI have spent my last 4 days on this problem , tried many forums and questions but feel completely lost in the world of pythonocc ( opencascade ) . My code reads as follows :"
"I 've found out that you can use collections in relationship in order to change the type of return value , specifically I was interested in dictionaries.Documentation gives an example : And it works . However I was hoping that it will make list values if there are more than just one key with the same name . But it only puts the last value under unique key name.Here is an example : item.notes will return something like this : Where ids of foo and bar objects are 2 and 4 respectively.What I 'm looking for is to get something like this : Is it possible to get dict of lists from relationship in sqlalchemy ?"
"I am coming from Java and learning Python , now . I try to understand the concept of class members in Python.Here is an example program in Java : That is what I did in Python , following some examples I found : Both programs return : Here are my questions : Is the Python Code correct ? The programming style of Python appears to me to be more compact , compared to Java . So I am wondering , WHY does Python require the passing of a `` self '' -- Parameter . At this point Python seems to be more `` complex '' than Java . Or is there a way to remove the `` self '' -- Parameter ?"
"I 'm trying to find how many characters I would need to delete to make the two words the same . For instance `` at '' , `` cat '' would be 1 because I could delete the c , `` boat '' and `` got '' would be 3 because I could delete the b , a and g to make it ot . I put the words into a dictionary with their count as the value . Then I iterate over the dictionary and see if that key exists in the other dictionary otherwise I add 1 to the difference . Is this a very inefficient algorithm ? But it is overestimating the number of deletions I need ."
"Using python and selenium I need to locate and click a specific button on a webpage . While under normal circumstances this can be done with the commandsthat wo n't work in this case due to the webpages coding . Where both the button leading to the previous page and the one leading to the next share the same class name . Thus this code will only go back and forth between the first and second pageAs far as I can tell the only way to tell the two buttons apart , is that the button leading to the previous page lies within and the one that leads to the next page lies withinIs there some way of having selenium only select and click the button that lies in the `` next '' li class ? The complete button code : the previous button : the next button :"
What does a single * without identifier mean in the Python function arguments ? Here is an example that works with Python3.2 : For me the star after the self is strange.I have found it here ( from row 46 ) : http : //code.activestate.com/recipes/577720-how-to-use-super-effectively/
"I 'm working with financial data , which is recorded at irregular intervals . Some of the timestamps are duplicates , which is making analysis tricky . This is an example of the data - note there are four 2016-08-23 00:00:17.664193 timestamps : In this example , there are only a few duplicates , but in some cases , there are hundreds of consecutive rows , all sharing the same timestamp . I 'm aiming to solve this by adding 1 extra nanosecond to each duplicate ( so in the case of 4 consecutive identical timestamps , I 'd add 1ns to the second , 2ns to the 3rd , and 3ns to the fourth . For example , the data above would be converted to : I 've struggled to find a good way to do this - my current solution is to make multiple passes , checking for duplicates each time , and adding 1ns to all but the first in a series of identical timestamps . Here 's the code : This is obviously quite inefficient - it often requires hundreds of passes , and if I try it on a 2 million row dataframe , I get a MemoryError . Any ideas for a better way to achieve this ?"
"Using F2PY as a wrapper , is it possible to use subroutines with subroutine calls ? And if so , how ? In case I am unclear , I mean like this : The add subroutine is as follows : Trying f2py -c -m average average.f and importing to python I get : ImportError : ./average.so : undefined symbol : add_Also , adding intents to the second subroutine does not fix the issue ."
"I 'm looking for a way to replicate the encode behaviour in Stata , which will convert a categorical string column into a number column.Which results in : I 'd like to convert the cat column from strings to integers , mapping each unique string to an ( arbitrary ) integer 1-to-1 . It would result in : Or , just as good : Any suggestions ? Many thanks as always , Rob"
"I have data of a plot on two arrays that are stored in unsorted way , so the plot jumps from one place to another discontinuously : I have tried one example of finding the closest point in a 2D array : Can I use it somehow to `` clean '' my data ? ( in the above code , a can be my data ) Exemplary data from my calculations is : After using radial_sort_line ( of Joe Kington ) I have received the following plot :"
I am trying to solve following differential equation using python package PyDDE : Below is the python code to solve this equationI am getting following error : DDE Error : Something is wrong : perhaps one of the supplied variables has the wrong type ? DDE Error : Problem initialisation failed ! DDE Error : The DDE has not been properly initialised ! None
"Suppose one wanted to find the period of a given sinusoidal wave signal . From what I have read online , it appears that the two main approaches employ either fourier analysis or autocorrelation . I am trying to automate the process using python and my usage case is to apply this concept to similar signals that come from the time-series of positions ( or speeds or accelerations ) of simulated bodies orbiting a star . For simple-examples-sake , consider x = sin ( t ) for 0 ≤ t ≤ 10 pi.Given a sine-wave of the form x = a sin ( b ( t+c ) ) + d , the period of the sine-wave is obtained as 2 * pi / b . Since b=1 ( or by visual inspection ) , the period of our sine wave is 2 * pi . I can check the results obtained from other methods against this baseline.Attempt 1 : AutocorrelationAs I understand it ( please correct me if I 'm wrong ) , correlation can be used to see if one signal is a time-lagged copy of another signal ( similar to how cosine and sine differ by a phase difference ) . So autocorrelation is testing a signal against itself to measure the times at which the time-lag repeats said signal . Using the example posted here : Since x and t each consist of 100 elements and result consists of 199 elements , I am not sure why I should arbitrarily select the last 100 elements.Attempt 2 : FourierSince I am not sure where to go from the last attempt , I sought a new attempt . To my understanding , Fourier analysis basically shifts a signal from/to the time-domain ( x ( t ) vs t ) to/from the frequency domain ( x ( t ) vs f=1/t ) ; the signal in frequency-space should appear as a sinusoidal wave that dampens over time . The period is obtained from the most observed frequency since this is the location of the peak of the distribution of frequencies . Since my values are all real-valued , applying the Fourier transform should mean my output values are all complex-valued . I would n't think this is a problem , except for the fact that scipy has methods for real-values . I do not fully understand the differences between all of the different scipy methods . That makes following the algorithm proposed in this posted solution hard for me to follow ( ie , how/why is the threshold value picked ? ) . This outputs 0.01 , meaning the period is 1/0.01 = 100 . This does n't make sense either.Attempt 3 : Power Spectral DensityAccording to the scipy docs , I should be able to estimate the power spectral density ( psd ) of the signal using a periodogram ( which , according to wikipedia , is the fourier transform of the autocorrelation function ) . By selecting the dominant frequency fmax at which the signal peaks , the period of the signal can be obtained as 1 / fmax.The periodogram shown below peaks at 49.076 ... at a frequency of fmax = 0.05 . So , period = 1/fmax = 20 . This does n't make sense to me . I have a feeling it has something to do with the sampling rate , but do n't know enough to confirm or progress further.I realize I am missing some fundamental gaps in understanding how these things work . There are a lot of resources online , but it 's hard to find this needle in the haystack . Can someone help me learn more about this ?"
The documentation suggests : You can also specify the axis argument to .loc to interpret the passed slicers on a single axis.However I get an error trying to slice along the column index.So I could always assign the slice and re-transpose . That though feels like a hack and the axis=1 setting should have worked .
"I have the following code : Now , if the exception is thrown lock doesnt seem to be deleted , cause the mutex is not released . I can ofcourse do `` del lock '' everywhere , but that takes away the whole point of qmutexlocker . Does this have to do with Python garbage-collection ? If so , that must mean QMutexLocker is not usable at all in Python ?"
"I am trying to install sklearn-pandas.On my attempt : I get the result : The package setup script has attempted to modify files on your systemthat are not within the EasyInstall build area , and has been aborted.This package can not be safely installed by EasyInstall , and may notsupport alternate installation locations even if you run its setupscript by hand . Please inform the package 's author and the EasyInstallmaintainers to find out if a fix or workaround is available.I 'm on windows 7 ( I admit it ! ) , using Python 2.7.3This is the first time I 've gotten any error like this . Possible ideas I 've explored are the more basic solutions : The authors did n't write this package to be installed with easy_installI have some sort of file permission problem ( ? ) There is some sort of dependencies issuesIf anyone has ever gotten this error or has any insight into this please let me know ! Much thanks ."
Is there a single line expression to accomplish the following : My initial idea was to create two lists and then zip them up . That would take three lines.The list will have an even number of elements .
"Or , why doesn'twork ? c.f ."
"I 'm trying to see if a list of dates are valid dates . I 'm using the dateutil library , but I 'm getting weird results . For example , when I try the following : I get the result 1984-10-12T00:00:00 which is wrong . Does anyone know why this 12 gets added to the date ?"
"I would like to use a doctest comment block to demonstrate the usage of a particular base class , but either this can not be done with doctest or I am doing something wrong.Here is my simple demo code.The code does n't run . Here 's the first error issued :"
"I am trying to use numba to improve the speed of some code that I 've written that is rather slow . The majority of the time spent is in a single function . First I tried using just before my function definition , which improved timing a bit . Then , I tried using instead . From what I 've read in the documentation , the numpy methods that I am using within the function should be supported ( e.g . transpose ) . However , I am getting an error"
"This is what I am trying to run . When I run the server and run these lines within a view and then return an HttpResponse , then everything goes fine . However when I run python manage.py shell and then try to run through these lines then I get an error : Output : edit : and here is my settings.py : Also , I am using django 1.8 ."
Possible Duplicate : List comprehension for running total I 'm trying to write a concise list comprehension statement to create a cdf : For example : A standard procedure would look like this ( I want to write a list comprehension for the function f ( ) ) : Edit : I found a function numpy.cumsum ( ) . I 'll check if it uses list comprehensions .
"I do like the way I can treat lists in Python . It does any recursion solution to look easy and clean . For instance the typical problem of getting all the permutations of elements in a list , in Python looks like : I do like the way I can simple get new instances of modified lists by doing things likenumbers [ : i ] + numbers [ i+1 : ] or sol + [ numbers [ i ] ] If I try to code exactly the same in Java , it looks like : To create the same recursion I need to do : Which is quite ugly and it gets worse for more complex solutions . Like in this exampleSo my question is ... are there any buil-in operators or helper functions in the Java API that would make this solution more `` Pythonic '' ?"
"If I exclude my custom transformer the GridSearchCV runs fine , but with , it errors . Here is a fake dataset : and the error isHow can I make GridSearchCV work while there is a custom transformer in my pipeline ?"
"I ca n't get `` empty '' inner joins to work with a MultiIndex . Under 0.10.1 , I have : which gives meIs there any good way around this ? I 'd like to be able to tell in advance if the intersection is empty , so I can avoid the exception ."
"I have two numpy arrays holding 2d vectors : As you can see , a.shape is ( 4,2 ) and b.shape is ( 5,2 ) .Now , I can get the results I want thusly : My question is : What 's a more 'numpythonic ' way of getting the above ( i.e without the nested list comprehensions ) ? I 've tried every combination of np.cross ( ) I can think of , and I usually get results like this :"
"I am trying to write a generic `` Master '' spider that I use with `` start_urls '' and `` allowed_domains '' inserted dynamically during execution . ( Eventually , I will have these in a database , that I will pull and then use to initialize and crawl a new spider for each DB entry . ) At the moment , I have two files : MySpider.py -- Establishes my `` master '' spider class.RunSpider.py -- proof of concept for executing the initialization of my dynamically generated spiders . For writing these two files , I referenced the following : Passing Arguments into spiders at Scrapy.org Running Scrapy from a script at Scrapy.org General Spider structure within Pyton at Scrapy.orgThese two questions here on StackOverflow were the best help I could find : Creating a generic scrapy spider ; Scrapy start_urlsI considered scrapyD , but I do n't think its what I 'm looking for ... Here is what I have written : MySpider.py -- RunSpider.py -- PROBLEM : Here is my problem -- When I execute this , it appears to successfully pass in my arguments for allowed_domains and start_urls ; HOWEVER , after MySpider is initialized , when I run the spider to crawl , the specified urls / domains are no longer found and no website is crawled . I added the print statement above to show this : Why is my spider initialized correctly , but when I try to execute the spider the urls are missing ? Is this a basic Python programming ( class ? ) error that I am just missing ?"
"I have a ReconnectingClientFactory in a module . I 'd like the module to be as flexible as possible . I only need a single TCP connection . I use the factory as a persistent interface to this connection . In the past the factory would respond to disconnections by endlessly retrying the connection , never informing the top level script ( the script that imports the module ) that there were connection problems.Here 's a brief example of what I have : I think it 's best if I inform the top level script ( the script that imports the module ) when there are connection problems . This way the top level script can define disconnect resolution behavior rather than it all being hard coded in the module . But what is the best way to communicate the connection problems to the top level script ? I could raise an exception , but where would it be caught ? I guess the reactor would catch it , but how does that help ? There are no callbacks or errbacks I can fire to inform the top script of the connection problem.The top script could provide specific functions [ as arguments ] to be called when connection problems occur . Is this good design though ?"
Trying to understand the followingWhy is it that the ID 's assigned by Python are different for the same lists ?
"This code of course raises UnboundLocalError : local variable ' a ' referenced before assignment . But why is this exception raised at the print ( a ) line ? If the interpreter executed code line by line ( like I thought it did ) , it would n't know anything was wrong when print ( a ) was reached ; it would just think that a referred to the global variable.So it appears the interpreter reads the entire function in advance to figure out whether a is used for assignment . Is this documented anywhere ? Is there any other occasion where the interpreter looks ahead ( apart from checking for syntax errors ) ? To clarify , the exception itself is perfectly clear : global variables can be read without global declaration , but not written ( this design prevents bugs due to unintentionally modifying global variables ; those bugs are especially hard to debug because they lead to errors that occur far from the location of the erroneous code ) . I 'm just curious why the exception is raised early ."
"I am writing a script to find the best-fitting distribution over a dataset using scipy.stats . I first have a list of distribution names , over which I iterate : Now , after this loop , I select the minimum D-Value in order to get the best fitting distribution . Now , each distribution returns a specific set of parameters in ps , each with their names and so on ( for instance , for 'alpha ' it would be alpha , whereas for 'norm ' they would be mean and std ) .Is there a way to get the names of the estimated parameters in scipy.stats ? Thank you in advance"
"The question : Given N integers [ N < =10^5 ] , count the total pairs of integers that have a difference of K. [ K > 0 and K < 1e9 ] . Each of the N integers will be greater than 0 and at least K away from 2^31-1 ( Everything can be done with 32 bit integers ) .1st line contains N & K ( integers ) .2nd line contains N numbers of the set . All the N numbers are assured to be distinct.Now the question is from hackerrank . I got a solution for the question but it does n't satisfy the time-limit for all the sample test cases . I 'm not sure if its possible to use another algorithm but I 'm out of ideas . Will really appreciate if someone took a bit of time to check my code and give a tip or two ."
"I am grateful to the answers below , but sorry I still did n't resolve this issue maybe I did n't understand them correctly . Therefore I put a bounty for this for clearer answer.After user entering some information in the form , these information works as a query to filter the database to get the result , if there is no corresponding record in the database , how could I have an alert displaying on the current page or redirected page alerting users `` No corresponding data '' .Take an example as picture : if user enters `` EU '' and `` India '' , for sure there is no corresponding record in the database . And the form allows the user to leave the fields blank.I used to use raise ValidationError , if query result does n't match database , it will go to a yellow `` Exception '' page which is not user-friendly . I want to display an error message on the SAME form page right after submitting it : views.pyhtmlIf I do n't use the ValidationError method , it will redirect to result page showing everything as `` None '' . But I want to display an alert message . I saw there was an ajax example online , which is a little bit complicated . Is there any easier way to realize it？Thanks in advance.Thanks ."
"I 'm trying to do some combinatorial stuff with data in Python . I looked the question How to generate all permutations of a list in Python , but think that does n't fit my needs.. I have data of this type ... : ... and i need to make all possible combinations of three elements with only one from each group , without repetitions , saving to a list every combination . I know in this case there are 18 possible different combinations ( 3*3*2=18 ) , but do n't know how could i write this code.I ve read about the Pyncomb package , but do n't know the function to apply in this case ; maybe there 's a function which does the job.Hope anyone could help me ... Thanks in advance ; Peixe"
"I know how function comparison works in Python 3 ( just comparing address in memory ) , and I understand why.I also understand that `` true '' comparison ( do functions f and g return the same result given the same arguments , for any arguments ? ) is practically impossible.I am looking for something in between . I want the comparison to work on the simplest cases of identical functions , and possibly some less trivial ones : Note that I 'm interested in solving this problem for anonymous functions ( lambda ) , but would n't mind if the solution also works for named functions.The motivation for this is that inside blist module , it would be nice to verify that two sortedset instances have the same sort function before performing a union , etc . on them . Named functions are of less interest because I can assume them to be different when they are not identical . After all , suppose someone created two sortedsets with a named function in the key argument . If they intend these instances to be `` compatible '' for the purposes of set operations , they 'd probably use the same function , rather than two separate named functions that perform identical operations.I can only think of three approaches . All of them seem hard , so any ideas appreciated . Comparing bytecodes might work but it might be annoying that it 's implementation dependent ( and hence the code that worked on one Python breaks on another ) .Comparing tokenized source code seems reasonable and portable . Of course , it 's less powerful ( since identical functions are more likely to be rejected ) .A solid heuristic borrowed from some symbolic computation textbook is theoretically the best approach . It might seem too heavy for my purpose , but it actually could be a good fit since lambda functions are usually tiny and so it would run fast.EDITA more complicated example , based on the comment by @ delnan : Would I expect the key functions for s1 and s2 to evaluate as equal ? If the intervening code contains any function call at all , the value of fields may be modified , resulting in different key functions for s1 and s2 . Since we clearly wo n't be doing control flow analysis to solve this problem , it 's clear that we have to evaluate these two lambda functions as different , if we are trying to perform this evaluation before runtime . ( Even if fields was n't global , it might have been had another name bound to it , etc . ) This would severely curtail the usefulness of this whole exercise , since few lambda functions would have no dependence on the environment.EDIT 2 : I realized it 's very important to compare the function objects as they exist in runtime . Without that , all the functions that depend on variables from outer scope can not be compared ; and most useful functions do have such dependencies . Considered in runtime , all functions with the same signature are comparable in a clean , logical way , regardless of what they depend on , whether they are impure , etc.As a result , I need not just the bytecode but also the global state as of the time the function object was created ( presumably __globals__ ) . Then I have to match all variables from outer scope to the values from __globals__ ."
"I 'm trying to use numba to do np.diff on my GPU.Here is the script I use ; And here is the error I get ; Does anybody knows the reason for this ? PS : I used this video to do my script ; https : //youtu.be/jKV1m8APttU ? t=388EDIT : Thanks for the fast answers ! I added the dtype='float32 ' in np.matrix but now I have this error ; Known signatures : * ( float32 , float32 ) - > float32 File `` '' , line 5 [ 1 ] During : resolving callee type : Function ( signature= ( float32 , float32 ) - > float32 > ) [ 2 ] During : typing of call at ( 5 ) I also tried to use float32 [ : ] into the signature but it does n't work and in the video I followed they do n't do that"
"I wrote a code that performs a spline interpolation : But in the new dataset generated new_x and new_y the original points are eliminated , only the first and the last values are kept . I would like to keep the original points ."
"The figure above is a great artwork showing the wind speed , wind direction and temperature simultaneously . detailedly : The X axes represent the date The Y axes shows the wind direction ( Southern , western , etc ) The variant widths of the line were stand for the wind speed through timeseries The variant colors of the line were stand for the atmospheric temperature This simple figure visualized 3 different attribute without redundancy . So , I really want to reproduce similar plot in matplotlib . My attempt nowDoes someone has a more interested way to achieve this ? Any advice would be appreciate !"
"First , when I ask about units I mean units of measurement like inches , feet , pixels , cells . I am not referring to data types like int and float.Wikipedia refers to this as logical data type rather than physical data type.I 'd like to know the best way to name variables.Here is some code to walk through what I 'm asking : Notice that these are both integers ( or floats , I do n't care ) , yet I ’ ve changed units . I ’ ve also kept the variable name the same . I could establish a convention , and that ’ s the purpose of this question . Without guidance , I might do something like this : I would consider this an ad-hoc way of doing things . Or , I might establish a convention : Or other variants that I equally dislike . How might I name variables in a descriptive way , yet stay as close to PEP-08 as possible ? Just to be as clear as I can , the variables may have different data types , yet the units would be the same ( inches would have the same naming regardless of if it was stored as and integer or a float )"
"I have a raster of ecological habitats which I 've converted into a two-dimensional Python numpy array ( example_array below ) . I also have an array containing `` seed '' regions with unique values ( seed_array below ) which I 'd like to use to classify my habitat regions . I 'd like to 'grow ' my seed regions 'into ' my habitat regions such that habitats are assigned the ID of the nearest seed region , as measured 'through ' the habitat regions . For example : My best approach used the ndimage.distance_transform_edt function to create an array depicting the nearest `` seed '' region to each cell in the dataset , which was then substituted back into the habitat array . This does n't work particularly well , however , as the function does n't measure distances `` through '' my habitat regions , for example below where the red circle represents an incorrectly classified cell : Below are sample arrays for my habitat and seed data , and an example of the kind of output I 'm looking for . My actual datasets are much larger - over a million habitat/seed regions . Any help would be much appreciated !"
"I have a list of items with heterogeneous data types contained in strings.From that , I would like to obtain a new list with only the positive numbers : new_lst= [ 1,155 ] I have tried to avoid negative numbers as shown below . However , I am not able to avoid the strings and empty strings :"
"I have found that comparing the results of the keys ( ) and values ( ) methods of the dict built-in to themselves results in inconsistent results : Running the above code in Python 2.7 will return True for both calls , leading me to believe that there is some implementation detail in Python 3 's dict_values that causes this strange behaviour.Is there a reason for this behaviour or have i stumbled upon some obscure bug ?"
"I just noticed that you can do this in Python : …In other words , f ( ) behaves like a method that is not defined on any class , but can be attached to one . And of course it will produce a runtime error if it expects methods or fields on the object that it is attached to , which do n't exist.My question : is this useful ? Does it serve a purpose ? Are there situations where it solves a problem ? Maybe it 's a way of defining interfaces ?"
"I 'm using wtforms to handle data from my post requests . One certain post requests sends a variety of data including a boolean value . My form looks like this : I can see that when I receive the request the data looks like this : You can see the boolean field is `` false '' , and printing the raw data shows that too However , when I print the actual form field I get true.I read that WTForms might not know how to handle false boolean values . What is the proper way of doing this ? Using an IntegerField instead ? I have another form with a booleanfield that is handling false boolean values from my postgres database just fine ."
"In requires_dist section of a package 's json response from pypi , it is given : can anyone make it clear the second statement of each dependency , extra == 'bcrypt ' and extra == 'argon2 ' ?"
"I 'm working with SQLAlchemy to run SQL queries against an Oracle database . I have read access to the database , but the user I have does not own any of the tables I 'm working with.The database updates on a regular basis , so rather than explicitly listing the MetaData , I was hoping to use reflection . I found this question , that describes an issue similar to what I 'm having . However , I do n't have a way to change ownership of the tables , nor modify the database in any way . I just have read access.Is there a way to reflect Oracle tables in SQLAlchemy if I do n't have ownership of those tables ? ( Edit ) Example Code : I receive an exception of sqlalchemy.exc.NoSuchTableError : studentsHowever , when I run the following : I receive the output that I expected from the table , which is a tuple of all the fields for each row.So instead of trying to reflect a single table , I try to reflect all of them : The output is immutabledict ( { } ) .So essentially it 's nothing . All of these tables are owned by user A where as I 'm logging in with a read-only of user B ."
"Ok I am a ( very ) novice Python user , but I am trying to translate a piece of Python code into R , and I have run into a confusing problem with array reshaping.Lets make some example data : Ok so I 've made a 2D array with 2 rows and 4 columns . I 'm happy with this . The confusion arises with this line of code : So I know that I have added an extra dimension ( which I think is the 3rd digit 1 in the reshape command ) , but I do n't understand what else this had done . The shape implies it is still got 2 rows and 4 columns , but clearly something else is changed . Again my motivation here is to do the same operation in R , but until I know I understand what I 've transformed here I am stuck . ( Forgive me if this is an awful question I only started Python yesterday ! )"
"While the following code works well in windows , in Linux server ( of pythonanywhere ) the function only returns 0 , without errors . What am I missing ? Ref : Code from https : //stackoverflow.com/a/37367965/6546440"
"I have two Python CLI tools which share a set of common click.options . At the moment , the common options are duplicated : Is it possible to extract the common options in to a single decorator that can be applied to each function ?"
"The Story : Currently , I have a function-under-test that expects a list of lists of integers with the following rules : number of sublists ( let 's call it N ) can be from 1 to 50number of values inside sublists is the same for all sublists ( rectangular form ) and should be > = 0 and < = 5values inside sublists can not be more than or equal to the total number of sublists . In other words , each value inside a sublist is an integer > = 0 and < NSample valid inputs : Sample invalid inputs : I 'm trying to approach it with property-based-testing and generate different valid inputs with hypothesis library and trying to wrap my head around lists ( ) and integers ( ) , but can not make it work : the condition # 1 is easy to approach with lists ( ) and min_size and max_size argumentsthe condition # 2 is covered under Chaining strategies togetherthe condition # 3 is what I 'm struggling with - cause , if we use the rectangle_lists from the above example , we do n't have a reference to the length of the `` parent '' list inside integers ( ) The Question : How can I limit the integer values inside sublists to be less than the total number of sublists ? Some of my attempts : This one was very far from meeting the requirements - list is not strictly of a rectangular form and generated integer values can go over the generated size of the list.Here , the # 1 and # 2 are requirements were being met , but the integer values can go larger than the size of the list - requirement # 3 is not met ."
"I want to move a file , but in the case it is not found I should just ignore it . In all other cases the exception should be propagated . I have the following piece of Python code : errno == 2 is the one , that has 'No such file or directory ' description . I wonder if this is stable across Python versions and platforms , and etc ."
"I 'd like to import a C function into an IPython notebook using Cython . Currently , I 'm trying to replicate the example in the Cython documentation , but I get a compilation error.My Python code ( from an iPython notebook ) : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- new cellMy C code : Running this code , I get the following traceback and error message : I 've tried searching Google for this error , but I ca n't seem to find anything relevant ."
"I 'm sure this has been answered somewhere but I was n't sure how to describe it.Let 's say I want to create a list containing 3 empty lists , like so : I thought I was being all clever by doing this : But I discovered , after debugging some weird behavior , that this caused an append update to one sublist , say lst [ 0 ] .append ( 3 ) , to update the entire list , making it [ [ 3 ] , [ 3 ] , [ 3 ] ] rather than [ [ 3 ] , [ ] , [ ] ] .However , if I initialize the list withthen doing lst [ 1 ] .append ( 5 ) gives the expected [ [ ] , [ 5 ] , [ ] ] My question is why does this happen ? It is interesting to note that if I dothen the 'linkage ' of cell 0 is broken and I get [ [ 5,3 ] , [ ] , [ ] ] , but lst [ 1 ] .append ( 0 ) still causes [ [ 5,3 ] , [ 0 ] , [ 0 ] .My best guess is that using multiplication in the form [ [ ] ] *x causes Python to store a reference to a single cell ... ?"
I have unsorted array of indexes : I also have an array of values of the same length : I have array with zeros of desired values : Now I want to add to elements in d values of v based on it 's index in i . If I do it in plain python I would do it like this : It is ugly and inefficient . How can I change it ?
"I have written a python utility script that uses optparse to include options and flags at script launch.Everything works great , but when I import google API oauth2client and run its execute function , it overrides my add_options into the options it uses.When I say 'overrides ' I mean that even though my script add options to my option parser , when I execute the script like so : I get a detailed response of the options I added to my script : But , when I actually execute my script like so : I get the following error : Another important thing to know is that I 'm using my own module that wraps oauth2client like so : Then my script looks something like that : How can I avoid this option override ? Shahar"
"I 'm doing pairwise distance for something with a weird distance metric . I have a dictionary like { ( key_A , key_B ) : distance_value } and I want to make a symmetric pd.DataFrame like a distance matrix . What is the most efficient way to do this ? I found one way but it does n't seem like the best way to do this . Is there anything in NumPy or Pandas that does this type of operation ? or just a quicker way ? My way is 1.46 ms per loop"
"I am using this django queryMy distinct query is returning two objects For example : How can i get case insensitive results ? like in this case onlyusing django 1.9.6 , python 2.7"
"While writing documentation using Sphinx , I know that I can link to a class by using Intersphinx : But how do I link directly to a specific method , like logging.Logger.warning ( ) ?"
"I just started playing with LSTM in keras and I find the possibility of learning the behaviour of time series very fascinating.I 've read several tutorials and articles online , most of them showing impressive capabilities in predicting time series , so I gave it a go . The first thing that I noticed is that all the articles I 've found always use the validation data in av very unfair way . My idea of predicting a time series is that I use the training data to build a model and use the last N elements of the training data to estimate the future behaviour of the series . To do that , the model has to use its own predictions as inputs to step forward in the future.What I 've seen people doing , instead , is to estimate the accuracy of the testing set at any time in the future , using the ground truth as input for the estimation . This is very unfair , because it does not produce a real prediction ! I tried to code my own LSTM prediction in Keras ( please find the code below ) , and I started with a relatively simple case , a combination of a parabola and a sinusoid . Unfortuntely , results are quite unsatisfying . Here are a few examples , obtained by changing the parameters of the network : Do you have any suggestion to get better results ? How can LSTM predict complex behaviours if they ca n't predict such a `` simple '' signal ? Thank you , Alessandro"
"I need to render a wordcloud on my dash application . According to this thread https : //community.plot.ly/t/solved-is-it-possible-to-make-a-wordcloud-in-dash/4565 , there is no wordcloud build-in component in dash . One workaround is to use WordCloud module to produce the wordcloud as image and use dash_html_components.Img to show on layout . I 'm new to Dash . Not sure how I can render the image . Do I need to save wordcloud as temp image everytime I produce a wordcloud ? Really appreciate it if anyone with some expertise in Dash can help with that . The code is below :"
"Objective : Convert binary to stringExample : 0111010001100101011100110111010001100011011011110110010001100101 - > testCode ( without space ) I use a dictionary and my function , i search a better way and more efficientthank by advance ,"
"I am going to get every horizontal tensor in a variable but I got one dimension lost.This is my code : and the output is : and how can I get [ torch.FloatTensor of size 1x4 ] ? ? ? If I can not make my question understood , please write it down and do not just vote down this question . It is not polite . Thanks very much !"
"I just learned about the apparently undocumented \K behavior in Ruby regex ( thanks to this answer by anubhava ) . This feature ( possibly named Keep ? ) also exists in PHP , Perl , and Python regex flavors . It is described elsewhere as `` drops what was matched so far from the match to be returned . `` Is this behavior identical to the positive lookbehind marker as used below ? If not , what differences do the two exhibit ?"
"I have an array of values , said v , ( e.g . v= [ 1,2,3,4,5,6,7,8,9,10 ] ) and an array of indexes , say g ( e.g . g= [ 0,0,0,0,1,1,1,1,2,2 ] ) .I know , for instance , how to take the first element of each group , in a very numpythonic way , doing : returns : Is there any numpythonic way ( avoiding explicit loops ) to get the maximum of each subset ? Tests : Since I received two good answers , one with the python map and one with a numpy routine , and I was searching the most performing , here some timing tests : As a result I get : Interestingly , most of the slowdown of the map method is due to the list ( ) call . If I do not try to reconvert my map result to a list ( but I have to , because python3.x returns an iterator : https : //docs.python.org/3/library/functions.html # map )"
"In the latest release of pytest , it is easy to create fixtures that are function , class , module or session-scoped like this : That creates a fixture that is going to be called only once for each python module in which it is used.But what about fixtures that need to be called once per python package ? ( With nose , it can be done using setUp/tearDown methods in the __init__.py of the package )"
"Here is the setup : Using ngBoilerplate ( grunt , bower , angular , the works ... ) to create a SAP application . On my localhost , it launches a NodeJS server so I can test the app . This all works fine minus the database/apis . Using Grunt , it will create a /build folder ( which is all the non-minified source , assets , for debugging ) and a /bin folder with the production code.For the backend I have a Python flask app ( which I 'll use for REST API 's ) on Heroku . Inside the main Python script : I push the code to Heroku , it detects a Python app ( which I believe is good as I will need Python to make my api requests ) , and it serves the correct index.html . I see Angular making requests to /vendor/angular.js /css/angular.css etc , when those files technically live in /build/vendor/angular.js . I 'm not sure if I 'm suppose to tell Angular where to grab the files or if it 's Python related . Am I suppose to change the DOCROOT ( WWW ) like in LAMP land ? Do I change the routeprovider/urlrouterprovider in Angular to tell it to serve the files to a different location ? Or do I change what I 'm doing in Python ? The project directory looks like :"
"How to outline pixel boundaries in matplotlib ? For instance , for a semi-random dataset like the one below , it is quite clear that a contour matching the pixel edges of image would be preferred to the default behavior of the contour function , where the contour lines are effectively drawn across the diagonals of edge pixels.How to make the contours align with the pixels ? I 'm looking for a solution within numpy and matplotlib libraries ."
"Is the order for returned elements from Mindom getElementsByTagName the same as it is in document for elements in the same hierarchy / level ? I need to know if image_siblings will contain the images in the same order , they are placed in the document for the same hierarchy.I found a similar question for JavaScript , but I 'm unsure if this is also true for Python ( version 3.5.2 ) Minidom getElementsByTagName ."
"I have a timeseries dataset and I am trying to train a network so that it overfits ( obviously , that 's just the first step , I will then battle the overfitting ) .The network has two layers : LSTM ( 32 neurons ) and Dense ( 1 neuron , no activation ) Training/model has these parameters : epochs : 20 , steps_per_epoch : 100 , loss : `` mse '' , optimizer : `` rmsprop '' .TimeseriesGenerator produces the input series with : length : 1 , sampling_rate : 1 , batch_size : 1.I would expect the network would just memorize such a small dataset ( I have tried even much more complicated network to no avail ) and the loss on training dataset would be pretty much zero . It is not and when I visualize the results on the training set like this : I get : The predictions have somewhat smaller amplitude but are precisely inverse to the targets . Btw . this is not memorized , they are inversed even for the test dataset which the algorithm has n't trained on at all.It appears that instead of memorizing the dataset , my network just learned to negate the input value and slightly scale it down . Any idea why this is happening ? It does n't seem like the solution the optimizer should have converged to ( loss is pretty big ) .EDIT ( some relevant parts of my code ) : EDIT ( different , randomly generated dataset ) : I had to increase number of LSTM neurons to 256 , with the previous setting ( 32 neurons ) , the blue line was pretty much flat . However , with the increase the same pattern arises - inverse predictions with somewhat smaller amplitude.EDIT ( targets shifted by +1 ) : Shifting the targets by one compared to predictions does n't produce much better fit . Notice the highlighted parts where the graph is n't just alternating , it 's more apparent there.EDIT ( increased length to 2 ... TimeseriesGenerator ( length=2 , ... ) ) : With length=2 the predictions stop tracking the targets so closely but the overall pattern of inversion still stands ."
"I 'm trying to run python script from Azure webjob . This is what I 've done following this linkAccess the kudu tool via the url https : // < webapp name > .scm.azurewebsites.net and installed Python 364x86 via Site Extensions tab Confirmed Python 364x86 is installed in the following path : D : \home\python364x86Added my script trading.py in D : \home\python364x86Created run.bat file with this line of code D : \home\python364x86\python.exe trading.pyIncluded run.bat and trading.py in the webjob zip fileDeployed , but getting error Functions.csAbove code executes python script . It works locally , but once I deploy it to production it fails . Tried so many times , spent plethora of hours , but still unsure why prod does n't work . Help is appreciated.trading.py"
I have the following model : This does n't seem to work . I always get the following error : What am I doing wrong ?
"I 'm trying to sync my db from a view , something like this : The issue is , it will block the dev server by asking for user input from the terminal . How can I pass it the ' -- noinput ' option to prevent asking me anything ? I have other ways of marking users as super-user , so there 's no need for the user input , but I really need to call syncdb ( and flush ) programmatically , without logging on to the server via ssh . Any help is appreciated ."
"It 's a weird error since when I try/catch it , it prints nothings.I 'm using sanic server to asyncio.gather a bunch of images concurrently , more than 3 thousand images.I have n't got this error when dealing with a smaller sample size.Simplified example : What could this error be ? If it is some kind of limitation of my host/internet , how can I avoid it ? I 'm using a basic droplet from DigitalOcean with 1vCPU and 1GB RAM if that helpsFull stack error :"
"After searching for a while , I found the following solutions for an api call that requires the Delete method.First try : ( httplib library ) This returns : { u'error ' : { u'status ' : 400 , u'message ' : u'Empty JSON body . ' } } Second try : ( urllib2 library ) This returns : HTTP 400 Bad RequestI have other functions where the JSON is working , so I guess the problem is with the DELETE method but I ca n't get it working . Besides this , the webapp is running on google app engine so i ca n't install packets so I would like to remain in the pre-installed librarys.Anyone has a good way to do a Delete request on GAE ? ( I need to send both data and headers ) The API is spotify : developer.spotify.com/web-api/ and I 'm trying to delete a track from a playlist ."
"I 'd like to use doctests to test the presence of certain warnings . For example , suppose I have the following module : If I run python -m doctest testdocs.py to run the doctest in my class and make sure that the warning is printed , I get : It looks like the warning is getting printed but not captured or noticed by doctest . I 'm guessing that this is because warnings are printed to sys.stderr instead of sys.stdout . But this happens even when I say sys.stderr = sys.stdout at the end of my module.So is there any way to use doctests to test for warnings ? I can find no mention of this one way or the other in the documentation or in my Google searching ."
I need to find the percentage of a MultiIndex column ( 'count ' ) . My DataFrame looks like : and I 'd like to add a column 'percent ' to make I know that I can find the totals I want by but I ca n't seem to figure out how to turn this into a percentage .
"Summary and Test CasesThe core issue is that Tensorflow throws OOM allocations on a batch that is not the first , as I would expect . Therefore , I believe there is a memory leak since all memory is clearly not being freed after each batch.Explanation : Essentially , it runs 144 batch with a batch size of 500 before failing on the 145th batch , which seems strange . If it ca n't allocate enough memory for the 145th batch , why should it work for the first 144 ? The behavior can be replicated.Note that each batch DOES vary in size , since each one has dimensions [ BATCH_SIZE , MAX_SEQUENCE_LENGTH ] , and depending on the sequences sampled , the sequence length varies , but the program does not fail on the largest batch ; it fails later on a smaller one . Therefore , I conclude that a single oversized batch is not causing the memory error ; it appears to be a memory leak.With a larger batch size , the program fails earlier ; with a smaller batch size , it fails later.The full error is here : Code snippet ( from models.py ) Full code ( extremely similar to NMT tutorial , simplified ) .Model code is in models.py , iterator code is in data_pipeline.py , main is main.py.https : //github.com/nave01314/tf-nmt"
"I am trying to create a very huge sparse matrix which has a shape ( 447957347 , 5027974 ) .And , it contains 3,289,288,566 elements.But , when i create a csr_matrix using scipy.sparse , it return something like this : The source code for creating matrix is : And , I also found when I convert an 3 billion length python array to numpy array , it will raise an error : But , when I create three 1 billion length python arrays , and convert them to numpy array , then append them . It works fine.I 'm confused ."
"Suppose I have a peewee model which looks more or less as follows : And I wish to add a property to that model as follows : This is helpful sometimes ; now , if Object is a MyModel instance , I can easily check its diff with Object.diff.What I ca n't do is the following : And this is since MyModel.diff is a simple property , and is probably always greater than 17 . It is not an Expression like MyModel.a < 17.It would be very nice to expose diff as if it was a field ; so the user of that API will not need to know whether the specific implementation has a and b as real fields and diff as a virtual one , or rather a and diff as real fields and b as a virtual one.Of course , my real intention is to use properties which involve , in some cases , much more sophisticated calculation that that presented on diff ; an example isOn the other hand , it can be a very simple property , such asThis means it can not , in general , be converted to SQL , but should rather filter the results after they are retrieved from the database.Is that possible ? UpdateI 've just discovered peewee playhouse 's hybrid methods/properties . These provide a partial solution to my question.For example , my diff method can become a hybrid_property , and work as expected . My complicated_property can not become one , or at least it seems like it ; the if condition in its beginning will return either True or False constantly , and will not act as a function.Peewee probably has some more magic hiding there ; I 'll keep looking and report my findings ."
"I 'm trying to cache some of my DRF api calls in a CDN . I need the following headers Cache-Control : public , max-age=XXXXThis is pretty easy when you 're using traditional django templating , you just add the @ cache_page ( ) @ cache_control ( public=True ) decorators , but for DRF , I ca n't find anything similar . There 's quite a bit about in mem caches , which I already have up , but I 'd really like to get the CDN to take that load off my server all together , I 'd like to cache the resulting queryset . I 'm also using modelViewSets if that matters for anything :"
I 'm trying to call the reboot function from libc in Python via ctypes and I just can not get it to work . I 've been referencing the man 2 reboot page ( http : //linux.die.net/man/2/reboot ) . My kernel version is 2.6.35.Below is the console log from the interactive Python prompt where I 'm trying to get my machine to reboot- what am I doing wrong ? Why is n't ctypes.get_errno ( ) working ? Edit : Via Nemos reminder- I can get get_errno to return 22 ( invalid argument ) . Not a surprise . How should I be calling reboot ( ) ? I 'm clearly not passing arguments the function expects . = )
"This code is not working ... ... So I should write down like this..But the Timer class has Thread.__init__ , Thread.__init__ has `` daemon '' for input parameter.I do n't have any idea why it does n't work ..."
"I 'm trying to strengthen my Python skills , and I came across Open-Source code for Saltstack that is using types.FunctionType , and I do n't understand what 's going on.salt.cloud.clouds.cloudstack.pyFunction create ( ) has the following bit of code : The function get_image , and get_size are passed to a function 'namespaced_function ' as so : salt.utils.functools.pyHas the namespaced functionI can see that they are dynamically creating a function get_image , but I do n't understand the benefit of doing it this way . Why not just create the function ?"
"I am using the Python package ezdxf to extract information from a dxf file . Is there a way I can find out the height and width of the file , so than I can create an image of the same size and draw the entities on it for my reference . I tried to extract the information from the DXF Header using ` as mentioned in this link.But I am not clear about what they mean . Also would request for some links having good details about Python ezdxf ."
I uses autoenv for automatic virtualenv activate . Python project 's top folder has .env file with following contentsThis command executed whenever cd to any sub folder of the project . Then throwsIt failed because it is trying to execute activate relative to sub folder . Why it executes even in subfolder ? How to resolve the issue ?
"I am using movielens dataset ( ratings.dat ) , and pandas dataframe to read and process the data . I have to split this data into test and training set . By using pandas dataframe.sample function , and can divide the data into random splits.For example : train = df.sample ( frac=0.8 , random_state=200 ) test = df.drop ( train.index ) Now I am trying to sort data on user_id and then on timestamp , and I need to divide data into 80 % -20 % per user in training set and test set respectively.So , for example if user1 rated 10 movies , then the entries for this user should sorted from oldest to latest according to timestampratings = pd.read_csv ( 'filename ' , sep='\t ' , engine='python ' , header=0 ) sorted_df = ratings.sort ( [ 'user_id ' , 'timestamp ' ] , ascending= [ True , True ] ) and the splitting should be in such a way that the first 8 entries with oldest timestamp will be in training set and the latest 2 entries will be in the test set.I have no idea how could I do that . Any suggestions ? ThanksData :"
"I would like to have a function that can detect where the local maxima/minima are in an array ( even if there is a set of local maxima/minima ) . Example : Given the arrayI would like to have an output like : As you can see from the example , not only are the singular values detected but , also , sets of local maxima/minima.I know in this question there are a lot of good answers and ideas , but none of them do the job described : some of them simply ignore the extreme points of the array and all ignore the sets of local minima/maxima.Before asking the question , I wrote a function by myself that does exactly what I described above ( the function is at the end of this question : local_min ( a ) . With the test I did , it works properly ) .Question : However , I am also sure that is NOT the best way to work with Python . Are there builtin functions , APIs , libraries , etc . that I can use ? Any other function suggestion ? A one-line instruction ? A full vectored solution ? Note : I tried to enrich the code with some comments to let understand what I do . I know that the function that I propose is not clean and just prints the results that can be stored and returned at the end . It was written to give an example . The algorithm I propose should be O ( n ) .UPDATE : Somebody was suggesting to import from scipy.signal import argrelextrema and use the function like : To have something like that is what I am really looking for . However , it does n't work properly when the sets of local minima/maxima have more than two values . For example : The output is : Of course in test03 [ 4 ] I have a minimum and not a maximum . How do I fix this behavior ? ( I do n't know if this is another question or if this is the right place where to ask it . )"
"Mostly curious.I 've noticed ( at least in py 2.6 and 2.7 ) that a float has all the familiar rich comparison functions : __lt__ ( ) , __gt__ , __eq__ , etc.but an int does notWhich is odd to me , because the operator itself works fineEven strings support the comparison functionsbut all the int has is __cmp__ ( ) Seems strange to me , and so I was wondering why this came to be . Just tested and it works as expected in python 3 , so I am assuming some legacy reasons . Still would like to hear a proper explanation though ; )"
"Suppose I have a code snippet that I 'd like to run every time I open a jupyter notebook ( in my case it 's opening up a Spark connection ) . Let 's say I save that code in a .py script : -- startup.py -- I want to be able to have that code snippet run every time I open a kernel . I 've found some stuff about the Jupyter Configuration File , but it does n't seem like variables defined there show up when I try to run in a notebook . Is there a command-line option that I could use -- something like : or do I have to include something likein all of the notebooks where I want those variables to be defined ?"
I am using Ubuntu 12.04 LTS . When I try something like this in terminal : Symbols are shown correctly . But if try to print unicode symbols using python 2.7 I get this : As python shows I have utf-8 encoding by default for terminal :
"I was wondering what the neatest way would be to convert ( from Python ) a list comprehension into Javascript . Is there anything which will make this readable and not a mess ? This is quite a good example of a list comprehension , as it has multiple fors and and an if.I should add that the range bit is covered here ( I ca n't live without range ) ."
"My `` setup.py '' currently includes the following statement : Is there a way for me to specify `` any Python 3 , from 3.0 upwards '' , without explicitly enumerating all the existing and future Pythons ? The reason I ask this is because , even though the general `` Programming Language : : Python : : 3 '' is specified in the above , an Anaconda install fails with : The pip install works fine.Thanks !"
"I 'm trying to write a spellchecker module.It loads a text , creates a dictionary from 16 mb file and then checks if encountered word is similar to the word in dictionary ( similar = varies up to two chars ) if so then it changes it to the form from dictionary.Right now I 'm using a Levenshtein Distance algorithm and processing of a 50 words set takes 3 min ... I 'm pretty sure that there must be a faster solution.Profiler told me that my app spends more than 80 % of it 's time in Levenshtein Distance function.Are there any better solutions/algorithms ? Here is the implemented of version of the algorithm I use :"
"So what exactly is Django implementing ? Seems like there are Models = Database mappings Views = Grab relevant data from the models and formats it via templates Templates = Display HTML depending on data given by ViewsEDIT : S. Lott cleared a lot up with this in an edit to a previous post , but I would still like to hear other feedback . Thanks ! Is this correct ? It really seems like Django is nowhere near the same as MVC and just confuses people by calling it that ."
"I have a pandas.core.series.TimeSeries named ts like this : Index of this TimeSerie are irregularly spaced.I would like to have value of ts for a given datetime such as 2013-08-11 14:20:00I just need to interpolate ONE value , not the whole TimeSerieI just want to interpolate data using a linear function between the previous index ( 2013-08-11 14:23:49 ) and the next index ( 2013-08-11 14:19:14 )"
"I have a dataframe that contains a column , let 's call it `` names '' . `` names '' has the name of other columns . I would like to add a new column that would have for each row the value based on the column name contained on that `` names '' column.Example : Input dataframe : pd.DataFrame.from_dict ( { `` a '' : [ 1 , 2 , 3,4 ] , `` b '' : [ -1 , -2 , -3 , -4 ] , `` names '' : [ ' a ' , ' b ' , ' a ' , ' b ' ] } ) Output dataframe : pd.DataFrame.from_dict ( { `` a '' : [ 1 , 2 , 3,4 ] , `` b '' : [ -1 , -2 , -3 , -4 ] , `` names '' : [ ' a ' , ' b ' , ' a ' , ' b ' ] , `` new_col '' : [ 1 , -2,3 , -4 ] } ) Thank you"
"I created an python application that is using the Youtube api ( so examples are in python , but does n't really matter , the concepts should be the same ) . I managed to get it working where I can connect and make api calls . However , when I connect to the api , I have to define a flow that checks if a the credentials storage file exists . If it does n't , then I have to manually sign in using the flow . After sign in the file ( main.py-oauth2.json ) , is created with the token . I would like to be able to download the credentials without having to sign manually sign in . I was hoping there was a way to make a POST request for that token , like I have seen here , but I have been able to do this with Youtube api . Does anyone know how to implement the desired feature ? main.pyTrying to use a google service account throws 401 errors on upload.Evidence this can be done The oauth2client.service_account.ServiceAccountCredentials class is only used with OAuth 2.0 Service Accounts . No end-user is involved for these server-to-server API calls , so you can create this object directly without using a Flow object.youtube apiOauth flow docshttps : //developers.google.com/identity/protocols/OAuth2 # serviceaccount"
"I am trying to create ( not exactly restore ) an object which has its attributes saved in a database . Therefore , I do not want to call __init__ . This desire appears to be inline with Guido 's intended use for __new__ . I do not understand why __init__ is not getting called.Take the following snippet for example , it returns an instance of class User without calling __init__.This is the exact behavior I want . However , my question is that I do not understand why ? According to the python docs __init__ is supposed to be called when __new__ `` returns an instance of cls . `` So why is __init__ not being even called , even though __new__ returns a class instance ?"
"I am setting the sys.excepthook so that I can log every exception that occurs . Instead of writing to a log , let 's use the following example : Now let 's say I create a type error , like so : Without being caught , this goes into the excepthook and properly prints out the 3 variables : What I would like to do , is have it ALSO print out the full Exception that was sent to the excepthook ( so , in this case , a TypeException ) . In other words , I 'd like it to also display the following information ) .If I add the following line : it will display the exception properly ; however , it will also display an error with the term raise : Altering it to : Will print out the following error : I want it to print out only the 2nd chunk ( the original exception ) . Is this possible ?"
"I want to use Gaussian Processes to solve a regression task . My data is as follow : each X vector has a length of 37 , and each Y vector has a length of 8.I 'm using the sklearnpackage in Python but trying to use gaussian processes leads to an Exception : x : [ [ 136 . 137 . 137 . 132 . 130 . 130 . 132 . 133 . 134 . 135 . 135 . 134 . 134 . 1139 . 1019 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 70 . 24 . 55 . 0 . 9 . 0 . 0 . ] [ 136 . 137 . 137 . 132 . 130 . 130 . 132 . 133 . 134 . 135 . 135 . 134 . 134 . 1139 . 1019 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 70 . 24 . 55 . 0 . 9 . 0 . 0 . ] [ 82 . 76 . 80 . 103 . 135 . 155 . 159 . 156 . 145 . 138 . 130 . 122 . 122 . 689 . 569 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . ] [ 156 . 145 . 138 . 130 . 122 . 118 . 113 . 111 . 105 . 101 . 98 . 95 . 95 . 759 . 639 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . ] [ 112 . 111 . 111 . 114 . 114 . 113 . 114 . 114 . 112 . 111 . 109 . 109 . 109 . 1109 . 989 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . ] [ 133 . 130 . 125 . 124 . 124 . 123 . 103 . 87 . 96 . 121 . 122 . 123 . 123 . 399 . 279 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . ] [ 104 . 109 . 111 . 106 . 91 . 86 . 117 . 123 . 123 . 120 . 121 . 115 . 115 . 549 . 429 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . ] [ 144 . 138 . 126 . 122 . 119 . 118 . 116 . 114 . 107 . 105 . 106 . 119 . 119 . 479 . 359 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . 0 . ] ] y : [ [ 7 . 9 . 13 . 30 . 34 . 37 . 36 . 41 . ] [ 7 . 9 . 13 . 30 . 34 . 37 . 36 . 41 . ] [ -4 . -9 . -17 . -21 . -27 . -28 . -28 . -20 . ] [ -1 . -1 . -4 . -5 . 20 . 28 . 31 . 23 . ] [ -1 . -2 . -3 . -1 . -4 . -7 . 8 . 58 . ] [ -1 . -2 . -14.33333333 -14 . -13.66666667 -32 . -26.66666667 -1 . ] [ 1 . 3.33333333 0 . -0.66666667 3 . 6 . 22 . 54 . ] [ -2 . -8 . -11 . -17 . -17 . -16 . -16 . -23 . ] ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Exception Traceback ( most recent call last ) in ( ) 11 gp = gaussian_process.GaussianProcess ( theta0=1e-2 , thetaL=1e-4 , thetaU=1e-1 ) 12 -- - > 13 gp.fit ( x__ , y__ ) /usr/local/lib/python2.7/site-packages/sklearn/gaussian_process/gaussian_process.pyc in fit ( self , X , y ) 300 if ( np.min ( np.sum ( D , axis=1 ) ) == 0 . 301 and self.corr ! = correlation.pure_nugget ) : -- > 302 raise Exception ( `` Multiple input features can not have the same '' 303 `` target value . '' ) 304 Exception : Multiple input features can not have the same target value.I 've found some topics related to a scikit-learn issue , but my version is up-to-date ."
I want to query createThemes to my graphql.My graphql query is : So it errors : mutate ( ) got multiple values for argument 'name ' Can you solve and explain why its printing such error.My code below :
When I run : it prints __main__.But when I run : it prints __builtin__.How to make the second example to also print __main__ ? What I try to achieve is to run a piece of code with exec so that from the perspective of the it looks like it was run from command line.I would like to tun the code with clean scope but the second example breaks the code relying on if __name__ == `` __main__ '' . How to fix this ?
"Possible Duplicate : How can I programmatically change the argspec of a function in a python decorator ? argspec is a great way to get arguments of a function , but it does n't work when the function has been decorated : Argspec should return arg1 , arg2 , arg3 . I think I need to define wrapper differently as to not use *a and **k , but I do n't know how ."
"I have been using Vim to write Stata scripts in Windows for a while now at the university . I am learning R at the moment , and I want to switch completely to Linux as my OS ( I 've recently switched to Ubuntu on my laptop ) . R works fine with Vim in both Windows and Linux , however I still need to use Stata sometimes . In Windows I have been using a simple AutoIt script provided by a Stata user to send lines / the whole file to stata for evaluation . This script doesnt work in Linux . This is what the script looks likewith the following in my vimrcThis is really practical and virtually the only reason I 'm still sticking to Windows . How would I go about getting something like that for Ubuntu ? I 'm new to linux , and dont really know much about programming besides statistics . Any help is greatly appreciated . ( Please dont suggest emacs , emacs support for stata is faulty , and though its integration with R is much better , I would like to keep using Vim for now . ) On a possibly related topic : I 'm considering learning Python , as I 'll probably be working with data and doing empirical analysis for a longer time , and I think it might be useful for some tasks , e.g . to solve problems like this or parsing data from websites . Is this recommended , or should I look at another language ( or forget the idea completely ) ?"
"I 've hit a wall with this one . I need to create a python based com server , package it as a windows exe and deploy it on windows . It has to have a `` full '' interface - because the consumer requires idispatch and a specific interfce to function . Now I have created the com server and have it running under the interpreter and it functions flawlessly with my picky client . However , when packaging as an EXE - its an localserver - I get an error in the log when the system tries to instantiate it ( even from a vbs script ) . So here 's everyting . I searched high and low in the itnernet and it looks like an import problem but I do n't know how to import my own python object for the localserver to use . This is python 2.7 with pywin32 extensions installed . So first - the IDL I created for the server : imtg.idlNext the Python code - now this gets a bit tricky because when I distribute this I do n't want to create the .tlb - so I 'm not distributing the .idy - just make sure you have the .tbl to register . Use an as admin cmd prompt if necessary . imtg_server.pyNext the setup for py2exeI had to add the funky import of the modulefinder because win32com.shell was n't included in the packaged executablesetup_imtg.pyWhen you run the generated EXE you can register with imtg_server -- register but you wo n't see abou output -- unregister unregistersYou can use this vbs file to test it.t.vbsWhen you run there will be a .log created that looks like this : So what I need is to get around this error . This class is certainly in my object . I 'm concerned that the value I have specified in my server as : Is incorrect . The main may refer to the wrapped exe and not my own server which has no main specified . I did try to create main as well with no better results . I just do n't know how py2exe proxies the classes . I tried using my file name imtg_server.CImtg and that fails with module not found . I tried just CImtg and that fails . I tried using variations of win32com and pythoncom - but it just does n't do it . What I have seems `` right '' so maybe I need an additional reg tag or something ? Any help is greatly appreciated . Thank you ."
"I 'm working on a small flask app in which I want to return strings containing umlauts ( in general German special characters , e.g ' ß ' ) . As the default JSONEncoder in flask has ensure_ascii=True , this will always convert my string to this : My first approach was to just create a very basic custom JSONEncoderIf I use this , by settingmy return jsonify ( ... ) will actually return the string containing ' ß ' . However , as ensure_ascii is an attribute of the default Encoder , I thought it might be better to just change it by setting it to FalseThis will actually set the property to False which I checked before returning . But somehow , the string is still returned without ' ß ' but with \u00df . How can this be , as all I 'm doing in my custom Encoder , is setting this attribute to False ?"
"I have dataframe with following columns : I am interested to find what is most common combination of name and surname and how much it occurs as well.Would be nice also to see list of top 10 combinations.My idea for top one was : But I think it is not giving me correct answer.Help would be much appreciated ! Thanks , Neb"
"I want to plot a chart with two subplots in plotly dash . My entire chart looks like this : What I need is a so called crosshair that is common in trading charts . Basically it consists of two lines that are connected to x and y axes and moves with cursor . This is a screenshot from tradingview.com charts : However in my chart there is a little icon that appears when the cursor is on candlesticks : What I have found out so far is that when the cursor is on the scatter plot , the icon disappears and it works fine . I think that is because I set hovertemplate= [ ] in the scatterplot . I can not do that in the candlestick plot because there is no such parameter for it . Moreover , this icon only appears if I set hovermode= ' x unified ' . If I set it to x , the little icon does n't appear . But I need it to be exactly like the tradingview.com example that I showed.Is there any way to replicate that crosshair ? UPDATE 1 : I tried fig.update_layout ( hoverdistance=0 ) . But the problem is that when the cursor is not on the candlesticks , the crosshair is just not right . I took two screenshots : the first one is from tradingview.com charts and the second one is from my code with hoverdistance set to 0.As can be seen , when the cursor is not on the candlesticks , in the first screenshot the crosshair is still correct . However , in the second screenshot it is just not working correctly . It only works if the cursor is on the candlesticks ONLY . I just want to copy tradingview.com crosshair . Nothing less and nothing more.UPDATE 2 : I think the answer could be on these plotly docs . I am working on it currently . Please share your comments about this update ."
"The equality operator '== ' gives False in the trivial case above . Is there any straightforward approach ? EDIT : Here 's a slightly improved version ( based on the accepted answer ) , which can also deal with nested arrays :"
"What is the best practice to throw an ArgumentTypeError exception from my own custom action and let the argparse to catch it for me ? It seems that argparse 's try/except block does not handle this exception for my custom actions . Though it does that just fine for its built-in actions.andFor example , in the code above If I would have duplicate IPs then the exception would fall down to the main function and print the ugly stacktrace . I do n't want that and neither I do n't want to put a try/except block inside main ."
"I have a pandas dataframe which has two columns key and value , and the value always consists of a 8 digit number something likeNow I need to take the value column and split it on the digits present , such that my result is a new data frameI can not change the input data format , the most conventional thing I thought was to convert the value to a string and loop through each digit char and put it in a list , however am looking for something more elegant and faster , kindly help . EDIT : The input is not in string , it is integer ."
"I have to create PDF file in which need to add lines at bottom left like footer.Following code is working : Now if text have multiple lines then this is not working because length of text is greater than width of Page size . Understood.I try with Frame and paragraph but still can not write text in correct position in a PDFFollowing is code : Not understand why size of page change , because I set ( 432 , 648 ) but is show ( 288.0 , 504.0 ) Also Frame size : Do not know how to fix this issue.I refer this link"
"Normally , list comprehensions are used to derive a new list from an existing list . Eg : Should we use them to perform other procedures ? Eg : or should I avoid the above and use the following instead ? :"
"Is there a simple way to hide the repeated axis titles in a faceted chart using plotly express ? I tried settingIn the code below , but that also hid the y axis tick labels ( the values ) . Ideally , I would like to set hiding the repeated axes titles a default for faceted plots in general ( or even better , just defaulting to showing a single x and y axis title for the entire faceted figure.Here is the test code : Final Code ( accepted answer ) . Note plotly > = 4.9"
"I have a file that contains lines in this type of format . I first split the line by ' : ' which gives me a list with 2 entries . I 'd like to split this line into a dictionary with a key and value , but where the score key has multiple sub keys with a value.So I am using something like this ... However I get an error on the score element of the line : I can adjust the split on '= ' to this , so it stops after the first '='However I lose the sub values within the curly brackets . Does anybody know how I can achieve this multi layer dictionary ?"
"I 'm a little curious about the difference between if and inline if , in Python . Which one is better ? Is there any reason to use inline if , other than the fact that it 's shorter ? Also , is there anything wrong with this statement ? I 'm getting a syntax error : SyntaxError : ca n't assign to conditional expression"
"The issueSo I have 50 netCDF4 data files that contain decades of monthly temperature predictions on a global grid . I 'm using np.mean ( ) to make an ensemble average of all 50 data files together while preserving time length & spatial scale , but np.mean ( ) gives me two different answers . The first time I run its block of code , it gives me a number that , when averaged over latitude & longitude & plotted against the individual runs , is slightly lower than what the ensemble mean should be . If I re-run the block , it gives me a different mean which looks correct.The codeI ca n't copy every line here since it 's long , but here 's what I do for each run.And I repeat that 49 times more for other datasets . Each tas11 , tas12 , etc file has the shape ( 1812 , 64 , 128 ) corresponding to time length in months , latitude , and longitude.To get the ensemble mean , I do the following.When I check a coordinate & month , the ensemble mean is off from what it should be . Here 's what a plot of globally averaged temperatures from 1950-2100 looks like with the first mean ( with monhly values averaged into annual values . Black line is ensemble mean & colored lines are individual runs.Obviously that deviated below the real ensemble mean . Here 's what the plot looks like when I run alltas [ : , : , : ,50 ] =np.mean ( alltas , axis=3 , dtype=np.float64 ) a second time & keep everything else the same.Much better.The questionWhy does np.mean ( ) calculate the wrong value the first time ? I tried specifying the data type as a float when using np.mean ( ) like in this question- Wrong numpy mean value ? But it did n't work . Any way I can fix it so it works correctly the first time ? I do n't want this problem to occur on a calculation where it 's not so easy to notice a math error ."
"If i write a would become [ [ 1,0 ] , [ 1,0 ] ] If I write a would become [ [ 1,0 ] , [ 0,0 ] ] Can anyone tell me why ?"
"In myria-python , we use setuptools with install_requires to configure which packages are needed . In our particular setup file , we include requests-toolbelt and requests in that list.When we create a new virtual environment and then run python setup.py install , it fails the first time with Can not find required distribution requests . This happens seemingly because pip identifies requests toolbelt-0.3.1 ( note the space ) as the right match for package requests.Running python setup.py install again seems to install requests after all.Here is a GitHub issue with a full log of the install process.Steps to reproduce : git clone https : //github.com/uwescience/myria-python.gitcd myria-pythonmkvirtualenv myria-pythonpython setup.pyThe entire lab seems to have this issue , however all of us use Mac OS X with either 10.9 or 10.10 installed . Here are my machine 's specs : OS X 10.10.1Python 2.7.9 ( default , Dec 10 2014 , 23:46:04 ) pip 1.5.6mkvirtualenv 1.11.6I was also able to duplicate it on one of our Ubuntu servers : Ubuntu 14.04.1 LTS \n \lPython 2.7.6pip 1.5.4mkvirtualenv 1.11.4Here is the tail of the error log : How can I fix this so that the package installs without running setup.py twice ?"
"I 'm trying to write a Python program that could be extended by third parties . The program will be run from the command line with whatever arguments are supplied.In order to allow third parties to create their own modules , I 've created the following ( simplified ) base class : Any arguments that they supply via get_args ( ) will be added to a subparser for that particular module . I want them to be able to specify any type of argument.I 'm not sure of the best way to declare and then get the arguments from the subclassed modules into my main program . I successfully find all subclasses of MyBaseClass and loop through them to create the subparsers , but I can not find a clean way to add the individual arguments to the subparser.Here is the current code from the main program : How can I best specify the arguments in the external modules via get_args ( ) or similar and then add them to the subparser ? One of my failed attempts looked like the following , which does n't work because it tries to pass every possible option to add_argument ( ) whether it has a value or is None :"
"I want to match amount like Rs . 2000 , Rs.2000 , Rs 20,000.00 ,20,000 INR 200.25 INR . Output should be2000,2000,20000.00,20000,200.25The regular expression i have tried is thisBut it is not matching numbers with inr or rs after the amountI want to match it using re library in Python ."
"In my database , I have a record where the year field is 2016 but I need to change it to 2017 . When I use Django admin to change it 2017 , I get `` Ensure this value is less than or equal to 2016. '' . What is wrong w/ my model ?"
I have the following code : I was wondering how many times does Python sort the list using the sorted ( ) method ? Is it for each iteration or just once ? In other words is the following code more efficient ?
"So , I have a Web application at work that need to gather information and build some reports and run some basic data analysis.The thing is that I 'm a complete newbie to HTML , Ajax ( Asynchronous JavaScript and XML ) , Python and Selenium.What I gather so far is this : Ajax nature is to perform asynchronous Web Browser activities and in my case , sending server requests to push/pull some dataSelenium handles the asynchronous events performing Wait actions like : time.sleep ( 'time in ms ' ) # using the time library . So not REALLY Selenium ; Explicit Waits : you define to wait for a certain condition to occur before proceeding further in the code ; EC stands for expected conditions represented by : title_is ; title_contains ; presence_of_element_located visibility_of_element_located visibility_of presence_of_all_elements_located text_to_be_present_in_element text_to_be_present_in_element_value frame_to_be_available_and_switch_to_it invisibility_of_element_located element_to_be_clickable staleness_of element_to_be_selected element_located_to_be_selected element_selection_state_to_be element_located_selection_state_to_be alert_is_present Implicit Waits : tell WebDriver to poll the DOM ( Document Object Model ) for a certain amount of time when trying to find an element or elements if they are not immediately available ; driver.implicitly_wait ( 10 ) -Executing JavaScript using Java and applies wait : j Query keeps a count of how many Ajax calls are active in its query.active variable ; FluentWait : FluentWait option to handle uncertain waits ; WebdriverWait : use ExpectedCondition and WebDriverWait strategy.What to use since I have the following situation : Button to send a clear request via Ajax.This is the event of the button : function ( a ) { ! e._instance.btn.disabled & & c.ui.executeEventHandlingFunction ( e , e._proto.EVT_ONCLICK ) & & ( e._instance.multiClicks || ( e._instance.btn.disabled = ! 0 , f.add ( e._instance.btn , `` disabled '' ) ) , e.context.binding & & e.context.binding.set ( `` value '' , ! 0 ) , e.context.trigger ( function ( a ) { e._instance.btn.disabled = ! 1 ; f.remove ( e._instance.btn , `` disabled '' ) ; setTimeout ( function ( ) { c.ui.executeEventHandlingFunction ( e , e._proto.EVT_ONBOUNDARYEVT , a.status ) } ) } , { callBackForAll : ! 0 } ) ) } Then , my network informs that the ajaxCoach proceeds to the following requestsIs it possible to selenium to see/find if an AJAX action concluded the page actualization action in Python ?"
"We have a folder with 50 datafiles ( next-gen DNA sequences ) that need to be converted by running a python script on each one . The script takes 5 hours per file and it is single threaded and is largely CPU bound ( the CPU core runs at 99 % with minimal disk IO ) . Since I have a 4 core machine , I 'd like to run 4 instances of this script at once to vastly speed up the process . I guess I could split the data into 4 folders and in run the following bash script on each folder at the same time : But there must be a better way of running it on the one folder . We can use Bash/Python/Perl/Windows to do this . ( Sadly making the script multi threaded is beyond what we can do ) Using @ phs xargs solution was the easiest way for us to solve the problem . We are however requesting the original developer implements @ Björn answer . Once again thanks !"
"this question relates to performance penalities that may or may not arise from having a large number of sleeping python threads on a webserver.Background : I am implementing an online shop using django/satchmo . A requirement is for delayed payment . The customer can reserve a product and allow a third party to pay for it at a later date ( via a random and unique URL ) .To handle unreserving an item I am creating a thread which will sleep for the reservation time and then remove the reservation/mark the product as sold when it awakes . It looks like this : I am using the same technique when culling the unique URLs after they have expired , only the Timer sleeps for much longer ( typically 5 days ) .So , my question to you SO is as follows : Is having a large numnber of sleeping threads going to seriously effect performance ? Are there better techniques for scheduling a one off event sometime in the future . I would like to keep this in python if possible ; no calling at or cron via sys.The site is n't exactly high traffic ; a ( generous ) upper limit on products ordered per week would be around 100 . Combined with cart reservation , this could mean there are 100+ sleeping threads at any one time . Will I regret scheduling tasks in this manner ? Thanks"
"I am working on a django based web app that takes python file as input which contains some function , then in backend i have some lists that are passed as parameters through the user 's function , which will generate a single value output.The result generated will be used for some further computation.Here is how the function inside the user 's file look like : At present the approach that i am using is taking user 's file as normal file input . Then in my views.py i am executing the file as module and passing the parameters with eval function . Snippet is given below.Here modulename is the python file name that i had taken from user and importing as moduleWhich is working absolutely fine . But i know this is not the secured approach.My question , Is there any other way through which i can run users file securely as the method that i am using is not secure ? I know the proposed solutions ca n't be full proof but what are the other ways in which i can run this ( like if it can be solved with dockerization then what will be the approach or some external tools that i can use with API ) ? Or if possible can somebody tell me how can i simply sandbox this or any tutorial that can help me.. ? Any reference or resource will be helpful ."
"I need to write a very `` high '' two-column array to a text file and it is very slow . I find that if I reshape the array to a wider one , the writing speed is much quicker.For exampleWith the same number of elements in the three data matrixes , why is the last one much more time-consuming than the other two ? Is there any way to speed up the writing of a `` high '' data array ?"
"When I make the `` data '' variable a class variable , the following works , but when I make it an object variable , the descriptor is not called . Please help ."
I have the impression that ( using setuptools ) : Wo n't use wheels when installing required packages ( specified in install_requires ) .Questions : is my impression correct ? is there a way to force it to use wheel ? I am talking about this particular setup script .
"I have deploy an application on Heroku but the problem is when my application send email it does n't append the name of my server in the URL : But the link I 'm receiving in my email is : I have problems with this URL : It is not https and it should be since it 's host on herokuThe server name does n't appear in the URL only /// like if the server name was blankWhat should I do to have the correct URL https : //my-server-name/register_account ... ? EDITI tried to set in my config.py file with the following variable to : SERVER_NAME = `` http : //my-server-58140.herokuapp.com '' It generated errors in my path and I could n't access any URL for example , the following one could be access before but when defining my SERVER_NAME it did n't anymore : EDITMy flask application is configured : Where the environnement variable is set to 0.0.0.0:5000 on my localhost and : my-server-58140.herokuapp.com on my production server"
pytest fails to import modules even though these modules import fine under the vanilla python interpreter . Especially modules imported from conftest fails to resolve . ImportErrori is raised when pytest tries to rewrite assert information in modules.An example traceback : How to resolve ?
"I want to serve a Django application that serves multiple web sites by single database but different user sets . Think like a blog application , it will be used by several domains with different themes , but use same database by adding a site field to models.I use Django 's SitesFramework for that job . But the problem is , I could n't separate user models for different sites . I want to use same user model with a site field and email field that unique per site.I tried to extend AbstractUser model like that : But gives that error : 'Member.email ' must be unique because it is named as the 'USERNAME_FIELD'.What is the best practice for that issue ?"
"Consider the following XML exampleHere I would like to get an ( R or Pandas ) dataframe from this XML that contains the columns name and hobby . However , as you see , there is an alignment problem because hobby is missing in the second node and John has two hobbies.in R , I know how to extract specific values one at a time , for instance using xml2 as follows : but how can I align this data correctly in a dataframe ? That is , how can I obtain a dataframe as follows ( note how I join with a | the two hobbies of John ) : In R , I would prefer a solution using xml2 and dplyr . In Python , I want to end-up with a Pandas dataframe . Also , in my xml there are many more variables I want to parse . I would like a solution that has allows the user to parse additional variables without messing too much with the code.Thanks ! EDIT : thanks to everyone for these great solutions . All of them were really nice , with plenty of details and it was hard to pick up the best one . Thanks again !"
"I 've just come across Python decorators . Just out of interest , can you apply your own decorator to a built-in object method somehow ? Say I wanted to apply this : To this : in order to remove empty strings . Is it possible ? Or even a good idea ?"
"sklearn provides LASSO method for regression estimation . However , when I try to fit LassoCV ( X , y ) with y a matrix , it throws an error . See screenshot below , and the link for their documentation . The sklearn version I am using is 0.15.2.http : //scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html # sklearn.linear_model.LassoCVIts document says y can be a ndarray : When I use just Lasso ( ) to fit the same X , and y , it works fine . So I wonder if the LassoCV ( ) is broken or I need to do something else ? It seems that ElasticCV ( ) and Elastic ( ) pair has the same situation , the former ( ) suggest to use multitask-ElasticCV ( ) and the latter works fine for 2d matrix ."
"I 've got a collection of O ( N ) NxN scipy.sparse.csr_matrix , and each sparse matrix has on the order of N elements set . I want to add all these matrices together to get a regular NxN numpy array . ( N is on the order of 1000 ) . The arrangement of non-zero elements within the matrices is such that the resulting sum certainly is n't sparse ( virtually no zero elements left in fact ) .At the moment I 'm just doing which works but is a bit slow : of course the sheer amount of pointless processing of zeros which is going on there is absolutely horrific.Is there a better way ? There 's nothing obvious to me in the docs.Update : as per user545424 's suggestion , I tried the alternative scheme of summing the sparse matrices , and also summing sparse matrices onto a dense matrix . The code below shows all approaches to run in comparable time ( Python 2.6.6 on amd64 Debian/Squeeze on a quad-core i7 ) and logs outalthough you can get one approach or the other to come out ahead by a factor of 2 or so by messing with N , S , D parameters ... but nothing like the order of magnitude improvement you 'd hope to see from considering the number of zero adds it should be possible to skip ."
"I 'm trying to identify columns which contain dates as strings in order to then convert them to a better type ( DateTime or something numeric like UTC ) . The date format used is 27/11/2012 09:17 which I can search for using a regex of \d { 2 } /\d { 2 } /\d { 4 } \d { 2 } : \d { 2 } .My current code is : I 'm sure this is not taking advantage of the capabilities of pandas . Is there a better way , either to identify the columns , or to convert them to DateTime or UTC timestamps directly ?"
"How can I get the name of the original function ? result : wrapped_f , but I need A !"
"Say I have code from multiple languages in a single buffer , can I have emacs syntax highlight each snippet according to its corresponding language ? For example , the following code is part of a python script , but it contains SQL code : It would be great to have the SQL part highlighted as SQL syntax , while the rest of the file highlighted as Python . Is this possible in Emacs ? Any way to hint to Emacs what highlighter to use , perhaps taking advantage of the comments ?"
I want to use word boundary in a regex for matching some unicode text . Unicode letters are detected as word boundary in Python regex as here : What should I do in order to make the word boundary symbol not match unicode letters ?
The HDF5 format apparently does not support categoricals with format= '' fixed '' . The following exampleReturns the error : NotImplementedError : Can not store a category dtype in a HDF5 dataset that uses format= '' fixed '' . Use format= '' table '' .How do I construct the categorical series with format='table ' ?
"I have used pymouse to help automate repetitive games in the past with success.However , when playing a game downloaded on BlueStacks , pymouse will appear to move to the correct position on the screen , but then no clicks will `` register '' . If I put something else native to the OS in the same position , it will be clicked . I do not understand why the clicks dont `` work '' in this case when I move the mouse to a position over the game being played in Bluestacks.Here is the code : This ( below ) will return values even if the mouse is hovered over the window from Bluestacks ."
"I am trying to do a dataframe transformation that I can not solve . I have tried multiple approaches from stackoverflow and the pandas documentation : apply , apply ( lambda : ... ) , pivots , and joins . Too many attempts to list here , but not sure which approach is the best or if maybe I tried the right approach with the wrong syntax.Basically , I have a dataframe , and I need to 1 ) offset the columns , 2 ) the number of columns to offset by varies and depends on a variable in the dataframe , 3 ) create columns at the end of the dataframe where needed to accommodate the offset , and 4 ) place zeros in the newly created intervals.This data set will have c. 500 rows and c. 120 columns . The amount of the offset will very between 0-12 . I thought about doing this with base Python functions , but I also found that difficult and the time consumer by the program would defeat the ultimate purpose which is to remove some tasks being done in Microsoft Excel . I complain a lot about how Excel is inferior for big tasks like this , but it seems so far that the current spreadsheet offset ( ) function in excel does do this in a very easy to use way but with thousands of formulas , is very slow . I have sold my workplace on the benefits of Python over Excel , and this is my first real trial , so speed is very important to me because I 'm trying to convince my colleagues that Python can gobble up this spreadsheet much quicker than the current excel file weighing in a 96Mb in file size.I came pretty close with the melt ( ) function , and then taking the former column numbers and added the offset to them . However , I 've had a lot of problems trying to reform the dataframe using pivot . No luck with apply or apply ( lambda ) ! Thanks for any help anyone can give !"
"I have a lot of callable objects and they all have the __doc__ string correctly filled out , but running help on them produces the help for their class instead of help based on __doc__.I want to change it so that running help on them produces customized help that looks essentially like what I would get if they were actual functions instead of instances of a class that implements __call__.In code , I 'd like to make the output of this : Look more like the output of this :"
"I 'm trying to use Python 's exec in a project to execute embedded Python code.The problem I 've encountered is that variables created at the module-level in an exec statement are inaccessible from functions defined in the same module.Let 's say you have the following Python program : If you put the above four lines in a file and ran it , it would work no problemo.However , if you try running this same piece of code from within an exec statement , it wo n't work.Here 's our previous program , inside an exec statement : On execution , instead of working , it yields the following error : I thought module-level variables were stored globally , but it seems that , at least in exec , they 're not.For example , in the previous example , if you replace the call to foo ( ) with : You get : So anything defined in the module-level ( including x ) is stored in locals ( ) .But it 's impossible to access x from anywhere except the module-level ( of the exec statement ) . In particular , as we saw above , the local scope of x is invisible to functions defined in the same exec statement.WorkaroundsI 've found two ways to workaround this issue , and make x accessible again.The first one is using the global keyword in the function : The second one is using the same dictionary for globals ( ) and locals ( ) in exec : However , these are just half-fixes/workarounds that do n't address the original issue.So my questions are : Why are module-level variables in an exec stored locally , and why are inaccessible from anywhere but the module-level ? Some closely related StackOverflow posts : globals and locals in python exec ( ) Can not change global variables in a function through an exec ( ) statement ?"
"I have a large DataFrame , which I would like to split into a test set and a train set for model building . However , I do not want to duplicate the DataFrame because I am reaching a memory limit.Is there an operation , similar to pop but for a large segment , that will simultaneously remove a portion of the DataFrame and allow me to assign it to a new DataFrame ? Something like this :"
"My Softwares : Python 3.4 -64 bit PyODBC 64 bitMS office package Installed ( 32 bit ) Problem : Now , I try to access MS Access 2010 installed in my computer using PYODBC . It does not work regardless of what I try . My error is always this : I already looked at : pyodbc and ms access 2010 connection errorCannot connect to Access DB using pyodbcIt is suggested that it is 32-64 bit problem which I am sure it is . Is there some modification that can be done to make it work without installing 32 bit python and 32 bit pyodbc ? I checked this website.Using ODBC ( 32-bit and 64-bit ) on 64-bit Windows which explains about accessing the control panel to modify ODBC connnection and/or drivers . However , I do n't know much about windows database connection internals to commence some changes.Is there something that I can do to make it work ?"
"In Matlab , I can do the following : I often find myself wanting to quickly collapse the trailing dimensions of an array , and do not know how to do this in numpy.I know I can do this : but x.reshape ( x.shape [ : -2 ] + ( -1 , ) ) is a lot less concise ( and requires more information about x ) than simply doing x ( : , : ) .I 've obviously tried the analogous numpy indexing , but that does not work as desired : Any hints on how to collapse the trailing dimensions of an array in a concise manner ? Edit : note that I 'm after the resulting array itself , not just its shape . I merely use size ( ) and x.shape in the above examples to indicate what the array is like ."
"I 'm using opencv remap function to map an image to another coordinate system.However , my initial tests indicate that there are some issues with the interpolation . Here , I give a simple example of a constant 0.1 pixel shift for a image that is 0 everywhere but at position [ 50,50 ] .givesHowever , I would expect 0.9 and 0.1 instead , given the linear interpolation method . Am I doing something wrong or is this some numeric issue ? Are there any more precise remapping algorithms around ? Thanks ."
"I am a newb to PyPI ... so let me qualify with that . I am trying to put a package on PyPI but having a bit of trouble when I try to install it with pip . When I upload the file to PyPI , I get a warning ( but the setup.py script finishes with not fatal errors and a 200 status ) : And then when I go to install it in pip , I get an error : From other answers on SO , I 've tried changing up my MANIFEST.in and my setup.py files , with no luck . Here is my current MANIFEST.in : and setup.py : EDIT : I 've also tried leaving out the MANIFEST.in file just to see if that was messing anything up but I get the same result ."
"I am trying to use StringIO to feed ConfigObj.I would like to do this in my unit tests , so that I can mock config `` files '' , on the fly , depending on what I want to test in the configuration objects.I have a whole bunch of things that I am taking care of in the configuration module ( I am reading several conf file , aggregating and `` formatting '' information for the rest of the apps ) . However , in the tests , I am facing a unicode error from hell . I think I have pinned down my problem to the minimal functionning code , that I have extracted and over-simplified for the purpose of this question.I am doing the following : It produces the following traceback : I am using Python-2.7.2 ( 32 bits ) on linux . My locale for the console and for the editor ( Kile ) are set to fr_FR.utf8.I thought I could do this.From the io.StringIO documentation , I got this : The StringIO object can accept either Unicode or 8-bit strings , but mixing the two may take some care.And from ConfigObj documentation , I can do this : and this : infile : NoneYou do n't need to specify an infile . If you omit it , an empty ConfigObj will be created . infile can be : 'encoding ' : NoneBy default ConfigObj does not decode the file/strings you pass it into Unicode [ 8 ] . If you want your config file as Unicode ( keys and members ) you need to provide an encoding to decode the file with . This encoding will also be used to encode the config file when writing.My question is why does it produce this ? What else did I not understand from ( simple ) Unicode handling ? ... By looking at this answer , I changed : to ( importing codecs module breforehand ) : in order to get rid of possible included byte order mark , but it did not help.Thanks a lotNB : I have the same traceback if I use StringIO.StringIO instead of io.StringIO ."
"This feels like a bug to me . I am unable to replace a character in a string with a single backslash : I know that '\ ' is n't a legitimate string because the \ escapes the last '.However , I do n't want the result to be ' a\\b ' ; I want it to be ' a\b ' . How is this possible ?"
"Plus equal is giving a different answer than assigning to the explicit sum ( which is the answer one expects ) , when slicing is involved . Is there a reason for this ? Should plus equal be avoided ?"
"I have a set of real data and I want use this data to find a probability distribution and then use their property to generate some random points according to their pdf . A sample of my data set is as following : How could I do this ? My first attempt was to fit a polynomial to the binned data and find the probability distribution of weights in each magnitude bin , but I reckon it might be a smarter way to do it . For instance , using scipy.stats.rv_continuous for sampling data from the given distribution but I do n't know how it can work and there are not enough examples.Update : As I got a lot of comments to use KDE , I used scipy.stats.gaussian_kde and I got the following results . I am wondering whether it is a good probability distribution to represent the property of my data ? First , how could I test it , and second , whether there is a possibility to fit more than one gaussian kde with scipy.stats ?"
"I 'm using the excellent pandas package to deal with a large amount of varied meteorological diagnostic data and I 'm quickly running out of dimensions as I stitch the data together . Looking at the documentation , it may be that using the MultiIndex may solve my problem , but I 'm not sure how to apply it to my situation - the documentation shows examples of creating MultiIndexes with random data and DataFrames , but not Series with pre-existing timeseries data.BackgroundThe basic data structure I 'm using contains two main fields : metadata , which is a dictionary consisting of key-value pairs describing what the numbers aredata , which is a pandas data structure containing the numbers themselves.The lowest common denominator is timeseries data , so the basic structure has a pandas Series object as the data entry , and the metadata field describes what those numbers actually are ( e.g . vector RMS error for 10-meter wind over the Eastern Pacific for a 24-hour forecast from experiment Test1 ) .I 'm looking at taking that lowest-common-denominator and gluing the various timeseries together to make the results more useful and allow for easy combinations . For instance , I may want to look at all the different lead times - I have a filter routine that will take my timeseries that share the same metadata entries except for lead time ( e.g . experiment , region , etc . ) and return a new object where the metadata field consists of only the common entries ( i.e . Lead Time has been removed ) and now the data field is a pandas DataFrame with the column labels given by the Lead Time value . I can extend this again and say I want to take the resulting frames and group them together with only another entry varying ( e.g . the Experiment ) to give me a pandas Panel . for my entry where the item index is given by the Experiment metadata values from the constituent frames and the object 's new metadata does not contain either Lead Time or Experiment.When I iterate over these composite objects , I have an iterseries routine for the frame and iterframes routine for the panel that reconstruct the appropriate metadata/data pairing as I drop one dimension ( i.e . the series from the frame with lead time varying across the columns will have all the metadata of its parent plus the Lead Time field restored with the value taken from the column label ) . This works great.ProblemI 've run out of dimensions ( up to 3-D with a Panel ) and I 'm also not able to use things like dropna to remove empty columns once everything is aligned in the Panel ( this has led to several bugs when plotting summary statistics ) . Reading about using pandas with higher-dimensional data has led to reading about the MultiIndex and its use . I 've tried the examples given in the documentation , but I 'm still a little unclear how to apply it to my situation . Any direction would be useful . I 'd like to be able to : Combine my Series-based data into a multi-indexed DataFrame along an arbitrary number of dimensions ( this would be great - it would eliminate one call to create the frames from the series , and then another to create the panels from the frames ) Iterate over the resulting multi-indexed DataFrame , dropping a single dimension so I can reset the component metadata.Edit - Add Code SampleWes McKinney 's answer below is almost exactly what I need - the issue is in the initial translation from the Series-backed storage objects I have to work with to my DataFrame-backed objects once I start grouping elements together . The Data-Frame-backed class has the following method that takes in a list of the series-based objects and the metadata field that will vary across the columns . Once I have the frame given by this routine , I can easily apply the various operations suggested below - of particular utility is being able to use the names field when Icall concat - this eliminates the need to store the name of the column key internallysince it 's stored in the MultiIndex as the name of that index dimension.I 'd like to be able to implement the solution below and just take in the list of matching Series-backed classes and a list of keys and do the grouping sequentially . However , I do n't know what the columns will be representing ahead of time , so : it really does n't make sense to me to store the Series data in a 1-D DataFrameI do n't see how to set the name of the index and the columns from the the initial Series - > Frame grouping"
"I would like to use custom intermediate table for products and its attributes . I have the following model defined , in views Adding Products objects to context and from template trying to get the attributes by saying I am getting the name but the value does n't showup . I tried inspecting sql statement getting executed.the value is in 'attributes_mapping ' table , Select statement has a reference but not selecting that field.Thanks in advance for any help or suggestions ."
"I 'm writing a toy meeting-point/relay server listening on port 5555 for two clients `` A '' and `` B '' . It works like this : every byte received by the server from the firstly-connected client A will be sent to the secondly-connected client B , even if A and B do n't know their respective IP : This code is currently working : and you can test it by launching it on a server , and do two netcat connections to it : nc < SERVER_IP > 5555 . How can I then pass the information to the clients A and B that they can talk directly to each other without making the bytes transit via the server ? There are 2 cases : General case , i.e . even if A and B are not in the same local networkParticular case where these two clients are in the same local network ( example : using the same home router ) , this will be displayed on the server when the 2 clients will connect to the server on port 5555 : Remark : a previous unsuccesful attempt here : UDP or TCP hole punching to connect two peers ( each one behind a router ) and UDP hole punching with a third party"
"Why do we have to use __getitem__ rather than the usual operator access ? We get TypeError : 'super ' object is not subscriptable.Instead we must use super ( ) .__getitem__ ( key ) , but I never fully understood why - what exactly is it that prevented super being implemented in a way that would allow the operator access ? Subscriptable was just an example , I have the same question for __getattr__ , __init__ , etc.The docs attempt to explain why , but I do n't understand it ."
I want to make a custom layer which is supposed to fuse the output of a Dense Layer with a Convolution2D Layer.The Idea came from this paper and here 's the network : the fusion layer tries to fuse the Convolution2D tensor ( 256x28x28 ) with the Dense tensor ( 256 ) . here 's the equation for it : y_global = > Dense layer output with shape 256y_mid = > Convolution2D layer output with shape 256x28x28Here 's the description of the paper about the Fusion process : I ended up making a new custom layer like below : I think I got the __init__ and build methods right but I do n't know how to concatenate y_global ( 256 dimesnions ) with y-mid ( 256x28x28 dimensions ) in the call layer so that the output would be the same as the equation mentioned above.How can I implement this equation in the call method ? Thanks so much ... UPDATE : any other way to successfully integrate the data of these 2 layers is also acceptable for me ... it does n't exactly have to be the way mentioned in the paper but it needs to at least return an acceptable output ...
"If you compile a regex inside a function , and that function gets called multiple times , does Python recompile the regex each time , or does Python cache the compiled regex ( assuming the regex does n't change ) ? For example :"
"BackgroundFor some background , I 'm trying to create a tool that converts worksheets into API calls using Python 3.5For the conversion of the table cells to the schema needed for the API call , I 've started down the path of using javascript like syntax for the headers used in the spreadsheet . e.g : Worksheet Header ( string ) Python DictionaryIt 's also possible that the header schema could have nested arrays/dicts : And I also need to append to the object after it has been created as I go through each header.What I 've triedadd_branchBased on https : //stackoverflow.com/a/47276490/2903486 I am able to get nested dictionaries setup using values like one.two.three.four and I 'm able to append to the existing dictionary as I go through the rows but I 've been unable to add in support for arrays : My own version of add_branchWhat still needs helpMy branch solution works pretty well actually now after adding in some things but I 'm wondering if I 'm doing something wrong/messy here or if theres a better way to handle where I 'm editing nested arrays ( my attempt started in the if IsInArray section of the code ) I 'd expect these two headers to edit the last array , but instead I end up creating a duplicate dictionary on the first array : Outputs : Instead of :"
"Question more or less says it all . from my understanding , this should be using the cuda pow function instead , but it is n't ."
"In Python 2.7 and 3.x , why does integer division give me a non-correct number when dividing by a number 0 < x < 1 ? Negative numbers -1 < x < 0 even work correctly : I understand that integer division with a negative ( or positive ) number rounds toward negative infinity , however I would have thought 1//.1 should result in 10.0 since 1 can be divided by .1 without remainder ."
"I have two dataframes : I need to concatenate them across index , but I have to preserve the index of the first dataframe and continue it in the second dataframe , like this : My guess is that pd.concat ( [ df1 , df2 ] , ignore_index=True ) will do the job . However , I 'm worried that for large dataframes the order of the rows may be changed and I 'll end up with something like this ( first two rows changed indices ) : So my question is , does the pd.concat with ignore_index=True save the index succession within dataframes that are being concatenated , or there is randomness in the index assignment ?"
"Problem : Given a number n , is there an efficient algorithm to obtain a list of 2-combinations from the set { 1 ... n } , sorted by the value of the product of the combination ? I need this in order to determine the largest product of two *-digit numbers that satisfies a certain condition . If the list is unsorted , I must first determine all combinations that satisfy the condition , then iterate through those to find the combination with the largest product , which is inefficient.As an example , given n = 3 , the combinations possible are : Sorted by the value of the product in descending order , this is : Extra background : I just solved a Project Euler question regarding finding the largest palindromic number that is a product of two 3 digit numbers . My approach was to iterate downward from 999 ( the largest 3 digit number ) with two factors and find the product of each combination , additionally checking whether the number was palindromic : Note that the first list in the example iterates over factors in exactly the same order as this code . My initial assumption was that since I was iterating downwards , the first palindrome I found would be the largest one . This is clearly not the case , since j iterates all the way to 100 before i is decremented.I am looking for a way to iterate such that the values are yielded are in descending order , since this allows me to get the answer simply by invoking next ( maxpal ) once , which is much more efficient.EDIT : In the interests of not disqualifying a good answer in a language that is n't Python , I 'm OK with an attempt in any language as long as you explain it so that I ( or anyone else ) can sufficiently understand it ."
"I followed the instructions here to create a onefile flask-app deployed to apache2 with mod-wsgi on ubuntu . That all works fine when using the original flask app . However , when adding import nltk to the flask app apache hangs ( no 500 ) .I use python 2.7 and nltk 2.0.4Others seem to have had similar problems with other packages . Setting in the VirtualHost configuration seemed to have helped . However , I still get the same behavior . Did anybody run into the same issue ? Thanks for the help ! Here is the VirtualHost Configuration file : Here is the modified flask code"
"Is it possible to write a Jinja2 Extension that , while rendering , has access to the template context ? I want to write an extension that accesses a context variable and outputs some data based on that variable . I could n't find enough information on how to write such an extension.Right now , I have this : But , in foo.jinjaI get I thought I 'd get a NameError since somehow_get_variable ( ) is not defined . I need to know a ) how to get the variable from the current context , and b ) how to write the Extension correctly.Also , why line 7 ? The { % csrf % } tag is on line 5 . Even when I trim foo.jinja to have only one line with the { % csrf % } tag in it , it says line 7 ."
"Does anybody know how to zoom an element in android via appium python client ? I am currently usingself.driver.zoom ( self.element , percent ) but this gives an errorI also tried through MultiAction.But again this does not perform any zoom.Instead it scrolls down the list.Does anybody have any idea about what 's wrong here.Appium Logs"
"I am trying to remove a user from a security group using Python and pywin32 , but so far have not been successful . However I am able to add a user to a security group . The error is below : I have also tried adding using GetObject to get the user and remove it that way , however I get the same error . Any help would be much appreciated as I 've hit a dead-end here.EDITI have also now tried using Tim Golden 's active_directory module to try and remove the group member.However this also does n't work , and I encounter the below error.EDITWherby suggested that I change to Python 2.7 and give that a go . I have just tried this : ... but I 'm still getting an errorThe user and group are definitely found correctly , as I can print their LDAP paths using print user.path ( ) and print group.path ( ) Are there any other active directory libraries for Python 3.3 that anyone can recommend ?"
"I am new to Python . So , please forgive me if this is a basic question . I researched this topic on the Internet and SO , but I could n't find an explanation . I am using Anaconda 3.6 distribution.I am trying to create a simple getter and setter for an attribute . I will walk you through the errors I get . This prints the first name I agree that I have n't overridden print or getattribute method . Also , there is no property here . This was to test whether the basic code works.Let 's modify the code to add property : As soon as I write above code in PyCharm , I get a yellow bulb icon , stating that the variable must be private . I do n't understand the rationale . Ignoring above , if I run above code , I get : Now , I researched this topic , and I found that there are two fixes ( without knowing why this works ) : Fix # 1 : Change the variable name to _nameThis works well in that it prints the output correctly . Fix # 2 : Change property name to from name ( self ) to _name ( self ) and revert variable name from _name to nameNow , this works prints as expected . As a next step , I created setter , getter , and deleter properties using decorators . They follow similar naming conventions as described above -- i.e . either prefix _ to the variable name or the method name : Question : I am not really sure why Python 3.x is enforcing adding _ to variable name or method name . As per Python property with public getter and private setter , What is the difference in python attributes with underscore in front and back , and https : //www.python.org/dev/peps/pep-0008/ # naming-conventions , an underscore prefix is a weak indicator to the user that this variable is a private variable , but there is no extra mechanism in place ( by Python , similar to what Java does ) to check or correct such behavior . So , the big question at hand is that why is it that I need to have underscores for working with properties ? I believe those underscore prefixes are just for users to know that this is a private variables.I am using Lutz 's book to learn Python , and above example is inspired from his book ."
"The following code in python2.6 throws syntax errorbut this syntax is valid in python3.0 . I would like to know what should I import in my interpreter to make it work.ie . from import __future__ ? ? ? ? for importing print function of 3.0 , I would do from __future__ import print_functionsimilarly this defination is invalid in 2.6while it is legal in 3.0"
"When having a Pandas DataFrame like this : But with about 100 000 entries , I am looking to find the additions and removals of those lists in the two columns on a row-wise basis.It is comparable to this question : Pandas : How to Compare Columns of Lists Row-wise in a DataFrame with Pandas ( not for loop ) ? but I am looking at the differences , and Pandas.apply method seems not to be that fast for such many entries.This is the code that I am currently using . Pandas.apply with numpy 's setdiff1d method : This works fine , however it takes about a minute for 120 000 entries . So is there a faster way to accomplish this ?"
"Is there any way to do a `` reinterpret_cast '' with numpy arrays ? Here 's an example : I can call tostring ( ) and then fromstring ( ) to convert from an array to raw bytes and then back to another array . I 'm just wondering if there 's a way for me to skip the intermediate step . ( not that it 's a big deal , I would just like to understand . )"
"I 'm trying to do some computations on date , I have a timedelta object , and I want to get the number of seconds . It seems like dt.total_seconds ( ) does exactly what I need but unfortunately it was introduced in Python 2.7 and I 'm stuck with an older version.If I read the official documentation , it states the following : Return the total number of seconds contained in the duration . Equivalent to ( td.microseconds + ( td.seconds + td.days * 24 * 3600 ) * 10**6 ) / 10**6 computed with true division enabled.And after looking at the source of the datetime module ( in C ) , I see something like this : So while the computation of total_seconds ( ) seems trivial , that leaves me wondering what this true division actually means . I could n't find any info on the topic . What happens if I just use regular division , why do we need this true division and what does it do ? Can I just write total_seconds ( ) in Python with the equivalent given in the doc ?"
"Suppose you have the following In some python packages , if you import b , you only get the symbols defined in b . To access b.c , you have to explicitly import b.c or from b import c. In other words , you have toIn other cases I saw an automatic import of all the subpackages . This means that the following code does not produce an errorbecause b/__init__.py takes care of importing its subpackages.I tend to prefer the first ( explicit better than implicit ) , and I always used it , but are there cases where the second one is preferred to the first ?"
"In pandas documentation one can read `` Under the hood , these frequency strings are being translated into an instance of pandas DateOffset '' when speaking of freq string such as `` W '' or `` W-SUN '' .Then , how can I get an instance of a DateOffset given a string ? Ultimately want to configure my program with frequency as string ( say `` W-SUN '' ) , but internally want to do something like but defining offset from string.Thanks"
"I 'm currently using Pint to handle units and unit conversions . This seems to work well for the units that are already defined in Pint , for exampleI tried to define my own units , which represent percentage . As far as unit conversions go , a percentage is simply 100 times a dimensionless fraction , which is how I defined it.However I can not seem to convert back and forth between fraction ( 'dimensionless ' ) and 'pct'.What I 'd essentially like to do is be able to convert between e.g . `` 0.73 '' and `` 73 % '' . How can I define and use such a unit ?"
"i have the following dataframe : Since it is not clear from the above pasted dataframe , below is a snapshot : The months are in 1,2 3 ... Is it possible to rename the month index to Jan Feb Mar format ? Edit : I am having a hard time implementing the example by @ ChihebNexusMy code is as follows since it is a datetime :"
"I do have some more complex projects where if you run pytest -- collect-only you will endup with lots of import failures caused by discovered test files that have imports of stuff that were not installed yet.I want to change these test files in such way that they would not fail on collection . This is because the user may want to run tests using a specific pattern like pytest -k foo , but he would not be able to do this if collection fails on unrelated tests.I know that I can define a pytest_configure method that would be called durind collection but if I move the imports into it I would still get failures later when the interpreter is reaching code trying to use the missing imports.My example is clearly over similified as we all know that I could add the import again in the test method but I do not want to do this in tens of methods . I am looking for a cleaner solution ."
"I 'm using IPython.I declared a simple class MyClass ( object ) , and in the console , when using the name MyClass , after the dot operator , i hit Tab.One of the first suggestions i got was mro , meaning MyClass.mro . I hit enter , and the output i get is : Now i did n't define this method , and of course it returns the method resolution order of my class.This method does n't appear in the list returned by dir ( MyClass ) , and so here comes my question : How would I find any other such hidden features of classes or other objects ?"
"I find conda install faster ( something like numpy ) however I have to sometimes do pip install to get versions that 's not available in conda.Is it safe to mix them inside conda environment ? So I did ,"
"I 'm running Django 1.6.6 on production and have recently upgraded to 1.9.7 on staging ( dev server ) . This update was performed on the server and I followed the steps outlined here Upgrading from South.I noticed that the structure of the migration files have changed , and they no longer include a create statement . This causes issues because if I pull this new code from my GitHub repo and run python manage.py makemigrations or python manage.py migrate , it says : django.db.utils.OperationalError : no such table : appname_modelnameThe traceback points to my urls.py because I 'm referencing the model in a queryset : queryset=list ( chain ( models.modelname.objects.filter ( booleanField=True ) .order_by ( object ) , models.aDifferentModel.objects.all ( ) ) ) , Prior to the 1.9 upgrade , syncdb created the tables for me , but this is n't the case with migrate . I 've also tried python manage.py migrate -- run-syncdb but this gives the same error.However , if I copy the SQLite database from my production or staging environments to my local machine and run the command , it works ( because the table is already in the database ) . Do I have to manually create these tables ( though I assume not ) or am I doing something wrong ? Edit : Added code snippets and tracebacks . Sorry for not doing this initially.models.py ... urls.py ... traceback"
"Consider the following script in which I test two ways of performing some calculations on generators obtained by itertools.tee : Here is what I obtain when I run the script in `` normal '' mode : And here in parallel mode : The initial generator is apparently somehow `` copied '' , and executed twice.I would like to avoid this because in my real application , this seems to induce a bug in one of the external libraries I 'm using to make the initial generator ( https : //github.com/pysam-developers/pysam/issues/397 ) , and still be able to do computations in parallel on the same generated values.Is there a way to achieve what I want ?"
"I am wondering if anyone knows some of the key differences between the parakeet and the Numba jit ? I am curious , because I was comparing Numexpr to Numba and parakeet , and for this particular expression ( which I expected to perform very very well on Numexpr , because it was the one that is mentioned in its documentation ) So the results are and the functions I tested ( via timeit - minimum of 3 repetitions and 10 loops per function ) I you can also grab the IPython nb if you 'd like to double-check the results on your machine.If someone is wondering if Numba is installed correctly ... I think so , it performed as expected in my previous benchmark :"
"I need to do some real-time data analysis to monitor for operational errors . More specifically , I 'm controlling a winch on a buoy which is lowering an instrument package down through the water . I need to detect if it has hit the bottom , and stop it if it has . I 've got the following data : depth of sensor , rate at which winch is unspooling . I get updates at 1Hz and the entire process lasts about 5 minutes . If the sensor hits the bottom , the depth value will usually slow dramatically and eventually stop It can be assumed that under ideal circumstances the rate of descent is linear , but due to waves , there can be a fair amount of noise.I came up with this method : Does this make sense , or is there a better way ? Here 's some sample data from current system . It was n't a particularly rough day , and there was no bottom strike . The current system uses a Motorola 68k based TattleTale controller runing their version of basic . The bottom strike algorithm just compares every x samples , and if the difference is n't big enough , it stops . While this works , it is prone to false positives when it is rough , and has poor response in calm conditions : It was not a very interesting day from a data perspective either ."
"I converted to cython a python function by just adding some types and compiling it.I was getting small numerical differences between the results of the python and cython functions.After some work I found that the differences came from accessing a numpy array using unsigned int instead of int.I was using unsigned int indices to speed up access according to : http : //docs.cython.org/src/userguide/numpy_tutorial.html # tuning-indexing-furtheranyway I thought it was harmless to use unsigned ints.See this code : prints : Why does this happen ? ! ! ! is it a bug ? Ok , as requested here is a SSCCE with the same types and values that I used in my original function printsI use python 2.7.3 cython 0.18 and msvc9 express"
"I just want to do some numerical validation inside the custom layer . Suppose we have a very simple custom layer : And the main program : If I set a breakpoint debug at the line m = x * x , the program will pause here when executing y = test_layer ( ) ( input ) , this is because the graph is built , the call ( ) method is called.But when I use model.predict ( ) to give it real value , and wan na look inside the layer if it work properly , it does n't pause at the line m = x * xMy question is : Is call ( ) method only called when the computational graph is being built ? ( it wo n't be called when feeding real value ? ) How to debug ( or where to insert break point ) inside a layer to see the value of variables when give it real value input ?"
"I 'm trying to compress high quality video into less size and I 'm able to reduce the size of video that I 've compressed using the following objective-c code : But my main problem is that when I compress the 500MB video ( i.e average video ) file and it takes approximately 20 to 30+ minutes . It reduce the video size to approximately 130MB . I 'm using the Native AVFoundation Library to compress the video to reduce its size.I need to compress the video size very fast just like Apple Compressor application , it compresses the 500MB file within 30 seconds only ... https : //itunes.apple.com/en/app/compressor/id424390742 ? mt=12I 've also used FFMPEG library for that , but that is also slow I did not found that library anymore useful . I 've also tried to find the solution using other languages like , java , python . but did not found any solution was found.If anyone has the solution for this particular problem , or has some libraries ( i.e Paid library or Open Source Library ) that can do the compression with less time at least 1 minute ... Please do share with me . Or some other piece of code that can overcome the compression time problem from 20 - 30 minutes to at least 1 minute.Thanks ..."
"What placeholders can I use with pymssql . I 'm getting my values from the html query string so they are all of type string . Is this safe with regard to sql injection ? What mechanism is being used in this case to avoid injections ? There is n't much in the way of documentation for pymssql ... Maybe there is a better python module I could use to interface with Sql Server 2005.Thanks , Barry"
"I have a list of lists in python full of texts . It is like set words from each document . So for every document i have a list and then on list for all documents.All the list contains only unique words . My purpose is to count occurrence of each word in the complete document . I am able to do this successfully using the below code : But I want to use dictionary comprehension to do the same . This is the first time , I am trying to write dictionary comprehension and using previous existing posts in stackoverflow , I have been able to write the following : Previous post for reference : Simple syntax error in Python if else dict comprehensionAs suggested in above post , I have also used the following code : The above code was successful in producing empty lists but ultimately threw the following traceback : Any help in improving my current understanding would be much appreciated.Looking at the above error , I also tried This ran without any error but the output was empty lists only ."
"So , I 'm using django-registration in my project to enable user self-registration in the app I 'm building . I 've configured django-registration to send confirmation emails to the registered users.My settings.py : But , after the user fills the registration form and clicks the register button , the page keeps waiting for the email sending process . It looks like the response ( the confirmation page ) is received only after the email has been sent . I 've read another thread that shows how to send an email in a thread . Is there a way to send emails , using django-registration , in such a way that the response for the form registration submit does n't keep blocked until the email is sent ? I mean , I do n't want to modify the django-registration itself ."
I 've recently upgraded Python to 3.7.6 and my existing code : Is now throwing this warning : How would I go about converting this to a list as advised and where ?
"Say I have a string a.I need to sort this string in such a way that the output should beie , Sort the string in such a way that all words are in alphabetical order and all integers are in numerical order . Furthermore , if the nth element in the string is an integer it must remain an integer , and if it is a word it must remain a word.This is what I tried.I am getting the right output . But I am using two separate sorting function for sorting integers and the words ( which I think is expensive ) . Is it possible to do the entire sorting using a single sort function ? ."
"I have a bunch of graphs each with around 15 to 20 lines on each . I would like to cycle through colours and linestyles to get many unique lines . If I am using 6 colours and 4 linestyles there should be 20 unique lines , however the code below only produces 6 . What am I doing wrong ? Here is a fake data setThis is what I can glean from other posts : Note I have used duplicates in the linestyle cycler otherwise we get ValueError . And plotting"
I 've tried to replicate the main bottleneck in one of my programs.I want to get the linearly ( or rather bilinearly ) interpolated values of several non-integer pixel values simultaneously . It is not the case that each pixel coordinate is perturbed in the same way . Below is a complete/minimal script along with comments that demonstrates the problem . How can I speed up the calculation of result ?
"Since yesterday I am trying to understand how I can use the encoded base64 image from a certain view in an other view.I need to replace my form.company_logo_image_path.data which is the original image with the new image which is resized . The new resized image is sent via AJAX to a new view.Here my AJAX : I created a new view where the image is decoded and stored in the body variable : I tested this by saving the image to a folder on my local machine and it worked so the body variable stores the resized image correctly.Now I need this image to be sent to an other view where it will be uploaded to AWS3 and I will use this image instead of form.company_logo_image_path.data : The problem here is I get a Bad Request site if I try to access the result of the resize_image function from the first view . How can I access the new Image ? I am working on this problem since yesterday and it is the biggest issue I ever had so far , here is my old question with my progress : Old question with my progress and triesEDITIt doesnt matter what I try , sending to `` /profil/unternehmen-bearbeiten '' also results in a bad request error.Requesting the data a anywhere results in a bad request : The Exception here is the Bad RequestAlso requesting the canvas itself results in a bad request , the console in the browser doesnt tell me something useful which I can use/understand . Its the same as in the console in Eclipse , it gets a 400 Bad Request in the route where I try to send to : EDITFinally I made some serious progress ! I was trying to request the data in if form.validate_on_submit ( ) : . I put the code now outside if form.validate_on_submit ( ) : and I can now request the data , I am still getting problems , but from here I can keep on working ! I am getting again a Bad Request here , but I understand now why . form.validate_on_submit ( ) : itself is a POST request aswell , so I need the correct if condition and it will work ( I guess ) .Basically the problem is : My ajax request and the form.validate_on_submit ( ) : in the route where I am sending to are both POST requests , that is why I am getting Bad Request so often , there is a conflict . I was trying creating a custom form checkbox . I was trying moving the code and different other if conditions . I simply dont get it.My recent tries :"
"I have been successful in finding code for spawning a vim editor and creating a tempfile from a python script . The code is here , I found it here : call up an EDITOR ( vim ) from a python scriptThe problem I having is that I can not access the contents of the tempfile after I quit the editor.I getI did : How would I access the file in a python script once it has been edited with the editor ? Thank you"
"Say I have a Model : And I want to fill a ModelChoiceField using its values as a choice set : If I do so , I obtain a select widget with the following options : The problem is that I would like `` I do n't know '' to be the first option in the select ( as it is the most selected one ) . Is it posible to obtain a select widget with a choice set ( or to obtain a queryset ) with this custom order ( I mean , just to put first one of the options ? ) ? I have the option to solve it using Javascript , but it is preferable to solve it using Django ."
"According to the YAML spec , iso8601 dates with timezones should be recognised . However , on trying to parse them using PyYAML 3.10 ( on Windows 7 with ActivePython 2.7.2.5 ) I get naive dates : ( First format is the strict iso8601 and second is the 'relaxed ' format ; examples taken directly from YAML spec . ) Is this expected behaviour , or is my PyYaml not working correctly ?"
I am trying to do autocorrelation using Julia and compare it to Python 's result . How come they give different results ? Julia codegivesPython codegives
"This may be a terrible idea ( feel free to tell me if it is ) , but I 'm exploring the boundaries of Python , and I could see how this could be useful ( in fact , I 'm looking at a potential case for it right now ) . Here is the setup : -- - ( API File ) -- -- -- - ( My file ) -- -- I want to modify the output of the api_function slightly , basically just to override the render function in the object it returns . I feel like my options are : ( 1 , yuck ) to copy the contents of api_function into my_function , but just construct and return a MyNode instead of an APINode , or ( 2 , maybe ? ) to just call api_function from my_function , let its do its work -- constructing and returning an object of type APINode , and then I can somehow create a MyNode from that object in order to override that one method . It boils down to : In Python , is it possible to construct an instance of a child class from an instance of a parent class ? ( Look familiar or wondering what the actual case is ? I 'm trying to extend a Django template tag . )"
"I have Twisted server with constant connection to Redis.I 'm using library https : //github.com/fiorix/txredisapi.Problem is that from time to time Twisted lose connection to Redis and reconnects shortly after . I have no idea why . Same thing is happening both on my local machine and on my online VPS.Log : It happens all day and all night , my logs are getting big quickly.Log when someone connects while redis is disconnected :"
"How to get all the existing duplicated sets of records ( based on a column ) from a dataframe ? I got a dataframe as follows : Here flight_id is the column on which I need to check duplicates . And there are 2 sets of duplicates.Output for this specific example should look like -- [ ( 2,5 ) , ( 3,6 ) ] . List of tuples of record index values"
"Context : I have Flask routes defined for different API endpoints and each endpoint calls a controller class with certain parameters ( uid , project_id , etc. ) . The controller ( proj_cntr ) is responsible for determining , say , if the given PID is valid , wether the given user is allowed to perform the action , and other business logic validation . I noticed that I am c/pasting a lot of code like this in different controllers : Putting these checks ( validations ) in decorators seems like the best thing to do . But I am not sure which error handling pattern is best practice should the validation not pass . 1 ) Should I create specific custom exceptions ( InvalidProjException , PermissionsException , etc . ) for each decorator to raise ? Concerns : The catch block of the caller method will look bloated . Also , is it good to make the assumption that the caller knows what exceptions the decorators of the callee raise ? 2 ) The decorator passes an extra error argument to the method and the method decides what exception to raise . This way the caller method is aware what exception type to expect and handle.Concerns : Approach seems a little over-engineered and messy.Sorry for the verbose question . Any thoughts/ideas are greatly appreciated ."
"I use python snowflake connector in my python script ( plotly dash app ) and today the app stopped working without me changing the code . I tried a couple of things to find out what might be the issue and I even tried to run the example code from Snowflake documentation and I got the same error : code : error : The connection is established , I am capable of creating a table in my database but I can not seem to query and iterate the data.I am on macOS Catalina 10.15.1 , snowflake-connector-python==2.1.0 , Python 3.7.0 ."
"In my way to profile string methods in python so that I can use the fastest one.I have this code to test string concatenation in files , StringIO , StringIO and normal string.While the Python documentation site says that cStringIO is faster than StringIO but the results says that StringIO has better performance in concatenation , why ? The other hand is that , reading from cStringIO is faster than StringIO ( its behavior similar to file ) , as I read the implementation of file and cStringIO are in C , so why string concatenation is slow ? Is there any other way to deal with string more faster than these methods ?"
"I am trying to create a simple drawing application using Python , GTK3 and cairo . The tool should have different brushes and some kind of a highlighter pen . I figured I can use the alpha property of the stroke to create it . However , the connecting points are created overlapping and that creates a weird effect.Here is the code responsible for this red brush and the highlighter mode : The code that draws on mouse click : Can someone point out a way to prevent the overlapping points in this curve ? UpdateUli 's solution seems to offer a partial remedy , but the stroke is still not good looking , it seems that it 's redrawn over and over : Update with partially working codeI still have not succeeded in creating a highlighter pen with cairo . The closest I can get is in the following gist.The application shutter , has a similar functionality but it 's written in Perl on top of the libgoocanvas which is not maintained anymore . I hope a bounty here will change the situation ... updateavailable operators ( Linux , GTK+3 ) :"
"I ca n't for the life of me figure this out.I 'm trying to remove every other element in the second axis of an array . I did this in MATLAB with arr ( : , : ,2:2 : end ) = [ ] ; , but when I tried to do the same in Python and compare the two outputs , I get a different matrix.I 've tried arr = np.delete ( arr , np.arange ( 0 , arr.shape [ 2 ] ,2 ) ,2 ) and arr = arr [ : , : ,1 : :2 ] , but neither seem to come up with something I get with MATLAB.Example : MATLABoutput : Pythonoutput : I hope I 'm not overlooking something fundamental ."
"Suppose you have the following dataframe : Suppose I want an additional `` layer '' on top of this pandas dataframe , such that column A , row 0 would have its value , column B , row 0 would have a different value , column C row 0 would have something , column A row 1 and so on . So like a dataframe on top of this existing one.Is it possible to add other layers ? How does one access these layers ? Is this efficient , i.e . should I just use a separate data frame all together ? And one would save these multiple layers as a csv , by accessing the individual layers ? , or is there a function that would break them down into different worksheets in the same workbook ?"
"Is there a way in Python , to have more than one constructor or more than one method with the same name , who differ in the number of arguments they accept or the type ( s ) of one or more argument ( s ) ? If not , what would be the best way to handle such situations ? For an example I made up a color class . This class should only work as a basic example to discuss this , there is lot 's of unnecessary and/or redundant stuff in there.It would be nice , if I could call the constructor with different objects ( a list , an other color object or three integers ... ) and the constructor handles them accordingly . In this basic example it works in some cases with * args and * * kwargs , but using class methods is the only general way I came up with . What would be a `` best practice '' like solution for this ? The constructor aside , if I would like to implement an _ _ add _ _ method too , how can I get this method to accept all of this : A plain integer ( which is added to all values ) , three integers ( where the first is added to the red value and so forth ) or another color object ( where both red values are added together , etc . ) ? EDITI added an alternative constructor ( initializer , _ _ init _ _ ) that basicly does all the stuff I wanted.But I stick with the first one and the factory methods . Seems clearer.I also added an _ _ add _ _ , which does all the things mentioned above but I 'm not sure if it 's good style . I try to use the iteration protocol and fall back to `` single value mode '' instead of checking for specific types . Maybe still ugly tho.I have taken a look at _ _ new _ _ , thanks for the links.My first quick try with it fails : I filter the rgb values from the * args and * * kwargs ( is it a class , a list , etc . ) then call the superclass 's _ _ new _ _ with the right args ( just r , g , b ) to pass it along to init . The call to the 'Super ( cls , self ) ._ _ new _ _ ( ... . ) ' works , but since I generate and return the same object as the one I call from ( as intended ) , all the original args get passed to _ _ init _ _ ( working as intended ) , so it bails . I could get rid of the _ _ init _ _ completly and set the values in the _ _ new _ _ but I do n't know ... feels like I 'm abusing stuff here ; - ) I should take a good look at metaclasses and new first I guess.Source :"
"I 've often been frustrated by the lack of flexibility in Python 's iterable unpacking . Take the following example : Works fine . a contains `` This '' and b contains `` is a string '' , just as expected . Now let 's try this : Now , we get a ValueError : Not ideal , when the desired result was `` Thisisastring '' in a , and None or , better yet , `` '' in b.There are a number of hacks to get around this . The most elegant I 've seen is this : Not pretty , and very confusing to Python newcomers.So what 's the most Pythonic way to do this ? Store the return value in a variable and use an if block ? The *varname hack ? Something else ?"
"This question may be a duplicate . However , I read lot of stuff around on this topic , and I did n't find one that matches my case - or at least , I did n't understood it.Sorry for the inconvenance.What I 'm trying to do is fairly common , passing a list of kwargs to pool.starmap ( ) , to achieve multiprocessing.Here 's my reduced case : I supposed starmap ( ) will unpack the dict and pass it to compute ( ) as keyword args , but looking at the source code ( see also l.46 ) , it sends only keys ( or values ? ) .So it raises : It must be a clear , straight forward way to do this ... Any help would be appreciated.Here 's a quite similar question : Python Multiprocessing - How to pass kwargs to function ?"
"I am trying the following code which adds a number to every row in an RDD and returns a list of RDDs using PySpark . The content in the input file ( sample.txt ) is : I was expecting an output like this ( adding the numbers in the rdd with 0 , 1 , 2 respectively ) : whereas the actual output was : which means that the comprehension used only the value 3 for variable i , irrespective of the range ( 4 ) . Why does this behavior happen ?"
"I have 64 bit windows 10 OSI have installed python 3.6.8I have installed torch and torchtext using pip.torch version is 1.2.0I am trying to load AG_NEWS dataset using below code : On the last statement of above code , I am getting below error : I think the issue is with either windows os or torchtext because I am getting same error for below code as well.Can somebody please help ? and mainly I do n't have any large numerical values in the file ."
"We have a Django application that is connecting to multiple MS SQL database instances . There is a router.py for each app that handles routing the processes to each database.This is my first time setting up multiple database access.Django built-in apps are routed to the default database via this router.py : We are using LDAP authentication and Django 's built-in authentication . The idea is that intranet users can authenticate against our AD server . External users can register , and will be authenticated with Django 's authentication.When I have the default database set to : LDAP works , but I can not add user to Django 's authentication . The admin displays a `` success '' message , but the user is not added to the database.If I switch the default database back to SQLLite , I am able to authenticate against AD and add Django users.So , I do n't think it is an issue with the routers.py file . I worry that it might be an issue with the 'sql_server.pyodbc ' engine.EDIT : Per request , here are the database settings : NOTE : I think this might have to do with the way Django saves new users . Going to take a look there . I am able to use the createsuperuser command to add more superusers while both authentication backends are in place . Confirmed , I can create regular users via the shell , but not through the admin.Follow up note : Still have not sourced the problem with the admin , but I found that I could add users via a form . I 'm thinking the problem must be a bug in the admin.Edit : Per @ AndrewSmiley 's request : Django Admin Add User ViewUPDATE : Per @ user1797792 suggestions , I created a custom UserCreationForm to set the date_joined value , but we are still unable to add users via the Django Admin.Relevant forms.py & admin.py entries are below :"
"In this Python documentation the following is used as an example of a generator expression : I do n't understand how the second for loop , for fn in ( int , str ) , turns the int value into a string and adds an additional entry to the dictionary.I have found this Stack Overflow question , but I still was n't able to intuit how the second for loop works in this case ."
"I 'm building an OCR . For that I 'm using CNN , RNN and CTC Loss Function.My input layer gets image and output layer predicts what 's written on that image . Labels are converted into integer.If the image is ABC , training label will be 0,1,2 ( Single row vector ) I 'm able to accomplish this on single line . For eg . 'ABCDE ' is written on an image and model works great . But if the image isthen what should be the training label ? How can I tell the model about next line ? I want to train a model on multiple line ."
"I 'm new to Pandas and Zipline , and I 'm trying to learn how to use them ( and use them with this data that I have ) . Any sorts of tips , even if no full solution , would be much appreciated . I have tried a number of things , and have gotten quite close , but run into indexing issues , Exception : Reindexing only valid with uniquely valued Index objects , in particular . [ Pandas 0.10.0 , Python 2.7 ] I 'm trying to transform monthly returns data I have for thousands of stocks in postgres from the form : e.g . NB : The frequency of the reporting is monthly , but there is going to be considerable NaN data here , as not all of the over 6000 companies I have here are going to be around at the same time.…to the form described below , which is what Zipline needs to run its backtester . ( I think . Can Zipline 's backtester work with monthly data like this , easily ? I know it can , but any tips for doing this ? ) The below is a DataFrame ( of timeseries ? How do you say this ? ) , in the format I need : > data : The below is a TimeSeries , and is in the format I need. > data.AAPL : Note , there is n't return data here , but prices instead . They 're adjusted ( by Zipline 's load_from_yahoo—though , from reading the source , really by functions in pandas ) for dividends , splits , etc , so there 's an isomorphism ( less the initial price ) between that and my return data ( so , no problem here ) . ( EDIT : Let me know if you 'd like me to write what I have , or attach my iPython notebook or a gist ; I just doubt it 'd be helpful , but I can absolutely do it if requested . )"
"I 'm tryingBut this gives me an error : My Y looks like : And my Y.shape looks like : ( 14993 , 5 ) In my keras model , I want to use the class_weights as it is an uneven distribution :"
"Woking on personal project website with django 1.9 and python 3.4 . I am using FullCalendar . The idea is to pass a set of appointment objects into the html page containing the javascript for the calendar . But right now , I am just trying to pass a single default appointment.In views.py I have the following : In profile_patient.html : appt is not getting properly parsed I believe . When the web page is loaded , the calendar does not display at all . When I substitute appt with the direct string , it does work : When I call alert ( `` { { appt } } '' ) ; i get the following : So there 's something wrong with the way I 'm using ths varibale . Any ideas ?"
"I have started learning Python recently and I 've been going through the NumPy official quickstart guide which includes this example for iterating.However , if I just try to raise -1000 to the power of ( 1/3 . ) outside of the loop it returns a value.With parentheses around -1000 it also returns a value.Why is it that the same action returns nan in the for loop ? I 'm using Python 3.6.3 : : Anaconda custom ( 64-bit ) . I also tried with different fractions that do not round up and it 's the same . With a fraction that rounds up to .0 it works though.I could n't find a similar question . Excuse me if I 'm missing something very obvious.Edit : A few comments mentioned that the question duplicates NumPy , RuntimeWarning : invalid value encountered in power and it 's true , the problem was I did n't see such an error . The discussion there , however , seems to include a few possible workarounds ."
I have a dataframe and I am looking to calculate the mean based on store and all stores . I created code to calculate the mean but I am looking for a way that is more efficient . DFDF-DesiredMy AttemptI created two additional dataframes then did a left join
"Why this codein Python 2.7 throws this result : But the same code in Python 3.3 throws this other result : It 's more sense for me the result with Python 2.7 , because the ' i ' variable is in the same name scope where the lambda function is called from . I do n't understand these unequal behaviors of the lambdas functions in the two branches of Python ."
"I have found a plethora of questions regarding finding `` things '' in images using openCV , et al . in Python but so far I have been unable to piece them together for a reliable solution to my problem.I am attempting to use computer vision to help count tiny surface mount electronics parts . The idea is for me to dump parts onto a solid color piece of paper , snap a picture , and have the software tell me how many items are in it.The `` things '' differ from one picture to the next but will always be identical in any one image . I seem to be able to manually tune the parameters for things like hue/saturation for a particular part but it tends to require tweaking every time I change to a new part.My current , semi-functioning code is posted below : Here 's an example of the type of input image I am using : or this : And I 'm currently getting results like this : The results clearly show that the script is having trouble identifying some parts and it 's true Achilles heel seems to be when parts touch one another.So my question/challenge is , what can I do to improve the reliability of this script ? The script is to be integrated into an existing Python tool so I am searching for a solution using Python . The solution does not need to be pure Python as I am willing to install whatever 3rd party libraries might be needed ."
"I am trying to get urllib2 to work with PyWebKitGtk to support cookies . I think it 's mostly working , but cookies are n't working between sessions . The cookies.txt file is saved , and it does look like it uses the cookies in the requests ( examined in Wireshark ) , but the data I am seeing loaded into the browser window does n't appear to have been using the cookies . After I log in , shut down the app , then restart it , my login session is gone.My code"
"I want to get all the possible permutations of two different elements grouped by four without repeating the output but repeating elements ( obviously you need to repeat elements to make combinations of 4 with only 2 elements ) .So , some things I 've tried : returns an empty list . So I tried this instead : And it is closed to the output I expect , but full of redundant items , just a few of them : What I expected to get is actually : I tried also combinations ( ) and product ( ) instead of permutations ( ) without success , but maybe my arguments were n't fit to the problem.Any ideas ? Thanks a lot in advance ! PS . As pointed out by Antoine , there 's a workaround using a set instead of a list , like : That gives the expected output , still it is generating all the repetitions anyway but just happen to be filtered out because sets can not have repeated element , so it 's probably not the most efficient way to do it ."
"I 'm quite new to django , and moved to it from Drupal.In Drupal is possible to define module-level variables ( read `` application '' for django ) which are stored in the DB and use one of Drupal 's `` core tables '' . The idiom would be something like : The idea is that it would n't make sense to have each module ( app ) to instantiate a whole `` module table '' to just store one value , so the core provides a common one to be shared across modules.To the best of my newbie understanding of django , django lack such a functionality , but - since it is a common pattern - I thought to turn to SO community to check if there is a typical/standard/idiomatic way that django devs use to solve this problem . ( BTW : the value is not a constant that I could put in a settings file . It 's a value that should be refreshed daily , and should be read at each request ) ."
I 'd like to add to existing models new CharFields via one common mixin or abstract model but names of these fields depend on configuraton . so one model will have someprefix1_title field and another model - someprefix2_title.Is it possible to make this approach to work : so ModelOne could have fields id and someprefix1_title.upd : what about monkey-patching with add_to_class ( ) will it work or it 's an antipattern and should not be used ?
"Given dfBased on this answer , I created a function to calculate streaks ( up , down ) . The function works well , however is very long . I 'm sure there 's a much betterway to write this . I tried the other answer in but did n't work well.This is the desired output"
"I 'm trying to find the difference between two Pandas MultiIndex objects of different shapes . I 've used : and receiveMy indices are str and datetime , but I suspect there are NaNs hidden there ( the floats ) . Hence my question : What 's the best way to find the NaNs somewhere in the MultiIndex ? How does one iterate through the levels and names ? Can I use something like isna ( ) ?"
"So I 've been working on a Python script that combines some information into a `` bed '' format . Which means that I 'm working with features on a genome , my first column is the scaffold name ( string ) , the second the start position on that scaffold ( integer ) and the third column is the stop position ( integer ) , the other columns contain other information which is not relevant to my question . My issue is that my output is unsorted.Now I know I can sort my files using this bash command : But in the interest efficacy I 'd like to know if there 's a way to do this in Python . So far I 've only seen list based sorts that deal with one either lexicographical or numerical sort . Not a combination of the two.So , do you guys have any ideas ? Snippet of my data ( I want to sort by column 1 , 2 and 3 ( in that order ) ) :"
"I 'm currently building a data model using sqlalchemy through flask-sqlalchemyThe database is on a Postgresql serverI am having trouble when deleting rows from a table that has relationships . In this case I have a number of treatment types , and one treatment . the treatment has a single treatment type assigned.As long as I have one or more treatments assigned a particular treatment Type , I wish that the treatment type can not be deleted . As it is now it is deleted when I try.I have the following model : I can build some logic in my `` delete '' view that checks for assigned treatments , before deleting the treatment type , but in my opinion this should be a standard feature of a relational database . So in other words I must be doing something wrong.I delete the treatment type like so : As I said it is possible for me to do a check before deleting the treatment Type , but I would rather have sqlalchemy throw an error if there are relationship issues prior to deletion ."
"I 'm starting to use pytest to add unit test to a software that can analyse different kind of datasets.I wrote a set of test functions that I would like to apply to different datasets . One complication is that the datasets are quite big , so I would like to do : Load dataset1Run testsLoad dataset2Run testsand so on.Right now I 'm able to use one dataset using a fixture : and then passing datato each test function.I know that I can pass the params keyword to pytest.fixture . But , how can I implement the sequential load of the different datasets ( not loading all of them in RAM at the same time ) ?"
"I 'm using Flask , psycopg2 and uWSGI . I 'm using psycopg2.ThreadConnectionPool for DB connection pooling and only cursor.callproc is used for quering DB.The problem : sometimes , during concurrent requests , procedure call results are mixed up , the code is querying procedure_1 but is getting results for procedure_2 ( and vice versa for another concurrent client ) . Even if in uwsgi.ini there is threads=1 , and only processes are used for concurrency . maxconn for psycopg2.ThreadConnectionPool is set to 1 . The problem disappears if processes is set to 1 in uwsgi.iniWhat can cause the problem ? Here is the simplified code that reproduces the problem : Flask app : uwsgi.ini : Procedures : Reproducing ( it happens not on every request , but rather often ) : Thanks in advance !"
"QuestionIs there any way to automate a tkFileDialog selection to run it through unittest ? The following is the only use of tkinter in my application : Edit : I did n't mention that this part of the code was trapped in a method call from a class outside my control.BackgroundI 've built a local app that creates an http server on localhost and runs its GUI with HTML/CSS/JS in a web browser . Because of browser restrictions , I ca n't use the built-in file dialog and so have to send this request through Python . I want this to run on a OSX with the built-in Python 2.5 . I 'm not very familiar with Tcl/Tk.Attempt # 1If I could get to the underlying widgets , I could generate the clicks like in this question . However , looking at the dialog source , it appears to me that the Tcl call in lines 48-50 is blocking . Is this a correct assumption ? Attempt # 2I thought there might be a way using Tcl commands directly through root.tk.call . Since I 'm on Python2 , I think the underlying Tcl is a single call to tk_getOpenFile . Would I have to ensure the Tcl interpreter is threaded ? Is there any Tcl/Tk command that can help me out here ? Attempt # 3I could implement the file selection from scratch using os.listdir etc . ( Probably in a separate HTML page communicating back and forth with the server ) . It would be more than a little painful and hopefully avoidable.SolutionBased on A. Rodas 's answer below , I came up with the following :"
I have a bunch of class objects which all inherit from a base class . Some of them override a method ( save ) and do stuff . For this particular use case I want to temporarily not allow the child save method to be used ( if it exists ) but rather force the use of the parent save method.How can I call obj parent save from outside the child such that it prints `` Base Called '' ?
"I want to use Python to resize any image based on the following 2 conditions.1 ) If an image is landscape , get width , if greater than 1280 resize image width to 1280 maintaining aspect ratio . 2 ) If an image is portrait , get height , if greater than1280 resize height to 1280 maintaining aspect ratio.In Python what is the best package/approach to achieve this ? Without knowing what to use this is how I see it working . Pseudocode : I was looking at Pillow ( PIL ) ."
"Consider API returning four lists as output . Let 's consider output asNow , first we want to compare these lists are equal or not.Lists are equal only if elements and there indexes matches.For example , from above lists , a and b are equal . But a and c are not equal.If the lists are not equal , then output is expected as : this element at this index in this list is not same as other . For comparing and getting differences of two lists , I have written below code.Now question is how to achieve this for all four above lists ?"
I 'm using a custom email backend in my Django application ( CeleryEmailBackend in this case ) : My logging configuration : The Admin error emails also get sent by the same email backend.So if there is a problem with the email backend ( e.g . Celery is not running ) . Then I wo n't receive the server error emails.Is there a way to make AdminEmailHandler use a custom email backend ?
"have a look at the following piece of code : the output : but when i try to limit the list with if : Shows the following exception : If a make global b it works , why this is happening ?"
"In a nutshell : My implementation of the Wave Collapse Function algorithm in Python 2.7 is flawed but I 'm unable to identify where the problem is located . I would need help to find out what I 'm possibly missing or doing wrong.What is the Wave Collapse Function algorithm ? It is an algorithm written in 2016 by Maxim Gumin that can generate procedural patterns from a sample image . You can see it in action here ( 2D overlapping model ) and here ( 3D tile model ) .Goal of this implementation : To boil down the algorithm ( 2D overlapping model ) to its essence and avoid the redondancies and clumsiness of the original C # script ( surprisingly long and difficult to read ) . This is an attempt to make a shorter , clearer and pythonic version of this algorithm.Characteristics of this implementation : I 'm using Processing ( Python mode ) , a software for visual design that makes image manipulation easier ( no PIL , no Matplotlib , ... ) . The main drawbacks are that I 'm limited to Python 2.7 and can NOT import numpy . Unlike the original version , this implementation : is not object oriented ( in its current state ) , making it easier to understand / closer to pseudo-codeis using 1D arrays instead of 2D arraysis using array slicing for matrix manipulationThe Algorithm ( as I understand it ) 1/ Read the input bitmap , store every NxN patterns and count their occurences . ( optional : Augment pattern data with rotations and reflections . ) For example , when N = 3:2/ Precompute and store every possible adjacency relations between patterns.In the example below , patterns 207 , 242 , 182 and 125 can overlap the right side of pattern 2463/ Create an array with the dimensions of the output ( called W for wave ) . Each element of this array is an array holding the state ( True of False ) of each pattern . For example , let 's say we count 326 unique patterns in input and we want our output to be of dimensions 20 by 20 ( 400 cells ) . Then the `` Wave '' array will contain 400 ( 20x20 ) arrays , each of them containing 326 boolan values.At start , all booleans are set to True because every pattern is allowed at any position of the Wave.4/ Create another array with the dimensions of the output ( called H ) . Each element of this array is a float holding the `` entropy '' value of its corresponding cell in output.Entropy here refers to Shannon Entropy and is computed based on the number of valid patterns at a specific location in the Wave . The more a cell has valid patterns ( set to True in the Wave ) , the higher its entropy is.For example , to compute the entropy of cell 22 we look at its corresponding index in the wave ( W [ 22 ] ) and count the number of booleans set to True . With that count we can now compute the entropy with the Shannon formula . The result of this calculation will be then stored in H at the same index H [ 22 ] At start , all cells have the same entropy value ( same float at every position in H ) since all patterns are set to True , for each cell.These 4 steps are introductory steps , they are necessary to initalize the algorithm . Now starts the core of the algorithm:5/ Observation : Find the index of the cell with the minimum nonzero entropy ( Note that at the very first iteration all entropies are equal so we need to pick the index of a cell randomly . ) Then , look at the still valid patterns at the corresponding index in the Wave and select one of them randomly , weighted by the frequency that pattern appears in the input image ( weighted choice ) .For example if the lowest value in H is at index 22 ( H [ 22 ] ) , we look at all the patterns set to True at W [ 22 ] and pick one randomly based on the number of times it appears in the input . ( Remember at step 1 we 've counted the number of occurences for each pattern ) . This insures that patterns appear with a similar distribution in the output as are found in the input.6/ Collapse : We now assign the index of the selected pattern to the cell with the minimum entropy . Meaning that every pattern at the corresponding location in the Wave are set to False except for the one that has been chosen.For example if pattern 246 in W [ 22 ] was set to True and has been selected , then all other patterns are set to False . Cell 22 is assigned pattern 246.In output cell 22 will be filled with the first color ( top left corner ) of pattern 246 . ( blue in this example ) 7/ Propagation : Because of adjacency constraints , that pattern selection has consequences on the neighboring cells in the Wave . The arrays of booleans corresponding to the cells on the left and right , on top of and above the recently collapsed cell need to be updated accordingly.For example if cell 22 has been collapsed and assigned with pattern 246 , then W [ 21 ] ( left ) , W [ 23 ] ( right ) , W [ 2 ] ( up ) and W [ 42 ] ( down ) have to be modified so as they only keep to True the patterns that are adjacent to pattern 246.For example , looking back at the picture of step 2 , we can see that only patterns 207 , 242 , 182 and 125 can be placed on the right of pattern 246 . That means that W [ 23 ] ( right of cell 22 ) needs to keep patterns 207 , 242 , 182 and 125 as True and set all other patterns in the array as False . If these patterns are not valid anymore ( already set to False because of a previous constraint ) then the algorithm is facing a contradiction.8/ Updating entropiesBecause a cell has been collapsed ( one pattern selected , set to True ) and its surrounding cells updated accordingly ( setting non adjacent patterns to False ) the entropy of all these cells have changed and needs to be computed again . ( Remember that the entropy of a cell is correlated to the number of valid pattern it holds in the Wave . ) In the example , the entropy of cell 22 is now 0 , ( H [ 22 ] = 0 , because only pattern 246 is set to True at W [ 22 ] ) and the entropy of its neighboring cells have decreased ( patterns that were not adjacent to pattern 246 have been set to False ) .By now the algorithm arrives at the end of the first iteration and will loop over steps 5 ( find cell with minimum non zero entropy ) to 8 ( update entropies ) until all cells are collapsed.My scriptYou 'll need Processing with Python mode installed to run this script.It contains around 80 lines of code ( short compared to the ~1000 lines of the original script ) that are fully annotated so it can be rapidly understood . You 'll also need to download the input image and change the path on line 16 accordingly.ProblemDespite all my efforts to carefully put into code all the steps described above , this implementation returns very odd and disappointing results : Example of a 20x20 outputBoth the pattern distribution and the adjacency constraints seem to be respected ( same amount of blue , green , yellow and brown colors as in input and same kind of patterns : horizontal ground , green stems ) .However these patterns : are often disconnectedare often incomplete ( lack of `` heads '' composed of 4-yellow petals ) run into way too many contradictory states ( grey cells marked as `` CONT '' ) On that last point , I should clarify that contradictory states are normal but should happen very rarely ( as stated in the middle of page 6 of this paper and in this article ) Hours of debugging convinced me that introductory steps ( 1 to 5 ) are correct ( counting and storing patterns , adjacency and entropy computations , arrays initialization ) . This has led me to think that something must be off with the core part of the algorithm ( steps 6 to 8 ) . Either I am implementing one of these steps incorrectly or I am missing a key element of the logic . Any help regarding that matter would thus be immensely appreciated ! Also , any answer that is based on the script provided ( using Processing or not ) is welcomed.Useful additionnal ressources : This detailed article from Stephen Sherratt and this explanatory paper from Karth & Smith.Also , for comparison I would suggest to check this other Python implementation ( contains a backtracking mechanism that is n't mandatory ) .Note : I did my best to make this question as clear as possible ( comprehensive explanation with GIFs and illustrations , fully annotated code with useful links and ressources ) but if for some reasons you decide to vote it down , please leave a brief comment to explain why you 're doing so ."
What I want is to write a code to detect the longest and shortest words from those given in a list . I tried to write a code to find the longest word using the number of characters which failed really bad ... I 'm wondering if there is a simple function or line that I 'm missing.though i do get a print of the number of characters of each word i cant figure out how to proceed.PS : id like to know how to create a list from the values obtained as the result from the above function
"Is there a maximum number of characters ( and therfore value ) to the seed in Python ? I 'm not sure why someone would pick such a high value , but I just want to know if has a limit ."
"I am trying to find all the polygons ( including the filled in one ) in the image below . Currently , I 'm trying to use Hough Transform to accomplish this , but it does not detect all the lines in the image . In addition , because of how wide the lines are , it counts each line twice . Is there a way to apply some filters to the image to make the Hough Transform perform better or is there an entirely different way to find the polygons that I 'm missing ? Thanks ! Here is the picture I am processing , and my code is below ."
"I wrote a little script which let to find an object in a global picture by the SIFT descriptors method . But I have a question about multiple detections in the same picture . I have this global picture : I have this template : My script looks like : And the result is : My question is : How I can detect this others lamps ? Because all lamps are very similar and I want to match with all lamps which are present in the picture.Thank you so much ! EDIT With Micka 's answer : Nothing appears at 0.2 scale distance , but if I put 0.75 :"
"I 'm having a DataFrame with two columns . One column is filled with timestamps , the other column contains the offset in hours to UTC of the timestamp in the same row.The DataFrame looks like this : What i like to achieve is to add the offset per row to the timestamp : I 've tried with to replace tzinfo but failed to find a proper solution . I 'm thinking about something like the following ( pseudo code ) : Any help is appreciated.Thanks , Thomas"
"I am packaging my source code , but I do not want to include tests and docs because it will be too big.To do that I include in my setup.py : When doing a I can see that my root tests/ and docs/ dirs and everything inside are still included in the generated distribution.It seems that onlyis sensible to the exclude parameter.Why ? is it possible to exclude dirs for 'setup.py sdist ' ?"
"I am working through the traits presentation from PyCon 2010 . At about 2:30:45 the presenter starts covering trait event notifications , which allow ( among other things ) the ability to automatically call a subroutine any time a trait has changed.I am running a modified copy of the example he gave ... In this trial , I am trying to see whether I can fire a static event whenever I make a change to volume or inputs.When I run this script , I can see This one goes to eleven ... so far , we have seen [ 5.0 , 11.0 ] in the console output , so I know that _volume_changed ( ) gets fired when I assign 11.0 to spinal_tap.volume.However , I never see any events from _inputs_changed ( ) . No matter what example I cook up , I ca n't get a List to fire an event.This is the output I am seeing ... note that there is no evidence that _inputs_changed ( ) ever fires.I have run this both under Python2.6 / Cygwin / Windows 7 and Python 2.5 / Linux ( all using traits version 4.0.0 that I easy_install directly off Enthought 's site ) . The results are the same no matter what I have tried so far.Should a List be able to fire a static event when using traits ? If so , am I doing something wrong ?"
"Current df : I have the df with ID and Date . ID is unique in the original df.I would like to create a new df based on date . Each ID has a Max Date , I would like to use that date and go back 4 days ( 5 rows each ID ) There are thousands of IDs.Expect to get : I tried the following method , i think use date_range might be right direction , but I keep get error . pd.date_range"
I implemented a python web client that I would like to test.The server is hosted in npm registry . The server gets ran locally with node before running my functional tests.How can I install properly the npm module from my setup.py script ? Here is my current solution inspired from this post :
"I have a python list like this : I am trying to write the code to join the dictionaries with the same name by also adding the quantities . The final list will be that : I have tried a few things but I am struggling to get the right code . The code I have written below is somewhat adding the values ( actually my list is much longer , I have just added a small portion for reference ) .can you please help me to get the correct code ?"
"I 'm trying to embed Cython code into C following O'reilly Cython book chapter 8 . I found this paragraph on Cython 's documentation but still do n't know what should I do : If the C code wanting to use these functions is part of more than one shared library or executable , then import_modulename ( ) function needs to be called in each of the shared libraries which use these functions . If you crash with a segmentation fault ( SIGSEGV on linux ) when calling into one of these api calls , this is likely an indication that the shared library which contains the api call which is generating the segmentation fault does not call the import_modulename ( ) function before the api call which crashes.I 'm running Python 3.4 , Cython 0.23 and GCC 5 on OS X . The source code are transcendentals.pyx and main.c : main.ctranscendentals.pyxI 'm compiling those files using setup.py and Makefile : setup.py : MakefileHowever , I came to error Segmentation fault : 11 . Any idea can help with this ? Thanks !"
"How to compare two lists or dictionaries in easy way , eg.If I want to check two lists in python nose tests , Is there any built-in function can let me use ? Does compare two lists is a bad practice when doing testing ? ( because I 've never see it ) If there is no built-in , plugin in nose , is there any handy package can do it for me ."
"In class B below I wanted the __set__ function in class A to be called whenever you assign a value to B ( ) .a . Instead , setting a value to B ( ) .a overwrites B ( ) .a with the value . Class C assigning to C ( ) .a works correctly , but I wanted to have a separate instance of A for each user class , i.e . I do n't want changing ' a ' in one instance of C ( ) to change ' a ' in all other instances . I wrote a couple of tests to help illustrate the problem . Can you help me define a class that will pass both test1 and test2 ?"
"It seems strange to me that pandas.read_csv is not a direct reciprocal function to df.to_csv . In this illustration , notice how when using all the default settings the original and final DataFrames differ by the `` Unnamed '' column.It seems the default read_csv should instead be or the default to_csv should instead bewhich when read gives [ 4 rows x 3 columns ] ( Perhaps this should instead be sent to the developer/s but I am genuinely interested why this is the default behavior . Hopefully it also can help someone else avoid the confusion I had ) ."
Are these equivalent ? andThe second one seems better for readability and one could put pass at the end but it does not seem necessary.Is the comment treated as a pass ?
"I 'm creating a GUI application that can monitor and manipulate a stream of messages . I 'm trying to create a simple means to let the user script some of this functionality and I 'm looking for possible candidates . Initially I wanted to use XML since it can naturally take care of embedded code : For parsing I was planning on using ElementTree and just build states out of the code . Writing and reading XML is n't the easiest thing to do , especially since I ca n't assume that the writers of the script will have any sort of experience . I was wondering if anyone had any alternatives that is easier to read/write and process in Python . I looked into JSON but because it 's a script , order matters . Can anyone suggest any possible alternatives ? Thanks ."
"I 'm using Python Matplotlib to plot contours . Here 's some code I have below as a basis . If you run this , you 'll see that the labels are almost at vertical . I 'd like to get the labels orientated horizontal , but I have no idea how can achieve this . I 've tried with ClabelText , which the documentation suggests , but do n't understand how this is supposed to work . I 'd appreciate if someone could suggest a way to orientate the labels , either with or without ClabelText ."
"How can one change the formatting of output from the logging module in Google App Engine ? I 've tried , e.g . : However this results in duplicate logging output : One with the logging handler from google/appengine/tools/dev_appserver.py ( or somewhere in the Google code ) , and one from my new StreamHandler above . The above code outputs : Where the top line is clearly from dev_appserver.py , the bottom line from my code.So I guess the corollary question is : How can change the formatting of Google App Engine , yet avoid the duplicate output ? Thank you for reading.Brian"
"First of all , my apologies if this question has already be asked elsewhere . I really searched for it , but did n't find anything.The situation is the following : In a folder mod , I have the files __init__.py and sub.py.They contain the following data : __init__.py : sub.py : Now let 's do the following : But when doing import mod.sub , why is mod/__init__.py executed again ? It had been imported already.The same strange feature exists if we just call : Can I change the behaviour by changing the import __init__ ? This is the line that seems most likely wrong to me ."
"Is it possible to do glmm in Python ( like the GENLINMIXED analysis in SPSS ) ? I 'm a big fan of statsmodels , but this library does n't seem to support glmm ... Are there any alternatives ? -edit-Decided to do it with R and r2py ..."
"This is for a homework assignment , I solved the problem but I 'm trying to find a faster solution.The problem is as follows : I need to figure out how many possible aminoacid ( aa ) sequences exist that have a total mass m. I have a table of aminoacids ( single letter strings ) and their corresponding mass ( int ) which I put in a dictionary.My initial solution was to create all possible combinations of aa and compare each combination 's total mass to the mass m. This works for small numbers of m but the number of combinations gets ridiculously high when m starts being in the hundreds.I did some minor optimization and got it to work reasonably fast for m < 500 which is good enough for this problem but I want to know how to make it work for higher masses.This is what I have so far : I can get a solution for m = 300 in about a second but m = 500 takes about 40 secondsI tried an alternative using itertools but it was n't any faster : Sample input : m = 270output:22"
"I am starting with Tensorflow 2.0 and trying to implement Guided BackProp to display Saliency Map . I started by computing the loss between y_pred and y_true of an image , then find gradients of all layers due to this loss . However , I do n't know what to do with gradients in order to obtain the Guided Propagation.This is my model . I created it using Keras layers : I am glad to provide more code if needed ."
"I 'm tryinig to get haystack working with a class-based generic view according to the documentation here . I can get results from a SearchQuerySet in the shell , so the models are being indexed . But I ca n't get the view to return a result on the page.The main reason for using the generic view is that I want to extend later with more SQS logic . I 'm probably missing something obvious ... views.py : search_indexes.py : forms.py"
"I want to embed Python 3.3.4 in my C++ application so that : Python 's standard library is always taken from a zip archive alongside my app 's executable ( should n't depend on any environment vars etc ) ; my own custom .py modules are imported from another folder or zip archive alongside the executable.And , in fact , I 've almost managed to do it right . The only thing that still does not work is importing the standard library from a ZIP archive : it works ok as a simple directory , but whenever I try to zip it , initialization fails with the following error : Is it even possible with latest Python ? I 've googled a lot for it and lots of sources claim that putting correct `` python33.zip '' near the executable should work . Still , my experiments prove otherwise . What am I missing ? Here 's my test code - a minimal console application made by MS Visual Studio 2010 , running on Windows XP SP3 , with some comments as to what I tried and what are the results :"
Here is my project folder structure : When I run the following command python manage.py startapp budget it creates a new folder named budget beside the folder budgetApp.But I want to create all my apps folder inside the `` apps '' folder .
"I 'm learning to program and I am using Python to start . In there , I see that I can do something like this : However , if I try and do the same with print it fails : Why is print different than a function I create ? This is using Python v2.7.5 ."
"I 'm trying desperately to understand the taps argumentin the theano.scan function . Unfortunately I'mnot able to come up with a specific question.I just do n't understand the `` taps '' mechanism.Well I ok . I know in which order the sequencesare passed to the function , but I do n't know the meaning . For example ( I borrowed this code fromanother Question Python - Theano scan ( ) function ) : Setting taps to -1 does make sense to me . As far as I understandit 's the same as not setting the taps value and the whole vector 'x0'is being passed to the addf function . x0 will then be addedwith the `` step '' parameter ( int 2 which will be broadcasted to the same size ) .In the next iteration the result [ 4 , 5 ] will be the input and so onwhich yields the following output : Setting taps to -3 however yields the following output : I do n't have any explanation how the scan function creates thisoutput . Why is it just a list now ? The `` print ( a1 ) '' turns out to be as expected Although I know that this is the value that a1 should have , I do not know how to interpret it . What is the t-3 th value of x0 ? The theano documentationdoes n't seem to be all to detailed about the taps argument ... so hopefully one of you guys will be.Thx"
"We all know that calling this function in Python will produce RuntimeError : maximum recursion depth exceededI wrote its sligtly modified version : The second function runs forever without throwing RuntimeError . What 's more , I was not able to stop it with CtrlC combination . I dont understand why calling f2 ( ) does not throw RuntimeError . Can you explain it please ?"
"I would like to know what the best practices are for using SQLALchemy declarative models within business logic code . Perhaps stackexchange.codereview may have a been a better place to ask this , but I 'm not sure . Here 's some background . Let 's say I have a bunch of classes doing various things . Most of them have little or nothing to do with each other.Each such class has between a hundred to thousand lines of code doing things that have precious little to do with the database . In fact , most of the classes are n't even database aware so far . I 've gotten away with storing the actual information in flat files ( csv , yaml , so on ) , and only maintaining a serial number table and a document path - serial number mapping in the database . Each object retrieves the files it needs by getting the correct paths from the database ( by serial number ) and reconstructs itself from there . This has been exceedingly convenient so far , since my 'models ' have been ( and admittedly , continue to be ) more than fluid . As I expand the involvement of the database in the codebase I currently have , I seem to have settled on the following model , separating the database bits and the business logic into two completely separate parts , and joining them using specific function calls instead of inheritance or even composition . Here is a basic example of the kind of code I have now ( pseudocode-quality ) : module/db/models.py : module/db/controller.py : module/example.py : I 'm not convinced this is the way to go . I have a few models following this pattern , and many more that I should implement in time . The database is currently only used for persistence and archiving . Once something is in the database , it 's more or less read only from there on in . However , querying on it is becoming important . The reason I 'm inclined to migrate from the flatfiles to the database is largely to improve scalability . Thus far , if I wanted to find all instances ( rows ) of Example with some_var = 3 , I 'd have to construct all of the instances from the flat files and iterate through them . This seems like a waste of both processor time and memory . In many cases , some_var is actually a calculated property , and reached by a fairly expensive process using source data contained in the flat file . With the structure above , what I would do is query on Example , obtain a list of 'id 's which satisfy my criterion , and then reconstruct just those module instances.The ORM approach , however , as I understand it , would use thick models , where the objects returned by the query are themselves the objects I would need . I 'm wondering whether it makes sense to try to move to that kind of a structure.To that end , I have the following 'questions ' / thoughts : My instinct is that the code snippets above are anti-patterns more than they are useful patterns . I ca n't put my finger on why , exactly , but I 'm not very happy with it . Is there a real , tangible disadvantage to the structure as listed above ? Would moving to a more ORM-ish design provide advantages in functionality / performance / maintainability over this approach ? I 'm paranoid about tying myself down to a database schema . I 'm also paranoid about regular DB migrations . The approach listed above gives me a certain peace of mind in knowing that if I do need to do some migration , it 'll be limited to the _load_from_db and _sync_to_db functions , and let me mess around willy nilly with all the rest of the code . I 'm I wrong about the cost of migrations in the thick-Model approach being high ? Is my sense of security in restricting my code 's db involvement more of a false sense of security rather than a useful separation ? If I wanted to integrate Example from module/db/models.py with Example from module/example.py in the example above , what would be the cleanest way to go about it . Alternatively , what is an accepted pattern for handling business-logic heavy models with SQLAlchemy ? In the code above , note that the business logic class keeps all of it 's information in 'private ' instance variables , while the Model class keeps all of it 's information in class variables . How would integrating these two approaches actually work ? Theoretically , they should still 'just work ' even if put together in a single class definition . In practice , does it ? ( The actual codebase is on github , though it 's not likely to be very readable )"
"Let 's preface this question by saying that you should use __new__ instead of __init__ for subclassing immutable objects.With that being said , let 's see the following code : This works in python2 , but in python3 I get : Why does this happen ? What changed in python3 ?"
"I would like to retrieve some data from quandl and analyse them in Julia . There is , unfortunately , no official API available for this ( yet ) . I am aware of this solution , but it is still quite limited in functionality and does n't follow the same syntax as the original Python API.I thought it would be a smart thing to use PyCall to retrieve the data using the official Python API from within Julia . This does yield an output , but I 'm not sure how I can convert it to a format that I would be able to use within Julia ( ideally a DataFrame ) .I have tried the following.Julia converts this output to a Dict { Any , Any } . When using returns = `` numpy '' instead of returns = `` pandas '' , I end up with a PyObject rec.array.How can I get data to be a Julia DataFrame as quandl.jl would return it ? Note that quandl.jl is not an option for me because it does n't support automatic retrieval of multiple assets and lacks several other features , so it 's essential that I can use the Python API.Thank you for any suggestions !"
"The operation I 'm confused about looks like this . I 've been doing this on regular Numpy arrays , but on a memmap I want to be informed about how it all works.This is what I used on a normal numpy array.Now . Considering arr1 is now a 20GB memmapped array , I have a few questions:1 : arr2 would be a regular numpy array , I 'd assume ? So executing this would be disastrous memory wise right ? Considering I 've now created arr2 as a Memmapped array of correct size ( filled with all zeroes ) . 2 : vsWhat is the difference ? 3.Would it be more memory efficient to separately calculate np.argsort as a temporary memmapped array and np.argsort ( np.argsort ) as a temporary memmapped array and then do the operation ? Since the argsort array of a 20GB array would itself be pretty huge ! I think these questions will help me get clarified about the inner workings of memmapped arrays in python ! Thanks ..."
Can anyone help me understand the difference between rolling and expanding function from the example given in the pandas docs.I give comment to each row to show my understanding of the calculation . Is that true for rolling function ? What about expanding ? Where are 3 and 7 in 3rd and 4th rows coming from ?
"I have python code using tabula-py for reading PDF to extract the text and then change it to tabular form via tabula-py . But it gives me a warning.This warning is of tabula-py , And Tabula-py is written in Java . So I can not simply use -W ignore to suppress the above warning.Is there any way to remove or suppress the above warning ."
"I wanted to manually code a 1D convolution because I was playing around with kernels for time series classification , and I decided to make the famous Wikipedia convolution image , as seen here . Here 's my script . I 'm using the standard formula for convolution for a digital signal.And here 's the plot I get : As you can see , for some reason , my convolution is shifted . The numbers in the curve ( y values ) are identical , but shifted about half the size of the filter itself . Anyone know what 's going on here ?"
"I have seen other posts addressing similar problem . I know how to generate N positive integers . I also know how to restrict the sum of randomly generated integers . The only problem is satisfying the condition that none of the N values fall out of the specified range.e.g . generate_ints ( n , total , low , high ) should generate n value array such that each value is between low and high and the sum adds up to the total . Any pointers/ help would be greatly appreciated.e.g.generate_ints ( 4 , 40 , 4 , 15 ) should generate something like I do n't care if the numbers are repeated , as long as they are not highly skewed . I am using np.randon.randint ( 5,15 , n ) to select the integer.So far , I have tried the following , but it does n't work -Thanks again ."
"I 'm looking for a Pythonic way to validate arguments when their validation logically depends on the value ( s ) parsed from other argument ( s ) . Here 's a simple example : In this case , parsing this command should cause an error : Adding a mutually exclusive group does n't seem to help here , as the other combos are OK : The validation error should ideally not be tied to -- animal argument nor to -- with-shoes argument , since the interface can not tell you which value needs to change here . Each value is valid , yet they ca n't be used in combination.We can do this with post-processing the args namespace , but I 'm looking for a solution which would cause the parser.parse_args ( ) call to fail , i.e . we actually fail during argument parsing , not afterwards ."
"I am using Visual Studio Code to debug my Python code.I want to redirect one text file in stdin and the output be written in another file.I can reach that by running python directly using the following syntax : Is there a way to allow that while debugging the script ? If that is not possible , can I create a configuration for running python with that parameters . I tried using args parameter in the launch.json , but those are all placed into quotes that defeats the purpose of this redirection ."
"Let 's say I have : I want to declare a function that takes a subclass of A as an argument : What should I put in WHAT_HERE ? If I make this : PyCharm thinks that I should give an instance of A as an argument , not the class itself ."
"It is quite a common mistake to mix up the datetime.strptime ( ) format string and date string arguments using : instead of the other way around : Of course , it would fail during the runtime : But , is it possible to catch this problem statically even before actually running the code ? Is it something pylint or flake8 can help with ? I 've tried the PyCharm code inspection , but both snippets do n't issue any warnings . Probably , because both arguments have the same type - they both are strings which makes the problem more difficult . We would have to actually analyze if a string is a datetime format string or not . Also , the Language Injections PyCharm/IDEA feature looks relevant ."
"I have written a Python module , and I have two versions : a pure Python implementation and a C extension . I 've written the __init__.py file so that it tries to import the C extension , and if that fails , it imports the pure Python code ( is that reasonable ? ) .Now , I 'd like to know what is the best way to distribute this module ( e.g . write setup.py ) so it can be easily used by people with or without the facility to build , or use , the C extension , just by running : My experience is limited , but I see two possible cases : User does not have MS Visual Studio , or the GCC compiler suite , installed on their machine , to build the C extensionUser is running IronPython , Jython , or anything other than CPython . I only have used CPython . So I 'm not sure how I could distribute this module so that it would work smoothly and be easy to install on those platforms , if they 're unable to use the C extension ."
"I am making a game with the pygame module and now I got a problem . The program itself works fantastic , but the fullscreen mode which I wanted to enable does not work . I made a test program for fullscreen mode which works perfect , but when I tried to make the game fullscreen , the display does very strange.First the program starts.You can see it entering in fullscreen mode and displaying a text saying : `` Loading ... '' Then the window disappears and reappears in it 's original non-fullscreen size.the explorer bar on the bottom of the screen is displayed double and then the 2e explorer bar disappears.The game then runs in non-fullscreen mode.This is the program I use : The game runs fine , but not in fullscreen mode.DISPLAYSURF is not edited in the programs . Which means that I do not call pygame.display.set_mode ( ) .I use windows 8 and python 3.4 .Is it because I pass the window object as an argument ? I have no clue of what I did wrong ."
"In numpy I have a 2d array of 1s and 0s . I need to calculate a new array ( same dimensions ) where each element contains the distance to the nearest 1 from the corresponding point in the mask array.e.g.I need b to look like this : PS . I 'll be doing this over very large arrays , so the more efficient the better ! Thanks !"
"I have two custom Django fields , a JSONField and a CompressedField , both of which work well . I would like to also have a CompressedJSONField , and I was rather hoping I could do this : but on import I get : I can find information about using models with multiple inheritance in Django , but nothing about doing the same with fields . Should this be possible ? Or should I just give up at this stage ? edit : Just to be clear , I do n't think this has anything to do with the specifics of my code , as the following code has exactly the same problem : edit 2 : I 'm using Python 2.6.6 and Django 1.3 at present . Here is the full code of my stripped-right-down test example : customfields.pymodels.pyas soon as I add the compressed_json_field = CompressedJSONField ( ) line I get errors when initializing Django ."
"Is it possible to pass one fixture object to another in Pytest ? For example , suppose I want to have two fixture objects : one that 's a numpy array , and another that 's some model of that array : Here arr is type function but arr ( ) is NoneType inside model , which confuseth me.The use case for this is a case where some tests need access to the raw arr itself , while others need access to the models.To make this work , however , one needs to pass one fixture to another ( so we can build the model using the array ) . Is this possible ? Any help others can offer would be greatly appreciated !"
"I am slowly developing a data processing application in Python 2.6 ( * ) . My test data are very small , like 5000 cases , but it is expected that there will be a million cases in the near future and I wondering if my current approach is workable under those conditions.Structure of the problem : I have two csv files , one contains call ( 5000 rows , 20 columns ) and another one details for a call ( 500 rows , 10 columns ) . I have to build a third csv file which will contain all cases from the `` call '' file where additional details where found . Behind the scenes there is some heavy lifting going on ( merging and restructuring data in the details list , data comparison between lists ) . But I am very nervous about building the output list : at the moment the code looks like this : I think that this algorithm will take a very long time , simply because I have to loop about two large lists.So my questions boils down to : How fast are lists in Python and are there better alternatives ? Additionally : The double-List are somewhat inconvenient to handle , because I have to remember the index position of each column - is there a better alternative ? *=I am calling SPSS Version 19 later on , which refuses to work with newer versions of python ."
"I 've got a book on python recently and it 's got a chapter on Regex , there 's a section of code which I ca n't really understand . Can someone explain exactly what 's going on here ( this section is on Regex groups ) ?"
"Suppose I have a dataframe with columns with Strings , Series and Integers that I would like to combine into a new dataframe with the String and the Integer combined with every entry in the Series . How could I go about it ? Given this example : I would like to get a 3x10 dataframe with ; I guess it should n't be too complex but I ca n't find a neat solutions ."
"Please help me find my misunderstanding.I am writing an RPG on App Engine . Certain actions the player takes consume a certain stat . If the stat reaches zero the player can take no more actions . I started worrying about cheating players , though -- what if a player sent two actions very quickly , right next to each other ? If the code that decrements the stat is not in a transaction , then the player has a chance of performing the action twice . So , I should wrap the code that decrements the stat in a transaction , right ? So far , so good.In GAE Python , though , we have this in the documentation : Note : If your app receives an exception when submitting a transaction , it does not always mean that the transaction failed . You can receive Timeout , TransactionFailedError , or InternalError exceptions in cases where transactions have been committed and eventually will be applied successfully . Whenever possible , make your Datastore transactions idempotent so that if you repeat a transaction , the end result will be the same.Whoops . That means that the function I was running that looks like this : Well , that 's not gon na work because the thing is n't idempotent , right ? If I put a retry loop around it ( do I need to in Python ? I 've read that I do n't need to on SO ... but I ca n't find it in the docs ) it might increment the value twice , right ? Since my code can catch an exception but the datastore still committed the data ... huh ? How do I fix this ? Is this a case where I need distributed transactions ? Do I really ?"
"I 'm relatively new to Flask . I have multiple files in my flask project . Up until now , I was using current_app if I wanted to access app object from outside of app.py file . Now I am trying to add cache to my app with flask-caching extension . I initialize this in my app.pyHowever I 'm having troubles with uisng it with views.py file.I have a resource class : I do n't know how to get cache object here to achieve this : I tried to do : from app import cache - > ImportError : can not import name 'cache ' @ current_app.cache.cached - > RuntimeError : Working outside of application context.Part of the structure of my project :"
"I read this SO post on a similar issue but the suggestions there do n't seem to be working . I installed VS Code on my windows machine and added the Python extension . Then I changed the python path for my project to C : \Users\yatin\.conda\envs\tom\python.exe . The .vscode/settings.json has this in it : The status bar in VSCode also shows : But when I do conda env list even after doing conda activate tom in the terminal I get the output : Intead of : Also the packages not installed in base do n't get imported when I try python app.py . What should I do ? Edit : where python runs but does n't give any outputAlso , gives"
"I am familiar with the Pandas Rolling window functions , but they always have a step size of 1 . I want to do a moving aggregate function in Pandas , but where the entries do n't overlap.In this Dataframe : will yield : N/A 519 566 727 1099 12385But I want a fixed window with a step size of 2 , so it yields:519 727 12385Because with a fixed window , it should step over by the size of that window instead ."
"I can not get my Django app to pick up static files in production environment.Within settings.py I 've specifiedand I 've included django.contrib.staticfiles in INSTALLED_APPS.I 've run manage.py collectstatic and it is moving the static files to the /var/www/static/.The html resulting shows , for example < link rel= '' stylesheet '' type= '' text/css '' href= '' /static/myStyleSheet.css '' > from a django file which had < link rel= '' stylesheet '' type= '' text/css '' href= '' { % static 'myStyleSheet.css ' % } '' > .It seems to me it should work but I 'm clearly missing something . I 'm getting 404 errors on the static files ."
I am doing a project in Django . I have installed python 3.7.5 and Django 1.11 . When I try to run the command I am getting The same thing is happening forI have been brainstorming for the last 2 days on how to fix this issue but no luck . Can someone help me out here in fixing this issue ?
"I 'm using Factory Boy for testing a Django project and I 've run into an issue while testing a model for which I 've overridden the save method.The model : The factory : The test : When I run the tests , I get the following error : If I comment out the last last/second 'super ' statement in the Profile save method the tests pass . I wonder if this statement is trying to create the profile again with the same ID ? I 've tried various things such as specifying in the Meta class django_get_or_create and various hacked versions of overriding the _generation method for the Factory with disconnecting and connecting the post generation save , but I ca n't get it to work . In the meantime , I 've set the strategy to build but obviously that wo n't test my save method.Any help greatly appreciated.J ."
I 'm currently trying to port some Scala code to a Python project and I came across the following bit of Scala code : weights is a really long list of tuples of items and their associated probability weighting . Elements are frequently added and removed from this list but checking how many elements have a non-zero probability is relatively rare . There are a few other rare-but-expensive operations like this in the code I 'm porting that seem to benefit greatly from usage of lazy val . What is the most idiomatic Python way to do something similar to Scala 's lazy val ?
"I 'm trying to implement a tcp 'echo server'.Simple stuff : Client sends a message to the server.Server receives the messageServer converts message to uppercaseServer sends modified message to clientClient prints the response.It worked well , so I decided to parallelize the server ; make it so that it could handle multiple clients at time.Since most Python interpreters have a GIL , multithreading wo n't cut it.I had to use multiproces ... And boy , this is where things went downhill.I 'm using Windows 10 x64 and the WinPython suit with Python 3.5.2 x64.My idea is to create a socket , intialize it ( bind and listen ) , create sub processes and pass the socket to the children.But for the love of me ... I ca n't make this work , my subprocesses die almost instantly.Initially I had some issues 'pickling ' the socket ... So I googled a bit and thought this was the issue . So I tried passing my socket thru a multiprocessing queue , through a pipe and my last attempt was 'forkpickling ' and passing it as a bytes object during the processing creating.Nothing works.Can someone please shed some light here ? Tell me whats wrong ? Maybe the whole idea ( sharing sockets ) is bad ... And if so , PLEASE tell me how can I achieve my initial objective : enabling my server to ACTUALLY handle multiple clients at once ( on Windows ) ( do n't tell me about threading , we all know python 's threading wo n't cut it ¬¬ ) It also worth noting that no files are create by the debug function.No process lived long enough to run it , I believe.The typical output of my server code is ( only difference between runs is the process numbers ) : The server code : edit : fixed the signature of `` listen '' . My processes still die instantly.edit2 : User cmidi pointed out that this code does work on Linux ; so my question is : How can I 'made this work ' on Windows ?"
I got this err : How to fix it ? This is my code :
"I am using argparse to build a command with subcommands : mycommand [ GLOBAL FLAGS ] subcommand [ FLAGS ] I would like the global flags to work whether they are before or after the subcommand . Is there a clean way to do this that does n't involve repeating code ? For example : I want to do this for many flags , so repeating myself over and over seems ... unpythonic ."
"Say I have 3 dictionaries of the same length , which I combine into a unique pandas dataframe . Then I dump said dataframe into an Excel file . Example : This code produces an Excel file with a unique sheet : My question : how can I slice this dataframe in such a way that the resulting Excel file has , say , 3 sheets , in which the headers are repeated and there are two rows of values in each sheet ? EDITIn the example given here the dicts have 6 elements each . In my real case they have 25000 , the index of the dataframe starting from 1 . So I want to slice this dataframe into 25 different sub-slices , each of which is dumped into a dedicated Excel sheet of the same main file.Intended result : one Excel file with multiple sheets . Headers are repeated ."
"I 'm addind documentation to my Django project ( github link , the project is open source ) using sphinx , but I 'm getting a lot of bugs when a try to generate autodoc of python files . I 'm including a models.py file with docstrings but , when a run make html I 'm getting different bugs . I have made some changes and the bug is changing , but I 'm not sure if I am fixing them or only generating a new bug . If a remove the inclusion of the models.py file , all run perfectly . In other words , the bug is only generated when I include the following lines in a .rst file : Let me show you what have I done.My first bug was the following , when I run the make html command : WARNING : autodoc : failed to import module u'account.models ' ; the following exception was raised : No module named account.modelsI have added the following lines to the sphinx confg.py file : I have created a folder called docs to include all files generated by the sphinx-quickstart command , for this reason , the value of the abspath is ../../.Ok , now , when I run the make html command , I got the second error : ImproperlyConfigured : Requested setting USE_I18N , but settings are not configured . You must either define the environment variable DJANGO_SETTINGS_MODULE or call settings.configure ( ) before accessing settings.I have integrated the internazionalitation Django module to enable multiple languages in the application , I 'm not sure how it is affecting the docs generation , but , to fix this bug , I have added the following lines to the sphinx conf.py file : Now , if I run the make html command , I have the following message : `` The translation infrastructure can not be initialized before the `` AppRegistryNotReady : The translation infrastructure can not be initialized before the apps registry is ready . Check that you do n't make non-lazy gettext calls at import time.To `` fix it '' ( I 'm not sure if it really has fixed it ) , I have the following lines to the sphinx conf.py file : But now , I 'm getting the following message when I run the make html command : RuntimeError : Model class django.contrib.contenttypes.models.ContentType does n't declare an explicit app_label and is n't in an application in INSTALLED_APPS.And now I ca n't find some option to fix it . If I remove the inclusion of the .py files from my .rst files , all run perfectly , but I need to include the docstrings created in all my python files.How can I fix it ? Thank you so much.Important links : My project settings : settings.pySphinx folder : docs/Note : I have added locally the following lines to the conf.py files : These changes are no visible in the github repository ."
"I want to print a C string using the Python print statement . The array vendorName contains the ASCIIZ C string A ANTHONY & SONS INC. My motivation is to convert the C string to a Python string such that I can use all of the Python string methods . I have a struct : I want to print the string `` vendorName '' which is ASCIIZ . I can print it using printf like this : I have tried this print ( vendrRecord.vendorName ) but it just prints the address . Based on information from Jamie Nicholl-Shelley , I tried print ( cast ( vendrRecord.vendorName , c_char_p ) .value ) but that gives b ' A ANTHONY & SONS INC ' . I want simply A ANTHONY & SONS INC Note that print ( vendrRecord.ytdPayments ) prints correctly ."
"There is one previous question that I could find : Using Django and s3boto , some admin images ( icon_clock.gif & icon_calendar.gif ) do n't displayBut it is very dated ( 2013 ) . I am running django 1.9.1 , apache , wsgi on Ubuntu 14.04.3 LTS.First the problem was that jquery files were missing , but running collectstatic ( manage.py ) from within the virtualenv fixed that problem . However , the two admin media files are still missing . The 404 URL calls are : The strange URL prefix leads one to find several very old questions related to that problem , but it seems to have been depreciated for django 1.9.1.My settings.py looks like this : The outcommented lines were suggestions I found in outdated questions related to the same problem ( none worked ) . All other static files work fine , including most of the admin ones.I 've run out of ideas ."
"I am reading Flask 's tutorial about testing . There 's such code there : My question is : why is there a variable called rv ? What does this abbreviation mean ? I 've been looking for it in Flask source code for a while , and I found several variables called that way : andBut , as you can see , rv here is not even possibly related to rv as response object in testing.I 've been able to find out that `` r '' stays probably for `` response '' , but what does the `` v '' mean ?"
"I have a pandas dataframe df with pandas.tseries.index.DatetimeIndex as index.The data is like this : ... .I want to replace one datapoint , lets day 2.389 in column Close with NaN : Replace did not change 2.389 to NaN . Whats wrong ?"
"I have a numpy array of zeros and ones : y= [ 1,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1 ] I want to calculate the indices of groups of ones ( or zeros ) . So for the above example the result for groups of ones should be something similar to : result= [ ( 0,2 ) , ( 8,9 ) , ( 16,19 ) ] ( How ) Can I do that with numpy ? I found nothing like a group-by function.I experimented around with np.ediff1d , but could n't figure out a good solution . Not that the array may or may not begin/end with a group of ones : I also found a partial solution here : Find index where elements change value numpyBut that one only gives me the indices where the values change ."
"I wish to extend Python socket.socket with a new attribute ( in particular , a queue.Queue , but could be anything else ) . I am tryingto decide whether I should use inheritance or composition , but at some point both pose aproblem I am not sure how to solve : A ) If I use inheritance , with something like : then I have problems when I use operations likebecause the accept method of sockets returns objects of type socket.socket , and not of type MySocket , and I am not sure how to convert one into the other . If there is is a trivial OOP way to do this , I do not know it.B ) The problem before is trivially solved if I used composition instead : I would simply implement something likeBut then I have another problem . When I need to doselect works for socket.socket . With the A ) version I would expect this to work just as is , but with composition , now that MySockets are not instances of socket.socket , select is broken.So , what is the best , pythonistic approach for this ? EDIT : I forgot to say that the first thing I tried was to add the attribute directly to instances of ` socket.socket ' : but I got an exception saying the attribute is unknown for 'socket.socket ' ."
"Consider the following program.All I want is a graceful shutdown when I hit `` CTRL+C '' . However , the executor thread seems to prevent that ( even though I do call shutdown )"
"I have 2 topics being published by an intel realsense camera . One topic is posting a depth image , and the other is posting a color image . Every color image has a corresponding depth image with the same timestamp in the header . I want to merge the color and depth topic into one topic that would publish the pairs of depth and color images . Is there a ROS function that does this based on timestamps ? Here are the subscribers I have created : I want to be able to do something like this ( psuedocode ) : Is there a standard way of doing this in ROS , or a simple way of doing it ?"
"On a Django 2.0 project , i have the following issue on my unit tests and I ca n't find the cause. -- UPDATE : I am using Postgres 10.1 . The problem does n't occur when I switch to sqlite3I am implementing a model which tracks any change on another modelMost of my unit test fails with the following tracebackAny idea , how to fix this problem ? -- UPDATE : 2 unit test which shows the problem . Both are successful when executed alone . It seems that the problem occurs on the unit test tearDown . The Foreign Key constraint fails at this moment because the User has already been deleted ."
"In a big application I am working , several people import same modules differently e.g.import xorfrom y import xthe side effects of that is x is imported twice and may introduce very subtle bugs , if someone is relying on global attributese.g . suppose I have a package mypakcage with three file mymodule.py , main.py and init.pymymodule.py contentsmain.py contentsit printsbecause now there are two lists in two different modules , similarly class A is differentTo me it looks serious enough because classes itself will be treated differentlye.g . below code prints FalseQuestion : Is there any way to avoid this ? except enforcing that module should be imported one way onyl . Ca n't this be handled by python import mechanism , I have seen several bugs related to this in django code and elsewhere too ."
"I have a dataframe that includes columns with multiple attributes separated by commas : df = pd.DataFrame ( { 'id ' : [ 1,2,3 ] , 'labels ' : [ `` a , b , c '' , `` c , a '' , `` d , a , b '' ] } ) ( I know this is n't an ideal situation , but the data originates from an external source . ) I want to turn the multi-attribute columns into multiple columns , one for each label , so that I can treat them as categorical variables . Desired output : I can get the set of all possible attributes ( [ a , b , c , d ] ) fairly easily , but can not figure out a way to determine whether a given row has a particular attribute without row-by-row iteration for each attribute . Is there a better way to do this ?"
I have an array and I would like to produce a smaller array by scanning a 2x2 non-overlappingly windows and getting the maximum . Here is an example : So a matrix like this : Should become this : How can I do this more efficiently ?
"Why ca n't I use an offset when rolling a multi-index DataFrame ? For example , with : If I try grouping and rolling with an offset I get `` ValueError : window must be an integer '' : Not that these following variants meet my needs , but note that grouping and rolling with an int works : And I can roll with an offset on a single-index subset of the DataFrame : If it 's truly impossible to do rolling with offsets on multi-index DataFrames , what would be the most efficient workaround to apply one to each level-0 index item ?"
"I am trying to create a Flask server that streams data to the client using sse . The piece of test-code below seems to do the trick , but I stumbled upon a problem related to handling client disconnects.When using Firefox as the client ( versions 28 or 29 ) , data starts streaming as expected . However , when I reload the page , a new stream is opened ( as expected ) but the old stream still remains . The eventgen ( ) thread handling the stream is never terminated . On other clients ( I tried IE using Yaffle ’ s Polyfill EventSource implementation as well as Chrome ) , reloading or closing the page results in a client disconnect , which results in a server-side socket error 10053 ( client disconnected from the host ) . This terminates the loop and only keeps the active streams alive , which is the expected behavior.Using Process Explorer , I noticed that the TCP connection on the client ( Firefox ) side hangs in the state FIN_WAIT2 , while the connection at the server side hangs in the state CLOSE_WAIT . The strange thing is that on 1 out of the 3 machines ( all Win 7 x64 ) running Firefox I tested this on , the disconnects were handled correctly . Running on Python 2.6.5 and 2.7.6 produced the same results.I also tried replacing the built-in Flask server with the greenlet-based gevent WSGIserver , but this results in exactly the same behavior . Furthermore , some form of threading/eventlets should be used since otherwise running the eventgen ( ) loop blocks the server.The test code below serves the page defined in make_html ( ) when browsing to localhost:5000 and opens a stream to /stream . The stream shows massages of the form { `` content '' : 0.5556278827744346 , `` local_id '' : 4 , `` msg '' : 6 } , where local_id is the id of the stream opened and msg is the number of the current message in this stream ."
"I understand the purpose of joining a thread , I 'm asking about resource use . My specific use-case here is that I have a long-running process that needs to spawn many threads , and during operation , checks if they have terminated and then cleans them up . The main thread waits on inotify events and spawns threads based on those , so it ca n't block on join ( ) calls , because it needs to block on inotify calls.I know that with pthreads , for instance , not joining a terminated thread will cause a resource leak : PTHREAD_JOIN ( 3 ) : Failure to join with a thread that is joinable ( i.e. , one that is not detached ) , produces a `` zombie thread '' . Avoid doing this , since each zombie thread consumes some system resources , and when enough zombie threads have accumulated , it will no longer be possible to create new threads ( or processes ) .Python 's documentation says no such thing , though , but it also does n't specify that the join ( ) can be disregarded without issue if many threads are expected to end on their own without being joined during normal operation.I 'm wondering , can I simply take my thread list and do the following : For each check , or will this leak ? Or must I do the following ?"
"I have a python package that I 'm writing and I 'm having an issue where the standard library is being imported instead of my files because of name clashes.For example , a file structure like below : package/__init__.pypackage/module.pypackage/signal.pyI get the following results when I run this : I would like to get : The actual question : So , when I run module.py , it 's import signal goes to the stdlib version . How am I able to force module.py to import signal.py instead ? As noted in the tags , this needs to be able to run on python-2.4.3 . While that is an old version , it 's what is included in RHEL 5.Some additional informationJust for a bit more information , I explicitly have the following setup : Please note that when I ran ./package/module.py that the print statement in ./package/signal.py was not fired . This implies that the signal that was loaded is the one from the stdlib ."
"My project name is timecaptureHere is relevant portion of timecapture/settings.pyAnd here is timecapture/models.pyThe tables in db after running fresh migrate are : I have played around with class meta settings in my timeuser model , but none of them gave any different results . When I try to create user or super user it gives error :"
"I have a problem with contourf function of matplotlib . I have a txt data file from which I am importing my data . I have columns of data ( pm1 and pm2 ) and I am performing a 2D histogram . I want to plot this data as a 3D histogram and as a contour plot to see where is located the maximum values.This is my code : I can plot the 3d bar graph but I am not able to plot the contour one , If I place hist in the contourf function I get the error : Length of x must be number of columns in z and if I place dz I get Input z must be a 2D arrayI also have tried using xedges and yexges but this does n't solve the problem.I think that the problem is related with the shape of the return of the function histogram2D . But I do n't know how to solve it.I would also like to perform a 3D bar plot with a colorcode changing form the minimum to the maximum value . Is there anyway to make this ? Thank you"
"If I have a numpy.ndarray A and a scipy.sparse.csc_matrix B , how do I take A dot B ? I can do B dot A by saying B.dot ( A ) , but the other way I can only think of this : Is there a more direct method to do this ?"
"I have a table that goes something like this : and I 'd like to select all the rows that belong to a group of IDs . I know that I can do something liketo select all of one ID 's row , and I could doto get two IDs ' rows . But imagine I have ~100,000 rows with over 1,000 unique IDs ( which are distinct from row indices ) , and I want to select all the rows that match a set of 10 IDs.Intuitively I 'd like to do this : but Python rejects this syntax . The only way I can think of is to do the following : which seems very inelegant to me - too much code and no flexibility for different lengths of lists.Is there a way around my problem , such as using list comprehensions , or the .any ( ) function ? Any help is appreciated ."
"I have a DataFrame df1 : I need to calculate a variable called thr , where Nis the number of rows of df1.The problem is that when I calculate thr , Python throws a negative value ( although all of the inputs are positive ) : Possible hintThe number of rows N istherefore , Taking into account that the max value that can be assigned to a int32 is 2147483647 , I suspect that NumPy considers type ( thr ) = < int32 > , when it should be < int64 > . Does this make sense ? Please note that I have not written the code that generates df1 becauseHowever , if it is needed to reproduce the error , let me know . Thanks in advance ."
"I am using h5py to store intermediate data from numerical work in an HDF5 file . I have the project under version control , but this does n't work well with the HDF5 files because every time a script is re-run which generates a HDF5 file , the binary file changes even if the data within does not.Here is a small example to illustrate this : I have looked in the HDF5 file format documents and at the h5py documentation but have n't found anything which helps me with this . My questions are : Why does the file change even though I 'm saving the same data ? How can I stop it changing , so version control only sees a new version of the file when the actual numerical content has changed ? Thanks"
When I make a class definition I always go a = A ( 'hello ' ) print a.arg 'hello'But what I found in line 133 and 134 of https : //github.com/Pylons/webob/blob/master/src/webob/request.py made me think what is the difference between what I did in Class A with : b = B ( 'goodbye ' ) print b.arg 'goodbye '
"BackgroundLet 's say I have a url pattern with parameters that will link me to a view in django : And lets say I have a thing and a feature : Now Django gives me the awesome ability to get a url that brings me to a page dedicated to a specific feature of a specific thing . I can do that like so : QuestionNow I 've stumbled into a situation that I need to specifically address . I 'm not looking for a workaround - I 'm looking to solve the following problem : Given a url 's name , how do I get the list of kwarg argument names needed to reverse that url ? I am looking for the function get_kwarg_names_for_url . It behaves like so : url_kwarg_names is now the list of every keyword I need to supply to Django 's reverse function in order to reverse the url named `` thing_feature '' .Any help is appreciated ! SolutionBased on knbk 's answer I was able to come up with the following solution :"
I want to read as many 24 bit chunks as possible from a file.How can I do this using bitstrings ' ConstBitStreamwhen I do n't now how many chunks there are ? Currently I do this : ( here I have to calculate the number of events beforehand )
"I 'm making a program for AIX 5.3 in Python 2.6.1 that interfaces with an IMAP server . I 'm getting an exception which I do n't know how to catch - it does n't seem to have a name that I can use with `` except '' . The error seems to be some kind of timeout in the connection to the server.The last part of the stack trace looks like this : I only want to catch this specific error , so that I can reconnect to the IMAP server when it happens . What 's the syntax for catching this kind of exception ?"
"I 'd like to use a self.attr of a unittest.TestCase class , however it seems it is not persistent between tests : This gives the following output : Does the instance of unittest.TestCase change between test runs ? Why ?"
Scenario 1This involves using one `` gateway '' route in app.yaml and then choosing the RequestHandler in the WSGIApplication.app.yamlmain.pyScenario 2 : This involves defining two routes in app.yaml and then two separate scripts for each ( page1.py and page2.py ) . app.yamlpage1.pypage2.pyQuestionWhat are the benefits and drawbacks of each pattern ? Is one much faster than the other ?
"I 'm following the instructions mentioned here : https : //api.stackexchange.com/docs/authenticationBut since there is no code provided , I 'm not able to understand the flow correctly.I 've been trying to get the authentication part done using two methods below but I have hit a deadend.1 ) 2 ) I 'm not sure how to go forward from here ? I need to use the access token that is returned and use it to query the API . A sample code would really really help ! Thanks.EDIT : This is the code I 'm using currently : Instead of having to click on the authorization_url and get the token , is there a way I can directly fetch the token within the script itself ?"
"Suppose I have a Jinja2 template , and I 'm using Flask-Babel to translate my project . For example : So I have a sentence with a link and an emphasis . Suppose I then want to translate my sentence . The obvious way would be to use gettext ( ) or the { % trans % } tag : Obviously the problem is that this breaks up the sentence into multiple fragments that do n't translate well . This would result in the translator considering the string `` The '' , `` best '' , `` way of using the Internet is to use '' , and `` our site '' as all separate strings , plus the punctuation . Of course , the translator will want to restructure the sentence , and choose what words to link and emphasize separately.So in light of that , what 's the solution ? How do I have a sentence with tags in it that translates as one unit ?"
I am trying to create a program that asks the user for three words and prints 'True ' if the words are entered in dictionary order . E.G : Here is my code so far :
But I ca n't set it to a variable ... I want to be able to set it to a variable so I can do stuff with it .
"I have a tensorflow model which I am training on google-colab . The actual model is more complex , but I condensed it into a reproducible example ( removed saving/restoring , learning rate decay , asserts , tensorboard events , gradient clipping and so on ) . The model works reasonably ( converges to acceptable loss ) and I am looking for a way to speed up the training ( iterations per second ) . Currently on colab 's GPU it takes 10 minutes to train for 1000 iteration . With my current batch size of 512 it means that the model processes ~850 examples per second ( I would prefer to have a batch size of 512 unless other sizes provide reasonable speedup . By itself changing batch size does not change the speed ) .So currently I have a data stored in tfrecord format : here is a 500Mb example file , the total data-size is ~0.5Tb . This data passes through a reasonably heavy preprocessing step ( I ca n't do preprocessing beforehand as it will increase the size of my tfrecords way above what I can afford ) . Preprocessing is done via tf.data and the output tensors ( ( batch_size , 8 , 8 , 24 ) which is treated as NHWC , ( batch_size , 10 ) ) are passed into a model . The example colab does not contain a simplified model which serves just as an example.I tried a few approaches to speedup the training : manual device placement ( data pre-processing on cpu , propagations on gpu ) , but all my attempts resulted in worse speed ( from 10 % to 50 % increase ) .improve data preprocessing . I reviewed tf.data video and data tutorials . I tried almost every technique from that tutorial got no improvement ( decrease in speed from 0 % to 15 % ) . In particular I tried : dataset.prefetch ( ... ) passing num_parallel_calls to mapcombining map and batch in tf.contrib.data.map_and_batchusing parallel_interleaveThe code related to data preprocessing is here ( here is a full reproducible example with example data ) : I am looking for practical solutions ( I have tried significant amount of theoretical ideas ) which can improve the the speed of training ( in terms of examples/second ) . I am not looking for a way to improve the accuracy of the model ( or modify the model ) as this is just a test model.I have spent significant amount of time trying to optimize this ( and gave up ) . So I would be happy to award a bounty of 200 for a working solution with a nice explanation ."
Here is a string : I 'd like to capture from `` ADDTIONAL '' to `` Languages '' so I wrote this regex : However it only catches everything in between ( [ \s\S ] + ) . It does NOT catch ADDTIONAL & Languages . What am I missing here ?
"I 'm trying to import xml.dom.minidom but pycharm does n't find it , altough it finds xml.entree / xml.parser / xml.sax.The rest of the standart libraries work all fine.The xml file ( beispiel.xml ) should n't be the problem , because it has n't `` xml '' in the name.I 'm not shure , what it could be . It could be , because i have installed python in D : ... and pycharm in C : ... but i dont think that 's the problem ."
"Assume I have N lists ( vectors ) and I want to choose x of them 1 < x < [ N ] ( x is not predetermined ) so I will get the maximum value of func ( lists ) .For example : Possible results : If I 'll write a naive solution and just run all the combinations it will take forever 2^N.I 'm trying to find a solution using cvxpy or maybe scipy.optimize.minimizebut I find it hard to understand the kind of function I need to use for my problem , thought maybe I should try evolutionary algorithm to find an approximate answer , Or maybe I should use portfolio optimization instead ."
"I 've got an array which contains a bunch of points ( 3D vectors , specifically ) : And I would like to multiply each one of those points by a transformation matrix : How can I do this efficiently ?"
"I wanted a quick and dirty way to get some file names without typing in my shell , so I have this following piece of code : Now this all works fine , but it does create a superfluous tkinter GUI that needs to be closed . I know I can do this to suppress it : But it does n't mean it 's not loaded on the back . It just means now there 's a Tk ( ) object that I ca n't close/destroy.So this brought me to my real question.It seems each time I create a Tk ( ) , regardless if I del or destroy ( ) it , the memory is n't freed up . See below : As seen , python does n't free up the usage even after every instance of Tk ( ) is destroyed and roots deleted . This however is n't the case for other objects : So my question is , why is it different between Tk ( ) and my Foo ( ) , and why does n't destroying/deleting the Tk ( ) created free up the memory taken up ? Is there something obvious I 've missed ? Is my test inadequate to confirm my suspicion ? I 've searched here and Google but found little answers.Edit : Below are a few other methods I 've tried ( and failed ) with the recommendations in the comments :"
"I 'm using pycharm.2017.1.2.I installed anaconda2 with py3 environment.in Pycharm , I 'm using Python3 interpreter , and the code is simply : In Python console in Pycharm , it prints builtins.If I click the 'run ' button , it prints main as expected.Why does the PyCharm Python console print builtin instead of main ?"
"I 'm trying to set up a python package for distribution . I have n't done this before , so I 'm pretty hazy about best practices , or even how to make things work , and I 've run into a problem . My project structure looks like this : The problem is with the JS files under static . I started with distutils . I included this line in MANIFEST.in : ... and that caused the JS to be included in the package . Nice . Then I read that if I want my setup script to automatically install dependencies , I need to use setuptools rather than distutils . So , I switched : I import setup from setuptools rather than distutils . But now , when I say setup.py sdist , this line appears in the output : ... and the JS files are not included in the distribution . After a long session with the debugger , I located the problem in setuptools/__init__.py . Here it is : os.walk does not by default follow symlinks . So the setuptools version of findall is n't looking in my js directory . distutils findall did n't use os.walk -- instead it had its own directory walking code . From the changelog , it appears that distutils findall had a problem with broken symlinks , which this replacement was intended to address . It would have been easy enough to use os.walk ( dir , followlinks=True ) , but perhaps there was some reason why that was a bad idea ? So I 'm kind of stuck . Either I change the structure of my project , which I 'm loathe to do , or I muck with distutils and setuptools internals , a prospect that makes me even less happy . Any suggestions ?"
"Update to the newly release ipython5 today . Started up the interactive prompt and received : Yanked out my old config settings to customize and colorize the prompt and went looking for the new way to customize the prompt and found it , very cool . Used the new class style from the example code : Put this into a startup script and it works great , except it by default does n't colorize the Token line , the Token.Prompt is made light green . Attempted to use the old config method colors , ( r ' { color.Green } ' ) but that does n't work here . Any pointers in the correct direction would be great.Thanks !"
"As I understand , numpy.linalg.lstsq and sklearn.linear_model.LinearRegression both look for solutions x of the linear system Ax = y , that minimise the resdidual sum ||Ax - y||.But they do n't give the same result : What am I overlooking ?"
"When testing if a numpy array c is member of a list of numpy arrays CNTS : I get : ValueError : The truth value of an array with more than one element is ambiguous . Use a.any ( ) or a.all ( ) However , the answer is rather clear : c is exactly CNTS [ 1 ] , so c in CNTS should return True ! How to correctly test if a numpy array is member of a list of numpy arrays ? The same problem happens when removing : ValueError : The truth value of an array with more than one element is ambiguous . Use a.any ( ) or a.all ( ) Application : test if an opencv contour ( numpy array ) is member of a list of contours , see for example Remove an opencv contour from a list of contours ."
"Consider the following ( excel ) dataset : My goal is to fill in missing values using the following condition : Denote as R the pairwise correlation between the above two columns ( around 0.68 ) . Denote as R* the correlation after the empty cells have been filled in . Fill in the table so that ( R - R* ) ^2 = 0 . This is , I want to keep the correlation structure of the data intact.So far I have done it using Matlab : where the function my_correl is : This function computes the correlation manually , where every missing data is a variable x ( i ) .The problem : my actual dataset has more than 20,000 observations . There is no way I can create that rho formula manually.How can I fill in my dataset ? Note 1 : I am open to use alternative languages like Python , Julia , or R. Matlab it 's just my default one.Note 2 : a 100 points bounty will be awarded to the answer . Promise from now ."
"In numpy and tensorflow it 's possible to add matrices ( or tensors ) of different dimensionality if the shape of smaller matrix is a suffix of bigger matrix . This is an example : For these two matrices operation x+y is a shortcut for : In my case however I have matrices of shape ( 10,7,5 ) and ( 10,5 ) and likewise I would like to perform the + operation using similar logic : In this case however x+y operation fails as neither numpy nor tensorflow understands what I want to do . Is there any way I can perform this operation efficiently ( without writing python loop myself ) ? In so far I have figured that I could introduce a temporary matrix z of shape ( 10,7,5 ) using einsum like this : but this creates an explicit three-dimensional matrix ( or tensor ) and if possible I would prefer to avoid that ."
"I have a simple code that runs a GET request per each item in the generator that I 'm trying to speed up : Right now this runs on a single thread and takes forever since each REST call waits until the previous REST call finishes.I have used multithreading in Python from a list before using this great answer ( https : //stackoverflow.com/a/28463266/1150923 ) , but I 'm not sure how to re-use the same strategy on a generator instead of a list.I had some advise from a fellow developer who recommended me that I break out the generator into 100-element lists and then close the pool , but I do n't know how to create these lists from the generator.I also need to keep the original order since I need to yield record in the right order ."
"I already know that one can implement a class that inherits from SimpleTestCase , and one can test redirection by : However , I am wondering what is the way I can check for redirection using pytest : However , I am getting the following error : E TypeError : assertRedirects ( ) missing 1 required positional argument : 'expected_url'Is there any way I can use pytest to verify redirection with Django ? Or I should go the way using a class that inherits from SimpleTestCase ?"
"I have a GUI application that needs to fetch and parse diverse resources from network beside the GUI main loop.I 've searched for options using the python multiprocessing module since these fetch actions not only contain blocking IO but also contain heavy parsing , so multiprocessing may be better option here than python threads.It would be easy using Twisted but this time Twisted is not a option.I found a simple solution here : Python subprocess : callback when cmd exitsThe problem is that the callback magically is n't called within in the MainThread.So I come up with the following solution : delegate.pyusageoutputSo far this works pretty well , but my experience with the multiprocessing module are moderate and I 'm a bit unsure if this will run without effects . My question is - what are the things I should especially care of while using multiprocessing in such a way ... or are there 'more correct ' patterns for a asynchronous callback mechanism using the python standard lib ?"
"I have a pandas dataframe like the following : There can be 100s of columns , in the above example I have only shown 4.I would like to extract top-k columns for each row and their values.I can get the top-k columns using : which , for k=3 gives : But what I would like to have is : Is there a pand ( a ) oic way to achieve this ?"
"I have a 3d grid with coordinatesand I need to index points ( i.e . x [ i ] , y [ j ] , z [ k ] ) within some radius R of a position ( x0 , y0 , z0 ) . N_i can be quite large . I can do a simple loop to find what I needbut this quite slow . Is there a more natural/ faster way to do this type of operation with numpy ?"
"I 'm writing a very simple Tree class : I 'd like to be able to perform both DFS and BFS traversal with a simple loop , i.e . : In C++ , for example , you can have multiple types of iterators - so I could define both a DFS and a BFS iterator and use one or the other depending on what type of traversal I wanted to do . Is this possible to do in Python ?"
"I am using scikit learning 's StandardScaler ( ) and notice that after I apply a transform ( xtrain ) or fit_transform ( xtrain ) , it also changes my xtrain dataframe . Is this supposed to happen ? How can I avoid the StandardScaler from changing my dataframe ? ( I have tried using copy=False ) At this stage , I would expect xtrain to NOT have changed while xtrain2 to be a scaled version of xtrain . But when I run describe ( ) on the 2 dataframes , I see they are both the same and both have been scaled . Why is that ? I experience the same problem when I do :"
When I run the command : I get the error : How can I get the pydoc documentation for django.views.generic ?
"I 'm interested in getting more information from a file using python.I know that using os.stat ( such as below ) returns information on the file , but I 'd like to get other attributes from the file such as 'Where from ? 'However , these are n't the attributes I 'm interested . I 'm interested in getting all of the below attributes ( in particular 'Where from ' ) How can I get the 'Where from ' field ? I 've tried using os.access , but that does n't return what I want , and have been searching the documentation on os , so I 'm not sure what else I can try ."
"I have defined a regular expression of hexadecimal value of length 4 as follows : And it works fine and grammatically is also a valid hexadecimal number , but I want to discard it from the valid matching and therefore looking forward to hint on how to extend the current regular expression so that Thanks"
"This is basically a question about the lifetime of temporaries . If a function returns an object , but the reference is not assigned to a variable and is only used to call a method on the returned object , is the temporary reference automatically cleared ? To give a concrete example , suppose there is this chain of method calls : Is the temporary reference returned by o.method_a ( ) automatically cleared when the call to method_b ( ) finishes , as if the line were written like : EDIT : I am interested in a general answer . CPython finalizes objects as soon as the reference count drops to 0 . Other Python implementations might not finalize objects immediately . I am wondering if the Python language is like C++ , which guarantees that temporary objects are destroyed at the end of the statement for which they were created . ( Except that in Python , the question is whether temporary references are cleared at the end of the statement for which they were created . ) In C++ , similar code might be implemented with : The C++ standard states `` Temporary objects are destroyed as the last step in evaluating the full-expression ... that ( lexically ) contains the point where they were created . This is true even if that evaluation ends in throwing an exception . '' In this example , it means that the temporary std : :shared_ptr < B > object created by the call to A : :method_a ( ) is destroyed immediately at the end of evaluation of the full-expression o.method_a ( ) - > method_b ( ) ; . Destroying a std : :shared_ptr means clearing a reference to the shared object ."
I 've tried setting meld as my mergetool to use with git doing : As outlined in answers to questions like : How to set Meld as git mergetoolI used to have this working on my old work machine but now on my new machine where I have Python3 installed instead of 2.7 I am getting the following error whenever I try git mergetool : C : /Program Files/Git/mingw64/libexec/git-core/mergetools/meld : c : /Progra~2/Meld/bin/meld : C : /msys64/MINGW32/bin/python3.exe : bad interpreter : No such file or directoryAny ideas what extra steps I need to make to get this to work with Python3 ? EDIT : I have tried pointing directly to Meld.exe too but that causes the following crash :
Ahead Warning ! A Python newbie and quite a spoiled question ahead ! Is there a way to shorthand : Into something like : Using operator overloading like syntax or otherwise ? Not talking about old style string formatting which took typed % s ...
"I 'm updating some calculations where I used pymc2 to pymc3 and I 'm having some problems with samplers behavior when I have some discrete random variables on my model . As an example , consider the following model using pymc2 : It 's not really representative of anything , it 's just a model where one of the unobserved variables is discrete . When I sample this model with pymc2 I get the following results : But when I try the same with PYMC3 , I get this : It looks like the variable A is not being sampled at all . I did n't read a lot about the sampling method used in pymc3 , but I noticed it seems to be particulary aimed for continuous models . Does this means it rules out discrete unobserved variables on the model or is there some way to do what I 'm trying to do ?"
"I 'm trying to generate the morris sequence in python . My current solution is below , but I feel like I just wrote c in python . Can anyone provide a more pythonic solution ?"
"I would like to create a Cython function that reads an array and returns an array . This function would be called from within other cdef functions , not python def functions . Here 's what I have . In my .pxd file : In my .pyx file : But when I compile , I get the error : `` Operation not allowed without gil '' Can someone please help ?"
Here is some dummy data I have created for my question . I have two questions regarding this : Why is split working by using str in the first part of the query and not in the second part ? How come [ 0 ] is picking up the first row in part 1 and the first element from each row in part 2 ?
"I have been googling for a while and could n't figure out a way to do this . I have a simple Flask app which takes a CSV file , reads it into a Pandas dataframe , converts it and output as a new CSV file . I have managed to upload and convert it successfully with HTMLwhere after I click submit , it runs the conversion in the background for a while and automatically triggers a download once it 's done . The code that takes the result_df and triggers download looks likeI 'd like to add a progress bar to pd_convert which is essentially a pandas apply operation . I found that tqdm works with pandas now and it has a progress_apply method instead of apply . But I 'm not sure if it is relevant for making a progress bar on a web page . I guess it should be since it works on Jupyter notebooks . How do I add a progress bar for pd_convert ( ) here ? The ultimate result I want is : User clicks upload , select the CSV file from their filesystemUser clicks submitThe progress bar starts to runOnce the progress bar reaches 100 % , a download is triggered1 and 2 are done now . Then the next question is how to trigger the download . For now , my convert function triggers the download with no problem because the response is formed with a file . If I want to render the page I form a response with return render_template ( ... ) . Since I can only have one response , is it possible to have 3 and 4 with only one call to /convert ? Not a web developer , still learning about the basics . Thanks in advance ! ====EDIT====I tried the example here with some modifications . I get the progress from the row index in a for loop on the dataframe and put it in Redis . The client gets the progress from Redis from the stream by asking this new endpoint /progress . Something likeIt is currently working but with some issues . The reason is definitely my lack of understanding in this solution . Issues : I need the progress to be reset to 0 every time submit button is pressed . I tried several places to reset it to 0 but have n't found the working version yet . It 's definitely related to my lack of understanding in how stream works . Now it only resets when I refresh the page.How to handle concurrent requests aka the Redis race condition ? If multiple users make requests at the same time , the progress should be independent for each of them . I 'm thinking about giving a random job_id for each submit event and make it the key in Redis . Since I do n't need the entry after each job is done , I will just delete the entry after it 's done.I feel my missing part is the understanding of text/event-stream . Feeling I 'm close to a working solution . Please share your opinion on what is the `` proper '' way to do this . I 'm just guessing and trying to put together something that works with my very limited understanding ."
"I 'm trying to extract the blood vessels from an image , and to do so , I 'm first equalizing the image , applying CLAHE histogram to obtain the following result : And then I 'm using OTSU threshold to extract the blood vessels , but failing to do it well : Here 's the result : Obviously there 's a lot of noise . I 've tried using Median blur , but it just clusters the noise and makes it into a blob , in some places . How do I go about removing the noise to get the blood vessels ? This is the original image from which I 'm trying to extract the blood vessels :"
I want to apply type annotation to the returned object of csv.writer in order to comply with a larger codebase . Unfortunately I can not figure out the fitting return type.If I try to use this classname : I get the following mypy error : Does someone know which type to use in this case . Of course I could use typing.Any but this nullify the sense of a type annotation .
"I have a pandas DataFrame that includes a column of text , and I would like to vectorize the text using scikit-learn 's CountVectorizer . However , the text includes missing values , and so I would like to impute a constant value before vectorizing.My initial idea was to create a Pipeline of SimpleImputer and CountVectorizer : However , the fit_transform errors because SimpleImputer outputs a 2D array and CountVectorizer requires 1D input . Here 's the error message : QUESTION : How can I modify this Pipeline so that it will work ? NOTE : I 'm aware that I can impute missing values in pandas . However , I would like to accomplish all preprocessing in scikit-learn so that the same preprocessing can be applied to new data using Pipeline ."
"I have a few thousands of PDF files containing B & W images ( 1bit ) from digitalized paper forms . I 'm trying to OCR some fields , but sometime the writing is too faint : I 've just learned about morphological transforms . They are really cool ! ! ! I feel like I 'm abusing them ( like I did with regular expressions when I learned Perl ) . I 'm only interested in the date , 07-06-2017 : People filling this form seems to have some disregard for the grid , so I tried to get rid of it . I 'm able to isolate the horizontal line with this transform : I can get the vertical lines as well : Now I can get rid of the grid : The best I got was this , but the 4 is still split into 2 pieces : Probably at this point it is better to use cv2.findContours and some heuristic in order to locate each digit , but I was wondering : should I give up and demand all documents to be rescanned in grayscale ? are there better methods in order to isolate and locate the faint digits ? do you know any morphological transform to join cases like the `` 4 '' ? [ update ] Is rescanning the documents too demanding ? If it is no great trouble I believe it is better to get higher quality inputs than training and trying to refine your model to withstand noisy and atypical dataA bit of context : I 'm a nobody working at a public agency in Brazil . The price for ICR solutions start in the 6 digits so nobody believes a single guy can write an ICR solution in-house . I 'm naive enough to believe I can prove them wrong . Those PDF documents were sitting at an FTP server ( about 100K files ) and were scanned just to get rid of the dead tree version . Probably I can get the original form and scan again myself but I would have to ask for some official support - since this is the public sector I would like to keep this project underground as much as I can . What I have now is an error rate of 50 % , but if this approach is a dead end there is no point trying to improve it ."
The following code is a very simple implementation of a SqlAlchemy ORM with one simple table . The Mytable class tries to inherit from BaseAbstract.The code throws the following exception : Message : metaclass conflict : the metaclass of a derived class must be a ( non-strict ) subclass of the metaclasses of all its basesIf you change the class declaration line to class Mytable ( Base ) : the code will work fine . Also if you change class BaseAbstract ( ABC ) : to class BaseAbstract ( object ) : the code will again work fine.How do I inherit from an abstract class in SQLAlchemy ?
"Using matplotlib 's mpl_connect functionality , one can bind events to function calls . However , the left and right arrow key are by default bound to go `` back '' and `` forward '' in the figure 's history . I would like to disable this default binding.For example : Pressing the left key will now print Left ! to the console . However , when we zoom into the figure , then the left key will also go `` back '' , and zoom back out . ( The right key will go `` forward '' and zoom back in . ) I would like this to not happen - how do I do that ? Making on_key_press return False does n't do the trick . ( Background info : I have bindings set up so that , when the user clicks on the figure , a cursor will appear , centered on the plotted point ( as given by [ 0,1,2,3,4 ] and [ 5,2,1,2,5 ] ) that is closest to where the user clicked . I can make the left and right keys move this cursor to the previous/next data point , but if the user happens to be zoomed in , or has done any other manipulation to the graph , things go bad . )"
"( I have tagged this question as Python as well since I understand Python code so examples in Python are also welcome ! ) .I want to create a simple window in wxWidgets : I create a main panel which I add to a formI associate a boxsizer to the main panel ( splitting it in two , horizontally ) .I add LeftPanel to the boxsizer , I add RightPanel to the boxsizer , I create a new boxsizer ( vertical ) I create another boxsizer ( horizontal ) I create a Notebook widgetI create a Panel and put it inside the Notebook ( addpage ) I add the notebook to the new boxsizer ( vertical one ) I add the vertical sizer in the horizontal oneI associate the horizontal sizer to the RightPanelI add the Left and Right panel to the main sizer.This does n't work ... Maybe I have missed something ( mental block about sizers ) but what I would like to do is to expand the notebook widget without the use of the vertical sizer inside the horizontal one ( it does n't work anyway ) .So my question is . Assuming I want to expand the Notebook widget inside the RightPanel to take up the rest of the right side area of the form , how would I go about doing that ? For those that understand Erlang , This is what I have so far :"
"I have a regex in Python that contains several named groups . However , patterns that match one group can be missed if previous groups have matched because overlaps do n't seem to be allowed . As an example : Produces the output : The 'long ' group does not find a match because 'AAA ' was used-up in finding a match for the preceding 'short ' group.I 've tried to find a method to allow overlapping but failed . As an alternative , I 've been looking for a way to run each named group separately . Something like the following : Is it possible to extract the regex for each named group ? Ultimately , I 'd like to produce a dictionary output ( or similar ) like : Any and all suggestions would be gratefully received ."
"I was doing some practice problems in Coding Bat , and came across this one..My solution was the following.Is there a more pythonic way of doing this ?"
"After compiling a Python function with Numba such as : how can I retrieve the generated LLVM code ( as a string ) of the compiled function ? It looks as though this was available in a previous version of Numba via the lfunc property of the compiled function , but this is n't working.Similar functionality also appeared to exist in the form of dumping the generated LLVM assembly ( during compilation ) . However , this does n't seem to work anymore either - unless I 'm doing something wrong . Having to run a terminal command would not be ideal anyway , as I would really like the code within Python , though I know I can do this with a subprocess.This is to attempt to create a portable version of the Python code at runtime which will be translate ; I welcome any suggestions relating to this.Thanks"
I want to schedule a periodic task with Celery dynamically at the end of another group of task.I know how to create ( static ) periodic tasks with Celery : But I want to create periodic jobs dynamically from my tasks ( and maybe have a way to stop those periodic jobs when some condition is achieved ( all tasks done ) . Something like :
"I would like to decompose a number into a tuple of numbers as close to each other in size as possible , whose product is the initial number . The inputs are the number n we want to factor and the number m of factors desired.For the two factor situation ( m==2 ) , it is enough to look for the largest factor less than a square root , so I can do something like thisSo calling this with 120 will result in 10,12.I realize there is some ambiguity as to what it means for the numbers to be `` close to each other in size '' . I do n't mind if this is interpretted as minimizing Σ ( x_i - x_avg ) or Σ ( x_i - x_avg ) ^2 or something else generally along those lines.For the m==3 case , I would expect that 336 to produce 6,7,8 and 729 to produce 9,9,9.Ideally , I would like a solution for general m , but if someone has an idea even for m==3 it would be much appreciated . I welcome general heuristics too . EDIT : I would prefer to minimize the sum of the factors . Still interested in the above , but if someone has an idea for a way of also figuring out the optimal m value such that the sum of factors is minimal , it 'd be great !"
"Are Unicode literals not allowed in __all__ in Python 2.7.5 ? I 've got an __init__.py file with from __future__ import unicode_literals at the top , along with coding utf-8 . ( There are also some unicode strings in it , hence the future import . ) To ensure that only some of the modules are visible when imported using from mypackage import * , I 've added my class to __all__ . But I get TypeError : Item in `` from list '' not a string . Why is that ? Bug ? However , when I cast the class name to str in __all__ , it works just fine . [ It also works when I specify from mypackage import SomeClass in run.py below ... since the items in __all__ are not processed . ] mypackage/somemodule.py : mypackage/ __init__.pyrun.py : To avoid the error , I change the 'all ' declaration to : which , of course , pylint complains about.My other option is to not import unicode_literals and explicitly cast all the strings in init to unicode with u'uni string ' ."
"I 'm coming from the C # world , so my views may be a little skewed . I 'm looking to do DI in Python , however I 'm noticing a trend with libraries where they all appear to rely on a service locator . That is , you must tie your object creation to the framework , such as injectlib.build ( MyClass ) in order to get an instance of MyClass.Here is an example of what I mean -Is there a way in Python to create a class while automatically inferring dependency types based on parameter names ? So if I have a constructor parameter called my_class , then an instance of MyClass will be injected . Reason I ask is that I do n't see how I could inject a dependency into a class that gets created automatically via a third party library ."
"I 'm looking to migrate from matplotlib to plotly , but it seems that plotly does not have good integration with pandas . For example , I 'm trying to make a weighted histogram specifying the number of bins : But I´m not finding a simple way to do this with plotly . How can I make a histogram of data from a pandas.DataFrame using plotly in a straightforward manner ?"
"I want every url that does not start with 'api ' to use the foo/urls.pyurls.pyfoo/urls.pyThis does not work , any idea ?"
"I am using Python 3.6 for data fitting . Recently , I came across the following problem and I ’ m lacking experience wherefore I ’ m not sure how to deal with this.If I use numpy.polyfit ( x , y , 1 , cov=True ) and scipy.curve_fit ( lambda : x , a , b : a*x+b , x , y ) on the same set of data points , I get nearly the same coefficients a and b . But the values of the covariance matrix of scipy.curve_fit are roughly half of the values from numpy.polyfit.Since I want to use the diagonal of the covariance matrix to estimate the uncertainties ( u = numpy.sqrt ( numpy.diag ( cov ) ) ) of the coefficients , I have three questions : Which covariance matrix is the right one ( Which one should I use ) ? Why is there a difference ? What does it need to make them equal ? Thanks ! Edit : If I use the statsmodels.api , the result corresponds to that of curve_fit ."
"I need to passively install Python in my applications package installation so i use the following : according this : 3.1.4 . Installing Without UI I use the PrependPath parameter which should add paths into Path in Windows environment variables.But it seems not to work . The variables does not take any changes.If i start installation manually and select or deselect checkbox with add into Paths then everything works.Works same with clear installation also on modify current installation . Unfortunately i do not have other PC with Win 10 Pro to test it.I have also tried it with Python 3.6.3 with same results.EDIT : Also tried with PowerShell Start-Process python-3.5.4-amd64.exe -ArgumentList /passive , PretendPath=1 with same results.Also tested on several PCs with Windows 10 , same results , so the problem is not just on single PCEDIT : Of cource all attempts were run as administrator ."
"I need to split space-delimited TCL lists on double braces ... for instance ... This should parse into ... I have tried ... However , that trims the braces in the center ... I ca n't figure out how to only split on the spaces between } } { { . I know I can cheat and insert missing braces manually , but I would rather find a simple way to parse this out efficiently.Is there a way to parse OUTPUT with re.split ( or some other python parsing framework ) for an arbitrary number of space-delimited rows containing { { content here } } ?"
"Why does raising to an integer expressed as a floating point number give different results to raising the same number in its integer form ? E.g . : I 've tried using mpmath 's power ( ) instead , but get the exact same numbers - and error - as the second form.How can I raise to very large non-integer powers and perform mod on them ( e.g . steps taken to perform RSA-like logic using pure math ) in Python ?"
"I would like to create a text file in Python , write something to it , and then e-mail it out ( put test.txt as an email attachment ) . However , I can not save this file locally . Does anyone know how to go about doing this ? As soon as I open the text file to write in , it is saved locally on my computer.I am using smtplib and MIMEMultipart to send the mail ."
"While searching for faster ways to parse command-line arguments in my scripts I came across the argh library . I really like the features of argh but I ’ ve encountered one drawback that stops me from using it , and this has to do with the default help message that gets displayed if I ’ m invoking the —help option : per default the function ’ s docstring is displayed on top of the arguments list . This is great , however the initial formatting is lost . See , for example , the following example scriptwhich would result in the following output Is there an ( easy ) way to get an output like the following ? I 'd prefer to not use annotations to define the argument help messages since that would require me to alter both the function 's docstring AND the help text each time there is something to change ."
"I 'm working on writing tests for a new GeoDjango project I 've started . Normally I used Factory Boy and Faker to create model instances for testing . However it 's not clear to me how you can mock GeoDjango PointField fields . When looking at the record in Spacialite it appears as a binary blob.I 'm totally new to GIS stuff , and a little confused as to how I can create factories for PointFields in Django ."
"I 'm facing a weird situation where a batch file I wrote reports an incorrect exit status . Here is a minimal sample that reproduces the problem : bug.cmdIf I run this script ( using Python but the problem actually occurs when launched in other ways too ) , here is what I get : Note how exit status is reported as 0 even though exit /b 1 should make it be 1.Now the weird thing is that if I remove the inside if clause ( which should not matter because everything after exit /b 1 should not be executed anyway ) and try to launch it : ok.cmdI launch it again : Now the exit status is correctly reported as 1.I 'm at loss understanding what is causing this . Is it illegal to nest if statements ? How can I signal correctly and reliably my script exit status in batch ? Note : calling exit 1 ( without the /b ) is not an option as it kills the whole interpreter and prevents local script usage ."
I 'm working in Python 2.7 and I fond that issue that puzzling me.That is the simplest example : That is OK like expected ... now I 'm trying to change the a ( ) method of object a and what happen is that after change it I ca n't delete a any more : Just to do some checks I 've print the a.a reference before and after the assignmentFinally I used objgraph module to try to understand why the object is not released : As you can see in the post-backref-graph.png image there is a __self__ references in b that have no sense for me because the self references of instance method should be ignored ( as was before the assignment ) .Somebody can explain why that behaviour and how can I work around it ?
"Basically , I 've written an API to www.thetvdb.com in Python . The current code can be found here.It grabs data from the API as requested , and has to store the data somehow , and make it available by doing : What is the `` best '' way to abstract this data within the Tvdb ( ) class ? I originally used a extended Dict ( ) that automatically created sub-dicts ( so you could do x [ 1 ] [ 2 ] [ 3 ] [ 4 ] = `` something '' without having to do if x [ 1 ] .has_key ( 2 ) : x [ 1 ] [ 2 ] = [ ] and so on ) Then I just stored the data by doing self.data [ show_id ] [ season_number ] [ episode_number ] [ attribute_name ] = `` something '' This worked okay , but there was no easy way of checking if x [ 3 ] [ 24 ] was supposed to exist or not ( so I could n't raise the season_not_found exception ) .Currently it 's using four classes : ShowContainer , Show , Season and Episode . Each one is a very basic dict , which I can easily add extra functionality in ( the search ( ) function on Show ( ) for example ) . Each has a __setitem__ , __getitem_ and has_key.This works mostly fine , I can check in Shows if it has that season in it 's self.data dict , if not , raise season_not_found . I can also check in Season ( ) if it has that episode and so on.The problem now is it 's presenting itself as a dict , but does n't have all the functionality , and because I 'm overriding the __getitem__ and __setitem__ functions , it 's easy to accidentally recursively call __getitem__ ( so I 'm not sure if extending the Dict class will cause problems ) .The other slight problem is adding data into the dict is a lot more work than the old Dict method ( which was self.data [ seas_no ] [ ep_no ] [ 'attribute ' ] = 'something ' ) . See _setItem and _setData . It 's not too bad , since it 's currently only a read-only API interface ( so the users of the API should only ever retrieve data , not add more ) , but it 's hardly ... Elegant.I think the series-of-classes system is probably the best way , but does anyone have a better idea for storing the data ? And would extending the ShowContainer/etc classes with Dict cause problems ?"
In the following sample code : pre_save_parent does n't fire when I a Child is created : Is this expected behaviour ?
"The inputs are as follows.For this input , I want to create a combination for each group and create one DataFrame . How can I do it ? The output I want to get :"
I was trying to install Datastax Python Cassandra driver on Ubuntu 14.04.5 . LTS . Installation succeeds but subsequent attempt to use it fails with the error : same installation process and same code works well on RedHat . Google search for error code returns nothing . Anybody has an idea of what could be the issue ?
"I have been working on a script which calculates the rotational shift between two images using cv2 's phaseCorrelate method.I have two images , the second is a 90 degree rotated version of the first image . After loading in the images , I convert them to log-polar before passing them into the phaseCorrelate function . From what I have read , I believe that this should yield a rotational shift between two images . The code below describes the implementation . I am unsure how to interpret the results of this function . The expected outcome is a value similar to 90 degrees , however , I get the value below.How can I make the output correct ?"
"I read this question , but it did n't give me a clear answer : How does Python interpreter look for types ? How does python interpreter know the type of a variable ? I 'm not looking how do get the type . I 'm here looking at what happens behind the scene . In the example below , how does it associate the class int or string to my variable.How does it know that is an int : or that string :"
"I have set up a Sphinx documentation for my project and would like to extract doc strings for the source files and embed them into the final documentation . Unfortunately , the source file 's language ( VHDL ) is not supported by Sphinx . There seems to be no Sphinx domain for VHDL.So my ideas is as follows : Hook into the Sphinx run and execute some Python code before SphinxThe Python codes extracts text blocks from each source file ( the top-most multi-line comment block ) and assembles one reST file per source file , consisting of this comment block and some other reST markup.All source files are listed in an index.rst , to generate the apropriate .. toctree : : directive.The text extraction and transformation is done recursively per source code directory.So the main question is : How to hook into Spinx ? Or should I just import and run my own configuration in conf.py ? I ca n't modify the build process files : Makefile and make.bat , because the real build process runs on ReadTheDocs.org . RTDs only executes conf.py ."
"In order to make it easy for others to help meI have put all the codes here https : //pastebin.com/WENzM41kit will starts as 2 agents compete each other.I 'm trying to implement Monte Carlo tree search to play 9-board tic-tac-toe in Python . The rules of the game is like the regular tic-tac-toe but with 9 3x3 sub-boards . The place of the last piece is placed decides which sub-board to place your piece . It 's kind like ultimate tic-tac-toe but if one of the sub-board is won the game ends . I 'm trying to learn MCTS and I found some code on here : http : //mcts.ai/code/python.htmlI used node class and UCT class on the website and added my 9 board tic-tac-toe game state class and some other codes . All codes are here : Run the code it will starts as 2 agents compete each other.However , the agent can not play the game well . The poor play is not acceptable . For example , if the ai got 2 on a row in a sub-board and it is his turn again , he does not play the winning move . Where should I start to improve and how ? I tried to change the code of Node class and UCT class but nothing worked.Update : If the board is in below state , and it 's my AI ( playing X ) turn to play on board 8 ( middle sub-board of the third row ) . Should I write specific code that let AI do not play on 1 or 5 ( because then the opponent will win ) or I should let the AI to decide . I tried to write code tell the AI but that way I have to loop through all the sub-board ."
"I have a gappy timeseries stored in a pandas dataframe with a datetimeindex . I now want to identify gaps in the timeseries in order to identify the continuous segments in order to process them individually ( and in some cases glue together segments with short enough gaps between them ) .There 's two main ways I can see to do this . The first is to re-index using various approaches to get a regular timeseries and observe the filled NA values in the gap regions . In my case that leads to lots of additional rows ( i.e . some lengthy gaps ) . You then still need to make an additional step to identify the continuous segments.The other approach , and what I 'm currently using , is to use np.diff to difference the index and find the gaps using np.where . But is there a more native pandas approach to this ? This seems like a fairly common task . I note there are issues with np.diff and pandas with some combinations of numpy and pandas versions so a pandas only solution would be preferable.What would be perfect would be something likefor the dataframe data ."
"Guys , I need to develop a tool which would meet following requirements : Input : XHTML document with CSS rules within head section . Output : XHTML document with CSS rules computed in tag attributesThe best way to illustrate the behavior I want is as follows.Example input : Example output : What tools/libraries will fit best for such task ? I 'm not sure if BeautifulSoup and cssutils is capable of doing this . Python is not a requirement.Any recommendations will be highly appreciated ."
"I am trying to program a method to convert subtitle files , such that there is always just one sentence per subtitle.My idea is the following : For each subtitle:1.1 - > I get the subtitle duration1.2 - > Calculate the characters_per_second1.3 - > Use this to store ( inside dict_times_word_subtitle ) the time it takes to speak the word iI extract the sentences from the entire textFor each sentence:3.1 I store ( inside dict_sentences_subtitle ) the time it take to speak the sentence with the specific words ( from which I can get the duration to speak them ) I create a new srt file ( subtitle file ) which starts at the same time as the original srt file and the subtitle timings can then be taken from the duration that it takes to speak the sentences.For now , I have written the following code : The issue : There are no error messages , but when I apply this to real subtitle files and then watch the video , the subtitles begin correctly , but as the video progresses ( error progression ) the subtitles get less and less aligned with what is actually said.Example : The speaker has finished his talk , but the subtitles keep appearing.SIMPLE EXAMPLE to testThe output looks like this ( yes , there is still some work to do on the sentence recognition par ( i.e . Dr. ) ) : As you can see the original last time stamp is 00:00:29,800 whereas in the output file it is 00:00:29,859 . This might not seem like much in the beginning , but as the video gets longer , the difference increases.The full sample video can be downloaded here : https : //ufile.io/19nuvqb3The full subtitle file : https : //ufile.io/qracb7ai ! Attention : The Subtitle file will be overridden , so you might want to store a copy with another name to be able to compare.Any help to fix that would be very much welcomed ! Method how it could be fixed : The exact timing for words starting or ending an original subtitle is known . This could be used to cross-check and adjust timing accordingly.EDIT : Here is a code to create a dictionary which stores character , character_duration ( average over subtitle ) , and start or end original time stamb , if it exists fot this character.More videos to try : Here you may download more videos and their respective subtitle files : https : //filebin.net/kwygjffdlfi62pjsEdit 3 : ."
"In the example below , attribute x is accessed from the slots of the object even though x is present in __dict__ ( this is not a typical or probably useful case , but I 'm curious ) : Is this order of access ( slots first ) a documented behavior ? or simply an implementation detail ?"
"I am starting with Flask for a couple weeks and is trying to implement i18n and l10n to my Flask app.This is the behavior that I really want to implement : User enters website.com will be redirected to website.com/en/ or website.com/fr/ depends on their Accept-Languages header or default language in their settings.This is my current implementation : This way , when I type website.com or website.com/en/ it will respond with just website.com . Unless I type website.com/fr it would respond with /fr/ . However , I want to always include /en/ even if it is the default option.I have tried the guide URL Processors pattern in Flask doc , but when I typed website.com it responded with 404 error . It only worked fine when I include the language_code into the url—which is not the behavior that I want.Thank you in advance !"
"This is a follow up to my other post Installing signal handler with Python . In short , Linux blocks all signals to PID 1 ( including SIGKILL ) unless Init has installed a signal handler for a particular signal ; as to prevent kernel panic if someone were to send a termination signal to PID1 . The issue I 've been having , is it would seem that the signal module in Python does n't install signal handlers in a way the system recognises . My Python Init script was seemingly , completely ignoring all signals as I think they were being blocked.I seem to have found a solution ; using ctypes to install the signal handlers with the signal ( ) function in libc ( in this case uClibc ) . Below is a python based test init . It opens a shell on TTY2 from which I can send signals to PID1 for testing . It seems to work in the KVM im using for testing ( I 'm willing to share the VM with anyone interested ) Is this the best way around this issue ? Is there a 'better ' way to install the signal handlers without the signal module ? ( I am not at all concerned with portably ) Is this a bug in Python ?"
From the numba website : Is there a way to have numba use python type hints ( if provided ) ?
From Painless Streaming Plots with Bokeh it shows how to stream live data of a single variable . How do you stream multiple lines where there is more than one y variable .
"I want to plot pandas histogram to an axis , but the behavior is really strange . I do n't know what 's wrong here.The error jupyter notebook returns is : The pandas source code is here : https : //github.com/pydata/pandas/blob/d38ee272f3060cb884f21f9f7d212efc5f7656a8/pandas/tools/plotting.py # L2913Totally have no idea what 's wrong with my code ."
"Suppose I was trying to organize sales data for a membership business . I only have the start and end dates . Ideally sales between the start and end dates appear as 1 , instead of missing . I ca n't get the 'date ' column to be filled with in-between dates . That is : I want a continuous set of months instead of gaps . Plus I need to fill missing data in columns with ffill.I have tried different ways such as stack/unstack and reindex but different errors occur . I 'm guessing there 's a clean way to do this . What 's the best practice to do this ? Suppose the multiindexed data structure : And the desired result"
"I 'm trying to understand how Python 's type annotations work ( e.g . List and Dict - not list or dict ) . Specifically I 'm interested in how isinstance ( list ( ) , List ) works , so that I can create my own custom annotations.I see that List is defined as : I 'm familiar with metaclass = xxx but I ca n't find any documentation on this extra = xxx . Is this a keyword or just an argument , and if so , where does it come from and does it do what I 'm after ? Is it even relevant for isinstance ?"
I 'm trying to adapt the following resources to this question : Python conversion between coordinateshttps : //matplotlib.org/gallery/pie_and_polar_charts/polar_scatter.htmlI ca n't seem to get the coordinates to transfer the dendrogram shape over to polar coordinates.Does anyone know how to do this ? I know there is an implementation in networkx but that requires building a graph and then using pygraphviz backend to get the positions.Is there a way to convert dendrogram cartesian coordinates to polar coordinates with matplotlib and numpy ? I just tried this and it 's closer but still not correct :
"I 'm trying to run a python file on a system without python installed . I 'm using py2exe , which gives me a .pyc file which runs fine on my system , but when I give it to a friend without python it tells him Windows ca n't run the file.My setup.py file contains this ; When I run py2exe in the commandline , this is output"
"I 'm trying to use rollapply with a formula that requires 2 arguments . To my knowledge the only way ( unless you create the formula from scratch ) to calculate kendall tau correlation , with standard tie correction included is : I 'm also aware of the problem with rollapply and taking two arguments , as documented here : Related Question 1Github IssueRelated Question 2Still , I 'm struggling to find a way to do the kendalltau calculation on a dataframe with multiple columns on a rolling basis.My dataframe is something like thisTrying to create a function that does thisIn a very preliminary approach I entertained the idea of defining the function like this : Off course It did n't work . I got : I understand is not a trivial problem . I appreciate any input ."
"In python there is this interesting , and very useful tool by which you can pattern match values from tuples on function signature.I do n't see any literature on use of this . What is the vocabulary the python community uses for this ? Is there a compelling reason to not use this ?"
I have the following python function to recursively find all partitions of a set : Can someone help me to translate this into ruby ? This is what I have so far : I get the error `` LocalJumpError : no block given '' . I guess this is because the yield functions work differently in Python and Ruby .
"When choosing a name for a tensorflow variable , this is ok : However , this is not : Putting a / in a name is allowed , but it probably breaks some scoping things : Are there any defined rules as to what character set is allowed in variable names ? And ( with regards to the a/b name ) are there any extra guidelines for what should n't be used ?"
"I would like to sort a QTreeWidget by the alpha_numeric column.Update : Following ekhumoro 's example , I re-implemented the QTreeWidgetItem __lt__ method to use the desired natural sorting.Relevant code : Updated full working example :"
"I 've been spending some of my spare time lately trying to wrap my head around Haskell and functional programming in general . One of my major misgivings has been ( and continues to be ) what I perceive to be the unreadability of terse , functional-style expressions ( whether in Haskell or any other language ) .Here 's an example . I just made the following transformation in some code for work : toThe latter was a lot more satisfying to write ( and , keeping in mind that I myself am the one who wrote it , to read too ) . It 's shorter and sweeter and declares no pesky variables that might become a headache if I made any mistakes in typing out the code . ( BTW I realize I could have used the dict 's get ( ) method in the imperative-style function , but I realized it as I was performing this transformation , so I left it that way in the example . ) My question is : despite all that , is the latter version of the code really more readable than the former ? It seems like a more monolithic expression that has to be grokked all at once , compared to the former version , where you have a chance to build up the meaning of the block from the smaller , more atomic pieces . This question comes from my frustration with trying to decode supposedly easy-to-understand Haskell functions , as well as my insistence as a TA for first-year CS students that one of the major reasons we write code ( that 's often overlooked ) is to communicate ideas with other programmers/computer scientists . I am nervous that the terser style smacks of write-only code and therefore defeats the purpose of communication and understanding of code ."
"I have a Python script that works perfectly fine on my development PC . Both are Windows 7 with the same Python version ( 2.7.9 ) . However on the target machine I get a ValueError : ca n't format dates this earlyThe error seems to come from pywin32 module.The code uses a third-party library invoked by pywin32 : and then fails later on : Now I 'm lost why this is working on my PC and not on the target machine . Both have a `` corporate install '' of Windows 7 , for example , the same Regional and date-time settings.What is the issue ? How might I resolve it ? EDIT : See comments . The cause is probably which C++ runtime is used . I 'm still investigating . I now suspect that it matters which runtimes are present at install time of pywin32 . Why ? Because DependenyWalker on my development PC says that pywin depends on MSVCR90.DLL in my Lotus Notes installation . This tells me it sure is n't `` hard '' linked.Update 30.06.2015 : I was all wrong ... The issue now also happens on my PC.Some further info . The script reads data files and inserts the readmeta-data into a database . Only older files seemed to be affected by thebug , not new ones ( I now think this is assumption is wrong ) . So the idea was to to the initial load on my Dev PC and then hope the issue will never occur again with new files.In case of the PC were the script will run , the files it reads are on aWindows Shared drive ( mapped network drive ) . I do n't have access to thatdrive so I just copied the files into my PC . Now for doing the initialload I requested access to said network drive and BOOM . It also does notwork from my Dev . machine when reading from the shared drive.The issue does not always happen with the same file . I now think it has nothing to do with a specific file . I also tried it on a 64-bit PC with 64-bit python . There it took longer till the error occurred . In fact a file was successfully read which failed on my PC . I now think it is some kind of memory issue ? I believe that it then always fails on the date line because all other lines just return null or an empty string which causes no problem and is entirely possible such a value can be null . But for the date it is a problem and it should not be null and then the error is thrown.EDIT of Update : On my PC it always fails on the same file . Loading that file alone works perfectly fine . I now think it 's some kind of counter/number overflow that after reading n files , the issue occurs . It has to do with the amount of files I load per run of the script and not the file itself . Files that fail work when loading them individually ."
If i have an iterator it and want to exhaust it I can write : Is there a builtin or standard library call which allows me to do it in a one-liner ? Of course i could do : which will build a list from the iterator and then discard it . But i consider that inefficient because of the list-building step . It 's of course trivial to write myself a helper function that does the empty for loop but i am curious if there is something else i am missing .
"I have a google appengine app where I want to set a global variable for that request only . Can I do this ? In request_vars.pyIn another.pyIn main.pyQuestions : Considering multithreading , concurrent requests , building on Google AppEngine , and hundreds ( even thousands ) of requests per second , will the value of time_start always be equal to the value set in global_vars.time_start in main.py per request ? Is this safe to use with multithreading/threadsafe enabled ?"
"In my Django project that uses Celery ( among a number of other things ) , I have a Celery task that will upload a file to a database in the background . I use polling to keep track of the upload progress and display a progress bar for uploading . Here are some snippets that detail the upload process : views.py : tasks.py : databaseinserter.py : The uploading code all works well , and the progress bar also works properly . However , I 'm not sure how to send a Django message that tells the user that the upload is complete ( or , in the event of an error , send a Django message informing the user of the error ) . When the upload begins , I do this in views.py : And I want to do something like this when an upload is successful : I ca n't do that in views.py since the Celery task is fire and forget . Is there a way to do this in celery.py ? In my DBTask class in celery.py I have on_success and on_failure defined , so would I be able to send Django messages from there ? Also , while my polling technically works , it 's not currently ideal . The way the polling works currently is that it will endlessly check for a task regardless of whether one is in progress or not . It quickly floods the server console logs and I can imagine has a negative impact on performance overall . I 'm pretty new to writing polling code so I 'm not entirely sure of best practices and such as well as how to only poll when I need to . What is the best way to deal with the constant polling and the clogging of the server logs ? Below is my code for polling.views.py : And the corresponding jQuery code : ( These last two snippets come largely from previous StackOverflow questions on Django+Celery progress bars . )"
"I am trying to save a Keras model in a H5 file . The Keras model has a custom layer.When I try to restore the model , I get the following error : Could you please tell me how I am supposed to save and load weights of all the custom Keras layers too ? ( Also , there was no warning when saving , will it be possible to load models from H5 files which I have already saved but ca n't load back now ? ) Here is the minimal working code sample ( MCVE ) for this error , as well as the full expanded message : Google Colab NotebookJust for completeness , this is the code I used to make my custom layer.get_config and from_config are both working fine ."
Can a python module have a __repr__ ? The idea would be to do something like : EDIT : precision : I mean a user-defined repr !
"How do I convert a string , e.g . a user ID plus salt , to a random looking but actually a deterministically repeatable uniform probability in the semi-open range [ 0.0 , 1.0 ) ? This means that the output is ≥ 0.0 and < 1.0 . The output distribution must be uniform irrespective of the input distribution . For example , if the input string is 'a3b2Foobar ' , the output probability could repeatably be 0.40341504.Cross-language and cross-platform algorithmic reproducibility is desirable . I 'm inclined to use a hash function unless there is a better way . Here is what I have : I 'm using the latest stable Python 3 . Please note that this question is similar but not exactly identical to the related question to convert an integer to a random but deterministically repeatable choice ."
"I have the below code : I ran the above program in three ways : First Sequential Execution ALONE ( look at the commented code and comment the upper code ) Second Multithreaded Execution ALONEThird Multiprocessing Execution ALONEThe observations for end_time-start time are as follows : Overall Running times ( 'Time Taken = ' , 342.5981313667716 ) -- - Running time by threaded execution ( 'Time Taken = ' , 232.94691744899296 ) -- - Running time by sequential Execution ( 'Time Taken = ' , 307.91093406618216 ) -- - Running time by Multiprocessing executionQuestion : I see sequential execution takes least time and Multithreading takes highest time . Why ? I am unable to understand and also surprised by results.Please clarify.Since this is a CPU intensive task and GIL is acquired , my understanding was Multiprocessing would take least time while threaded execution would take highest time.Please validate my understanding ."
"I use the Pillow ( PIL ) 6.0 and add text in the image . And I want to put the text in the center of the image . Here is my code , Here is the output : Question : The text is in the center horizontally , but not in the center vertically . How can I put it in the center horizontally and vertically ?"
"I got an excellent answer from this post , that covered everything I wanted.However , the question still lingering around is how to remove the spaces in the equal assignments . For example : Must beThat 's it , I want to remove the white spaces between the = . The .properties file is generated with ConfigParser in Python , but it seems those white spaces are creating problems . Of course I can use other things in order to remove those white spaces , but is there a more elegant solution using StringIO , ConfigParser or any other of the Python Libraries ? ** Edit **This question is n't a duplicate because we are trying to find an easier way to remove the white spaces around the = using the API , instead of rewriting the ConfigParser class ."
"I have a simple dataframe with the time as index and dummy values as example . [ ] I did a simple scatter plot as you see here : Simple question : How to adjust the xaxis , so that all time values from 00:00 to 23:00 are visible in the xaxis ? The rest of the plot is fine , it shows all the datapoints , it is just the labeling . Tried different things but did n't work out.All my code so far is :"
"I 'm attempting to retrieve a filtered list from a MySQL database . The query itself looks fine , but the JSON returned shows this : when the spec that I need to build to wants this : Here 's my serializerAny idea how to wrap the returned results in this way ? EDIT : I tried wrapping this in another serializer like soBut this throws the following error : Got AttributeError when attempting to get a value for field people on serializer PeopleListWrapperSerializer . The serializer field might be named incorrectly and not match any attribute or key on the Person instance . Original exception text was : 'Person ' object has no attribute 'people ' ."
"Let 's suppose we have migrations with the following dependency graph ( all applied ) : Initial stateNow , for some reason we want to revert database schema to the state after applying migration 0006_f . We type : and now we have the following state : One branch revertedThe problem is that Django does not revert the right branch , so now we have some migrations applied from left branch and some from the right one.One way to avoid it is to migrate back to 0002_b and forward to 0006_f but this can cause data loss . Also some of migrations 0006_f , 0005_e , 0004_d , 0003_c can be irreversible.Another way is to run the following : Now , to achieve the desired state we only have to revert the migration 0004_d1 and I do not see a way to undo 0004_d1 without undoing 0006_f , 0005_e and 0004_d except for opening DB shell and reverting it manually.Is there a way to explicitly undo only one migration ? Is there another way to properly undo migrations from parallel branch ? Is there some reason for Django not to automatically revert migrations from parallel branch when undoing merge migration ?"
"I want to filter for Django objects such that their `` id modulo K == N '' . Here 's a way to do it in python , but I want it in the filter ( ) : So , I can use extra ( ) with a query like this ( please correct me if I 'm wrong ) : Foo.objects.extra ( where [ 'id % % % s = % s ' % ( K , N ) ] ) But is there a way to use F ( ) ? Django supports the use of addition , subtraction , multiplication , division and modulo arithmetic with F ( ) objectsNote : this is very wrong : Foo.objects.filter ( id=F ( 'id ' ) % K ) I would need something like : Foo.objects.filter ( id__mod ( K ) =N )"
"Is there a library or code snippet available that can take two strings and return the exact or approximate mid-point string between the two strings ? Preferably the code would be in Python.Background : This seems like a simple problem on the surface , but I 'm kind of struggling with it : Clearly , the midpoint string between `` A '' and `` C '' would be `` B '' .With base64 encoding , the midpoint string between `` A '' and `` B '' would probably be `` Ag '' With UTF-8 encoding , I 'm not sure what the valid midpoint would be because the middle character seems to be a control character : U+0088 c2 88 < control > Practical Application : The reason I am asking is because I was hoping write map-reduce type algorithm to read all of the entries out of our database and process them . The primary keys in the database are UTF-8 encoded strings with random distributions of characters . The database we are using is Cassandra.Was hoping to get the lowest key and the highest key out of the database , then break that up into two ranges by finding the midpoint , then breaking those two ranges up into two smaller sections by finding each of their midpoints until I had a few thousand sections , then I could read each section asynchronously.Example if the strings were base-16 encoded : ( Some of the midpoints are approximate ) :"
"I am trying to create a song-artist-album relationship in Django . I have the following models : I have created my ArtistSerializer so that I can retrieve all the songs of the artist when I get the info of any particular artist . This is the serializer I have created : A quick profiling on the GET method of my artist revealed some troubling facts . Following are the results of the profiling ordered by time and number of calls : http : //pastebin.com/bwcKsn2i.But , when I removed the songs_artist field from my serializer , following was the output of the profiler : http : //pastebin.com/0s5k4w7i.If I read right , the database is being hit 1240 times when I use source ! Is there any other alternative to doing this ? Thanks in advance ."
"I have a an array that is relatively sparse , and I would like to go through each row and shuffle only the non-zero elements.Example Input : Example Output : Note how the zeros have not changed position.To shuffle all elements in each row ( including zeros ) I can do this : What I tried to do then is this : But it has no effect . I 've noticed that the return type of X [ i , np.nonzero ( X [ i , : ] ) ] is different from X [ i , : ] which might be thecause ."
"I 'm doing a POST request to a rest service made with django and piston uploading a file , but when I do the request i get this ( strange ? ) error : What does it mean ? And how can I debug it ?"
I have below structureand want to getTried the code without success
I created an enum base class to standardize reverse lookups on simple enums.Is there is an official way to get one 's hands on the _value2member_map_ dict ? ( or is there a standard way to do this that I missed ? ) Thanks !
"I have this problem where I have to `` audit '' a percent of my transtactions.If percent is 100 I have to audit them all , if is 0 I have to skip them all and if 50 % I have to review the half etc . The problem ( or the opportunity ) is that I have to perform the check at runtime . What I tried was : So if percent is 50 So I have to audit 1 and skip 1 audit 1 and skip 1 ..If is 30 audit = 100 / 30 ( 3.3 ) I audit 2 and skip the third.Question I 'm having problems with numbers beyond 50 % ( like 75 % ) because it gives me 1.333 , ... When would be the correct algorithm to know how many to audit as they go ? ... I also have problems with 0 ( due to division by 0 : P ) but I have fixed that already , and with 100 etc . Any suggestion is greatly appreciated ."
"I have a sample doctest like this one.When I execute this code , I got this error.This error is caused by accessing ' f ' which was not accessible when testing hello ( ) method.Is there any way to share the object which is created before ? Without it , one need to create object all the time when it 's necessary ."
"ConsiderColumn `` A '' has mixed data types . I would like to come up with a really quick way of determining this . It would not be as simple as checking whether type == object , because that would identify `` C '' as a false positive.I can think of doing this with But calling type atop applymap is pretty slow . Especially for larger frames.Can we do better ( perhaps with NumPy ) ? I can accept `` No '' if your argument is convincing enough . : - )"
"I 'm writing a Python class in C and I want to put assertions in my debug code . assert.h suits me fine . This only gets put in debug compiles so there 's no chance of an assert failure impacting a user of the Python code*.I 'm trying to divide my 'library ' code ( which should be separate to the code linked against Python ) so I can use it from other C code . My Python methods are therefore thinnish wrappers around my pure-C code . So I ca n't do this in my 'library ' code : because this pollutes my pure-C code with Python . It 's also far uglier than a simpleI believe that the Distutils compiler always sets NDEBUG , which means I ca n't use assert.h even in debug builds.Mac OS and Linux.Help ! *one argument I 've heard against asserting in C code called from Python ."
"I have two Python lists with the same number of elements . The elements of the first list are unique , the ones in the second list - not necessarily so . For instanceI want to remove all the `` first encountered '' elements from the second list and their corresponding elements from the first list . Basically , this means removing all unique elements and the first element of the duplicates . With the above example , the correct result should beThat is , the element 'e1 ' was removed because its corresponding 'h1 ' was encountered for the first time , 'e2 ' was removed because 'h2 ' was seen for the first time , 'e3 ' was left because 'h1 ' was already seen , 'e4 ' was removed because 'h3 ' was seen for the first time , 'e5 ' was left because 'h1 ' was already seen , 'e6 ' was left because 'h2 ' was already seen , and 'e7 ' was removed because 'h4 ' was seen for the first time.What would be an efficient way to solve this problem ? The lists could contain thousands of elements , so I 'd rather not make duplicates of them or run multiple loops , if possible ."
"I would like to be able to test whether two callable objects are the same or not . I would prefer identity semantics ( using the `` is '' operator ) , but I 've discovered that when methods are involved , something different happens . I 've reproduced this with both Python 2.7 and 3.3 ( CPython ) to make sure it 's not an implementation detail of the older version . In other cases , identity testing works as expected ( interpreter session continued from above ) : According to the question How does Python distinguish callback function which is a member of a class ? , a function is wrapped when bound as a method . This makes sense and is consistent with case ( 3 ) above . Is there a reliable way to bind a method to some other name and then later have them compare equal like a callable object or a plain function would ? If the `` == '' does the trick , how does that work ? Why do `` == '' and `` is '' behave differently in case ( 1 ) above ? EditAs @ Claudiu pointed out , the answer to Why do n't methods have reference equality ? is also the answer to this question ."
"I installed Flask-Images to dynamically resize images according to the doco here : https : //mikeboers.github.io/Flask-Images/My code is like this : JinjaAnd in views.pyThe images do not change size even though I can see that the modified url is being returned on dev tools : I know something similar was asked here a while ago Flask-Images does not workbut the only answer there was n't helpful.I 've been at this for 2 days now . If anyone has experience incorporating Flask-Images into their project , I 'd appreciate any suggestions . Would people recommend using Flask-Resize instead ? https : //github.com/jmagnusson/Flask-Resize ."
"outputs : wheras : outputs ( correct fibonacci sequence ) : Are n't they the same ? I mean a , b = b , a+b is essentially the same as a = b and b = a+b written separately -- no ?"
"I have an already existing ARIMA ( p , d , q ) model fit to a time-series data ( for ex , data [ 0:100 ] ) using python . I would like to do forecasts ( forecast [ 100:120 ] ) with this model . However , given that I also have the future true data ( eg : data [ 100:120 ] ) , how do I ensure that the multi-step forecast takes into account the future true data that I have instead of using the data it forecasted ? In essence , when forecasting I would like forecast [ 101 ] to be computed using data [ 100 ] instead of forecast [ 100 ] .I would like to avoid refitting the entire ARIMA model at every time step with the updated `` history '' .I fit the ARIMAX model as follows : Now , the following code allows me to predict values for the entire dataset , including the testHowever in this case after 100 steps , the ARIMAX predicted values quickly converge to the long-run mean ( as expected , since after 100 time steps it is using the forecasted values only ) . I would like to know if there is a way to provide the `` future '' true values to give better online predictions . Something along the lines of : I know I can always keep refitting the ARIMAX model by doingbut this leads to me training the ARIMAX model again and again which does n't make a lot of sense to me . It leads to using a lot of computational resources and is quite impractical . It further makes it difficult to evaluate the ARIMAX model cause the fitted params to keep on changing every iteration.Is there something incorrect about my understanding/use of the ARIMAX model ?"
"I have recently come across the Pyodide project.I have built a little demo using Pyodide , but although I 've spent a lot of time looking at the source , it is not obvious ( yet ) to me how to redirect print output from python ( other than modifying the CPython source ) , and also , how to redirect output from matplotlib.pyplot to the browser.From the source code , FigureCanvasWasm does have a show ( ) method with the appropriate backend for plotting to the browser canvas - however , it is not clear to me how to instantiate this class and invoke it 's show ( ) method or indeed , if there is another more obvious way of redirecting plots to canvas.My questions therefore are : How do I redirect print ( ) messagesHow do I force pyodide to plot matplotlib figures in the browser ? Here is my test page :"
"In Django 1.8 , let us say we have this very simple model : When I save a model instance to my PostgreSQL database , the timestamp field will have microseconds . I noticed that if I attempt to edit a particular model instance from the Django admin , the timestamp field will lose microsecond resolution when saved back into the database.Most application would not need that level of accuracy , but for applications that do require such , it would be nice to be able to eliminate all possible causes of this loss of resolution . Is there any known way/feature to prevent this or is this a bug/limitation ?"
"I have a django application that uses 2 database connections : To connect to the actual data the app is to produceTo a reference master data system , that is maintained completely outside my controlThe issue that I 'm having , is that my webapp can absolutely NOT touch the data in the 2nd database . I solved most of the issues by using 2 ( sub ) apps , one for every database connection . I created a router file that router any migration , and writing to the first appI also made all the models in the 2nd app non managed , using the option.To be sure , the user I connect to the 2nd database has read only accessThis works fine for migrations and running . However , when I try to run tests using django testcase , Django tries to delete and create a test_ database on the 2nd database connection.How can I make sure that Django will NEVER update/delete/insert/drop/truncate over the 2nd connectionHow can I run tests , that do not try to create the second database , but do create the first.Thanks ! edited : codemodel ( for the 2nd app , that should not be managed ) : router : settings : one testcase : output from when running this tests : ( before this point , django creates the test database for my first connection = OK ) = > This next lines should never happen . Fails because I use a read only user ( luckily )"
"I want to have a piece of code run after each line of another block of code . For example , want to be able to evaluate a global variable before or after executing the next line of a function.For example , below I try to print 'hello ' before each line of the foo ( ) function . I think that a decorator can help me but it would need some introspection feature in order to edit each line of my foo ( ) function and add what I want before or after it.I am trying to perform something like this : How can I perform this ? would the __code__ object help ? do I need a decorator & introspection at the same time ? EDIT : Here is another example of the goal of this thread : In this new case , before printing each `` bar '' , I want to print a `` hello '' .The main goal of this is to be able to execute another function or test any kind of global variable before executing the next line of the code . Imagine if a global variable is True , then the code goes to the next line ; while if the global variable is False , then it stops the function execution.EDIT : In a way , i am looking for a tool to inject code inside another block of code.EDIT : Thank 's to unutbu i have achieved this code :"
"A web app I am working on works fine under Firefox and IE8 from virtual box . But when I try to load it with IE9 , it tries to load the page but after a while stops loading . Then I try to load the same url with Firefox again and it does n't load at all . Then I restart Flask and the same happens - I can work normally with the app through FF but not IE9.Looks like a kind of a bug , does n't it ? Here is the exception Python throws : I have uploaded code to my hosting and it works fine there.I think the issue from this thread is similar to my one ."
"As I am going through tutorials on Python 3 , I came across the following : My understanding is that `` equals no blank spaces.When I try the following the shell terminal , I get the output shown below it : Can someone please help explain what is happening ?"
"From everything I can find online , there is absolutely no difference between New York and Detroit timezones , including DST . And this was just from checking the first two . They 've got ta have a reason for having both , right ? edit : expanding on my research , all the timezones listed which are UTC -5 have the same exact DST info , so it seems even more redundant now . I could remove all the below with one for Eastern Standard Time ... Is there anything wrong with doing that ?"
"Let 's generate an array : Then find the eigenvalues of the covariance matrix : Now let 's run the matplotlib.mlab.PCA function on the data : Why are the two matrices different ? I thought that in finding the PCA , first one had to find the eigenvectors of the covariance matrix , and that this would be exactly equal to the weights ."
"I have an image with values ranging from 0 to 1 . What I like to do is simple averaging.But , more specifically , for a cell at the border of the image I 'd like to compute the average of the pixels for that part of the neighbourhood/kernel that lies within the extent of the image . In fact this boils down to adapt the denominator of the 'mean formula ' , the number of pixels you divide the sum by . I managed to do this as shown below with scipy.ndimage.generic_filter , but this is far from time-efficient . Detailskernel = square array ( circle represented by ones ) Padding with 2 's and not by zeroes since then I could not properly separate zeroes of the padded area and zeroes of the actual rastercountkernel = number of ones in the kerneln = number of cells that lie within image by excluding the cells of the padded area identified by values of 2Correct the sum by subtracting ( number of padded cells * 2.0 ) from the original neighbourhood total sumUpdate ( s ) 1 ) Padding with NaNs increases the calculation with about 30 % :2 ) Applying the solution proposed by Yves Daoust ( accepted answer ) , definitely reduces the processing time to a minimum:3 ) Building on Yves ' tip to use an additional binary image , which in fact is applying a mask , I stumbled upon the principle of masked arrays . As such only one array has to be processed because a masked array 'blends ' the image and mask arrays together.A small detail about the mask array : instead of filling the inner part ( extent of original image ) with 1 's and filling the outer part ( border ) with 0 's as done in the previous update , you must do vice versa . A 1 in a masked array means 'invalid ' , a 0 means 'valid'.This code is even 50 % faster then the code supplied in update 2 ) : -- > I must correct myself here ! I must be mistaken during the validation , since after some calculation runs it seemed that scipy.ndimage. < filters > can not handle masked_arrays in that sense that during the filter operation the mask is not taken into account.Some other people mentioned this too , like here and here . The power of an image ... grey : extent of image to be processedwhite : padded area ( in my case filled with 2.0 's ) red shades : extent of kerneldark red : effective neighbourhoudlight red : part of neighbourhood to be ignoredHow can this rather pragmatical piece of code be changed to improve performance of the calculation ? Many thanks in advance !"
"I want to generate in Python all possible RPN ( Reverse Polish notation ) expressions , that use letters from an input list ( such as [ ' a ' , ' b ' , ' c ' ] ) and contain operators [ '+ ' , '- ' , '* ' , '/ ' ] .My idea was that we can add elements to the current expression until one of the following happens : either we 've used all letters or expression is complete ( i.e . we ca n't add more operators ) .So I wrote the following functions:1 ) 2 ) 3 ) Next I wrap it into recursion : There are two difficulties with that function : The first one is that if there are repeated letters in list ofletters , it wo n't return all possible results . For example if letters = [ ' a ' , ' b ' , ' a ' ] : So the output missing 'ab+a+ ' and so on . But I do want to return all possible combinations in that case too.The second problem is that there are many `` equivalent '' strings in the output . Since we have the commutative and associativeproperties in prefix form , expressions like ab+c+/abc++/ca+b+should be considered as equivalent : I want only one of each group in theoutput of the function rec ( ) .How could we change the functions above to overcome such difficulties ? What is the most elegant solution to problem ?"
"I 'm new to programming and in my project , I 'm trying to print basic dice graphics.I 'm trying to make a function that accepts two numbers from 1 to 6 , and prints corresponding two dice faces next to each other . I 've tried several approaches , but this is the only one that worked , and it 's quite chunky : Is there a more compact and elegant way to do this ?"
"I 'm new in Keras and Neural Networks . I 'm writing a thesis and trying to create a SimpleRNN in Keras as it is illustrated below : As it is shown in the picture , I need to create a model with 4 inputs + 2 outputs and with any number of neurons in the hidden layer.This is my code:1 ) Does my model implement the graph ? 2 ) Is it possible to specify connections between neurons Input and Hidden layers or Output and Input layers ? Explanation : I am going to use backpropagation to train my network.I have input and target values Input is a 10*4 array and target is a 10*2 array which I then reshape : It is crucial for to able to specify connections between neurons as they can be different . For instance , here you can have an example :"
"This is how I would create a Dictionary in C # .In Python if you have a dictionary like so : As you can see , Counter is very useful when comparing dictionary objects.Is there a library in C # , that will allow me to do something similar to this , or do I have to code it from scratch ?"
"I 'm struggling with properly loading a csv that has a multi lines header with blanks . The CSV looks like this : What I would like to get is : When I try to load with pd.read_csv ( file , header= [ 0,1 ] , sep= ' , ' ) , I end up with the following : Is there a way to get the desired result ? Note : alternatively , I would accept this as a result : Versions used : Python : 2.7.8Pandas 0.16.0"
"I have markup language which is similar to markdown and the one used by SO.Legacy parser was based on regexes and was complete nightmare to maintain , so I 've come up with my own solution based on EBNF grammar and implemented via mxTextTools/SimpleParse.However , there are issues with some tokens which may include each other , and I do n't see a 'right ' way to do it.Here is part of my grammar : First problem is , spoiler , strong and emphasis can include each other in arbitrary order . And its possible that later I 'll need more such inline markups.My current solution involves just creating separate token for each combination ( inline_noast , inline_nostrong , etc ) , but obviously , number of such combinations grows too fast with growing number of markup elements.Second problem is that these lookaheads in strong/emphasis behave VERY poorly on some cases of bad markup like __._.__*__.__ ... ___._.____.__**___*** ( lots of randomly placed markup symbols ) . It takes minutes to parse few kb of such random text.Is it something wrong with my grammar or I should use some other kind of parser for this task ?"
"With the file super5.py : we can do the following : To me , this does n't make sense , because when I execute x.m ( ) , I expect the following to happen : The first line of m of D is executed and thus `` m of D called '' is output.The second line , super ( ) .m ( ) is executed , which first takes us to m of B.In m of B , `` m of B called '' is first output , and then , m of A is executed due to the super.m ( ) call in m of B , and `` m of A called '' is output.m of C is executed in a fashion analogous to 3.As you can see , what I expect to see is : Why am I wrong ? Is python somehow keeping track of the number of super ( ) calls to a particular superclass and limiting the execution to 1 ?"
Have a look at this code : I would expect it to print 'Value is : True ' but it does n't . Why is this and what is the solution ?
"I have a simple http client in python that send the http post request like this : On my server side with Flask , I can define a simple functionAnd the data on server will be the same dictionary as the data on the client side.However , now I am moving to Klein , so the server code looks like this : and the request that 's been used in Klein does not support the same function . I wonder is there a way to get the json in Klein in the same way I got it in Flask ? Thank you for reading this question ."
What is the fastest way to iterate through all one dimensional sub-arrays of an n dimensional array in python.For example consider the 3-D array : The desired sequence of yields from the iterator is :
"I have what I considered to be a simple question . Within my model I have a models.ImageField which looks like that : But I would like to upload it to '.media/ ' + self.pk+ '.png ' I tried to update the field within the save method of the model , but this does not work , as the pk is not known when 'save ' is called . I also tried to add a custom function for upload_to as suggested here : Django : Any way to change `` upload_to '' property of FileField without resorting to magic ? . But this just leaves the field empty . What can I do ? EDIT : I use Django 1.6EDIT : I used a post_save signal which is not very nice :"
"I have a dataframe containing open/close , candle color and number of consecutive candles . I 'm trying to calculate the absolute value of the difference between the open of the first candle in the run and the close of the last candle in the run and apply the difference to each line . The result would look likeI have read two other posts that come close but do n't quite get to the solution I 'm looking for : get first and last values in a groupbyPandas number of consecutive occurrences in previous rowsI 'm using df.groupby ( ( df [ 'color ' ] ! = df [ 'color ' ] .shift ( ) ) .cumsum ( ) ) to group the rows by the color of the candle ( this is how I calculated the color and the run count ) and I can get the first and last values of the group using .agg ( [ 'first ' , 'last ' ] ) .stack ( ) but this does n't allow me to apply the difference per line of the original dataframe ."
"I implemented my own User class from scratch in Django . But when I log in I have this error : The following fields do not exist in this model or are m2m fields : last_loginI really do n't want the field last_login.I do some reasearch and the problem is here : contrib.aut.models.pyI found a workaround but it 's not an ellegant solution . I added user_logged_in.disconnect ( update_last_login ) in my models.py file , where my User class is defined.Is there any better solution for this ?"
"Stupid question time . I seem to be able to create an object for a Django model even though I omit a column that was defined as NOT NULL and I do n't understand why . Here 's my model : When I run python manage.py sql I see : Also , if I run the command \d movies from the psql , I can see that all columns are designated NOT NULL.But here 's what I do n't understand . When I run the Python shell , I can issue the following command and a new row with an empty 'name ' column will be created : However , if I issue ( what I believe to be ) the equivalent SQL command : ... I get the error I would expect : `` ERROR : null value ... violates not-null constraint . `` Why does Django 's ORM allow me to create an object that lacks a parameter for a NOT NULL CharField column ? Does it assume that I 'm using model validators ? If so , it seems to be a pretty dumb and trusting ORM.I 'm using Python 2.7.1 , Django 1.4 , and PostgreSQL 9.1.4.Thanks for your help ."
"This is an example from Python 3.8.0 interpreter ( however , it is similar in 3.7.5 ) getsizeof ( ) returns how much bytes Python object consumes together with the garbage collector overhead ( see here ) . What is the reason that basic python classes consume the same amount of memory ? If we take a look at instances of these classesThe default argument is 0 and these two instances have the same amount of memory usage for this argument . However , if I try to add an argumentand this is where it gets strange . Why does the instance memory usage increase for int but not for float type ?"
"I have a project that has the following structure : The importable package proj lives under the src directory . Tests are installed along with it , in the proj.tests sub-package . The code , the tests themselves , and the installation all work fine . However , I have trouble passing options to the tests.I have an option , -- plots , that is defined in conftest.py ( under Project/src/proj/tests/ ) , which creates a .plots/ folder in the root folder and places some graphical debugging there : There are two different ways I 'd like to have to run this test : The first is from the command line , in the Project directory : This fails immediately withIf I add the package directory , the run goes fine : It also goes well if I specify the tests sub-directory : The second way is to run the test function of the package itself . The test function is defined in Project/src/proj/__init__.py like this : The function is intended to be run after Project is installed like this : Generally , this works OK , but if I doI get the same error as usual : For this test , I ran python setup.py develop and then cd'dto/some/other/folder ` to make sure everything installed correctly.My goal is to have both options working correctly when I pass in the -- plots command line option . It seems that I have found a workaround for option # 1 , which is to manually pass in one of the packages to pytest , but I do n't even fully understand how that works ( why can I pass in either src/proj or src/proj/tests ? ) .So the question is , how to I get pytest to consistently run my test package so that the correct conftest.py gets picked up ? I am willing to consider just about any reasonable alternative that allows me to get the -- plots option working consistently in both the source ( running in a shell from Project ) version and the proj.test ( ) version.For reference , here is my pytest.ini file : It does n't appear to make any difference.I am running with pytest 3.8.0 , on Pythons 2.7 , 3.3 , 3.4 , 3.5 , 3.6 and 3.7 ( in anaconda ) . In all versions , the results are reproducible ."
"According to the selenium documentation , interactions between the webdriver client and a browser is done via JSON Wire Protocol . Basically the client , written in python , ruby , java whatever , sends JSON messages to the web browser and the web browser responds with JSON too.Is there a way to view/catch/log these JSON messages while running a selenium test ? For example ( in Python ) : I want to see what JSON messages are going between the python selenium webdriver client and a browser when I instantiate the driver ( in this case Chrome ) : webdriver.Chrome ( ) , when I 'm getting a page : driver.get ( 'http : //google.com ' ) and when I 'm closing it : driver.close ( ) .FYI , in the # SFSE : Stripping Down Remote WebDriver tutorial , it is done via capturing the network traffic between the local machine where the script is running and the remote selenium server . I 'm tagging the question as Python specific , but really would be happy with any pointers ."
"I am new to geodjango . I am using django-1.4.5 and my database settings , And i got the error 'module ' object has no attribute 'GeoSQLCompiler'Please solve my problem ."
"I have been bitten by something unexpected recently . I wanted to make something like that : Well , It was working fine until I passed an object inheriting from Exception which was supposed to be added.Unfortunetly , an Exception is iterable . The following code does not raise any TypeError : Does anybody know why ?"
"I want to convert a > 1mn record MySQL database into a graph database , because it is heavily linked network-type data . The free version of Neo4J had some restrictions I thought I might bump up against , so I 've installed OrientDB ( Community 2.2.0 ) ( on Ubuntu Server 16.04 ) and got it working . Now I need to access it from Python ( 3.5.1+ ) , so I 'm trying pyorient ( 1.5.2 ) . ( I tried TinkerPop since I eventually want to use Gremlin , and could n't get the gremlin console to talk to the OrientDB . ) The following simple Python code , to connect to one of the test graphs in OrientDB : gives a weird error : Does anyone know what that is or how I can fix it ? Should I really be using TinkerPop instead ? If so I 'll post a seperate question about my struggles with that ."
"While parsing attributes using __dict__ , my @ staticmethod is not callable.How is this possible ? How to check if a static method is callable ? I provide below a more detailed example : Script test.pyResult for python2Same result for python3"
I have a dataframe df with float values in column A. I want to add another column B such that : B [ 0 ] = A [ 0 ] for i > 0 ... B [ i ] = if ( np.isnan ( A [ i ] ) ) then A [ i ] else Step3B [ i ] = if ( abs ( ( B [ i-1 ] - A [ i ] ) / B [ i-1 ] ) < 0.3 ) then B [ i-1 ] else A [ i ] Sample dataframe df can be generated as given below
Python 's poetry dependency manager allows specifying optional dependencies via command : Which results in this configuration : However how do you actually install them ? Docs seem to hint to : but that just throws and error :
"I have an accounting tree that 's stored with indents/spaces in the source : There are a fixed number of levels , so I 'd like to flatten the hierarchy by using 3 fields ( actual data has 6 levels , simplified for example ) : I can do this by checking the number of spaces prior to the account name : Is there a more flexible way to achieve this based on the indentation of the current row compared to the previous row rather than assuming it 's always 3 spaces per level ? L1 will always have no indent , and we can trust that lower levels will be indented further than their parent , but maybe not always 3 spaces per level.Update , ended up with this as the meat of the logic , since I ultimately wanted the account list with the content , it seemed easiest to just use the indent to decide whether to reset , append , or pop the list : So at each row of output the accountList is complete ."
"I 'm taking my first foray into the Pyramid security module . I 'm using this login code to set the auth_tkt : It seems to work ok , but there are some puzzling details : First of all , 2 cookies actually get set instead of one . The 2 cookies are identical ( both with name `` auth_tkt '' ) except for one difference : one has a host value of `` .www.mydomain.com '' while the other cookie has a host value of `` www.mydomain.com '' Why are 2 cookies being set instead of one ? What 's the significance of the difference host values ? Question 2 , web tools reports that neither cookie is secure . What can I do to make sure the cookie/s are secure ? Question 3 : Both cookies have an expiration value of `` At end of session '' . What does this mean and how can I customize the expiration value myself ? What 's the recommended practice for login cookie expiration times ? Question 4 : I do n't understand why the first argument of `` remember '' is self.request instead of self.request.response . Should n't the data be remembered on the response object , not the request object ?"
"A source of constant headache when tracking down bugs in my Python code are seemingly innocuous snippets like this : This fails because I 've overwritten the function list ( ) with the variable list.A contrived example obviously , but the point is Python happily lets me overwrite built-in functions with variables . I realise this is a crucial feature in Python but I would quite like it if the interpreter would warn me when I do it in my code as I usually do n't mean to do this.Can anyone suggest a solution ( other than just being more careful ) - as I keep tripping over this problem ?"
"I am trying to display a table in a jupyter notebook using python and the visualization library Bokeh . I use the following code to display my table in an jupyter notebook where result is a dataframe : Previously I was using vform although this now seems to be depreciated and no longer works as expected either . This occurred after my jupyter notebook version was updated . Regardless of what I set the width my column headers do n't line up and have a weird overlap with the table : This did not happen before , I was able to get a nice table where everything lined up . Even if I adjust the headers they still wont line up . This does not happen when I save the table as an html file instead of calling show ( ) directly in the Jupyter notebook . What do I need to change ? Is there a better way to do this ? Full ExampleThis is running on a system with the following versions : Jupyter 4.2.0 Python 2.7.12 ( Anaconda 2.3.0 64 bit ) Bokeh 0.12.2"
"I have a dictionary likeHow can I remove elements from this dictionary without creating gaps in values , in case the dictionary is not ordered ? An example : I have a big matrix , where rows represent words , and columns represent documents where these words are encountered . I store the words and their corresponding indices as a dictionary . E.g . for this matrixthe dictionary would look like : If I remove the words 'apple ' and 'banana ' , the matrix would contain only two rows . So the value of 'orange ' in the dictionary should now equal 0 and not 1 , and the value of 'pear ' should be 1 instead of 3 . In Python 3.6+ dictionaries are ordered , so I can just write something like this to reassign the values : or , alternativelyI think , this is far from being the most efficient way to change the values , and it would n't work with unordered dictionaries . How to do it efficiently ? Is there any way to easily reassign the values in case the dictionary is not ordered ?"
"I am implementing a cython-based interface to a C++ library . I implemented a callback system that works with normal functions but fails strangely when passing in bound methods . Here is a portion of my cython code : And here is how I 'm trying to call it : When this code is run initially , this is printed : < bound method RenderCanvas.OnDrillButtonPress of < UI.RenderCanvas ; proxy of < Swig Object of type 'wxGLCanvas * ' at 0x42b70a8 > > > That seems right . The problem is when the callback is triggered , this is printed : < bound method Shell.readline of < wx.py.shell.Shell ; proxy of < Swig Object of type 'wxStyledTextCtrl * ' at 0x3a12348 > > > A totally different bound method is being called . However , when I make OnDrillButtonPress a static method , < function OnDrillButtonPress at 0x042FC570 > is printed both on initialization and triggering calls.Is there an incompatibility with saving bound methods as void* ?"
"What should an app developer place in setup.py and requirements.txt ? Should an app even have a setup.py ? The docs seem to only cover the scenario of writing a module . But in an app , how should package and module dependency be encoded ? Using both setup.py and requirements.txt ? I 've been handed a 2.7.8 project with some questionable coding , and I 'm not sure if this setup.py is reasonable . I 'm surprised by the amount of boilerplate coding : I 'm new to Python , but I see some potential problems : An irrelevant version number which needs to be manually updated.The 'test ' imports are n't relevant to the project ( `` multiprocessing '' ) . We do n't run tests by executing setup.py ; we execute the nosetests command.install_requires depends on the contents of requirements.txt . But afaik it should be the other way around : requirements.txt needs to be generated based on the install_requires section.It seems to check for a python version which the app is n't coded to support ."
"In a classic relational database , I have the following table : I am trying to convert this table into a Google App Engine table . My issue is with the fields MotherId and FatherId . I tried the code below , but no chance . Python says that it does n't know the object type Person.Does someone know how we can model a recursive relationship in a Google App Engine table ? How could I work around the limitation of App Engine ? UPDATEI want to expand the problem a little bit ... What if I wanted to add a collection of children ? I tried this and it does n't work . Any idea what I am doing wrong ? Thanks !"
"Suppose we have a list of numbers , l. I need to COUNT all tuples of length 3 from l , ( l_i , l_j , l_k ) such that l_i evenly divides l_j , and l_j evenly divides l_k . With the stipulation that the indices i , j , k have the relationship i < j < k I.e . ; If l= [ 1,2,3,4,5,6 ] , then the tuples would be [ 1,2,6 ] , [ 1,3,6 ] , [ 1,2,4 ] , so the COUNT would be 3.If l= [ 1,1,1 ] , then the only tuple would be [ 1,1,1 ] , so the COUNT would be 1.Here 's what I 've done so far , using list comprehensions : This works , but as l gets longer ( and it can be as large as 2000 elements long ) , the time it takes increases too much . Is there a faster/better way to do this ?"
"Say I have several histograms , each with counts at different bin locations ( on a real axis ) . e.g.How can I normalize these histograms so that I get PDFs where the integral of each PDF adds up to one within a given range ( e.g . 0 and 100 ) ? We can assume that the histogram counts events on pre-defined bin size ( e.g . 10 ) Most implementations I have seen are based , for example , on Gaussian Kernels ( see scipy and scikit-learn ) that start from the data . In my case , I need to do this from the histograms , since I do n't have access to the original data.Update : Note that all current answers assume that we are looking at a random variable that lives in ( -Inf , +Inf ) . That 's fine as a rough approximation , but this may not be the case depending on the application , where the variable may be defined within some other range [ a , b ] ( e.g . 0 and 100 in the above case )"
For some reason memcache does not seem to likein this following codeThis is the error in apache error_log : Something is probably going on with the `` result '' .otherwise instead of `` result '' .. something else such as..works just fine .
"I am trying to deploy my flask app . Usually I would have an app.py and put all code in it.for my really small projects . But then I have a slightly larger app and follow the larger app guide by flask.So I have this : I now have all my routes and views in views.py and running the app in __init__.py : ( This is just an example ) So now I follow the guide by running it with pip install -e . and running with : > set FLASK_APP=app ( name I set in setup.py ) flask run and it works . Except I do not know how to run it with one command . Since there is no one file to run I can not use gunicorn or anything like that . I am not sure how to go about executing this app . How would I run pip install . on the cloud server heroku ? My problem is because I have to import the app from __init__.py and views using import blog . [ insert import ] ( models , views etc . ) Any help is appreciated . Thank you.EDIT : I do not want to use blueprints though . That might be too much . My app is medium , not small but not large either"
I am trying to understand maybe Monad but most of the examples I saw used some language-specific feature . To ensure that I have gotten it conceptually right I thought of writing a generic implementation . Below is what I came up with . Can someone tell if I have gotten it conceptually right ? Is there a better way to generalize it ? What if I wanted to add the values from these functions and abort if any of those was NULL ; any suggestion ?
"In the following code , the None is used to declare the size of the placeholders . As I know , this None is used to specify a variable batch dimension . But , in each code , we have a variable that shows the batch size , such as : So , is there any reason to use None in such cases instead of simply declaring the placeholders as ?"
"Building a string through repeated string concatenation is an anti-pattern , but I 'm still curious why its performance switches from linear to quadratic after string length exceeds approximately 10 ** 6 : For example , on my machine ( Windows 10 , python 3.6.1 ) : for 10 ** 4 < n < 10 ** 6 , the time_per_iteration is almost perfectly constant at 170±10 µsfor 10 ** 6 < n , the time_per_iteration is almost perfectly linear , reaching 520 µs at n == 10 ** 7 . Linear growth in time_per_iteration is equivalent to quadratic growth in total_time.The linear complexity results from the optimization in the more recent CPython versions ( 2.4+ ) that reuse the original storage if no references remain to the original object . But I expected the linear performance to continue indefinitely rather than switch to quadratic at some point.My question is based made on this comment . For some odd reason running takes incredibly long time ( much longer than quadratic ) , so I never got the actual timing results from timeit . So instead , I used a simple loop as above to obtain performance numbers.Update : My question might as well have been titled `` How can a list-like append have O ( 1 ) performance without over-allocation ? '' . From observing constant time_per_iteration on small-size strings , I assumed the string optimization must be over-allocating . But realloc is ( unexpectedly to me ) quite successful at avoiding memory copy when extending small memory blocks ."
"I see in the code on this Sage wiki page the following code : Is this ( 1..n ) syntax unique to Sage or is it something in Python ? Also , what does it do ?"
"I created some Cython files , and import them in a Python file using , When I run the py file , the C compiler ( VC++14 ) generated the following errorsfor each Cythonmodule.How to fix this and does it affect the performance or can be erroneous in the execution ."
"Say I have the following dataframe ( a column of integers and a column with a list of integers ) ... And also a separate list of IDs ... Given that , and ignoring the df [ 'ID ' ] column and any index , I want to see if any of the IDs in the bad_ids list are mentioned in the df [ 'Found_IDs ' ] column . The code I have so far is : This works but only if the bad_ids list is longer than the dataframe and for the real dataset the bad_ids list is going to be a lot shorter than the dataframe . If I set the bad_ids list to only two elements ... I get a very popular error ( I have read many questions with the same error ) ... I have tried converting the list to a series ( no change in the error ) . I have also tried adding the new column and setting all values to False before doing the comprehension line ( again no change in the error ) .Two questions : How do I get my code ( below ) to work for a list that is shorter thana dataframe ? How would I get the code to write the actual ID foundback to the df [ 'bad_id ' ] column ( more useful than True/False ) ? Expected output for bad_ids = [ 15533 , 876544 ] : Ideal output for bad_ids = [ 15533 , 876544 ] ( ID ( s ) are written to a new column or columns ) : Code :"
"I have a 2 dimensional numpy array : I want the ( 1 , 1 ) , ( 1,3 ) , ( 3,1 ) , and ( 3,3 ) cells to take a value of 1.Only assigns 1 to ( 1,1 ) and ( 3,3 ) . Does not work because it make a copy of the data not a view . What 's the right way to do this ?"
"I am making a post request with some javascript to a python script in my /var/www/cgi-bin on my web server , and then in this python script I want to save the image file to my html folder , so it can later be retrieved.Located at /var/www/html , but right now the only way I know how to do this is to set the python script to chmod 777 which I do not want to do.So how else can I save a file that I grab from my webpage using javascript and then send to server with javascript via POST ? Currently when I do this I get an error saying the python does not have permission to save , as its chmod is 755.I here is python code , I know it works as the error just says I dont have permission to write the file"
"I am working on writing unit testcases for web based application which is written in django . For unittesting of django classes I am using TestCase class available from django.test . Now When I test get_queryset ( ) method by client.get ( ) method it returns error : raise DatabaseError ( 'This query is not supported by the database . ' ) DatabaseError : This query is not supported by the database.here is my method which I am trying to test : and here is testcase I am writing for the same : There is no error at url reversing , that I can assure you . I have checked it from python shell . here is url pattern : Error is showing at it would be great if someone help me with this , i will be very much thankful.This is full traceback of error : $ python manage.py test lib.tests : IngredientAllTestCase $ python manage.py test lib.tests : IngredientAllTestCaseEERROR : test_IngredientAll ( lib.tests.IngredientAllTestCase ) Traceback ( most recent call last ) : File `` C : \Apache2\htdocs\iLab\api\lib\tests.py '' , line 94 , in test_IngredientAll response = self.client.get ( url ) File `` C : \Python27\lib\site-packages\django\test\client.py '' , line 442 , in get response = super ( Client , self ) .get ( path , data=data , **extra ) File `` C : \Python27\lib\site-packages\django\test\client.py '' , line 244 , in get return self.request ( **r ) File `` C : \Python27\lib\site-packages\django\core\handlers\base.py '' , line 111 , in get_responseFile `` C : \Python27\lib\site-packages\djangorestframework-2.3.8-py2.7.egg\rest_framework\compat.py '' , line 127 , in view return self.dispatch ( request , *args , **kwargs ) File `` C : \Python27\lib\site-packages\django\views\decorators\csrf.py '' , line 39 , in wrapped_view resp = view_func ( *args , **kwargs ) File `` C : \Python27\lib\site-packages\django\views\decorators\csrf.py '' , line 52 , in wrapped_view return view_func ( *args , **kwargs ) File `` C : \Python27\lib\site-packages\djangorestframework-2.3.8-py2.7.egg\rest_framework\views.py '' , line 399 , in dispatch response = self.handle_exception ( exc ) File `` C : \Python27\lib\site-packages\djangorestframework-2.3.8-py2.7.egg\rest_framework\views.py '' , line 396 , in dispatch response = handler ( request , *args , **kwargs ) File `` C : \Apache2\htdocs\iLab\api\lib\views.py '' , line 431 , in get return Response ( serializer.data ) File `` C : \Python27\lib\site-packages\djangorestframework-2.3.8-py2.7.egg\rest_framework\serializers.py '' , line 505 , in data self._data = [ self.to_native ( item ) for item in obj ] File `` C : \Python27\lib\site-packages\django\db\models\query.py '' , line 107 , in _result_iter self._fill_cache ( ) File `` C : \Python27\lib\site-packages\django\db\models\query.py '' , line 774 , in _fill_cache self._result_cache.append ( self._iter.next ( ) ) File `` C : \Python27\lib\site-packages\django\db\models\query.py '' , line 275 , in iterator for row in compiler.results_iter ( ) : File `` build\bdist.win-amd64\egg\djangotoolbox\db\basecompiler.py '' , line 225 , in results_iter self.check_query ( ) File `` build\bdist.win-amd64\egg\djangotoolbox\db\basecompiler.py '' , line 273 , in check_query raise DatabaseError ( 'This query is not supported by the database . ' ) DatabaseError : This query is not supported by the database. -- -- -- -- -- -- -- -- -- -- > > begin captured logging < < -- -- -- -- -- -- -- -- -- -- django.request : ERROR : Internal Server Error : /allingredientsTraceback ( most recent call last ) : File `` C : \Python27\lib\site-packages\django\core\handlers\base.py '' , line 111 , in get_response response = callback ( request , *callback_args , **callback_kwargs ) File `` C : \Python27\lib\site-packages\djangorestframework-2.3.8-py2.7.egg\rest_framework\compat.py '' , line 127 , in view return self.dispatch ( request , *args , **kwargs ) File `` C : \Python27\lib\site-packages\django\views\decorators\csrf.py '' , line 39 , in wrapped_view resp = view_func ( *args , **kwargs ) File `` C : \Python27\lib\site-packages\django\views\decorators\csrf.py '' , line 52 , in wrapped_view return view_func ( *args , **kwargs ) File `` C : \Python27\lib\site-packages\djangorestframework-2.3.8-py2.7.egg\rest_framework\views.py '' , line 399 , in dispatch response = self.handle_exception ( exc ) File `` C : \Python27\lib\site-packages\djangorestframework-2.3.8-py2.7.egg\rest_framework\views.py '' , line 396 , in dispatch response = handler ( request , *args , **kwargs ) File `` C : \Apache2\htdocs\iLab\api\lib\views.py '' , line 431 , in get return Response ( serializer.data ) File `` C : \Python27\lib\site-packages\djangorestframework-2.3.8-py2.7.egg\rest_framework\serializers.py '' , line 505 , in data self._data = [ self.to_native ( item ) for item in obj ] File `` C : \Python27\lib\site-packages\django\db\models\query.py '' , line 107 , in _result_iter self._fill_cache ( ) File `` C : \Python27\lib\site-packanosetests lib.tests : IngredientAllTestCase -- verbosity=1Destroying test database for alias 'default ' ... ges\django\db\models\query.py '' , line 774 , in _fill_cache self._result_cache.append ( self._iter.next ( ) ) File `` C : \Python27\lib\site-packages\django\db\models\query.py '' , line 275 , in iterator for row in compiler.results_iter ( ) : File `` build\bdist.win-amd64\egg\djangotoolbox\db\basecompiler.py '' , line 225 , in results_iter self.check_query ( ) File `` build\bdist.win-amd64\egg\djangotoolbox\db\basecompiler.py '' , line 273 , in check_queryDatabaseError : This query is not supported by the database. -- -- -- -- -- -- -- -- -- -- - > > end captured logging < < -- -- -- -- -- -- -- -- -- -- -Ran 1 test in 0.900s"
"I 'm trying to write code that will produce disparity maps using numpy and scipy , but the values that I store in my numpy array for my images are completely different from the values that are actually showing up in my output images , saved with misc.imsave . For example , in the array , none of the values are greater than 22 , but in the image , I have a full range of values from 0 to 255 . I thought that perhaps imsave was stretching the values so that the max value showed up as 255 in the image , but I have other images created with imsave that have a max below 255.These are the functions I 'm using to create my disparity maps , given two pgm images that have been shifted along the x axis : Ignore the SAD/SADstore return values , I have ensured that these are not effecting my current process . This is the code I 'm using to get my output : As it current is , nothing in disp12im should be > 23 . If I run a for loop to check this on the array , this remains true . However , if I load the saved image and run that same for loop on the values , I get tons of numbers over 23 . What am I doing wrong ?"
"I 'm trying to learn pdb using this tutorial referenced from another stackoverflow question , and I 'm on the 3rd step of the Getting Started section.It looks like n ( ext ) should move me to the next line in the current frame : And seems to do that successfully in the tutorial 's example . But in my ipython it appears to be moving me to the next line within the pdb.set_trace ( ) code.How do I simply navigate to the ' b = `` bbb '' ' line ?"
"I was just checking out some docs on ABCs for a project of mine , where I need to do some type-related work . Those are the official docs about the ValuesView type , in both Python 2 and 3 : https : //docs.python.org/2/library/collections.html # collections.ValuesViewhttps : //docs.python.org/3/library/collections.abc.htmland this is the source ( Python 2 , but same happens in Python 3 ) https : //hg.python.org/releases/2.7.11/file/9213c70c67d2/Lib/_abcoll.py # l479I was very puzzled about the ValuesView interface , because from alogical standpoint it should inherit from Iterable , IMHO ( it 's evengot the __iter__ Mixin method ) ; on the contrary the docs say that it justinherits from MappingView , which inherits from Sized , which doesn'tinherit from Iterable.So I fired up my 2.7 interpreter : It looks iterable , after all , because of Iterable 's own subclasshook.But I do n't understand why ValuesView is n't explicitly Iterable . Other ABCs , like Sequence or Set , are explicitly Iterable . Is there some arcane reason behind that , or it 's just a documentation+implementation shortcoming for a little-used feature ? EDIT : ValuesView works as expected from Python 3.7 onwards . Seems no more than an oversight ."
"After fitting a local level model using UnobservedComponents from statsmodels , we are trying to find ways to simulate new time series with the results . Something like : Is it possible to use trained_model to simulate new time series given the exogenous variable X [ 70 : ] ? Just as we have the arma_process.generate_sample ( nsample=100 ) , we were wondering if we could do something like : The motivation behind it is so that we can compute the probability of having a time series as extreme as the observed y [ 70 : ] ( p-value for identifying the response is bigger than the predicted one ) . [ EDIT ] After reading Josef 's and cfulton 's comments , I tried implementing the following : But this resulted in simulations that does n't seem to track the predicted_mean of the forecast for X_post as exog . Here 's an example : While the y_post meanders around 100 , the simulation is at -400 . This approach always leads to p_value of 50 % . So when I tried using the initial_sate=0 and the random shocks , here 's the result : It seemed now that the simulations were following the predicted mean and its 95 % credible interval ( as cfulton commented below , this is actually a wrong approach as well as it 's replacing the level variance of the trained model ) . I tried using this approach just to see what p-values I 'd observe . Here 's how I compute the p-value : For context , this is the Causal Impact model developed by Google . As it 's been implemented in R , we 've been trying to replicate the implementation in Python using statsmodels as the core to process time series.We already have a quite cool WIP implementation but we still need to have the p-value to know when in fact we had an impact that is not explained by mere randomness ( the approach of simulating series and counting the ones whose summation surpasses y_post.sum ( ) is also implemented in Google 's model ) .In my example I used y [ 70 : ] += 10 . If I add just one instead of ten , Google 's p-value computation returns 0.001 ( there 's an impact in y ) whereas in Python 's approach it 's returning 0.247 ( no impact ) .Only when I add +5 to y_post is that the model returns p_value of 0.02 and as it 's lower than 0.05 , we consider that there 's an impact in y_post.I 'm using python3 , statsmodels version 0.9.0 [ EDIT2 ] After reading cfulton 's comments I decided to fully debug the code to see what was happening . Here 's what I found : When we create an object of type UnobservedComponents , eventually the representation of the Kalman Filter is initiated . As default , it receives the parameter initial_variance as 1e6 which sets the same property of the object.When we run the simulate method , the initial_state_cov value is created using this same value : Finally , this same value is used to find initial_state : Which results in a normal distribution with 1e6 of standard deviation.I tried running the following then : Which resulted in : I tested then the p-value and finally for a variation of +1 in y_post the model now is identifying correctly the added signal.Still , when I tested with the same data that we have in R 's Google package the p-value was still off . Maybe it 's a matter of further tweaking the input to increase its accuracy ."
"For example , I have a string : With re.sub , it will replace the matched with a predefined string . How can I replace the match with a transformation of the matched content ? To get , for example : If I write a simple regex ( ( \w+- ) +\w+ ) and try to use re.sub , it seems I ca n't use what I matched as part of the replacement , let alone edit the matched content :"
"Consider the following project structure : with test_a.py importing module a : As expected , running nosetests in the test directory results in import error : However , I noticed that adding an empty __init__.py file to the test directory makes import work with nosetests ( but not when you run test_a.py with Python ) . Could you explain why ? I understand that adding __init__.py makes test a package . But does it mean that import includes the directory containing the package in the lookup ?"
This unit test fails with the following exception : ValueError : Trying to compare non-ordered queryset against more than one ordered valuesWhat am I doing wrong ?
"I intend to change the monitor where I show a fullscreen window.This is especially interesting when having a projector hooked up.I 've tried to use fullscreen_on_monitor but that does n't produce any visible changes.Here is a non-working example : I get to see the window on the very same monitor ( out of 3 ) , regardless of the value I provide.My question is : how do I make the fullscreen window appear on a different monitor ?"
"I occasionally use Python string formatting . This can be done like so : But , this can also be done like this : As well as using % s : My question is therefore : why would I ever use anything else than the % r or % s ? The other options ( % i , % f and % s ) simply seem useless to me , so I 'm just wondering why anybody would every use them ? [ edit ] Added the example with % s"
"I am using NumPy to handle some large data matrices ( of around ~50GB in size ) . The machine where I am running this code has 128GB of RAM so doing simple linear operations of this magnitude should n't be a problem memory-wise.However , I am witnessing a huge memory growth ( to more than 100GB ) when computing the following code in Python : Please note that initial memory allocations are done without any problems . However , when I try to perform the subtract operation with broadcasting , the memory grows to more than 100GB . I always thought that broadcasting would avoid making extra memory allocations but now I am not sure if this is always the case.As such , can someone give some details on why this memory growth is happening , and how the following code could be rewritten using more memory efficient constructs ? I am running the code in Python 2.7 within IPython Notebook ."
"I was playing around with sys 's getsizeof ( ) and found that False ( or 0 ) consists of less bytes than True ( or 1 ) . Why is that ? In fact , other numbers ( also some that consist of more than one digit ) are 28 bytes . Even more : sys.getsizeof ( 999999999 ) is also 28 bytes ! sys.getsizeof ( 9999999999 ) , however , is 32.So what 's going on ? I assume that the booleans True and False are internally converted to 0 and 1 respectively , but why is zero different in size from other lower integers ? Side question : is this specific to how Python ( 3 ) represents these items , or is this generally how digits are presented in the OS ?"
"I am just trying to reproduce this simple example of an animation in Matplotlib but using PyPlot in Julia . I am having difficulties with the definition of the iterator simData ( ) that is passed to the function funcAnimation , because it seems that PyPlot does n't recognize the iterator that I defined in Julia ( via a Task ) as such . Here is my approach to define the same function simData ( ) : As you can check , this kind of iterator yields in theory the same values than the python simData ( ) generator of the example ( try for example collect ( simData ( ) ) . However , I got this error when I try to do the animationAs I mentioned , I think the problem is that the Julia iterator is not recognized as such by Python . Do you have any idea about how to fix that ? PS : Here is a Jupyter notebook with the full code that I used to do the animation ."
I have the following test code for my PyParsing grammar : Output is quite unexpected - seems that I only get the last expression in tokenized : How do I fix this ?
"What follows is a regular expression I have written to match multi-line pre-processor macros in C / C++ code . I 'm by no means a regular expressions guru , so I 'd welcome any advice on how I can make this better.Here 's the regex : It should match all of this : But only some of this ( should n't match the next line of code : And also should n't match single-line preprocessor macros.I 'm pretty sure that the regex above works - but as I said , there probably a better way of doing it , and I imagine that there are ways of breaking it . Can anyone suggest any ?"
"Now I 'm developing C # app running on Windows . Some of processes are written in Python , that called via pythonnet ( Python for .NET ) .The processes are calculation-heavy , so I want to do them in parallel.They are CPU-bounded and can be handled independently.As far as I know , there are 2 possible ways to realize it : Launch multiple Python runtimeThe first way is launching multiple Python interpreters but it seems unfeasible.Because pythonnet aparently can manage only one interpreter that initialialized by static method , PythonEngine.Initialize ( ) .From the Python.NET documentation : Important Note for embedders : Python is not free-threaded and uses a global interpreter lock to allow multi-threaded applications to interact safely with the Python interpreter . Much more information about this is available in the Python C-API documentation on the www.python.org Website . When embedding Python in a managed application , you have to manage the GIL in just the same way you would when embedding Python in a C or C++ application . Before interacting with any of the objects or APIs provided by the Python.Runtime namespace , calling code must have acquired the Python global interpreter lock by calling the PythonEngine.AcquireLock method . The only exception to this rule is the PythonEngine.Initialize method , which may be called at startup without having acquired the GIL.Use multiprocessing package in PythonThe other way is using multiprocessing package.According to Python documentation , following statement is necessary if the code runs on Windows to ensure spawn finite process : if __name__ == `` __main__ '' : However , the function written in Python is taken as a part of module since it 's embedded to .NET.For example , following code is executable , but spawns processes infinitely . Is there good idea to solve above problem ? Any comments would be appreciated . Thanks in advance ."
"I have a parent Python script that launches a child ( which launches grandchildren ) , and after some time , I terminate the child , but the grandchildren continue to pump to stdout . After I kill the child , I want to suppress/redirect the stdout and stderr of the grandchildren ( and all their descendants ) .Here is the parent : Here is the child script which I can not modify.Here is a noisy grandchild which I can not modify.I tried doing this right before killing the child : But it does n't seem to work . The output is :"
"I am trying to compute the volume of a 10 dimensional sphere with python , but my computation does n't work . Here is my code : Here the error : Could someone who knows this algorithm try to run it and tell me what would be corret to implement , to get the volume of this 10-dim sphere ? Thanks !"
"So , I 'm using python3.2 and bulbs on mac-osx with rexster and orientdb . Details : orientdb - standard download from their page~/workspace/orientdb-community-1.7-rc1Running the server , ./bin/server.shdatabase - orientdb database~/databases/orientdb/dev-db-01rexster - standard checkout from githubgit clone git : //github.com/tinkerpop/rexster.wiki.git ~/workspace/config/rexster.xml : Python code : Problem : What I think is that the url in the config of the python code is incorrect ( I 've tried all kinds of variations ) . But I do n't know that ; it works if I leave the rexster.xml untouched and just use the standard graph constructor ; but then that 's a problem , because it 's not adding nodes to the orientdb database that I want , dev-db-01 , it 's putting them in a default database . So to make sure that I connected to the right database , I disabled all but the orientdb database I had created . How do I make it connect properly ?"
"I use Keras to create a GRU model.I want to gather information from all the node vectors of the GRU model , instead of the last node vector.For example , I need to get the maximum value of each vector , like the image description , but I have no idea on how to do this ."
"Here is the message that I get . For what it 's worth , I opened my registry and noticed that there is no 2.7 folder , PythonCore leads straight into InstallPath . Any ideas ?"
"I 've been through itertools inside and out and I can not figure out how to do the following . I want to take a list.x = [ 1,2,3,4,5,6,7,8 ] and I want to get a new list : I need a list of all slices , but not combinations or permutations.x = list ( zip ( x [ : :2 ] , x [ 1 : :2 ] ) ) is close , but does n't do exactly what I 'm hoping"
"I was tracking down an out of memory bug , and was horrified to find that python 's multiprocessing appears to copy large arrays , even if I have no intention of using them . Why is python ( on Linux ) doing this , I thought copy-on-write would protect me from any extra copying ? I imagine that whenever I reference the object some kind of trap is invoked and only then is the copy made . Is the correct way to solve this problem for an arbitrary data type , like a 30 gigabyte custom dictionary to use a Monitor ? Is there some way to build Python so that it does n't have this nonsense ? Running :"
"I 'm trying to get through the process of authenticating a Google token for accessing a user 's calendar within a Django application . Although I 've followed several indications found on the web , I 'm stuck with a 400 error code response to my callback function ( Bad Request ) .views.pymodels.pyI 've downloaded the client_secrets.json file directly from the Google Dev Console.The specified Client ID type in the Dev Console is `` web application '' , which I think is correct.What I 've noticed is , if I remove the token validation code block : everything works correctly , flow and credentials get correctly stored in the database and I 'm allowed to read the calendar . What can I possibly be wrong with ? EDIT : I 've also checked outgoing ( to Google ) and incoming ( to callback ) data : OUTGOING : INCOMING : Data is identical , at least to a print to console . Also , the generation/validation operations via console work correctly ( xsrfutil.validate_token returns True , both with test and real data , including User model instances ) . I 'm even more puzzled ."
"I 'm trying to understand how to replicate the poly ( ) function in R using scikit-learn ( or other module ) .For example , let 's say I have a vector in R : And I want to generate 3rd degree polynomial : I get the following : I 'm relatively new to python and I 'm trying understand how to utilize the PolynomiaFeatures function in sklearn to replicate this . I 've spent time time looking at examples at the PolynomialFeatures documentation but I 'm still a bit confused . Any insight would be greatly appreciated . Thanks !"
"I 've generally been told that the following is bad practice.The main reasoning ( or so I 've been told ) , is that you could possibly import something you did n't want , and it could shadow a similarly named function or class from another module.However , what about PyQtEvery example I 've ever seen is written this way , mainly because everything exported from Qt starts with `` Q '' , so it 's not going to shadow anything . What 's the concensus ? Is it always bad to use * imports ? EDIT : Just to be clear , this question is specifically in regards to using PyQt4 . It has nothing to do with the way I am designing some other project.Basically , I 've found that coding to PEP8 has improved my code readability , except with regards to importing PyQt4 , and so I 've disregarded the frowns from purists until now . But now my dev group is deciding on one convention and I 'm wondering if this is a scenario `` where practicality beats purity '' , or if I should just suck it up and deal with monstrous PyQt4 imports"
"My Python program was too slow . So , I profiled it and found that most of the time was being spent in a function that computes distance between two points ( a point is a list of 3 Python floats ) : To analyze why this function was so slow , I wrote two test programs : one in Python and one in C++ that do similar computation . They compute the distance between 1 million pairs of points . ( The test code in Python and C++ is below . ) The Python computation takes 2 seconds , while C++ takes 0.02 seconds . A 100x difference ! Why is Python code so much slower than C++ code for such simple math computations ? How do I speed it up to match the C++ performance ? The Python code used for testing : The C++ code used for testing :"
"I 'm working on a simple translator from SQL INSERT statements to a dataset XML file to be used with DbUnit.My current definition looks like this : Now , I want to support case insensitive commands of SQL , for example , accept all of INSERT INTO , Insert Into , insert into and iNsErT inTO as the same thing.I wonder if there is a way to PLY use re.I so that it will ignore the case , or yet another alternative to write the rule that I 'm not familiar with ."
"I was playing around with metaclasses in Python and found something very curious . I can create two classes with the same name , but that are actually different objects . See : I thought names within a namespace should be unique . Is it not the case , then ?"
"Let 's say I disabled a pytest plugin in my pytest.ini file like : Now I would like to be able to enable it sometimes with command line arguments , something like : Is that possible ? Please , if you have better recommendations , I would like to know that too ."
"I want to encrypt file with simple AES encryption , here is my python3 source code.It works fine for some files , encounter error info for some files such as below : encrypt_file ( `` qwertyqwertyqwer '' , '/tmp/test1 ' , out_filename=None , chunksize=64*1024 ) No error info , works fine . encrypt_file ( `` qwertyqwertyqwer '' , '/tmp/test2 ' , out_filename=None , chunksize=64*1024 ) How to fix my encrypt_file function ? Do as t.m.adam say , to fix as To try with some file ."
"When using argparse , passing -- help to the program generates help text . Unfortunately , it 's hard to read because there are no blank lines between options . Here 's an excerpt to illustrate : Notice that in some cases , such as between -p and -s or between -c and -- version , it is difficult to tell at a glance which help text applies to which option . There should be a blank line between entries . For example : How can I accomplish this ? Several other questions recommend using argparse.RawTextHelpFormatter . The problem with that is that if I use it , I have to write my own logic to wrap the help text as the raw text help formatter does no formatting . The obvious answer would be to append '\n\n ' to the end of the help text and use the default formatter . But inexplicably , newlines get stripped.What 's the way forward here ? I 'm using Python 3.4 ."
"How to double a number of binary digits in an integer ? For example , if bin ( x ) = '' 1001 '' then bin ( y ) must be `` 11000011 '' . Is there any smart and fast algorithm ? UPDATE : Here is an elegant solution : where X is bin ( int_x ) [ 2 : ] However , I am interested in a more faster way and for the integers of any size . Maybe an arithmetical transformation should help ."
"I 'm creating a multi-processing program to process multiple batches , but my logging is unable to record the batch into log file , only root log.info will be recorded , how can set logging to properly print to log file ? The log will only print such a line `` INFO : root : this is root logging '' running on windows/python2.7"
"I have a class sysprops in which I 'd like to have a number of constants . However , I 'd like to pull the values for those constants from the database , so I 'd like some sort of hook any time one of these class constants are accessed ( something like the getattribute method for instance variables ) ."
"I do n't understand broadcasting . The documentation explains the rules of broadcasting but does n't seem to define it in English . My guess is that broadcasting is when NumPy fills a smaller dimensional array with dummy data in order to perform an operation . But this does n't work : The error message hints that I 'm on the right track , though . Can someone define broadcasting and then provide some simple examples of when it works and when it does n't ?"
"I 'm about to put a beta version of the site I 'm working on up on the web . It needs to have a beta code to restrict access . The site is written in django.I do n't want to change the fundamental Auth system to accommodate a beta code , and I do n't care particularly that the security of the beta code is iron-clad , just that it 's a significant stumbling block.How should I do this ? It 's a fairly large project , so adding code to every view is far from ideal.That solution works well . The Middleware Class I ended up with this this :"
"I have a Django App I 'm building , which we will call foo.Because of the way Foo is built , it requires a number of third-party django apps to function . For example , to run Foo an install apps might look like : In fact , for Foo to even be functional , 'prereq1 ' , prereq2 ' have to be installed in django . Now , I can add requirements to requirements.txt or setup.py to make sure the libraries are installed when someone goes to install Foo , but I ca n't figure out if there is a way to have them installed in Django itself.The reason for this is if someone wants to use Foo , I do n't want to include instructions like : In your INSTALLED_APPS add foo but also add scary_looking_library_name and thing_you_dont_understand.So is it possible for an app in INSTALLED_APPS to somehow require or inject further apps into that list ?"
How do I get a histogram of percentages of total instead of a histogram of count using Altair and Pandas ? I have this at the moment : Which I got by doing this :
"I 've cloned the flaskr application from Github and am trying to follow the Testing Flask Applications tutorial . Following Bonus : Testing the Application , I 've added a subdirectory test to the top-level flaskr directory , so that my directory tree looks like this : Note that there are also 'built-in ' tests in the directory tests ; however , I 'm writing tests in test_flaskr.py in the directory tests . So far I 'm trying just one test : However , if I try to run this I get the following error : I do n't understand this error . My flaskr.py is the same as the one on https : //github.com/pallets/flask/blob/master/examples/flaskr/flaskr/flaskr.py and has an init_db function defined . How can I make the unit test run ?"
"I am a fan of the outmoded game Age of Empires II ( AoE ) . I want to write a parser of AoE game record ( .mgx files ) using Python.I did some searching on GitHub and found little projects on this , the most useful one is aoc-mgx-format which provide some details of .mgx game record files.Here is the problem : according to the reference , structure of a .mgx file is like : | header_len ( 4byte int ) | next_pos ( 4byte int ) | header_data | ... ... |The hex data 's byte order in mgx format is little endian.header_len stores data length of the Header part ( header_len + next_post + header_data ) header_data stores useful imformation i need , but its compressed with zlibI tried to decompress data in header_data with zlib module like this : but I got this after running the program : Error -3 while decompressing data : incorrect header checkPS : Sample .mgx files can be found here : https : //github.com/stefan-kolb/aoc-mgx-format/tree/master/parser/recs"
"I 'm writing code that will run on Linux , OS X , and Windows . It downloads a list of approximately 55,000 files from the server , then steps through the list of files , checking if the files are present locally . ( With SHA hash verification and a few other goodies . ) If the files are n't present locally or the hash does n't match , it downloads them.The server-side is plain-vanilla Apache 2 on Ubuntu over port 80.The client side works perfectly on Mac and Linux , but gives me this error on Windows ( XP and Vista ) after downloading a number of files : This link : http : //bytes.com/topic/python/answers/530949-client-side-tcp-socket-receiving-address-already-use-upon-connect points me to TCP port exhaustion , but `` netstat -n '' never showed me more than six connections in `` TIME_WAIT '' status , even just before it errored out.The code ( called once for each of the 55,000 files it downloads ) is this : UPDATE : I find by greping the log that it enters the download routine exactly 3998 times . I 've run this multiple times and it fails at 3998 each time . Given that the linked article states that available ports are 5000-1025=3975 ( and some are probably expiring and being reused ) it 's starting to look a lot more like the linked article describes the real issue . However , I 'm still not sure how to fix this . Making registry edits is not an option ."
"I am trying to find the most efficient way to take the Map output from the GraphFrames function shortestPaths and flatten each vertex 's distances map into individual rows in a new DataFrame . I 've been able to do it very clumsily by pulling the distances column into a dictionary and then convert from there into a pandas dataframe and then converting back to a Spark dataframe , but I know there must be a better way.What I want is to take the output above and flatten the distances while keeping the ids into something like this : Thanks ."
"Apache v2.4.12-2Mod_wsgi v4.4.8-1Python v3.4.2python-flask v0.10.1-5Arch linux - kernel 3.12.36I 'm using mod_wsgi and flask to host a server . I am able to reproduce this issue with the following simplified code and generic .wsgi script : MainServer.py : MainServer.wsgi : Expected : a file with the contents 'test ' is written in /tmpActual outcome : No file is written . No errors reported in logIf I run the same code but instead point to any other directory which my user has permission to write , it creates the file as expected . /tmp is the only directory where I am having this issue.If I run the above code directly and use flask 's built in server ( app.run ) , it can create the file in /tmp as expected without any issues.I 've ensured that the mod_wsgi server is running as the same user as the script with app.run is and that this user is able to write to /tmp. -- edit -- Running httpd directly from the command line does not cause this issue . Starting httpd as a systemd service does"
"I want to exclude , programatically , a field in my form.Currently I have this : How can I exclude the active field only when a new model is being created ?"
"In the below code snippet , I would expect the logs to print the numbers 0 - 4 . I understand that the numbers may not be in that order , as the task would be broken up into a number of parallel operations.Code snippet : But when the above code is run , I see this instead : Instead of 0 - 4 , I see a series of 1 printed first , and an extra 0 . I have noticed the `` extra '' rows of value 1 occurring every time I have set up a Dask DataFrame and run an apply operation on it.Printing the dataframe shows no additional rows with value 1 throughout : My question is : Where are these rows with value 1 coming from ? Why do they appear to consistently occur prior to the `` actual '' rows in the dataframe ? The 1 values seem unrelated to the values in the actual rows ( that is , it is not as though it is for some reason grabbing the second row an extra few times ) ."
"I have the fallowing problem , Im supposed to get user input in the form of 10:10:10 ( hh : mm : ss ) or 10:10 ( mm : ss ) or 10 ( ss ) . Now i need check the fallowing parameters : If I 'm getting only seconds then there is no limit.If I 'm getting mm : ss then the seconds are limited to 0..59 and minutes are unlimited.If I 'm getting hh : mm : ss then both seconds and minutes are limited to 0..59 while hours are unlimited.Then return a TimeDelta object.The naive way is to write multiply if statements to check all this . But im looking for a smoother way ."
"I am using a sparse tensor array manipulation I built using dictionaries and Counters in Python . I would like to make it possible to use this array manipulation in parallel . The bottom line is that I have ended up having Counters on each node which I would like to add together using MPI.Allreduce ( or another nice solution ) . For instance with Counters one can do thissuch thatI would like to do this same operation but with all the relevant nodes , however , MPI does n't seem to recognize this operation on dictionaries/Counters , throwing an error expecting a buffer or a list/tuple . Is my best option a ` User-Defined Operation , ' or is there a way to get Allreduce to add Counters ? Thanks , EDIT ( 7/14/15 ) : I have attempted to create a user operation for dictionaries but there have been some discrepancies . I wrote the followingand when I told MPI about the function I did this : and in the code I used it asHowever , it threw unsupported operand '+ ' for type dict . so I wroteand now it throws dict1 [ key ] += dict2 [ key ] TypeError : 'NoneType ' object has no attribute '__getitem__ ' so apparentlyit wants to know those things are dictionaries ? How do I tell it they do have type dictionary ?"
"I 'm attempting to upgrade Django from 1.10.7 to 1.11.4 with Python 2.7.11 . I 've run pip install Django -U and python -c `` import django ; print django.get_version ( ) '' confirms that Django 1.11.4 is installed . However , when I then go to run tests , I get ImportError : No module named notmigrations . Does anyone have any advice ? Here is the full stack trace :"
"I am using gunicorn and flask for a web service . I am trying to get my head around running a streaming route ( not sure if that is the correct terminology ) .my route looks like this : I expect that the server would yield the output each time that delay_inner does a yield . But , what I am getting is all the json responses at once , and only when the delay_inner finishes execution.What am I missing here ? -- EDIT -- I have fixed the issue for Flask and Gunicorn , I am able to run it as expected by using the flask server , and by going to the Gunicorn port . It streams the data as expected . However , and I should have mentioned this in the original post , I am also running behind nginx . And that is not set up correctly to stream . Can anyone help with that ?"
"I 'm trying to connect a web page with mechanize but I 'm getting a http 401 error.Here 's my code ; Both add_password and addheaders are not working . Is it because I never specified a realm ? How can I get what realm is that web page using ? The username and password that I 'm using are correct , as I can login using chrome with those credentials ."
SetupSuppose I haveI need the position of maximal values by unique bin in bins.Those functions are hacky and are n't even generalizable for negative values.QuestionHow do I get all such values in the most efficient manner using Numpy ? What I 've tried.LINK TO TESTING AND VALIDATION
"I have an expression which has both sines and cosines and would like to write it using only sines ( or cosines ) , possibly using the power-reduction formula.I tried to use SymPy but I can not make it to `` rewrite '' to the desired output : Is there any way to tell Sympy to rewrite such expression using only sines ( or cosines ) ?"
"Going through the twisted finger tutorial and seen the SO questions : Question-1Question-2However , I ca n't ( yet ) write a twisted program that can read & write from multiple serial ports , especially where the protocol involves reading single or multiple lines , and writing back to the device accordingly.What I am trying to do is open 2 pairs ( i.e . total of 4 ) serial ports , for 2 modems . Communication with modems is using Hayes AT command set . While most of the command/response exchanges with modem is via the command-port , there are few diagnostic information that are available only via the diagnostic-port , for each modem . The diagnostic information should lead to a state-machine ( device-state , connection-state ) to be modified.Here is a rough skeletal program of what I understand as the potential approach ( based on single port examples ) : However , I am at loss , as to how do I do the following : How/where do I accept CLI input from user , that then triggers sending a set of AT command to the modems ? Correlate the information received on command port for ttyUSB0 & ttyUSB1 for modem1 , and similarly for the other pair for modem2 ? Note that each modem has it 's own state-machine ( device-state and connection-state ) Does twisted provide any mechanism for management of multiple state-machines by application ? It is possible that the USB-serial connection to the modem is destroyed due to the modem being unplugged , and re-established on being plugged back-in . How can I detect such events and add the monitoring of the corresponding device-ports to the reactor ? Currently , I 'm doing it statically in the main application ."
Just started toying around with Python so please bear with me : ) Assume the following list which contains nested lists : In a different representation : How would you go about extracting those inner lists so that a result with the following form would be returned : Many thanks ! EDIT ( Thanks @ falsetru ) : Empty inner-list or mixed type lists will never be part of the input .
"I am moving a Perl ( of which I have very little knowledge ) script to python.I can ( hopefully ) see what this line does , either set the variable 'path ' to the environment variable 'SOME_NAME ' or failing that then print an error message to the user . ( Side note : anyone know how to get a search engine to search for special characters like '|| ' ? ) I 've tried to implement it in a `` pythonic '' way ( Easier to Ask Forgiveness than Permission ) using : but this seems rather cumbersome , especially as I 'm doing it for 3 different environment variables.Any ideas if there is a better implementation or would you say this is the `` pythonic '' way to go about it ? Many Thanks"
"I am trying to use list_editable to make all my fields editable on the same page . But unless I also have something in list_display_links I get errors . The problems I do n't have any unused fields to put there . I am probably misunderstanding a concept somewhere.What I have done is create a 'dummy ' field in the model : dummy = None . This is not only clunky and probably wrong - but it also causes the dummy field to appear in my admin . What am I doing wrong ? I tried reading the docs but I ca n't find the solution to my problem . I would like to go about this the `` right way '' , whatever that may be.Here is my code : models.pyadmin.py"
"I run the SLIC ( Simple Linear Iterative Clustering ) superpixels algorithm from opencv and skimage on the same picture with , but got different results , the skimage slic result is better , Shown in the picture below.First one is opencv SLIC , the second one is skimage SLIC . I got several questions hope someonc can help.Why opencv have the parameter 'region_size ' while skimage is 'n_segments ' ? Is convert to LAB and a guassian blur necessary ? Is there any trick to optimize the opecv SLIC result ? ===================================OpenCV SLICSkimage SLICImage with superpixels centroid generated with the code belowbut there is one superpixel less ( top-left corner ) , and I found that len ( regions ) is 64 while len ( np.unique ( labels ) ) is 65 , why ?"
In both Python 2 and Python 3 the code : returns as error Foo object is not callable . Why does that happen ? PS : With old-style classes it works as expected.PPS : This behavior is intended ( see accepted answer ) . As a work-around it 's possible to define a __call__ at class level that just forwards to another member and set this `` normal '' member to a per-instance __call__ implementation .
"I 'm implementing a quantization algorithm from a textbook . I 'm at a point where things pretty much work , except I get off-by-one errors when rounding . This is what the textbook has to say about that : Rounded division by 2^p may be carried out by adding an offset and right-shifting by p bit positionsNow , I get the bit about the right shift , but what offset are they talking about ? Here 's my sample code : I 'm checking for negative input as bit shifting a negative integer appears to be implementation-dependent.My output : What is the offset that is mentioned in the textbook , and how can I use it to get rid of this error ?"
What is the difference between the two classes below ? Do you have some related information about this case ? Thank you very much .
"Let 's say I typein a .py file in vim and when I type `` a . '' and hit TAB , I would like to get suggestion menu that is related to lists.Edit 1 in response to Robin 's comment : I think it 's possible in vim , because there is a plugin that checks if a given python code is a valid code ( I do n't know what the plugin is called ) . Take a look :"
"I 'm using Apple 's EPFImporter tool http : //www.apple.com/itunes/affiliates/resources/documentation/epfimporter.htmlIt 's a Python script that will take space separated EPF file lists and import them into my database.Here 's what I have : Here 's what the CLI returns : The tool was capable of creating the database . It just wo n't add any of the entries to the database . It obviously sees 2mil+ records , and spends the time combing through them ... deleting and merging the blank tables ... but it 's just that - tables are still blank . I thought possibly it 's a permissions thing with mySQL . I double checked and made sure everything was granted to the user account I was using . Still nothing . Any ideas of what this might could be ?"
"I need a way to round a float to a given number of decimal places , but I want to always round down.For example , instead ofI would rather have"
"Some background first : I have a few rather simple data structures which are persisted as json files on disk . These json files are shared between applications of different languages and different environments ( like web frontend and data manipulation tools ) . For each of the files I want to create a Python `` POPO '' ( Plain Old Python Object ) , and a corresponding data mapper class for each item should implement some simple CRUD like behavior ( e.g . save will serialize the class and store as json file on disk ) . I think a simple mapper ( which only knows about basic types ) will work . However , I 'm concerned about security . Some of the json files will be generated by a web frontend , so a possible security risk if a user feeds me some bad json.Finally , here is the simple mapping code ( found at How to convert JSON data into a Python object ) : What possible security issues do you see ? NB : I 'm new to Python . Edit : Thanks all for your comments . I 've found out that I have one json where I have 2 arrays , each having a map . Unfortunately this starts to look like it gets cumbersome when I get more of these . I 'm extending the question to mapping a json input to a recordtype . The original code is from here : https : //stackoverflow.com/a/15882054/1708349.Since I need mutable objects , I 'd change it to use a namedlist instead of a namedtuple : Is it still safe ?"
"When creating decorators for use on class methods , I 'm having trouble when the decorator mechanism is a class rather than a function/closure . When the class form is used , my decorator does n't get treated as a bound method . Generally I prefer to use the function form for decorators but in this case I have to use an existing class to implement what I need . This seems as though it might be related to python-decorator-makes-function-forget-that-it-belongs-to-a-class but why does it work just fine for the function form ? Here is the simplest example I could make to show all goings on . Sorry about the amount of code : Output :"
Here I 'm attempting to implement a neural network with a single hidden layer to classify two training examples . This network utilizes the sigmoid activation function.The layers dimensions and weights are as follows : I 'm experiencing an issue in back propagation where the matrix dimensions are not correct . This code : Returns error : How to compute error for previous layer ? Update : Returns error : Have incorrectly set matrix dimensions ?
"I 'm trying to get some Python code working on a fresh installation on Anaconda on Mac OS X Mojave with Python 2.7 . This was all stuff that was working before on the same machine.The error I 'm getting is this : Lowercase does n't work either : I 've done a lot of searching , and the suggestions for how to deal with this error are mostly of the form `` make sure you 've installed X . '' Here 's what I 've got : The brew install steps changed the error from something different , but as you can see they did not leave me with a working setup ."
"I have a class similar to the following : I want to have the following Python code work : I was thinking of __setattr__ and __getattr__ , but not sure if it works . An alternative is to implement the following Python : not as good as the previous one , but might be easier to implement ( with __slice__ ? ) .PS . I am using sip to do the binding.Thanks ."
"I have two dataframes df1 and df2 . df1 contains the information of the age of people , while df2 contains the information of the sex of people . Not all the people are in df1 nor in df2I want to have the information of the sex of the people in df1 and setting NaN if I do not have this information in df2 . I tried to do df1 = pd.merge ( df1 , df2 , on = 'Name ' , how = 'outer ' ) but I keep the information of some people in df2 that I do n't want ."
I just tried to run this script with Python 3.3.Unfortunately it 's about twice as slow than with Python 2.7.Here are the results : Profiling shows that the additional time is spend in print : How can I avoid this overhead ? Has it something to do with UTF-8 ?
"I am looking for a way to use a method of a class which is not defined in that class , but handled dynamically . What I would like to achieve , to take an example , is to move fromto the possibility of using aaa and bbb ( and others ) within the class without the need to define them explicitly . For the example above that would be a construction which receives the name of the method used ( aaa for instance ) and format a message accordingly . In other other words , a `` wildcard method '' which would itself handle its name and perform conditional actions depending on the name . In pseudocode ( to replicate the example above ) Is such a construction possible ?"
"I am trying to multiply two Series , both with MultiIndex : The problem is that I can not reindex the Series from 2 to 3 levels : I found this workaround : But I think there should be a method to perform this operation in just 1 step ."
"Consider this snippet : which evaluates to : Can anyone explain why the 'in ' keyword has a different meaning for sets and lists ? I would have expected both to return True , especially when the type being tested has equality methods defined ."
"I have to add dynamic fields at run time in my django application , but I do n't know the proper way how to add new fields at run time . I want to add the code which will generate the dynamic field and will update database too . I am using postgresql database . please help if anyone can.My `` model.py '' is simply like this :"
"A simple recursive factorial method works perfectly : But I wanted to experiment a little and use a dict instead . Logically , this should work , but a bunch of print statements tell me that n , instead of stopping at 0 , glides down across the negative numbers until the maximum recursion depth is reached : Why is that ?"
"I assume that this question has been addressed somewhere , but I have spent an inordinate amount of time looking around for the answer including digging into the source code a bit . I have tried to put the problem in the first paragraph . The rest shows a basic example of the problem.I am attempting to compile a module that contains a USE statement pointing to another , more general , module . I would prefer to keep the used module separate so that it can be used in several `` packages '' as a set of general settings . When I compile the two modules using f2py everything works as advertised from the fortran side , but from the python side USE appears to be ignored . If I allow f2py to generate a signature file , the file contains a USE statement as is appropriate , but if I complete the compilation and import from the resulting library the parameters from the used module are not available in the module that contains the use statement . Below are two modules illustrating the situation : In order to show the intermediate step I ran f2py -h test.pyf test.f90 test2.f90 . The following signature file is generated ; note that the `` test2 '' module contains `` use test '' : If I now compile with f2py -- fcompiler=gfortran -c test.pyf test.f90 test2.f90 I obtain test.so ( same as running f2py -- fcompiler=gfortran -m test -c test.f90 test2.f90 without creating the signature file first ) . Importing from this library in python exposes test.test.a and test.test2.b , but does not expose test.test2.a as can be seen here : Just to illustrate that b is defined properly in test2 from the perspective of fortran , the following code uses test2 and prints both b and b : After compiling with `` f2py -m run_test -c test.f90 test2.f90 run_test.f90 '' and obtaining run_test.so , run_test can be imported in python and works as expected : Any help with this issue would be greatly appreciated.TL ; DR : When an F90 module that contains a USE is compiled by f2py it does not expose the parameters that are defined in the `` used '' module as attributes in Python ."
"I 'm trying to understand how super ( ) works . I understand what it does , but I do n't understand the mechanics of what 's going on behind the scenes . One thing I do n't exactly understand is the difference between : and : Maybe there is no difference in these two examples , but I know there are other situations where the placement of super ( ) matters . For example , this Django method I needed help with the other day , I was instructed to move super ( ) above the if statement , rather than at the bottom . I would like to know why this matters ."
"I cant understand when should I use pack and unpack functions in struct library in python ? I also cant understand how to use them ? After reading about it , what I understood is that they are used to convert data into binary . However when I run some examples like : I cant make any sense out of it.I want to understand its purpose , how these conversions take place , what does '\x ' and other symbols represent/mean and how does unpacking work ."
"I have a data frame with a column named SAM with following dataNow I want to Insert 12 , 15 and 43 respectively in the Nan values ( because 9+3=12 , 12+3=15 , and 40+3=43 ) . In other words , fill any Nan row by adding 3 to previous row ( which can also be Nan ) . I know this can be done by iterating through a for loop . But can we do it in a vectorized manner ? Like some modified version of ffill ( which could have been used here if we did n't have consecutive NaNs ) in pandas.fillna ( ) ."
"Given any iterable , for example : `` ABCDEF '' Treating it almost like a numeral system as such : ABCDEFAAABACADAEAFBABBBC ... .FFAAAAAB ... .How would I go about finding the ith member in this list ? Efficiently , not by counting up through all of them . I want to find the billionth ( for example ) member in this list . I 'm trying to do this in python and I am using 2.4 ( not by choice ) which might be relevant because I do not have access to itertools.Nice , but not required : Could the solution be generalized for pseudo- '' mixed radix '' system ? -- - RESULTS -- -times :"
How would one run unit tests with nose for Apache Spark applications written in Python ? With nose one would usually just call the commandto run the tests in the tests directory of a Python package . Pyspark scripts need to be run with the spark-submit command instead of the usual Python-executable to enable the import of the pyspark-module . How would I combine nosetests with pyspark to run tests for my Spark application ?
Is it possible to use line_profiler with Numba ? Calling % lprun on a function decorated with @ numba.jit returns an empty profile : There 's a workaround for Cython code but ca n't find anything for Numba .
"In Python the try statement supports an else clause , which executes if the code in try block does not raise an exception . For example : Why is the else clause needed ? Ca n't we write the above code as follows : Wo n't the execution proceed to data = f.read ( ) if open does not raise an exception ?"
"To be able to run a TensorFlow lite model that supports native TensorFlow operations , the libtensorflow-lite static library has to be re-compiled . The instructions for doing this in C++ can be found HERE . It states that When building TensorFlow Lite libraries using the bazel pipeline , the additional TensorFlow ops library can be included and enabled as follows : Enable monolithic builds if necessary by adding the -- config=monolithic build flag . Add the TensorFlow ops delegate library dependency to the build dependencies : tensorflow/lite/delegates/flex : delegate . Note that the necessary TfLiteDelegate will be installed automatically when creating the interpreter at runtime as long as the delegate is linked into the client library . It is not necessary to explicitly install the delegate instance as is typically required with other delegate types.The thing is that the standard way of building the static lib is via a shell script/make ( see the docs HERE ; this is for arm64 , but there are scripts that can be used for x86_64 as well ) . There 's no obvious way for me to build tensorflow-lite via bazel and modify the build commands there.Has anybody successfully built this when trying to build models for arm64/x86_64 architectures and can share this ? I 'm new to bazel and can not find a detailed walkthrough.EDITAfter troubleshooting steps proposed by @ jdehesa , I was able to build libtensorflowlite.so , but ran into another problem . My app built successfully , but upon execution of the app , the .so file can not be found : The paths are correct due to other .so files being located in the same directory which can be found . Also , the app works if using the static library.To reproduce the issue , I used the tensorflow/tensorflow : devel-gpu-py3 docker build image ( instructions found here ) . I executed the configure script with default settings , and used the commandto create the library . I have uploaded by built library on my personal repo ( https : //github.com/DocDriven/debug-lite ) ."
"I want to create share button that will use android ACTION_SEND intent for sharing image . It 's my code : But it does n't work ) It throws this error jnius.jnius.JavaException : Invalid instance of u'android/net/Uri ' passed for a u'java/lang/String ' in this line shareIntent.putExtra ( Intent.EXTRA_STREAM , uri ) . How can i fix this ?"
"Why does the __get__ method in a python descriptor accept the owner class as it 's third argument ? Can you give an example of it 's use ? The first argument ( self ) is self evident , the second ( instances ) makes sense in the context of the typically shown descriptor pattern ( ex to follow ) , but I 've never really seen the third ( owner ) used . Can someone explain what the use case is for it ? Just by way of reference and facilitating answers this is the typical use of descriptors I 've seen : Given that instance.__class__ is available all I can think of is that explicitly passing the class has something to do with directly accessing the descriptor from the class instead of an instances ( ex Container.managed_attr ) . Even so I 'm not clear on what one would do in __get__ in this situation ."
"Given a set of N elements colored with C colors , how can I find every possible combination of length L that contains no more than a maximum of M colors ? I tried this algorithm that uses itertools.combinations to generate all the possible combinations , and then filter out those that do not hold the maximum colors condiction.the output is like : The problem is that generating all possible combinations can be computationally very expensive . In my case , for instance , L is often 6 and the number of elements N is around 50 , so it gives us Bin ( 50,6 ) = 15890700 possible combinations . If maximum number of colors allowed in a comination is small , most of combinations are `` useless '' and so they are discarded in the filter step . My intuition is that I should put the filtering step inside/before the combinatory step , to avoid the explotion of combinations , but I do n't see how ."
"In JS , we can write closure like : However , if I write following code in pythonThen I get UnboundedLocalError.Can anyone tell me the difference between closure in python and JS ?"
"We have a dataframe likeI would like to create new column here , based on groups of customer_id , random strings of 8 characters assigned to each group.For example the output would then look likeI am used to R and dplyr , and it is super easy to write this transformation using them . I am looking for something similar in Pandas to this : I can figure out the random character part . Just curious on how Pandas groupby works in this case.Thanks !"
"So I want to know , is there any simple code for making an Hour Glass pattern with odd or even input using Java or Python ? Because my code is not simple ( I 'm using Python ) .Here 's the output example : And then , here is my code : And this is the result from my code :"
"I am having a hard time converting a string representation of non-UTC times to UTC due to the timezone abbreviation . ( update : it seems that the timezone abbreviations may not be unique . if so , perhaps i should also be trying to take this into account . ) I 've been trying to look for a way around this using datetutil and pytz , but have n't had any luck.Suggestions or workaround would be appreciated.I 'd like to convert that into UTC time , accounting for daylight savings when appropriate.UPDATE : Found some references that may help more experienced users answer the Q.Essentially , I would imagine part of the solution doing the reverse of this.FINAL UPDATE ( IMPORTANT ) Taken from the dateutil docs examples.Some simple examples based on the date command , using the TZOFFSET dictionary to provide the BRST timezone offset . parse ( `` Thu Sep 25 10:36:28 BRST 2003 '' , tzinfos=TZOFFSETS ) datetime.datetime ( 2003 , 9 , 25 , 10 , 36 , 28 , tzinfo=tzoffset ( 'BRST ' , -10800 ) ) parse ( `` 2003 10:36:28 BRST 25 Sep Thu '' , tzinfos=TZOFFSETS ) datetime.datetime ( 2003 , 9 , 25 , 10 , 36 , 28 , tzinfo=tzoffset ( 'BRST ' , -10800 ) ) Combine this with a library such as found here . and you will have a solution to this problem ."
"I setthe string to parse is : I use : but get the following error : Traceback ( most recent call last ) : File `` /Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py '' , line 1531 , in globals = debugger.run ( setup [ 'file ' ] , None , None , is_module ) File `` /Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py '' , line 938 , in run pydev_imports.execfile ( file , globals , locals ) # execute the script File `` /Applications/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py '' , line 18 , in execfile exec ( compile ( contents+ '' \n '' , file , 'exec ' ) , glob , loc ) File `` /Users/adieball/Dropbox/Multiverse/Programming/python/repositories/kindle/kindle2en.py '' , line 250 , in main ( sys.argv [ 1 : ] ) File `` /Users/adieball/Dropbox/Multiverse/Programming/python/repositories/kindle/kindle2en.py '' , line 154 , in main note_date = parser.parse ( result.group ( 2 ) ) File `` /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/dateutil/parser.py '' , line 1164 , in parse return DEFAULTPARSER.parse ( timestr , **kwargs ) File `` /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/dateutil/parser.py '' , line 555 , in parse raise ValueError ( `` Unknown string format '' ) ValueError : Unknown string formata debug show that parser is not using the `` correct '' dateutil values ( german ) , it 's still using the english ones.I 'm sure I 'm missing something obvious here , but ca n't find it.Thanks ."
"I 'd like to create an array of the Unicode code points which constitute white space in JavaScript ( minus the Unicode-white-space code points , which I address separately ) . These characters are horizontal tab , vertical tab , form feed , space , non-breaking space , and BOM . I could do this with magic numbers : That 's a little bit obscure ; names would be better . The unicodedata.lookup method passed through ord helps some : But this does n't work for 0x9 , 0xb , or 0xc -- I think because they 're control characters , and the `` names '' FORM FEED and such are just alias names . Is there any way to map these `` names '' to the characters , or their code points , in standard Python ? Or am I out of luck ?"
"I have a data set like below : How can I extract the key value pairs , and turn them into a dataframe expanded all the way out ? Expected output : The message contains multiple different key types . Any help would be greatly appreciated ."
"I want to create a windows desktop widget . I will create a custom UI for the widget in Qt Designer and add functionality using Python . But , I do not want the application to have an icon on the taskbar at all . How should I modify my code and make my application ( and its instances or other similar applications ) to have no task bar footprint ? How can I hide the taskbar icon on windows ? Here is an example code : and this is its ui , `` try.ui '' : Edit : Here is how default icon looks like on the taskbar . I just do not want it there , as expected from a widget ."
"I have a list of dates , for example : How do I find the contiguous date ranges contained within those dates ? In the above example , the ranges should be : Thanks ."
"Given a 2d Numpy array , I would like to be able to compute the diagonal for each row in the fastest way possible , I 'm right now using a list comprehension but I 'm wondering if it can be vectorised somehow ? For example using the following M array : I would like to compute the following array :"
"I have problems to make it working the ec2.py script for dynamic inventory.Installed ansible on CentOS 7 : Configure a simple hosts file and ssl key access . Running ansible : Get the expected output . Ansible is working.Installed aws-cli : Configure credentials file in ~/.aws . Running aws : Get the expected output . Aws-cli is working.Installed boto.Downloaded ec2.py and ec2.ini from links as in the official documentation ( http : //docs.ansible.com/ansible/latest/intro_dynamic_inventory.html # example-aws-ec2-external-inventory-script ) . Running it I get the following error : Tried to install ansible from git and with pip , same as boto . Always get the same error.What I am missing here ?"
"I have an Excel file ( .xlsx ) with about 800 rows and 128 columns with pretty dense data in the grid . There are about 9500 cells that I am trying to replace the cell values of using Pandas data frame : The replace ( ) takes 60 seconds . Any way to speed this up ? This is not huge data or task , I was expecting pandas to move much faster . FYI I tried doing the same processing with same file in CSV , but the time savings was minimal ( about 50 seconds on the replace ( ) )"
"So I am using python to call methods in a shared C++ library . I am having an issue converting a numpy 2D array to a C++ 2D array of shorts as a function input . I have a created a toy example that exhibits the problem . Feel free to compile and try it out ! Here is the python code ( soexample.py ) : Here is the C++ library ( soexample.cpp ) : which I compile with the following command : When I run the python file , I get the following error : How do I properly correct this unfortunate TypeError ?"
"Is it possible to create a spider which inherits the functionality from two base spiders , namely SitemapSpider and CrawlSpider ? I have been trying to scrape data from various sites and realized that not all sites have listing of every page on the website , thus a need to use CrawlSpider . But CrawlSpider goes through a lot of junk pages and is kind of an overkill.What I would like to do is something like this : Start my Spider which is a subclass of SitemapSpider and pass regexmatched responses to the parse_products to extract usefulinformation method.Go to links matching the regex : /reviews/ from the products page , and sending the data to parse_review function . Note : `` /reviews/ '' type pages are not listed in sitemapExtract information from /reviews/ pageCrawlSpider is basically for recursive crawls and scraping -- -- -- -ADDITIONAL DETAILS -- -- -- -The site in question is www.flipkart.comThe site has listings for a lot of products , with each page having its own detail page.Along with the details page , their is a corresponding `` review '' page for the product . The link to the review page is also available on the product details page . Note : Review pages are not listed on the sitemap ."
"I was trying to run Jenkins on a simple Python test . It 's my first time , so I followed steps mentioned in this tutorial : LinkMy Jenkins console output after I triggered build shows that it failed because it could n't find any revision.Here is the console output : My Git installation folder is : home/rahul/PycharmProjects/ , and my system is a Ubuntu 14.04 LTS.I tried changing the Branches to Build under the Source Code Management configuration to be ** in place of */master as a workaround but it did n't work.Has anybody worked on this before ? What should be the solution to this.Edit 1 : As per suggestions I tried using the git push to push the code , but I am getting this :"
"EDIT : Looks like this is a very old `` bug '' or , actually , feature . See , e.g. , this mail I am trying to understand the Python scoping rules . More precisely , I thought that I understand them but then I found this code here : In Python 3.4 the output is : If I replace the inner class by a function then it reasonably gives UnboundLocalError . Could you explain me why it behaves this strange way with classes and what is the reason for such choice of scoping rules ?"
"I have a set of nested tuples : I would like to combine lists with similar prefixes , resulting in : Here is another example : would be merged to : These are intended to store paths from the root to the tree leaves : 'baz ' - > 'bing ' - > 'fizz ' , aka . ( 'baz ' ( 'bing ' ( 'fizz , ) ) ) 'baz ' - > 'zap ' - > 'zang ' , aka ( 'baz ' ( 'zap ' , ( 'zang ' , ) ) ) 'baz ' - > 'bing ' - > 'frazz ' - > 'blop ' , aka ( 'baz ' , ( 'bing ' , ( 'frazz ' , ( 'blop , ) ) ) ) I want to merge the elements where the leaves are reached by the same path . I hope this provides some amount of clarification.I 've written some code to do this , but it is ugly , verbose , and probably fragile . Is there some generic , concise , and/or efficient way of doing this ? I imagine there may be some sort of itertools magic that I do n't know about which would provide some elegant solution.Note : I 'm using python 2.4"
"I 'm using colander to validate ( and deserialize json data ) input to some web services.I would like to add a rule to a colander schema to catch an empty list , but I can not figure out how to do it . Right now I have the following example , demonstrating a call to the function f ( ) with two different sets of data . I would like the later to trigger the colander.Invalid exception because of the empty events listSuggestions ?"
"I am counting the number of peaks and troughs in a numpy array.I have a numpy array like so : Plotted , this data looks like something like this : I am looking to find the number of peaks in this time series : This is my code , which works well for an example like this where there are clear peaks and troughs in the time series representation . My code returns the indexes of the array where peaks have been found.Result : There are 8 clear peaks that have been found and can be correctly counted.My solution does not appear to work well with data that is less clear cut and more messy.An array below does not work well and does not find the peaks that I need : Plotted , this data looks like : And the same code returns : This output incorrectly counts data points as peaks.Ideal OutputThe ideal output should return the number of clear peaks , 11 in this case which are located at indexes : I believe my problem comes about because of the aggregated nature of the argrelextrema function ."
"matplotlib python : How do you change the background color of a line plot according to a given column ? Say I have the following data fileThe first column represents the y-values , and the 2nd column should control the background color . Say , it plots the ( black ) line on a white-gray alternating background ( zebra-like ) as proceeding further in x-direction , where the transition in color occurs anytime the integer in the 2nd column increments . Or other possible solution : Use 2nd column as function argument to determine background color.How would one do this with matlibplot ?"
"I 'd like to have a small browser that uses my own CSS.The problem is that CSS is not loaded or , I guess , it loads but without any effect.Here is the full code ( I do n't use an Interface Builder ) : The code runs without errors . Printsbut there is no effect of my CSS in the WebView.I tried different solutions like adding a link to the DOM : but in either case it does n't work.I do n't want to use javascript to load CSS.Can anybody tell me what 's wrong with setting user CSS in my code ?"
"I 've written a function that saves all numbers between two digit groups to a text file , with a step option to save some space and time , and I could n't figure out how to show a percentage value , so I tried this.What I tried to do here was make the program do an iteration as many times as it would usually do it , but instead of writing to a file , I just made percent_quotient variable count how many times iteration is actually going to be repeated . ( I called j dummy variable since it 's there only to break the loop ; I 'm sorry if there is another expression for this . ) The second part is the actual work and I put counter variable , and I divide it with percent_quotient and multiply with 100 to get a percentage.The problem is , when I tried to make a dictionary from length of 1 to length of 8 , it actually took a minute to count everything . I imagine it would take much longer if I wanted to make even bigger dictionary.My question is , is there a better/faster way of doing this ?"
"In one of my Python packages the __init__.py file contains the statementWhat does the `` . '' mean here ? I got this technique by looking at another package , but I do n't understand what it means . Thanks !"
"I fully realize that the order of unit tests should not matter . But these unit tests are as much for instructional use as for actual unit testing , so I would like the test output to match up with the test case source code.I see that there is a way to set the sort order by setting the sortTestMethodsUsing attribute on the test loader . The default is a simple cmp ( ) call to lexically compare names . So I tried writing a cmp-like function that would take two names , find their declaration line numbers and them return the cmp ( ) -equivalent of them : When I run this , I get this output : indicating that the test cases are not running in the declaration order.My sort function is just not being called , so I suspect that main ( ) is building a new test loader , which is wiping out my sort function ."
"The code is here : When I run the code , it gives me an error message and I do n't really understand what 's going on here . Do functions decorated by njit not support creating new arrays inside the functions ? Error message is the following : EDIT : I forgot to transpose the array before edit , it should be a 10^9 by 3 array ."
"I have a situation where I have a lot of < b > tags : As you can see , the second last tag is empty . When I call : Which gives me : I would like to have : Is there a way to get the empty value ? My current work around is to call : And then parsing through each html tag myself ( the empty tags are here , which is what I want ) ."
"I have a python script which does many simulations for different parameters ( Q , K ) , plots results and stores it to disk.Each set of parameters ( Q , K ) produces a 3D volumetric grid of data 200x200x80 datapoints , which requires ~100 MB of data . A part of this volumetric grid is then plot , layer by layer , producing ~60 images.The problem is that python obviously does not release memory during this process . I 'm not sure where the memory leak is , or what the rules are governing how python decides which objects are deallocated . I 'm also not sure if the memory is lost in numpy arrays or in matplotlib figure objects.Is there a simple way to analyze which objects in python persist in memory and which were automatically deallocated ? Is there a way to force python to deallocate all arrays and figure objects which were created in particular loop cycle or in particular function call ? The relevant part of code is here ( however , it will not run ... the bigger part of the simulation code including ctypes C++/python interface is omitted because it is too complicated ) :"
"I have a pandas dataframe which contains 3 columns , each containing a site that a user has visited during a session.In some cases , a user may have not visited 3 sites in a single session . This is shown by a 0 , denoting that no site has been visited . In the example above , user 0 has visited sites 5 , 8 and 1 . User 1 has visited site 8 only , and user 2 has visited sites 1 and 17.I would like to create a new column , last_site , which shows the last site visited by the user in that session.The result I want is this : How can I do this in a concise way using pandas ?"
"I 'm using pipenv inside a docker container . I tried installing a package and found that the installation succeeds ( gets added to the Pipfile ) , but the locking keeps failing . Everything was fine until yesterday . Here 's the error : What could be wrong ? EDITAfter removing Pipfile.lock and trying to install a package , I got :"
"Somehow the memory my Python program takes more and more memory as it runs ( the VIRT and RES ) column of the `` top '' command keep increasing . However , I double checked my code extremely carefully , and I am sure that there is no memory leaks ( did n't use any dictionary , no global variables . It 's just a main method calling a sub method for a number of times ) . I used heapy to profile my memory usage by each time the main method calls the sub method . Surprisingly , it always gives the same output . But the memory usage just keeps growing . I wonder if I did n't use heapy right , or VIRT and RES in `` top '' command do not really reflect the memory my code uses ? Or can anyone provide a better way to track down the memory usage in a Python script ? Thanks a lot !"
"I have a dataframe in which one column contains tuples : I would like to select the rows where an element I provide is in the tuple . For example , return rows where 4 is in a tuple , expect outcome would be : I have tried : But this returns an empty dataframe :"
The results are the same if I set borderwidth to zero . I ca n't find the setting or property that explains or controls these 4 extra pixels .
I am trying to the run the following code but I get a local variable ' a ' referenced before assignment . The statement works without the % timeit magic.Is there something I am missing ? Thank you .
"My code currently has to count things in a heavily nested dict into another . I have items that need to be indexed by 3 values and then counted . So , before my loop , I initialize a nested defaultdict like so : Which allows me to count the items within a tight loop like so : I feel like initializing all those defaultdicts feels a lot like making a type declaration in something like Java . Is there a more idiomatic/Pythonic way of doing something like this ?"
"I 'd assume it would just be loaded into settings.py , and then the object would just be imported from the settings , but I just wanted to know if there was a more standard or recommended way to do this . Where do people usually load their items that their entire project needs see ? Ex . Thanks ."
"The following program raises an error : It looks like whenever I try to use addstr ( ) to write a character in the last column of the last line of a window ( even when it is smaller than the screen ) , it raises an error . I do n't want to scroll , I do n't care about the cursor 's position . All I want is being able to write characters in every single position of the window . Is it possible at all ? How can I do this ?"
"I have some multiprocessing code where I 'd like to share a nested dictionary among processes . The dictionary is never modified by the processes ; just read.In the simplest form , the problematic code is as follows : I get a KeyError at the print statement where it says that the key I 'm using does not exist . I 'm not sure why this is happening , since I just inserted the key into the dict . When I change this into a regular dict , the problem goes away ."
"on Python 3.6.2 , Win10-x64Just something curious that I encountered and could n't quite explain.In : Out : How come it just turns into inf ? Why does n't it throw an exception ? If I replace the *= in the last line with **= for example , then by the second iteration it raises OverflowError : ( 34 , 'Result too large ' ) , which makes sense ( since it is a mere 10.000.000.000^10.000.000.000 ) .Does this mean that there is a kind of `` soft '' limit on floats that -- when exceeded -- turns them into inf ? And if so , what is that limit and is it the same regardless of the arithmetic operation ? And would n't that imply something like inf == this_limit + anything ? .ADD : I get that there is the sys.float_info.max . Is that the limit ? I just got an idea and tested some stuff out : This gives me : This is strange to me ..."
"How come the dir ( ) function in Python does n't show all of the callable attributes ? Result : Location is callable and returns the file path , but is not listed in the first result . It also does n't show up with code completion tools . Is it because it 's being fetched through a getter method ? I see it listed under _prop_map_get_ and _prop_map_put_.Also , why does currentTrack.Location return a file path when currentTrack._prop_map_get_ [ 'Location ' ] returns `` ( 1610874880 , 2 , ( 8 , 0 ) , ( ) , 'Location ' , None ) ? '' Where is it getting the file path string ?"
"I have a python script that correctly sets the desktop wallpaper via gconf to a random picture in a given folder.I then have the following entry in my crontab* * * * * python /home/bolster/bin/change-background.pyAnd syslog correctly reports executionBut no desktopy changey , Any ideas ?"
"The french Sécurité Sociale identification numbers end with a check code of two digits . I have verified that every possible common transcription error can be detected , and found some other kinds of errors ( e.g. , rolling three consecutive digits ) that may stay undetected.My Python 2.7 code ( working fragment above ) makes heavy use of generators . I was wondering how I could adapt them in OCaml . I surely can write a function maintaining some internal state , but I 'm looking for a purely functional solution . May I study the library lazy , which I 'm not too familiar with ? I 'm not asking for code , just directions ."
I have a little command line tool that reads from stdin.On the command line I would run either ... or ... With a gziped file I can runin Python I can do ... but I ca n't doI wind up having to runAm I doing something wrong ? Why ca n't I use gzip.open ( 'bar ' ) as an stdin arg to Popen ?
I am getting this error when I deployed my python app with Flask on Google AppEngine . I will be grateful if someone help me.ps : My local server works like a charm
"I need to “ override ” some of the base class ' nested class members , while keeping the rest intact.This is what I do : In fact , I want InternApplicationForm.Meta to be exactly like InternGenericForm.Meta , except that its exclude tuple should contain one more item.What is a more beautiful way of doing this in Python ? I wish I did n't have to write boilerplate code like model = InternGenericForm.Meta.model that is also prone to errors ."
"I want to set the height of a treeview widget to accommodate N rows , in order to do that I need to identify a single row height . I am using the code below : After creating the tree widget , filling the data model , and selecting the first row , height is set to 0 . What I am missing ? P.S . : The same code will return a valid height when run from a `` row_activated '' signal handler ."
"I 'm using a list of lists to store a matrix in python . I tried to initialise a 2x3 Zero matrix as follows.However , when I change the value of one of the items in the matrix , it changes the value of that entry in every row , since the id of each row in mat is the same . For example , after assigningmat is [ [ 1 , 0 ] , [ 1 , 0 ] , [ 1 , 0 ] ] . I know I can create the Zero matrix using a loop as follows , but can anyone show me a more pythonic way ?"
"Sorry , trying to understand and get used to dictionary and list objects . I 'm calling eBay 's API through their ebaysdk , and want to store the items from it to a collection as documents in Mongo . Simple.Here 's a sample of the schema that will be returned : I 've tried 500 iterations of this code , stripped down to the most basic here 's what I have.Will get this error : Simple stuff . All I want to do is add each item as a document ."
"in Python 3.6 , I use unittest.mock.patch to patch a function like this : This passes a mock.MagicMock ( ) as mocked_f and everything works fine . However , when I want to use a custom mock object instead of the default mock.MagicMock ( ) using new argument , the patch decorator does not pass the mocked object to the test_f method . Running this code will raise a TypeError : My question is : why is this happening ?"
"There are at least two ways to reverse a list in Python , but the iterator approach is much faster ( at least in Python 2.7.x ) . I want to understand what contributes to this speed difference.I suspect the speed difference is due to at least the following : reversed is written in Creversed is an iterator , so less memory overheadI tried to use the dis module to get a better view of these operations , but it was n't too helpful . I had to put these operations in a function to disassemble them.What all exactly happens during a slicing operation , is there a lot of memory overhead ? Maybe slicing is implemented in pure Python ?"
Question : How can proxy scrapy requests with socks5 ? I know I can use polipo to convert Socks Proxy To Http ProxyBut : I want to set a Middleware or some changes in scrapy.Requestwhat should I assign to proxies variable ?
I want to use sqlalchemy in a way like this : Usually sqlalchemy will create two entries for the account and link each email address to this . If i set the accountname as unique sqlalchemy is throwing an exception which tells me about an entry with the same value already exists . This makes all sense and works as expected.Now i figured out an way by my own which allows the mentioned code and just creates an account only once by overwriting the the new Constructor of the AccountModel Class : This is working perfectly for me . But the question is : Is this the correct way ? Is there an sqlalchemy way of achieving the same ? I 'm using the latest 0.8.x SQLAlchemy Version and Python 2.7.xThanks for any help : )
"I want to split Brazilian names into parts . However there are names like below where `` de '' , `` da '' ( and others ) that are not separate parts and they always go with the following word . So normal split does n't work.My expected output would be : For the special cases I tried this pattern : but the output is not what I expected : Any idea how to fix it ? Is there a way to just use one pattern for both `` normal '' and `` special '' case ?"
"I have this C++ function , which I can call from Python with the code below . The performance is only half compared to running pure C++ . Is there a way to get their performance at the same level ? I compile both codes with -Ofast -march=native flags . I do not understand where I can lose 50 % , because most of the time should be spent in the C++ kernel . Is Cython making a memory copy that I can avoid ? I have this .pyx fileI call this function in PythonThis is the setup.py : And here the C++ reference file that reaches twice the performance :"
"I am currently trying to implement a multi-input model in Keras . The input consists of multiple batches , and each includes different samples , but I get a 'different samples'-error . My implementation looks like this : The model site looks as follows : The site where the error occurs : The error I get is : The abstract model architecture looks as follow :"
"Say I have an input file , and a target directory . How do I determine if the input file is on the same hard-drive ( or partition ) as the target directory ? What I want to do is the copy a file if it 's on a different , but move it if it 's the same . For example :"
"When I program , I like to give my variables very meaningful name.From my understanding , in C++ and other compiled language , these two lines are totally equivalent : line1 : line 2 : the reason is : it will be compiled , and variable names are lost in the process ( ? ) .On the other hand , and that is what I am asking , it seems that the two following will not be equivalent in python : line1 : line 2 : the reason being it is interpreted , and the interpreter will use some dictionaries with variable name as key that it will be querying ( ? ) . I have no idea how these dictionaries are implemented , but that does not sound crazy to me that hashing long variable name might takes longer ( ? ) , and in certain cases have an impact.So , should I be careful in certain case to keep my variable name length reasonable ? note 1 : this thread is very similar : Does how you name a variable have any impact on the memory usage of an application ? but no clear answer concerning python emerges ( selected answer says it has an impact on interpreted languages in general , other answers say it does not have impact on python in particular ) note 2 : my point is not to advocate misplaced micro-optimizations that result of unreadable code . I want to be aware if by giving extra long names , I am doing some sort of compromise on execution speed or not ."
"I am writing Python code with the BigQuery Client API , and attempting to use the async query code ( written everywhere as a code sample ) , and it is failing at the fetch_data ( ) method call . Python errors out with the error : ValueError : too many values to unpackSo , the 3 return values ( rows , total_count , page_token ) seem to be the incorrect number of return values . But , I can not find any documentation about what this method is supposed to return -- besides the numerous code examples that only show these 3 return results.Here is a snippet of code that shows what I 'm doing ( not including the initialization of the 'client ' variable or the imported libraries , which happen earlier in my code ) .What are the specific return results I should expect from the job_results.fetch_data ( ... ) method call on an async query job ?"
"In Ruby , you can call Enumerable # any ? on a enumerable object to see if any of its elements satisfies the predicate you pass in the block . Like so : In Python , there 's an any function that does something similar , but on a list of booleans.Of course , for a reasonably-sized list , I 'd just do : However , if my list is very long , I do n't want to have to do the entire map operation first.So , the question : Is there a generic short-circuiting any function in Python ? Yes , I know it 's really trivial to write one myself , but I 'd like to use speedy builtin functions ( and also not reinvent any wheels ) ."
"My problem comes directly from the CS circles site . It 's the last problem on the bottom of this page called 'Primed for Takeoff ' . The basic rundown is they want a list of 1,000,001 length , where the index of each item is True if the index is prime , while the index of each item is False if it is not prime . So , for example , isPrime [ 13 ] is True . isPrime [ 14 ] is False.The first little bit of the list 'isPrime ' would look like : My problem is the 7-second time limit they have . I have a working code below with a lower number , 101 , for debugging purposes . When I bump it up to their required 1,000,001 list length , it takes way too long , I ended up killing the program after a few minutes.This is my working ( at length 101 ) , very slow code : Then there 's this one I found here . It has a quicker run time , but only outputs a list of primes , not a list of n length with list [ n ] returning True for primes and false otherwise . I 'm not sure if this 2nd bit of code really holds the key to creating a prime list of length 1,000,001 in under 7 seconds , but it was the most relevant thing I could find in my research.CS circles seems pretty commonly used , but I was n't able to find a working version of this for Python . Hopefully this will be an easy solve for someone.This question differs from this one because am not trying to only create a list of primes quickly , but to create a list of all positive integers from 0 to n that are marked as prime by True and non prime by False ."
"My website supports a number of Indian languages . The user can change the language dynamically . When user inputs some string value , I have to split the string value into its individual characters . So , I 'm looking for a way to write a common function that will work for English and a select set of Indian languages . I have searched across sites , however , there appears to be no common way to handle this requirement . There are language-specific implementations ( for example Open-Tamil package for Tamil implements get_letters ) but I could not find a common way to split or iterate through the characters in a unicode string taking the graphemes into consideration.One of the many methods that I 've tried :"
"Suppose i have a DataFrame : which looks likeI group it : And now , let me show , what i want with the help of example on one group ' b ' : group b : I need : In the scope of each group , count diff ( ) between VALUE values , skipping all NaNs and 0s . So the result should be :"
I used inheritance model in my project after changing the model ; but I give non-nullable field error . What should I do ? I am using Django 1.7After migrations :
"I am trying to run a Flask app which consists of : Yielding API requests on the flyUploading each request to a SQLalchemy databaseRun jobs 1 and 2 as a background processFor that I have the following code : All of the above works , BUT NOT AS A BACKGROUND PROCESS . The app hangs when cache ( ) is called , and resumes only when the process is done . I run it with gunicorn -c gconfig.py app : app -w 4 -- threads 12what am I doing wrong ? EDIT : If simplify things in order do debug this , and write simply : THEN the process runs in the background.However , if I add another job : background does not work again.In short : Background only works if I run ONE job at a time ( which was the limitation behind the idea of using queues in the first place ) .seems like the problem is binding the background process to SQLalchemy , do n't know . totally lost here ."
"I am new to django and I am confused with the image display problem . Now I have a image of word-cloud generated in the backend ( let 's say , topicWords.py ) and I do n't know hot to deal with it . ( 1 ) How can I store it in the image field of model UserProfile ? In the models.py , I have : Is it right to directly do like : ( 2 ) What should the setting of MEDIA_ROOT and MEDIA_URL be ? ( 3 ) How should I display it in the .html ? Thank you very much !"
"I have some code that samples a posterior distribution using MCMC , specifically Metropolis Hastings . I use scipy to generate random samples : Generally , I try to avoid using explicit for loops in python - I would try to generate everything using pure numpy . For the case of this algorithm however , a for loop with if statements is unavoidable . Therefore , the code is quite slow . When I profile my code , it is spending most time within the for loop ( obviously ) , and more specifically , the slowest parts are in generating the random numbers ; stats.beta ( ) .pdf ( ) and stats.norm ( ) .pdf ( ) .Sometimes I use numba to speed up my code for matrix operations . While numba is compatible with some numpy operations , generating random numbers is not one of them . Numba has a cuda rng , but this is limited to normal and uniform distributions.My question is , is there a way to significantly speed up the code above using some sort of random sampling of various distributions compatible with numba ? We do n't have to limit ourselves to numba , but it is the only easy to use optimizer that I know of . More generally , I am looking for ways to speed up random sampling for various distributions ( beta , gamma , poisson ) within a for loop in python ."
"Another question provides a nice , simple solution for implementing a test for equality of objects . I 'll repeat the answer for context : I would like to do this for a class that uses __slots__ . I understand that both the base class and the subclass will have to use slots , but how would you define __eq__ for this to work with slots ?"
"I have written python code to programmatically generate a convolutional neural network ( CNN ) for training and validation .prototxt files in caffe . Below is my function : Is there a way to similarly generate deploy.prototxt for testing on unseen data that is not in an lmdb file ? If so , i would really appreciate it if someone can point me to a reference ."
"Here 's the situation . I got a list on my Django REST API : /playerslist/It returns me a list of players just like this one : http : //pastebin.com/JYA39gHTThis is exactly what I want for the moment . But now , I need this : Going for /playerslist/1/ gives me different infos for the Player Number 1 . The list is here only for listing players with basic informations . But I need detailed view for players , containing info from other models and with different serialization , it must be a basic issue , but as I 'm totally new to Django and Python in general , I must misunderstanding something.Here is my Viewset : How can I achieve this ? Must I use @ detail_route to have something like playerslist/1/detail ? I 've already tried but DRF 's documentation only show a single example and it 's not clear at all for me ."
"I am writing a little optimization tool for purchasing stamps at the post office.In the process I am using a dictionary , which I am sorting according to what I learned in this other `` famous '' question : Sort a Python dictionary by valueIn my case my dictionary is mildly more complex : - one four-item-tuple to make the key - and another five-item-tuple to make the data.The origin of this dictionary is an iteration , where each successful loop is adding one line : This is just a tiny example of a trivial run , trying for 75 cents : { ( 0 , 0 , 1 , 1 ) : ( 22 , 75 , 2 , 2 , 0 ) ( 0 , 0 , 0 , 3 ) : ( 31 , 75 , 3 , 1 , 0 ) ( 0 , 0 , 2 , 0 ) : ( 2521 , 100 , 2 , 1 , 25 ) ( 0 , 1 , 0 , 0 ) : ( 12511 , 200 , 1 , 1 , 125 ) ( 1 , 0 , 0 , 0 ) : ( 27511 , 350 , 1 , 1 , 275 ) } So far I am using this code to sort ( is is working ) : I am sorting by my evaluation-score , because the sorting is all about bringing the best solution to the top . The evaluation-score is just one datum out of a five-item-tuple ( in the example those are the evaluation-scores : 22 , 31 , 2521 , 12511 and 27511 ) .As you can see in the example above , it is sorting ( as I want it ) by the second tuple , index 1 . But I had to ( grumpily ) bring my `` evaluation-score '' to the front of my second tuple . The code is obviously using the entire second-tuple for the sorting-process , which is heavy and not needed.Here is my question : How can I please sort more precisely . I do not want to sort by the entire second tuple of my dictionary : I want to target the first item precisely . And ideally I would like to put this value back to its original position , namely to be the last item in the second tuple - and still sort by it.I have read-up on and experimented with the syntax of operator.itemgetter ( ) but have not managed to just `` grab '' the `` first item of my second item '' .https : //docs.python.org/3/library/operator.html ? highlight=operator.itemgetter # operator.itemgetter ( note : It is permissible to use tuples as keys and values , according to : https : //docs.python.org/3/tutorial/datastructures.html ? highlight=dictionaryand those are working fine for my project ; this question is just about better sorting ) For those who like a little background ( you will yell at me that I should use some other method , but I am learning about dictionaries right now ( which is one of the purposes of this project ) ) : This optimization is for developing countries , where often certain values of stamps are not available , or are limited in stock at any given post office . It will later run on Android phones.We are doing regular mailings ( yes , letters ) . Figuring out the exact postage for each destination with the available values and finding solutions with low stocks of certain values is a not-trivial process , if you consider six different destination-based-postages and hundreds of letters to mail.There are other modules which help turning the theoretical optimum solution into something that can actually be purchased on any given day , by strategic dialog-guidance ... About my dictionary in this question : I iterate over all reasonable ( high enough to make the needed postage and only overpaying up to a fraction of one stamp ) combinations of stamp-values . Then I calculate a `` success '' value , which is based on the number of stamps needed ( priority ) , the number of types needed ( lower priority ) ( because purchasing different stamps takes extra time at the counter ) and a very high penalty for paying-over . So lowest value means highest success.I collect all reasonable `` solutions '' in a dictionary where the tuple of needed-stamps serves as the key , and another tuple of some results-data makes up the values . It is mildly over-defined because a human needs to read it at this phase in the project ( for debugging ) .If you are curious and want to read the example ( first line ) : The colums are : number of stamps of 350 cents number of stamps of 200 cents number of stamps of 50 centsnumber of stamps of 25 centsevaluation-scorecalculated applied postagetotal number of stamps appliedtotal number of stamp-typesover-payment in cents if anyOr in words : ( Assuming a postal service is offering existing stamps of 350 , 200 , 50 and 25 cents ) , I can apply postage of 75 cents by using 1x 50 cents and 1x 25 cents . This gives me a success-rating of 22 ( the best in this list ) , postage is 75 cents , needing two stamps of two different values and having 0 cents overpayment ."
"I need a Python/C/C++/Java implementation which can pause hashing progress and store that progress in a file in such a way that the progress is recoverable from that file at a later stage.No matter in what language it is written from above listed , it should work properly in Python . It is recommended that you may provide that to work well with `` hashlib '' however it is not necessary . Also , if such a thing already exist , a link to that is sufficient.For an idea , what your implementation should achieve.This should give the same output as we had done simple hashing of `` Hello World '' with standard sha256 function ."
"I have a bunch of code : I want to divide the statements into blocks and write to a log after each block . The logging is a bit complex : I want to log things like the running time of each block and the state of a particular data structure after the block executes . So I created a decorator called log_block which handles all of these details . Now my code looks like this : This works just fine , but it 's a little clunky . It 's annoying that I have to separately call the three block functions , and if I want to share a variable between the blocks then I either have to give the block functions arguments and return statements or use global variables , neither of which is particularly palatable . What I really want is syntax which looks like this : so that I am decorating the statements directly rather than enclosing them in auxiliary block functions . Is there any way to achieve something like this ?"
"Could n't find a way to phrase the title better , feel free to correct.I 'm pretty new to Python , currently experimenting with the language.. I 've noticed that all built-ins types can not be extended with other members.. I 'd like for example to add an each method to the list type , but that would be impossible . I realize that it 's designed that way for efficiency reasons , and that most of the built-ins types are implemented in C.Well , one why I found to override this behavior is be defining a new class , which extends list but otherwise does nothing . Then I can assign the variable list to that new class , and each time I would like to instantiate a new list , I 'd use the list constructor , like it would have been used to create the original list type.Output : The idea can be generalize of course by defining a method that gets a built it type and returns a class that extends that type . Further more , the original list variable can be saved in another variable to keep an access to it.Only problem I 'm facing right now , is that when you instantiate a list by its literal form , ( i.e . [ 1,2,3,4 ] ) , it will still use the original list constructor ( or does it ? ) . Is there a way to override this behavior ? If the answer is no , do you know of some other way of enabling the user to extend the built-ins types ? ( just like javascript allows extending built-ins prototypes ) .I find this limitation of built-ins ( being unable to add members to them ) one of Python 's drawbacks , making it inconsistent with other user-defined types ... Overall I really love the language , and I really do n't understand why this limitation is REALLY necessary ."
"I was writing a code in python to find factor pairs for an integer . But making pairs resulted in reverse pairs as well . I want to eliminate those reverse pairs using a simple method without importing any modules.for eg . [ [ 1 , 200 ] , [ 2 , 100 ] , [ 4 , 50 ] , [ 5 , 40 ] , [ 8 , 25 ] , [ 10 , 20 ] , [ 20 , 10 ] , [ 25 , 8 ] , [ 40 , 5 ] , [ 50 , 4 ] , [ 100 , 2 ] , [ 200 , 1 ] ] the output should be : This is what I 've got so far :"
"How can I specify a minimum or maximum floating point argument using argprase ? I 'd like to be able to provide a command-line argument between a min and max floating point value.The closest thing I can find is the choices option in add_argument ( ) , but that only specifies allowable values for the argument.The command-line argument 0.5 for L fails :"
"I have a function that searches a string in a list of lists then returns a list containing the matching lists : Now I want to extend it to handle advanced searching with queries like : Are there any python modules ( preferably builtin ) that I can use in my function to handle these queries ? PS : I 'm aware of Swoosh but it 's not the right fit for me at the moment . Also , I 'm currently using App Engine.What I 'm trying to do basically is full-text search in memory , since app engine doens't support full-text search yet . I query the datastore , put the entities into lists and loop through those lists to find query matches ."
output : 2 When this code is saved into a file and run.How could it be 2 when it 's the main thread ? Does python run another thread by default in addition to the main thread when we run a foo.py file ?
"I wrote my data to a file using pprint.PrettyPrinter and I am trying to read it using ast.literal_eval.This has been working for me for quite some time , and I am reasonably satisfied with the text representation produced.However , today I got this error on deserialization : How do I fix this specific file ? The file in question is 17k lines/700kb.I loaded it into Emacs -- the parens are balanced . There are no non-ASCII characters in the file.I can `` divide and conquer '' ( split the file in half and try to real each half ) - but this is rather tedious.Is there anything better ? I modified ast.literal_eval : _convert to print the offending node - it turned out to be < _ast.UnaryOp object at 0x110696510 > . Not very helpful.How do I ensure that this does not happen in the future ? I hope JSON is not the answer . ; - ) I am not using JSON becauseJSON can not handle non-string dict keysJSON inserts either too many newlines or none at all"
"I am trying to understand why running multiple parsers in parallel threads does not speed up parsing HTML . One thread does 100 tasks twice as fast as two threads with 50 tasks each.Here is my code : Output on my 4 cores CPU machine : According to the FAQ ( http : //lxml.de/FAQ.html # can-i-use-threads-to-concurrently-access-the-lxml-api ) two threads should work faster than one thread . Since version 1.1 , lxml frees the GIL ( Python 's global interpreter lock ) internally when parsing from disk and memory , as long as you use either the default parser ( which is replicated for each thread ) or create a parser for each thread yourself ... . The more of your XML processing moves into lxml , however , the higher your gain . If your application is bound by XML parsing and serialisation , or by very selective XPath expressions and complex XSLTs , your speedup on multi-processor machines can be substantial.So , the question is why two threads are slower than one thread ? My environment : linux debian , lxml 3.3.5-1+b1 , same results on python2 and python3BTW , my friend tried to run this test on macos and got same timings for one and for two threads . Anyway , that is not as it supposed to be according to the documentation ( two threads should be twice as fast ) .UPD : Thanks to spectras . He pointed that it needs to create a parser in each thread . The updated code of the func function is : The output is : That is exactly what I wanted ! : )"
"I frequently need to check if an instance of a class has a property and if that property exists , make a comparison with its value . Is there no way to do it besides the following ?"
"I am using keras ( ver . 2.0.6 with TensorFlow backend ) for a simple neural network : It is only a test for me , I am `` training '' the model with the following dummy data.then i do : The result is : Ok , It works , but it is just a test , i really do not care about accuracy etc . I would like to understand how i can work with output of different size.For example : passing a sequence ( numpy.array ) like : I would like to get 4 dimensions output as prediction : Is that possibile somehow ? The size could vary I would train the model with different labels that can have different N-dimensions.Thanks"
"I just started with python and wrote myself a nice , small script that usesgnome-notifications via pynotify , like this : This works great , but the thing is , when I execute the script twice in a row it takes a while for the first notification to go away . The second gets shown after that . Since the first one is obsolete when i execute the script for the second time , I want to remove the first one programmatically prior to showing the second ( or replace it ) . Is this possible , and if it is , how ? A bit of context to understand why I need this : Since I often switch my mouse from left- to right-handed and the other way around , I want a script that just inverts this preference and tells me in a notification `` switched to left-handed '' and `` switched to right-handed '' ."
"The answer to the related post How to connect to an existing Jupyter Notebook kernel and inspect variables using the Variable explorer ? seems to be that it 's just not possible . But what if we try to connect a Jupyter Notebook to a spyder kernel ? Could it then be possible to have the kernel available to the Jupyter Notebook , Spyder and the Variable explorer in Spyder ? The article Connecting Spyder IDE to a remote IPython kernel makes it sound somewhat possible : Doing data science in Jupyter notebook is fun but sometimes you need the ability to poke into variables using the handy variable explorer in Spyder . Connecting Spyder to a remote kernel is as straightforward as the notebooks . I will walk you through the process.But I 'm not able to reproduce the example , and I 'm a bit afraid that I have n't quite understood the authors meaning with the whole article.This is what I 've tried:1. conda install spyder-kernels works fine.2 . The same does python -m spyder_kernels.console : But I 'm having problems with the rest , and the article says nothing about connecting Jupyter Notebook to -- existing kernel-19909.json . I 'm also having problems with the part python -m spyder_kernels.console — matplotlib= ’ inline ’ — ip=x.x.x.x -f=./remotemachine.json.Any suggestions as to how I can take this further ? System info :"
"SciPy 's stats module have objects of the type `` random variable '' ( they call it rv_frozen ) . It makes it easy to plot , say , cdf 's of random variables of a given distribution . Here 's a very simple example : I wondered whether there 's a way of doing basic arithmetic manipulations on such random variables . The following example is a wishful thinking ( it does n't work ) .This wishful-thinking example should produce the graph of the cumulative distribution function of the random variable which is the average of 100 i.i.d . random variables , each is distributed uniformly on the set { 2,3,4 } ."
"I use aiohttp to request the url . Most of the time it runs normally , but sometimes it stops without raising any exception.As you can see in the code , I catch all the exceptions , but when it stops no log of exceptions is printed.The logs look like : but the 'res = yield from r.json ( ) ' does not print , it stops and does not throw any exceptions ."
"If I have a waveform x such aswith arbitrary W and Ph , and I calculate its ( Real ) FFT f withI can get the original x withNow , what if I need to extend the range of the recovered waveform a number of samples to the left and to the right ? I.e . a waveform y such that len ( y ) == 48 , y [ 16:32 ] == x and y [ 0:16 ] , y [ 32:48 ] are the periodic extensions of the original waveform.In other words , if the FFT assumes its input is an infinite function f ( t ) sampled over t = 0 , 1 , ... N-1 , how can I recover the values of f ( t ) for t < 0 and t > =N ? Note : I used a perfect sine wave as an example , but in practice x could be anything : arbitrary signals such as x = range ( 16 ) or x = np.random.rand ( 16 ) , or a segment of any length taken from a random .wav file ."
"For a project , I need to replicate some results that currently exist in Stata output files ( .dta ) and were computed from an older Stata script . The new version of the project needs to be written in Python.The specific part I am having difficulty with is matching quantile breakpoint calculations based on the weighted version of Stata 's xtile command . Note that ties between data points wo n't matter with the weights , and the weights I am using come from a continuous quantity , so ties are extremely unlikely ( and there are no ties in my test data set ) . So miscategorizing due to ties is not it.I have read the Wikipedia article on weighted percentiles and also this Cross-Validated post describing an alternate algorithm that should replicate R 's type-7 quantiles.I 've implemented both of the weighted algorithms ( code at the bottom ) , but I 'm still not matching very well against the computed quantiles in the Stata output.Does anyone know the specific algorithm used by the Stata routine ? The docs did not describe this clearly . It says something about taking a mean at flat portions of the CDF to invert it , but this hardly describes the actual algorithm and is ambiguous about whether it 's doing any other interpolation.Note that numpy.percentile and scipy.stats.mstats.mquantiles do not accept weights and can not perform weighted quantiles , just regular equal-weighted ones . The crux of my issue lies in the need to use weights.Note : I 've debugged both methods below quite a lot , but feel free to suggest a bug in a comment if you see one . I have tested both methods on smaller data sets , and the results are good and also match R 's output for the cases where I can guarantee what method R is using . The code is not that elegant yet and too much is copied between the two types , but all that will be fixed later when I believe the output is what I need.The problem is that I do n't know the method Stata xtile uses , and I want to reduce mismatches between the code below and Stata xtile when run on the same data set.Algorithms that I 've tried :"
I wrote a simple Python extension function in C that just reads a Numpy array and it crashes.Here is how it is called : What 's wrong with my code ?
"I am deploying a Google Cloud function from a source repository . I can get the Google Function to deploy fine as long as I do n't make any reference in my main.py to any of my user-defined functions in the repository ( i.e . scripts/functions in either package or package2 below other than main.py ) .I have 2 recent Stack Overflow questions regarding this function deploy here and here . They have collectively guided me towards a solution where main.py and requirements.txt are within a package and I deploy the Google Cloud Function with that package as the source . The problem now though is that I need to be able to access functions/scripts in other packages/scripts . If I include a statement like from package.script1 import foo in main.py I get an error like the one below when I deploy the function : I am currently deploying this function with this google function deploy statement : Moving main.py and requirements.txt back to the root directory does n't work due to the issues in this stack overflow question.I need a way to import user-defined packages/functions during a gcloud functions deploy ... statement . My main.py makes use of functions in package.script1.py , package.script2.py , package2.script3.py , and package2.script4.py . How can this be accomplished ? EDIT/UPDATED INFORMATIONI do have __init__.py files in each of the packages ( package and package2 ) , but they 're currently blank/empty.I have tried the following gcloud deploy ... options : main.py and requirements.txt in the root directory ( instead of the configuration as shown above/originally ) with this deploy command : This produces the following error : In an attempt to explicitly call out the root directory as source , I 've also tried this with the same file configuration as in # 1It gives me the same error as # 1 : The using the original file structure in this question ( with main.py and requirements.txt in package ) , I have tried the deploy statement provided above . It resolves the main.py not found issue , but gives me the error as stated in the original question aboveWhat else should I try ?"
I started writing a Plugin for Sublime Text 2.I created a new folder in `` Packages/RailsQuick '' And Created 2 files : RailsQuick.pyRailsQuick.sublime-commandsThe problem is that i cant find RailsQuick : Generators in the Command Platte ( CTRL + SHIFT + P ) Console logs after saving both files : What am i doing wrong ?
"Hi I have a filter 'm ' set that is flexible enough to change by me . Sometimes , I want to filter by Car or x_acft_body , or any of the various other fields , etc . Sometime I want to have all of the rows returned by commenting and uncommenting the required lines . But without changing the subsequent code , after the filter 'm ' line.How can I have a filter that will return true for ALL rows , when I do n't want the filters applied ? For e.g . something like 1==1 but i know this does n't work.I do n't want to set dfdata.somefield.notnull ( ) etc . as I will not be too sure if this field will be always not null or not . also I DO NOT want to change subsequent code to be like dfdata.groupby . i.e . without the [ m ]"
"Sometimes , I want to plot discrete value in pcolormesh style . For example , to represent a 2-d array in the shape of 100x100 which contain int 0~7The figure shows like this : How to generate the colorbar in legend style . In other word , each color box corresponds to its value ( e.g pink colorbox -- > 0 ) An illustration here ( Not fit this example ) :"
"I have a 4 dimensional numpy array of shape ( N , N , Q , Q ) . So given a row and column index ( i , j ) , mat [ i , j ] is a QxQ matrix . I want to reshape this array to shape ( N*Q , N*Q ) such thatgoes to You can see that mat [ 0,0 ] goes to new_mat [ 0:2 , 0:2 ] . Currently mat.reshape ( N*Q , N*Q ) takes mat [ 0,0 ] to new_mat [ 0:4 , 0 ] ( which is what I do not want ) . How can I use reshape or rollaxis or something similar to reshape this array ? I eventually want to plot it with imshow , am currently stuck . I figure it 's easy to do , I just have n't yet figured it out ."
"I 'm using pytest to test some code based on TensorFlow.A TestCase is defined for simplicity like : The problem is tf.test.TestCase provide a useful function self.test_session ( ) , which was treated as a test method in pytest since its name starts with test_.The result pytest report more succeed tests than test methods I defined due to test_session ( ) methods.I use the following code to skip test_session : However there would be some `` s '' in test report indicating there are some skip tests.Is there anyway I can mark one exact method not a test method without changing pytest test discovery rules globally ?"
"I recently wrote a rather ugly looking one-liner , and was wondering if it is better python style to break it up into multiple lines , or leave it as a commented one-liner . I looked in PEP 8 , but it did not mention anything about thisThis is the code I wrote : But would something like this be better style ? : Or perhaps something in between ?"
"Given a structured numpy array , I want to remove certain columns by name without copying the array.I know I can do this : But this creates a temporary copy of the array which I want to avoid because the array I am dealing with might be very large.Is there a good way to do this ?"
"I got a 2 dimensional list : In which I need to change every second element to 0 , starting from first one . So it should look like this : Algorithm I use : But what I get is this : Some elements are not changed , and it 's because ( I added index print to see that ) it thinks that index of those elements are 7 and 9 for some reason . What can it be , because I am looking for a bug for so long still can not find..I double checked , there are not extra spaces or anything in the list ."
"I have a class called Bones I have 5 Bones in my skeleton dictionary . However in my actual implementation there are 300+ bones , that 's why I am asking this question today on stackoverflow.Each Bone has : ID : An int to identify a bonew : w position ( float between -1 and 1 ) x : x position ( float between -1 and 1 ) y : y position ( float between -1 and 1 ) z : z position ( float between -1 and 1 ) Bone.pyExplanation of the problemI am trying to make a tkinter GUI that looks something like this : Legend : Green - represents a Frame ( just my annotation to explain ) Red - are attributes of the object ( just my annotation to explain ) Black - are methods of the object ( just my annotation to explain ) Blue - are text and buttons displayed to meAs you can see , it shows the ID , w , x , y , z . And under it , there is a + button and a - button . Each time these buttons get clicked , I want to decrease the corresponding value in the object and update the tkinter number displayed . I know how to do this manually , but as per my requirement , I have 300+ Bones . I can not make these frames manually.How can I create these frames in a loop and update the value displayed on the GUI and object when a + or - button is clicked ? main.py"
"Why does one use max_length of CharField equal to 2^n whilst others use 2^n-1 ? For example : in django.contrib.gis.db.backends.postgis.models ( django 1.3 ) : in django_openid_auth.models ( djano-openid-auth 0.3 ) : Although it is not scientific measure , 2048 seems to be more popular than 2047 , but 255 is more popular than 256 . Django documentation says , that in MySQL max_length is restricted to 255 characters if you are using unique=True . But why would I use 2^n-1 instead od 2^n in other cases ?"
"I have read Expert Python Programming which has an example for multi-inheritance . The book author has explained but I did not understand it , so I would like to have another view . The example shows that object B is created two times ! Could you please give me an intuitive explanation.The book author said : This happens due to the A.__init__ ( self ) call , which is made with the C instance , thus making super ( A , self ) .__init__ ( ) call B 's constructorThe point from which I did n't get its idea is how A.__init__ ( self ) call will make super ( A , self ) .__init__ ( ) call B 's constructor"
"We use namedtuple like this : I found the first argument of namedtuple seems useless , since : Firstly , we can not use it ( to create an instance , for example ) : Secondly , there seems no constraint for it ( for example , we do n't have to make it unique ) : I did not find a explanation from the manual or by googling . There is a relevant question here , but it did not answer why namedtuple need the first argument and how it can be used or when it 's necessary ."
"Suppose we have a list L. The cartesian product L x L could be computed like this : How can the cartesian power L x L x L x ... x L ( n times , for a given n ) be computed , in a short and efficient way ?"
"I have a multi-index DataFrame with the first level as the group id and the second level as the element name . There are many more groups but only the first is shown below.I have another DataFrame containing only 1 index that is the group id . The columns for both are the same and they are dates.I want to use the second DataFrame to filter the first one by checking if each element is smaller than the value of the group on that date to get something like this : Ultimately , I am only interested in the elements that were True and the dates in which they were True . A list of elements that were true over an iteration of dates would be great , which I 've though to do by making the False NaN and then using dropNa ( ) .I know I can write bunch of nested for loops to do this but time is of crucial importance ; I ca n't think of a way to use pandas dataframe structure intrinsically and pythonically to do this . Any help would greatly appreciated !"
"Sorting a list of tuples ( dictionary keys , values pairs where the key is a random string ) is faster when I do not explicitly specify that the key should be used ( edit : added operator.itemgetter ( 0 ) from comment by @ Chepner and the key version is now faster ! ) : Gives : If however I create a custom object passing the key=lambda x : x [ 0 ] explicitly to sorted makes it faster : Gives : Is this expected ? Seems like second element of the tuple is used in the second case but should n't the keys compare unequal ? Note : uncommenting the comparison method gives worse results but still the times are at one half : As expected built in ( address comparison ) is faster.EDIT : here are the profiling results from my original code that triggered the question - without the key method : and here are the results when I specify the key : Apparently it is the __cmp__ and not the __eq__ method that is called ( edit cause that class defines __cmp__ but not __eq__ , see here for the order of resolution of equal and compare ) .In the code here __eq__ method is indeed called ( 8605 times ) as seen by adding debug prints ( see the comments ) .So the difference is as stated in the answer by @ chepner . The last thing I am not quite clear on is why are those tuple equality calls needed ( IOW why we need to call eq and we do n't call cmp directly ) .FINAL EDIT : I asked this last point here : Why in comparing python tuples of objects is __eq__ and then __cmp__ called ? - turns out it 's an optimization , tuple 's comparison calls __eq__ in the tuple elements , and only call cmp for not eq tuple elements . So this is now perfectly clear . I thought it called directly __cmp__ so initially it seemed to me that specifying the key is just unneeded and after Chepner 's answer I was still not getting where the equal calls come in.Gist : https : //gist.github.com/Utumno/f3d25e0fe4bd0f43ceb9178a60181a53"
"Hello I 'm trying to simulate a mouse click while holding the SHIFT key . I have been trying to do this with the pynput module.This is my code so far : I know the code for holding the shift key is working ( If I try to press the `` a '' button in the code I see an `` A '' ) . Also I know the mouse click is working . However , together it does not work.Also I tried another code from a StackOverflow post : Pyautogui - Need to hold shift and clickI tried the following code from it : This worked for a minute then it stopped working ! Very strange . It fails like 9 out of 10 times ."
"I was just reading about the 'unexpected result of is operator ' which happens because Python cache numbers between -5 and 256.This was discussed here : '' is '' operator behaves unexpectedly with integersand here : '' is '' and `` id '' in Python 3.5When I run one of the examples given there , I get different results between Python Idle and Python IDE ( I 'm using Jetbrains Pycharm professional edition - 5.0.4 ) .When using Python IDLE this is the result : when using Pycharm 5.0.4 this is the result : how could this be ? I 've rechecked , and my project 's Python-Interpreter is exactly the same in both cases ( both are Python 3.5.1 ) .Not sure if this is something I 've done wrong , and I was hoping if someone could explain this.Edit : I know ' a ' is ' b ' == true iff id ( a ) == id ( b ) , and that you can check it like some of you mentioned in the comments . Perhaps I should have been more clear , what I do n't understand is how could it be that an IDE has different behavior ? I thought ( and please , correct me , as it seems I 'm wrong ) that an IDE is just a user-friendly environment that uses external compilers / interpreters , and this is why these are independent of those IDE 's ( for instance , pycharm supports not only Python , and I could run Eclipse with C compiler , or Java etc . ( all of which are not parts of the IDE ) .Thanks , Alon ."
Notice in the code below that foobar ( ) is called if any Exception is thrown . Is there a way to do this without using the same line in every Exception ?
"I have a C++ library ( we 'll call it Example in the following ) for which I wrote Python bindings using the boost.python library . This Python-wrapped library will be called pyExample . The entire project is built using CMake and the resulting Python-wrapped library is a file named libpyExample.so.When I use the Python bindings from a Python script located in the same directory as libpyExample.so , I simply have to write : and this executes a hello_world ( ) function exposed by the wrapping process.What I want to doFor convenience , I would like my pyExample library to be available from anywhere simply using the commandI also want pyExample to be easily installable in any virtualenv in just one command . So I thought a convenient process would be to use setuptools to make that happen . That would therefore imply : Making libpyExample.so visible for any Python scriptChanging the name under which the module is accessedI did find many things about compiling C++ extensions with setuptools , but nothing about packaging a pre-compiled C++ extension . Is what I want to do even possible ? What I do not want to doI do n't want to build the pyExample library with setuptools , I would like to avoid modifying the existing project too much . The CMake build is just fine , I can retrieve the libpyExample.so file very easily ."
"My current workflow is github PRs and Builds tested on Travis CI , with tox testing pytest and reporting coverage to codeclimate . travis.ymltox.iniHowever , Travis is n't passing my environmental variables for pull requests , which makes my coverage reporting fail . Travis documentation shows this as solution : However , in tox this does n't work as tox is using subprocess python module and does n't recognize if as a command ( naturally ) . How do I run codeclimate-test-reporter only for builds and not for pull requests based on the TRAVIS_PULL_REQUEST variable ? Do I have to create my own script and call that ? Is there a smarter solution ?"
"I am trying to make the following thing work but without success : I defined my own type Unit ( inherit from build-in type float ) to implement algebra for quantities with units . It does things in the way that : However , this __rmul__ is only invoked when float being the left operand . If I make something like this : Then Unit.__rmul__ will not be invoked , and the same numpy ndarray is returned since now Unit ( 'm/s ' ) was treated like a plain float with value 1.0What I expect is : after ndarray * Unit , a function similar to Unit.to can be attacted to the instance of ndarray as a method as well as an attribute unit , so I can further call ndarray.to to return a copy ( or modified version , if it could , for memory efficiency ) of the original ndarray that associated with new values and unit . How do I proceed ? According what I have known and searched , __mul__ of the left operand will be of the prior during * , i.e. , the interpretor checks LO.__mul__ ( ) first , if it fails , then goes to RO.__rmul__ ( ) . I do n't quite want to override numpy.ndarray.__mul__ because I really do n't know how complicated it would be , and whether there would be a big mess in case that it breaks the rules that ndarray acting on other objects.And , actually I even can not find where are the codes that defines __mul__ for ndarray . I simply used inspect.getsource ( np.ndarray ) but without success . Why does it fail on this ? the exception was barely an IOError.Thank you so much for your concern !"
"I have a list which contains the values : I have an if statement which will check if the values are contained within the list then output a statement : Considering the list does n't contain value `` 2 '' , it should not print the statement , however it is : Output : Can someone explain why this is the case ? Is it the way Python reads the list that is making this occur ?"
"I 'm appending values to a log file every 6th second . Every 30 sec I 'm transferring this log to a server as a file . But instead of transfering the whole file , I just want to append the collected data to the file on my server . I have n't been able to figure out how to open the server file and then append the values.My code so far : Instead , do I have to download the server file open and append , then send it back ? Do n't ask why I 'm not sending the logged data straight to a DB ; ) ABOVE THIS ROW WAS MY QUESTION , FULL SOLUTION BELOW"
"I 'm putting together a form using Flask & WTForms , however , when it comes to dropdowns I want to have a 'please select ' option for each dropdown , upon selected causes validation required to be false , e.g . a value has n't yet been selected.Do I need to use regex validation or custom validation to achieve this ? If I do need custom validation , then how do I go about constructing one ?"
"Given is an array a : With numpy , I can do the following : Resulting in : Assuming that a is instead a pytorch tensor , the following operation fails : The first parameter of div is expected to be a tensor of matching length/shape.If I substitute 1.0 with an array b filled with ones , its length equal to the length of a , it works . The downside is that I have to allocate memory for b. I can also do something like a = 1.0 / a which will yet again allocate extra ( temporary ) memory.How can I do this operation efficiently `` in-place '' ( without the allocation of extra memory ) , ideally with broadcasting ?"
I have used django-constance as a library.Although one thing I notice is that when I tried using ADMIN and MANAGERsending of emails is not working.In MANAGER I have tried this : still sending emails is not working . Am I missing a wrong implementation ? Or can you suggest any other library which can override ADMIN and MANAGER in settings.py . I am using Django 1.8.5 and Python 3.also when trying to import inside settings.py it produces error as well .
"I have defined a generator that yields log entries from Elasticsearch : How can I loop through two elements at the same time ? Something in the lines of : By two elements I mean this : A , B , B , C , ... , Y , Z ( for a generated list of A , B , ... , Y , Z ) ."
"I 'm writing app , which scans directory every second , checks for new files , and if they appeared - sends them via POST request and performs archiving . Assuming that number of files , which can appear in directory can be from 10 to 100 - I decided to use asyncio and aiohttp , to send requests concurrently.Code : So the question is : If i want to send requests concurrent , is this a good practice to add coroutines to loop like this : asyncio.ensure_future ( publish_file ( file ) ) ?"
"In pandas how to go from a : to b : I tried solutions in other answers , e.g . here and here but none seemed to do what I want.Basically , I want to swap rows with columns and drop the index , but how to do it ?"
"I wrote a python script trying to solve the 'calculate 24 ' problem , which originated from a game , drawing 4 cards from a deck of cards and try to get the value 24 using + , - , * , and /.The code is working , only that it have many duplications , for example , I input 2 , 3 , 4 , 5 to get the value of 24 , it will find and print that 2* ( 3 + 4 + 5 ) is 24 , but it will also print 2* ( 5 + 4 + 3 ) , 2* ( 5 + 3 + 4 ) , etc. , while it will find 4* ( 3 + 5 - 2 ) , it will also print 4* ( 5 + 3 - 2 ) . Could anyone please give me some hints on how to remove duplicated answers ? The code is as follows :"
"I checked other solutions to similar problems , but sqlite does not support row_number ( ) and rank ( ) functions or there are no examples which involve joining multiple tables , grouping them by multiple columns and returning only top N results for each group at the same time.Here 's the code i runNow the question is how do i get output like this in a single query ? I do n't need rows crossed with red lines since i only need top 2 ."
"I have two numpy masked arrays : If I try to divide x by y , the division operation is not actually performed when one of the operands is masked , so I do n't get a divide-by-zero error.This even works if I define my own division function , div : However , if I wrap my function with vectorize , the function is called on masked values and I get an error : Is there a way I can call a function with array arguments , and have the function only be executed when all of the arguments are unmasked ?"
"I 'm receiving a weird error , AttributeError : 'scoped_session ' object has no attribute '_autoflush ' , when attempting to execute a pre-built query on a scoped_session in the SQLAlchemy ORM.I 'm using a context manager to yield a scoped_session , and then am using it like so : This , however , raises the exception above.I note also that queries of the form session.query ( my_model ) work just fine.Where am I going wrong ? Many thanks !"
"I have a generator that I want to iterate through at two levels . The first level is unevenly spaced , then I want to chunk the next level into groups of 5 , say . I need it to be memory efficient and work on generator inputs , so I 'm doing something like the following . I have to think there may be a better way ? In particular , I do n't want the trailing Nones in the uneven length results ."
"I have a ForeignKey which can be null in my model to model a loose coupling between the models . It looks somewhat like that : On save the senders name is written to the sender_name attribute . Now , I want to be able to delete the User instance referenced by the sender and leave the message in place.Out of the box , this code always results in deleted messages as soon as I delete the User instance . So I thought a signal handler would be a good idea.Sadly , it is by no means a solution . Somehow Django first collects what it wants to delete and then fires the pre_delete handler . Any ideas ? Where is the knot in my brain ?"
"I am coding python in emacs . However , somehow the python interpreter running in emacs manages to surprise me.If I writein an emacs buffer , and tell emacs to start an interpreter and run the content of this buffer , I get a buffer containing ( Both __main__ and True are the outputs from the print statement ; the python buffer always displays the > > > and prints immediately after it . I am aware of this , this is not a problem . ) From the command line , both python and python -i show the 'indeed ' , as expected.How is Emacs able to the inconsistency of evaluating __name__=='__main__ ' to True , while not executing things inside if __name__ == '__main__ ' : ? And how do reconfigure it so it does not do so any more ?"
"Suppose i have one website with simple pages in php likeNow there is one page where i want some detailed functioning and i want to use python for that and it will look likeand in other page i want to use java likeProvided i have installed python , java on webserver.Is it possible ?"
"I am trying to clean up a html table using lxml.html.clean.Cleaner ( ) . I need to strip javascript attributes , but would like to preserve inline css style . I thought style=False is the default setup : However , when I call cleaner.clean_html ( doc ) will becomeBasically , style is not preserved . I tried to add : It wo n't help.Update : I am using Python 2.6.6 + lxml 3.2.4 on Dreamhost , and Python 2.7.5 + lxml 3.2.4 on local Macbook . Same results . Another thing : there is a javacript-related attribute in my html : < td style= '' cursor : pointer ; '' > Ticker < /td > Could it be lxml stripped this javacript related style and treated other styles the same ? Hope not.Thanks for any insights !"
"I find that in lots of different projects I 'm writing a lot of code where I need to evaluate a ( moderately complex , possibly costly-to-evaluate ) expression and then do something with it ( e.g . use it for string formatting ) , but only if the expression is True/non-None . For example in lots of places I end up doing something like the following : ... which I guess is basically a special-case of the more general problem of wanting to return some function of an expression , but only if that expression is True , i.e . : but without re-typing the expression ( or re-evaluating it , in case it 's a costly function call ) .Obviously the required logic can be achieved easily enough in various long-winded ways ( e.g . by splitting the expression into multiple statements and assigning the expression to a temporary variable ) , but that 's a bit grungy and since this seems like quite a generic problem , and since python is pretty cool ( especially for functional stuff ) I wondered if there 's a nice , elegant , concise way to do it ? My current best options are either defining a short-lived lambda to take care of it ( better than multiple statements , but a bit hard to read ) : or writing my own utility function like : ... but since I 'm doing this in lots of different code-bases I 'd much rather use a built-in library function or some clever python syntax if such a thing exists"
"I want to read a triangle of integer values from a file into a 2D array of ints using Python . The numbers would look like this:7595 6417 47 8218 35 87 1020 04 82 47 65 ... The code I have so far is as follows : I have a feeling this could be done in a tighter , more Pythonesque way . How would you do it ?"
"Complex question I assume , but studying OWL opened a new perspective to live , the universe and everything . I 'm going philosophical here.I am trying to achieve a class C which is subclass of B which in turn is subclass of C. Just for fun , you know ... So here it isclearly , python is smart and forbids this . However , in OWL it is possible to define two classes to be mutual subclasses . The question is : what is the mind boggling explanation why this is allowed in OWL ( which is not a programming language ) and disallowed in programming languages ?"
"Often , I would like to build up complex regexps from simpler ones . The only way I 'm currently aware of of doing this is through string operations , e.g . : Is anybody aware of a different method or a more systematic approach ( maybe a module ) in Python to have composable regexps ? I 'd rather compile each regexp individually ( e.g . for using individual compile options ) , but then there does n't seem to be a way of composing them anymore ! ?"
"Is there an analog of setattr ( ) that allows for appending an arbitrary list property of an instantiated class object ? If not , is there a recommended way of doing so ? This is a trivialized version of what I 'm doing currently : This seems clunky and inefficient.Edit : this assumes that foo.names ( or whatever arbitrary value might be assigned to the attr variable ) is in fact a list ."
"I am trying to test a webpage 's behaviour to requests from different referrers . I am doing the following so farThe problem is that the webpage has ajax requests which will change some things in the html , and those ajax requests should have as referer the webpage itself and not the referer i gave at the start . It seems that the referer is set once at the start and every subsequent request be it ajax or image or anchor takes that same referer and it never changes no matter how deep you browse , is there a solution to choocing the referer only for the first request and having it dynamic for the rest ? After some search i found this and i tried to achieve it through selenium , but i have not had any success yet with this : Any ideas ?"
"I 've been trying to use tornado-redis ( which is basically a fork of brükva slightly modified to work with tornado.gen interface instead of adisp ) in order to deliver events by using redis ' pubsub.So I wrote down a little script to test things out inspired by this example.Unfortunately , as I PUBLISHed through redis-climemory usage kept on rising.In order to profile memory usage I first tried to use guppy-pe but it wouldn ’ t work under python 2.7 ( Yes even tried trunk ) so I fell back to pympler.Now each time I PUBLISHed I could see that some objects were never released : Now that I know there 's really a memory leak , how do I track where those objects are created ? I guess I should start here ?"
"How to find the values of A , B , C and D ? I assumed the values would be integers and positive and did this : But that does not work . Even then I extend range to a = range ( -100,100 ) .After solving the equation by hand ( with Google 's help ) I know that floats are involved , e.g . A = 3.5 etc.But then how to solve it with Python ."
"I ’ m running filter on an interable and want to store the result in a sequence ( I need a sequence so that I can use random.choice on it ) . I noticed that creating a set from a filter object is a lot faster than creating a list or a tuple . Why is that ? I first though that the filter type is a subtype of set , which would explain this , but the filter function is actually identical to a generator expression , so it can not really be a set internally.I ran the following test to check the speed : And the results were speaking clearly for using a set : So why is creating a set from a filter so much faster ? Shouldn ’ t it usually take as long as creating a set from a sequence , where every element has to be hashed ? Or is it somehow getting a boost from the internal filter representation ? For comparison , when running the test on a range expression , set takes about twice as long as list and tuple ( which are both nearly identical in speed ) .edit : Sven ’ s answer is totally right , but for the completeness an updated test that will run on an actual filter : The result actually shows what makes more sense with list and tuple both being the fastest , although set isn ’ t really slower , so it won ’ t make any difference what to use :"
"In Python , dictionaries created for the instances of a class are tiny compared to the dictionaries created containing the same attributes of that class : When using Python 3.5.2 , the following calls to getsizeof produce:288 - 96 = 192 bytes saved ! Using Python 2.7.12 , though , on the other hand , the same calls return:0 bytes saved.In both cases , the dictionaries obviously have exactly the same contents : so this is n't a factor . Also , this also applies to Python 3 only.So , what 's going on here ? Why is the size of the __dict__ of an instance so tiny in Python 3 ?"
"I am creating a dictionary as follows : However my actual y contains about 40 million tuples , is there a way to use the multiprocessing to speed up this process ? Thanks"
"I 'm trying to make an accesible cache of user data using Pyramid doc 's `` Making A “ User Object ” Available as a Request Attribute '' example . They 're using this code to return a user object to set_request_property : I do n't understand why they 're using unauthenticated_userid ( request ) to lookup user info from the database ... is n't that insecure ? That means that user might not be logged in , so why are you using that ID to get there private info from the database ? Shouldn'tbe used instead to make sure the user is logged in ? What 's the advantage of using unauthenticated_userid ( request ) ? Please help me understand what 's going on here ."
"In python , is it possible to unpickle objects that were pickled as old-style python classes after being converted to new-style ones ? ( Ie , objects with different `` class signatures '' ) . * For example , imagine some instances were saved as : and then a few more objects were pickled after the class was changed to inherit from object : Objects that are pickle.dump'ed with one can be pickle.load'ed by the same style class , but neither one will load both . I think that the strategy used by pickle changes based on the inheritance ( the class that inherits from object has a __reduce__ method automatically defined , but the one without inheritance does n't ) . When trying to load a with-inheritance old-style pickle from code without inheritance ( old-style definition ) , I get the same argument error as seen in this SO question ; even though the second argument is redundant , it still changes the `` class signature '' and expected arguments , and prevents loading . To solve this , I 'd be happy to write an unpickler , although I 'm afraid it may involve two separate sub-classed unpicklers a la the docs ... If I knew how to correctly unpickle each one I 'd be fine doing that.A little background on my situation ... I 'm using a TrialHandler class to save and reload button presses and reaction times for behavioral psychology experiments . We refactored the TrialHandler class to inherit from a more abstract , baseTrialHandler , but in doing so temporarily changed the class signature to inherit from object . However , we could n't unpickle older trial_handler files , so it was changed back . I 'd like to look at data from the same experiment that was run with both versions of the trial handler , and so want to unpickle both types of saved log file in the same script.Alternatively , if I ca n't write a custom unpickler that will unpickle both objects , is there another way to serialize them ? I tried dumping straight to yaml , but it looks like I 'll have to register the class as something that can be yaml'ized , anyway . A full description of the problem with specific errors is on the PsychoPy mailing list . Any blog posts explaining the details intermediate pickling , or even python inheritance , would be most welcome ; the closest I found was a good explanation of why pickling is insecure describing pickling as a `` simple stack-based virtual machine '' , which is nice , but does n't get me enough to figure out even the basics of unpicking myself ."
"I have created the following class using pybind11 : However I have no idea how I would call this constructor in Python.. I see that Python expects a float in the place of the double* , but I can not seem to call it.I have tried , ctypes.data_as ( ctypes.POINTER ( ctypes.c_double ) ) but this does not work ... Edit : I have distilled the answer from @ Sergei answer ."
I 'm wondering if there 's an easy way to do the following in Python 3.x . Say I have two lists structured as follows : What 's the simplest way to produce a generator ( here represented by calling a function funky_zip ) that would let me iterate through these two lists like so : I could just dobut I 'm wondering if there 's a nice way to do this without having to unpack the tuples . Thanks !
"I 'm trying to compile the Open Kinect drivers for Python on Windows I made a Make file with CMake ( link ) and tried compiling it with VC++ Express.Everything seems to compile alright ( the viewer sample works ) , except for the Python wrapper . I changed the output folder of freenect , which places freenect.lib in another folder to prevent a collision . When I changed this it and try to compile cython_freenect I get : It 's strange that it ca n't find the library , because it just created it and I manually linked it to the library.Any suggestions as to what I 'm doing wrong ? As far as I know I followed all the steps in the Python Wrapper for Windows guide ."
"I 've got the following code to run a continuous loop to fetch some content from a website : The contents are correctly read . But for some reason , the TCP connections wo n't close . I 'm observing the active connection count from a dd-wrt router interface , and it goes up consistently . If the script continue to run , it 'll exhaust the 4096 connection limit of the router . When this happens , the script simply enter waiting state ( the router wo n't allow new connections , but timeout has n't hit yet ) . After couple minutes , those connections will be closed and the script can resume again.I was able to observe the state of those hanging connections from the router . They share the same state : TIME_WAIT . I 'm expecting this script to use no more than 1 TCP connection simultaneously . What am I doing wrong ? I 'm using Python 3.4.2 on Mac OS X 10.10 ."
"I have two curves that look like this : I 'm looking for a way to smoothly connect the blue curve with the red one by extending the former ( blue line ) upwards and to the right , while leaving the latter ( red line ) untouched . The direction is important , I mention this because it looks as if it would be easier to continue the blue line to the left . I ca n't do this ( it would n't make sense in my larger code ) so it has to be upwards and to the right.Here 's what I 've managed to do so far ( the section where the two lines get close is zoomed in ) : Basically I 'm interpolating a new curve using a sample of points from both curves ( the black dots ) The MWE code to obtain this plot is below.What I need to do now is find a way to trim the green line from the point where it meets the red line to the point where it meets the blue line and extend the blue line replacing the little last segment that is now no longer necessary.This is how the blue line should look after the changes above are applied ( made by hand ) : where the trimmed section of the green line is now a part of the blue line . Notice that I have : discarded the extra points of the green line that extend beyond the intersection with the red linediscarded the extra points of the green line that extend beyond the intersection with the blue line.attached the remaining section of the green line to the blue line after discarding the portion of the blue line that extended to the left beyond the intersection of the green and blue lines.Since I already have the interpolating curve ( the green line ) , all I need is a way to : Trim it up to the points where it intersects the other two curves as explained above.Replace the last portion of the blue line with this trimmed portion of the new interpolated line.In this particular example I 've used fixed lists to plot and perform the calculations , but I have several pairs of lines for which I need to perform a similar operation , so the solution would have to be general enough to contemplate cases with similarly shaped but different curves . How could I do this ? I 'm open to solutions making use of numpy , scipy or whatever is necessary.Here 's the MWE :"
"I 'm writing some unittests for some code that uses SQLAlchemy . I want to test filter calls , but it seems that SQLAlchemy BinaryExpression objects created with the same arguments do n't compare equal : I suppose I can cast them both to strings and compare those , but that seems hacky , and I 'd really rather not be forced to try to debug string comparisons if I do n't have to . Are there any better/more-structured ways to compare BinaryExpressions in unittests ?"
"Update : What I really wanted all along were greenlets.Note : This question mutated a bit as people answered and forced me to `` raise the stakes '' , as my trivial examples had trivial simplifications ; rather than continue to mutate it here , I will repose the question when I have it clearer in my head , as per Alex 's suggestion.Python generators are a thing of beauty , but how can I easily break one down into modules ( structured programming ) ? I effectively want PEP 380 , or at least something comparable in syntax burden , but in existing Python ( e.g . 2.6 ) As an ( admittedly stupid ) example , take the following : Being an ardent believer in DRY , I spot the repeated pattern here and factor it out into a method : ... which of course does n't work . The parent must call the new function in a loop , yielding the results : ... which is even longer than before ! If I want to push part of a generator into a function , I always need this rather verbose , two-line wrapper to call it . It gets worse if I want to support send ( ) : And that 's not taking into account passing in exceptions . The same boilerplate every time ! Yet one can not apply DRY and factor this identical code into a function , because ... you 'd need the boilerplate to call it ! What I want is something like : Does anyone have a solution to this problem ? I have a first-pass attempt , but I 'd like to know what others have come up with . Ultimately , any solution will have to tackle examples where the main generator performs complex logic based on the result of data sent into the generator , and potentially makes a very large number of calls to sub-generators : my use-case is generators used to implement long-running , complex state machines ."
"My website has an AJAX POST view that can be called from any page on the app ( event tracking ) . This view is protected by CSRF . In some cases , the CSRF cookie is not set , and the POST call fails.Instead of manually decorating all views with @ ensure_csrf_cookie , I 'm thinking of writing I created a middleware that enforces Django to set the CSRF cookie on all responses . Is this approach correct ? Does it create a security flaw I 'm not aware of ? Update : here is the middleware code :"
"I know that the reccomended way to install Zope is with Buildout , but I ca n't seem to find a simple buildout.cfg to install a minimal Zope 2 environment . There are lots to install Plone and other things.I 've tried : But I get :"
"I am trying to access a USB device which appears in /dev/ as hidraw4.I have the specification of the communication interface ( based on RS232 serial port communication ) , but I can not even got the connection with pyserial to work : Code : Error : I do not really understand what is the difference between the hidraw and tty files that I can found in /dev/ . Can Pyserial connect to such devices ( and if not , is there a workaround ) ?"
"OVERVIEWRight now I got these 2 programs on my windows taskbar : SublimeText3 target : VS2015 x64 Native Tools Command Prompt target : Goal here is running Sublime Text with vs2015 environment enabled.One option would be open the vs command prompt and then run sublime text from there , > sublime_text ( this is not good one , I want it to be a non-interactive process ) Another option would be modifying somehow the sublimetext symlink target from the taskbar so I could open sublime with vs2015 environment enabled just clicking the iconQUESTIONHow could I acomplish option 2 ? NS : I want to get Sublime Text 3 to run vcvarsall.bat properly only once at startup ( not at build time on any build system ) ATTEMPTSMy first attempt was trying to understand how bat files executed so I testedsome basic batch files : bat1.bat : It opens sublime text succesfullybat2.bat : It opens vs command prompt succesfully and it waits for user to type commandsbat3.bat : Open vs command prompt but it wo n't open ST , unless you type exit once the command prompt is shownbat4.bat : Open vs command prompt but it does n't open ST , same case than bat3.batbat5.bat : Open vs command prompt but it does n't open ST , same case than bat { 4,3 } .batAfter these attempts I 've decided to read some docs trying to find some hints about cmd but it did n't make any difference.Another idea was using conemu customized tasks , something like : and then having a bat file calling conemu like this : the result was +/- what I wanted , but 2 terminals and a new sublime session would be spawned . What I 'm looking for is just opening a SublimeText session with the right environment , so I consider this not a good solution , plus , it requires to have conemu installed.After all those attempts I thought maybe using python to open a command prompt and typing & running `` magically '' some commands could be helpful but I do n't know how to do it.Conclusion : Till the date the problem remains unsolved ..."
"I am trying to figure out how to configure the base url of and IPython notebook server running . So instead of the default : I want to configure all requests so that the go through ipython , as in : Is this possible ?"
I want to make a set of tuples in which the order of tuples should n't matter.For eg.- If the tuples I want to add is : It should output like this : Is there any efficient way of doing this in python ?
"SOLVED : Rebooting the machine appears to have removed the issue . I will update if the problem returns.I 'm having an issue where Python2.6 hangs after an exception is raised , specifically when foo.py is called with an absolute path ( /home/user/bar/foo.py ) . I am then required to ctrl+c out of the program . If called from within the bar directory as ./foo.py or from the root directory as ./home/user/bar/foo.py , the program terminates correctly.foo.py : orI may also mention that a sys.exit ( ) works fine , without issues.What is happening to the exception that is failing to terminate the program ? This is likely specific to my configuration . Where should I begin looking for a solution ? EDIT : execfile ( '/home/user/bar/foo.py ' ) works fine if running interactive mode . Additionally , running nohup /home/user/bar/foo.py & results in a hanging process that must be killed.Running CentOS release 6.3 ( Final ) . This issue did not always exist . This only started about a month ago over a weekend ( I was not using the machine at that time ) .UPDATE : Debugging with GDB , the backtrace points to libpthread.so.0 . Anybody know what this means ?"
"I have a highly imbalanced dataset and I want to perform a binary classification . When reading some posts I found that sklearn provides class_weight= '' balanced '' for imbalanced datasets . So , my classifier code is as follows.Then I performed 10 fold cross validation as follows using the above classifier.However , I am not sure if class_weight= '' balanced '' is reflected through 10-fold cross validation . Am I doing it wrong ? If so , is there any better way of doing this in sklearn ? I am happy to provide more details if needed ."
"I 'm trying to write metadata to a pdf file using the following python code : This appears to work fine ( no errors to the consoled ) , however when I examine the metadata of the file it is as follows : and the original file had the following metadata : So the problems are , it is not appending the metadata , and it is clearing the previous metadata structure . What do I need to do to get this to work ? My objective is to append metadata that reference management systems can import ."
"I 'm trying to create an abstract enum ( Flag actually ) with an abstract method.My final goal is to be able to create a string representation of compound enums , based on the basic enums I defined.I 'm able to get this functionality without making the class abstract.This is the basic Flag class and an example implementation : An example usage is : Switching to TranslateableFlag ( Flag , ABC ) fails due to MetaClass conflicts . ( I did n't understand this post - Abstract Enum Class using ABCMeta and EnumMeta , so I 'm not sure if it 's answering my question ) .I would like get a functionality like this somehow : Is it possible to achieve this ?"
"I have a dataframe which you can build with this : And looks like this : I want to check if the item in column `` check '' is in the list in column `` checklist '' . So I want the resulting dataframe to look like : I have tried several things including using .isin in various forms including apply/lambda . and directly . This : produces : which has two Falses.Trying this : df [ 'checkisin ' ] =df.apply ( lambda x : x.check.isin ( x.checklist ) ) gives this error : Trying this : gives this error : I 'm sure I 'm missing something simple here . I know I could loop this , but looking for a Pandas Dataframe column wise solution as the DF I have is very large and trying to `` most '' efficiently handle . Thanks !"
"I 'm trying to do the following , and repeat until convergence : where each Xi is n x p , and there are r of them in an r x n x p array called samples . U is n x n , V is p x p. ( I 'm getting the MLE of a matrix normal distribution . ) The sizes are all potentially large-ish ; I 'm expecting things at least on the order of r = 200 , n = 1000 , p = 1000.My current code doesThis works okay , but of course you 're never supposed to actually find the inverse and multiply stuff by it . It 'd also be good if I could somehow exploit the fact that U and V are symmetric and positive-definite.I 'd love to be able to just calculate the Cholesky factor of U and V in the iteration , but I do n't know how to do that because of the sum.I could avoid the inverse by doing something like ( or something similar that exploited the psd-ness ) , but then there 's a Python loop , and that makes the numpy fairies cry.I could also imagine reshaping samples in such a way that I could get an array of A^-1 x using solve for every x without having to do a Python loop , but that makes a big auxiliary array that 's a waste of memory.Is there some linear algebra or numpy trick I can do to get the best of all three : no explicit inverses , no Python looping , and no big aux arrays ? Or is my best bet implementing the one with a Python loop in a faster language and calling out to it ? ( Just porting it directly to Cython might help , but would still involve a lot of Python method calls ; but maybe it would n't be too much trouble to make the relevant blas/lapack routines directly without too much trouble . ) ( As it turns out , I do n't actually need the matrices U and V in the end - just their determinants , or actually just the determinant of their Kronecker product . So if anyone has a clever idea for how to do less work and still get the determinants out , that would be much appreciated . )"
"I am using Django and deploy my stack with Ansible . Finally , I am using Fabric to deploy my Django project pulling my code from GitHub . My question : What is the best practice to deal with private settings in Django 's settings.py file , such as passwords for email or S3 ? Currently , I file-transfer a settings_production.py from my machine to the production machine at the end of my deployment script before restarting the application server . This file contains the settings that I am not putting into settings.py as part of the repo.At the end of my settings.py I am adding something likeAre there better ways to do this ?"
"I 'm trying to maximize pt.show ( ) on Mac . I 'm using Python.mac OS X ==10.10.3 , python==3.4.2matplotlib==1.4.3I have already tried the following :"
I 'm trying to evaluate polynomial ( 3 'd degree ) using numpy.I found that doing it by simpler python code will be much more efficient.Did I miss something ? Is there another way in numpy to evaluate a polynomial ?
"It is common that people have trouble setting up django to be serves by apache and mod-wsgi . The common symptom is `` Import error '' ... for some reason ( generally slightly different in each case ) settings.py or similar does n't import ( see `` Related '' in the right column on this page for numerous examples ! ) .I have read through the other questions on this topic , and none appear to have a solution that works for my situation ( one was a basic misconfig by the poster identified by an answerer - I do n't appear to have this issue , others apply to using wsgi from other modules etc ) . When apache/mod-wsgi fails to load , how can you debug it ? What can you do to make something give you a better message than `` Import error '' ? Obviously , I 'm looking for a method that will identify what 's wrong in my case . But I really would like to know how to approach debugging this sort of failure : there seems to be no way that I can find to get information about what it is causing it to fail.In my case , I am trying to do what appears to be a straightforwards django deploy with mod-wsgi - really by the book , the same as the doc , but get the error : I ca n't see why it ca n't find this module . /home/cmc/src/cm_central/cm_central/settings.py exists , can be loaded by pythonn without error , and in fact works OK with ./manage.py runserver.Is it possible that there is some import error occurring in the context of apache , that does n't occur when I load it myself ? I 'm wondering because of the words `` Is there an import error in the settings file ? '' ... why does it ask that ? If there were an import error , how would I debug it ? I have this in /etc/apache2/sites-enabled/cm-central : And this in wsgi.py : ( it I have n't modified it from what django generated ) The full traceback is : If an answer could help me spot what 's wrong , that 'd be great - but even better is an answer to `` how to debug this '' . How does one find out why the settings.py wo n't load ?"
"I 'm working on appengine-mapreduce function and have modified the demo to fit my purpose.Basically I have a million over lines in the following format : userid , time1 , time2 . My purpose is to find the difference between time1 and time2 for each userid . However , as I run this on Google App Engine , I encountered this error message in the logs section : Exceeded soft private memory limit with 180.56 MB after servicing 130 requests totalWhile handling this request , the process that handled this request was found to be using too much memory and was terminated . This is likely to cause a new process to be used for the next request to your application . If you see this message frequently , you may have a memory leak in your application.Can anyone suggest how else I can optimize my code better ? Thanks ! ! Edited : Here 's the pipeline handler : Mapreduce.yaml : The rest of the files are exactly the same as the demo.I 've uploaded a copy of my codes on dropbox : http : //dl.dropbox.com/u/4288806/demo % 20compressed % 20fail % 20memory.zip"
"In python we can say : and similarly , we can overload the comparision operators like : but what methods of the types of the operands of those interval comparisions are actually called ? is the above equivalent to Edit : re S.Lott , Here 's some output that helps to illustrate what actually happens ."
"So i have a relatively convoluted setup for something I 'm working on explained as follows : This is is python . and more of a rough outline , but it covers everything I need . Though the process next function is the same so feel free to clean that up if you want.That is a barebones of what I have , but it illustrates my problem : Doing the belowI initially expected my output to be 36 6 * ( 4 + 2 ) I know why its not , foo1.value and foo1.return_bah ( ) gets passed as an evaluated parameter ( correct term ? ) .What I really want is to pass the reference to the variable or the reference to the method , rather than having it evaluate when I put it in my event queue.Can anyone help me.I tried searching , but I could n't piece together what I wanted exactly.TO get what I have now I initially looked at these threads : Calling a function of a module from a string with the function 's name in PythonUse a string to call function in PythonBut I do n't see how to support parameters from that properly or how to support passing another function or reference to a variable from those.I suppose at least for the method call , I could perhaps pass the parameter as foo1.return.bah and evaluate in the process_next method , but I was hoping for a general way that would accept both standard variables and method calls , as the event_queue will take both.Thank you for the helpUpdate edit : So I following the suggestion below , and got really close , but : Ok , so I followed your queue suggestion and got really close to what I want , but I do n't completely understand the first part about multiple functions.I want to be able to call a dictionary of objects with this as well.for example : Then when attempting to push via lambdadoes n't work . When teh event actually gets processed it only runs on whatever name_objs [ name ] references , and if the name variable is no longer valid or has been modified outside the function , it is wrong.This actually was n't surprising , but adding a : then pushing that did n't either . it again only operates on whatever name_obj_hold last referenced.Can someone clarify the multiple funcs thing . I 'm afraid I 'm having trouble wrapping my head around it.basically I need the initial method call evaluated , so something like : names_objs [ name ] .some_func ( # something in here # ) gets the proper method and associated with the right class object instance , but the # something in here # does n't get evaluated ( whether it is a variable or another function ) until it actually gets called from the event queue ."
"I 'll do my best to provide a reproducible example here.I have an image : This image of Aaron Eckhart is ( 150 , 150 ) My goal is to perturb a ROI of this image by doing mathematical operations on the pixels , however , the issue is that the math must be done as a tensorflow tensor because the mathematical operation to be done is to multiply the tensor by it 's scaled gradient ( which is also a tensor of size ( row_pixels , column_pixels , 3 ) ) So here 's the process I imagine : Read in image as numpy array RGB size : ( 1 , 150 , 150 , 3 ) ( 1 is batchsize ) w , h = img.shaperet = np.empty ( ( w , h , 3 ) , dtype=np.uint8 ) ret [ : , : , 0 ] = ret [ : , : , 1 ] = ret [ : , : , 2 ] = imgMake pixel values between 0 and 1img = ( faces1 - min_pixel ) / ( max_pixel - min_pixel ) for i in range ( steps ) : ( a ) extract ROI of image this is the part I do n't understand how to do ( b ) calculate gradient of smaller img ROI tensor 's loss ( c ) multiply img ROI tensor by gradient of loss ( d ) place this newly perturbed ROI tensor back into the same positions it was in the original tensor this is the other part I do n't understand how to do This will result in an image where only some of the pixel values have been perturbed and the rest remain the same"
"I have to send a lot of HTTP requests , once all of them have returned , the program can continue . Sounds like a perfect match for asyncio . A bit naively , I wrapped my calls to requests in an async function and gave them to asyncio . This does n't work . After searching online , I found two solutions : use a library like aiohttp , which is made to work with asynciowrap the blocking code in a call to run_in_executorTo understand this better , I wrote a small benchmark . The server-side is a flask program that waits 0.1 seconds before answering a request.The client is my benchmarkSo , an intuitive implementation with asyncio does n't deal with blocking io code . But if you use asyncio correctly , it is just as fast as the special aiohttp framework . The docs for coroutines and tasks do n't really mention this . Only if you read up on the loop.run_in_executor ( ) , it says : I was surprised by this behaviour . The purpose of asyncio is to speed up blocking io calls . Why is an additional wrapper , run_in_executor , necessary to do this ? The whole selling point of aiohttp seems to be support for asyncio . But as far as I can see , the requests module works perfectly - as long as you wrap it in an executor . Is there a reason to avoid wrapping something in an executor ?"
"I have a pandas dataframe , and I created a function . I would like to apply this function to each row of the dataframe . However the function has a third parameter that does not come from the dataframe and is constant so to say.I think I have to use the apply function but I do n't see how I would pass this constant argument ."
"I am having a very complicated tests.py file.Actually the tests classes and methods are generated at run time w/ type ( to account for data listed in auxiliary files ) . I am doing things in the following fashion ( see below for more code ) : FYI , with the usual django test runner , all those tests get run when doing ./manage.py test myapp ( thanks to the setattr shown above ) .What I want to do is run only part of those tests , without listing their names by hand.For example , I could give each test `` tags '' in the class names or method names so that I could filter on them . For example I would then perform : run all tests which method name contains the string `` test_postgres_backend_ '' I tried using django-nose because of nose 's -m option , which should be able to select tests based on regular expressions , an ideal solution to my problem.Unfortunately , here is what is happening when using django-nose as the django test runner : ./manage.py test myapp is not finding automatically the type-generated test classes ( contrarily to the django test runner ) neither ./manage.py test -m `` . * '' myapp nor ./manage.py test myapp -m `` . * '' find ANY test , even if normal TestCase classes are present in the fileSo : Do you have another kind of solution to my general problem , rather than trying to use django-nose -m ? With django-nose , do you know how to make the -m work ? mcveAdd the following to an empty myapp/tests.py file : If makes for this output run ( in alphab order ) by django test runnner : F.F.F.F.F.F.FF.F.F.F.F..F.F.F.F.F.FF.F.F.F.F..F.F.F.F.F.FF.F.F.F.F..F.F.F.F.F.FF.F.F.F.F..F.F.F.F.F..If you change to django_nose test runner , nothing happens on ./manage.py test myapp.After fixing this , I would then like would be able to run only the test methods which name end with a 0 ( or some other kind of regexable filtering )"
"I understand that del d [ key ] will delete the key-value pair , whereas d [ key ] =None only de-references the value.However , in terms of memory management , is there any difference ? Does setting a value None trigger garbage collection immediately , assuming that there is no other variable referencing this value ? I ran a little experiment : Not sure if the approach is valid , but it seems no difference in terms of the size of the dictionary at all . I must miss something here ."
"I 've configured ( almost default ) supervisord.conf and started supervisord . Tasks launched and xmlrpc interfaces are up , but givesxmlrpclib.Fault : < Fault 1 : 'UNKNOWN_METHOD ' > on evey xmlrpc request , even when launching supervisorctl itself.There is a same message in the log :"
"I am new to Python and I wonder if there is any way to aggregate methods into 'subspaces ' . I mean something similar to this syntax : I am writing an API wrapper and I 'm going to have a lot of very similar methods ( only different URI ) so I though it would be good to place them in a few subspaces that refer to the API requests categories . In other words , I want to create namespaces inside a class . I do n't know if this is even possible in Python and have know idea what to look for in Google.I will appreciate any help ."
"I benchmarked these two functions ( they unzip pairs back into source lists , came from here ) : Results with timeit.timeit ( five rounds , numbers are seconds ) : So clearly f1 is a lot faster than f2 , right ? But then I also measured with timeit.default_timer and got a completely different picture : So clearly f2 is a lot faster , right ? Sigh . Why do the timings totally differ like that , and which timing method should I believe ? Full benchmark code :"
I have some Python classes with class attributes and metaclass : I would like to use dataclasses to improve the code : how to deal with the class properties c_type and c_brand ? I am thinking of the following which seems to fill my need : Is there a more appropriate approach ?
"Suppose I have the list of dictionary dataset like this , I need to iterate the list of dictionary and put the keys as column headers and its values as the rows and write it to the CSV file.This is what I triedFor brevity , I have reduced the keys to 2 and list of values to 3 . How to approach this problem ? Thanks"
"Perl has a lovely little utility called find2perl that will translate ( quite faithfully ) a command line for the Unix find utility into a Perl script to do the same . If you have a find command like this : It finds all the directories ending in share below /usrNow run find2perl /usr -xdev -type d -name '*share ' and it will emit a Perl script to do the same . You can then modify the script to your use.Python has os.walk ( ) which certainly has the needed functionality , recursive directory listing , but there are big differences.Take the simple case of find . -type f -print to find and print all files under the current directory . A naïve implementation using os.walk ( ) would be : However , this will produce different results than typing find . -type f -print in the shell . I have also been testing various os.walk ( ) loops against : The difference is that os.walk ( ) counts links as files ; find skips these . So a correct implementation that is the same as file . -type f -print becomes : Since there are hundreds of permutations of find primaries and different side effects , this becomes time consuming to test every variant . Since find is the gold standard in the POSIX world on how to count files in a tree , doing it the same way in Python is important to me.So is there an equivalent of find2perl that can be used for Python ? So far I have just been using find2perl and then manually translating the Perl code . This is hard because the Perl file test operators are different than the Python file tests in os.path at times ."
"I have to create a data structure to store distances from each point to every other point in a very large array of 2d-coordinates . It 's easy to implement for small arrays , but beyond about 50,000 points I start running into memory issues -- not surprising , given that I 'm creating an n x n matrix.Here 's a simple example which works fine : cdist is fast , but is inefficient in storage since the matrix is mirrored diagonally ( e.g . d [ i ] [ j ] == d [ j ] [ i ] ) . I can use np.triu ( d ) to convert to upper triangular , but the resulting square matrix still takes the same memory . I also do n't need distances beyond a certain cutoff , so that can be helpful . The next step is to convert to a sparse matrix to save memory : The problem is getting to that sparse matrix quickly for a very large dataset . To reiterate , making a square matrix with cdist is the fastest way I know of to calculate distances between points , but the intermediate square matrix runs out of memory . I could break it down into more manageable chunks of rows , but then that slows things down a lot . I feel like I 'm missing some obvious easy way to go directly to a sparse matrix from cdist ."
"Suppose I have a Python class that I want to add an extra property to.Is there any difference between and using something like : ? If not , why do people seem to do the second rather than the first ? ( Eg . here http : //concisionandconcinnity.blogspot.com/2008/10/chaining-monkey-patches-in-python.html )"
I have a loop condition : but only want to use the first 10 elements of contents . What is the most pythonic way to do so ? Doing a : seems somewhat un-Pythonic . Is there a better way ?
"I used the pelican-quickstart to create a static website and this comes with a default pelicanconf and publishconf . I have a GOOGLE_ANALYTICS variable in my publishconf , but when I publish my static page in Github Pages , using this snippet : , _setAccount becomes empty string.Should I move GOOGLE_ANALYTICS from publishconf to pelicanconf ? What 's the difference between them ?"
"The argparse package does a great job when dealing with command line arguments . I 'm wondering however if there is any way to ask argparse to check for file extension ( e.g `` .txt '' ) . The idea would be to derived one the class related to argparse.FileType . I would be interested in any suggestion . Keep in mind that I have more than 50 subcommands in my program all having there own CLI . Thus , I would be interest in deriving a class that could be imported in each of them more than adding some uggly tests in all my commands . Thanks a lot ."
I am trying to find a better way to implement this : I am learning functional programming so I am just trying random example that come to my head : )
"I originally asked this question on the Python capi-sig list : How to pass arguments to tp_new and tp_init from subtypes ? I 'm reading the Python PEP-253 on subtyping and there are plenty of good recommendations on how to structure the types , call tp_new and tp_init slots , etc.But , it lacks an important note on passing arguments from sub to super type.It seems the PEP-253 is unfinished as per the note : ( XXX There should be a paragraph or two about argument passing here . ) So , I 'm trying to extrapolate some strategies well known from the Python classes subtyping , especially techniques that each level strips-off arguments , etc.I 'm looking for techniques to achieve similar effect to this , but using plain Python C API ( 3.x ) : What would be the equivalent in Python C API ? How to deal with similar situation but with arguments specific to derived class expected in different order ? It is arguments given at the end of the args tuple ( or kwds dict , I assume principle would be same ) .Here is some ( pseudo- ) code that illustrates the situation : Note , if the a was expected first : it would be similar situation to the Shape and ColoredShape above.It would also be easier to deal with , I assume.Could anyone help to figure out the missing XXX comment mentioned above and correct technique for passing arguments from subtype up to super type ( s ) on construction ? UPDATE 2012-07-17 : Inspired by ecatmur 's answer below I looked through Python 3 sources and I found defdict_init constructor of collections.defaultdict type object interesting . The type is derived from PyDictObject and its constructor takes additional argument of default_factory . The constructor signature in Python class is this : Now , here is how the default_factory is stripped from original args tuple , so the rest of arguments is forwarded to the tp_init of base type , it is PyDictObject : Note , this snipped presence only the relevant part of the defdict_init function ."
"In 64 bit Python 2.7.6 this is True , however in 32 bit Python 2.7.3 it is False : So how does Python 2.7.3 hash strings used to seed random number generators ?"
"I ran into a somewhat complicated XPath problem . Consider this HTML of part of a web page ( I used Imgur and replaced some text ) : I first want to search all img tags in the document and finding their corresponding srces . Next , I want to check if the img src link contains an image file extension ( .jpeg , .jpg , .gif , .png ) . If it does n't contain an image extension , do n't grab it . In this case it has an image extension . Now we want to figure out which link we want to grab . Since the parent href exists , we should grab the corresponding link.Desired Result : //i.imgur.com/ahreflink.jpgBut now let 's say the parent href does n't exist : Desired Result : //i.imgur.com/imgsrclink.jpgHow do I go about constructing this XPath ? If it helps , I am also using Python ( Scrapy ) with XPath . So if the problem needs to be separated out , Python can be used as well ."
I have a dict in this format : and I have to iterate over it 's items : except for the pair :
"I have an application with a heirarchy of packages . There are a fair number of modules that reference other modules higher up in the package heirarchy . As exemplified below , I can use relative imports to solve this problem . However , running the module directly for testing fails with an Attempted relative import in non-package exception.Is there a better way to organize my application or my import statements so that modules can both be executed individually for testing and imported from other modules ? Examplebase.pychild.pyuser.pyExisting solutionI 've found that I can add the following header added to the top of a module that needs to reference modules higher in the hierarchy : The downside is that I need to add this code all over the place , and it is n't constant : the '.. ' relative path varies based on the depth of the package in the hierarchy.Other possible solutions ... I could create a .pth file with my application root . Unfortunately , this must be added to the site-packages folder of my Python installation , so it 's not very application-specific.I could temporarily add my application root to the PYTHONPATH , and make all my modules reference packages from the root . This eliminates the simplicity of running a script by simply calling python.exe whatever.py , though.I could write some sort of module that , when imported , would look for a marker file in the root folder of my application . This generic module could then be installed to my local Python installation and imported by every module in my app , thus guaranteeing that the root is in PYTHONPATH , regardless of which module I 'm executing ."
"We can obtain test_client for sample application in way like : However , if we wrap app with DispatcherMiddleware - we will get error like AttributeError : 'DispatcherMiddleware ' object has no attribute 'test_client'.Are there way to test composition of flask applications ? We want to be able to do something like : When all_apps is middleware like :"
"I am new on python ( and even programing ! ) , so I will try to be as clear as I can to explain my question . For you guys it could be easy , but I have not found a satisfactory result on this yet.Here is the problem : I have an array with both negative and positive values , say : I would like to sum ONLY the negative values that are continuous , i.e . only sum ( -1 , -6 , -6 ) , sum ( -5 , -2 , -1 , -4 ) and so on . I have tried using numpy.where , as well as numpy.split based on the condition.For example : However , as you can expect , I just got the summation of all negative values in the array instead . In this case sum ( -1 , -6 , -6 , -5 , -5 , -2 , -1 , -4 ) Could guys share with me an aesthetic and efficient way to solve this problem ? I will appreciate any response on this.Thank you very much"
"I want to figure out the type of the class in which a certain method is defined ( in essence , the enclosing static scope of the method ) , from within the method itself , and without specifying it explicitly , e.g.Is this possible ?"
"I was trying to implement a `` download link '' and put it beside one of my report table so that users can download a csv file and open it with applications like Excel.The records are generated dynamically based on the query made by users.So somewhere in my controller there 's something like : This works in both FireFox & Chrome , but fails in IE.When I print out the response headers , I found that several headers were added to my response by web2py : 'Expires ' , 'Cache-Control ' , etc ... And when I remove the 'Cache-Control ' header by doing the following : It works in IE.So it seems like IE has trouble dealing with a downloadable file with 'Cache-Control ' set to certain value.Now , my question is : Why does web2py add these response headers , implicitly ? and maybe without a way to set it off ? is there any side effect when i delete the 'Cache-Control ' header this way ? Thanks in advance ."
"I 've been trying to dump a relatively small amount of data ( 80 rows or so of django-cms text plugin1 ) remotely via Heroku toolbelt : But I get random incomplete output that gets closer to EOF every run ( presumably cached ? ) .Anyone run into something similar ? I 'm using Django 1.4 with postgresql.1 although , it is blobs of HTML o_0 : see docs.Edit : assume this is just a limitation ? ? pg_dump's/ restore was my `` backup '' plan ."
"I thought they were the same before I ran this code : It raises AttributeError : ' C ' object has no attribute '__a ' with the getattr statement , but why ?"
"I 'm using Python 2.7.3.Consider a dummy class with custom ( albeit bad ) iteration and item-getting behavior : Make an example and see the weird behavior : But now , let 's make a function and then do argument unpacking on zz : So , argument unpacking is somehow getting the item data from zz but not by either iterating over the object with its implemented iterator and also not by doing a poor man 's iterator and calling __getitem__ for as many items as the object has.So the question is : how does the syntax add3 ( *zz ) acquire the data members of zz if not by these methods ? Am I just missing one other common pattern for getting data members from a type like this ? My goal is to see if I could write a class that implements iteration or item-getting in such a way that it changes what the argument unpacking syntax means for that class . After trying the two example above , I 'm now wondering how argument unpacking gets at the underlying data and whether the programmer can influence that behavior . Google for this only gave back a sea of results explaining the basic usage of the *args syntax.I do n't have a use case for needing to do this and I am not claiming it is a good idea . I just want to see how to do it for the sake of curiosity.AddedSince the built-in types are treated specially , here 's an example with object where I just maintain a list object and implement my own get and set behavior to emulate list.In this case , But instead , if I ensure iteration stops and always returns 3 , I can get what I was shooting to play around with in the first case : Then I see this , which is what I originally expected : SummaryThe syntax *args relies on iteration only . For built-in types this happens in a way that is not directly overrideable in classes that inherit from the built-in type.These two are functionally equivalent : foo ( * [ x for x in args ] ) foo ( *args ) These are not equivalent even for finite data structures.foo ( *args ) foo ( * [ args [ i ] for i in range ( len ( args ) ) ] )"
"I recently installed Python 3 on my Mac OSX 10.6.8 and I have n't had any problems with modules or imports until now . I 'm writing a function that tests whether or not a triangle is right angled based on the length of the sides and the guide that the exercise was in has a bunch of equalities to test so I can see if it works : I should apparently import a function called testEqual ( a , b , c ) from a module called test , since the example programme in the guide starts with from test import testEqual , but when I typed that into my file I got this message : I suppose I should specify the path to the test module , but I ca n't find it my Python 3 library anywhere in my computer – just the 2.x ones that came installed with the computer , which are in /Library/Python . import turtle and import math worked , so it must be somewhere ."
"Suppose I have a n × m array , i.e . : And I what to generate a 3D array k × n × m , where all the arrays in the new axis are equal , i.e . : the same array but now 3 × 3 × 3.How can I get it ?"
"I stumbled across this extra , no-underscores mro method when I was using __metaclass__ = abc.ABCMeta . It seems to be the same as __mro__ except that it returns a list instead of a tuple . Here 's a random example ( ideone snippet ) : I found later that this is n't unique to ABCMeta but is available in all new-style classes.So ... why ? What is this doing that __mro__ is n't ?"
"This morning I decided to handle keyboard interrupt in my server program and exit gracefully . I know how to do it , but my finicky self did n't find it graceful enough that ^C still gets printed . How do I avoid ^C getting printed ? Press Ctrl+C to get out of above program and see ^C getting printed . Is there some sys.stdout or sys.stdin magic I can use ?"
"I am trying to serve bokeh documents via Django using the bokeh-server executable , which creates a Tornado instance . The bokeh documents can be accessed via URL provided by the Session.object_link method . When navigated to , the bokeh-server executable writes this to the stdout ( IP addresses have been replaced with ellipses ) : This appears to be communication between the python instance running the Django WSGI app ( initialized by Apache running mod_wsgi ) and the bokeh-server executable.When the browser is sent the response , including the graphs and data etc . required for the bokeh interface , there is some initial networking to the browser , followed by networking if there is any interaction with the graphs which have python callbacks . When the user closes the window or browser , the same networking above continues . Moreover , the networking only stops when the Django or bokeh-server processes are killed.In order to start a bokeh session and pass a URL back to the Django template , it is necessary to start the bokeh session in a new thread : self.session and self.document were both initialized before the thread was started . So at the point where get_bokeh_url is called , there are some graphs on the document , some of which have interaction callbacks and session has been created but not polled via poll_document ( which appears necessary for interaction ) .The thread keeps running forever unless you kill either Django or bokeh-server . This means that when more requests come through , more threads build up and the amount of networking increases.My question is , is there a way to kill the thread once the document is no longer being viewed in a browser ? One answer that I have been pondering would be to send a quick request to the server when the browser closes and somehow kill the thread for that document . I 've tried deleting the documents from the bokeh interface , but this has no effect ."
"I use cmd Windows , chcp 65001 , this is my code : Result : But , when i use this code : Result : My screen : And my question is : Why python 2.7 need a space when print unicode character ? How to fix IOError : [ Errno 2 ]"
"I need to add functionality to a class that inherits to deque , but prefer to see the code in collections.deque to implement a new class final.Where can I find the source code of collections.deque ? this is source collections , but does not include collections.deque . - > http : //hg.python.org/cpython/file/2.7/Lib/collections.py"
"Two questions about kwargs in Python.Is every key of kwargs guaranteed to be of type str ? If so , I can avoid type checking.If # 1 is true , would every key be guaranteed to not have spaces ?"
"I have a lot of model classes with ralations between them with a CRUD interface to edit . The problem is that some objects ca n't be deleted since there are other objects refering to them . Sometimes I can setup ON DELETE rule to handle this case , but in most cases I do n't want automatic deletion of related objects till they are unbound manually . Anyway , I 'd like to present editor a list of objects refering to currently viewed one and highlight those that prevent its deletion due to FOREIGN KEY constraint . Is there a ready solution to automatically discover referers ? UpdateThe task seems to be quite common ( e.g . django ORM shows all dependencies ) , so I wonder that there is no solution to it yet.There are two directions suggested : Enumerate all relations of current object and go through their backref . But there is no guarantee that all relations have backref defined . Moreover , there are some cases when backref is meaningless . Although I can define it everywhere I do n't like doing this way and it 's not reliable . ( Suggested by van and stephan ) Check all tables of MetaData object and collect dependencies from their foreign_keys property ( the code of sqlalchemy_schemadisplay can be used as example , thanks to stephan 's comments ) . This will allow to catch all dependencies between tables , but what I need is dependencies between model classes . Some foreign keys are defined in intermediate tables and have no models corresponding to them ( used as secondary in relations ) . Sure , I can go farther and find related model ( have to find a way to do it yet ) , but it looks too complicated.SolutionBelow is a method of base model class ( designed for declarative extention ) that I use as solution . It is not perfect and does n't meet all my requirements , but it works for current state of my project . The result is collected as dictionary of dictionaries , so I can show them groupped by objects and their properties . I hav n't decided yet whether it 's good idea , since the list of referers sometimes is huge and I 'm forced to limit it to some reasonable number.Thanks to all who helped me , especially stephan and van ."
"I 'm writing a command-line interface in Python . It uses the readline module to provide command history and completion.While everything works fine in interactive mode , I 'd like to run automated tests on the completion feature . My naive first try involved using a file for standard input : The command file contained a tab , in the hopes that it would invoke the completion feature . No luck . What 's the right way to do the testing ?"
"Given a DataFrame with Product Id and Amount : I want to add a `` Description '' column based on Amount , which is expected to look like : When I use f-strings formatting , the result is aggregating the whole column Amount as a series instead of using the value of a particular row for string concatenation : However , it works fine with simple string concatenation using + : Why does f-strings formatting in a Pandas DataFrame behave like that ? How should I fix it to use f-strings formatting ? Or is it not suggested to use f-strings formatting for string concatenation in Pandas ?"
"Primary issue : the QGraphicsView.mapToScene method returns different answers depending on whether or not the GUI is shown . Why , and can I get around it ? The context is I 'm trying to write unit tests but I do n't want to actually show the tools for the tests.The small example below illustrates the behavior . I use a sub-classed view that prints mouse click event positions in scene coordinates with the origin at the lower left ( it has a -1 scale vertically ) by calling mapToScene . However , mapToScene does not return what I am expecting before the dialog is shown . If I run the main section at the bottom , I get the following output : Before show ( ) , there is a consistent offset of 34 pixels in x and 105 in y ( and in y the offset moves in reverse as if the scale is not being applied ) . Those offset seem rather random , I have no idea where they are coming from.Here is the example code : This example is in PyQt5 on Windows , but PyQt4 on Linux does the same thing ."
"I have a list of functions of the type : and an array of shape [ 4 , 200 , 200 , 1 ] ( a batch of images ) . I want to apply the list of functions , in order , along the 0th axis . EDIT : Rephrasing the problem . This is equivalent to the above . Say , instead of the array , I have a tuple of 4 identical arrays , of shape ( 200 , 200 , 1 ) , and I want to apply function1 on the first element , function2 on the second element , etc . Can this be done without a for loop ?"
"I 'm working inside the Python REPL , and I want to save my work periodically . Does anybody have a script to dump all the variables I have defined ? I 'm looking for something like this : This seems like something other people have done , so I wanted to ask before re-inventing the wheel ."
"I 'm using Python 2.7.4 and new to Tkinter , and I 'm stuck with the following code.I generated an event `` test '' and set the `` data '' option with a string , but an error occurred when retrieving it from event.Error - > AttributeError : Event instance has no attribute 'data ' I ca n't find the related Python docs for this case , so I referred to the tcl document as belowhttp : //www.tcl.tk/man/tcl8.5/TkCmd/event.htm # M14Does TKinter of Python 2.7 support `` data '' option ? Thanks !"
"after installing nltk i import nltk and then use nltk.download ( ) but when i try to use this `` from nltk.book import * '' it shows attribute error.from nltk.corpus import * and from nltk import * works fine i am new to natural language processing so i dont know much about this please helpfrom nltk.book import ** Introductory Examples for the NLTK Book *Loading text1 , ... , text9 and sent1 , ... , sent9Type the name of the text or sentence to view it.Type : 'texts ( ) ' or 'sents ( ) ' to list the materials.Traceback ( most recent call last ) : File `` '' , line 1 , in File `` C : \Program Files ( x86 ) \Python 3.5\lib\site-packages\nltk\book.py '' , line 19 , in File `` C : \Program Files ( x86 ) \Python 3.5\lib\site-packages\nltk\text.py '' , line 295 , in initFile `` C : \Program Files ( x86 ) \Python 3.5\lib\site-packages\nltk\corpus\reader\util.py '' , line 233 , in lenFile `` C : \Program Files ( x86 ) \Python 3.5\lib\site-packages\nltk\corpus\reader\util.py '' , line 291 , in iterate_fromFile `` C : \Program Files ( x86 ) \Python 3.5\lib\site-packages\nltk\corpus\reader\plaintext.py '' , line 117 , in _read_word_block words.extend ( self._word_tokenizer.tokenize ( stream.readline ( ) ) ) File `` C : \Program Files ( x86 ) \Python 3.5\lib\site-packages\nltk\tokenize\regexp.py '' , line 126 , in tokenize self._check_regexp ( ) File `` C : \Program Files ( x86 ) \Python 3.5\lib\site-packages\nltk\tokenize\regexp.py '' , line 121 , in _check_regexp self._regexp = compile_regexp_to_noncapturing ( self._pattern , self._flags ) File `` C : \Program Files ( x86 ) \Python 3.5\lib\site-packages\nltk\internals.py '' , line 56 , in compile_regexp_to_noncapturing return sre_compile.compile ( convert_regexp_to_noncapturing_parsed ( sre_parse.parse ( pattern ) ) , flags=flags ) File `` C : \Program Files ( x86 ) \Python 3.5\lib\site-packages\nltk\internals.py '' , line 52 , in convert_regexp_to_noncapturing_parsed parsed_pattern.pattern.groups = 1AttributeError : ca n't set attribute"
"I am attempting to adapt the Jinja2 WithExtension to produce a generic extension for wrapping a block ( followed by some more complex ones ) .My objective is to support the following in templates : And for wrapper.html.j2 to look like something like : I believe my example is most of the way there , WithExtension appears to parse the block and then append the AST representation of some { % assign .. % } nodes into the context of the nodes it is parsing.So I figured I want the same thing , those assignments , followed by an include block , which I 'd expect to be able to access those variables when the AST is parsed , and to pass through the block that was wrapped as a variable content.I have the following thus far : However , it falls over at my nodes.Include line , I simply get assert frame is None , 'no root frame allowed ' . I believe I need to pass AST to nodes.Template rather than a template name , but I do n't really know how to parse in additional nodes for the objective of getting AST rather than string output ( i.e . renderings ) – nor whether this is the right approach . Am I on the right lines , any ideas on how I should go about this ?"
"I 'm having a hard time understanding what happens when I try to nest descriptors/decorators . I 'm using python 2.7.For example , let 's take the following simplified versions of property and classmethod : Trying to nest them : The __get__ method of the inner descriptor MyClassMethod is not getting called.Failing to figure out why , I tried throwing in ( what I think is ) a no-op descriptor : Trying to use the no-op descriptor/decorator in nesting : I do n't understand why B ( ) .prop1 works and B ( ) .prop2 does not.Questions : What am I doing wrong ? Why am I getting a object is not callable error ? What 's the right way ? e.g . what is the best way to define MyClassProperty while re-using MyClassMethod and MyProperty ( or classmethod and property )"
I have two models and both have field 'status ' which has different meaning for them.Their forms looks like : I want to edit both this fields on the same page . My view.py looks likeAnd template : But in result form.status gets values from form_edit.status which is not correct . I need to solve this problem without changing names of model fields but do n't know how .
"I am trying to display a progress bar when I perform `` vector '' progress_apply operations on pandas dataframes , in MS Visual Studio Code.In VS Code with the Python extension enabled , I tried in a cellAnd the result isand the progress bar is not rendered.I would like a progress bar to render a bit like it happens withI am puzzled because the syntax above works fine in `` classic '' Jupyter notebooks on FirefoxHow can I visualize the progress bar when I run pandas progress_apply in vscode ?"
"I 'm trying to find out why lxml can not parse an XSL document which consists of a `` root '' document with various xml : includes . I get an error : That tells me where in the lxml source the error is , but is there a way to get more through lxml about where in the xsl the error is , or should I be using a different method ? I 'm trying to provide a service that accepts XSL documents , so I do n't have access to an XML editor to debug manually . What I would like to do though is give feedback about why a transformation did n't succeed ."
I am trying to obtain a list of columns in a DataFrame if any value in a column contains a string . For example in the below dataframe I would like a list of columns that have the % in the string . I am able to accomplish this using a for loop and the series.str.contains method but doens't seem optimal especially with a larger dataset . Is there a more efficient way to do this ? DataFrameCurrent MethodOutput
I want to plot a line chart ( with connected point markers ) in Altair . I know how to change the linewidth ( by setting the strokeWidth ) but I do n't know how I can change the size of these point markers . Below is my code :
I would like to integrate my Bokeh Server Application in Electron . So what I did is to run bokeh server using python-shell like thisThe problem here is that the window never pops up because electron does not detect the server as loaded .
"So I can read from a global variableAnd I can also assign itWhen people say `` global variables are bad '' , do they mean that both reading and assigning are bad , or just assigning is bad ? ( my impression is that reading is not dangerous )"
This does n't work . NameError : name 'SomeClass ' is not definedIs there any way to refer to the class within the class ?
"In python , it is illegal to create new attribute for an object instance like thisthrows However , for a function object , it is OK.What is the rationale behind this difference ?"
"I would like to draw an arrow indicating the gradient of a function at a point , by pointing in the direction tangent to the function . I would like the length of this arrow to be proportional to the axis size , so that it is visible at any zoom level.Say we want to draw the derivative of x^2 at x=1 ( the derivative is 2 ) . Here are two things I tried : Here 's what they look like at two zoom levels . To be clear , I want the red line 's length and the black line 's direction :"
"I have a dataframe that looks like this : And what I would like to obtain is this ( could be a matrix as well ) : That is , the sum of Size values for all possible pairs in ID.For now I have this simple but unefficient code : It works fine for small examples like this , but for my actual data it gets too long and I am sure it is possible to avoid the nested for loops . Can you think of a better way to do this ? Thanks for any help !"
"I have a piece of code that runs perfectly most of the time , but every once in awhile I get this error in the traceback : I know what issubclass ( ) does and understand the error , but I never called it ; that line in the code is pure arithmetic , so I do n't know why this TypeError is raised in the first place . My only theory is that Numpy is calling it behind the scenes , but then the traceback should show the problematic line in the Numpy source , right ? What 's going on ? Updates : wv is an array of floats and sm2 is a float scalar . The error is actually thrown by numpy.log , i.e . the ( new ) lineNo more information is provided in the error message , though.More updates : My current version of Numpy ( from a Python prompt ) : I changed the problem line toand got the outputSo it makes sense that there would be some kind of error , but if I do this ( at a Python prompt ) I get the error I would expect ( and am already handling in the code in question , via the warning module ) :"
"Since the announcement about XMLA endpoints , I 've been trying to figure out how to connect to a URL of the form powerbi : //api.powerbi.com/v1.0/myorg/ [ workspace name ] as an SSAS OLAP cube via Python , but I have n't gotten anything to work.I have a workspace in a premium capacity and I am able to connect to it using DAX Studio as well as SSMS as explained here , but I have n't figured out how to do it with Python . I 've tried installing olap.xmla , but I get the following error when I try to use the Power BI URL as the location using either the powerbi or https as the prefix.I 'm sure there are authentication issues involved , but I 'm a bit out of my depth here . Do I need to set up an `` app '' in ActiveDirectory and use the API somehow ? How is authentication handled for this kind of connection ? If anyone knows of any blog posts or other resources that demonstrate how to connect to a Power BI XMLA endpoint specifically using Python , that would be amazing . My searching has failed me , but surely I ca n't be the only one who is trying to do this ."
"I want to get the domain of the cookie from the HTTP response . Code is : this shows the cookie ascookie= Set-Cookie : Cycle=MA==|MA==|MA== ; Domain=.abc.xyz.net ; expires=Tue , 05-Oct-2021 04:15:18 GMT ; Path=/I want to extract the domain from the above result.I am trying None of these work . Thanks"
"I am using Pony ORM for a flask solution and I 've come across the following.Consider the following : When I need the result in Jinja2 -- which is looks like thisI get aIf I uncomment the for r in res part , it works fine . I suspect there is some sort of lazy loading that does n't get loaded with res = q2 [ : ] .Am I completely missing a point or what 's going on here ?"
"I am working on a python application which allows me to post updates/statuses to Facebook.I am facing a basic problem with getting access tokens.Since this application is working on the desktop , there is no question of using some web server.I understand that I have to open a URL : ..and get the tokens out of the redirected URL.But even this is out of option , since mine is a console based application . The closest I can get to using embedded browser is by using Python 's mechanize module.Is there any other option ?"
"I am trying to import sentences from Shakespeare 's NLTK corpus – following this help site – but I am having trouble getting access to the sentences ( in order to train a word2vec model ) : Returns the following : Why are all the acts None ? None of the methods defined here ( http : //www.nltk.org/howto/corpus.html # data-access-methods ) ( .sents ( ) , tagged_sents ( ) , chunked_sents ( ) , parsed_sents ( ) ) seem to work when applied to the shakespeare XMLCorpusReaderI 'd like to understand :1/ how to get the sentences 2/ how to know how to look for them in an ElementTree object"
"I have a client written in C # , and a server written in python . The messages that I send over the socket are 8 bytes followed by the data , the 8 bytes are the data length.In C # before sending , I convert the 8-byte data length too big endian as shown : The message is sent successfully and I am getting tied up in the python server code since the unpack format should be correct here should n't it ? Is n't the big-endian character ' > ' ? Why is my length so long ? UPDATE : There was a bug where I was unpacking the wrong 8 bytes , that has been fixed , now that I am unpacking the correct data I still have the same question.The correct length is given only if I unpack using little endian even though I sent the bytes to the server using big-endian ... so I am expecting > Q to work , but instead Here is how I am receiving the bytes in python :"
"I have a list of strings such as : And I would like to create a list for each element in the list , that would be named exactly as the string : How can I do that in Python ?"
"I 'm not sure I completely understand what is going on with the following regular expression search : search ( ) call takes minutes to complete , CPU usage goes to 100 % . The behavior is reproduceable for both 2.7.5 and 3.3.3 Python versions.Interesting fact that if the string is less than 20-25 characters in length , search ( ) returns like in no time.What is happening ?"
"In a prototype application that uses Python and SQLAlchemy with a PostgreSQL database I have the following schema ( excerpt ) : I want to build a query , using SQLAlchemy , that retrieves the list of guests , to be displayed in the back-office.To implement pagination I will be using LIMIT and OFFSET , and also COUNT ( * ) OVER ( ) to get the total amount of records while executing the query ( not with a different query ) .An example of the SQL query could be : If I were to build the query using SQLAlchemy , I could do something like : And if I wanted to count all the rows in the guests table , I could do something like this : Now the question I am asking is how to execute one single query , using SQLAlchemy , similar to the one above , that kills two birds with one stone ( returns the first 50 rows and the count of the total rows to build the pagination links , all in one query ) .The interesting bit is the use of window functions in PostgreSQL which allows the abovementioned behaviour , thus saving you from having to query twice but just once.Is it possible ? Thanks in advance ."
"I 'm looking up country by ip range for tens of millions rows . I 'm looking for a faster way to do the lookup.I have 180K tuples in this form : ( The integers are ip addresses converted into plain numbers . ) This does the job right , but just takes too long : Can anyone point me in the right direction to a faster way of doing this lookup ? Using the method above , 100 lookups take 3 seconds . Meaning , I think , 10M rows will take several days ."
"I want to link a plot containing patches ( from a GeoJSONDataSource ) with a line chart but i 'm having trouble getting the attributes of the selected patch.Its basically a plot showing polygons , and when a polygon is selected , i want to update the line chart with a timeseries of data for that polygon . The line chart is driven by a normal ColumnDataSource.I can get the indices of the selected patch by adding a callback combined with geo_source.selected [ '1d ' ] [ 'indices ' ] . But how do i get the data/attributes which correspond to that index ? I need to get a 'key ' in the attributes which i can then use to update the line chart.The GeoJSONDataSource has no data attribute in which i can lookup the data itself . Bokeh can use the attributes for things like coloring/tooltips etc , so i assume there must be a way to get these out of the GeoJSONDataSource , i cant find it unfortunately.edit : Here is working toy example showing what i 've got so far.Save the code in a .py file and load with bokeh serve example.py -- show"
"Using Blueprint , I can SpawnActorFromClass with a StaticMeshActor , but with a Python script via the builtin Python Script plugin , I got : What am I missing ?"
"I am working on using hyperopt to tune my ML model but having troubles in using the qloguniform as the search space . I am giving the example from official wiki and changed the search space . But getting the following error . ValueError : ( 'negative arg to lognormal_cdf ' , array ( [ -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 , -3.45387764 ] ) ) I have tried without log transform as below but the output values turns out to be log transformation ( ex- 1.017,1.0008,1.02456 ) , which is wrong . It is consistent with the documentation.Thanks"
"I am using python logging in my django application . A class that connects to a backend api initialises this logger with a filehandler if needed . The class gets instantiated everytime an api call is made . I have tried making sure additional handlers are not added every time , but shows an increasing amount of handlers on my log file and after a while my server fails due to this open file limit.I realise this may not be the best way to do this , but I have not found the error in my implementation so far ."
"We can define an alias in ipython with the % alias magic function , like this : This escapes to the shell command date when you type d into ipython . But I want to define an alias to execute some python code , in the current interpreter scope , rather than a shell command . Is that possible ? How can we make this kind of alias ? I work in the interactive interpreter a lot , and this could save me a lot of commands I find myself repeating often , and also prevent some common typos ."
"I 'm using Elasticsearch 's multi search to run a list of queries through the python library . It seems to be working ... mostly . The problem is that if I send up 200 queries , it returns 100 results ( no errors ) . I 've tried running even 20 queries and still get 10 results . For some reason it seems to always return only half of the queries . Which also means I do n't know which query the results correlate to.The queries all work individually but would take too long . There 's no mention in the docs of the possibility of the response not containing the same number of queries so I 'm not sure where to go here . I 've stepped into the code as much as the python library will let me to verify all of the queries are being sent up and it seems to work as expected.Thanks for any help or a push in the right direction ! Edit : Here 's an example of the query I use.I create a list of however many queries like these I 'd like to run and execute them as so , as per the python elasticsearch documentation :"
"I 'm trying to populate item using ItemLoader parsing data from multiple pages . But as I can see now , I ca n't change selector that I used when I initialized ItemLoader . And documentation says about selector attribute : selector The Selector object to extract data from . It ’ s either the selector given in the constructor or one created from the response given in the constructor using the default_selector_class . This attribute is meant to be read-only.Here 's example code : As for now , I ca n't scrape info from the second page ."
"I have a lot of data in database in ( x , y , value ) triplet form.I would like to be able to create dynamically a 2D numpy array from this data by setting value at the coords ( x , y ) of the array.For instance if I have : The resulting array should be : I 'm new to numpy , is there any method in numpy to do so ? If not , what approach would you advice to do this ?"
"This is similar to the problem I asked here . However , I found out that the data I am working is not always consistent . For , example say : Now you can see that X does not have corresponding c column and Y does not have corresponding b column . Now when I want to create the multi-level index , I want the dataframe to look like this : So as you can see , I want the split in such a way that all upper level columns should have the same lower level columns . Since , the dataset is positve , I am thinking of filling the missing columns with -1 , although I am open for suggestions on this . The closest thing I found to my problem was this answer . However , I can not make it to somehow work with MultiLevel Index like in my previous question . Any help is appreciated ."
"I 'm a beginner programmer so this question might sound trivial : I have some text files containg tab-delimited text like : Now I want to generate unordered .html lists out of this , with the structure : My idea was to write a Python script , but if there is an easier ( automatic ) way , that is fine too . For identifying the indentation level and item name I would try to use this code :"
I have an example csv file with name 'r2.csv ' : I have the following python code to read a csv file and transfer the columns to arrays : It runs fine except for the last row and has the following error message : How could I avoid it ?
I want to resolve a status 405 that I get from the task queue when trying to generate a report : The code that creates the task is and I have it routed with webapp2 : then I should be able to make it a cron job but when I test it I get a 405 from the access of this code which times out if I try to run it directly : How can I resolve the status 405 and get through the execution ? Thank you
"I have a problem with my current attempt to build a sequential model for time series classification in Keras . I want to work with channels_first data , because it is more convenient from a perprocessing perspective ( I only work with one channel , though ) . This works fine for the Convolution1D layers I 'm using , as I can specify data_sample='channels_first ' , but somehow this wo n't work for Maxpooling1D , which does n't have this option as it seems . The model I want to build is structured as follows : With window_length = 5000 I get the following summary after all three layers are added : Now , I wonder if this is correct , as I would expect the third dimension ( i.e . the number of neurons in a feature map ) and not the second ( i.e . the number of filters ) to be reduced by the pooling layer ? As I see it , MaxPooling1D does not recognize the channels_first ordering and while the Keras documentation says there exists a keyword data_format for MaxPooling2D , there 's no such keyword for MaxPooling1D.I tested the whole setup with a channels_last data format , and it worked as I expected . But since the conversion from channels_first to channels_last takes quite some time for me , I 'd really rather have this work with channels_first . And I have the feeling that I 'm simply missing something . If you need any more information , let me know ."
"I 'm trying to solve a problem related to graphs in Python . Since its a comeptitive programming problem , I 'm not using any other 3rd party packages . The problem presents a graph in the form of a 5 X 5 square grid.A bot is assumed to be at a user supplied position on the grid . The grid is indexed at ( 0,0 ) on the top left and ( 4,4 ) on the bottom right . Each cell in the grid is represented by any of the following 3 characters . ‘ b ’ ( ascii value 98 ) indicates the bot ’ s current position , ‘ d ’ ( ascii value 100 ) indicates a dirty cell and ‘ - ‘ ( ascii value 45 ) indicates a clean cell in the grid . For example below is a sample grid where the bot is at 0 0 : The goal is to clean all the cells in the grid , in minimum number of steps . A step is defined as a task , where eitheri ) The bot changes it positionii ) The bot changes the state of the cell ( from d to - ) Assume that initially the position marked as b need not be cleaned . The bot is allowed to move UP , DOWN , LEFT and RIGHT.My approachI 've read a couple of tutorials on graphs , and decided to model the graph as an adjacency matrix of 25 X 25 with 0 representing no paths , and 1 representing paths in the matrix ( since we can move only in 4 directions ) . Next , I decided to apply Floyd Warshell 's all pairs shortest path algorithm to it , and then sum up the values of the paths.But I have a feeling that it wo n't work.I 'm in a delimma that the problem is either one of the following : i ) A Minimal Spanning Tree ( which I 'm unable to do , as I 'm not able to model and store the grid as a graph ) .ii ) A* Search ( Again a wild guess , but the same problem here , I 'm not able to model the grid as a graph properly ) .I 'd be thankful if you could suggest a good approach at problems like these . Also , some hint and psuedocode about various forms of graph based problems ( or links to those ) would be helpful . Thank"
"I have a very large array similar to elevation data of the format : where x , y , z are all floats in metres . You can create suitable test data matching this format with : I want to be able to efficiently find the corresponding z-value for a given ( x , y ) pair . My research so far leads to more questions . Here 's what I 've got : Iterate through all of the triplets : Drawbacks : slow ; a , b must exist , which is a problem with comparing floats.Use scipy.spatial.cKDTree to find nearest : Drawbacks : returns nearest point rather than interpolating.Using interp2d as per comment : Drawbacks : does n't work on a large dataset . I get OverflowError : Too many data points to interpolate when run on real data . ( My real data is some 11 million points . ) So the question is : is there any straightforward way of doing this that I 'm overlooking ? Are there ways to reduce the drawbacks of the above ?"
"I am making an excel comparing program that takes a certain amount of sheets , compares them and checks to see if the values in one of the sheets do not exist in the other . However , I am receiving a repetitive problem . To clarify , here is my code : However , when I check my csv file , the `` differences '' outputted appear to be on both of the files . Does anyone have any suggestions for helping me out ? Anything would be great thank you"
"If I do the followingAnd then type help myfunction , I get : So the name and docstring are correctly copied over . Is there a way to also copy over the actual call signature , in this case ( a , b , c ) ?"
"I 'm trying to implement simple log handler that uses Python 's standard logging library to log events to remote server . So I created custom class inherited from logging.Handler called RemoteLogHandler that accepts LogRecord and sends it to remote server . The handler is attached to root logger in standard addHandler ( ) manner.This works as intended , but can obviously cause locking of the calling thread when the remote_url becomes inaccessible or starts responding slowly . So I 'm trying to figure best way to make this call independent on the calling thread.What I 've considered : Including some non-standard library that would make the http request asynchronousUsing QueueHandler and QueueListener as outlined hereUsing asyncioAll these solutions seem to be way too complex/unnecessary for achieving such simple task . Is there some better approach with less overhead in order to make this handler non-blocking ?"
"Sample data : gives : I want to resample by '2D ' and get the max value , something like : The expected result should be : Can anyone help me ?"
"In python , is it possible to ask a thread what its currently doing ? Some code might look like this : I 'm looking to do this to monitor threads to see if they 're hanging . The heartbeat event checks whether they 're active and progressing normally . However , if they hang somewhere , I 'd like to be able to find out where . That 's where I 've invented get_thread_position ( ) which I would like to return something like a traceback to the function its currently executing . That way , I can use that information to figure out how its getting stuck in some infinite loop ."
In C # I could easily write the following : Is there a quick way of doing the same thing in Python or am I stuck with an 'if ' statement ?
"I have a dataframe that is created from a pivot table , and looks similar to this : I 'm looking to iterative over the upper level of the multiindex column to divide each company by it 's sum to create a percentage : I do n't know the company names beforehand . This is a variation of a question asked yesterday : Summing multiple columns with multiindex columns"
"I am trying to get a list of articles using a combo of the googlesearch and newspaper3k python packages . When using article.parse , I end up getting an error : newspaper.article.ArticleException : Article download ( ) failed with 403 Client Error : Forbidden for url : https : //www.newsweek.com/donald-trump-hillary-clinton-2020-rally-orlando-1444697 on URL https : //www.newsweek.com/donald-trump-hillary-clinton-2020-rally-orlando-1444697I have tried running as admin when executing script and the link works when opening straight in a browser.Here is my code : Here is my full error output : I expected it to just output the text of the article . Any help you can give would be great . Thanks !"
"I am trying to convert MNIST dataset to RGB format , the actual shape of each image is ( 28 , 28 ) , but i need ( 28 , 28 , 3 ) . But i get the following error :"
"Can I include multiple `` where '' clauses ( or an AND operator ) in an endpoint query string ? I 'd like to do something like this : I 've tried few different syntaxes , but I ca n't get it to work.Is this possible ? I know I can do it using MongoDB syntax , but I would like to use Python syntax.Note : I 'm not trying to concatenate an list of parameteres using python , I 'm trying to use Eve 's filtering feature using native python syntax ."
"I am following through the tutorial here : https : //pythonprogramming.net/train-test-tensorflow-deep-learning-tutorial/I can get the Neural Network trained and print out the accuracy.However , I do not know how to use the Neural Network to make a prediction.Here is my attempt . Specifically the issue is this line - I believe my issue is that I can not get my input string into the format the model expects : Here is a larger listing :"
"Background : I am working on a web scraper to track prices at online stores . It uses Django . I have a module for each store , with functions like get_price ( ) and get_product_name ( ) written for each one , so that the modules can be used interchangeably by the main scraper module . I have store_a.py , store_b.py , store_c.py , et cetera , each with these functions defined . In order to prevent duplication of code , I 've made StoreTestCase , which inherits from TestCase . For each store , I have a subclass of StoreTestCase , like StoreATestCase and StoreBTestCase.When I manually test the StoreATestCase class , the test runner does what I want . It uses the data in the child class self.data for its tests , and does n't attempt to set up and test the parent class on its own : However , when I manually test against the module , like : It first runs the tests for the child class and succeeds , but then it runs them for the parent class and returns the following error : store_test.py ( parent class ) test_store_a.py ( child class ) How do I ensure the Django test runner only tests the child class , and not the parent class ?"
"I decorated a method with login_required , but I am surprised that its not executing at all , allowing in anonymous users . Printing the current_user within the method returns this : Is it not supposed to reject users which return false in user.is_autheticated ( ) ? What did I do wrong ? I have setup FL this way : in views.py : the actual view :"
"PremiseI 'm trying to decode the data from the barcode format currently used on tickets issued by Deutsche Bahn ( german railway ) . I have found this very useful website ( german ) that already does a similar thing and offers a python script.The website states that the data is compressed with zlib , the resulting blob is signed with DSA and all of it is stored in the barcode ( Aztec format ) .Example of such a barcodeProblemI have used the script provided on the website to successfully decode a ticket.Installed the python-pyasn1 library . Read the barcode ( used BCTester as per instructions , had some trouble with NeoReader app ) and converted the result to hex . Saved the hex data as plain text file ( as is for some reason required by the script ) and parsed the file with the script . It worked.But the script is doing too much . I 'd like to do the parsing myself , but I ca n't get the zlib decompression to work and I understand to little of the code to make sense of it.I know almost no Python . I have some programming experience , though.If you simply look at the data from the barcode , it looks like this : https : //gist.github.com/oelna/096787dc18596aaa4f5fThe first question would be : What is the DSA signature and do I need to split it from the actual compressed data first ? The second : What could a simple python script look like that reads the barcode blob from a file and simply decompresses it , so I can further parse the format . I had something in mind likebut it 's not working . Any hint in the right direction would be appreciated.Here is the hex data that is readable by the script if saved to a file : Update/Solution : Mark Adler 's tip set me on the right track . It took me hours , but I hacked together a working solution to this particular problem . If I had been smarter , I would have recognized the zlib header 78 9C at offset 68 . Simply split the data at this point and the second half decompresses without complaint . Be warned , very sad pythonIf there is an easy solution to this , feel free to comment . I 'll continue working on this for a little more and try to make it a more robust solution that actively seeks out the zlib header , without hardcoding the offset . The first half is an identifier code , like # UT010080000060 , , followed by a ASN.1 DSA signature , which luckily I do n't need to verify or modify ."
"Second Edit : After a bit of digging , the question changed from how to log an exception with local variables to how to prevent celery from sending the 2nd log message which does not has the local vars . After the below attempt , I actually noticed that I was always receiving 2 emails , one with local vars per frame and the other without.First Edit : I 've managed to sort of get the local variables , by adding a custom on_failure override ( using annotations for all tasks like so : But the problem now is that the error arrives 3 times , once through the celery logger and twice through root ( although I 'm not propagating 'celery ' logger in my logging settings ) Original question : I have a django/celery project to which I recently added a sentry handler as the root logger with level 'ERROR ' . This works fine for most errors and exceptions occurring in django , except the ones coming from celery workers.What happens is that sentry receives an exception with the traceback and the locals of the daemon , but does not include the f_locals ( local vars ) of each frame in the stack . And these do appear in normal python/django exceptions.I guess I could try to catch all exceptions and log with exc_info manually . But this is less than ideal ."
"I want to test function calls with optional arguments.Here is my code : I am not able to parametrize it due to optional arguments , so I have written separate test functions for each api call in pytest.I believe there should be better way of testing this one ."
"I want to construct a tree decomposition : http : //en.wikipedia.org/wiki/Tree_decomposition and I have the chordal graph and a perfect elimination ordering . I am following advice given in a previous thread , namely : To construct a non-nice ( in general ) tree decomposition of a chordal graph : find a perfect elimination ordering , enumerate the maximal cliques ( the candidates are a vertex and the neighbors that appear after it in the ordering ) , use each clique as a decomposition node and connect it to the next clique in the ordering that it intersects.This does not work however and I can not figure out why . Consider the following example : Perfect elimination ordering : Chordal graph : Tree decomposition : I am using python and my current algorithm is the following : where eo is a list of the perfect ordering and 'chordal_graph ' is a graph object for networkx ."
"The Python docs for str.swapcase ( ) say : Note that it is not necessarily true that s.swapcase ( ) .swapcase ( ) == s.I 'm guessing that this has something to do with Unicode ; however , I was n't able to produce a string that changed after exactly two applications of swapcase ( ) . What kind of a string would fail to produce an identical result ? Here 's what I tested ( acquired here ) :"
"The access token im getting with gcloud auth print-access-token is obviously a different access token than the one i can get with some basic python code : What i am trying to do is get a token that would work with : I 'd prefer not to install gcloud utility as a dependency for my app , hence my tries to obtain the access token progrmatically via oath google credentials"
"I have a class named Server which can be started and stopped . Certain methods should not be called unless the Server is started , in which case a NotConnectedException should be raised . Is there a way to call a method before every method in a class and determine if class variable _started is set to True ? I tried using a decorator , but the decorator function does not have access to the class variable . I was trying to do something like this :"
"I am working on an application which is using many widgets ( QGroupBox , QVBoxLayout , QHBoxLayout ) . Initially it was developed on normal HD monitors . But , recently many of us upgraded to 4K resolution monitors . Now some of the buttons and sliders are compressed so small that they are unusable . Now I tried to make some changes so that the application can be used with both HD and 4K monitors.I started reading the link below : https : //leomoon.com/journal/python/high-dpi-scaling-in-pyqt5/enter link description here I thought whenever my window is opened in a particular monitor I can call the following code : Then I tried to get the monitor resolution ( pixel_x and pixel_y ) using below code from using related post here.screen_width = 0 , screen_height = 1 gives me the resolution of my primary monitor ( mostly laptops in our case which are HD ) . screen_width = 78 , screen_height = 79 gives me the combined resolution of virtual machines . But I do not understand how I can dynamically get these values depending upon where my application opened.My application window is developed in such a way that it will open in the same monitor where it was closed last time . The problem is now I want to get the active monitor resolution whenever my GUI is called and adapt to that resolution . I would be glad if someone can help me out.I am interested to know if I can call the screen resolution calculation every time that I drag my window from an HD monitor to a 4K monitor and Vice versa . Edit : I have found something similar in this post here But I could not get much from this . Edit2 : Based on @ Joe solution , Primary Screen Detection , Why is my primary screen always my laptop resolution even though I run the application on a 4K screen ? I just tried to get the dpi of all the screens using the code below :"
static files are not working in production even for the admin page.I have not added any static files.I am having issues with my admin page style.I have followed the below tutorial to create the django application.https : //tutorial.djangogirls.org/en/below is my settings.pyI have already run collectstatic in production .
"1 . Why does the following Python code using the concurrent.futures module hang forever ? The call raises an invisible exception [ Errno 24 ] Too many open files ( to see it , replace the line executor.submit ( super ( ) .f ) with print ( executor.submit ( super ( ) .f ) .exception ( ) ) ) .However , replacing ProcessPoolExecutor with ThreadPoolExecutor prints `` called '' as expected.2 . Why does the following Python code using the multiprocessing.pool module raise the exception AssertionError : daemonic processes are not allowed to have children ? However , replacing Pool with ThreadPool prints `` called '' as expected.Environment : CPython 3.7 , MacOS 10.14 ."
"I wish to have a function which takes a list of conditions , of any length , and places an ampersand between all the conditions . Example code below.I would appreciate any help with this ."
"How to execute setup and teardown functions once for all nosetests tests ? Note that there exists a similar question with an answer that is not working with python 2.7.9-1 , python-unittest2 0.5.1-1 and python-nose 1.3.6-1 after replacing the dots with pass and adding the line import unittest.Unfortunately my reputation is too low to comment on that ."
I 'm attempting to wrap text using wrap=True but it does n't seem to be working for me . Running the example from matplotlib below : gets me this : Text wrapping gone wrongAny ideas on what 's the issue ?
I have following data in one of my columns : I want to convert this to a datatype column.I tried following : How can I get the date as 1968-01-06 instead of 2068-01-06 ?
"I am new to using OpenCV , Python and Numpy , but have been a Java , C++ , C programmer for a while now . I am implementing a sigma-delta background detector , which does the following : let i1 be first image , let i2 be second imageI 'm basically trying to iterate through the 2D image array and compare the pixel value with the other image , but I 'm struggling to get that working with numpy arrays using for loops . I 've tried using a nested for loop , but I get an error saying that I ca n't access the elements of that array . Edit : This is working but does n't seem very elegant or efficient . I am hoping for a faster solution ... Any suggestions would be most welcome"
"tlndr : how to tell in a function if it 's called from an except block ( directly/indirectly ) . python2.7/cpython.I use python 2.7 and try to provide something similar to py3 's __context__ for my custom exception class : This seems to work fine : Sometimes I need MyErr to be raised outside of an exception block . This is fine too : If , however , there has been a handled exception before this point , it 's being incorrectly set as a context of MyErr : I guess the reason is that sys.exc_info simply returns the `` last '' and not the `` current '' exception : This function returns a tuple of three values that give information about the exception that is currently being handled . < ... > Here , “ handling an exception ” is defined as “ executing or having executed an except clause. ” So , my question is : how to tell if the interpreter is executing an except clause ( and not has it executed in the past ) . In other words : is there a way to know in MyErr.__init__ if there is an except up on the stack ? My app is not portable , any Cpython specific hacks are welcome ."
"I am running a boo.py script on AWS EMR using spark-submit ( Spark 2.0 ) .The file finished successfully when I useHowever , it failed when I runThe log on yarn logs -applicationId ID_number shows : The python and boto3 module I am using is How do I append this library path so that spark-submit could read the boto3 module ?"
I 've been trying to figure out a way to get the number of pages from password protected pdf with python3 . So far I have tried modules pypdf2 and pdfminer2.Both are failing because the file is not decrypted.This code will produce an Error : Is there a way to get the number of pages without decrypting ?
"I found this python example online and I 'd like to understand exactly how the number formatting works : I can see by experimentation that this prints a ( with precision xprecision ) , a tab and then b ( with precision yprecision ) . So , as a simple example , if I runthen I getI understand how % g usually works . I also understand how the % generally works . What is confusing me in this example is the construct % .*g . How does the * work here ? I can see that it is somehow taking the desired precision value and substituting it into the print expression , but why is that happening ? Why does the precision number appear before the number being formatted in ( xprecision , a ... ) ? Can someone break this down and explain it to me ?"
"Following is the XML file : book.xmlFollowing is the Python code : book_dom.pyOutput : ~/ $ python book_dom.pyMy aim is to store the Books in `` Books '' table and Author info in `` Authors '' table ( preserving the book - > author relationship ) [ MySQL DB ] . How do I go about storing the data in the database if I have few thousand books and authors ( and chapters ) ? I am having trouble with uniquely identifying the dataset for each book/author . I can use the IDs and pass them to the functions to preserve the relation but I am not sure if that is the best way to do it . Any pointers are highly appreciated.p.s : I am working on the SQL part of the script and will update once I test it . Feel free to post your thoughts , code samples . Thanks !"
I need an empty string instead .
"I have an architectural problem . I 'm using Django ( with admin panel ) and DRF ( api with stateless authentication using JWT ) .Django has Admin users represented by model which is more or less the same as default Django User Model . Admins work with Django Admin only and ca n't use DRF api.DRF has API users that are allowed to use only api through DRF and ca n't interact with Django Admin or Django Session etc.I know that the best approach is to use multi model inheritance , like : but the problem is that , those users are completly different . Eg : API user has complex composite key as an username field which is impossible in combination to simple Admin username field . And many other differences . ..The question is : may I use a user object that is not an AUTH_USER_MODEL instance in DRF ? So self.request.user will store a model instance that is n't connect in any way with an AUTH_USER_MODEL . Has any of you done something similar ?"
This is my best solution so far to the problem of accessing the calling module from within a function : ... but implicit in the handling of the KeyError is the ( largely unfounded ) assumption that it can happen only if filename is being run as __main__.Does the Python standard library provide a more robust way to do this ?
"After loading a reference to an assembly with something like : How can I unload the assembly again ? Why would anyone ever want to do this ? Because I 'm recompiling foo.dll and want to reload it , but the compiler is giving me a fuss , since IronPython is allready accessing foo.dll ."
"There are several excellent tutorials for generating .gv files for graphviz and then creating the approprate graph using the render command . However , I am looking to simply take a pregenerated .gv file and call graphviz and generate the image . One problem I was noticing with the render command was that it performs the action of saving the .gv file and therefore deletes the file that the user specifies . So my question again is : How can I call graphviz to generate the graph from a pregenerated .gv file rather than creating the .gv file with graphviz commands ? Example : Existing .gv file G.gvSudo Python Code : Other info : Yes I have Graphviz installed and yes using render the way various tutorials use it works I just ca n't seem to pinpoint how to just call graphviz ... Thanks !"
"Currently , I have a dictionary , with its key representing a zip code , and the values are also a dictionary.And then a second dictionary , which associates which state the zip code belongs to.If the zip code in the dictionary states matches one of the keys in db then it would sum up those values and put it into a new dictionary like the expected output . Expected Output : So far this is the direction I am looking to go into but I 'm not sure when both the keys match , how to create a dictionary that will look like the expected output ."
"Given a set strings of containing numbers , how can I find those strings that are the superset . For example if strings '139 24 ' and '139 277 24 ' appear then I want to keep '139 277 24 ' as '139 24 ' can be found inside it . Also these numbers may appear in any order inside a string . The result for the above data is given below . Edit : I am splitting each string and putting individual numbers in a set and then comparing this through the sets created from the whole list . I can find a solution using this approach but I think there should be some other elegant way to perform the same . I was trying the following code and felt that it is becoming unnecessarily complex ."
"I have a vector : And I 'd like to do something like : Is there a function like that in R ? I 've been googling around , but `` R Roll '' mostly gives me pages about Spanish pronunciation ."
"I have been reading about division and integer division in Python and the differences between division in Python2 vs Python3 . For the most part it all makes sense . Python 2 uses integer division only when both values are integers . Python 3 always performs true division . Python 2.2+ introduced the // operator for integer division.Examples other programmers have offered work out nice and neat , such as : How is // implemented ? Why does the following happen : Should n't x // y = math.floor ( x/y ) ? These results were produced on python2.7 , but since x and y are both floats the results should be the same on python3+ . If there is some floating point error where x/y is actually 4.999999999999999 and math.floor ( 4.999999999999999 ) == 4.0 would n't that be reflected in x/y ? The following similar cases , however , are n't affected :"
"I 'm using Selenium with Python bindings to scrape AJAX content from a web page with headless Firefox . It works perfectly when run on my local machine . When I run the exact same script on my VPS , errors get thrown on seemingly random ( yet consistent ) lines . My local and remote systems have the same exact OS/architecture , so I 'm guessing the difference is VPS-related.For each of these tracebacks , the line is run 4 times before an error is thrown.I most often get this URLError when executing JavaScript to scroll an element into view.Occasionally I 'll get this BadStatusLine when reading text from an element.A couple times I 've gotten a socket error : I 'm scraping from Google without a proxy , so my first thought was that my IP address is recognized as a VPS and put under a 5-time page-manipulation limitation or something . But my initial research indicates that these errors would not arise from being blocked.Any insight into what these errors mean collectively , or on the necessary considerations when making HTTP requests from a VPS would be much appreciated.UpdateAfter a little thinking and looking into what a webdriver really is -- automated browser input -- I should have been confused about why remote_connection.py is making urllib2 requests at all . It would seem that the text method of the WebElement class is an `` extra '' feature of the python bindings that is n't part of the Selenium core . That does n't explain the above errors , but it may indicate that the text method should n't be used for scraping.Update 2I realized that , for my purposes , Selenium 's only function is getting the ajax content to load . So after the page loads , I 'm parsing the source with lxml rather than getting elements with Selenium , i.e . : However , page_source is yet another method that results in a call to urllib2 , and I consistently get the BadStatusLine error the second time I use it . Minimizing urllib2 requests is definitely a step in the right direction.Update 3Eliminating urllib2 requests by grabbing the source with javascript is better yet : ConclusionThese errors can be avoided by doing a time.sleep ( 10 ) between every few requests . The best explanation I 've come up with is that Google 's firewall recognizes my IP as a VPS and therefore puts it under a stricter set of blocking rules.This was my initial thought , but I still find it hard to believe because my web searches return no indication that the above errors could be caused by a firewall.If this is the case though , I would think the stricter rules could be circumvented with a proxy , though that proxy might have to be a local system or tor to avoid the same restrictions ."
"So what I 'm trying to do is fix some id3tags of mp3 files . It all works , except for files with any kind of accent , because os.walk seems to strip them.For example , I have the file 01.Co Słychać.mp3 , which in this code : Shows up as [ '01.Co Slychac.mp3 ' ] , later resulting in a 'No such file or directory ' error.How can this be fixed ?"
"I have two matrices , A and B : I want to index each row of A with each row of B , producing the slice : That is , I essentially want something like : Is there a way to fancy-index this directly ? The best I can do is this `` flat-hack '' :"
"I 'm running into a weird problem where using the apply function row-wise on a dataframe does n't preserve the datatypes of the values in the dataframe . Is there a way to apply a function row-wise on a dataframe that preserves the original datatypes ? The code below demonstrates this problem . Without the int ( ... ) conversion within the format function below , there would be an error because the int from the dataframe was converted to a float when passed into func.Here is the output from running the above code : Notice that even though the int_col column of df has dtype int64 , when values from that column get passed into function func , they suddenly have dtype numpy.float64 , and I have to use int ( ... ) in the last line of the function to convert back , otherwise that line would give an error.I can deal with this problem the way I have here if necessary , but I 'd really like to understand why I 'm seeing this unexpected behavior ."
"I have a list with lenght of : 370000 . In this list i have items like : `` a '' , `` y '' , `` Y '' , `` q '' , `` Q '' , `` p '' , `` P '' , , meaning this is a list of words but from time to time i get those single characters.I want to remove those characters from the list , i am pretty new in python but the first thing that came to my mind was to do something like : In a list with 370.000 items this method is taking a loooot of time . Seriously , a lot.Does anyone have another awesome idea on how to get a better performance ? Thanks in advance ."
"Python Pandas DataFrame has a to_latex method : Output : '\begin { tabular } { |l|c|c| } \n\hline\n { } & 0 \\\n\hline\n0 & 0 \\\n1 & 1 \\\n2 & 2 \\\n3 & 3 \\\n\hline\n\end { tabular } \n ' I would like to overlay this table on a matplotlib plot : However , I get this error : RuntimeError : LaTeX was not able to process the following string : '\begin { tabular } { |l|c|c| } 'My question is : How do I render output of Pandas 'to_latex ' method in a matplotlib plot ?"
"I was taking a look at the hierarchy of the built-in python exceptions , and I noticed that StopIteration and GeneratorExit have different base classes : Or in code : When I go to the specific description of each exception , I can read following : https : //docs.python.org/2/library/exceptions.html # exceptions.GeneratorExit exception GeneratorExit Raised when a generator ‘ s close ( ) method is called . It directly inherits from BaseException instead of StandardError since it is technically not an error.https : //docs.python.org/2/library/exceptions.html # exceptions.StopIteration exception StopIteration Raised by an iterator ‘ s next ( ) method to signal that there are no further values . This is derived from Exception rather than StandardError , since this is not considered an error in its normal application.Which is not very clear to me . Both are similar in the sense that they do not notify errors , but an `` event '' to change the flow of the code . So , they are not technically errors , and I understand that they should be separated from the rest of the exceptions ... but why is one a subclass of BaseException and the other one a subclass of Exception ? .In general I considered always that Exception subclasses are errors , and when I write a blind try : except : ( for instance calling third party code ) , I always tried to catch Exception , but maybe that is wrong and I should be catching StandardError ."
"Let 's say I have a python test using argparse , with several arguments : IP ( default : 127.0.0.1 ) enabled_features ( default : [ A , B , C ] ) Sometimes , I 'd like to change the default enabled_features , let 's say to [ A , B , C , D ] : depending on something I need the IP to find out ( so it ca n't really be a default value in the way argparse has default values ) only if the user has n't specified the `` enabled_features '' ... that 's what I have trouble to know ! So is there an attribute in the argparse classes to know , after : ... that an argument was actually specified by the user , i.e . that one has used something like : and not : Thanks !"
"Suppose a function with a mutable default argument : If I run this : Or this : Instead of the following one , which would be more logical : Why ?"
"My a* algorithm does n't always take the shortest path.In this image the robot has to traverse to the black squares , the river and trees are obstacles . The black line is the path it took , which is clearly not the shortest path as it should not have dipped.http : //imgur.com/GBoq9pyHere is my code for a* and the heuristic I am using : http : //pastebin.com/bEw8x0LxThanks for any help ! And feel free to ask me to clarify anything . Edit : removing the heuristic by returning 0 solves this problem . Which suggests the problem lies with my heuristic . Would anyone know what might be causing it ?"
"I am learning perl Inline : :Python library . In the example of cpan website , we haveIs it possible to put python code into string so that I can create the python code in the runtime ? For example , something like : We have a projects that will dynamically create python functions at the runtime . And we want to call these functions . Is py_eval ( ) the way to go ? Thanks in advance ."
"I am trying to find a simple way to create a form that allows the editing of two models with foreign key relationship simultaneously.After some research , it seems that Inline formsets come very close to what I want to do.The django documentation offers this example : And then , Let 's suppose Author has a second field , city . Can I use the fields argument to add a city to the form ? If inline formsets are not the way to go , is there another way that generates this joint form ? After some more research , I found django model Form . Include fields from related models from 2009 which hints that inline form sets might not be the way to go.I would be very much interested if there 's a default solution with a different factory ."
"The following is an example I found at the website for Doug Hellman in a file named `` masking_exceptions_catch.py '' . I ca n't locate the link at the moment . The exception raised in throws ( ) is discarded while that raised by cleanup ( ) is reported.In his article , Doug remarks that the handling is non-intuitive . Halfway expecting it to be a bug or limitation in the Python version at the time it was written ( circa 2009 ) , I ran it in the current production release of Python for the Mac ( 2.7.6 ) . It still reports the exception from cleanup ( ) . I find this somewhat amazing and would like to see a description of how it is actually correct or desirable behavior.Program output :"
"I 'm trying to do file upload to GAE - serverside code : With HTML , I can get it to work : But I must be able to do a multipart post request from Java . I have a flask project hosted on openshift ( flask , request.files [ 0 ] instead ) in which the Java code appears to work : But when I run it on GAE , im getting an index out of bounds on upload_files [ 0 ] when uploading - Where is the error ? I can not seem to find it , the code is so similar to the following which i have confirmed to work ( flask , openshift ) : UPDATE : The complete error log :"
"I 'm trying to make a templated string that will print values of a given dict . However , the key may or may not exist in the dict . If it does n't exist , I 'd like it to return an empty string instead.To demonstrate what I mean , let 's say I have a dict : And a template string : I 'd like the following output : But instead I get :"
"I have a < form > like this all the times . Is there any macro or equivalent in reStructuredText so that I can get the same result , for example ?"
"Input : array length ( Integer ) indexes ( Set or List ) Output : A boolean numpy array that has a value 1 for the indexes 0 for the others.Example : Input : array_length=10 , indexes= { 2,5,6 } Output : Here is a my simple implementation : Is there more elegant way to implement this ?"
"an unpopular but `` supported '' python hack ( see Guido : https : //mail.python.org/pipermail/python-ideas/2012-May/014969.html ) that enables __getattr__ usage on module attributes involves the following : On import , this the imported module becomes a class instance : However , in Python-2.7 , all other imported modules within the original module is set to None . In Python-3.4 , everything works : This feels like something to do with Imported modules become None when running a function , but , anyone knows why this happens internally ? It seems that if you store the original module ( without fully replacing it in Python-2 ) then everything continues to work :"
I have some questions about the performance of this simple python script : Here 's the output I 'm getting on linux : Why is urllib2 so much slower than asyncore in the first run ? And why does the discrepancy seem to disappear on the second run ? EDIT : Found a hackish solution to this problem here : Force python mechanize/urllib2 to only use A requests ? The five-second delay disappears if I monkey-patch the socket module as follows :
"Take the following example script : When I run this script with Python 2.7.11 I get : It appears that the @ classmethod decorator is preserved across the classes , but @ staticmethod is not.Python 3.4 behaves as expected : Why does Python2 not preserve @ staticmethod , and is there a workaround ? edit : taking two out of a class ( and retaining @ staticmethod ) seems to work , but this still seems strange to me ."
"I have built an API with Django Rest Framework . I want to change pagination for a better user experience . The problem : The client makes a call to request all posts . The request looks like : This returns the first page of posts . However , new posts are always being created . So when the user requests : The posts are almost always the same as page 1 ( since new data is always coming in and we are ordering by -created ) .Possible Solution ? I had the idea of sending an object id along with the request so that when we grab the posts . We grab them with respect to the last query . And when we paginate we filter where post_id < 12345But this only works assuming our post_id is an integer . Right now I 'm currently only using a basic ListAPIViewIs there a better way of paginating ? so that the next request for that user 's session does n't look like the first when new data is being created ."
"I want to play with the thread-bugs with PyGTK . I have this code so far : It does that NonGUIThreads are running for 3 seconds concurently . Afterwards , the window is shown and other threads are stoped ! After closing the window , threads are running again.How it is possible , that gtk.main ( ) is able to block other threads ? The threads do not depend on any lock , so they should work during the window is shown ."
"Is there an accepted naming convention for regular expressions in Python ? Or if there 's not , what are some suggestions on how to name them ? Usually I name them something like look_for_date or address_re but I 've read several places that using a suffix like '_re ' in a variable name is n't good . To me it seems like the regex needs something to indicate it 's a regex , since if you named it just date or address , you would n't be able to do stuff like this , which seems intuitive :"
"This is in reference to understanding , internally , how the probabilities for a class are predicted using LightGBM.Other packages , like sklearn , provide thorough detail for their classifiers . For example : LogisticRegression returns : Probability estimates.The returned estimates for all classes are ordered by the label ofclasses.For a multi_class problem , if multi_class is set to be “ multinomial ” the softmax function is used to find the predicted probability of eachclass . Else use a one-vs-rest approach , i.e calculate the probabilityof each class assuming it to be positive using the logistic function.and normalize these values across all the classes.RandomForest returns : Predict class probabilities for X.The predicted class probabilities of an input sample are computed asthe mean predicted class probabilities of the trees in the forest . Theclass probability of a single tree is the fraction of samples of thesame class in a leaf.There are additional Stack Overflow questions which provide additional details , such as for : Support Vector MachinesMultilayer PerceptronI am trying to uncover those same details for LightGBM 's predict_proba function . The documentation does not list the details of how the probabilities are calculated.The documentation simply states : Return the predicted probability for each class for each sample.The source code is below : How can I understand how exactly the predict_proba function for LightGBM is working internally ?"
"I updated my system ( Ubuntu 18.04 ) from Python 3.6 to Python 3.8 , and reset the defaults so that python3 now points to Python 3.8 ( and not 3.6 ) . However , since then , the terminal has refused to open using Ctrl + Alt + T , and other obvious methods such as clicking on the icon itself . When I run gnome-terminal - I get the following : I do n't know what this means but I guess it definitely points to the fact that something went wrong during the update . I understand that there are other existing threads on similar issues , but most of them were about updating from Python2 to Python3 , so I 'm not sure if they 're relevant . Could someone help , please ? Important Update : So , after reading this answer - I changed the gnome-terminal script 's first line to # ! /usr/bin/python3.6 instead of # ! /usr/bin/python3.8 - and that solves the problem.Also , when I type python3 in the terminal , I 'm greeted with Python 3.8.2 , as desired . The question remains - Why did this work ? What was the actual problem ? An explanation would help , so I really know what I 'm doing.Thanks !"
"I do n't understand how to implement the log-uniform probability distribution in Scipy . According to the comments of this post , it is possible to do so by defining only _pdf . Also , I know from this source the actual derivation for the pdf . However , I ca n't figure out where to put the a and b parameters and how to set a such that a > 0 . Also , please note that I want a and b to be the actual minimum and maximum values of the range . Please also note that , in the end , I really just want to be able to use the .rvs ( ) method so any trick with the uniform distribution is acceptable.Here is my curent ( non-working ) code :"
"I am trying to improve a code performance.I use Pandas 0.19.2 and Python 3.5.I just realized that the .loc writing on a whole bunch of values at a time has very different speed depending on dataframe initialization.Can someone explain why , and tell me what is the best initialization ? It would allow me speed up my code.Here is a toy example . I create 'similar ' dataframes.Then I time them : Have I done anything wrong ? ? ? ? Why is df2 performing so much faster than the others ? Actually in multi-index case it is much faster to set the elements one by one using .at . I implemented this solution in my code but I 'm not happy about it , I think there must be a better solution . I would prefer to keep my nice multi-index dataframes , but if I really need to go mono-index I 'll do it ."
"How would one expose `` static '' variables like thisvia the C API ? The only variable on the PyTypeObject that looks like it would work is tp_members , but I see no flag in the PyMemberDef to indicate that the member should be per-class , not per-instance.For a bit more clarification , since it may change the answer , I 'm trying to expose a C enum to Python such that the enumerationCan be accessed in Python as :"
I am trying to make a watch request using python as referred to in the google APIs but it does not work . I could not find a library or a package to use gmail.users ( ) function . How do I make a watch request using an access token ?
"What is the difference in Python between unpacking a function call with [ ] , with ( ) or with nothing ?"
"I am looking into how the intensity of a ring changes depending on angle . Here is an example of an image : What I would like to do is take a circle of values from within the center of that doughnut and plot them vs angle . What I 'm currently doing is using scipy.ndimage.interpolation.rotate and taking slices radially through the ring , and extracting the maximum of the two peaks and plotting those vs angle.However I do n't think this is doing what I 'm actually looking for . I 'm mostly struggling with how to extract the data I want . I have also tried applying a mask which looks like this ; to the image , but then I do n't know how to get the values within that mask in the correct order ( ie . in order of increasing angle 0 - 2pi ) Any other ideas would be of great help !"
"Yet another question about matplotlib 3d surfaces ... I have code which adds a scatter point to a matplotlib surface graph.The problem that I have is that the point always appears behind the surface , regardless of which angle you view it from . If I cobble an ( admittedly ugly ) version using 3 short lines to mark the same point , it is visible.I have turned off the depthshade function , so it is n't this . Can anybody explain what is going on and how I can correct it ? Here is a simplified version of the code :"
"I have a the following strings defined which are specifying a python module name , a python class name and a static method name.I would like to invoke the static method specified by method_name variable.How do i achieve this in python 2.7+"
"Suppose there is a production database , there is some data in it . I need to migrate in the next tricky case.There is a model ( already in db ) , say Model , it has foreign keys to other models.And we need to create one more model ModelY to which Model should refer . And when creating a Model , an object should have some default value related to some ModelY object , which is obviously not yet available , but we should create it during migration.So the migration sequence should be : Create ModelY tableCreate a default object in this table , put its id somewhereCreate a new field y in the Model table , with the default value takenfrom the previous paragraphAnd I 'd like to automate all of this , of course . So to avoid necessity to apply one migration by hands , then create some object , then write down it 's id and then use this id as default value for new field , and only then apply another migration with this new field.And I 'd also like to do it all in one step , so define both ModelY and a new field y in the old model , generate migration , fix it somehow , and then apply at once and make it work.Are there any best practices for such case ? In particular , where to store this newly created object 's id ? Some dedicated table in same db ?"
"pop is a great little function that , when used on dictionaries ( given a known key ) removes the item with that key from the dictionary and also returns the corresponding value . But what if I want the key as well ? Obviously , in simple cases I could probably just do something like this : But if , say , I wanted to pop the key-value pair with the lowest value , following the above idea I would have to do this ... ... which is hideous as I have to do the operation twice ( obviously I could store the output from min in a variable , but I 'm still not completely happy with that ) . So my question is : Is there an elegant way to do this ? Am I missing an obvious trick here ?"
"Let 's say I want to initialize a 2D Python list with all 0 's , I 'd do something like : Then I start modifying values in the first list ... But that affects the first item of ALL lists for some reason : What 's going on with that ? Why is it making a deep copy of the inner list , but a shallow copy of the outer list ?"
"I have a blank grid of 100 , 100 tiles . Start point is ( 0,0 ) , goal is ( 99,99 ) . Tiles are 4-way connections.My floodfill algorithm finds the shortest path in 30ms , but my A* implementation is around 10x slower.Note : A* is consistently slower ( 3 - 10x ) than my floodfill , no matter what kind of size of grid or layout . Because the floodfill is simple , then I suspect I 'm missing some kind of optimisation in the A*.Here 's the function . I use Python 's heapq to maintain a f-sorted list . The 'graph ' holds all nodes , goals , neighbours and g/f values ."
"In a program I 'm writing , I need to multiply each element of a list with all the other elements , like this : I 've been messing around with loops for a while now , but I ca n't seem to get it to work . This is what I have so far ( does n't work , I know why it does n't work , just asking for advice ) :"
How to annotate parameters of the variadic function ? Example : Are there any typing annotations for that ?
"I would like to use with on an object that uses __getattr__ to redirect calls.Howerver , this does not seem to work with the method __enter__Please consider the following , simplified code to reproduce the error : Output : Why does n't it fall back to __getattr__ since __enter__ does not exist on the object ? Of course , I could make it work if I just create an __enter__ method and redirect from there instead , but I 'm wondering why it does n't work otherwise.My python version is the following :"
"Usually , when raw_input asks you to type something in and press Return , feedback is printed on a new line . How can I print over the prompt 's line ? Could a CR work in this case ? Demo : Simulated output after typing an answer and pressing Return : Desired output :"
I 'm using a slightly altered version of the pairwise recipe from itertools which looks like thisNow it turns out I need to run the code with python 2.5 where the next ( ) function throws the following exception : < type 'exceptions.NameError ' > : global name 'next ' is not definedIs there a way to use next ( ) with python 2.5 ? Or how do I need to modify the function to make it work anyhow ?
"A generic function is dispatched based on the type of all its arguments . The programmer defines several implementations of a function . The correct one is chosen at call time based on the types of its arguments . This is useful for object adaptation among other things . Python has a few generic functions including len ( ) .These packages tend to allow code that looks like this : A less dumb example I 've been thinking about lately would be a web component that requires a User . Instead of requiring a particular web framework , the integrator would just need to write something like : Python has a few generic functions implementations . Why would I chose one over the others ? How is that implementation being used in applications ? I 'm familiar with Zope 's zope.component.queryAdapter ( object , ISomething ) . The programmer registers a callable adapter that takes a particular class of object as its argument and returns something compatible with the interface . It 's a useful way to allow plugins . Unlike monkey patching , it works even if an object needs to adapt to multiple interfaces with the same method names ."
"Is there a way to force a Python 3 unittest to fail , rather than simply print a warning to stderr , if it causes any ResourceWarning ? I 've tried the following : Which results in this output from unittest : Note the `` Exception ignored '' message . I 'd rather the test failed , instead of requiring me to read its output looking for ResourceWarnings ."
"I have a running installation of Keras & Theano on Windows ( by following this tutorial ) . Now I 've tried to switch the backend to Tensorflow which worked quite fine.The only issue I have , is that Tensorflow does not detect my GPU , which Theano in contrast does : yields no results but when running with Theano backend , it works quite nicely : Apparently there is some configuration missing , but I do n't know what . For Theano to run correctly , I needed a file called ~/.theanorc with the following content : Maybe something similar is missing or maybe I need to add environment variables like for Theano ? . Possibly related question on Linux ( ? ) .The full installation log ( which included a strange exception ) can be found in this Gist.Any ideas , how to make the GPU visible to Tensorflow ?"
i have string how can i convert to datetime object and store it in datastore ?
"I 've been reading up on python 's special class methods in Dive into Python , and it seems like some methods have odd or inconsistent syntax.To get the items from a dictionary you would call the dictionary class method items ( ) However , to determine the number of keys in that dictionary you would call len ( ) and supply it with the dictionary as an argument.I always assumed that methods like len ( ) were n't actually part of whatever class you called them on given their syntax but after reading chapter 5 of Dive into Python I see that len ( ) actually does result in a dictionary method being called.So why is n't it and methods like it called like a typical class method ? Is there a convention I 'm unaware of ?"
I am developing a simple pyramid application where I am using JQuery to do AJAX requests . I have until now had my javascript code within my chameleon templates . Now I want to extract my javascript into another location ( e.g . as static resources ) .My problem is that I find my javascript code relies on dynamically generated content like so : The dynamic element being : Which is calling the route_url method of the request object within the template.Is there a recognised pattern for separating such javascript files into their own templates and providing routes and views for them or do I simply keep my javascript in my page template ?
"If we make a pathological potato like this : We can break sets and dicts this way ( note : it 's the same even if __eq__ returns True , it 's mucking with the hash that broke them ) : Also len ( { p : 0 , p : 0 } ) == 2 , and { p : 0 } [ p ] raises KeyError , basically all mapping related stuff goes out the window , as expected . But what I did n't expect is that we ca n't break listsWhy is that ? It seems that list.__contains__ iterates , but it 's first checking identity before checking equality . Since it is not the case that identity implies equality ( see for example NaN object ) , what is the reason for lists short-circuiting on identity comparisons ?"
"I have a dict where the values are is a list , for example ; In this example , the max items in a list is 4 , but it could be greater than that.I would like to convert it to a dataframe like :"
"I have an array A whose shape is ( N , N , K ) and I would like to compute another array B with the same shape where B [ : , : , i ] = np.linalg.inv ( A [ : , : , i ] ) .As solutions , I see map and for loops but I am wondering if numpy provides a function to do this ( I have tried np.apply_over_axes but it seems that it can only handle 1D array ) .with a for loop : with map :"
"Trying to access the price of a product , Using Docs . But getting Attribute error.To see all attributes of strategy I doSo fetch_for_product is not in attributes of strategy . Now how can I access the price of a particular product ?"
"I have a list of lists containing [ yyyy , value ] items , with each sub list ordered by the increasing years . Here is a sample : What I need is to insert all the missing years between min ( year ) and max ( year ) and to make sure that the order is preserved . So , for example , taking the first sub-list of A : should look like : Moreover , if any sublist contains only a single item then the same process should be applied to it so that the original value preserves its supposed order and rest of the min to max ( year , value ) items are inserted properly.Any ideas ? Thanks ."
"I have a dict subclass that adds new methods and features , The main thing that I wanted to support is recursive update which works by updating every nested dict one by one unlike the dict.update method.I 'm using the copy.deepcopy function just like any other object , the problem is that it 's not working when I added attribute access to this class : Now I get this error : The copy.deepcopy function is trying to use a __deepcopy__ method on this object : Why after adding the __getattr__ this happened ? is there a way to fix it without implementing a __deepcopy__ method ? I never used the __deepcopy__ method so I tried adding one , and this is what I have : Is this the right way to implement __deepcopy__ ?"
"Say I define this descriptor : And I use it in this : So value is a class attribute and is shared by all instances.Now if I define this : In this case value is an instance attribute and is not shared by the instances.Why is it that when value is a descriptor it can only be a class attribute , but when value is a simple integer it becomes an instance attribute ?"
"First of all , the dbf module is great . I 've been using it with great success . I 'm trying to open a dbf file on a network share which is a read-only file system . When I try to open it like this , I get an error which says that the .dbf file is read-only . Looking at the documentation , it looks like there 's a way to open a table in read-only mode , but I ca n't figure it out . If you have a second , would you be able to help me out ? Thanks ! Kyle"
I am learning Cython . I have problem with passing numpy arrays to Cython and do n't really understand what is going on . Could you help me ? I have two simple arrays : I want to compute a dot product of them . In python/numpy everything works fine : I translated the code to Cython ( as here : http : //docs.cython.org/src/tutorial/numpy.html ) : It compiled with no problems but returns an error : Could you tell me why and how to do it correctly ? Unfortunately Google was not helpful ... Thanks !
"Using SymPy to find a derivative ( see this question : https : //math.stackexchange.com/questions/726104/apply-chain-rule-to-vector-function-with-chained-dot-and-cross-product ) , I came up with this code : Which yields this beauty : there are several multiple occurrences of the sqrts and pows of the same numbers , which could be computed once to improve readability and time of execution . But I do not know how ... Q1 : Do you know of a way to make sympy do this automatically ? Q2 : Do you know of a way to postprocess this code with an other tool ? Q3 : Can gcc optimize this at compile time ? Why ?"
"What I would like to do : I am using macOS and Anaconda 2 . I would like to install a Python package ( specifically PyTorch ) from source . I would like to install all the dependencies and the package itself within an Anaconda environment.I do n't want this Anaconda environment to be the default/ root Anaconda environment , but an environment I particularly created for installing this package and its dependencies from source.What I have done : First , I created the environment as followsNow , the instructions for installing PyTorch from source are as follows : Now , my questions are : Following this instructions , requires me to specify anaconda root directory for the CMAKE_PREFIX_PATH . What should that directory be given that I want everything set-up in my_env ? Is it reasonable to create an extra environment for a package installed from source and its dependencies ? Why would one do or not do it ? My motivation is mainly fear that one day I may screw my system up big time and hence want things to be cleanly separated . If you can only answer one of the two questions , that is already greatly appreciated . Thanks !"
"I have a list of values like : and I want to try all combinations on this list like : etc.What would be the most straightforward way to get all these possible combinations of operations in the most succinct way possible ? I would imagine having two lists , [ 1,2,3,4 ] and [ + , * , - , / ] and then taking all combinations of the numbers of all lengths , and then filling in the blanks with all combinations.So selecting [ 1 , 2 , 3 ] and then selecting all permutations of the operations and combining them together . This seems messy and I 'm hoping there 's a clearer way to code this ?"
"I 'm trying to build a class that inherits methods from Python 's list , but also does some additional things on top ... it 's probably easier just to show code at this point ... My problem is this ... if I want to perform the same kind of object admission tests on insert ( ) as I did on append ( ) , I ca n't find a way to code the new methods without to sacrificing support for one list expansion method ( i.e . list.append ( ) , list.insert ( ) , or list.extend ( ) ) . If I try to support them all , I wind up with recursive loops . What is the best way around this problem ? EDIT : I took the suggestion about subclassing collections.MutableSequence instead of Python 's list ( ) The resulting code ... posting here in case , it helps someone ..."
"I find a strange statement when I 'm reading PIL document . In 1.1.6 and later , load returns a pixel access object that can be used to read and modify pixels . The access object behaves like a 2-dimensional array , so you can do : What does pix [ x , y ] mean here ? It 's not slicing syntax because , used rather than : ."
"I 'm studying Codility Counting Lesson ( https : //codility.com/media/train/2-CountingElements.pdf ) and I need help to understand the fastest solution.I am wondering why the difference is divided by 2 in line 8 d //= 2 ? Should n't the difference be sufficient to find the elements that we can swap between the arrays ? The Problem : You are given an integer m ( 1 < m < 1000000 ) and two non-empty , zero-indexed arrays A and B of n integers , a0 , a1 , ... , an−1 and b0 , b1 , ... , bn−1 respectively ( 0 < ai , bi < m ) . The goal is to check whether there is a swap operation which can be performed on these arrays in such a way that the sum of elements in array A equals the sum of elements in array B after the swap . By swap operation we mean picking one element from array A and one element from array B and exchanging them.The solution :"
"I am using PyAMF to transfer a dynamically generated large image from Flex to Django.On the Django side i receive the encodedb64 data as a parameter : My Item model as an imagefield.What i have trouble to do is saving the data as the File Django Field.That would not work because my File object from StringIO misses some properties such as mode , name etc.I also think that using StringIO will load the image data completely in memory which is bad so i may just give up on the AMF for this particular case and use POST.What do you think ?"
"I have an image , stored in a numpy array of uint8s , of shape ( planes , rows , cols ) . I need to compare it to the values stored in a mask , also of uint8s , of shape ( mask_rows , mask_cols ) . While the image may be very large , the mask is usually smallish , normally ( 256 , 256 ) and is to be tiled over image . To simplify the code , lets pretend that rows = 100 * mask_rows and cols = 100 * mask_cols.The way I am currently handling this thresholding is something like this : The largest array I can process this way before getting slapped in the face with a MemoryError is a little larger than ( 3 , 11100 , 11100 ) . The way I figured it , doing things this way I have up to three ginormous arrays coexisting in memory : image , the tiled mask , and my return out . But the tiled mask is the same little array copied over and over 10,000 times . So if I could spare that memory , I would use only 2/3 the memory , and should be able to process images 3/2 larger , so of size around ( 3 , 13600 , 13600 ) . This is , by the way , consistent with what I get if I do the thresholding in place withMy ( failed ) attempt at exploiting the periodic nature of mask to process larger arrays has been to index mask with periodic linear arrays : For small arrays it does produce the same result as the other one , although with something of a 20x slowdown ( ! ! ! ) , but it terribly fails to perform for the larger sizes . Instead of a MemoryError it eventually crashes python , even for values that the other method handles with no problems.What I think is happening is that numpy is actually constructing the ( planes , rows , cols ) array to index mask , so not only is there no memory saving , but since it is an array of int32s , it is actually taking four times more space to store ... Any ideas on how to go about this ? To spare you the trouble , find below some sandbox code to play around with :"
Why Pycrypto AES decryption gives different output when decrypted with AES object used for encryption and right output when decrypted with AES object used solely for decryption ?
"I created a python 3.3 app on RedHat 's Openshift cloud service . By default it has setup.py for my project . I 'm learning Udemy course called `` Build a SaaS app with Flask '' ( source code ) Now I wanted to use python-click , as recommended by the course . It needs another setup.py for cli project ; so to put that file in the project root folder , I renamed it to setup_cli.py . Now there are two files : setup.py and setup_cli.py.Pip install seems to automatically look into setup.py.Can pip install -- editable be used to point to setup_cli.py ?"
"According to sklearn.pipeline.Pipeline documentation , The pipeline has all the methods that the last estimator in the pipeline has , i.e . if the last estimator is a classifier , the Pipeline can be used as a classifier . If the last estimator is a transformer , again , so is the pipeline.The following example creates a dummy transformer with a custom , dummy function f : I was expecting to be able to access the f function of the C transformer , however calling ppl.f ( ) results in AttributeError : 'Pipeline ' object has no attribute ' f'Am I misinterpreting the documentation ? Is there a good and reliable way to access the last transformer 's functions ?"
"A distribution is beta-binomial if p , the probability of success , in a binomial distribution has a beta distribution with shape parameters α > 0 and β > 0 . The shape parameters define the probability of success.I want to find the values for α and β that best describe my data from the perspective of a beta-binomial distribution . My dataset players consist of data about the number of hits ( H ) , the number of at-bats ( AB ) and the conversion ( H / AB ) of a lot of baseball players . I estimate the PDF with the help of the answer of JulienD in Beta Binomial Function in PythonNext , I write a loglikelihood function that we will minimize.Now , I want to write a function that minimizes loglike_betabinomThe result is [ -6.04544138 2.03984464 ] , which implies that α is negative which is not possible . I based my script on the following R-snippet . They get [ 101.359 , 287.318 ] ..Can someone tell me what I am doing wrong ? Help is much appreciated ! !"
My routes are as follows : and Middleware is something similar to
"My essential problem is that I ca n't get -- no-site-packages to `` work . `` I have read a bunch of posts on SO , including this post . I 'm a huge Ubuntu noob , and not much better when it comes to how python interacts with the os.Other posts suggested that printenv would display PYTHONPATH When I am in my virtualenv , printenv does n't appear to list `` PYTHONPATH '' , but it 's quite possible that what I should be looking for is a particular dir that I 'm not aware of instead of the uppercase letters.What I have noticed , however , is that when I run the python shell within the virtualenv , all of my global packages are listed.I 'm not sure if this is an issue with virtualenv , .bashrc , Ubuntu or my brain . Any help would be greatly appreciated.If there 's some kind , knowledgeable soul out there who is willing to help me out in a sort of back-and-forth process , I 'd be very grateful . I have n't listed any of my output because , honestly , aside from the above , I 'm not entirely sure where to start.Edit in response to comments ; UTC : 07:41 19 Nov 2015Starting the environment : Entering the environment : System Packages : sys.path : You can clearly see that site-packages is included in sys.path which is super annoying.Python3 seems slightly better , not that I know how to use pip freeze with it ( or even if I can ) : Next edit . My mind is bottled further . UTC 08:00 19 Nov 2015Because I 'm a conspiracy theorist , I tried creating a venv outside of Dropbox.What ... the ... heck ... ? I mean , I suppose that 's some sort of progress ? Maybe there 's something to do with my filenames ( \ $ \ $ \ P/ ) that is causing issues ? From what I 've read , as long as I sudo rm -rf venv it should make no difference that I have created and deleted virtualenvs in the same directory previously . Is that right ? Argh ."
"I am running a very simple experiment with ColumnTransformer with an intent to transform an array of columns , [ `` a '' ] in this example : Which gives me : Obviously , TfidfVectorizer can do fit_transform ( ) on its own : What could be a reason for such an error and how to correct for it ?"
"When using LabelPropagation , I often run into this warning ( imho it should be an error because it completely fails the propagation ) : /usr/local/lib/python3.5/dist-packages/sklearn/semi_supervised/label_propagation.py:279 : RuntimeWarning : invalid value encountered in true_divide self.label_distributions_ /= normalizerSo after few tries with the RBF kernel , I discovered the paramater gamma has an influence.EDIT : The problem comes from these lines : I do n't get how label_distributions_ can be all zeros , especially when its definition is : Gamma has an influence on the graph_matrix ( because graph_matrix is the result of _build_graph ( ) that call the kernel function ) . OK . But still . Something 's wrongOLD POST ( before edit ) I remind you how graph weights are computed for the propagation : W = exp ( -gamma * D ) , D the pairwise distance matrix between all points of the dataset.The problem is : np.exp ( x ) returns 0.0 if x very small.Let 's imagine we have two points i and j such that dist ( i , j ) = 10.In practice , I 'm not setting gamma manually but I 'm using the method described in this paper ( section 2.4 ) .So , how would one avoid this division by zero to get a proper propagation ? The only way I can think of is to normalize the dataset in every dimension , but we lose some geometric/topologic property of the dataset ( a 2x10 rectangle becoming a 1x1 square for example ) Reproductible example : In this example , it 's worst : even with gamma = 20 it fails ."
"In Stanford Dependency Manual they mention `` Stanford typed dependencies '' and particularly the type `` neg '' - negation modifier . It is also available when using Stanford enhanced++ parser using the website . for example , the sentence : `` Barack Obama was not born in Hawaii '' The parser indeed find neg ( born , not ) but when I 'm using the stanfordnlp python library , the only dependency parser I can get will parse the sentence as follow : and the code that generates it : Is there a way to get similar results to the enhanced dependency parser or any other Stanford parser that result in typed dependencies that will give me the negation modifier ?"
"I would like to fill regex variables with string.args variable is now a dict with { `` action '' : '' delete '' } .How i can reverse this process ? With args dict and regex pattern , how i can obtain the string `` /robert/delete/ '' ? it 's possible to have a function just like this ? Thank you"
"I 've been thinking about what I would miss in porting some Python code to a statically typed language such as F # or Scala ; the libraries can be substituted , the conciseness is comparable , but I have lots of python code which is as follows : Where the decorators do a huge amount : replacing the methods with callable objects with state , augmenting the class with additional data and properties , etc.. Although Python allows dynamic monkey-patch metaprogramming anywhere , anytime , by anyone , I find that essentially all my metaprogramming is done in a separate `` phase '' of the program . i.e . : These phases are basically completely distinct ; I do not run any application level code in the decorators , nor do I perform any ninja replace-class-with-other-class or replace-function-with-other-function in the main application code . Although the `` dynamic '' ness of the language says I can do so anywhere I want , I never go around replacing functions or redefining classes in the main application code because it gets crazy very quickly . I am , essentially , performing a single re-compile on the code before i start running it.The only similar metapogramming i know of in statically typed languages is reflection : i.e . getting functions/classes from strings , invoking methods using argument arrays , etc . However , this basically converts the statically typed language into a dynamically typed language , losing all type safety ( correct me if i 'm wrong ? ) . Ideally , I think , I would have something like the following : Essentially , you would be augmenting the compilation process with arbitrary code , compiled using the normal compiler , that will perform transformations on the main application code . The point is that it essentially emulates the `` load , transform ( s ) , execute '' workflow while strictly maintaining type safety.If the application code are borked the compiler will complain , if the transformer code is borked the compiler will complain , if the transformer code compiles but does n't do the right thing , either it will crash or the compilation step after will complain that the final types do n't add up . In any case , you will never get the runtime type-errors possible by using reflection to do dynamic dispatch : it would all be statically checked at every step.So my question is , is this possible ? Has it already been done in some language or framework which I do not know about ? Is it theoretically impossible ? I 'm not very familiar with compiler or formal language theory , I know it would make the compilation step turing complete and with no guarantee of termination , but it seems to me that this is what I would need to match the sort of convenient code-transformation i get in a dynamic language while maintaining static type checking.EDIT : One example use case would be a completely generic caching decorator . In python it would be : While higher order functions can do some of this or objects-with-functions-inside can do some of this , AFAIK they can not be generalized to work with any function taking an arbitrary set of parameters and returning an arbitrary type while maintaining type safety . I could do stuff like : But all the casting completely kills type safety.EDIT : This is a simple example , where the function signature does not change . Ideally what I am looking for could modify the function signature , changing the input parameters or output type ( a.l.a . function composition ) while still maintaining type checking ."
"I 'm trying to modify a datetime based on a timezone on save and on load the following way : An input datetime , along with a input timezone are sent to the server and the server should update the datetime to reflect the timezone . So when it saves in the database ( PostregSQL ) , the UTC time is saved ( after the offset caused by the timezone , of course ) .To reflect this here 's a simpler example that fails in the same way : Some imports : Creating the two inputs : As you can see , the timezone is not 5h ( 24 - 19 = 5 ) , but 4h56 . At this stage I 'm thinking that 's OK , it may be related to the Daylight Saving Time.Now I 'm replacing the timezone on the input date : As expected , the time has n't changed , but the timezone has , which is fine.I 'll assign this value to a project ( the launch_date is a DateTimeField without any specific option ) : Now I 'll save this into ( and refresh from ) the database , leaving Django/PostgreSQL do the maths : As expected the date is now 4h56 ahead of the previous date . I 'm trying now to get back the local time : This time , the offset is perfectly 5h . And I 'm missing 4 minutes.3 questions here : Where is this 4 minutes coming from ? Why is astimezone not using the 4 minutes as well ? How can a datetime be converted to UTC , saved , loaded and converted back to local ?"
"When I ran the code below , the memory was increasing . However if I deleted time.sleep ( 3 ) , it was 0.1 in top and never increased.It seems process not be terminated correctly , but why ? Code ( Python 2.7.11 ) :"
"Hi ! I need to implement a query using GeoAlchemy to get all the Points that are near a given Point ( for example , within a 10 meter radius ) . For storing points , i am using Geography fields in my PostGIS database . From the SQLAlchemy documentation I have figured out that i would need to use the ST_DWithin function , but i 'm not exactly sure how to implement this function in my Flask application.Here are the relevant code snippets : Thank you very much for your time !"
"I have a python script that is using the following to restart : Most the time this works fine , but occasionally the restart fails with a no module named error . Examples : Edit : I attempted gc.collect ( ) as suggested by andr0x and this did not work . I got the same error : Edit 2 : I tried sys.stdout.flush ( ) and im still getting the same error . I 've noticed I am only every getting between 1-3 successful restarts before an error occurs ."
"I am currently trying to create the following database schema with SQLAlchemy ( using ext.declarative ) : I have a base class MyBaseClass which provides some common functionality for all of my publicly accessible classes , a mixin class MetadataMixin that provides functionality to query metadata from imdb and store it.Every class that subclasses MetadataMixin has a field persons which provides a M : N relationship to instances of the Person class , and a field persons_roles which provides a 1 : N relationship to an object ( one for each subclass ) which stores the role a concrete Person plays in the instance of the subclass.This is an abbreviated version of what my code looks like at the moment : What I 'm trying to do is to create a generic creator function for association_proxy that creates either a PersonMovieRole or a PersonShowRole object , depending on the class of the concrete instance that a Person is added to . What I 'm stuck on at the moment is that I do n't know how to pass the calling class to the creator function.Is this possible , or is there maybe even an easier way for what I 'm trying to accomplish ?"
"I am trying to run my Django application with two db 's ( 1 master , 1 read replica ) . My problem is if I try to read right after a write the code explodes . For example : p = Product.objects.create ( ) Product.objects.get ( id=p.id ) ORIf user is redirected to Product'sdetails pageThe code runs way faster than the read replica . And if the read operation uses the replica the code crashes , because it did n't update in time . Is there any way to avoid this ? For example , the db to read being chosen by request instead of by operation ? My Router is identical to Django 's documentation :"
"I am starting out on learning Python 3.I am wondering how to perform a custom sort . For instance , I might want to sort a list of animals in the following manner : sort by first character ascending , then by length descending , then by alphanumeric ascending.A list made up of `` ant '' , `` antelope '' , `` zebra '' , `` anteater '' when properly sorted would become `` anteater '' , `` antelope '' , `` ant '' , `` zebra '' .I have read a little on the docs but do n't quite get the `` key '' argument to the sort method . Any examples ? PS : this is n't a College assignment question . I am just looking to play around with python a bit.I learnt java a long , long time ago and might have implemented the custom sort something like the following :"
"If a GSettings schema exists and has been compiled , there is usually no problem reading from it . However , if it does n't exist , an error is usually thrown which can not be handled . Try this in a Python file or console : I 'm being as broad as possible with the except , but this is the error that is thrown . ( process:10248 ) : GLib-GIO-ERROR ** : Settings schema 'com.example.doesnotexist ' is not installedWhat I basically want to do is find out if the com.example.doesnotexist schema exists or not ; if not , then tell the user to run my setup script before using my application . Any other suggestions on doing this would be welcome ."
"I have the following numpy array and function : I evaluate the function for every element in my numpy array i using two different appraoches : Approach 1 : Approach 2 : The two approaches result in two different answers . They are different because after element 50 , approach 1 starts to return inf and nan . Approach 2 also does not give the correct value for every element of i . However , it is able to calculate more of them . Approach 2 fails for i > = 64.Both approaches give me an answer in approximately the same time ( 0.7 s for len ( i ) = 15000 , determined using timeit ) . What I do not understand are the different results . This is because I learned to avoid for loops in python as much as possible . This does not seems to be the case this time.The idea that it had to do with memory also crossed my mind . However , evaluating one single element , i.e . print func ( 0 , 64 ) also returns 0 . ( equal to the output of approach 2 ) .What is going on ?"
"I am using OpenCV to estimate a webcam 's intrinsic matrix from a series of chessboard images - as detailed in this tutorial , and reverse project a pixel to a direction ( in term of azimuth/elevation angles ) .The final goal is to let the user select a point on the image , estimate the direction of this point in relation to the center of the webcam , and use this as DOA for a beam-forming algorithm.So once I have estimated the intrinsic matrix , I reverse project the user-selected pixel ( see code below ) and display it as azimuth/elevation angles.My problem is that I 'm not sure whether my results are coherent . The major incoherence is that , the point of the image corresponding to the { 0,0 } angle is noticeably off the image center , as seen below ( camera image has been replaced by a black background for privacy reasons ) : I do n't really see a simple yet efficient way of measuring the accuracy ( the only method I could think of was to use a servo motor with a laser on it , just under the camera and point it to the computed direction ) .Here is the intrinsic matrix after calibration with 15 images : I get an error of around 0.44 RMS which seems satisfying.My calibration code : EDIT : another test method would be to use a whiteboard with known angles points and estimate the error by comparing with experimental results , but I do n't know how to set up such a system"
"What 's the most efficient way to calculate a rolling ( aka moving window ) trimmed mean with Python ? For example , for a data set of 50K rows and a window size of 50 , for each row I need to take the last 50 rows , remove the top and bottom 3 values ( 5 % of the window size , rounded up ) , and get the average of the remaining 44 values.Currently for each row I 'm slicing to get the window , sorting the window and then slicing to trim it . It works , slowly , but there has to be a more efficient way.Example for a window size of 5 . For each row we look at the last 5 rows , sort them and discard 1 top and 1 bottom row ( 5 % of 5 = 0.25 , rounded up to 1 ) . Then we average the remaining middle rows.Code to generate this example set as a DataFrameExample code for the naive implementationA note about DataFrame vs list vs NumPy arrayJust by moving the data from a DataFrame to a list , I 'm getting a 3.5x speed boost with the same algorithm . Interestingly , using a NumPy array also gives almost the same speed boost . Still , there must be a better way to implement this and achieve an orders-of-magnitude boost ."
"In the following code , why does n't Python compile f2 to the same bytecode as f1 ? Is there a reason not to ?"
I am reading Python Programming by John Zelle and I am stuck on one the exercises shown in the picture below.You can view my code below . I know the code is very ugly . ( Any tips are appreciated ) Here 's my code so far : When I run the program the regression line never appears to be the real line of best fit . I believe I am interpreting the regression equation incorrectly in my code . What needs to be changed to get the correct regression line ?
"I am trying to send a message with custom properties using the Python binding of Qpid Proton , but I ca n't find the right way to do it ... Results in ... Any help welcome ! TIA , Thomas ."
"So I recently asked a question about memoization and got some great answers , and now I want to take it to the next level . After quite a bit of googling , I could not find a reference implementation of a memoize decorator that was able to cache a function that took keyword arguments . In fact , most of them simply used *args as the key for cache lookups , meaning that it would also break if you wanted to memoize a function that accepted lists or dicts as arguments . In my case , the first argument to the function is a unique identifier by itself , suitable for use as a dict key for cache lookups , however I wanted the ability to use keyword arguments and still access the same cache . What I mean is , my_func ( 'unique_id ' , 10 ) and my_func ( foo=10 , func_id='unique_id ' ) should both return the same cached result.In order to do this , what we need is a clean and pythonic way of saying 'inspect kwargs for whichever keyword it is that corresponds to the first argument ) ' . This is what I came up with : The crazy thing is that this actually works . Eg , if you decorate like this : Then from your code you can call either FooBar ( '12345 ' , 20 ) or FooBar ( irrelevant=20 , unique_id='12345 ' ) and actually get the same instance of FooBar . You can then define a different class with a different name for the first argument , because it works in a general way ( ie , the decorator does n't need to know anything specfic about the class it 's decorating in order for this to work ) .The problem is , it 's an ungodly mess ; - ) It works because inspect.getcallargs returns a dict mapping the defined keywords to the arguments you supply it , so I supply it some phony arguments and then inspect the dict for the first argument that got passed . What would be much nicer , if such a thing were to even exist , is an analogue to inspect.getcallargs that returned both kinds of arguments unified as a list of the arguments instead of as a dict of the keyword arguments . That would allow something like this : The other way I can see of tackling this would be using the dict provided by inspect.getcallargs as the lookup cache key directly , but that would require a repeatable way to make identical strings from identical hashes , which is something I 've heard ca n't be relied upon ( I guess i 'd have to construct the string myself after sorting the keys ) .Does anybody have any thoughts on this ? Is it Wrong to want to call a function with keyword arguments and cache the results ? Or just Very Difficult ?"
"For a project in Django I have to use two databases : default and remote . I have created routers.py and everything works fine.There was a requirement to create a table on the remote database and I created migration , run it and the table django_migrations was created . I want to have only one table django_migrations , in the default database.The relevant part of routers.py is here : I run the migration like this : Now when I do : I get the following warning : You have 1 unapplied migration ( s ) . Your project may not work properly until you apply the migrations for app ( s ) : my_app . Run 'python manage.py migrate ' to apply them.The tables for my_app are created in the remote database , and in django_migrations inside the remote database the migrations are marked as applied.EDIT : How to force Django to use only one table django_migrations , but still apply the migrations into different databases ? How to apply the migrations in different databases so that no warnings are raised ?"
"I have the following pandas DataFrame : first_column is a binary column of 0s and 1s . There are `` clusters '' of consecutive ones , which are always found in pairs of at least two . My goal is to create a column which `` counts '' the number of rows of ones per group : This sounds like a job for df.loc ( ) , e.g . df.loc [ df.first_column == 1 ] ... somethingI 'm just not sure how to take into account each individual `` cluster '' of ones , and how to label each of the unique clusters with the `` row count '' . How would one do this ?"
"I was trying out python -mtimeitso I put python -mtimeit `` n = 0 ; while n < 10 : pass '' Then an invalid syntax error showed up . same with semicolon and for loop.However , when I try semicolon and loop individually . Both worked fine . Why is this so and how can I test while loop in timeit ? Thank you very much !"
"I am creating a Python Flask app and created the decorator and views below . The decorator works great when viewing the index , but when you logout and it redirects using the url_for index it throws a builderror . Why wouldAny ideas ? The error is : BuildError : ( 'index ' , { } , None )"
"I want to parse some words and some numbers with pyparsing . Simple right . The code above prints [ '123 ' , 'abc ' , '456 ' , ' 7 ' , 'd ' ] . So everything worked . Now I want to do some work with these parsed values . For this task , I need to know if they matched A or B . Is there a way to distinguish between these two . The only thing what I found after some research was the items method of the ParseResults class . But it only returns [ ( ' A ' , ' 7 ' ) , ( ' B ' , 'd ' ) ] , only the last two matches . My plan / goal is the following : How do I distinguish between A and B ?"
When I do a data [ genres ] .sum ( ) I get the following resultBut when I try to sort on the sum using np.sortI get the following resultThe expected result should be like this : It seems like numpy is ignoring the genre column.Could somebody help me understand where I am going wrong ?
I have a pandas series of dates and cumulative values like this : Can I use pandas to convert them in `` deltas '' like this ? Or should I just do it manually ?
"Is there any clever way to avoid making a costly query with an IN clause in cases like the following one ? I 'm using Google App Engine to build a Facebook application and at some point I ( obviously ) need to query the datastore to get all the entities that belong to any of the facebook friends of the given user.Suppose I have a couple of entities modeled as such : andAt some point I query Facebook to get the list of friends of a given user and I need to perform the following queryIf I did that , AppEngine would perform a subquery for each id in friend_ids , probably exceeding the maximum number of subqueries any query can spawn ( 30 ) .Is there any better way to do this ( i.e . minimizing the number of queries ) ? I understand that there are no relations and joins using the datastore but , in particular , I would consider adding new fields to the User or Thing class if it helps in making things easier ."
"Ca n't seem to find a clue to this online and ca n't figure it out myself so : How would I go about slicing a list so that I return a list of slices of contiguous non-zero integers . ie : and I want to produce : I have tried various methods of iterating through the list , have been leaning towards a generator that lets me know when the contiguous groups start and stop , by testing if there is a 0 before or after , but then I am a bit stumped ."
"I have a DataFrame like this : Now I have to add a column named Visit_period which takes one of 4 values [ morning , afternoon , evening , night ] when maximum time spent by that person ( row ) fell into : so for above five row out put will be something like this.I have mentioned maximum time spent because , it may happen that some person 's first_seen is at 14:30 and last_seen is 16:21 . I would like to assign the value afternoon as he spent 30 mins in afternoon slab and 21 in evening slab.I am using python 2.7 ."
"The goal is to get vertical infinite lines in every subplot , at x=1.In this example , I 'll just try a single plotly shape of type= '' line '' in the first row , first columnSo it looks like adding a shape with row and column overrides the yref and xref properties , returning a segment of a line instead of an infinite line.Forcing yref to be `` paper '' before printing ... ... I get this : This is arguably worse , a line that 's relative to the whole figure instead of the subplot y axis . Has anyone stumbled with this problem before ? Any ideas ?"
"tl ; drOf the same numpy array , calculating np.cos takes 3.2 seconds , wheras np.sin runs 548 seconds ( nine minutes ) on Linux Mint.See this repo for full code.I 've got a pulse signal ( see image below ) which I need to modulate onto a HF-carrier , simulating a Laser Doppler Vibrometer . Therefore signal and its time basis need to be resampled to match the carrier 's higher sampling rate.In the following demodulation process both the in-phase carrier cos ( omega * t ) and the phase-shifted carrier sin ( omega * t ) are needed.Oddly , the time to evaluate these functions depends highly on the way the time vector has been calculated.The time vector t1 is being calculated using np.linspace directly , t2 uses the method implemented in scipy.signal.resample.As can be seen in the picture below , the time vectors are not identical . At 80 million samples the difference t1 - t2 reaches 1e-8.Calculating the in-phase and shifted carrier of t1 takes 3.2 seconds each on my machine.With t2 , however , calculating the shifted carrier takes 540 seconds . Nine minutes . For nearly the same 80 million values.I can reproduce this bug on both my 32-bit laptop and my 64-bit tower , both running Linux Mint 17 . On my flat mate 's MacBook , however , the `` slow sine '' takes as little time as the other three calculations.I run a Linux Mint 17.03 on a 64-bit AMD processor and Linux Mint 17.2 on 32-bit Intel processor ."
"Note : All code for a self-contained example to reproduce my problem can be found below.I have a tf.keras.models.Model instance and need to train it with a training loop written in the low-level TensorFlow API.The problem : Training the exact same tf.keras model once with a basic , standard low-level TensorFlow training loop and once with Keras ' own model.fit ( ) method produces very different results . I would like to find out what I 'm doing wrong in my low-level TF training loop.The model is a simple image classification model that I train on Caltech256 ( link to tfrecords below ) .With the low-level TensorFlow training loop , the training loss first decreases as it should , but then after just 1000 training steps , the loss plateaus and then starts increasing again : Training the same model on the same dataset using the normal Keras training loop , on the other hand , works as expected : What am I missing in my low-level TensorFlow training loop ? Here is the code to reproduce the problem ( download the TFRecords with the link at the bottom ) : This is the simple TensorFlow training loop : Below is the standard Keras training loop , which works as expected . Note that the activation of the dense layer in the model above needs to be changed from None to 'softmax ' in order for the Keras loop to work.You can download the TFRecords for the Caltech256 dataset here ( about 850 MB ) .UPDATE : I 've managed to solve the problem : Replacing the low-level TF loss functionby its Keras equivalentdoes the trick . Now the low-level TensorFlow training loop behaves just like model.fit ( ) .This raises a new question : What does tf.keras.backend.categorical_crossentropy ( ) do that tf.nn.softmax_cross_entropy_with_logits_v2 ( ) does n't that leads the latter to perform much worse ? ( I know that the latter needs logits , not softmax output , so that 's not the issue )"
"I 'm trying to find the cause of a crash in of our python scripts.The main structure is this : I get this stacktrace in our main error handling , not in an error email which I 'm supposed to.From what I see all of the write-calls to logger are inside the try-block , but since it 's not caught and handled in my email sending exception block it seems I 've missed something . I 've checked and the sendMail function does n't use the logging module at all . So the exception should n't originate in my except-block.I tried addingat the top of the file see where the exception originates but that did n't affect anything . And now I 'm out of ideas on how to find where the problem originates.The script runs once every hour and only crashes about once per week , which makes me assume that it 's something related to the input data , but that 's only handled by dostuff ( ) .UPDATE : I 've figured out why I only get one row of the stacktrace . Inside the emit ( ) I found this.And the relevant part of the handleError function looks like this : Which only prints the last part of the stacktrace ."
"I manually compiled python-openzwave to work with C++ library.I would like to use it as Kodi addon ( OpenELEC running on Pi 3 ) , so can not use standard installation.I 've compiled everything , downloaded missing six and louie libs , and now try to run hello_world.py . My current dirs structure is the following : But when I run hello_world.py , I get the following error - If I move libopenzwave.a and libopenzwave.so to root folder , then I get the following error : What is wrong with my setup ?"
"Thank you all in advance . I am wondering what 's the right way to # include all numpy headers and what 's the right way to use Cython and C++ to parse numpy arrays . Below is attempt : I know this might be wrong , I also tried other options but none of them works . The PyArray_Check routine throws Segmentation Fault . PyArray_CheckExact does n't throw , but it is not what I wanted exactly . and the implementation file is : The setup.py script isAnd finally the test script : I have created a git repo with all the scripts above : https : //github.com/giantwhale/study_cython_numpy/"
"Possible Duplicate : String comparison in Python : is vs. == Python string interning Why does comparing strings in Python using either '== ' or 'is ' sometimes produce a different result ? I used accidentally is and == for strings interchangeably , but I discovered is not always the same.Why in one case operators are interchangeable and in the other not ?"
"This triggers the following console output.These warnings are flooding my application console output . How can I use minify without generating warnings ? I 'm using Python 2.7.12 , and what is currently the latest library versions : slimit 0.8.1 , ply 3.10 ."
"I have a model in Keras which I 'm optimizing the mean squared error . However , if I use the same code as in losses.py from Keras in the metric , I get a different result . Why is this ? As a metric : For the model : This results in a loss of 6.07 but an MSE_metric of 0.47"
I want to upload a big file to dropbox . How can I check the progress ? [ Docs ] EDIT : The uploader offeset is same below somehow . What am I doing wrongEDIT 2 :
"I 'm doing a matrix by matrix pointwise division however there are some zeros in the divisor matrix . This results in a warning and in some NaNs . I want these to map to 0 , which I can do like this : However there are two issues with this , first of all it still gives a warning ( I do n't like warnings ) and second of all this requires a second pass over the matrix ( not sure if this is unavoidable ) and efficiency is very important for this part of the code . Ideas ?"
"After I have updated matplotlib to the current version I encounter a problem with node labels in networkX : if I use the nx.draw ( G ) command , I get a graph , but no labels are displayed . But let 's speak with examples : this returns a valid plot , but with no node labels . Even if I pass the labels directly like in there are still no labels . I 'm pretty sure it ( especially the upper one ) was working yesterday before the update . So was there a change from matplotlib 1.3.x ( do n't remember the exact one I was running previously ) ? The current versions are : Little Extra : if i run the upper code with nx.draw ( T , with_labels=True ) I get a plot ( extra window on my settings ) and when I close it a TypeError : bad argument type for built-in operation pops up . It does not happen if I run nx.draw ( T , with_labels=False ) , which is very confusing , since I thought with_labels argument takes a boolean ( see here ) , which it ( partially ) does not ... . ? Am I misunderstanding something here ? Edit : @ tcaswell hope that helps ! ( I 'm neither a mathematician nor a programmer , but willing to learn , so please be patient ! )"
"I have a list of dictionariesI need to get all the elements with the longest length from my list , i.e . { ' b ' : 2 , ' c ' : 3 } and { 'd ' : 4 , ' e ' : 5 } .I 'm not very knowledgeable in Python but I found that : And , an even better solution that returns the index of the longest length dictionary : I would like to use an expression that would return something likeand I feel like I 'm not far from the solution ( or maybe I am ) but I just do n't know how to get it ."
I am trying to write a program that has clients connect to it while the server is still able to send commands to all of the clients . I am using the `` Twisted '' solution . How can I go about this ? Here is the code I have so far ( I understand that Twisted already uses non-blocking sockets ) :
"I 'm catching exceptions with a try ... except block in Python . The program tries to create a directory tree using os.makedirs . If it raises WindowsError : directory already exists , I want to catch the exception and just do nothing . If any other exception is thrown , I catch it and set a custom error variable and then continue with the script.What would theoretically work is the following : Now I want to enhance this a bit and make sure that the except block for WindowsError only treats those exceptions where the error message contains `` directory already exists '' and nothing else . If there is some other WindowsError I want to treat it in the next except statement . But unfortunately , the following code does not work and the Exception does not get caught : How can I achieve that my first except statement specifically catches the `` directory already exists '' exception and all others get treated in the second except statement ?"
"Given two permutations A and B of L different elements , L is even , let 's call these permutations `` symmetric '' ( for a lack of a better term ) , if there exist n and m , m > n such as ( in python notation ) : Informally , considerTake any slice of it , for example 1 2 . It starts at the second index and its length is 2 . Now take a slice symmetric to it : it ends at the penultimate index and is 2 chars long too , so it 's 5 6 . Swapping these slices givesNow , A and B are `` symmetric '' in the above sense ( n=1 , m=3 ) . On the other handare not `` symmetric '' ( no n , m with above properties exist ) .How can I write an algorithm in python that finds if two given permutations ( =lists ) are `` symmetric '' and if yes , find the n and m ? For simplicity , let 's consider only even L ( because the odd case can be trivially reduced to the even one by eliminating the middle fixed element ) and assume correct inputs ( set ( A ) ==set ( B ) , len ( set ( A ) ) ==len ( A ) ) . ( I have no problem bruteforcing all possible symmetries , but looking for something smarter and faster than that ) .Fun fact : the number of symmetric permutations for the given L is a Triangular number.I use this code to test out your answers.Bounty update : many excellent answers here . @ Jared Goguen 's solution appears to be the fastest.Final timings :"
"I have two functions in my python Twisted Klein web service : When os.system ( `` command to upload the written file '' ) runs , it will show message saying `` start uploading '' then `` upload complete '' . I want to make the logging function asynchronous so that processing in logging handler happens after dostuff handler prints out `` check ! '' . ( I actually want processing to happen after returnValue ( `` 42 '' ) , but both of those are making the logging function async I think ? ) I thought the yield statement will make it non-blocking but it seems not the case , the `` check ! '' always got printed after `` start uploading '' and `` upload complete '' . I 'll appreciate if anyone can give me some feedback on it since I 'm new to async coding and got blocked on this for a while ..."
I am porting a Python2 script that uses Pango for drawing text to a Cairo surface . It works fine using the old PyGtk API with the pangocairo package . My system ( Debian Jesse ) does n't have Python3 packages for PyGtk and instead uses the newer Gtk+ libraries with the PyGObject API.I want to create a pangocairo.CairoContext object but it seems to be missing in the new API . The PangoCairo package has a create_context ( ) function but it generates a PangoContext object that does n't have the methods I need.So far I have this : The old Python2 code that works : Does anyone have a solution for this ? Is there any good documentation on how PangoCairo should be used with the new API ?
I am currently having trouble completing this challenge in `` Automate the boring stuff '' : My code is : And I am getting this error : I know I am doing SOMETHING wrong with how I wrote my code but I do n't understand what it is exactly . Any and all help is greatly appreciated ! Also I am using python 3 .
"I have a Python application . I am using Sphinx with the autodoc extension togenerate docs for it . In documenting function arguments , I see two main options : Option 1Option 2Note that option 2 can not be nested under a header like `` Args '' , as in option 1 , without breaking the rendered output . Option 2 has much better rendered output than option 1 , but makes the actual docstrings much less readable . Why should param need to be written a trillion times ? Option 1 ( from Google 's Python style guide ) provides much better docstrings , but the rendered output is poor . Does there exist a standard for function docstrings that produces both a clean raw docstring and good rendered output ?"
I want to add data of first row of the dataframe to its column name & delete first row.Source DataFrame : Need to rename column name as Column Name +'|'+ Data of First row :
"I wrote simple monte-carlo π calculation program in Python , using multiprocessing module.It works just fine , but when I pass 1E+10 iterations for each worker , some problem occur , and the result is wrong . I cant understand what is the problem , because everything is fine on 1E+9 iterations !"
"I 'm trying to write a dataframe as a CSV file on S3 by using the s3fs library and pandas.Despite the documentation , I 'm afraid the gzip compression parameter it 's not working with s3fs.This code saves the dataframe as a new object in S3 but in a plain CSV not in a gzip format.On the other hand , the read functionality it 's working OK using this compression parameter.Suggestions/alternatives to the write issue ? Thank you in advance ! ."
"I have a dataframe 'df ' that looks like this : What I would like to do is to groupby the id , then get the size for each id where date1=date2 . The result should look like : I have tried this : And get this error : You could certainly flag each instance where the date1 and date2 are equal , then count those flags for each id by each samedate , but I have to believe there is a groupby option for this ."
"I want to use a python regexp to remove the comments in a LaTeX file . In LaTeX a comment starts by `` % '' . But if the % character is escaped ( `` \ % '' ) then its not a comment , its the symbol percent . This task is just one among many regexp that I apply on my LaTeX text . I store all these reg exp in a list of dicts.The problem I face is that the regexp I use for pruning the comments does not work ( because I do not know how to specify the character set 'not backslash ' ) . The backslash in the character set escapes the closing ' ] ' and the regexp is incorrect.My code : Any help will be much appreciated . Thanks ! Gilles"
"When I run this example and create a rectangular selection if I zoom or move the plot window around the selection disappears until I deselect the move or zoom tool and click on the plot window again.I am using % matplotlib tkinter in an IPython notebook.I 've attempted hooking into the limit changes that occur when the window is zoomed and setting the rectangular selection to visible : But this does n't seem to do anything . It does n't even appear that toggle_selector.RS.visible is ever set to false.I 've also been looking at the source for RectangleSelector , but I have n't seen anything enlightening there.I 've also discovered that I have this issue when I modify the extent of the selected region using RectangleSelector.extents = new_extents . When .extents is modified , for example with a slider widget , the selected region disappears until I click on the plot again.All of these problems problems go away if the RectangleSelector is initialized with useblit=False as @ ImportanceOfBeingErnest suggests , but , as they say , it 's not a very performant solution ."
"I 'm trying to create a new node and set its attributes.For example printing one of the graph nodes I see that its attributes are : I can create a node like : but how to add key : `` T '' atribute ? i.e . what should be inside tf.AttrValue in this case ? Looking at attr_value.proto I have tried : UPDATE : I figured out that in Tensorflow it should be written like : But this gives an error : TypeError : b'float32 ' has type bytes , but expected one of : int , longAnd I 'm not sure which int value corresponds to float32.https : //github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/attr_value.proto # L23https : //github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/attr_value.proto # L35"
"So , let 's start with an example . Suppose we have several types that can be combined together , let 's say we are using __add__ to implement this . Unfortunately , due to circumstances beyond our control , everything has to be `` nullable '' , so we are forced to use Optional everywhere . We want utility function that does null-checking for us , but can generically handle `` combinable '' types . Most of the types can only be combined with themselves , and just to be more true to my actual use-case , let 's say one type combines with the other . I would have hoped that the overload decorator could have helped here , however mypy complains : Using mypy version : mypy 0.641Note , if I remove Optional madness , mypy does n't complain . I can even keep one of them as optional ! : This makes me suspect that the `` overlap '' is for NoneType , but feel like this should be resolvable , am I completely off base ? EditSo , I 'm really just flailing about here , but I suppose , when both argument are None this is definitely ambiguous , I would have hoped that the following would resolve it : But I am still getting : Edit2Going along the same garden-trail , I 've tried the following : I am now getting : Which is starting to make sense to me , I think fundamentally what I am trying to do is unsafe/broken . I may have to just cut the gordian knot another way ..."
"I am having trouble with a sleep statement hanging my multithreading function . I want my function to go about it 's buisness while the rest of the program runs . Here is a toy that recreates my problem : can anyone explain why this code outputs : instead of : # EDIT # I eventually want to run this function on a schedual , and test if it is running before I execute the function . This is an example :"
"Say you have a pandas dataframe df looking like : If you want to capture a single element from each column in cols at a specific index ind the output should look like a series : What I 've tried so far was : which gives the undesired output : Any suggestions ? context : The next step would be mapping the output of an df.idxmax ( ) call of one dataframe onto another dataframe with the same column names and indexes , but I can likely figure that out if I know how to do the above mentioned transformation ."
"I believe I have installed pygraphviz on my Debian build . I can import it if I run Python from the command line : However , if I try the same from a Python console from within PyCharm , I get this error : I notice that in the first example , the Python version iswhereas in the 2nd example the version is : So the 2nd example , which fails , appears to be 32 bit ( as a side note , why would it have installed a 32 bit version ? ) My /anaconda3/envs folder is empty.As far as I know , I only have one Python environment installed in Conda.So my question is ... Why does the import fail from within PyCharm , and why does it appear to be running a 32 bit version of Python when I only have one version installed ?"
"I have the following code : This expectedly prints : Is there a way to tell the map function to call spam with a particular keyword argument ? In this example , I 'd like the values params to be passed to the hello argument rather then the next in line ( which in this case is params ) .The real-world use case I 'm trying to solve is passing the params= value to a request.get call for a repeating URL ."
I have a very large graph represented in a text file of size about 1TB with each edge as follows.I would like to split it into its weakly connected components . If it was smaller I could load it into networkx and run their component finding algorithms . For example http : //networkx.github.io/documentation/latest/reference/generated/networkx.algorithms.components.connected.connected_components.html # networkx.algorithms.components.connected.connected_componentsIs there any way to do this without loading the whole thing into memory ?
"I want to create an empty Numpy array in Python , to later fill it with values . The code below generates a 1024x1024x1024 array with 2-byte integers , which means it should take at least 2GB in RAM.From getsizeof ( A ) , we see that the array takes 2^31 + 128 bytes ( presumably of header information . ) However , using my task manager , I can see Python is only taking 18.7 MiB of memory.Assuming the array is compressed , I assigned random values to each memory slot so that it could not be.The loop is still running , and my RAM is slowly increasing ( presumably as the arrays composing A inflate with the incompresible noise . ) I 'm assuming it would make my code faster to force numpy to expand this array from the beginning . Curiously , I have n't seen this documented anywhere ! So , 1 . Why does numpy do this ? and 2 . How can I force numpy to allocate memory ?"
"I have a collection of HTML files . I wish to iterate over them , one by one , editing the mark-up of a particular class . The code I wish to edit is of the following form , using the following class names : This can occur multiple times in the same document , with different text instead of `` Put me Elsewhere '' , but always the same classes.I want to change this to be of the form : Not too sure where to go after this or how to deal with the tags array ? Any help would be much appreciated . Thanks : )"
"What is the point of the Sphinx highlight_language config option if code-block : : does n't have an optional argument ? It says that it sets the default hightlight_language , but if every time that you specify code you need to state the language , why does it need the config option ? Am I doing something wrong ? Edit : From the logs when I remove the argument for codeblock ."
"Suppose I want to match a lowercase letter followed by an uppercase letter , I could do something likeNow I want to do the same thing for unicode strings , i.e . match something like 'aÅ ' or 'yÜ ' . Tried but that does not work.Any clues ?"
I would like to generate a plot like this with matplotlibtimescale http : //www.imagenetz.de/img.php ? file=37d8879009.jpg & pid=Currently I just do the 3 subplots with matplotlib and add the red lines in inkscape.I found out that I can create the dashed rectangles with Rectangle.I have n't found anything yet to draw the lines connecting the plots . Is there a function that can draw across axis borders ?
"I got interested in this small example of an algorithm in Python for looping through a large word list . I am writing a few `` tools '' that will allow my to slice a Objective-C string or array in a similar fashion as Python . Specifically , this elegant solution caught my attention for executing very quickly and it uses a string slice as a key element of the algorithm . Try and solve this without a slice ! I have reproduced my local version using the Moby word list below . You can use /usr/share/dict/words if you do not feel like downloading Moby . The source is just a large dictionary-like list of unique words . This script will a ) be interpreted by Python ; b ) read the 4.1 MB , 354,983 word Moby dictionary file ; c ) strip the lines ; d ) place the lines into a set , and ; e ) and find all the combinations where the evens and the odds of a given word are also words . This executes in about 0.73 seconds on a MacBook Pro.I tried to rewrite the same program in Objective-C . I am a beginner at this language , so go easy please , but please do point out the errors.The Objective-C version produces the same result , ( 13,341 words ) , but takes almost 3 seconds to do it . I must be doing something atrociously wrong for a compiled language to be more than 3X slower than a scripted language , but I 'll be darned if I can see why . The basic algorithm is the same : read the lines , strip them , and put them in a set . My guess of what is slow is the processing of the NSString elements , but I do not know an alternative.EditI edited the Python to be this : For the utf-8 to be on the same plane as the utf-8 NSString . This slowed the Python down to 1.9 secs . I also switch the slice test to short-circuit type as suggested for both the Python and obj-c version . Now they are close to the same speed . I also tried using C arrays rather than NSStrings , and this was much faster , but not as easy . You also loose utf-8 support doing that . Python is really cool ... Edit 2I found a bottleneck that sped things up considerably . Instead of using the [ rtr appendFormat : @ '' % c '' , [ inString characterAtIndex : i ] ] ; method to append a character to the return string , I used this : Now I can finally claim that the Objective-C version is faster than the Python version -- but not by much ."
"Assume I have a list.I am looking for a way to join the count of the string inside.Intended Output : I was able to solve it by using a list comprehension but I am looking for a way where I do n't have to specify the list [ 1 , 1 , 2 , 2 , 3 , 3 ] . Is it possible ?"
"I would like to replace values with column labels according to the largest 3 values for each row . Let 's assume this input : Given n = 3 , I am looking for : I 'm not concerned about duplicates , e.g . for index 3 , Top3 can be 'p1 ' or 'p4'.Attempt 1My first attempt is a full sort using np.ndarray.argsort : But in reality I have more than 4 columns and this will be inefficient.Attempt 2Next I tried np.argpartition . But since values within each partition are not sorted , this required a subsequent sort : This , in fact , works out slower than the first attempt for larger dataframes . Is there a more efficient way which takes advantage of partial sorting ? You can use the below code for benchmarking purposes.Benchmarking"
"I 'm using Pandas version 0.12.0 on Ubuntu 13.04 . I 'm trying to create a 5D panel object to contain some EEG data split by condition.How I 'm chosing to structure my data : Let me begin by demonstrating my use of pandas.core.panelnd.creat_nd_panel_factory.Essentially , the organization is as follows : setsize : an experimental condition , can be 1 or 2location : an experimental condition , can be `` same '' , `` diff '' or Nonevfield : an experimental condition , can be `` lvf '' or `` rvf '' The last two axes correspond to a DataFrame 's major_axis and minor_axis . They have been renamed for clarity : channels : columns , the EEG channels ( 129 of them ) samples : rows , the individual samples . samples can be though of as a time axis.What I 'm trying to do : Each experimental condition ( subject x setsize x location x vfield ) is stored in it 's own tab-delimited file , which I am reading in with pandas.read_table , obtaining a DataFrame object . I want to create one 5-dimensional panel ( i.e . Subject ) for each subject , which will contain all experimental conditions ( i.e . DataFrames ) for that subject.To start , I 'm building a nested dictionary for each subject/Subject : Full stack traceI understand that panelnd is an experimental feature , but I 'm fairly certain that I 'm doing something wrong . Can somebody please point me in the right direction ? If it is a bug , is there something that can be done about it ? As usual , thank you very much in advance !"
"I am interfacing with the nlme and lme4 R functions via RPy , and I would like to get access to an output summary from my python console.I run the following code : for nlme , and this for lme4 : To get a code snippet with data and explicit imports , please refer to this IPython notebook.In all cases , I get a huge amount of print output which includes a horridly long section looking like : I would like to get a summary more along the lines of : ( which is included in what I get , but only comes thousands of entries after the above ) How do I achieve that ?"
"Consider the following simple flask app : Here are my questions : Once the `` enter_string '' endpoint has been visited and the user has assigned a string to session [ `` string '' ] , where is the string stored ? Is it in the server 's memory or the user 's ? By default , the session expires when the browser exits . Is there a simple way to have some other event trigger the expiration of the session , such as closing the window but not necessarily the browser ? By default , will the session ever time out or is it kept until the browser exits no matter how long that takes ?"
"I 'd like to make a function which would also act as context manager if called with with statement . Example usage would be : This is very similar to how the standard function open is used.Here 's the solution I came up with : This code is not thread-safe , but this is not related to the problem . What I do n't like about this solution is that class constructor pretends to be a simple function and is used only for its side effects.Is there a better way to do this ? Note that I have n't tested this solution.Update : the reason why I do n't want to split function and context manager into separate entities is is naming . The function and the context manager do the same thing , basically , so it seems reasonable to use one name for both . Naming the context processor would be problematic if I wanted to keep it separate . What should it be ? active_language ? This name may ( and will ) collide with variable name . override_active_language might work , though ."
"I have two numpy arrays of integers , both of length several hundred million . Within each array values are unique , and each is initially unsorted.I would like the indices to each that yield their sorted intersection . For example : Then the sorted intersection of these is [ 4 , 5 , 11 ] , and so we want the indices that turn each of x and y into that array , so we want it to return : since then x [ mx ] == y [ my ] == np.intersect1d ( x , y ) The only solution we have so far involves three different argsorts , so it seems that is unlikely to be optimal.Each value represents a galaxy , in case that makes the problem more fun ."
I have a schema like this : models.py : forms.py : and a subclass of generic view UpdateView . I want to restrict access to that view to the user specified in that Evento instance . Where is the best approach to of that ?
"When I use pandas read_csv to read a column with a timezone aware datetime ( and specify this column to be the index ) , pandas converts it to a timezone naive utc DatetimeIndex.Data in Test.csv : DateTime , Temperature2016-07-01T11:05:07+02:00,21.1252016-07-01T11:05:09+02:00,21.1382016-07-01T11:05:10+02:00,21.1562016-07-01T11:05:11+02:00,21.1792016-07-01T11:05:12+02:00,21.1982016-07-01T11:05:13+02:00,21.2062016-07-01T11:05:14+02:00,21.2252016-07-01T11:05:15+02:00,21.233Code to read from csv : This results in an index that represents the timezone naive utc time : I tried to use a date_parser function : This gave the same result.How can I make read_csv create a DatetimeIndex that is timezone naive and represents the local time instead of the utc time ? I 'm using pandas 0.18.1 ."
Python 2.4.x - I can not install any non-standard modules . *nix environment.So I 've got an output that I have some color in using ansi coloring . Works great . But the output is so long I needed to either pipe to less or use pydoc 's page ( ) Once I do either - the color turns intoI saw over at Cucumber : pipe output without losing color that Adam says that lots of output that is piped lose it 's color - but if sent to terminal it 's just fine.Using pydoc 's ttypager retains the color output - but does n't provide a `` less '' type feature ( scrolling back up ) .How do I get the best of both worlds ? Color and Less type functionality ? Thank you .
I 've written a simple wrapper script for repeating commands when they fail called retry.py . However as I want to see the output of child command I 've had to pull some pty tricks . This works OK for programs like rsync but others like scp apply additional test for showing things like their progress meter.The scp code has a test that is broadly : Which fails when I run though the wrapper script . As you can see with my simple tty_test.c test case : and : I 've tried using the tcsetpgrp ( ) which ends up as an IOCTL on the pty fd 's but that results in an -EINVAL for ptys . I 'd prefer to keep using the Python subprocess machinery if at all possible or is manually fork/execve'ing going to be required for this ?
"Although the title is a question , the short answer is apparently no . I 've tried in the shell . The real question is why ? ps : string is some non-ascii characters like Chinese and XXX is the current encoding of stringThe example is above . I am using windows chinese simplyfied . The default encoding is gbk , so is the python shell . And I got the two unicode object unequal.UPDATES"
I 'm constantly doing the following pattern in Django : Is there a good pattern that other people follow that achieves the same effect ( i.e . I have access to constants with names and dictionaries that go both ways ) without so much code ?
"This is a recent problem , it began I think three or four days ago . It is not isolated to my own system , as I was running the software on a remote server as well ( Windows 10 , Windows Server ) . It is not also not isolated to any specific URL , as I ca n't get past any URL that has this check now.Title : `` Just a moment ... '' `` Checking your browser before accessingURL '' . `` This process is automatic . Your browser will redirect to yourrequested content shortly . '' `` Please allow up to 5 seconds ... '' `` DDosProtection by Cloudflare '' `` Ray Id : xxxxxxxxxxxxxxxxxx '' I 've attempted different systems ( both windows based ) I 've attempted different drivers ( gecko and chrome ) I 've attempted different urlsDoes anyone know how I can resolve this ; or is it time to put poor ol ' timmy ( the program ) down ?"
"I try to process such data : So that it produces a list with values updated to be a running sum . Now I do it with a multiline loop : How to make the loop one-liner , so that I got :"
"This question might be sound subjective , but as `` the Zen '' says , there is ( nearly always ) one way to preferred , it should n't be subjective at the end.What way is the better one ? ( 1 ) is ( IMO ) very clear , but in many answers , map ( ) is used . And if we do so , there is nearly equal readability between ( 2 ) and ( 3 ) ( IMO , at least ) .The same counts for many other tasks , but I have chosen this one , as it can stand for all of similiar ones ."
"I have some python objects with some methods in which i would like to do some check at the beggining , depending of this check , the method 's code would run , or an execption would be raised . Instead of replicating the `` check '' code at the beginning of every method I though of doing a decorator , I also want the decorator to be embedded inside the class itself , since it is closely related to it . So basically : instead of thisI would like to have thisMy first question is , am I going about this right ? or is there a better way . I have many methods of the A class that need to have this check , so that is why I do n't want to replicate the code unnecessarily.My second question is , if I go about this the way I described , I run into a problem when I want to derive a class from class A and performe the same decorator checks . Again I do n't want to replicate the code , so I want to reuse the decorator in the base class A to performe checks in the derived class . I read about turning the decorator into a @ classmethod however when I do this I am able to use the decorator in the derived class but not in the base class anymore ! So basically I would like something like this : Does anybody know of any clean way to do this ?"
"I 'm testing some Django models with bog-standerd django.test.Testcase . My models.py writes to a debug log , using the following init code : and then I write to the log with : In my settings.py , I 've set up a single FileHandler , and a logger for myapp , using that handler and only that handler . This is great . I see messages to that log . When I 'm in the Django shell , I only see messages to that log.When , however , I run my test suite , my test suite console also sees all those messages . It 's using a different formatter that I have n't explicitly defined , and it 's writing to stderr . I do n't have a log handler defined that writes to stderr.I do n't really want those messages spamming my console . I 'll tail my log file if I want to see those messages . Is there a way to make it stop ? ( Yes , I could redirect stderr , but useful output goes to stderr as well . ) Edit : I 've set up two handlers in my settings.py : and tried this : ... but the logging / stderr dumping behavior remains the same . It 's like I 'm getting another log handler when I 'm running tests ."
"I am somewhat new to Python ... I have an array of dicts that I got by reading a file containing JSON messages , i.e . using something like this : Each JSON message has , among other things , three fields : `` date '' and `` type '' and `` location '' . I need to sort the array first by date , then by type within each block of identical dates , then by location within each block of identical types . How can I do that ? Thx much !"
"I 'm wondering if any other C # developers would find it an improvement to have a compiler directive for csc.exe to make whitespace significant a la Haskell or Python where the kinds of whitespace create code blocks.While this would certainly be a massive departure from C-style languages , it seems to me that since C # is ultimately being compiled down to CIL ( which would still have the curly braces and semicolons ) , it really is just a parsing trick the compiler can handle either way ( that is , it can either deal with significant whitespaces or not ) . Since curlies and semicolons are often a barrier to entry to C # & they are really only parsing helpers ( they do n't in themselves impart meaning to your code ) , they could be removed a la Haskell/Python.F # handles this with the # light compiler directive which you can read about in Lightweight syntax option in F # 1.1.12.3.I 'd like to see the same thing in C # : a # SigSpace or somesuch directive that would direct csc.exe to treat the source like a Haskell file in terms of whitespace ( just as an example ) .Standard C # : Significant whitespace : I 'm not saying that I want this in C # , but I am interested in what the tradeoffs are . My guess is that most C # developers have gotten so used to the syntax that they wo n't be able to see how artificial it is ( though it may in the end make the code easier to read ) ."
"I want to convert b'\xc2\xa0\x38 ' into b'x38 ' in python3.In the webpage , the c2 a0 means NO-BREAK SPACE whose unicode point is U+00A0 . Notice : c2a0 is unprintable , character column is blank here . relationship on unicode point , character , utf-8 How to convert b'\xc2\xa0\x38 ' into b'\x38 ' with replace method ?"
"When you do Copy ( CTRL+C ) on a file , then in some programs ( example : it works in the Windows Explorer address bar , also with Everything indexing software ) , when doing Paste ( CTRL+V ) , the filename or directory name is pasted like text , like this : `` d : \test\hello.txt '' .I tried this : CTRL+C on a file or folder in Windows ExplorerRun : But I get this error : TypeError : Specified clipboard format is not availableQuestion : how to retrieve the filename of a file that has been `` copied '' ( CTRL+C ) in the Windows Explorer ?"
"I 've read some tutorials on Python metaclasses . I 've never used one before , but I need one for something relatively simple and all the tutorials seem geared towards much more complex use cases . I basically want to create a template class that has some pre-specified body , but takes its base class as a parameter . Since I got the idea from C++/D templates , here 's an example of what the code I want to write would look like in C++ :"
"operator provides attrgetter to make a function that retrieves a field from an object.Why is n't this included in operator ( or somewhere else in the standard libraries ) ? The only reason that I can think of is that there are edge cases where this straightforward approach will break . In which case , what are these edge cases so that I can try to trap/avoid them ?"
"So I have a little sieve of Eratosthenes function written in both Python and Julia , and I 'm comparing run times.Here is the Python code : And here is the Julia code : The Python code runs in about .005 seconds , while the Julia code takes about .5 seconds , so that means Python runs about 100x times faster . There 's probably a perfectly logical reason for this , but I really have no idea what I 'm doing ."
"I often end up writing code likeIn English it seems redundant to keep repeating x , is there an easier or shorter way to write out an if-statement like that ? Maybe checking of existence of x 's value in a tuple ( 1 , 5 , 10 , 22 , ) or something ?"
"I somewhat accidentally discovered that you can set 'illegal ' attributes to an object using setattr . By illegal , I mean attributes with names that ca n't be retrieve using the __getattr__ interface with traditional . operator references . They can only be retrieved via the getattr method . This , to me , seems rather astonishing , and I 'm wondering if there 's a reason for this , or if it 's just something overlooked , etc . Since there exists an operator for retrieving attributes , and a standard implementation of the setattribute interface , I would expect it to only allow attribute names that can actually be retrieved normally . And , if you had some bizarre reason to want attributes that have invalid names , you would have to implement your own interface for them.Am I alone in being surprised by this behavior ? This returns something that is both odd , and a little misleading : [ ... '__weakref__ ' , 'bar.baz ' ] And if I want to access foo.bar.baz in the 'standard ' way , I can not . The inability to retrieve it makes perfect sense , but the ability to set it is surprising.Is it simply assumed that , if you have to use setattr to set the variable , you are going to reference it via getattr ? Because at runtime , this may not always be true , especially with Python 's interactive interpreter , reflection , etc . It still seems very odd that this would be permitted by default . EDIT : An ( very rough ) example of what I would expect to see as the default implementation of setattr : This will not permit me to use invalid characters in my attribute names . Obviously , super ( ) could not be used on the base Object class , but this is just an example ."
"i need something like iMacros for Python . It would be great to have something like that : Do you know something like that ? Thanks in advance , Etam ."
When I deploy a python3 application using Google App Engine Flex I get the following error : My requirements.txt file include the following : The reason I am using grpcio version 1.27.2 instead of the most recent 1.29.0 is because of the information shown hereCan somebody help ?
"I have a list of objects of say foos . I have a loop for creating a new list . foo1 = { id:1 , location:2 } for e.g . foos = [ foo1 , foo2 , foo3 ] Now I want to create a new list based on location . What I want to know is there any way in which I can do something like this Can I use map function here ? if yes how ?"
"I 'd like to invoke celery tasks synchronously during my Django tests without need to run celery worker . To achieve this I have specified CELERY_ALWAYS_EAGER=True in my settings.py but it does n't seem to work . So I decided to apply override_settings decorator to specific test that looks like thisUnfortunately , this test still invokes task in my celery worker . What I can be missing ? To be specific , I 'm using Django 1.10 with Celery 4.0.0 ."
"Say I have the string `` blöt träbåt '' which has a few a and o with umlaut and ring above . I want it to become `` blot trabat '' as simply as possibly . I 've done some digging and found the following method : This will give me the string in unicode format with the international characters split into base letter and combining character ( \u0308 for umlauts . ) Now to get this back to an ASCII string I could do ascii_string = unicode_string.encode ( 'ASCII ' , 'ignore ' ) and it 'll just ignore the combining characters , resulting in the string `` blot trabat '' .The question here is : is there a better way to do this ? It feels like a roundabout way , and I was thinking there might be something I do n't know about . I could of course wrap it up in a helper function , but I 'd rather check if this does n't exist in Python already ."
"I am aware that the use of eval ( ) usually means bad code , but I stumbled upon a weird behavior of the eval ( ) function in internal functions that I could not understand . If we write : Running f ( 1 ) in this case yields a NameError , claiming that a is not defined . However , if we defineThen running f ( 1 ) prints 1 . There is something happening with local and global variables that I ca n't quite understand . Is a only a local variable in g ( ) when it is `` used '' for something ? What is going on here ?"
"I 'm working with quite a large OOP code-base , and I 'd like to inject some tracing/logging . The easiest way to do this would be to introduce a decorator around certain methods on some base classes , but unfortunately decorators are n't inherited.I did try something like the following : ... and while the __call__ method is wrapped ( which I can see from printing the instance information to the console ) the wrapper function is n't executed as expected.I also looked briefly at using a metaclass based on this answer , but it instantly breaks other parts of the system that use introspection , so I think that 's a no-go.Is there any other way I can force the application of these decorators around the __call__ method of classes that inherit from BaseClass ?"
"I have very good question which I would like an expert to comment on that for me please . ( perhaps Graham Dumpleton ) So I have a Django web application ( developed on ubuntu 16.04 ) which loges some failures as below on /var/log/apache2/APPNAME.log . since all files in /var/log/apache2 have root : adm owner , I granted ownership of my log file the same way and I made sure www-data is a member of adm group . Then I granted rwx to adm group for owner group and I tested everything was working fine.After 24hr the permission of the file and the parent folder has changed and I can see the write permission has been revoked from the log file and the parent directory causing permission denied error in error because the log file could n't be written.Here are my questions if you could kindly help:1 ) where is the right place to put Django log files ? 2 ) What process under what user permission writes the file ? 3 ) Which process resets permissions in the /var/log/apache and why ? Thank you much in advance , I hope this question help others too.Cheers , Mikeviews.pysettings.py"
"This line evaluates to a boolean True/False , as it checks whether a Pandas dataframe contains any NaN 's in its rows or columns . Is there a more concise/idiomatic way of checking this ?"
"I 'm having a problem wrapping my head around my Pyramid web application . I have it structured very much like the one described by Michael Merickel here , except I 'm using pure traversal to find my views . ( They are declared configured with context='path.toResource ' name='myView ' ) , pretty standard fare according to what I can tell from the traversal wiki tutorial . My application has a more complex URL structure though : My User resources are under /users/ { user_id } and my projects are under /projects/ { project_id } . All of my resources are persisted using the SQLAlchemy ORM ; I have a User and Project class with __name__ and __parent__ attributes , as well as other attributes extending Columns . I have a RootFactory , ProjectFactory and UserFactory that populate the appropriate __name__ and __parent__ attributes in their __get_item__ calls.So , in the view functions for the Project context , I get a Project instance in request.context . My issue is how the heck do I reference the corresponding User instance ? I ca n't do project.owner , because that User instance did n't go through the RootFactory chain , so its __parent__ and __name__ values are unset . This is bad , because I want to use request.resource_url to find the URL for the owner User , so I can put a link on the view page.What 's the solution here SO ? Do I do everything through request.root ? What if I want to make a complex query that returns User or Project instances ? Is there some kind of CrapFactory I can pass SQLAlchemy so all its instances get populated properly ? Is my approach just completely wrong ? I feel like I would n't be having these kinds of problems if I just stuck with URL Routing ..."
"Can someone explain why the following three examples are not all equal ? Is there a general 'correct ' way to create Decimal objects in Python ? ( ie , as strings or as floats )"
"Inspecting the slice class in Python with dir ( ) , I see that it has attributes __le__ and __lt__ . Indeed I saw that the following code works : However , I can not see which logic is implemented for this comparison , nor its usecase . Can anyone point me to that ? I am not asking about tuple comparison . Even if slice and tuple are compared the same way , I do n't think this makes my question a duplicate . What 's more , I also asked for a possible usecase of slice comparison , which the suggested duplicate does not give ."
"Given x = C.f after : What do I call on x that will return C ? The best I could do is execing a parsed portion of x.__qualname__ , which is ugly : For a use case , imagine that I want a decorator that adds a super call to any method it 's applied to . How can that decorator , which is only given the function object , get the class to super ( the ? ? ? below ) ?"
"The problemI have a RabbitMQ Server that serves as a queue hub for one of my systems . In the last week or so , its producers come to a complete halt every few hours . What have I triedBrute forceStopping the consumers releases the lock for a few minutes , but then blocking returns.Restarting RabbitMQ solved the problem for a few hours.I have some automatic script that does the ugly restarts , but it 's obviously far from a proper solution.Allocating more memoryFollowing cantSleepNow 's answer , I have increased the memory allocated to RabbitMQ to 90 % . The server has a whopping 16GB of memory and the message count is not very high ( millions per day ) , so that does not seem to be the problem.From the command line : And with /etc/rabbitmq/rabbitmq.config : Code & DesignI use Python for all consumers and producers.ProducersThe producers are API server that serve calls . Whenever a call arrives , a connection is opened , a message is sent and the connection is closed.ConsumersThe consumers slightly differ from each other , but generally use the following pattern - opening a connection , and waiting on it until a message arrives . The connection can stay opened for long period of times ( say , days ) .Design reasoningConsumers always need to keep an open connection with the queue serverThe Producer session should only live during the lifespan of the API callThis design had caused no problems till about one week ago.Web view dashboardThe web console shows that the consumers in 127.0.0.1 and 172.31.38.50 block the consumers from 172.31.38.50 , 172.31.39.120 , 172.31.41.38 and 172.31.41.38.System metricsJust to be on the safe side , I checked the server load . As expected , the load average and CPU utilization metrics are low.Why does the rabbit MQ each such a deadlock ?"
"I am trying to figure out how to check if a variable exists in the template context 'tmpl_context ' using Pylons and Python . What I am trying to do is : I 've got a Pylons layout template . This should contain a message section if , and only if , the variable c.messages exists in the context . The message section looks like this : This yields an error if the controller does not define c.messages . So I 'd like to include this only if the variable is defined . Is there a solution for this problem ?"
"I 've got a Django project working on an Apache server.I installed pandas and want to use it to start manipulating data - however something odd is happening.Anytime I use the import pandas on the production environment , the server will hang up and ( after a while ) throw a 408 timeout error.I can comment out the pandas portion and the server responds normally without issue . I ca n't recreate it in the development environment or command line interface with django.Here are the httpd-app.conf file : I know its hanging up on the import of pandas due to this : I can see the importing pandas in the log , however no following import doneAny help is greatly appreciated ! !"
"Following the documentation here , I am trying to create features from unicode strings . Here is what the feature creation method looks like , This will raise an exception , Naturally if I wrap the value in a str , it fails on the first actual unicode character it encounters ."
"I 've implemented a simple CNN program with Python that can machine learn on the MNIST data set . I 've implemented 3 layers : ConvPoolLayer , which convolves and then does mean pooling FullyConnectedLayer , which is a fully connected hidden layerSoftmaxLayer , which basically gives the softmax output of the network It 's in the ConvPoolLayer that I 've implemented mean pooling . Here 's the line of code that does mean pooling during forward propagation : And here 's the equivalent back-propagation code : All it 's doing is just upscaling the error.My question is , how do I implement the backpropagation for max pooling without loss in performance ? Or , is there a better way to do this without a function call ? I get around ~90-95 % accuracy after a few iterations with mean pooling , so I 'd like to see how max pooling affects performance.If there are any NumPy tricks that can be applied here , I would be glad to learn them . I want to understand myself what happens in a CNN , why things work the way they do , and whether operations can be optimised , so using frameworks is n't an option for me.Thanks for the help !"
"I have several test modules that are all invoked together via a driver script that can take a variety of arguments . The tests themselves are written using the python unittest module . My issue is that , within these test modules , I have certain tests I 'd like to skip based on different parameters passed to the driver . I 'm aware that unittest provides a family of decorators meant to do this , but I do n't know the best way to pass this information on to the individual modules . If I had a -- skip-slow argument , for example , how could I then annotate tests as slow , and have them skipped ? Thank you for your time ."
"Since Python 2.2 and PEP 261 , Python can be built in `` narrow '' or `` wide '' mode , which affects the definition of a `` character '' , i.e . `` the addressable unit of a Python Unicode string '' .Characters in narrow builds look like UTF-16 code units : ( The above seems to disagree with some sources that insist that narrow builds use UCS-2 , not UTF-16 . Very intriguing indeed ) Does Python 3.0 keep this distinction ? Or are all Python 3 builds wide ? ( I 've heard about PEP 393 that changes internal representation of strings in 3.3 , but this does n't relate to 3.0 ~ 3.2 . )"
"The following code is rejected by mypy as expected : output : But after introducing a default parameter of None , there is no error anymore : I would expect mypy to only allow None ( as argument and as the default ) if we change value from int to Optional [ int ] , but it seems like this is not needed . Why ?"
"For my application , I need to read multiple files with 15 M lines each , store them in a DataFrame , and save the DataFrame in HDFS5 format . I 've already tried different approaches , notably pandas.read_csv with chunksize and dtype specifications , and dask.dataframe . They both take around 90 seconds to treat 1 file , and so I 'd like to know if there 's a way to efficiently treat these files in the described way . In the following , I show some code of the tests I 've done.Here is what the files look like ( whitespace consists of a literal tab ) :"
"I 'm using NetworkX to generate a bipartite graph using either nx.bipartite.random_graph or nx.bipartite.gnmk_random_graph , as follows : However , I get an error : It is just a single line , so I 'm not sure how I could be doing this wrong and why their package would be returning ( what I assume is ) an invalid bipartite graph.Thanks . EDIT : I just realised that I need to specify a minimum number of edges/probability for the third argument . E.g . bipartite.random_graph ( 5,6,0.6 ) and having p > 0.5 gets rid of the error . Similarly , bipartite.gnmk_random_graph ( 5,6,11 ) where k > n+m . I did n't realise this was the case , as I assumed if the number of edges was lower than required to connect every vertex there would just be some floating vertices . Thanks for your help !"
"I am trying to split a string into a list by a delimiter ( let 's say , ) but the delimiter character should be considered the delimiter only if it is not wrapped in a certain pattern , in my particular case < > . IOW , when a comma is nested in < > , it is ignored as a delimiter and becomes just a regular character not to be delimited by.So if I have the following string : it should split into Needless to say , I can not just do a simple split by , because that will split the second token into two tokens , second token part 1 and second token part 2 , as they have a comma in between them.How should I define the pattern to do it using Python RegEx ?"
I 've been tokenizing an extremely large corpus . Each Unigram can occur in multiple Comments multiple times . I 'm storing the Comment.ids in a list that is attached to the Unigram in the database every 250K newly counted unigrams or so . What I 'm wondering is if there is a way to extend the comment id list -- or a similar data structure -- without querying and loading the existing list of comments tied to the Unigram ( it can number in the the thousands ) . Or is there no way around the slow IO ? Here is my model code : as well as the code that adds new counts and Comment.ids in :
I have a list of tuples of format : What I want to do is sort this tuple so that the the nodes with higher weight are at the topfor exampleshould give meThe first node is sorted alphabetically.Second node as per the decreasing weight ranks .
"Trying to address this issue , I 'm trying to wrap my head around the various functions in the Python standard library aimed at supporting RFC 2231 . The main aim of that RFC appears to be three-fold : allowing non-ASCII encoding in header parameters , noting the language of a given value , and allowing header parameters to span multiple lines . The email.util library provides several functions to deal with various aspects of this . As far as I can tell , they work as follows : decode_rfc2231 only splits the value of such a parameter into its parts , like this : decode_params takes care of detecting RFC2231-encoded parameters . It collects parts which belong together , and also decodes the url-encoded string to a byte sequence . This byte sequence , however , is then encoded as latin1 . And all values are enclosed in quotation marks . Furthermore , there is some special handling for the first argument , which still has to be a tuple of two elements , but those two get passed to the result without modification.collapse_rfc2231_value can be used to convert this triple of encoding , language and byte sequence into a proper unicode string . What has me confused , though , is the fact that if the input was such a triple , then the quotes will be carried over to the output . If , on the other hand , the input was a single quoted string , then these quotes will be removed.So it seems that in order to use all this machinery , I 'd have to add yet another step to unquote the third element of any tuple I 'd encounter . Is this true , or am I missing some point here ? I had to figure out a lot of the above with help from the source code , since the docs are a bit vague on the details . I can not imagine what could be the point behind this selective unquoting . Is there a point to it ? What is the best reference on how to use these functions ? The best I found so far is the email.message.Message implementation . There , the process seems to be roughly the one outlined above , but every field gets unquoted via _unquotevalue after the decode_params , and only get_filename and get_boundary collapse their values , all others return a tuple instead . I hope there is something more useful ."
"Currently , django.contrib.comments sends the user to the preview page if there is any error on the form . I am using comments in the context of a blog and I would much rather that the user stayed on the page they were on if something went wrong with the submission . As far as I can tell though , this is hard-coded in django.contrib.comments.views.comments.post_comment : Is there any way that I can change this behavior without changing the source code to django.contrib.comments ? Any pointer would be appreciated ... Thanks !"
"While playing with standard library i 've found a strange difference between python2 and python3 . If i try to catch a signal in python2 while TCPServer is running in a different thread the signal does not get handled , but in python3 it does.Here is a script that reproduces the problemThis is the output from python3 : And this is from python2 : '' ^C '' is a keyboard interrupt and `` Killed '' is sigkill that i sent to a process.Why shutdown was not called ?"
On Successfully POSTing to a form endpoint I redirect back to the same endpoint with some URL params that my client side code can interact with.But I ran into some issues trying to write a test for this code as I ca n't figure out how to test that those URL params are on my redirect URL . My incomplete test code is : I 've tried a few different methods to test this . With and without the context manager and I 've dug deep into the Flask and Werkzeug source on the test_client and test_request_context.I just want to test that the URL params for success and id exist on redirect after a valid POST .
"I am trying to compare two different lists to see if they are equal , and was going to remove NaNs , only to discover that my list comparisons still work , despite NaN == NaN - > False . Could someone explain why the following evaluate True or False , as I am finding this behavior unexpected . Thanks , I have read the following which do n't seem to resolve the issue : Why in numpy nan == nan is False while nan in [ nan ] is True ? Why is NaN not equal to NaN ? [ duplicate ] ( Python 2.7.3 , numpy-1.9.2 ) I have marked surprising evaluations with a * at the end"
"I 'm using libtorrent module in python to download torrent . I can download torrent from a private tracker but not from a public one . I tried using various torrents , which I can download using `` transmission '' . I checked it against 4 different connections , all the same.When I try I get : And it stop there.I do n't know if it help , but the private tracker is using http and not udp , and it does n't allow DHT ."
"Consider : It takes significantly longer to import only the Dispatch function rather than the entire module , which seems counter intuitive . Could someone explain why the overhead for taking a single function is so bad ? Thanks !"
"Let 's say I have a numpy array of the formWhat I would like to get from this is an array which contains only the rows which are NOT duplicates , i.e. , I expect from this exampleI 'm looking for a method which is reasonably fast and scales well . The only way that I can think to do this isFirst find the set of unique rows in x , as a new array y.Create a new array z which has those individual elements of y removed from x , thus z is a list of the duplicated rows in x.Do a set difference between x and z.This seems horribly inefficient though . Anyone have a better way ? If it is important , I 'm guaranteed that each of my rows will be sorted smallest to largest so that you 'll never have a row be [ 5 , 2 ] or [ 3 , 1 ] ."
"I need to use my own version of boto3 inside a Lambda ( Python 3.7 ) . The version included in the Lambda default Python 3.7 env ( 1.9.42 ) does not support the use of Textract for one reason or another.To do this , I did the following based on a guide : Create custom package using the following commands : pip freeze > requirements.txt which would yield this file : mkdir buildpip3 install -r requirements.txt -t build/cd buildzip -r ../boto3_layer.zip .Then I used the GUI to upload a new Lambda Layer ( called boto3Layer ) .Then I added the layer to my Lambda Function successfully.Problem is , I ca n't figure out how to import said layer into my code using the inline code editor.I have the following code : I get the error `` errorMessage '' : `` Unable to import module 'lambda_function ' : No module named 'boto3_layer ' '' I also tried importing just boto3 , but confirmed that it was the wrong version ( it was the version used by Lambda ) , so my importing did not override it.I 'd like to simply know how to import my custom layer into my code ! Thanksedit : trying the suggestion : For other users trying to accomplish the same task : virtualenv python -- python=python3.7source python/bin/activate and then pip3 install boto3zip -r boto3_layer.zip python/lib/Create new Lambda Layer with boto3_layer.zip and add layer to Lambda FunctionTried to run the save above codeFails with `` errorMessage '' : `` Unable to import module 'lambda_function ' : No module named 'boto3_layer ' '' , This ended up working by importing boto3 instead of my custom name ."
"The code below generates a 3D plot of the points at which I measured intensities . I wish to attach a value of the intensity to each point , and then interpolate between the points , to produce a colour map / surface plot showing points of high and low intensity.I believe to do this will need scipy.interpolate.RectBivariateSpline , but I am not certain on how this works - as none of the examples I have looked at include a 3D plot.Edit : I would like to show the sphere as a surface plot , but I 'm not sure if I can do this using Axes3D because my points are not evenly distributed ( i.e . the points around the equator are closer together ) Any help would be greatly appreciated.EDIT : UPDATE : I have formatted my data array ( named v here ) such that the first 8 values correspond to the first value of theta and so on . For some reason , the colorbar corresponding to the graph indicates I have negative values of voltage , which is not shown in the original code . Also , the values input do not always seem to correspond to the points which should be their positions . I 'm not sure whether there is an offset of some sort , or whether I have interpreted your code incorrectly ."
"I 've been immensely frustrated with many of the implementations of python radix sort out there on the web . They consistently use a radix of 10 and get the digits of the numbers they iterate over by dividing by a power of 10 or taking the log10 of the number . This is incredibly inefficient , as log10 is not a particularly quick operation compared to bit shifting , which is nearly 100 times faster ! A much more efficient implementation uses a radix of 256 and sorts the number byte by byte . This allows for all of the 'byte getting ' to be done using the ridiculously quick bit operators . Unfortunately , it seems that absolutely nobody out there has implemented a radix sort in python that uses bit operators instead of logarithms.So , I took matters into my own hands and came up with this beast , which runs at about half the speed of sorted on small arrays and runs nearly as quickly on larger ones ( e.g . len around 10,000,000 ) : This version of radix sort works by finding which bytes it has to sort by ( if you pass it only integers below 256 , it 'll sort just one byte , etc . ) then sorting each byte from LSB up by dumping them into buckets in order then just chaining the buckets together . Repeat this for each byte that needs to be sorted and you have your nice sorted array in O ( n ) time.However , it 's not as fast as it could be , and I 'd like to make it faster before I write about it as a better radix sort than all the other radix sorts out there . Running cProfile on this tells me that a lot of time is being spent on the append method for lists , which makes me think that this block : in radix_sort_offset is eating a lot of time . This is also the block that , if you really look at it , does 90 % of the work for the whole sort . This code looks like it could be numpy-ized , which I think would result in quite a performance boost . Unfortunately , I 'm not very good with numpy 's more complex features so have n't been able to figure that out . Help would be very appreciated.I 'm currently using itertools.chain.from_iterable to flatten the buckets , but if anyone has a faster suggestion I 'm sure it would help as well.Originally , I had a get_byte function that returned the nth byte of a number , but inlining the code gave me a huge speed boost so I did it . Any other comments on the implementation or ways to squeeze out more performance are also appreciated . I want to hear anything and everything you 've got ."
"I have many variable-sized lists containing instances of the same class with attribute foo , and for every list I must apply rules like : if there 's an element foo=A there can not be elements with foo in [ B , C , D ] if there 's an element foo=X there must by at least one with foo in [ Y , Z ] there can be between MIN and MAX elements foo=BARcombining the above three rules is probably enough to express any similar constraint I 'll ever need . It 's something like dependency checking in software packages but I have quantities and lack versions : ) a naïve approach would be : Is this a problem of Constraint programming ? I do n't actually need to solve something to get a result , I need to validate my list against some constraints and check if they are satisfied or not . How would you classify this problem and how would you solve it ? For what it 's worth , I 'm coding in Python , but I welcome a generic programming answer : ) If it turns out I must delve in constraint programming I 'll probably start by trying python-constraint ."
"So what I am doing is writing a WSGI streaming service which makes use of a Queue wrapped in an iterator to implement a multicast push . What follows is a simplified model of the service : And this is working great with twisted as the WSGI server ( as an aside , the serial generator is a separate process connected to the processor by an inter process queue ) . My question is how can I detect when a client disconnects and thus remove it from the queue ? My though is adding the queue as a tuple with the client socket i.e . ( socket , queue ) and then checking if the socket is still connected before I perform the put . However , I do n't know exactly what to grab from environ . Does any one have any experience with doing this right before I hack something together ? UpdatedHere is the solution I finally went with :"
"I just got GeoDjango up and running on my development machine . Problem is that I ca n't get a distance query to work correctly . No matter what SRID I use , the distance results are totally off . Here 's an example.The problem is that these places are much further than 1m away from point.I tried playing around with it , but have n't had much luck . Here 's an example with another SRID.I have a feeling I 'm just choosing the wrong SRIDs , but not a single one that I 've run into online has worked , or given any response that is even moderately useful.Any help is greatly appreciated !"
"I am trying to build a multi-label out-of-core text classifier . As described here , the idea is to read ( large scale ) text data sets in batches and partially fitting them to the classifiers . Additionally , when you have multi-label instances as described here , the idea is to build that many binary classifiers as the number of classes in the data set , in an One-Vs-All manner.When combining the MultiLabelBinarizer and OneVsRestClassifier classes of sklearn with partial fitting I get the following error : ValueError : The truth value of an array with more than one element is ambiguous . Use a.any ( ) or a.all ( ) The code is the following : You can imagine that the last three lines are applied to each minibatch , the code of which I have removed for the sake of simplicity.If you remove the OneVsRestClassifier and use MultinomialNB only , the code runs fine ."
Is there a simple 'wrapper ' framework for appengine ? Something like Sinatra or Juno ? So that one can write code like the following : UPDATE : I want to use the Python API ( not Java ) in GAE .
"I 've faced a really weird behaviour of textarea today . Have been bugfixing it for the whole day , and I still can not find the solution . Would appreciate any help , so thanks in advance.I 'm creating a GAE-python app.And I have this little form here : And I 'm sending data ( `` comments '' field ) via POST-request to python script.But ... Somehow I always get double line-breaks , or double CrLf , in the end which are stored in the database.But strangely when I debug the request there 's something weird ( both in FireFox+Firebug , and Chrome+DevTools ) .For example I write and send this comments content via textarea : cccIn the url-encrypted data I see c % 0D % 0Ac % 0D % 0Ac So it must be cCrLfcCrLfc But when I copy the not-encrypted var from FireBug ( DevTools ) to NotePad++ , it shows me this : c CRLF CRLFc CRLF CRLFc CRLF CRLFWhy is it doubled in the decoded format ? ! And of course when I print the result back from Database to Browser I get all those double break-lines . ( when I look at this TextProperty of the Entity via `` Datastore Viewer '' , it is written just as `` c c c '' ) .One more thing : I 'm having a flash app which sends post-requests to the same python-script , and all line-breaks made in flash 's textboxs are written correctly.But if I just try to open that database entity via textarea in browser-s interface and just save it ( without editing ) I receive all line-breaks doubled again.Is there any fixes of that ? Thank you ."
How do you overwrite the getter of the property in Python ? I 've tried to do : But the getter is calling the method of Vehicule and not of Car.What should I change ?
"I have a DataFrame which has two Timestamp columns , one of which gets used as an index for resampling , the other remains as a column . If I resample the DataFrame , the one remaining as a column gets dropped . I would like it to be resampled as if it were a numeric column ( which it is really ) : DataFrame has an index , 1 numeric column , and one Timestamp columnNow resample to daily : My init_time column has been dropped . I could convert it to a raw integer column first , but is there an easier way ?"
"I am trying to use matplotlib from Python 3 to achieve something like the image shown below : Similar question has been asked here but accepted answer is not sufficient for my need . I need to add text in the middle of dotted line ( which I can plot with plt.axvline ( ) function ) . Here is what I triedIf I can put this text in the middle of dotted line , it will be great ."
"I want to expand the list in a certain column ( in the example column_x ) to multiple rows . Soshall be transformed from toThe code I have so far does exactly this , and it does it fast.However , I have lots of columns . Is there a neat and elegant solution for repeating the whole data frame without specifying each column again ?"
"I am trying to find points that are closest in space in X and Y directions ( sample dataset given at the end ) and am looking to see if there are smarter approaches to do this than my trivial ( and untested ) approach . The plot of these points in space looks something like the following and am trying to find sets of points marked inside the boxes i.e . the output I am looking for is a set of groups : For the horizontal bands , I am thinking I could just go ahead and use small sliding windows of size say , 5 or 10 ( which should really be determined from the global information of which size will give the maximum grouped points but I am still exploring a good approach ) and search for contiguous points because a break would not be considered a horizontal band anymore . I am guessing the same approach works for the vertical bands as well but not in all cases because there is a subtle difference in horizontal and vertical bands : points should appear close to be considered a group horizontally but they can appear anywhere to be considered part of a vertical band . Observe the large vertical band in the figure . So I am guessing I could just look for points that have the same x-coordinate ( in this case , x=68 ) should give me a lot of points . Other than this trivial solution , I ca n't think of anything smart that can be done here as this problem appears deceptively simple to me . Am I missing something here ? Does this fall into some known class of problems and if so , is there a good and scalable approach to achieve this ? Sample Dataset :"
I have created a sample api by using djangorestframework which worked fine and I could use browsable api without any problem . Then I added outh2 authentication as mentioned on official website which also worked fine . I can use following to get access token.I can also use curl to browse my api by using access token.But while using browsable api I ca n't browse my api due to obvious reason that browsable api is neither getting any access token nor using any . I think I need to have login for this . So I am not getting where can I customise the current login I am having to use oauth .
I have this simple scrappy code . However get this error when i use response.urljoin ( port_homepage_url ) this portion of the code.What could be wrong ? Here is the error log .
"I am using the neurolab for Machine learning classfication problem , Link : - http : //code.google.com/p/neurolab/My question is , can we train the neural network incrementally ? To explain further , I have three parts of input data , I want to train the Neuro Net bywill the train call with first two parts will be effective in predicting the neural net parameters -OR- Will this use only last training data ?"
"I 'm trying to build a project that uses TensorFlow Serving , so I made a directory my_dir with a WORKSPACE file , cloned the serving repo into it , put my custom files into a directory my_project , configured tensorflow within tensorflow_serving , built tensorflow serving from my_dir/serving witheverything builds fine there , then I try to build a python file imitating mnist_export and put it in my_dir and make a BUILD fileHowever , when I run I get the following errors : In my WORKSPACE file , I have the following : My hypothesis is that because tensorflow is a sub-sub-project it is not putting its generated files in the grandparent-projects bazel-out . However , I 've tried many things and have n't been able to get it to work ."
This is giving me Where am I going wrong ?
"We are using jinja2 to create our html but , because of the many loops and other things we do in jinja to produce the html , the html 'looks ' ugly ... . ( note : this is just for aesthetics ) . Is there anything we can do to clean up the html ? ( Other than the obvious of cleaning up our jinja2 code , which would make our template somewhat unreadable to us staffers ) Something like beautiful soup 's prettify ? ( Yes , I realize this question is a pretty nit-picky question ... the ocd in me says to clean it up ) . for instance : Pretty ugly , eeh ?"
"How can I spread a long with clause in Python over multiple lines ? Right now , I haveI would like to follow Google 's Python style guide , which proscribes backslash line continuation ."
I am trying to convert Robert Penner 's Easing Functions into python and getting stuck ! Any help or any one else done this before ? https : //github.com/danro/jquery-easing/blob/master/jquery.easing.js
"I 'm using sklearns OrthogonalMatchingPursuit to get a sparse coding of a signal using a dictionary learned by a KSVD algorithm . However , during the fit I get the following RuntimeWarning : In those cases the results are indeed not satisfactory . I do n't get the point of this warning as it is common in sparse coding to have an overcomplete dictionary an thus also linear dependency within it . That should not be an issue for OMP . In fact , the warning is also raised if the dictionary is a square matrix.Might this Warning also point to other issues in the application ?"
"Creating a 2D array such as and indexing using the colon operator like thisworks as expected . It returns all of row 2.However , if I want to retrieve all of column 2 , I would instinctively doBut this also returnsWhat is the reasoning behind this ? I would intuitively think that this returns the column 2 of each row . ( Also , I am aware I can use numpy to do x [ : ,2 ] or I could use list comprehensions to accomplish this , that 's not my question )"
I have a list of words and I would like to store them in a nested dictionary . Here is the sample list : -The dictionary that I want to create is as follows : -The words are case insensitive .
"I am trying to fetch meta data of around 10k+ torrents per day using python libtorrent.This is the current flow of codeStart libtorrent Session.Get total counts of torrents we need metadata for uploaded within last 1 day.get torrent hashes from DB in chunks create magnet link using those hashes and add those magnet URI 's in the session by creating handle for each magnet URI.sleep for a second while Meta Data is fetched and keep checking whether meta data s found or not.If meta data is received add it in DB else check if we have been looking for meta data for around 10 minutes , if yes then remove the handle i.e . dont look for metadata no more for now.do above indefinitely . and save session state for future.so far I have tried this.I tried kept above script running overnight and found only around 1200 torrent 's meta data is found in the overnight session.so I am looking for improve the performance of the script . I have even tried Decoding the save_state file and noticed there are 700+ DHT nodes I am connected to . so its not like DHT is not running , What I am planning to do is , keep the handles active in session indefinitely while meta data is not fetched . and not going to remove the handles after 10 minutes if no meta data is fetched in 10 minutes , like I am currently doing it.I have few questions regarding the lib-torrent python bindings.How many handles can I keep running ? is there any limit for running handles ? will running 10k+ or 100k handles slow down my system ? or eat up resources ? if yes then which resources ? I mean RAM , NETWORK ? I am behind firewall , can be a blocked incoming port causing the slow speed of metadata fetching ? can DHT server like router.bittorrent.com or any other BAN my ip address for sending too many requests ? Can other peers BAN my ip address if they find out I am making too many requests only fot fetching meta data ? can I run multiple instances of this script ? or may be multi-threading ? will it give better performance ? if using multiple instances of the same script , each script will get unique node-id depending on the ip and port I am using , is this viable solution ? Is there any better approach ? for achieving what I am trying ?"
"I would like to call a Django URL with an argument containing a forward slash . For example , I would like to call mysite.com/test/ with 'test1/test2 ' . ( e.g . naively , the url would be mysite.com/test/test1/test2 ) urls.py : Accessing either of the following fails : mysite.com/test/test1/test2mysite.com/test/test1 % 2Ftest2 ( % 2F is the standard URL encoding for forward slash ) with the below error : Error : I would expect using the path in 1 to fail , but I am surprised the path in 2 fails.What is the correct way of going about this ? Using Django 2.0.2"
"The context : my Python code pass arrays of 2D vertices to OpenGL.I tested 2 approaches , one with ctypes , the other with struct , the latter being more than twice faster.Any other alternative ? Any hint on how to accelerate such code ( and yes , this is one bottleneck of my code ) ?"
"I am trying to analyze some messy code , that happens to use global variables quite heavily within functions ( I am trying to refactor the code so that functions only use local variables ) . Is there any way to detect global variables within a function ? For example : Here the global variable is y since it is n't given as an argument , and neither is it created within the function.I tried to detect global variables within the function using string parsing , but it was getting a bit messy ; I was wondering if there was a better way to do this ? Edit : If anyone is interested this is the code I am using to detect global variables ( based on kindall 's answer and Paolo 's answer to this question : Capture stdout from a script in Python ) : dis by default does not return output , so if you want to manipulate the output of dis as a string , you have to use the capture decorator written by Paolo and posted here : Capture stdout from a script in Python"
"Possible Duplicate : Weird timezone issue with pytz This seems wrong : 'America/Edmonton ' and 'US/Eastern ' should be the same time zone ( 17:00:00 STD ) . Not to mention 16:26:00 does n't make any sense. -- Update -- The above makes sense in context of Jon Skeet 's answer . However , things get strange when I do this : I created a naive date . Since 'America/Edmonton ' is my timezone , I try to set that manually : This should not have change anything because that is the correct TZ . However : This should give me an offset of 2 hours ( difference between 'US/Eastern ' and 'America/Edmonton ' ) but it gives me 3 hours 26 minutes ( which is 2 hours plus one hour 26 minutes : D ) inserting timezone ( 'US/Mountain ' ) produces the correct result in astimezone ( ) . Creating an aware datetime with 'America/Edmonton ' will also work correctly ."
"I am trying to generate some output on segmentation faults ( and other signals ) with a python script in gdb . The script looks like this : The problem is that I need to press c and Enter on every segmentation fault , the script does not continue . If I do in the handler , I get a StackOverflow . I think this is because execute ( ) never returns . If I do My handler is not invoked any longer . How can I continue after the handler ?"
"Having the following model : Something like : does n't work , cf . Using .aggregate ( ) on a value introduced using .extra ( select= { ... } ) in a Django Query ? and https : //code.djangoproject.com/ticket/11671.The raw SQL which works is the following : How can I get hte same results through Django 's ORM ? FYI : translates to :"
"Author note : You might think that this post is lacking context or information , that is only because I do n't know where to start . I 'll gladly edit with additional information at your request.Running scrapy I see the following error amongst all the link I am scraping : When I run scrappy simply on that single URL using : No errors are occurring . I am scrapping thousands of similar links with no problem but I see this issue on ~10 links . I am using the default 180 seconds download timeout from scrappy . I do n't see anything wrong with these links in my web browser too.The parsing is initiated by the request : Which is handled in the functions : Also : I did n't change defaults settings for download retries in scrappy ( but I do n't see any retries in my log files either ) .Additional notes : After my scraping completed and since dont_filter=True I can see that links that failed to download with the previous error at some point , did n't fail when called in previous and subsequent requests.Possible answer : I see that I am getting a KeyError on one of the spiders and that de-allocation of that spider failed ( remove_request ) . Is it possible that it is because I am setting dont_filter=True and doing several requests on the same URL and that the key of the spider seems to be that URL ? That the spider was de-allocated by a previous , concurrent request on the same URL ? In that case how to have a unique key per request and not indexed on the URL ? EDITI think my code in parse_player was the problem , I do n't know for sure because I edited my code since , but I recall seeing a bad indent on yield request . Let me know if you think that might have caused the issue ."
I have a df : I would like to use two different dictionaries to change the values in ColB . I would like to use d1 if the value in ColA is 1 and d2 if the value in ColB is 2.Resulting in : How would be the best way of achieving this ?
"I am having a saved list of Python dict keys : [ 'level_one ' , 'level_two ' , 'test ' ] These are the keys of a dictionary : Normally I could set the test key like this : mydict [ 'level_one ' ] [ 'level_two ' ] [ 'test ' ] = `` Hello StackOverflow '' Unfortunately , the list/dict are generated on the fly , and I do n't know how deep it can go . Is there a possibility to update the dict key by the list of keys without using braces ? I only was able to reflect this functionality for getting the string : Not a duplicate . This is regarding setting , not getting of nested dictionary ."
This is my function : test function : get_value does n't print : match not found !
i have the following problem . I have a numpy array like this : Now i want to convert it to a vtk Table . Is this possible ? Best regards !
"I have a DataFrame df where each record represents a soccer game . Teams will appear more than once . I need to compute some sort of a rolling mean for each team scores ( well , not exactly the rolling mean to the letter ) .What I need to calculate , is the mean score for each team ( home and away ) . For brevity , let 's just do the home column : This results in : Question starts hereNow , I need to compute `` rolling mean '' for teams . Let 's do it by hand for the group named Arsenal . At the end of this we should wind up with 2 extra columns , let 's call them : rmean_h and rmean_a . First record in the group ( 167 ) has scores of 0 and 2 . The rmean of these is simply 0 and 2 respectively . For second record in the group ( 164 ) , the rmeans will be ( 0+5 ) /2 = 2.5 and ( 2+1 ) / 2 = 1.5 , and for the third record , ( 0+5+0 ) /3 = 1.66 and ( 2+1+0 ) /3 = 1 . Our DataFrame should now looks like this : I want to carry out these calculations for my data , do you have any suggestions please ?"
"I 'd like to convert a MathML expression to an equation string in python , for which the MathDOM module should be good for.An example would be : should map to `` A + B '' . This should obviously work with more complex expressions.However , it is quite old and not working properly with new versions of the xml module ( trying to include the wrong module structure , etc . ) Does anyone know useful alternatives ?"
"I 'm attempting to move a project from virtualenv to buildout , but I do n't think I 've grasped the whole concept of buildout . All the tutorials I 've found discuss buildout in the context of using it with Zope , which I 'm not using and therefore ca n't see how to continue.My file structure with virtualenv is as follows : Which is run using /path/to/venvs/myapp/bin/python /path/to/myapp/script.py.With buildout , my file structure is : Running python bootstrap.py and bin/buildout gives me these additional files : At this point I 'm unsure how to `` run '' my app.Advice ?"
"I have a django project that uses a redis ( just one db in redis ) .Currently I do this : In my settings.py file , I have : Anytime I want to call this connection ( on my many views.py file in different apps in the project ) I do it this way : Is there a problem with this approach ? I do not want to keep creating new connections to redis if it is not necessary ."
"I have a DataFrame with intraday data indexed with DatetimeIndexso as can be seen there is a big gap between within the two days in df3df3.plot ( ) will plot every single hour from 2000-01-01 00:00:00 to 2000-01-02 05:00:00 , while actually from 2000-01-01 06:00:00 to 2000-01-02 00:00:00 there are actually no datapoint.How to leave those data point in the plot so that from 2000-01-01 06:00:00 to 2000-01-02 00:00:00 is not plotted ?"
In my views.py I haveThe value MY_SETTING is defined in settings.py . However I get the following error when I try and load the view : What 's going on ?
"I would like to have a function that formats a float to a variable length of precision . For example , if I pass in n=2 , I would expect a precision of 1.67 ; if I pass in n=5 , I would expect 1.66667.I currently have the following , but I feel like there would be an easier way to do so . Is there ?"
"Possible Duplicate : Understanding Python super ( ) Class B subclasses class A , so in B 's __init__ we should call A 's __init__ like this : But with super ( ) , I saw something like this : My questions are : Why not super ( B , self ) .__init__ ( self ) ? Just because the return proxy object is a bound one ? If I omit the second argument in super and the return proxy object is an unbound one , then should I write super ( B ) .__init__ ( self ) ?"
I wanted to have a list of lambdas that act as sort of a cache to some heavy computation and noticed this : AlthoughMeaning that the lambdas are unique functions but they somehow all share the same index value.Is this a bug or a feature ? How do I avoid this problem ? It 's not limited to list comprehensions ...
Is there any way to combine groups and the * features of regular expressions to act kindof like a tokenizer / splitter . I tried this : I was hoping my groups might look likeBut it does not . I was surprised by this because the ? and group features do work together :
"I have the problem on importing the class in the same package , and it seems not a circular dependency problem . So I 'm really confused now.I declared an exception in exceptions.py and wants to use it in lexer.py : exceptions.py : and in lexer.py : It should n't be circular dependency since lexer.py is the only file has import in it.Thanks ! !"
"The problem I have to solve is one that takes a nested list as an input , and returns the same nested list , except each element is the square of the element that previously existed in that spot . This is my codeNow I am getting an error that is saying 'int ' object is not iterable . I 'm assuming that means that it 's trying to run the loop for an integer type , which does n't make sense to me , given that I only rerun the function for those of list types ."
"I am trying to setup caching on a web server I have built using Pyramid . I am using pyramid_beaker , which creates a back-end to use Beaker for caching.I have been using cached region decorators to implement the caching . A sample cached region looks like this : Now that the caching works fine and I can trigger cache refresh on each region , I was wondering how I might refresh ALL regions ?"
"I 've written a Dataflow job that works great when I run it manually . Here is the relevant section ( with some validation code removed for clarity ) : Now I want to create a template and schedule it to run on a regular basis with a dynamic ENDDATETIME . As I understand it , in order to do this I need to change add_argument to add_value_provider_argument per this documentation : https : //cloud.google.com/dataflow/docs/templates/creating-templatesUnfortunately , it appears that ValueProvider values are not available when I need them , they 're only available inside the pipeline itself . ( please correct me if I 'm wrong here ... ) . So I 'm kind of stuck . Does anyone have any pointers on how I could get a dynamic date into my query in a Dataflow template ?"
"I am inserting a list of dictionaries to a PostgreSQL database . The list will be growing quickly and the number of dict values ( columns ) is around 30 . The simplified data : Inserting the data into the PostgreSQL database with the following code does work ( as in this answer ) , but I am worried about executing too many queries.Is there a better practice ? Thank you so much in advance for your help !"
"I have my data as a DataFrame : My input will be 10 rows ( already one-hot encoded ) . I want to create an n-dimensional auto encoded representation . So as I understand it , my input and output should be the same.I 've seen some examples to construct this , but am still stuck on the first step . Is my training data just a lot of those samples as to make a matrix ? What then ? I apologize for the general nature of the question . Any questions , just ask and I will clarify in the comments.Thank you ."
how do I get the author/username from an object using : I 'm using Google 's gdata.youtube.service Python libraryThanks in advance ! : )
"I am currently playing around with an example from the book Violent Python . You can see my implementation here I am now trying to implement the same script in Go to compare performance , note I am completely new to Go . Opening the file and iterating over the lines is fine , however I can not figure out how to use the `` crypto '' library to hash the string in the same way as Python 's crypt.crypt ( str_to_hash , salt ) . I thought it maybe something likeHowever , no cigar . Any help would be much appreciated as it 'd be really interesting to compare Go 's parallel performance to Python 's multithreaded . Edit : Python docs for crypt.crypt"
"I am trying to reproduce in Python two examples ( originally written in Java ) that I found in a book . The two functions check if a string contains repeated characters . The first function uses an integer ( checker ) as a bit vector , while the second function simply uses a list of booleans.I was expecting to have a better performance using the function with bits , but actually it performs worse.Why is that ? Did I write something wrong while `` translating '' from Java to Python ? Note : for simplicity we only use lowercase letters ( a to z ) , especially for the bit vector function.Results : The original Java functions are the following :"
"I 'm trying to catch mysql/sqlalchemy OperationalErrors and replace handle access denied ( 1045 ) differently from connection refused ( 2003 ) I just ca n't seem to find any documentation on how to tell these apart programmatically . I dived into the sources and thought I could check the value of err.orig.original_exception.errno but that was not the case.Edit : err.orig does n't seem to be defined for access denied which might be a bug.This issue really bugs me and even the bounty is running out with no news . I 'm starting to believe it must be a bug in sqlalchemy but the sqlalchemy documentation is not very descriptive in that regard and I 'm new to sqlalchemy and python in general so it 's really hard for me to tell . I could n't find support on irc either , where do I go from here ?"
"I have a QuerySet of some objects . For each one , I wish to annotate with the minimum value of a related model ( joined on a few conditions , ordered by date ) . I can express my desired results neatly in SQL , but am curious how to translate to Django 's ORM.BackgroundLet 's say that I have two related models : Book , and BlogPost , each with a foreign key to an Author : I 'm trying to find the first mystery book that a given author published after each blog post that they write . In SQL , this can be achieved nicely with windowing.Working solution in PostgreSQL 9.6Translating to Django 's ORMWhile the above SQL suits my needs well ( and I could use raw SQL if needed ) , I 'm curious as to how one would do this in QuerySet . I have an existing QuerySet where I 'd like to annotate it even furtherI 'm aware that Django 2.0 supports window functions , but I 'm on Django 1.10 for now.Attempted solutionI 'd first built a Q object to filter down to mystery books published after the blog post.From here , I attempted to piece together django.db.models.Min and additional F objects to acheive my desired results , but with no success.Note : Django 2.0 introduces window expressions , but I 'm currently on Django 1.10 , and curious how one would do this with the QuerySet features available there ."
"I 'm following the django-rest-framework tutorial and I ca n't figure out what is happening here.I 've created a UserSerializer class with a snippets attribute and I did all the importsThen I 've created the UserList and the UserDetails views : and I 've follow the tutorial pretty much , but when I try to access one of the users endpoints I got an AttributeErrorI am using django 1.7.5 and django-rest-framework 3.0.5I do n't know if it is a problem for this specific version.EDIT : This is my Models :"
"Here is a ( slightly messy ) attempt at Project Euler Problem 49.I should say outright that the deque was not a good choice ! My idea was that shrinking the set of primes to test for membership would cause the loop to accelerate . However , when I realised that I should have used a set ( and not worry about removing elements ) , I got a 60x speed-up.Anyway , before I thought to use a set , I tried it out in Pypy . I found the results to be rather suprising : Why is Pypy over five times slower on this code ? I would guess that Pypy 's version of the deque is the culprit ( because it runs faster on the set version ) , but I have no idea why that is ."
"I 've 2 DynamicDocuments : I want to filter the UserTasks document by checking whether the flag value ( from Tasks Document ) of the given task_id is 0 or 1 , given the task_id and user_id . So I query in the following way : -This fetches me an UserTask object.Now I loop around the task list and first I get the equivalent task and then check its flag value in the following manner.Is there any better/direct way of querying UserTasks Document in order to fetch the flag value of Tasks Document.PS : I could have directly fetched flag value from the Tasks Document , but I also need to check whether the task is associated with the user or not . Hence I directly queried the USerTasks document ."
"I 'm trying to implement an LSTM in CNTK ( using Python ) to classify a sequence.Input : Features are fixed length sequences of numbers ( a time series ) Labels are vectors of one-hot valuesNetwork : Output : A probability that the sequence matches a labelAll sizes are fixed , so I do n't think I need any dynamic axis and have n't specified any.However , CNTK is not happy and I get : If ( as per some of the examples ) I define label with a dynamic axisIt no longer complains about this , and gets further to : But dies with this error : What do I need to do to fix this ?"
"I 'm trying to come up with a regexp in Python that has to match any character but avoiding three or more consecutive commas or semicolons . In other words , only up to two consecutive commas or semicolons are allowed.So this is what I currently have : And it seems to work as expected : But as I start to increase the length of the input text , the regexp seems to need way more time to give a response.And finally it gets completely stuck at this stage and the CPU usage goes up to 100 % .I 'm not sure if the regexp could be optimized or there 's something else involved , any help appreciated ."
"I am just getting started with Django internationalization and trying to understand the best practices for using { % blocktrans % } . Is it preferable to use one { % blocktrans % } for each paragraph , or should I have one big { % blocktrans % } that contains many paragraphs ? Having one big { % blocktrans % } is faster and makes my template look cleaner , but my concern is that : it causes HTML tags ( like < p > ... < /p > ) to become part of the translation stringIf I change one thing in one part of my huge block , the msgid would change , which seems like it could affect the other paragraphs . If I have smaller blocks , the changes would be more isolated ( I suppose ) .If I make a formatting change like adding/removing a newline in between paragraphs , that would change the msgid.I am also wondering about formatting . Are there any complications to having line breaks inside a { % blocktrans % } ? Or having leading spaces ? e.g . : Any recommendations are welcome ."
"I 'm using python mock for patching some functions and classes when testing views in Django.If I run each test independently , all test works . But when I run the TestCase , some test dont work ( the patch has not effect ) .I have a print int the views to see each mocked method , when it works it prints : MagicMock name='get_user_category ' id='162815756'And when does n't works I see : function get_user_category at 0x8e0fb8cI tried the patcher start ( ) and stop ( ) but I still have problems.¿What is the problem ?"
I 'm using virtualenv and trying to host my django app . I 'm using Python 3.5 and Django 1.9.2 . I can run import django fine . When I run I get the errorMy Path : Any ideas ?
"I want to delete all the characters `` \L '' that I find when i read the file . I tried to use this function when I read a line : But it does n't delete this character . Does someone know how to do it ? I tried using the replace function as well , but it is not better : Thanks for your answers ."
"My questionSuppose I have a Series like this . I want to just construct a bar plot to compare month to month , which seems quite simple with I really do n't need anything much more than that , but my issue is that the dates look terrible on the x-axis - I subsequently wanted to format the dates so that I just provide the year and month , some format like % Y- % m . How can I do so ? My strugglesSo , looking at the documentation for pandas.Series.plot I see the xticks argument that can be passed , and figure I can use strftime to format the dates and pass as a sequence.. but this does n't work because the ticks require numerical values or dates , not strings . So then I figure I should just use raw matplotlib instead , so that I can use set_major_formatter with a DateFormatter to modify the tick labels . But if I just use plt.bar , that introduces a new issue - the entire date range is used , which makes sense . At this point I 'm quite convinced I 'm missing something and there 's a trivially simple way to do this ."
"I want to build a grid from sampled data . I could use a machine learning - clustering algorithm , like k-means , but I want to restrict the centres to be roughly uniformly distributed.I have come up with an approach using the scikit-learn nearest neighbours search : pick a point at random , delete all points within radius r then repeat . This works well , but wondering if anyone has a better ( faster ) way of doing this.In response to comments I have tried two alternate methods , one turns out much slower the other is about the same ... Method 0 ( my first attempt ) : Method 1 ( using the precomputed graph ) : Method 2 : Running these as follows : Method 0 and 2 are about the same ..."
"What I 'm trying to do in this example is wrap an image around a circle , like below.To wrap the image I simply calculated the x , y coordinates using trig.The problem is the calculated X and Y positions are rounded to make them integers . This causes the blank pixels in seen the wrapped image above . The x , y positions have to be an integer because they are positions in lists.I 've done this again in the code following but without any images to make things easier to see . All I 've done is create two arrays with binary values , one array is black the other white , then wrapped one onto the other.The output of the code is . I later found out about the Midpoint Circle Algorithm but I had worse result with thatCan anybody suggest a way to eliminate the blank pixels ?"
"I am using a 2D Lidar and getting the data as angle and distance with respect to lidar Position . I have to create a floor plan using Lidar and the data is given bellow is represent a room . I want to use the RANSAC algorithm to find the wall of the room . If I could fit RANSAC then I believe somehow I could find the floor flan . I have written a code but it 's not fitting into my wall . what modification should I do so that it will fit into my room wall ? I have converted this data into cartesian and plotted this using Matplotlib.After I have done the RANSAC , it shows a blue line in the picture that is not detecting the wall properly . I should find out 4 walls . I do not know which modification I should do in my code . Here is the code : I will be so glad if you could suggest me to achieve the floor plane and surface area . If you have any questions , please drop it into the comment section ."
"I have the following script 186.py : The last line prints `` Done '' but python does n't exit when this happens . I type the following command : And get the results : But I have to ctrl+C to get the time : After the program outputs `` Done '' python CPU usage is very little ... but the memory usage continues to grow ... I used ctrl+C once it got to 80 % of my system memory ( its an old system ) .What is going on here ? What is the program doing after Done is printed ? Should n't it be done ? Thanks , Dan"
"I am diving into the SciPy LinAlg module for the first time , and I saw this function : What does __array_wrap__ do exactly ? I found the documentation , but I do n't understand this explanation : Does this just mean it reconverts the output of whatever function back into an array since it was likely broken up into something else for element-by-element processing ? Relatedly , regardless of the explanation , what would it mean to get this wrap as an object ? What would you do with it ? I am looking at the code for numpy.linalg.inv ... what is wrap doing here ?"
"RethinkDB is a wonderfull and very handy NoSQL Database engine . I looking for the best way to insert Python datetime objects . RethinkDB strores UTC timestamps , so I found a solution to convert my datetime object in the right format.I use this litle function to convert my datetime object in somethink RethinkDB understand : My current timezone is CET ( GMT + 2 hours ) Is this a good solution for storing my dates in rethinkdb or a better solution exists ? Thanks for your help"
"I am wondering if there already exists a way to expand selection to all areas suggested by Sublime Text 2 boxes , so one does not have to keep pressing Ctrl-D and sometimes Ctrl-K-D as shown in How do I skip a match when using Ctrl+D for multiple selections in Sublime Text 2 ? 90 % of time the boxes on the screen are exactly what I want to multiple select , and it would be very handy if there was a one key option to do so.for example , if you have and you click on the first 2 , the boxes will be shown around only the three single 2s . These I would like to select with a single command/macro.If you go with Ctrl-D you have to skip the 2s in 42 and 23 with Ctrl-K-D.If nothing like this exists , any pointers on writing a plugin for such functionality would be much appreciated.UPDATE : I 've gotten a great suggestion to use Alt+F3 , which is awesome for making changes in the whole file . However , it would be also beneficial to limit the scope of multiple select to current visible page or tag or brackets or something else ."
"datetime.now ( ) and datetime.today ( ) return time in UTC on my computer even though the documentation says they should return local time.Here 's the script I ran : and here 's the output : The output of running date right after it is : Why is my installation returning time in UTC ? What can I do get those functions to return local time ? PS We are in MST , which is UTC-7.PS 2 I realize there are methods to convert a UTC time to local time , such as those explained in Convert a python UTC datetime to a local datetime using only python standard library ? . However , I am trying to understand the cause of the fundamental problem and not looking for a method to patch the problem in my own code.In response to comment by @ jwodder : The output of executingis :"
"I would like to create a python directory whose values are to be evaluated separately . So , for example , in the following non-working example I definefor which e.g.and after processing of the dict I want to have e.g.The example does not work since the value of key key3 is immediately evaluated , and foo ( 20 ) is no callable . The way it could work would be to use something likebut here foo will miss its arguments . One way to handle this would be to define the dict in the following way with the following processing schemein which process is a meaningful function checking if its argument is a list , or if the first element is a callable which gets called with the remaining arguments . My question is about a better way/idea to implement my basic idea ?"
Why is that invalidbut this one is not
"Say I haveand I want to write a function that may accept Foo instance or some of Foo subclasses ( not instances ) . So I wroteAnd get such errors : Is it impossible to specify that type in python , or I used wrong approach ?"
"I 'm using Django 1.6 with PostgreSQL and have following model : I 'm trying to get all games and to each of them add count of related videos , reviews and images as follows : Result query is : My question is - why there are all selected fields in GROUP BY clause ? And most importantly , how can I get rid of them besides raw SQL ? I gathered that it would be good to use .values ( ) , but I want every field in result query . If I just use GROUP BY `` content_game '' . `` id '' , the result is the same , but I do n't know how to use it like this with Django ORM ."
original dictionary keys are all integers . How can I convert all the integer keys to strings using a shorter approach ? prints :
"I would like to merge two DataFrames while creating a multilevel column naming scheme denoting which dataframe the rows came from . For example : If I use pd.merge ( ) , then I getWhich is what I expect with that statement , what I would like ( but I do n't know how to get ! ) is : Can this be done without changing the original pd.DataFrame calls ? I am reading the data in the dataframes in from .csv files and that might be my problem ."
"PEP 3119 states that : The @ abstractmethod decorator should only be used inside a class body , and only for classes whose metaclass is ( derived from ) ABCMeta . Dynamically adding abstract methods to a class , or attempting to modify the abstraction status of a method or class once it is created , are not supported.I can not find , however , an explanation of why that is . Specifically , I do not notice a difference in behavior when using only @ abstractmethod in a class that does not explicitly inherit from ABCMeta . In the following simple example , if I understand correctly , the proper way of doing things would be : However , if I let the Base class inherit simply from object , and only use the decorator when needed , I notice no change in behavior . I have found this to be the case even on more complex architectures , so I wonder : when does the latter method fail ?"
"What ravi file is : A RAVI file is a video file created by thermal imaging software , such as Micro-Epsilon TIM Connect or Optris PIX Connect . It contains video captured by thermal cameras and is saved in a format similar to the Audio Video Interleave ( .AVI ) format . RAVI files also store radiometric information , such as temperature and measurement area information collected by the thermal camera.My issue : I have to work with data from the ravi file . I need the temperature value for the pixels ( Or the maximum temperature of the frame is enough for me ) . I would like to check the maximum temperature on a certain frame . The final result would be a report which contains the maximum temperature values on frames ( It would be a graph ) . It is easy to check and process with Micro-Epsilon TIM Connect or Optris PIX Connect tools but I am not able to use them ( I have to write an own one ) . My questions : How can I get the data from ravi file ( Actually I need only the temperature values ) ? Is there any converter to convert ravi file to another ( It is not relevant if I can get the data from ravi file ) ? Note : The Python language is the preferred but I am open for every idea.I have to work with the ravi files and I am not able to record new files or modify the recording.I have found a site which provides a SDK for this type of camerabut it is not clear for me that get data from ravi file ispossible . Link to libirimager2 documentation : libirimager2If I play the ravi file with a media player then it says the used codeck is : Uncompressed packed YUV 4:2:2 ( You can see the getting stream below ) If I parse it with OpenCV or play with a media player , I can see something stream . But I am not sure how I can get the temperature ... CV2 code : Getting stream : ( I see the same `` pink and green '' stream in a simple media player as well . ) Stream in the official software : ravi file in HexEditor : I have found a site about AVI video format . You can see below some lines from begging of my file , perhaps it can help . Testing materials : If you download the PIX Connect Rel . 3.6.3046.0 Software from http : //infrarougekelvin.com/en/optris-logiciel-eng/ site , you can find several ravi files in the `` Samples '' folder inside zip.Additional info from an official documentation : Software for thermoIMAGER TIM Infrared camera documentationVideo sequences can both be saved as a radiometric file ( RAVI ) or as a non-radiometric file ( AVI ) . RAVI filescontain all temperature as well as measure area information.If Radiometric Recording , see Chap . 5.6.2 , is not activated the images will be saved as standard AVI fileonly containing color information . A later conversion of a RAVI file into an AVI file and vice versa is notpossibleUpdate : I have tried to use the PyAV module to get data . This module is able to handle the yuyv422 format . I got the same `` green-pink '' stream and I was not able to get the temperature from it ... Used code : The output of script :"
I 've made a Python application which can load plugins . These plugins are loaded based on a name and path . I am currently usingand then getting a class instance in the module this waySince the imp lib is deprecated I want to use importlib . And the only similar method of getting my class instance was to useThe weird thing here is that ( I am using pyCharm as IDE ) . when I run my code in debugging mode the above command works fine and I get my class instance . however running the code normally gives me the following error . Why is there a difference between run and debug.Is there an alternative way of doing what I want . Ive also tried Which also gives me the correct data however I can not load the module this way or at least I do not know howRegardsAnders
Could you help me solve the following incompatibility issue between Python 2.5 and 2.6 ? logger.conf : module_one.py : module_two.py : logger.py : Output of calling logger.py under Ubuntu 9.04 :
"I first explain my task : I have nearly 3000 images from two different ropes . They contain rope 1 , rope 2 and the background . My Labels/Masks are images , where for example the pixel value 0 represents the background , 1 represents the first rope and 2 represents the second rope . You can see both the input picture and the ground truth/labels here on picture 1 and 2 below . Notice that my ground truth/label has only 3 values : 0 , 1 and 2.My input picture is gray , but for DeepLab i converted it to a RGB Picture , because DeepLab was trained on RGB Pictures . But my converted picture still does n't contain color.The idea of this task is that the Neural Network should learn the structure from ropes , so it can label ropes correctly even if there are knotes . Therfore the color information is not important , because my ropes have different color , so it is easy to use KMeans for creating the ground truth/labels.For this task i choose a Semantic Segmentation Network called DeepLab V3+ in Keras with TensorFlow as Backend . I want to train the NN with my nearly 3000 images . The size of alle the images is under 100MB and they are 300x200 pixels.Maybe DeepLab is not the best choice for my task , because my pictures does n't contain color information and the size of my pictures are very small ( 300x200 ) , but i did n't find any better Semantic Segmentation NN for my task so far.From the Keras Website i know how to load the Data with flow_from_directory and how to use the fit_generator method . I do n't know if my code is logical correct ... Here are the links : https : //keras.io/preprocessing/image/https : //keras.io/models/model/https : //github.com/bonlime/keras-deeplab-v3-plusMy first question is : With my implementation my graphic card used nearly all the memory ( 11GB ) . I do n't know why . Is it possible , that the weights from DeepLab are that big ? My Batchsize is default 32 and all my nearly 300 images are under 100MB big . I already used the config.gpu_options.allow_growth = True code , see my code below.A general question : Does somebody know a good semantic segmentation NN for my task ? I do n't need NN , which were trained with color images . But i also do n't need NN , which were trained with binary ground truth pictures ... I tested my raw color image ( picture 3 ) with DeepLab , but the result label i got was not good ... Here is my code so far : Here is my code to test DeepLab ( from Github ) :"
"Are there any open source libraries ( any language , python/PHP preferred ) that will tokenize/parse an ANSI SQL string into its various components ? That is , if I had the following stringI 'd get back a data structure/object something likeRestated , I 'm looking for the code in a database package that teases the SQL command apart so that the engine knows what to do with it . Searching the internet turns up a lot of results on how to parse a string WITH SQL . That 's not what I want . I realize I could glop through an open source database 's code to find what I want , but I was hoping for something a little more ready made , ( although if you know where in the MySQL , PostgreSQL , SQLite source to look , feel free to pass it along ) Thanks !"
When using imshow ( ) the z-value of the mouse pointer is shown in the status line as shown in the screen shot ( on the right ) : How do I achieve the same behavior with pcolormesh ( ) ? The image was generated by the following code :
"I came across a file in our project , called - wait for it - celery.py . Yes , and celery.py imports from the installed celery module ( see http : //www.celeryproject.org/ ) which is not an issue because the project 's celery.py usesbefore importing from the installed celery module . Now , the problem comes from djcelery ( django-celery ) which also would like to import from celery ( the installed one , not the project celery.py ) . This is where the clash comes because djcelery encounters the project 's celery.py before it encounters the installed celery.How can I resolve this ?"
"Let 's assume there is a line of code to perform a query using the Django ORM that contains a very long 'lookup name ' : I 'd like to break the line to follow pep8 , specially 79 characters limitI know we can do something like : But I 'm wondering if there is another , maybe more pythonic/accepted , way ?"
"I 'm learning about comparison operators , and I was playing around with True and False statements . I ran the following code in the Python shell : As expected , this returned True . However , I then ran the following code : and there was a syntax error . Why was this ? If the first line of code is valid syntax , then surely the second line of code should also be valid . Where have I gone wrong ? ( To give a bit of background , my understanding is that = in Python is only used for variable assignment , while == is closely related to the mathematical symbol '= ' . )"
"A little background , I essentially need to define an int wrapper type , say MyInt ( among some other classes ) , and another generic Interval type which can accept MyInt objects as well as other types of objects . Since the types acceptable by the Interval do not fall into a neat hierarchy , I thought this would be a perfect use-case for the experimental Protocol , which in my case would require a couple of methods and a couple of @ classmethods . All the methods return a `` self-type '' , i.e. , MyInt.my_method returns a MyInt . Here is a MCVE : However , mypy complains : Which to me is hard to understand . Why is return-type expecting < nothing > ? I tried simply not annotating cls in the protocol : However , mypy complains with a similar error message : Which to me , makes even less sense . Note , if I make these instance methods : Then , mypy does n't complain at all : I 've read about self-types in protocols in PEP 544 , where it gives the following example : Furthermore , in PEP484 , regarding typing classmethods , we see this example : What is wrong with my Protocol / class definition ? Am I missing something obvious ? I would appreciate any specific answers about why this is failing , or any work-around . But note , I need these attributes to be accessible on the class . Note , I 've tried using a ClassVar , but that introduced other issues ... namely , ClassVar does not accept type-variables as far as I can tell ClassVar 's can not be generic . And ideally , it would be a @ classmethod since I might have to rely on other meta-data I would want to shove in the class ."
I want to create a list of dictionaries with the same index element from each list . I have this dictionary : The desired output is : I have tried something like this : But my output is : I think this is happening because the lists have different lengths .
"I created a file `` authentication.py '' at the same level as the `` settings.py '' file in my django project . The content of this file is : Then I added the following line in the `` settings.py '' file : However , the login does n't work . It was working before these 2 modification when the user 's credentials where stored in the database . I do n't know how I can debug this . Any idea ? Here are some pieces of my settings file : EDIT : I deleted the `` db.sqlite3 '' file at the root of my folder , then ran the django shell and did : Then I get this :"
"I have a C++ function that accepts a callback , like this : I want to call this function from Cython by giving it a closure , i.e . something I would have done with a lambda if I was calling it from C++ . If this was a C function , it would have some extra void* arguments : and then I would just pass PyObject* as user_data ( there is a more detailed example here ) .Is there way to do this more in C++ way , without having to resort to explicit user_data ?"
"I 'm using Python in order to save the data row by row ... but this is extremely slow ! The CSV contains 70million lines , and with my script I can just store 1thousand a second.This is what my script looks likeI reckon that for testing I might have to consider MySQL or PostgreSQL . Any idea or tips ? This is the first time I deal with such massive volumes of data . : )"
"I have two sorted lists of numbers A and B with B being at least as long as A . Say : I want to associate each number in A with a different number in B but preserving order . For any such mapping we define the total distance to be the sum of the squared distances between mapped numbers . For example : If we map 1.1 to 0 0 then 2.3 can be mapped to any number from 1.9 onwards . But if we had mapped 1.1 to 2.7 , then 2.3 could only be mapped to a number in B from 8.4 onwards.Say we map 1.1- > 0 , 2.3- > 1.9 , 5.6- > 8.4 , 5.7- > 9.1 , 10.1- > 10.7 . This is a valid mapping and has distance ( 1.1^2+0.4^2+2.8^2+3.4^2+0.6^2 ) .Another example to show a greedy approach will not work : If we map 1- > 1 then we have to map 2- > 10000 which is bad.The task is to find the valid mapping with minimal total distance.Is hard to do ? I am interested in a method that is fast when the lists are of length a few thousand ."
"Pyparsing worked fine for a very small grammar , but as the grammar has grown , the performance went down and the memory usage through the roof.My current gramar is : When I parse the followingIt takes quite long : And the memory usage goes up to 1.7 GiB ( sic ! ) .Have I made some serious mistake implementing this grammar or how else can I keep memory usage in bearable margins ?"
"I 'm playing with graphs and coded a mixin module for creating graphs . I want to have in it some alternative constructors.This is what I have : I want that to return a graph where nodes and edges are generated from the file . The method I wrote does n't work , I do n't know if it even makes sense . What I want to do is inside that method to use the __init__ method of Graph , but then add edges and nodes from the file . I could just write an instance level method to do this , but I have other altertive initializers in mind.Thanks !"
Hi I 'm trying to find the unique Player which show up in every Team.df = and the result should be : result = since Joe and Steve are the only Player in each Team
"Sanity check please ! I 'm trying to understand an unexpected test failure when including the exact message returned from an incorrect function call to the match parameter of pytest.raises ( ) .The docs state : match – if specified , asserts that the exception matches a text or regexThe sequence of instructions in the repl below pretty much says it all , but for some reason the last test fails.I thought that perhaps the ' ( ) ' might mean something in regex that would cause the strings not to match but : ... passes ."
"I have a list of indicesWhat 's the fastest way to convert this to a numpy array of ones , where each index shows the position where 1 would occur ? I.e . what I want is : I know the max size of the array beforehand . I know I could loop through each list and insert a 1 into at each index position , but is there a faster/vectorized way to do this ? My use case could have thousands of rows/cols and I need to do this thousands of times , so the faster the better ."
So I got these examples from the official documentation . https : //docs.python.org/2/library/timeit.htmlWhat exactly makes the first example ( generator expression ) slower than the second ( list comprehension ) ?
"Im trying to declare some global variables in some functions and importing the file with those functions into another . However , I am finding that running the function in the second file will not create the global variable . I tried creating another variable with the same name , but when i print out the variable , it prints out the value of the second file , not the global valueglobals.pymain.py this will print 0. if i dont have value = 0 in main , the program will error ( value not defined ) . If i declare value in globals.py outside of the function , main.py will take on the value of the global value , rather than the value set in default ( ) What is the proper way to get value to be a global variable in python ?"
"I am experimenting with 2 functions that emulate the zip built-in in Python 2.x and 3.x . The first one returns a list ( as in Python 2.x ) and the second one is a generator function which returns one piece of its result set at a time ( as in Python 3.x ) : This works well and gives the expected output of the zip built-in : Then I thought about replacing the list comprehension within the tuple ( ) calls with its ( almost ) equivalent generator expression , by deleting the square brackets [ ] ( why create a temporary list using the comprehension when the generator should be fine for the iterable expected by tuple ( ) , right ? ) However , this causes Python to hang . If the execution is not terminated using Ctrl C ( in IDLE on Windows ) , it will eventually stop after several minutes with an ( expected ) MemoryError exception.Debugging the code ( using PyScripter for example ) revealed that the StopIteration exception is never raised when the generator expression is used . The first example call above to myzip_2x ( ) keeps on adding empty tuples to res , while the second example call to myzip_3x ( ) yields the tuples ( 1 , 7 ) , ( 2 , 8 ) , ( 3 , 9 ) , ( 4 , ) , ( 5 , ) , ( ) , ( ) , ( ) , ... .Am I missing something ? And a final note : the same hanging behaviour appears if its becomes a generator ( using its = ( iter ( seq ) for seq in seqs ) ) in the first line of each function ( when list comprehensions are used in the tuple ( ) call ) .Edit : Thanks @ Blckknght for the explanation , you were right . This message gives more details on what is happening using a similar example to the generator function above . In conclusion , using generator expressions like so only works in Python 3.5+ and it requires the from __future__ import generator_stop statement at the top of the file and changing StopIteration with RuntimeError above ( again , when using generator expressions instead of list comprehensions ) .Edit 2 : As for the final note above : if its becomes a generator ( using its = ( iter ( seq ) for seq in seqs ) ) it will support just one iteration - because generators are one-shot iterators . Therefore it is exhausted the first time the while loop is run and on subsequent loops only empty tuples are obtained ."
"The following is a snippet of code . The script takes the input files from a `` Test '' folder , runs a function and then outputs the files with the same name in the `` Results '' folder ( i.e . `` Example_Layer.shp '' ) . How could I set it so that the output file would instead read `` Example_Layer ( A ) .shp '' ?"
"scrapy is not downloading files properly . I have URLs of my items , so I figured I can use wget to download the files.How can i use wget inside the scrapy process_item function ? Alternatively , is there another way of download files ?"
"I am running the command : as part of my build process in a Kivy iOS project . When I run the command myself in the terminal , pygments is installed correctly . When XCode runs it , the pygments package does not install any files in the pygments/formatters directory.I have verified that the same pip is used for my user and when XCode runs pip . Output when I install pygments : Output when Xcode runs the exact same command : I have downloaded the file that pip downloads and verified it contains files in the formatters directory . I have verified that the same pip program is being used for both myself and XCode : How or why would pip incorrectly unzip or install pygments when run in an Xcode build phase vs directly ?"
"I 'm trying to rewrite a function using numpy which is originally in MATLAB . There 's a logical indexing part which is as follows in MATLAB : When I try to make it in numpy , I ca n't get the correct indexing : What 's the proper way of getting a grid from the matrix via logical indexing ?"
"I 'm reading http : //eigenhombre.com/2013/04/20/introduction-to-context-managers/ . In it : Context managers are a way of allocating and releasing some sort of resource exactly where you need it . The simplest example is file access : This is essentially equivalent to : The article goes on to more explanation , but I 'm still not sure I understand in simple terms their purpose . Can someone clarify . Also what is the `` context '' ? I have looked at Trying to understand python with statement and context managers , but again I 'm not sure what the purpose of a context manager is ? Is it just an alternative syntax for `` try.. finally.. '' or is their some other purpose"
"I have a product model with a foreign key to some prices , and I really want to list the products with the `` best '' offer ... How to do that ? First I want all products with more than one price , that I got : Now i want the best 6 products with the largest diff between the two latest prices.Can anyone help with that ? I hope it makes sense ; )"
"I am saving a user 's database connection . On the first time they enter in their credentials , I do something like the following : I then have the conn ready to go for all the user 's queries . However , I do n't want to re-connect every time the view is loaded . How would I store this `` open connection '' so I can just do something like the following in the view : Update : it seems like the above is not possible and not good practice , so let me re-phrase what I 'm trying to do : I have a sql editor that a user can use after they enter in their credentials ( think of something like Navicat or SequelPro ) . Note this is NOT the default django db connection -- I do not know the credentials beforehand . Now , once the user has 'connected ' , I would like them to be able to do as many queries as they like without me having to reconnect every time they do this . For example -- to re-iterate again -- something like Navicat or SequelPro . How would this be done using python , django , or mysql ? Perhaps I do n't really understand what is necessary here ( caching the connection ? connection pooling ? etc . ) , so any suggestions or help would be greatly appreciated ."
"As a very noob with in Python I 'm printing all elements of a list in version 3 , and after a comprehensive research I could n't find an explanation for this kind of behavior . However , I know every function must return some value and when it 's not defined the function returns `` Null '' ( or `` None '' in Python ) . But why in this case , after printing all elements correctly it prints `` None '' for each element in another list ?"
"I found dozens of examples how to vectorize for loops in Python/NumPy . Unfortunately , I do n't get how I can reduce the computation time of my simple for loop using a vectorized form . Is it even possible in this case ?"
"I need to make a variable with similar behaviour like in C lanquage.I need byte or unsigned char with range 0-255.This variable should overflow , that means ..."
"According to the documentation ( https : //docs.scipy.org/doc/numpy/reference/ufuncs.html ) , two arrays are broadcastable if one of the following conditions are true : I tried to implement this in python , but having a trouble in understanding the 2nd and 3rd rule . Not getting the answer that I am expecting , but I want to know what error that I am making in my code and a possible solution to this error ."
"I 'm trying to generate platform specific binary distribution packages for windows , but python setup.py bdist_wheel generates universal file names . What do I need to change in my setup.py or otherwise to get wheels with platform specific names ? From what I gather setuptools is supposed to recognize when a package will not be universal , but this is not working for me.The project I 'm working on compiles several variants of libraries using cmake and mingw w64 . These compiled libraries are of course 32/64 bit specific . To create the individual distributions I create a virtualenv for the target platform from which I compile the libraries . The compiled libraries are then copied into the python package to be wheel 'd . Here is my setup.py . pyfvs_files is a list of compiled , '*.pyd ' files to include . This produces wheels named 'pyfvs-0.0.1a0-py27-none-any.whl ' where I would expect it to be 'pyfvs-0.0.1a0-py27-none-win32.whl ' and 'pyfvs-0.0.1a0-py27-none-win_amd64.whl ' . When installed via pip the packages function as expected for the target platform , but of course fail for the alternate platform . Ultimately I 'd like to support Python 2.7 , 3.4 , 3.5 on both windows 32 & 64 bit and linux 64 bit ."
"I want to find eigenvectors of a matrix in Sympy and wrote the following program but it does n't work . On the other hand , the A.eigenvects ( ) function in Sympy calculates the eigenvalues and eigenvects of the matrix A and I used a similar thing here but when I want to print the results , an empty list is shown . Would you please guide me ?"
"I have the following DF in pandas.Col_B should either be filled with True or False values . By default , it is False , but when the first 0 has been `` seen '' , the rest of DF should be True . The DF has over 100 000 rows . What will be the fastest way to set values in col_B equal to `` True '' since the first `` 0 '' value in Col_A appears ?"
I 've a list of dictionaries like this : I want to move every dictionary that has language ! = 'en ' to the end of the list while keeping the order of other of results . So the list should look like :
"I have several test methods in a class that use one type of patching for a object , so I have patched with class decorator . For a single another method i want to patch the same object differently . I tried the following approach , but the patch made as class decorator is in effect despite the method itself being decorated with different patch . I expected method patch to override class patch . Why is this not the case ? In this particular case I can remove class patch and patch individual methods , but that would be repetitive . How can I implement such overriding ( method overrides class patch ) mechanism ?"
"I have about 30 SEM ( scanning-electron microscope ) images like that : What you see is photoresist pillars on a glass substrate . What I would like to do , is to get the mean diameter in x and y-direction as well as the mean period in x- and y-direction.Now , instead of doing all the measurement manually , I was wondering , if maybe there is a way to automate it using python and opencv ? EDIT : I tried the following code , it seems to be working to detect circles , but what I actually need are ellipse , since I need the diameter in x- and y-direction . ... and I do n't quite see how to get the scale yet ? Source of inspiration : https : //www.pyimagesearch.com/2014/07/21/detecting-circles-images-using-opencv-hough-circles/"
"I am trying to upload a file around ~5GB size as below but , it throws the error string longer than 2147483647 bytes . It sounds like there is a limit of 2 GB to upload . Is there a way to upload data in chunks ? Can anyone provide guidance ? ERROR : UPDATE :"
"I tried to create a new object in a process when using multiprocessing module . However , something confuses me.When I use multiprocessing module , the id of the new object is the sameBut when I use threading , they are different : I am wondering why they are different ."
"Comparing the results of a floating point computation across a couple of different machines , they are consistently producing different results . Here is a stripped down example that reproduces the behavior : Different machines produce different results like [ -2.85753540e-05 -5.94204867e-05 -2.62337649e-04 ] [ -2.85751412e-05 -5.94208468e-05 -2.62336689e-04 ] [ -2.85754559e-05 -5.94202756e-05 -2.62337562e-04 ] but I can also get identical results , e.g . by running on two MacBooks of the same vintage . This happens with machines that have the same version of Python and numpy , but not necessarily linked against the same BLAS libraries ( e.g accelerate framework on Mac , OpenBLAS on Ubuntu ) . However , should n't different numerical libraries all conform to the same IEEE floating point standard and give exactly the same results ?"
Is it just callable ? In the meantime I have been using function as my IDE regards callable as a bool .
Given a pandas df one can copy it before doing anything via : How can I do this with a dask dataframe object ?
"I have a list of objects . Each object has two fieldsI will keep adding / deleting objects in the list and also changing attributes of objects , for example I may change ob1.status to 5.Now I have two dictsHow do I design a simple solution so that whenever I modify/delete/insert elements in the list , the maps get automatically updated . I am interested in a pythonic solution that is elegant and extensible . For example in future , I should be able to easily add another attribute and dict for that as well Also for simplicity , let us assume all attributes value are different . For example no two objects will have same status"
I am trying to make some piece of code more efficient by using the vectorized form in numpy . Let me show you an example so you know what I mean.Given the following code : It outputsWhen I now try to vectorize the code like this : I just get the first iteration correct : Is it possible to write the code above efficiently in a vectorized form ( where the next iteration always accesses the previous iteration ) or do I have to keep the for loop ?
"I have a dataframe of the form , df : Suppose the possible values of cat_var_1 in the dataset have the ratios- [ 'Orange ' : 0.6 , 'Banana ' : 0.4 ] and the possible values of cat_var_2 have the ratios [ 'Monkey ' : 0.2 , 'Cat ' : 0.7 , 'Dog ' : 0.1 ] .How to I split the data into train , test and validation sets ( 60:20:20 split ) such that the ratios of the categorical variables remain preserved ? In practice , these variables can be of any number , not just two . Also , clearly , the exact ratios may never be achieved in practice , but we would like it to be as near as possible.I have looked into the StratifiedKFold method from sklearn described here : how to split a dataset into training and validation set keeping ratio between classes ? but this is restricted to evaluating on the basis of one categorical variable only.Additionally , I would be grateful if you could provide the complexity of the solution you achieve ."
"I 'm trying to copy files inside a Python script using the following code : This works perfectly unedr OSX ( and other UNIX flavors I suspect ) but fails under Windows . Basically , the read ( ) call returns far less bytes than the actual file size ( which are around 10KB in length ) hence causing the write truncate the output file.The description of the read ( ) method says that `` If the size argument is negative or omitted , read all data until EOF is reached '' so I expect the above code to work under any environment , having Python shielding my code from OSs quirks.So , what 's the point ? Now , I resorted to shutil.copyfile , which suits my need , and it works . I 'm using Python 2.6.5Thank you all ."
I would like to store all my string using Utf8 String Codec with GZIP in python.I tried below code but compression not happening properly . I do n't know what 's missing here . How to insert data into redis using gzip compression technique.After insertion into redis it just printing some number like d49Appreciated your help in advance ! Thanks
"I have a pandas dataframe structured like this : This is just an example , the actual dataframe is bigger , but follows the same structure.The sample dataframe has been created with this two lines : I would like to aggregate the rows whose value is smaller that a given threshold : all these rows should be substituted by a single row whose value is the sum of the substituted rows.For example , if I choose a threshold = 6 , the expected result should be the following : How can I do this ? I thought to use groupby ( ) , but all the examples I 've seen involved the use of a separate column for grouping , so I do not know how to use it in this case.I can select the rows smaller than my threshold with loc , by doing df.loc [ df [ 'value ' ] < threshold ] but I do not know how to sum only these rows and leave the rest of the dataframe unaltered ."
"While installing packages with pip in python , it downloads and installs a whl file , e.g . following file for pyqt5 : After installation , does this file persist on disc and take space ? If so , can it be removed to free space on disk ? Also , is there a link regarding this anywhere in documentations ?"
"I have an expression/formula like thisI have done some research and it seems that if A , B , C values are known , by embedding Lua or Python in a C++ program , there are eval functions that can substitute A , B and C and return true or false.But what happens when I do n't know all the values ? Let 's say that A is known and it is -1 . If A is -1 then the formula will evaluate to `` false '' irrespective of the values of B or C. Can I evaluate a formula without knowing all the variables in advance ? For example if A is 10 , it makes sense to lookup for the value of B and re-evaluate again . How can we solve these problems ? Ideas ?"
"Ok , after some searching I ca n't seem to find a SO question that directly tackles this . I 've looked into masked arrays and although they seem cool , I 'm not sure if they are what I need.consider 2 numpy arrays : zone_data is a 2-d numpy array with clumps of elements with the same value . This is my 'zones'.value_data is a 2-d numpy array ( exact shape of zone_data ) with arbitrary values.I seek a numpy array of same shape as zone_data/value_data that has the average values of each zone in place of the zone numbers.example ... in ascii art form.zone_data ( 4 distinct zones ) : value_data : my result , call it result_data : here 's the code I have . It works fine as far as giving me a perfect result.My arrays are big and my code snippet takes several seconds . I think I have a knowledge gap and have n't hit on anything helpful . The loop aspect needs to be delegated to a library or something ... aarg ! I seek help to make this FASTER ! Python gods , I seek your wisdom ! EDIT -- adding benchmark scriptmy output :"
"The following minimal example of calling a python function from C++ has a memory leak on my system : script.py : main.cpp : compiled withand run with valgrindproduces the following summaryI 'm using Linux Mint 18.2 Sonya , g++ 5.4.0 , Python 3.5.2 and TensorFlow 1.4.1.Removing import tensorflow makes the leak disappear . Is this a bug in TensorFlow or did I do something wrong ? ( I expect the latter to be true . ) Additionally when I create a Keras layer in Pythonand run the call to Python from C++ repeatedlythe memory consumption of the application continuously grows ad infinitum during runtime.So I guess there is something fundamentally wrong with the way I call the python function from C++ , but what is it ?"
"( This is probably a dumb question , so please wear your stupidity shields ! ) I 've been a PHP programmer and am now learning Python + Flask . I recently had to struggle a lot with posting data through AJAX and returning a response . Finally , the code that worked was : If I change return json.dumps ( { 'status ' : 'success ' } ) to return 1 I get an Exception that int is not callable . First of all , I do n't understand who is trying to call that int and why ? Secondly , in PHP , it was frequently possible to just echo 1 ; and this would become the AJAX response . Why does n't return 1 work in Flask , then ?"
"One of the biggest annoyances I find in Python is the inability of the re module to save its state without explicitly doing it in a match object . Often , one needs to parse lines and if they comply a certain regex take out values from them by the same regex . I would like to write code like this : But unfortunately it 's impossible to get to the matched object of the previous call to re.match , so this is written like this : Which is rather less convenient and gets really unwieldy as the list of elifs grows longer.A hackish solution would be to wrap the re.match and re.search in my own objects that keep state somewhere . Has anyone used this ? Are you aware of semi-standard implementations ( in large frameworks or something ) ? What other workarounds can you recommend ? Or perhaps , am I just misusing the module and could achieve my needs in a cleaner way ? Thanks in advance"
"Executed Python code : gives : Executed analogical Java code : gives : Apparently , Java method inherited from base class ( Test ) uses also base class ' member variables . Python method uses the member variable of derived class ( Subclass ) .The question : Is there any way to achieve the same or at least similar behaviour in Java like in Python ?"
"I 've made a little project that includes importing various python files into another python files . Here 's my directory structure.This is basically my directory structure . This some_other.py is imported inside my my_main_file.py by the following command : from Sites import *I 'm importing everything from that directory . So , what I wanted to do was make this whole project into a standalone binary . I use pyintaller to convert my .py files into .exe . But , I 've only written scripts that have everything in 1 file , which makes the task easy . But , this time , I 'm trying to do something new.My python script takes command line arguments and it is working . The python script will not work without command line arguments . I can convert to exe , but that exe is doing nothing even when I give arguments.So , I got a .spec file from pyinstaller and modified it to get my some_other.py file . SPEC file looks like this : This makes an .exe , but that exe wo n't work.The exe wo n't show anything . But , it is like 11 MB in size . Got anything for this ? I tried nuitka , but that one gives out an error about not being able to find gendef.exe and I 'm not really interested in installing minGW.Btw , I 'm on a 64 bit machine and py2exe 's bundle_file:1 wo n't work for me ."
"In general , is it reasonable to return None from a __new__ method if the user of the class knows that sometimes the constructor will evaluate to None ? The documentation does n't imply it 's illegal , and I do n't see any immediate problems ( since __init__ is not going to be called , None not being an instance of the custom class in question ! ) . But I 'm worried aboutwhether it might have other unforeseen issueswhether it 's a good programming practice to have constructors return NoneSpecific example :"
"Can anyone confirm if it is possible to programmatically submit an assignment to Blackboard using Blackboard WebServices ? ( As documented here ) . Specifically I would like to know the correct way of using the Gradebook.WS , AttemptVO and the studentSubmission method to submit an assignment . Here is what I have tried so far which mostly works in that the attempt can be seen in Bb Gradebook except the attemptVO.studentSubmission is not visible in Blackboard Gradebook : Result : Many thanks ."
"I have some python code that 's throwing a KeyError exception . So far I have n't been able to reproduce outside of the operating environment , so I ca n't post a reduced test case here.The code that 's raising the exception is iterating through a loop like this : The del [ k ] line throws the exception . I 've added a try/except clause around it and have been able to determine that k in d is False , but k in d.keys ( ) is True.The keys of d are bound methods of old-style class instances.The class implements __cmp__ and __hash__ , so that 's where I 've been focusing my attention ."
"I have problem organizing my unittest based class test for family of tests . For example assume I implement a `` dictionary '' interface , and have 5 different implementations want to testing.I do write one test class that tests a dictionary interface . But how can I nicely reuse it to test my all classes ? So far I do ugly : In top of file and then use DictType in test class . To test another class I manually change the DictType to something else . How can do this otherwise ? Ca n't pass arguments to unittest classes so is there a nicer way ?"
"I have a look-up table ( LUT ) that stores 65536 uint8 values : I want to use this LUT to convert the values in an array of uint16s : and I want to do the conversion in place , because this last array can get pretty big . When I try it , the following happens : And I do n't understand what is going on . I know that , without an out argument , the return is of the same dtype as lut , so uint8 . But why ca n't a uint8 be cast to a uint16 ? If you ask numpy : Obviously the following works : But this also works : This really makes no sense , since now int32s are being cast to uint16s , which is definitely not a safe thing to do : My code works if I set the lut 's dtype to anything in uint16 , uint32 , uint64 , int32 or int64 , but fails for uint8 , int8 and int16.Am I missing something , or is this simply broken in numpy ? Workarounds are also welcome ... Since the LUT is not that big , I guess it is not that bad to have its type match the array 's , even if that takes twice the space , but it just does n't feel right to do that ... Is there a way to tell numpy to not worry about casting safety ?"
"With an objective of learning Keras LSTM and RNNs , I thought to create a simple problem to work on : given a sine wave , can we predict its frequency ? I would n't expect a simple neural network to be able to predict the frequency , given that the notion of time is important here . However , even with LSTMs , I am unable to learn the frequency ; I 'm able to learn a trivial zero as the estimated frequency ( even for train samples ) .Here 's the code to create the train set.Now , here 's a simple neural network for this example.As expected , this NN does n't learn anything useful . Next , I tried a simple LSTM example.However , this LSTM based model also does n't learn anything useful ."
"I have an array of x , y , z coordinates of several ( ~10^10 ) points ( only 5 shown here ) I want to make a new array with only those points which are at least some distance d away from all other points in the list . I wrote a code using while loop , This works , but it is taking really long to perform this calculation . I read somewhere that while loops are very slow . I was wondering if anyone has any suggestions on how to speed up this calculation.EDIT : While my objective of finding the particles which are at least some distance away from all the others stays the same , I just realized that there is a serious flaw in my code , let 's say I have 3 particles , my code does the following , for the first iteration of i , it calculates the distances 1- > 2 , 1- > 3 , let 's say 1- > 2 is less than the threshold distance d , so the code throws away particle 1 . For the next iteration of i , it only does 2- > 3 , and let 's say it finds that it is greater than d , so it keeps particle 2 , but this is wrong ! since 2 should also be discarded with particle 1 . The solution by @ svohara is the correct one !"
"I need to use one model in context of 2 admin classes . So , this is my model : And I want to use it twice . First , I want to show all the models , like : And also , I want a new page , where will be shown only models with status = 'pending ' , like that : But of course I get an error : AlreadyRegistered : The model Item is already registeredAny suggestions ? Hope to get help ."
"I was searching for ways to highlight SQL codes in jupyter notebook.I was able to highlight SQL cell magic only , but not line magic and custom settings.Case 1 ( works ) Highlight cell magic ( cell startswith % % sql ) Ref : adding syntax highlighting to Jupyter notebook cell magicCase 2 ( does not work ) Line Magic : line starts with % sqlMy attempt : Change the regex to ^ % sql but it did not work.Case 3 ( does not work ) How to syntax highlight custom cells ( cell startswith # # % % ) My attempt : Tried to changing regex to ^ # # % % sqlExample imageIn the image we can see that cell magic % sql commands are not highlighted.I want them to be highlighted.Related linksadding syntax highlighting to Jupyter notebook cell magicIPython change input cell syntax highlighting logic for entire session"
"I have a django app ( specifically django-rest ) . When I run the local copy of this site , my requests can be processed in 50-400ms.Next , I managed to deploy to Microsoft Azure App Service . Now , under the most expensive tier I can buy , responses are coming back in the 800-2000ms range.The app does simple queries on sqlite database . This database file is approximately 30 megabytes with the largest table is 12000 rows.I should point out all access to the database is read only , so no contention issues.Configuration : web.config : Python version 2.7.I have narrowed it down to SQLite performance . Static files and API index pages come back in ~60ms while the heaviest query comes back in ~2000ms . This is Time Till First Byte , and not overall response times , and I have ruled out network latency ( which is very low due to the geographic proximity ) . This is on P3 ( Premium Tier ) 4 core , 7GB ram ( as Azure calls it ) .Running on localhost , the response times are ~15ms for index pages , and ~380ms for the same requests on my Macbook 2.2 GHz Intel Core i7 16 GB 1600 MHz DDR3.App `` warm-up '' is not the issue as this is after its already `` hot '' ( timing is based on a few refreshes ) Update : I installed Django Rest Toolbar for more information.On macbook django DEV server ( pure python ? ) : On azure app service IIS and FastCGI ( see config above ) : Appreciate any insight !"
"I 've got a Perl function which takes a timestamp and returns either the unchanged timestamp ( if it 's never seen it before ) or otherwise , it appends some letters to make it unique : So if you call it four times , with the values 1 , 1 , 1 , and 2 , it will return 1 , then 1A , then 1B , then 2.Note : It only ever gets called with ever-increasing timestamps , so it does n't need to recall every one it 's ever seen , just the last one.Now I need to translate this function to Python . I 've learned that I can replace the `` state '' variables with globals ( yuck ! ) or perhaps attach them to the function as attributes , but neither of those is particularly elegant.Also , Python does n't have something like Perl 's magic autoincrement , where if you `` ++ '' a variable whose value is `` A '' , it becomes `` B '' -- or if it 's `` Z '' , it becomes `` AA '' . So that 's a curveball too.I 'm hacking together a solution , but it 's really ugly and hard to read . Translating from Perl to Python is supposed to have the opposite effect , right ? : ) So I 'm offering this as a challenge to SO users . Can you make it an elegant Python function ?"
"I have a docker image containing various bits , including Spark . Here is my Dockerfile : I can build then run that docker image , connect to it , and successfully import the pyspark libraries : Note the value of PYTHONPATH ! Problem is that the behaviour in PyCharm is different if I use this same docker image as the interpreter . Here 's how I have set up the interpreter : If I then run Python console in PyCharm this happens : As you can see PyCharm has changed PYTHONPATH meaning that I can no longer use the pyspark libraries that I want to use : OK , I could change PATH from the console to make it work : but its tedious to have to do that every time I open a console . I ca n't believe there is n't a way of telling PyCharm to append to PYTHONPATH rather than overwriting it but if there is I ca n't find it . Can anyone offer any advice ? How can I use a docker image as PyCharm 's remote interpreter and keep the value of PYTHONPATH ?"
"What is the best way to take the cross product of each corresponding row between two arrays ? For example : I know this can be done with a simple python loop or using numpy 's apply_along_axis , but I 'm wondering if there is any good way to do this entirely within the underlying C code of numpy . I currently use a simple loop , but this is by far the slowest part of my code ( my actual arrays are tens of thousands of rows long ) ."
"So a lot of times I use dictionary for a key/value lookup.But if I need to lookup multiple things , I usually have a for loop for a same.For example : Is there a better way to search for all vals in one shot rather than this for loop ? Like some_task = fetch_all_conds ( vals ) Not sure , if my question makes sense or not ?"
"Does anyone know how to do this in R ? That is , represent this cyclical data from the left plot to the right plot ? http : //cs.lnu.se/isovis/courses/spring07/dac751/papers/TimeSpiralsInfoVis2001.pdfHere is some example data ."
"I am trying to learn LSTM model for sentiment analysis using Tensorflow , I have gone through the LSTM model.Following code ( create_sentiment_featuresets.py ) generates the lexicon from 5000 positive sentences and 5000 negative sentences.The following code ( sentiment_analysis.py ) is for sentiment analysis using simple neural network model and is working fineI am trying to modify the above ( sentiment_analysis.py ) for LSTM modelafter reading the RNN w/ LSTM cell example in TensorFlow and Python which is for LSTM on mnist image dataset : Some how through many hit and run trails , I was able to get the below running code ( sentiment_demo_lstm.py ) : Output of len ( train_x ) = 9596 , len ( train_x [ 0 ] ) = 423 meaning train_x is a list of 9596x423 ? Tough I have a running code now , I still have lots of doubts.In sentiment_demo_lstm , I am not able to understand the following partI have print the following shapes : Here I took the number of hidden layers as 128 , does it need to be same as the number of inputs i.e . len ( train_x ) = 9596The value 1 inandis because train_x [ 0 ] is 428x1 ? The following is in order to match the placeholderx = tf.placeholder ( 'float ' , [ None , input_vec_size , 1 ] ) dimensions , right ? If I modified the code : asI get the following error : = > I ca n't include the last 124 records/feature-sets while training ?"
I want to write a program which will clone remote git repository and then do bunch of other stuff . The problem is that 'git clone ' asks for password . It does not work when I open pipes to stdin/out/err to 'git clone ' because it runs git-remote-http underneath which prompts for password on TTY.I would like to pass the password from my program . I am using Python and Popen from subprocess . Code below does not wotk.How can I achieve this ?
"How do I make sure packages installed using pip do n't install dependancies already installed using apt-get ? For example , on Ubuntu you can install the package Numpy using apt-get install python-numpy . Which gets installed to : I 've noticed that when I install a package that requires numpy using pip for example , pip install scipy instead of skipping the numpy dependancy it installs again to a different location . What pip should do is skip any python packages installed globally , right ?"
"In searching for a way of working with nested dictionaries , I found the following code posted by nosklo , which I would like to have explained , please.Testing : Output : I 'm a pretty newbie programmer . I have learned most of what I know on my own time on the side , with my only formal training being on Turbo Pascal back in high school . I understand and am able to use classes in simple ways , such as using __init__ , class methods , and storing data within instances of the class with foo.man = 'choo ' . I have no idea how the series of square brackets get directed , correctly , through the class ( I presume they are calling __getitem__ somehow ) and do n't understand how they each get handled so concisely without having to call the method three times individually.I was under the impression that the ( dict ) in the class declaration would be handled by an __init__ . I 've used try : except : before , though again , in quite simple ways . It looks to me like the try , when it runs , is calling a series of function __getitem__ . I gather that if the current level 's dictionary exists , the try will pass and go to the next dictionary . The except , I gather , runs when there 's a KeyError but I have n't seen self used like that before . Self 's being treated like a dictionary while I thought self was an instance of class AutoVivification ... is it both ? I have never assigned twice in a row like this foo = man = choo but suspect that value is pointing to self [ item ] while self [ item ] points to the result of type ( self ) . But type ( self ) would return something like this : < class '__main__.AutoVivification ' > would n't it ? I have no idea what the extra round brackets at the end there are for . Because I do n't know how the function is being called , I do n't understand where value is being returned.Sorry for all the questions ! There is so much in this that I do n't understand and I do n't know where to look it up short of reading through the documentation for hours in which I 'd retain very little . This code looks like it 'll serve my purposes but I want to understand it before using it.In case you want to know what I 'm trying to do in my program with nested dictionaries : I 'm trying to hold map data on an astronomical scale . While I ca n't create dictionaries/lists of 10^6 items nested 4 times ( that would be 10^24 items ! ) , the space is mostly empty so I can leave the empty values out completely and only assign when there 's something there . What was stumping me was an efficient way of handling the dictionaries ."
"Is there an easy way to do something at the beginning and end of each function in a class ? I 've looked into __getattribute__ , but I do n't think that I can use it in this situation ? Here 's a simplified version of what I 'm trying to do :"
"I am working on a Django project that is separated into 5 apps ( each have 3-6 models ) . As a whole , this project is definitely something I would reuse in other projects , but it would only be useful if it included all the apps because they are intrinsically related.Therefore , I want to package this project to make it reusable following the Django docs and Django cookiecutter django package . However , these examples show only how to package with a single app.From what I have read there are a few options : Make 5 packagesMake one app with 30 models and convert models.py into a moduleMake a package with 5 apps ( Similar to how django-allauth is setup ) I am planning on using option 3 because I think option 1 is overkill and option 2 is considered bad practice ( See this post and this post ) . The issue I see with option 3 is that then a project that uses this package would have to install all 5 apps in INSTALLED_APPS like so : Is there a way to avoid having to install all the apps in INSTALLED_APPS if I package the project with 5 apps ? Or is option 1 or 2 better for this use case ?"
"EDIT 1Hmm , I accept the answers that tar respects an empty file ... but on my system : Maybe I have a non-canonical version ? Hello all , I am testing some logic to handle a user uploading a TAR file . When I feed a blank file to tarfile.is_tarfile ( ) it returns True , which is not what I am expecting : If I add some text to the file , it returns False , which I am expecting : I could add a check at the beginning to check for a zero-length file , but based on the documentation for tarfile.is_tarfile ( name ) I think this is unecessary : Return True if name is a tar archive file , that the tarfile module can read.I went so far as to check the source , tarfile.py , and I can see that it is checking header blocks but I do not fully understand how it is evaluating those blocks.Am I misreading the documentation and therefore setting unfair expectations ? Thank you , Zachary"
"In order to simplify my code , I would like to implement a set that contains everything , i.e . a UniversalSet . I figure that the simplest of way of getting around this is to have a custom set that returns True for any query . In my particular case I am mostly interested in the __intersect__ of sets such that the following criteria are true : I have subclassed set in the following manner : This works for ( 1 ) , but ( 2 ) still fails . Is there a similarly easy way to make ( 2 ) to work ?"
To start multiprocess with for loop.I got the result.Why the subprocess created early execute later ? Why ca n't get the following outcome ? What Jean-François Fabre says is about how to create the following result : I can get it just to change p.join in for loop such as below : What I want to know is why my code result in the following output instead of : It is a different issue .
"I am trying to write a __reduce__ ( ) method for a cython class that contains C pointers but have so far found very little information on the best way to go about doing this . There are tons of examples around for how to properly write a __reduce__ ( ) method when using numpy arrays as member data . I 'd like to stay away from Numpy arrays as they seem to always be stored as python objects and require calls to and from the python API . I come from a C background so I am very comfortable working with memory manually using calls to malloc ( ) and free ( ) and am trying to keep python interaction to an absolute minimum.However I have run into a problem . I have a need to use something equivalent to copy.deepcopy ( ) on the class I am creating , from the Python script where it will ultimately be used . I have found that the only good way to do this is to implement the pickle protocol for the class by implementing a __reduce__ ( ) method . This is trivial with most primitives or python objects . However I am at an absolute loss for how to go about doing this for dynamically allocated C arrays . Obviously I ca n't return the pointer itself as the underlying memory will have disappeared by the time the object is reconstructed , so what 's the best way to do this ? I 'm sure this will require modification of both the __reduce__ ( ) method as well as one or both of the __init__ ( ) methods . I have read the python documentation on pickling extension types found here as well as just about every other question of stack overflow about picking cython classes such as this question.A condensed version of my class looks something like this :"
"After I make a request with the Flask test client , I want to access the cookies that the server set . If I iterate over response.headers , I see multiple Set-Cookie headers , but if I do response.headers [ `` Set-Cookie '' ] , I only get one value . Additionally , the headers are unparsed strings that are hard to test . Why does accessing the Set-Cookie header only give me one header ? How can I access the cookies and their properties for testing ?"
I found that : Does an iterator always have the __iter__ method ? According to https : //stackoverflow.com/a/9884259 an iterator also an iterable . If it is true that an iterator always has __iter__ method ?
I am using sklearn 's Pipeline and FunctionTransformer with a custom functionThis is my code : And I get this error : AttributeError : module '__ main__ ' has no attribute ' f'How can this be resolved ? Note that this issue occurs also in pickle
I 'm a beginner python user and I 've ran the following code on both python2.7 and python3.4.3On python2 i get a nice array like this : On python3 i get this : If I change this : to it works fine on both but I would want to use an array instead.Is there a way to get this to work with np.array ?
I downloaded and installed the python-markdown extension that lives inside the nbextensions repo here : jupyter_contrib_nbextensions/src/jupyter_contrib_nbextensions/nbextensions/python-markdown/python-markdown.yamlAt the end of the installation the following is posted : So I tried : Which gave : I am uncertain how to proceed here .
"Consider the following : I understand that tuples are immutable , but the item in the LHS is not a tuple ! ( The fact that the intended assignment in fact succeeded , the error message notwithstanding , makes the whole scenario only more bizarre . ) Why is this behavior not considered a bug ?"
"I have had problems with using matplotlib after a Windows update . I 'm running Windows 7 Service Pack 1 32 bit and I installed Python and matplotlib as part of Python ( x , y ) -2.7.6.1 . The problem appears related to FreeType , as the import fails on ft2font as shown in the stack trace below : I have tried reinstalling Python ( x , y ) , but this did not resolve the problem.From other answers on Stackoverflow I have learned that common failures here include missing msvcr90.dll and msvcp90.dll files . I downloaded Dependency Walker and opened c : \Python27\Lib\site-packages\matplotlib\FT2FONT.PYD . This showed issues with these files and with libbz2.dll . I downloaded and copied these files to c : \windows\system32.I have also tried checking my PATH and PYTHONPATH environment variables , but they appear to reference my Python install correctly : The problem manifests even when only using the Agg backend as shown in the session above , so I do n't think it has anything to do with Qt or tk ."
"Lets say you have a set : In the book I am currently reading , Pro Python , it says that using foo.pop ( ) will pop an arbitrary number from that selection . BUT ... When I try it out , it pops 1 , then 2 , then 3 ... Does it do it arbitrarily , or is this just a coincidence ?"
"I 've got a simple form in Django , that looks something like this : Which fails with a SyntaxError , because from is a Python keyword.I rather not change the name of the field ; It fits better than any of the alternatives , and I 'm fussy about how it appears to the end user . ( The form is using 'GET ' , so the field name is visible in the URL . ) I realize I could just use something , like from_ instead , but I was initially thought there might be some way to explicitly provide the field name , for cases like this . ( eg . By supplying a name='whatever ' parameter in the Field constructor . ) It turn 's out there isn't.At the moment I 'm using dynamic form generation to get around the issue , which is n't too bad , but it 's still a bit of a hack ... Is there any more elegant way of having a form field named from , or any other Python keyword ?"
"I want to add a custom assert method to a TestCase subclass . I tried to copy my implementation from the unittest module so that it would match the behaviour of the regular TestCase as closely as possible . ( I would prefer to just delegate to self.assertEqual ( ) but this causes even more backtrace noise , see below . ) The unittest module seems to automatically hide some internal details of its implementation when reporting failed assertions.The output of this is as such : Note that the custom assert method causes a stack trace with two frames , one inside the method itself , whereas the stock unittest method only has one frame , the relevant line in the user 's code . How can I apply this frame-hiding behaviour to my own method ?"
"I have created a decorator in my Django project to inject parameter values to the decorated method 's parameters.I do this by using inspect.getargspec to check which parameters are present in the method and place them in kwargs . Otherwise I get an error due to the incorrect number of parameters in the method.While this works properly in individual view methods , it fails when it comes to Django 's class based views.I believe this might be because the decorators are applied using @ method_decorator at the class level to the dispatch method instead of the individual get and post methods.I 'm a python newbie and might be overlooking something obvious here.Is there a better way to do what I 'm doing ? Is it possible to get the method parameter names in a class based view ? I 'm using Python 2.7 and Django 1.11The DecoratorA class based viewEDIT : The obvious solution of applying the decorator at the method level , does n't work with Django 's class based views . You need apply the decorator at the url configuration or apply the decorator to the dispatch method.EDIT : I 've posted code that was related to a workaround I was exploring , passing the parameter names as an argument into the decorator ."
"For quite a while I have this weird problem on my Windows 10 machine . When trying to launch a Django server with ( and sometimes even without ) debugging on PyCharm , I get this error : That 's how it looks without debugging : The strangest thing is that if I re-run , some of the times ( 1 success in about 4-6 reruns ) it will launch successfully . I also could n't reproduce this problem without PyCharm.Specs : Windows 10 Pro , Pycharm ( this happens on versions 5.x through 2016.3 ) x86 and x64 versions , Python 3.5x , Django 1.8-1.10.x.Any ideas why this happens or how to get some more detailed info about the crash ? EDIT This is what the Event Viewer reports about the error : EDIT 2 I 've narrowed down the problem . The error appears to originate from restart_with_reloader in autoreload.py after this statement : Maybe someone from the Python community has seen something similar ?"
"I have 2 classes A and B : How can I make it so B `` inherits '' A 's class-level variables ( x in this case ) by using decorators ? Is it at all possible ? The desired behavior ( if possible ) , after decorated , B would look like this : Note : If anyone wants/needs to know why I 'm asking this , it 's simply to make SQLAlchemy 's Concrete Table Inheritance look nicer in code , although I can see many use cases for such behavior ."
In Python how do I specify the encoding to fileinput.input ? I wrotebut it does n't work reliably . I get different results for script.py text.txt and script.py < text.txt
"when is lazy evaluation ? ( generator , if , iterator ? ) , when is late binding ? ( closure , regular functions ? )"
How do I generate all possible combinations of a string with spaces between the characters ?
"I wrote a Python script to access , manage and filter my emails via IMAP ( using Python 's imaplib ) .To get the list of attachment for an email ( without first downloading the entire email ) , I fetched the bodystructure of the email using the UID of the email , i.e . : and retrieve the attachment names from there.Normally , the `` portion '' containing the attachment name would look like : But on a couple of occasions , I encountered something like : I read somewhere that sometimes , instead of representing strings wrapped in double quotes , IMAP would use curly brackets with string length followed by the actual string ( without quotes ) .e.g.But the string above does n't seem to strictly adhere to that ( there 's a single-quote , a comma , and a space after the closing curly bracket , and the string itself is wrapped in single-quotes ) .When I downloaded the entire email , the header for the message part containing that attachment seemed normal : How can I interpret ( erm ... parse ) that `` abnormal '' body structure , making sense of the extra single-quotes , comma , etc ... And is that `` standard '' ?"
"I 'm new to Python and ran into a problem where methods in the class Gui always get executed twice.Here is the gui class which inherits from Tkinter : and here is where the methods get called from ( init.py ) : When I run my program , I get the following console output : So my methods were executed twice . I do n't know where this is coming from . Does anybody know how to solve this ? Fixed : Putting the code from init.py into a new module fixed this issue !"
"I have two sets of coordinates and want to find out which coordinates of the coo set are identical to any coordinate in the targets set . I want to know the indices in the coo set which means I 'd like to get a list of indices or of bools.The desired result would be one of the following two : My problem is , that ... np.isin has no axis-attribute so that I could use axis=1.even applying logical and to each row of the output would return True for the last element , which is wrong.I am aware of loops and conditions but I am sure Python is equipped with ways for a more elegant solution ."
"recently I started a small Django project that I developed on a local machine using a SQLite3 database and the integrated development server . I now copied the whole project to a server running Debian.Everything worked well as long as I kept using the SQLite3 database . Now I wanted to switch to a local MySQL database , so I changed the settings.py file in my project 's root folder , created the database and added a user . I then ran syncdb and it created the needed tables without any problems.Now I wanted to use the app , but I keep getting errors , because Django can not find the tables - neither the 'standard tables ' like django_sessions nor my own tables - even though they are there ( I checked the database ) .The one thing that seems awkward about the DatabaseError pages I get is the following line : It seems like Django is still using the SQLite3 backend even though I set it to use the MySQL backend . The other thing that nearly makes me freak out is this : I deleted the data.sqlite file in my app 's root folder using rm . But when I use my app , the file is being recreated ! Can anyone tell me where I went wrong ?"
"I am rather new to the python language , please bear with me on this one . I want to run two state machines in two separate threads , communicating to each other via a queue . I used the transitions package to create the state machine.My understanding is that all actions/code that performs actions has to be tucked into the transactions . These lines are taken from the example of the transitions package , showing where to place the code for the transitionHowever , how would I watch my queue , once i reached a state , in order to react to messages sent there that would should cause a transition from one state to another ? From my basic knowledge , I would expect code in my state to wait for a message that could trigger the transition and call the transition to the following state.I would have to create a thread inside the state machine , passing the state machine object as a argument in order to be able to trigger transitions . Am I understanding this package correctly ? Or am I using it wrongly ?"
"I need to generate all `` ordered subsets '' ( apologies if I 'm not using correct mathematical terminology ) of a sequence in Python , with omitted elements replaced with None.Given [ 1 , 2 ] , I want [ ( 1 , 2 ) , ( 1 , None ) , ( None , 2 ) , ( None , None ) ] . Each `` ordered subset '' should have the property that at each position , it is either exactly the same element as in the seed sequence , or it is None.I can fairly easily generate subsets with omitted elements missing with the following : I ca n't figure out what the most effective way of reconstructing the missing elements , would be though . My first thought is to do something like this : Just wondering if anyone can spot any obvious deficiencies in this , or if there 's a more efficient solution I 've missed . ( Note that the items will be a fairly short list , typically under 10 elements , so I am not concerned about the O ( N ) search of `` combination '' in the inner loop ) ."
"I 'm trying to evaluate a model based on its performance on historical sports beting.I have a dataset that consists of the following columns : The model will be doing a regression where the output is the odds that playerA wins the matchIt is my understanding that I can use a custom scoring function to return the `` money '' the model would have made if it bet every time a condition is true and use that value to measure the fitness of the model . A condition something like : In the custom scoring function I need to receive the usual arguments like `` ground_truth , predictions '' ( where ground_truth is the winner [ ] and predictions is prediction_player_A_win_odds [ ] ) but also the fields `` oddsPlayerA '' and `` oddsPlayerB '' from the dataset ( and here is the problem ! ) .If the custom scoring function was called with data in the exact same order as the original dataset it would be trivial to retrieve this extra data needed from the dataset . But in reality when using cross validation methods the data it gets is all mixed up ( when compared to the original ) .I 've tried the most obvious approach which was to pass the y variable with [ oddsA , oddsB , winner ] ( dimensions [ n , 3 ] ) but scikit did n't allow it.So , how can I get data from the dataset into the custom scoring function that is neither X nor y but is still `` tied together '' in the same order ?"
"If I use the code in the answer here : Extracting text from a PDF file using PDFMiner in python ? I can get the text to extract when applying to this pdf : https : //www.tencent.com/en-us/articles/15000691526464720.pdfHowever , you see under `` CONSOLIDATED INCOME STATEMENT '' , it reads down ... ie ... Revenues VAS Online advertising then later it reads the numbers ... I want it to read across , ie : Revenues 73,528 49,552 73,528 66,392 VAS 46,877 35,108 etc ... is there a way to do this ? Looking for other possible solutions other than pdfminer . And if I try using this code for PyPDF2 not all of the text even shows up :"
"I am trying to set up an API using Django Rest , I 've attempted to use the quick start guide , but all I can get is this error : 'Module_six_moves_urllib_parse ' object has no attribute 'urlsplit ' I ca n't find any reference to this error on the internet , let alone how to solve it.Here is my urls file : Full traceback : Environment : Request Method : GET Django Version : 1.6 Python Version : 2.7.6 Installed Applications : ( 'rest_framework ' , 'south ' , 'django.contrib.admin ' , 'django.contrib.admindocs ' , 'django.contrib.auth ' , 'django.contrib.contenttypes ' , 'django.contrib.sessions ' , 'django.contrib.messages ' , 'django.contrib.staticfiles ' , 'logs ' , 'django_nose ' ) Installed Middleware : ( 'django.contrib.sessions.middleware.SessionMiddleware ' , 'django.middleware.common.CommonMiddleware ' , 'django.middleware.csrf.CsrfViewMiddleware ' , 'django.contrib.auth.middleware.AuthenticationMiddleware ' , 'django.contrib.messages.middleware.MessageMiddleware ' , 'django.middleware.clickjacking.XFrameOptionsMiddleware ' ) Template error : In template /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/rest_framework/templates/rest_framework/base.html , error at line 87 'Module_six_moves_urllib_parse ' object has no attribute 'urlsplit ' 77 : rel= '' nofollow '' title= '' Make a GET request on the { { name } } resource '' > GET 78 : 79 : 80 : title= '' Specify a format for the GET request '' > 81 : 82 : 83 : 84 : { % for format in available_formats % } 85 : 86 : 87 : href= ' { % add_query_param request api_settings.URL_FORMAT_OVERRIDE format % } ' 88 : rel= '' nofollow '' 89 : title= '' Make a GET request on the { { name } } resource with the format set to { { format } } '' > 90 : { { format } } 91 : 92 : 93 : { % endfor % } 94 : 95 : 96 : 97 : Traceback : File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/core/handlers/base.py '' in get_response 139. response = response.render ( ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/response.py '' in render 105. self.content = self.rendered_content File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/rest_framework/response.py '' in rendered_content 59. ret = renderer.render ( self.data , media_type , context ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/rest_framework/renderers.py '' in render 733. ret = template.render ( context ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/base.py '' in render 140. return self._render ( context ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/base.py '' in _render 134. return self.nodelist.render ( context ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/base.py '' in render 840. bit = self.render_node ( node , context ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/debug.py '' in render_node 78. return node.render ( context ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/loader_tags.py '' in render 123. return compiled_parent._render ( context ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/base.py '' in _render 134. return self.nodelist.render ( context ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/base.py '' in render 840. bit = self.render_node ( node , context ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/debug.py '' in render_node 78. return node.render ( context ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/loader_tags.py '' in render 62. result = block.nodelist.render ( context ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/base.py '' in render 840. bit = self.render_node ( node , context ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/debug.py '' in render_node 78. return node.render ( context ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/defaulttags.py '' in render 305. return nodelist.render ( context ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/base.py '' in render 840. bit = self.render_node ( node , context ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/debug.py '' in render_node 78. return node.render ( context ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/defaulttags.py '' in render 196. nodelist.append ( node.render ( context ) ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/django/template/base.py '' in render 1125. return func ( *resolved_args , **resolved_kwargs ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/rest_framework/templatetags/rest_framework.py '' in add_query_param 86. return escape ( replace_query_param ( uri , key , val ) ) File `` /Users/taylorhobbs/.virtualenvs/Workout_log/lib/python2.7/site-packages/rest_framework/templatetags/rest_framework.py '' in replace_query_param 22 . ( scheme , netloc , path , query , fragment ) = urlparse.urlsplit ( url ) Exception Type : AttributeError at / Exception Value : 'Module_six_moves_urllib_parse ' object has no attribute 'urlsplit '"
"I have the following code : In Python 2.7 , this runs as expected and prints : However in Python 3.x , the first line is printed , but the second line is not . It seems to delete the variable in the enclosing scope , giving me the following traceback from the last print statement : It is almost as if a del e statement is inserted after the except block . Is there any reasoning for this sort of behavior ? I could understand it if the Python developers wanted except blocks to have their own local scope , and not leak into the surrounding scope , but why must it delete a variable in the outer scope that was previously assigned ?"
"I 'm struggling with parsing some flaky HTML tables down to lists with Beautiful Soup . The tables in question lack a < /td > tag.Using the following code ( not the real tables I 'm parsing , but functionally similar ) : Gives me : Rather than the expected : It seems that the lxml parser that Beautiful Soup is using decides to add the < /td > tag before the next instance of < /tr > rather than the next instance of < td > .At this point , I 'm wondering if there a good option to make the parser place the ending td tags in the correct location , or if it would be easier to use a regular expression to place them manually before tossing the string into BeautifulSoup ... Any thoughts ? Thanks in advance !"
"I have 5 image fields in my model , imageA , imageB , imageC , imageD and imageEI am trying to save the images in the following manner.The image are of type Base64ImageFieldIn the above code content contains the raw data.I am trying to update the image using the dict I created ( the key is the field name and value is the content ) .However the images saved in the imageField of the model are raw and not an actual image . How can I fix this ? This is what my serializer looks likeMore info : If I do something like thisit works fine and the problem is solved.However there are two problems with this approach first of all I do not know the extension . How do I extract an extension ? I am just guessing a jpeg here and it works . The next thing is Ill have to check for imageA , B , C , D and E if they exist and then save each one individually . If I could come up with a dynamic solution close to something that I have that would work as well . This is what my jsondata looks like that I am posting"
"While reading fmark 's answer to the question What are `` named tuples '' in Python ? I saw that the example given there had the same name and reference , i.e . the word Point appears twice in the following statement : Point = namedtuple ( 'Point ' , ' x y ' ) So I went to the original reference : https : //docs.python.org/2/library/collections.html # collections.namedtuple And here too I found two more examples : Ideally words are not repeated in Python . For instance the whole line ( for the the Point example ) could be replaced by the following : OROf course , that 's assuming that the named tuple has to have the same name and reference . So my question is : when is it advisable ( if at all it is permitted ) that a named tuple should have a different name and reference ? I am yet to come across an example ."
"I 'm trying to get a string from a website . I use the requests module to send the GET request.However , for some reason , the text appears in Gibberish instead of Hebrew : Tough when I sniff the traffic with Fiddler or view the website in my browser , I see it in Hebrew : By the way , the html code contains meta-tag that defines the encoding , which is utf-8.I tried to encode the text to utf-8 but it still in gibberish . I tried to deocde it using utf-8 , but it throws UnicodeEncodeError exception.I declared that I 'm using utf-8 in the first line of the script.Moreover , the problem is also happend when I send the request with the built in urllib module.I read the Unicode HOWTO , but still could n't manage to fix it . I also read many threads here ( both about the UnicodeEncodeError exception and about why hebrew turns into gibberish in Python ) but I still could n't manage to fix it up.I 'm using Python 2.7.9 on a Windows machine . I 'm running my script in the Python IDLE.Thanks in advance ."
Digression StartI just learnt what metaclasses are in Python . I do n't think that the creators of python wanted everyone to use them . I mean name something a metaclass which might not be a class in most cases is enough to divert most people away from this concept ! Digression endOn to my question . I wrote this simple metaclass to add a default doc string to all classes that are created in the module . But it is not working : Output : What am I doing wrong ?
"I just stumbled upon the following odd situation : The character I entered is always the µ sign on the keyboard , but for some reason it gets converted . Why does this happen ?"
"I was going to use Python function annotations to specify the type of the return value of a static factory method . I understand this is one of the desired use cases for annotations . PEP 3107 states that : Function annotations are nothing more than a way of associating arbitrary Python expressions with various parts of a function at compile-time.Trie is a valid expression in Python , is n't it ? Python does n't agree or rather , ca n't find the name : def from_mapping ( mapping ) - > Trie : NameError : name 'Trie ' is not definedIt 's worth noting that this error does not happen if a fundamental type ( such as object or int ) or a standard library type ( such as collections.deque ) is specified.What is causing this error and how can I fix it ?"
"I tried to convert an image to hsv and back to rgb , but somehow I lost color information.And I also replicated the problem on shell too , just writing this line after importing also gives the same result.Can you tell me what am I doing wrong ?"
"I have a list of numbers which I put into a numpy array : then I want to subtract a number from each value in the array . It can be done like this with numpy arrays : Unfortunately , my data often contains missing values , represented by None . For this kind of data I get this error : What I would like to get for the above example is : How can I achieve it in an easy and efficient way ?"
"I 'd like to give documentation for each member of a python enum in a way that IPython can find it . What I have right now is something like : This is n't great , as it duplicates the member names , and makes it harder to ask for the documentation for just one member.I can get what I 'm after withBut this still suffers from repetition of the names.Is there an easier way of doing this ?"
"Can please someone explain how one may use 'is ' in an 'if ' condition . I am working with the fractions module , and I 'm having some trouble : The only thing I found to work is : Is there a way to use 'is ' here ? Thanks ."
"I have an issue with the django-filter application : how to hide the items that will produce zero results . I think that there is a simple method to do this , but idk how.I 'm using the LinkWidget on a ModelChoiceFilter , like this : What I need to do is filter the queryset and select only the Provider that will produce at least one result , and exclude the others.There is a way to do that ?"
"I just ran across Eric Lippert 's Closing over the loop variable considered harmful via SO , and , after experimenting , realized that the same problem exists ( and is even harder to get around ) in Python.and , the standard C # workaround does n't work ( I assume because of the nature of references in Python ) I recognize that this is n't much of a problem in Python with its general emphasis on non-closure object structures , but I 'm curious if there is an obvious Pythonic way to handle this , or do we have to go the JS route of nested function calls to create actually new vars ?"
"I am using scrapy for a project . I ran the following commands for deploying the project : $ scrapy deploy -lThen i got the following o/p : scrapysite http : //localhost:6800/ $ cat scrapy.cfg $ scrapy deploy scrapysite -p scrapBibAs you can see , getting spiders as 0 , although i have written 3 spiders inside project/spiders/ folder . As as result i am unable to start the crawl with curl requests . Please help"
"I 'm trying to fill out and submit a form using Python , but I 'm not able to retrieve the resulting page . I 've tried both mechanize and urllib/urllib2 methods to post the form , but both run into problems . The form I 'm trying to retrieve is here : http : //zrs.leidenuniv.nl/ul/start.php . The page is in Dutch , but this is irrelevant to my problem . It may be noteworthy that the form action redirects to http : //zrs.leidenuniv.nl/ul/query.php.First of all , this is the urllib/urllib2 method I 've tried : However , when I try to print the retrieved html I get the original page , not the one the form action refers to . So any hints as to why this does n't submit the form would be greatly appreciated.Because the above did n't work , I also tried to use mechanize to submit the form . However , this results in a ParseError with the following code : where the last line exits with the following : `` ParseError : unexpected '- ' char in declaration '' . Now I realize that this error may indicate an error in the DOCTYPE declaration , but since I ca n't edit the form page I 'm not able to try different declarations . Any help on this error is also greatly appreciated.Thanks in advance for your help ."
"We can ( shallow ) copy a list by using [ : ] : We can also ( shallow ) copy it by using [ : : ] : and now z1 == z2 will be True . I understand how these slices work after reading the answers in Explain Python 's slice notation.But , my question is , is there any difference between these two internally ? Is one more efficient than the other in copying or do they do exactly the same things ?"
"In my feed that is published to feedburner I have Russian characters in campaign name in tracking settings Feed : $ { feedUri } $ { feedName } . The problem is that it results as incorrect __utmz cookie set by Google Analytics , and can not be processed by my backend ( which is web.py ) .This error occurred in Firefox , and I have managed to fix it with this code : But today I got this `` cookie_err=1 '' redirect even in Chrome . I tried this on some other sites that are based on web.py and Analytics , and they all raise internal server error . And this error keeps until the illegal cookie is removed , which is a difficult thing to do by a regular visitor.I want to know what other options I should consider . Maybe Python Cookie module is incorrect , or it is browser 's bug that lets in incorrect cookie . This stuff can be used for malicious purposes , because there are many Python websites that use Google Analytics and Cookie module.This is tracking query : utm_source=feedburner & utm_medium=twitter & utm_campaign=Feed % 3A+cafenovru+ % 28 % D0 % 9E % D0 % BF % D0 % B8 % D1 % 81 % D1 % 8C+ % D1 % 82 % D1 % 80 % D0 % B0 % D0 % BF % D0 % B5 % D0 % B7 % D0 % BD % D1 % 8B % D1 % 85+ % D0 % 92 % D0 % B5 % D0 % BB % D0 % B8 % D0 % BA % D0 % BE % D0 % B3 % D0 % BE+ % D0 % 9D % D0 % BE % D0 % B2 % D0 % B3 % D0 % BE % D1 % 80 % D0 % BE % D0 % B4 % D0 % B0 % 29Incorrect __utmz cookie value is 37098290.1322168259.5.3.utmcsr=feedburner|utmccn=Feed : % 20cafenovru % 20 ( Опись % 20трапезных % 20Великого % 20Новгорода ) |utmcmd=twitterIllegal cookie is set by Analytics javascript on the first page access , and server side error appears on subsequent requests ."
"I already have python3/ipython3 ( and notebook ) installed . So I followed the instructions , and I did : And it seems to have succeeded . But at nothing seems installed ( at least not in my PATH ) : Sorry , I 'm a noob when it comes to the python installation details , I 'm certainly missing something obvious , but I ca n't find it in the docs.Any help would be much appreciated ! ! : )"
"I try to reproduce results generated by the LSTMCell from TensorFlow to be sure that I know what it does.Here is my TensorFlow code : Here is its output : And here is my own implementation : And here is its output : As you might notice I was able to reproduce the first hidden vector , but the second one and all the following ones are different . What am I missing ?"
I 'd like to parse a numpydoc docstring and access each component programatically.For example : What I 'd like to do is : I 've been searching around and found things like numpydoc and napoleon but I have n't found any good leads for how to use them in my own program . I 'd appreciate any help .
"I have a class with ten different counters . I need to increment one or another of these at runtime , and the increment method is told the name of the counter to increment.I 'm wondering if there 's a cleaner way than this : I 'm not terribly worried about race conditions and the like.No , you ca n't change the way the function is called . Obviously , the real code is a little less trivial than the example ."
"I 've been using scikit-image to classify road features with some success . See below : . I am having trouble doing the next step which is to classify the features . For example , let 's say these features are located in the box ( 600 , 800 ) and ( 1400 , 600 ) .The code I 'm using to extract the information is : The objective is to have a table in the following form : feature_type would be based on colours , ideally shoulders would be one colour , trees and brush would be another , etc.How can I extract the data I need ? ( i.e : have scikit break the image into different components where I know the location of each component . I can then pass each component to a classifier which will identify what each component is ) Thanks !"
"I know you can use a dictionary as an alternative to a switch statement such as the following : However if conditions are used , other than equality ( == ) which return true of false these cant be mapped as easily i.e . : The above can not be directly converted to a dictionary key-value pair as is : A lambda can be used since it is hash-able but the only way to get the resulting string out of the map is by passing the same lambda object into the dictionary and not when the evaluation of the lambda is true : Does anyone know of a technique or method to create a mapping between a lambda evaluation and a return value ? ( this maybe similar to pattern matching in functional language )"
"suppose i a have a multiplicative expression with lots of multiplicands ( small expressions ) where for example c is ( x-1 ) , d is ( y**2-16 ) , k is ( xy-60 ) ... .. x , y are numbersand i know that c , d , k , j maybe zeroDoes the order i write the expression matters for faster evaluation ? Is it better to write cdkj ... .*w or python will evaluate all expression no matter the order i write ?"
"Is there a reason why looping through an implicit tuple in a for loop is okay , but when you do it in a comprehension you get a syntax error ? For example : But in a comprehension : Is there a reason for this ? I was n't sure about the correct terminology , so my searches yielded nothing useful . Update : Per the comments , this syntax is valid for Python 2.x , but not for Python 3.x ."
"I have a list of dicts that stores URLs . It has simply two fields , title and url . Example : However , I 'd to get a tree structure from this list of dicts . I 'm looking for something like this : My first attempt at this would be a urlparse soup in a bunch of for loops and I 'm confident that there 's a better and faster way to do this.I 've seen people on SO work magic with list comprehensions , lambda functions , etc . I 'm still in the process of figuring it out . ( For Django developers : I 'll be using this my Django application . I 'm storing the URLs in a model called Page which has two fields name and title )"
"I need to find the n largest elements in a list of tuples . Here is an example for top 3 elements.I tried using heapq.nlargest . However it only returns the first 3 largest elements and does n't return duplicates . For example , I can only think of a brute force approach . This is what I have and it works.Any other ideas on how I can implement this ? Thanks ! EDIT : timeit results for a list of 1 million elements show that mhyfritz 's solution runs in 1/3rd the time of brute force . Did n't want to make the question too long . So added more details in my answer ."
"I am learning Python these days , and this is probably my first post on Python . I am relatively new to R as well , and have been using R for about a year . I am comparing both the languages while learning Python . I apologize if this question is too basic.I am unsure why R outputs Inf for something python does n't . Let 's take 2^1500 as an example.In R : In Python : I have two questions : a ) Why is it that R provides Inf when Python doesn't.b ) I researched How to work with large numbers in R ? thread . It seems that Brobdingnag could help us out with dealing with large numbers . However , even in such case , I am unable to compute nchar . How do I compute above expression i.e . 2^1500 in R"
What are the differences between select and tablename.select ( ) ? When I pass column name to table.select like : the sql is like and when I pass a column to select ( instance method ) : the sql is likeI want the same result as select with table.select.When I read the docs of select it 's the same so it 's the same method but why they have a different behavior ?
"I understand that conditional expressions ( or ternary operators ) are lazy in Python . They represent conditional execution rather than conditional selection . In other words , only one of a or b is evaluated in the following : What I 'm interested to know is how this is implemented internally . Does Python convert to an if statement as below and , if so , at what stage does this conversion occur ? Or is the ternary operator actually a distinct and separate expression , defined entirely separately ? If so , can I access the CPython code for conditional expressions ? I 've looked at the following which explain what the ternary operator does , but none of them make clear how they are implemented : Does Python have a ternary conditional operator ? Putting a simple if-then-else statement on one linepython ? ( conditional/ternary ) operator for assignmentsIs there an equivalent of C ’ s ” ? : ” ternary operator ? Conditional expressionsEdit : You can assume CPython reference implementation ."
"I want to stream the output of several python3 scripts to my browser . I followed some suggestions from different SO answers and nearly solved it . But my for loop reading from stdout runs into an infinite loop after executing the script . The output is correct and everything is fine , but the endless loop is a problem . How can I end the stream after the output is finished ? When I poll the subprocess to see if it has finished , the output of the script is broken , because it does not print the entire stdout stream , when the prints come too fast . If I add a sleep ( 1 ) between each print , the problem does not occur.The code works , but runs into an infinite loop . If I uncomment these lines , the output is somehow buffered and parts of it get lost . I also tried to poll the subprocess in a separate thread and just have to toggle a flag like if not running : break , but this leads to the same behaviour like the uncommented code above ."
"Haskell and Python do n't seem to agree on Murmurhash2 results . Python , Java , and PHP returned the same results but Haskell do n't . Am I doing something wrong regarding Murmurhash2 on Haskell ? Here is my code for Haskell Murmurhash2 : And here is the code written in Python : Python returned 3650852671 while Haskell returned 3966683799"
"In another SO question , the performance of regexes and Python 's in operator were compared . However , the accepted answer uses re.match , which only matches the beginning of a string , and thus behaves completely different to in . Also , I wanted to see the performance gain of not recompiling the RE each time.Surprisingly , I see that the pre-compiled version seems to be slower.Any ideas why ? I am aware that there are quite a few other questions here that wonder about a similar issue . Most of them perform the way they do simply because they do not correctly reuse the compiled regex . If that is also my issue , please explain.Output :"
"Is there a python builtin that does the same as tupler for a set of lists , or something similar : so , for example : returns : or perhaps there is proper pythony way of doing this , or is there a generator similar ? ? ?"
"this may not be an earth-shattering deficiency of python , but i stillwonder about the rationale behind the following behavior : when iruni get : :i can avoid this exception by ( 1 ) deleting the trailing # ; ( 2 ) deleting or outcommenting the if __name__ == '__main__ ' : \nprint ( 'yeah ! ' ) lines ; ( 3 ) add a newline to very end of thesource.moreover , if i have the source end without a trailing newline rightbehind the print ( 'yeah ! ' ) , the source will also compile withouterror.i could also reproduce this behavior with python 2.6 , so it ’ s not newto the 3k series.i find this error to be highly irritating , all the more since when iput above source inside a file and execute it directly or have itimported , no error will occur—which is the expected behavior.a # ( hash ) outside a string literal should always represent thestart of a ( possibly empty ) comment in a python source ; moreover , thepresence or absence of a if __name__ == '__main__ ' clause shouldnot change the interpretation of a soure on a syntactical level.can anyone reproduce the above problem , and/or comment on thephenomenon ? cheers"
"On Windows it is easy . Just run your program with pythonw instead with python and code will be executed in the background.So , the thing I wish to achieve is easily arranged.I have an application which is really a service doing underground stuff . But this service needs a control panel.So , on Windows I use wxPython to create a GUI , even some wx stuff to provide needed service , and when user is done with adjustments she/he clicks Hide and Show ( False ) is called on main window.Thus the GUI disappears and the service continues its work in the background . User can always bring it back using a hotkey.The trouble is that on Mac OS X this strategy works only to some degree.When wx.Frame.Show ( False ) is called , the window disappears along with its menu bar and service works fine , but the Application is still visible there.You can switch to it regardless the fact that you can not do anything with it . It is still present in the Dock etc . etc.This happens when program is using python or pythonw or when it is bundled with Py2App.No matter what I do , the icon stays there.There must be some trick that allows a programmer to remove this naughty icon and thus stop bothering poor little user when she/he does n't want to be bothered.Hiding window is obviously not enough . Anyone knows the trick ? N.B . : I would really like to do it the way I described above and not mess with two separate processes and IPC.Edit : After much digging I found these : How to hide application icon from Mac OS X dockhttp : //codesorcery.net/2008/02/06/feature-requests-versus-the-right-way-to-do-itHow to hide the Dock iconAccording to last link the proper way to do it is to use : orSo what I want ( runtime switching from background to foreground and back ) is possible.But how to do it from Python ? ? ? Constants : NSApplicationActivationPolicyProhibited and NSApplicationActivationPolicyAccessory are present in AppKit , but I can not find setApplicationActivationPolicy function anywhere.NSApp ( ) does n't have it.I know there is a way of doing it by loading objc dylib with ctypes , delegating to NSApp and sending `` setApplicationActivationPolicy : < constant_value > '' , but I do n't know how much will this mess with wx.App ( ) . And it is a bit much work for something that should be available already.In my experience , NSApp ( ) and wx.App ( ) active at the same time dislike eachother pretty much.Perhaps we can get the NSApp ( ) instance that wx is using somehow and use wx 's delegate ? ? ? Remember please , already suggested solutions with starting as agent and switching to foreground or running multiple processes and doing IPC is very undesirable in my case.So , ideally , using setApplicationActivationPolicy is my goal , but how ? ( Simple and easy and no messup to wx.App ( ) please . ) Any ideas ? ? ?"
I have the following tuple of tuples : How could I turn it into a list of dictionary like this
"I 've got a binary tree , where the nodes interact with data . I initially implemented a standard post order recursive traversal . I thought I could improve it by using generators so that I can use the same traversal method for other uses , and not have to pass the same data around constantly . This implementation is shown below.However , this was far slower than the previous version ( ~50s to ~17s ) and used far more memory . Have I made a mistake in my generator function version ? I 'd prefer to use this method but not at the expense of performance.EDIT : Something I should have mentioned initially was that these results were obtained under PyPy 2.3.1 , rather than standard CPython ."
I am trying to create a merge request using the GitLab Merge Request API with python and the python requests package . This is a snippet of my codebut I keep getting the following message in the replyWhat should I change in my request to make it work ?
"I am having a model Player with my django app .As per my requirement i need to create a new model BookmarkPlayerwhich will have all fields of Player model.Right now i have two things into my mind to do this .I can extend Player class for BookmarkPlayer model . I can define all fields of Player model into BookmarkPlayer model.I just want to know which way is better to do this .Please share with my if there is another good way.Updated QuestionKnbb 's idea to create a base class is interesting but i am facing issue with one of my model which is already existed into database . My actual models : Models after changes : After these changes when i am running my django server i am getting errors given below.Answer : Finally i got answer if we are using the related_name attribute on a ForeignKey or ManyToManyField into a Abstract model.This would normally cause a problem in abstract base classes , since the fields on this class are included into each of the child classes , with exactly the same values for the attributes ( including related_name ) each time .To work around this problem , when you are using related_name in an abstract base class ( only ) , part of the name should contain ' % ( app_label ) s ' and ' % ( class ) s'.https : //docs.djangoproject.com/en/dev/topics/db/models/ # abstract-base-classesNow my BasePlayer model is"
What is the difference between hexbin and histogram2d ? You can see that the histogram2d gives a -90 degree rotation . I know the data should be like the hexbin plot .
"I 'm trying to write a unit test that will ensure an HTTPException is raised when necessary . Here is the test : Which produces the following : The exception is raised , but it is not caught by the test . This is similar to what happened in this question , but it is not quite the same . Can someone tell me what I 'm missing ?"
"I am using tornado as a server . I would like it to receive binary data . The server side is as simple as simple gets : This server is just used to visualize incoming data , not too special . The server works just find with standard ascii , but it explodes when it gets any unicode ( my test for fake binary data ) . I used the site http : //www.websocket.org/echo.html and redirected the sending to go to ws : //172.0.0.1:9500/ which is where I set up the server . The server then prompted me with the very nasty error : The character was ¡ , an upside down ! . Now I know that tornado can send binary , but apparently not receive ? I am probably doing some petty mistake , but where is it ?"
"Alright , so my title sucked . An example works better : I want to parse input , using the first word as the `` command '' , and the rest of the string as a parameter . Here 's the simple version of how my non-Pythonic mind is coding it : I like Python because it makes normally complicated things into rather simple things . I 'm not too experienced with it , and I am fairly sure there 's a much better way to do these things ... some way more pythonic . I 've seen some examples of people replacing switch statements with dicts and lambda functions , while other people simply recommended if..else nests ."
"I have inherited code that looks something like this : For that last line , I want to make the code 2to3 compatible , but MethodType only takes two arguments in python3.Simplest option is probably to let it break intelligently andBut then I realized that I do n't understand why I 'm adding the third argument in python2 , because setattr ( a , 'foo ' , types.MethodType ( foo , a ) ) works across languages.In Python2 , what is the third argument buying me , to bind it to the class vs not ?"
"I build the class for geometric transformation . When I run a Unit test it fails because of rounding errors coming from the operations inside my methods.In my test I compare the result from one of the method which should return the point ( 2,2,0 ) , but because of rounding errors it returns ( 1.9999999999999996 , 1.9999999999999996 , 0.0 ) From the calculation point of view it is acceptable , but I do n't want my code to fail on the simple unit test . What is a correct way to resolve problem like that ? Obviously I can not round up the output numbers or convert them to integers as normally it is not the case.Is there a way to code unit test to ignore rounding error ? Is there any other way to resolve problem ? Edit : The question can be solved by using self.assertAlmostEqual as rightly suggested in another answer but the problem is that I need to test entrance of a tuple . After all suggestions I try to do : but I need to automatise it somehow as later I need to test a list of tuples as the sample points ' coordinates for a vector field.The solution suggested as a duplicate is only a half measure as it would be a bit tedious write 300 more comparisons if there is 100 tuples in the list ."
"Given these F # type declarations ... ... is there an equally expressive definition of this specific state machine ... ... with Python ? Note that via the `` rec '' , we did n't have to do assignments in an order defined by a topological sort ... ( e.g . state0 is defined in terms of state1 , even though state1 is defined later on ) .P.S . The option of using strings as state identifiers ... ... leaves open the case of invalid keys ( i.e . invalid message specifiers in the state machine ) ."
"UPDATE 2018-10-13This issue has been fixed in OpenCV 3.4A similar question has been asked but unanswered here.How can I read a gray scale image in OpenCV with alpha channel ? For example , if I try to read the following image , all I get is 2d array of all zeros ."
I am trying to label multiple maximum but unable to display them properly . Also the label is appearing too far from the point overlapping with the legend.And the output I got Output GraphI am just beginning with python so very small hint may not be understandable by me.Please help .
"I am writing a Scrapy spider that crawls a set of URLs once per day . However , some of these websites are very big , so I can not crawl the full site daily , nor would I want to generate the massive traffic necessary to do so . An old question ( here ) asked something similar . However , the upvoted response simply points to a code snippet ( here ) , which seems to require something of the request instance , though that is not explained in the response , nor on the page containing the code snippet . I 'm trying to make sense of this but find middleware a bit confusing . A complete example of a scraper which can be be run multiple times without rescraping URLs would be very useful , whether or not it uses the linked middleware . I 've posted code below to get the ball rolling but I do n't necessarily need to use this middleware . Any scrapy spider that can crawl daily and extract new URLs will do . Obviously one solution is to just write out a dictionary of scraped URLs and then check to confirm that each new URL is/is n't in the dictionary , but that seems very slow/inefficient . SpiderItemsMiddlewares ( ignore.py )"
"is it possible to change the name of the imported python file ? in my views.py in django i havein my models i have a model of a Client , but when i used suds for the WSDL file , i import Client but i got the AttributeError in my Client 's model ... my question is , it is possible if i can change the name of the Client in suds.client ? can anyone have an idea about my situation ? thanks in advance ..."
"I searched for understanding about the all function in Python , and I found this , according to here : all will return True only when all the elements are Truthy.But when I work with this function it 's acting differently : Why is it that when all elements in input are False it returns True ? Did I misunderstand its functionality or is there an explanation ?"
"I have a numpy object array containing several lists of index numbers : I define a vectorized function to append a value to each list : I invoke the function . I do n't care about the return value , just the side effect.The index 99 was added twice to the first list . Why ? I 'm stumped.With other values of idxLsts , it does n't happen : My suspicion is it 's related to the documentation which says : `` Define a vectorized function which takes a nested sequence of objects or numpy arrays as inputs and returns a numpy array as output . The vectorized function evaluates pyfunc over successive tuples of the input arrays like the python map function , except it uses the broadcasting rules of numpy . ''"
"I am trying to plot data for a whole year as a polar chart in matplotlib , and having trouble locating any examples of this . I managed to convert the dates from pandas according to this thread , but I ca n't wrap my head around ( litterally ) the y-axis , or theta . This is how far I got : which gives me this plot : reducing the date range to understand what is going ontimes = pd.date_range ( `` 01/01/2016 '' , `` 01/05/2016 '' ) I get this plot : I gather that the start of the series is between 90 and 135 , but how can I 'remap ' this so that my year date range starts and finishes at the north origin ?"
"Suppose I have d = { 'dogs ' : 3 } . Using : would create the key 'cats ' and give it the value 2 . If I really intend to update a dict with a new key and value , I would use d.update ( cats=2 ) because it feels more explicit . Having automatic creation of a key feels error prone ( especially in larger programs ) , e.g . : Question : Is there a way to disable the automatic creation of a key that does n't exist through d [ key ] = value , and instead raise a KeyError ? Everything else should keep working though :"
"I 'm aware of raise ... from None and have read How can I more easily suppress previous exceptions when I raise my own exception in response ? .However , how can I achieve that same effect ( of suppressing the `` During handling of the above exception , another exception occurred '' message ) without having control over the code that is executed from the except clause ? I thought that sys.exc_clear ( ) could be used for this , but that function does n't exist in Python 3.Why am I asking this ? I have some simple caching code that looks like ( simplified ) : When there 's an exception in the API call , the output will be something like this : This is misleading , as the original KeyError is not really an error at all . I could of course avoid the situation by changing the try/except ( EAFP ) into a test for the key 's presence ( LBYL ) but that 's not very Pythonic and less thread-friendly ( not that the above is thread-safe as is , but that 's beside the point ) .It 's unreasonable to expect all code in some_api to change their raise X to raise X from None ( and it would n't even make sense in all cases ) . Is there a clean solution to avoid the unwanted exception chain in the error message ? ( By the way , bonus question : the cache thing I used in the example is basically equivalent to cache_dict.setdefault ( key , some_api.get_the_value_via_web_service_call ( key ) ) , if only the second argument to setdefault could be a callable that would only be called when the value needs to be set . Is n't there a better / canonical way to do it ? )"
"I am looking for some advice to avoid having to instantiate a class twice ; this is more of a design pattern question . I am creating an application using the Python Click library.I have a Settings class that first loads all initial default settings into a dictionary ( hard-coded into the application ) , then loads all settings overrides ( if specified ) from a TOML file on the user 's computer into a dictionary , and then finally merges the two and makes them available as attributes of the class instance ( settings. < something > ) .For most of these settings , I also want to be able to specify a command-line flag . The priority then becomes : Command-line flag . If not specified , then fallback to ... User setting in TOML file . If not specified , then finally fallback to ... Application defaultIn order to achieve this result , I am finding that , when using Click 's decorators , I have to do something like this : Why twice ? The settings = Settings ( ) line is needed to provide the @ click.option decorators with the default value . The default value could either come from the user override TOML file ( if present ) , or from the application default.The click.make_pass_decorator seems to be the recommended way for interleaved commands ; it 's even mentioned in their documentation . Inside of the function , in addition to the CLI parameters passed , I also sometimes needs to reference other attributes in the Settings class.My question is , which is better ? Is there a way to use the pass_settings decorator in the other click.option decorators ? Or should I ditch using click.make_pass_decorator entirely ?"
"A simple program which calculates square of numbers and stores the results : Could it be the difference in array handling for the two options ? My actual program would have something more complicated but this is the kind of calculation that I need to parallelize , as simply as possible , but not with such results ."
"I 'm developing a Python application and in the process of branching off a release . I 've got a PyPI server set up on a company server and I 've copied a source distribution of my package onto it.I checked that the package was being hosted on the server and then tried installing it on my local development machine . I ended up with this output : The reason is that I 'm trying to import a third-party library appdirs in my setup.py , which is necessary for me to compute the data_files argument to setup ( ) : However , I do n't have appdirs installed on my local dev machine and I do n't expect the end users to have it either.Is it acceptable to rely on third-party libraries like this in setup.py , and if so what is the recommended approach to using them ? Is there a way I can ensure appdirs gets installed before it 's imported in setup.py , or should I just document that appdirs is a required package to install my package ?"
"In Python 2.7 I want to print datetime objects using string formatted template . For some reason using left/right justify does n't print the string correctly . This will result : Instead of Is there some trick to making datetime objects print using string format templates ? It seems to work when I force the datetime object into str ( ) but I 'd rather not have to do that since I 'm trying to print a list of values , some of which are not strings . The whole point of making a string template is to print the items in the list . Am I missing something about string formatting or does this seem like a python bug to anyone ? SOLUTION @ mgilson pointed out the solution which I missed in the documentation . link Two conversion flags are currently supported : ' ! s ' which calls str ( ) on the value , and ' ! r ' which calls repr ( ) . Some examples :"
"Given two arrays , for example [ 0,0,0 ] and [ 1,1,1 ] , it is already clear ( see here ) how to generate all the combinations , i.e. , [ [ 0,0,0 ] , [ 0,0,1 ] , [ 0,1,0 ] , [ 0,1,1 ] , [ 1,0,0 ] , [ 1,0,1 ] , [ 1,1,0 ] , [ 1,1,1 ] ] . itertools ( combinations or product ) and numpy.meshgrid are the most common ways as far as I know.However , I could't find any discussions on how to generate this combinations randomly , without repetitions . An easy solution could be to generate all the combinations and then choose some of them randomly . For example : However , this is infeasible when the number of combinations is too big.Is there a way to generate random combinations without replacement in Python ( possibly with Numpy ) without generating all the combinations ? EDIT : You can notice in the accepted answer that we also got for free a technique to generate random binary vectors without repetitions , which is just a single line ( described in the Bonus Section ) ."
"I 've written a simple Python script which used MIMEMultipart and SMTPLib to send a mail to an array of recipients.The code looks something like this : This sends a mail successfully , but the Subject like in Outlook Mail client looks something like this :"
"I package my Python application with PIP , providing a setup.py.During installation , I want to ask the user for several values ( user name , other configuration values ) , these values are then saved inside the application configfile stored inside the user directory.Is there a special PIP/distutils-way to ask for these configuration values during setup ? Or should I just use input to ask the user , like this : Or should I leave out asking for these values , and instead let the user configure the application on the first start ? I know that all of these ways are possible , but are there any conventions or best practices for that ? Or do you know of a popular Python project doing similar stuff which is a good example ?"
"How can I rotate the fancy arrow in Matplotlib ( I do n't want to animate ) , I tried with rotation , and transform , none of them are working for me , I think , I 'm doing some mistake , please any help or alternative solution is appreciated , I have pasted my code below.The code result.My requirement : All the 3 arrow ( Arrow represent the wind direction ) should rotate to specified degree , keeping the center point as anchor ."
"I ’ m processing an image with shapes , and I ’ m trying to count the objects that have 1 hole and those who have 2.I found information about it , but it was in MATLAB Segment out those objects that have holes in itThey used the Euler characteristic , I know that in Python exist the skimage library , but I ca n't use it.There ’ s some code I found too , but I can ’ t understand it . http : //www.cis.rit.edu/class/simg782/homework/hw3/hw3solutions.pdf Page 16.The idea is to count the objects that have 1 hole , and those that have 2 , and highlight their edges . `` Number of objects with one hole : 2 '' '' Number of objects with two holes : 4 '' Here is what I have , I did it with a simple cv2.HoughCircles :"
"I have a numpy array as followsI want to pick elements such that y coordinates are unique . If two y coordinates are same I want to pick element with lesser x coordinate.Expected outputExplanationpick [ 6,5 ] over [ 7,5 ] pick [ 8,10 ] over [ 9,10 ] and [ 10,10 ] pick [ 9 , 11 ] Thanks"
"I have a dataframe structured like this : So there are three column levels . I want to add a new column on the second level where for each of the third levels a computation is performed , for example 'new ' = 'foo ' + 'bar ' . So the resulting dataframe would look like : I have found a workaround which is listed at the end of this post , but its not at all 'panda-style ' and prone to errors . The apply or transform function on a group seems like the right way to go but after hours of trying I still do not succeed . I figured the correct way should be something like : The new column is properly added within the function , but is not returned . Using the same function with transform would work if the 'new ' column already exists in the df , but how do you add a new column at a specific level 'on the fly ' or before grouping ? The code to generate the sample df is : And my workaround : Which works , but creating a new dataframe for each group and 'manually ' assigning the levels is a really bad practice.So what is the proper way to do this ? I found several posts dealing with similar questions , but all of these had only 1 level of columns , and that 's exactly what I 'm struggling with ."
"The following query returns data right away : Without the limit clause , it takes a long time before the server starts returning rows : I observe this both by using the query tool ( psql ) and when querying using an API.Questions/issues : The amount of work the server has to do before starting to return rows should be the same for both select statements . Correct ? If so , why is there a delay in case 2 ? Is there some fundamental RDBMS issue that I do not understand ? Is there a way I can make postgresql start returning result rows to the client without pause , also for case 2 ? EDIT ( see below ) . It looks like setFetchSize is the key to solving this . In my case I execute the query from python , using SQLAlchemy . How can I set that option for a single query ( executed by session.execute ) ? I use the psycopg2 driver.The column time is the primary key , BTW.EDIT : I believe this excerpt from the JDBC driver documentation describes the problem and hints at a solution ( I still need help - see the last bullet list item above ) : By default the driver collects all the results for the query at once . This can be inconvenient for large data sets so the JDBC driver provides a means of basing a ResultSet on a database cursor and only fetching a small number of rows.and Changing code to cursor mode is as simple as setting the fetch size of the Statement to the appropriate size . Setting the fetch size back to 0 will cause all rows to be cached ( the default behaviour ) ."
"I 'm building a Django site for discussions . Users can participate in discussions and can also cast votes of approval for discussions and messages in the discussions . A simplified data model is as follows : I want to select the top 20 `` most active '' discussions , which means ordering by the sum of the number of messages , total number of message approval votes , and number of discussion approval votes for that discussion , or ( in pseudocode ) : Using annotations , I can calculate the totals of each of the three scoring factors : I then thought I was on to something when I found the extra method : and would then be able to do : but the extra call gives a DatabaseError : and thus I was foiled . Can the extra method not reference annotated fields ? Is there another way to do what I 'm trying to do , short of using a raw sql query ? I 'm using Postgres if that makes a difference.Any insights would be greatly appreciated !"
"I wrote a function that plots a figure consisting of two subplots of different sizes : I should mention that this is a module-level function , so at the top of that module I make importsWhen I run myplot.draw_plot ( ... ) , I get RuntimeError . The thing is this behaviour is inconsistent . I can call the function , say , three times , and the first two times I get the error , whereas the third time it runs OK.The Traceback isThanks for any help ! EDITObviously I 've been trying to figure out myself what is going on , so following the Traceback I checked the clean ( ) function in cbook.py.In the function I added a line that would print mapping.items ( ) and I noticed that the error occurs when there are entries similar to < weakref at 0480EBA0 ; dead > among those items . I 'm totally unfamiliar with weak references so I 'm stuck again.EDIT 2It 's certainly not a good solution but commenting out the clean ( ) function body helps in my case without producing any new errors ."
"I always thought open and io.open were interchangeable.Apparently not , if I believe this snippet : Note : I tested in Python 2.6.6"
"I can get chunks of an iterator doing as follow : Now let 's say I have an asynchronous generator ( python 3.6 ) : How could I get chunks ( let 's say of size 3 that would yield [ 0 , 1 , 2 ] , [ 3 , 4 , 5 ] , [ 6 , 7 , 8 ] , [ 9 ] ) of the resulting async_generator so that I could write :"
"If I have a SQLAlchemy declarative model like below : And I would like to ensure that only one of the *test_id foreign keys is present in a given row , how might I accomplish that in SQLAlchemy ? I see that there is an SQLAlchemy CheckConstraint object ( see docs ) , but MySQL does not support check constraints.The data model has interaction outside of SQLAlchemy , so preferably it would be a database-level check ( MySQL )"
"I 've got the following code : notification is a Notification class instance . Notification class is not imported here , and PyCharm ca n't use that type hinting ( inferred type : unknown ) .I 've tried using full class name , it did n't work . The obvious way is to import the class , but it never used , so it would be a redundant import ( and PyCharm will remove it while optimizing imports before commit ) . Less obvious way is to do some weird thing like Celery.task do : Is there any clean way to do this ?"
Is there any way to write unittests or doctests for innerfunc ?
"Say you have an attribute in a base class with a single setter method that will be used in all subclasses , but with different getter methods in all subclasses . Ideally you only want to write the code for the setter method once in the base class . Additionally , you want to make the base class abstract because it does n't have a complete getter method . Is there any way to do this ? Naively , I think it should go something like this : But upper = uppercase ( ) fails with Ca n't instantiate abstract class uppercase with abstract methods str.If I change the @ abc.abstractproperty in class _mystring to @ property , then upper = uppercase ( ) works , but so does upper = _mystring ( ) , which I do n't want.What am I missing ? Thanks !"
"Cython documentation shows how to declare existing C++ classes with overloaded methods.However , if I define my own cppclass with an overloaded method ... ... I get Function signature does not match previous declarationOverloading the constructor gives the same error there : Am I doing this incorrectly , or is this feature missing ? Update : Default arguments seem to be unusable too : ... passes Cython , but gets tripped up by the C++ compiler ( `` wrong number of arguments '' )"
I am using wagtail as a REST backend for a website . The website is built using react and fetches data via wagtails API v2.The SPA website needs to be able to show previews of pages in wagtail . My thought was to override serve_preview on the page model and simply seralize the new page as JSON and write it to a cache which could be accessed by my frontend . But im having trouble serializing my page to json . All attempts made feel very `` hackish '' I 've made several attempts using extentions of wagtails built in serializers but without success : Atempt 1 : Feels very ugly to use router here and set a bunch of attrsAttempt 2 : Better but i get context issues : Any toughts ?
"I was reading about different ways to clean up objects in Python , and I have stumbled upon these questions ( 1 , 2 ) which basically say that cleaning up using __del__ ( ) is unreliable and the following code should be avoid : The problem is , I 'm using exactly this code , and I ca n't reproduce any of the issues cited in the questions above . As far as my knowledge goes , I ca n't go for the alternative with with statement , since I provide a Python module for a closed-source software ( testIDEA , anyone ? ) This software will create instances of particular classes and dispose of them , these instances have to be ready to provide services in between . The only alternative to __del__ ( ) that I see is to manually call open ( ) and close ( ) as needed , which I assume will be quite bug-prone.I understand that when I 'll close the interpreter , there 's no guarantee that my objects will be destroyed correctly ( and it does n't bother me much , heck , even Python authors decided it was OK ) . Apart from that , am I playing with fire by using __del__ ( ) for cleanup ?"
"I would like to catch a specific exception and handle it accordingly - then I would like to continue and perform the generic handling that other exceptions would have to.Coming from a C background , I could have previously utilised gotos to achieve the desired effect.This is what I 'm currently doing is and it works fine : Tldr : Ie - is there `` Pythonic '' way of doing the following : NB : I am aware of the finally clause - this is n't intended for tidy-up ( ie- closing files ) ·"
"I 'm looking for a simple example demonstrating how to use testtool 's concurrent tests . I 've found one example , from here : However , it suffers from a rather large problem . It does n't tell you if your tests passed or failed . Not only is the StreamResult object temporary , StreamResult 's methods do n't do anything ."
"The problemI wanted to include requirement for pytz in setup.py script in my library , but wanted also to set the minimal version required . But the version numbers used by pytz module ( eg . `` 2012f '' ) seem to be incompatible with what distutils wants to be provided ( eg . `` 1.1.3 '' ) .Is there any way to include requirement for specific version of pytz ( eg . > =2012f ) without altering pytz or distutils ? DetailsTo do that I did something like that in setup.py file : But when I was doing sudo python setup.py install , the following error appeared : Seemingly the issue is caused by distutils trying to match this regular expression : and when it does not match , the above error is raised.I have seen people altering the source code of pytz ( to change the version into something more like 2012.6 ) , but it looks like extremely bad idea to me . I hope there is another way that I missed.Changing pytz ( > =2012f ) into pytz works , but it then does not limit the requirement to specific version of pytz module ."
"As a followup to the question Using builtin __import__ ( ) in normal cases , I lead a few tests , and came across surprising results.I am here comparing the execution time of a classical import statement , and a call to the __import__ built-in function.For this purpose , I use the following script in interactive mode : As in the linked question , here is the comparison when importing sys , along with some other standard modules : So far so good , import is faster than __import__ ( ) .This makes sense to me , because as I wrote in the linked post , I find it logical that the IMPORT_NAME instruction is optimized in comparison with CALL_FUNCTION , when the latter results in a call to __import__.But when it comes to less standard modules , the results reverse : What is the reason behind this difference in the execution times ? What is the actual reason why the import statement is faster on standard modules ? On the other hand , why is the __import__ function faster with other modules ? Tests lead with Python 3.6"
"In Perl 5.10 , I can say : ... and it will print out : Does Python have something like this ?"
"This is an example from another answer that extracts only the lowercase letters . ( Python 3 ) In this case , the ' ' * len ( string.ascii_lowercase ) maps lowercase letters to an blank space . So my expectation is that the all the lowercase letters will be replaced with ' ' , a blank space , but this is the output : So here are my questions : Why is the output different from my expectation ? When I look at the documentation , translate only takes in one argument . Why is it passed two arguments ?"
"I am trying to parse a partially standardized street address into it 's components using pyparsing . I want to non-greedy match a street name that may be N tokens long.For example : Should be parsed into : How would I do this with PyParsing ? Here 's my initial code : but when I try parsing it , the street suffix gets gobbled up by the default greedy parsing behavior ."
"I 'm trying to represent the ring ; where theta is the root of a monic irreducible polynomial f with integer coefficients of degree d. This ring is a subring of the algebraic integers , which itself is a subring of the field ; I can represent this field with sympy 's AlgebraicField classIs there a way to represent the above integer subring in a similar way ?"
"I 'm creating a document with WeasyPrint.I have sections that have names , some of which might span across multiple pages.When a section is too long , a page break occurs . What I am trying to do is to re-display the current section 's name , ideally with the same formatting.The following MWE shows how the section title is not displayed after a page break : Output of weasyprint example.html example.pdf : I want First section to be displayed , as a < h1 > tag , at the top on the left page.I would like to do as this tex.stackexchange post which , as I understand it , basically consists in checking if the current page number exceeds the current total page count , and if it does , inserting the last section title encountered.I 'm not aware of the possibility to do so in HTML , does it exist ? Is there any workaround to do this ? If not , is it possible to have WeasyPrint execute custom Python code on some page-break hook ?"
"I am trying to understand this code from someone else 's project . If you want the context it 's here : https : //github.com/newsapps/beeswithmachineguns/blob/master/beeswithmachineguns/bees.py # L501IS_PY2 is just a boolean variable , True if the Python major version is 2.I know that a non-empty string is True , but for some reason I do n't understand openmode is assigned either ' w ' or 'wt ' rather than True or False.Could someone explain the result ?"
"I need to transform a DataFrame in which one of the columns consists of a list of tuples , each item in each of the tuples has to be a separate column.Here is an example and a solution in Pandas : This is the pandas solution . I need to be able to do the same but using PySpark ( 2.3 ) . I have started working on it , but immediately got stuck : Apparently , Spark does n't support indexing . Any pointers appreciated ."
"I 'm reading a file and unpacking each line like this : However , it 's possible that line may have more or fewer columns than the variables I wish to unpack . In the case when there are fewer , I 'd like to assign None to the dangling variables , and in the case where there are more , I 'd like to ignore them . What 's the idiomatic way to do this ? I 'm using python 2.7 ."
"I am currently processing a very large database of locations and trying to match them with their real world coordinates . To achieve this , I have downloaded the geoname dataset which contains a lot of entries . It gives possible names and lat/long coordinates . To try and speed up the process , I have managed to reduce the huge csv file ( of 1.6 GB ) to 0.450 GB by removing entries that do not make sense for my dataset . It still contains however 4 million entries.Now I have many entries such as : Slettmarkmountains seen from my camp site in Jotunheimen , Norway , last weekAdventuring in Fairy Glen , Isle of Skye , Scotland , UKMorning in Emigrant Wilderness , CaliforniaKnowing that string matching with such long strings , I used Standford 's NER via NLTK to get a better string to qualify my location . Now I have strings like : Slettmarkmountains Jotunheimen NorwayFairy Glen Skye Scotland UKEmigrant Wilderness CaliforniaYosemite National ParkHalf Dome Yosemite National ParkThe geoname dataset contains things like : Jotunheimen Norway Lat Long Slettmarkmountains Jotunheimen Norway Lat LongBryce Canyon Lat LongHalf Dome Lat Long ... And I am applying this algorithm to get a good possible match between my entries and the geoname csv containing 4M entries . I first read the geoname_cleaned.csv file and put all of the data into a list . For each entry I have I then call for each one of my entries string_similarity ( ) between the current entry and all the entries of the geoname_list I have tested the algorithm on a subset of my original dataset and it works fine but it is obviously terribly slow ( takes up to 40 seconds for a single location ) . Since I have more than a million entries to process , this will take a good 10000 hours or more . I was wondering if you guys had any idea on how to speed this up . I thought of parallel processing obviously but I do n't have any HPC solution available . Perhaps simple ideas could help me speed this up.I 'm open to any and every idea that you guys might have but would somehow prefer a python-compatible solution.Thanks in advance : ) .Edit : I have tried fuzzywuzzy with fuzz.token_set_ratio ( s1 , s2 ) and it gives worst performances ( running time is worse , and results are not as good ) . Matches are not as good as they used to be with my custom technique and running time has increased by a good 15 seconds for a single entry.Edit 2 : I also though of using some kind of sorting at the beginning to help with the matching but my naive implementation would not work . But I 'm sure there are some ways to speed this up , by perhaps getting rid of some entries in geoname dataset , or sorting them in some way . I already did a lot of cleaning to remove useless entries , but ca n't get the number much lower than 4M"
"I am working on a RNN controller , which takes the current state of the plant as the input to the RNN , and generates the output as the controlling signal . After executing the control , the updated plant state is fed back to the RNN as the input of next time step . In this looping , the input sequence is stacked step by step , rather than all given in advance . For now , no training is involved . Only the single-step forward simulation is needed . So a tensorflow RNN operation that can do this one-step RNN output is what I 'm looking for.I defined two kinds of input : Input_data for the batch_size sequences of input , and input_single for the input of current time step.The network is read out in two ways : y_single for each time step , and y_seq for the whole minibatch of the input ."
"I 've been writing a small Python script that executes some shell commands using the subprocess module and a helper function : The following lines reads the standard output and error : As you can see , stdout and stderr are not used . Suppose that I want to write the output and error messages to a log file , in a formatted way , e.g . : My question is , what 's the most Pythonic way to do it ? I thought of three options : Inherit the file object and override the write method.Use a Delegate class which implements write.Connect to the PIPE itself in some way.UPDATE : reference test scriptI 'm checking the results with this script , saved as test.py : Any ideas ?"
"I try to do a tutorial on Pipeline for students but I block . I 'm not an expert but I 'm trying to improve . So thank you for your indulgence.In fact , I try in a pipeline to execute several steps in preparing a dataframe for a classifier : Step 1 : Description of the dataframeStep 2 : Fill NaN ValuesStep 3 : Transforming Categorical Values into NumbersHere is my code : If I execute step 1 and 2 or step 1 and 3 it works but if I execute step 1 , 2 and 3 at the same time . I have this error :"
"I am trying to get my head around generators and learn about how to use them . I have seen a lot of examples and get that they produce results one at a time rather than outputting them at once like in a regular function . But all the examples I have seen have involved going through a list and printing values that are generated through the function . What if you want to actually create a list ? For example I have seen an example about even numbers which just generates even numbers and prints them out , but what if I want a list of even numbers like this : Does this defeat the purpose of using a generator as it then creates this in an even list . Does this method still save some memory/time ? Or is the below method without using generators just as efficient . In this case in what exact cases are generators useful ?"
"I 'm coming from this question Matplotlib : two x axis and two y axis where I learned how to plot two x and y axis on the same plot.Here 's a MWE : and this is the output : The right y axis and the top x axis correspond to the blue curve.As you can see , the second y label is missing even though it is defined . I 've tried a number of different approaches but I ca n't get it to show . Am I doing something wrong ? Add : apparently there 's an issue with the line : if I invert it like so : then it 's the second x label that will not show ."
"I 've raked through a lot of `` make for Python '' projects , but I ca n't find any with the simplicity of a cake file . What I 'm looking for is a Python equivalent that will let me : Keep the build commands in a single file in my project rootDefine each task as a simple function with a description that will automatically be displayed when the `` make '' file is run without argumentsImport my Python modulesI 'm picturing something like this : I 've searched and searched but found nothing like it . If it does n't exist , I will make it myself and probably answer this question with it.Thanks for your help !"
"I 'd like my students to be able to check their code as they write it in a Jupyter Notebook by calling a function from an imported module which runs a unittest . This works fine unless the function needs to be checked against objects which are to be picked up in the global scope of the Notebook.Here 's my check_test module : If the Notebook is : The error here is not suprising : add_n does n't know about the n I defined in check_add_n.So I got to thinking I could do something like : in the Notebook , and then passing n in the test : But this is causing me UnboundLocalError headaches down the line because of the assignment of n , even within an if clause : this is apparently stopping the Notebook from picking up n in global scope when it 's needed.For the avoidance of doubt , I do n't want insist that n is passed as an argument to add_n : there could be many such objects used but not changed by the function being tested and I want them resolved in the outer scope.Any ideas how to go about this ?"
"Have a function fix ( ) , as a helper function to an output function which writes strings to a text file . Turning on doctests , I get the following error : No matter what combination of \ and 's I use , the doctest does n't seem to to want to work , even though the function itself works perfectly . Have a suspicion that it is a result of the doctest being in a block comment , but any tips to resolve this ."
"I have to analyze methods a foreign API , and how I usually do it it to write a test script , or find an example code , do aWhere I want to experiment , and than take a look at currently available variables , objects and their methods . However , when I want to check the documentation the way Ipython offersI getIf I tryIt gives Does that mean that there is no documentation for the selected method , or am I using the wrong way of calling it ?"
"I read an example in David Beazley 's Python Essential Reference : It is mentioned that ... the classes have created a reference cycle in which the reference count never drops to 0 and there is no cleanup . Not only that , the garbage collector ( the gc module ) won ’ t even clean it up , resulting in a permanent memory leak.Can someone explain how this happens ? How can weakreferences help here ?"
"I have 1D NumPy array as follows : I want to calculate means of ( 1,2,6,7 ) , ( 3,4,8,9 ) , and so on.This involves mean of 4 elements : Two consecutive elements and two consecutive elements 5 positions after.I tried the following : This unfortunately does not give me the desired results . How can I do it ?"
"I have read Making an executable in Cython and BuvinJ 's answer to How to obfuscate Python code effectively ? and would like to test if the source code compiled with Cython is really `` no-more-there '' after the compilation . It is indeed a popular opinion that using Cython is a way to protect a Python source code , see for example the article Protecting Python Sources With Cython.Let 's take this simple example test.pyx : Then let 's use Cython : This produces a test.c . Let 's compile it : It works ! It produces a 140KB test.exe executable , nice ! But in this answer How to obfuscate Python code effectively ? it is said implicitly that this `` compilation '' will hide the source code . It does not seem true , if you run test.exe , you will see : which shows that the source code in human-readable form is still there.Question : Is there a way to compile code with Cython , such that the claim `` the source code is no longer revealed '' is true ? Note : I 'm looking for a solution where neither the source code nor the bytecode ( .pyc ) is present ( if the bytecode/.pyc is embedded , it 's trivial to recover the source code with uncompyle6 ) PS : I remembered I did the same observation a few years ago but I could not find it anymore , after deeper research here it is : Is it possible to decompile a .dll/.pyd file to extract Python Source Code ?"
"As you 've already understood I 'm a beginner and am trying to understand what the `` Pythonic way '' of writing this function is built on.I know that other threads might include a partial answer to this , but I do n't know what to look for since I do n't understand what is happening here.This line is a code that my friend sent me , to improve my code which is : The `` improved '' version : I wonder : What is happening here ? Is it a better or worse way ? since it is `` Pythonic '' I assume it would n't work with other languages and so perhaps it 's better to get used to the more general way ?"
"How do I compile a C-Python module such that it is local to another ? E.g . if I have a module named `` bar '' and another module named `` mymodule '' , how do I compile `` bar '' so that it imported via `` import mymodule.bar '' ? ( Sorry if this is poorly phrased , I was n't sure what the proper term for it was . ) I tried the following in setup.py , but it does n't seem to work : EditThanks Alex . So this is what I ended up using : with of course a folder named `` mymodule '' containing __init__.py ."
"Not sure how possible this is , but here goes : I 'm trying to write an object with some slightly more subtle behavior - which may or may not be a good idea , I have n't determined that yet.I have this method : Now , when I create an instance of this like so : I get a stacktrace containing my function : I do n't want that - I want the stack trace to read : Is this possible with a minimal amount of effort , or is there kind of a lot required ? I found this answer which indicates that something looks to be possible , though perhaps involved . If there 's an easier way , I 'd love to hear it ! Otherwise I 'll just put that idea on the shelf for now ."
"According to the manual , raw_input writes to stdout . I have this little program ( test_raw_input.py ) : And no matter how I run this : orThe prompt always ends up in xxx . Why is this happening ?"
"Python offers the function globals ( ) to access a dictionary of all global variables . Why is that a function and not a variable ? The following works : What is the rationale behind hiding globals in a function ? And is it better to call it only once and store a reference somewhere or should I call it each time I need it ? IMHO , this is not a duplicate of Reason for globals ( ) in Python ? , because I 'm not asking why globals ( ) exist but rather why it must be a function ( instead of a variable __globals__ ) ."
"I have a regex which works perfectly in Python 2 : this regex will split an expression into 5 parts , for example , etc.But in Python 3 this regex works quite differently , In general , in Python 3 , each character in a part will be split into a separate part , and removed spaces ( including none existing leading and trailing ) will become an empty part ( `` ) and will be added into the part list.I think this Python 3 regex behavior differs QUITE big with Python 2 , could anyone tell me the reason why Python 3 will change this much , and what is the correct regex to split an expression into 5 parts as in Python 2 ?"
"I have created a package called clearplot that wraps around matplotlib . I have also created a nice font that I want to distribute with my package . I consulted this section of the Python Packaging User guide , and determined that I should use the data_files keyword . I chose data_files instead of package_data since I need to install the font in a matplotlib directory that is outside of my package . Here is my first , flawed , attempt at a setup.py file : There are two issues with this setup.py : I attempt to import matplotlib before setup ( ) has a chance to install it . This is an obvious booboo , but I needed to know where mpl_ttf_dir was before I ran setup ( ) .As mentioned here , wheel distributions do not support absolute paths for data_files . I did n't think this would be a problem because I thought I would just use a sdist distribution . ( sdists do allow absolute paths . ) Then I came to find out that pip 7.0 ( and later ) converts all packages to wheel distributions , even if the distribution was originally created as a sdist.I was quite annoyed by issue # 2 , but , since then , I found out that absolute paths are bad because they do not work with virtualenv . Thus , I am now willing to change my approach , but what do I do ? The only idea I have is to distribute the font as package_data first and then move the font to the proper location afterwards using the os module . Is that a kosher method ?"
I divided a ( 512x512 ) 2-dimensional array to 2x2 blocks using this function . Now I need to put the same blocks to their original places after manipulation but I could n't see any function in skimage for that.What 's the best way to merge the non-overlapping arrays as it was before ? Thank you !
"I have this class : The documentation generated by sphinx ( in case it matters , I used the autodoc extension ) looks like this : class package.Class Bases : objectThe inheritance from object is n't useful information for the reader , and therefore I do n't want it in my documentation . The output I 'd like to see is this : class package.ClassIs there a way to exclude object from the list of base classes ?"
How do I convert column B into the transition matrix in python ? Size of the matrix is 19 which is unique values in column B.There are a total of 432 rows in the dataset.The matrix should contain the number of transition between them .
"I am using in python3 the stanford dependency parser to parse a sentence , which returns a dependency graph . It gives an error : I wonder if I could save a dependency graph either with or without pickle ."
I worte this python script which acts as an RPC server by modifying the default RPC example in RabbitMQ tutorial found here . It runs fine in my laptop . But when i run it in an amazon ec2 High CPU Medium Instance with these specs : 1.7 GiB of memory 5 EC2 Compute Units ( 2 virtual cores with 2.5 EC2 Compute Units each ) 350 GB of instance storageIt takes up 100 % CPU . Although my laptop with almost the same config runs this with less than 4 % CPU use.I run this in Ubuntu-12.04 in both my laptop and amazon . Here is my codeHow can i fix this ?
"As described in PEP435 , an enum can be defined this way : And the resulting enum members of Color can be iterated in definition order : Color.red , Color.green , Color.blue.This reminds me of Form in Django , in which fields can be rendered in the order they are declared in a Form subclass . They implemented this by maintaining a field counter , every time a new field is instantiated the counter value get incremented.But in the definition of Color , we do n't have something like a FormField , how can we implement this ?"
I was able to do it in Python and my Python code is : And the output is : How do I do this same thing in Java through HashMap ?
"I am in confusion when trying to understand the order that metaclass creates a class instance . According to this diagram ( source ) , I type the following codes to verify it.However , the result seems not like this follow.Any help will be appreciated ."
"Here is a quote from https : //stackoverflow.com/users/893/greg-hewgill answer to Explain Python 's slice notation . Python is kind to the programmer if there are fewer items than you ask for . For example , if you ask for a [ : -2 ] and a only contains one element , you get an empty list instead of an error . Sometimes you would prefer the error , so you have to be aware that this may happen.So when the error is prefered , what is the Pythonic way to proceed ? Is there a more Pythonic way to rewrite this example ? edit : the example was simpler to write using byte strings but my real code is using lists ."
"I have a text file of lines ( of several GB and ~ 12 millions of lines ) , where each line is a point x , y , z , + accessory info . I wish to read chunk-by-chunk the file , processing the point and split ( following an spatial index based on the position of the points respect a square grid of 0.25 m ) the result in several text file in a temporary folder.where `` ; '' is the delimiter and the first two columns the x and y any useful for give the ID positionthe output result is another text files where for each ID only one point is randomly extracted ex : where the first two columns are the IDthe final output will be ( example ) without the ID valuesI am using a solution from this blogmy code is the following : The main problem of my code is a decreasing of speed when ~ 2 millions of split text files are saved in the temporary folder . I wish to know with to the respect the solution of effbot.org if there is an optimized method to create a buffer ?"
I am trying to learn python with my own and i stucked at __repr__ function . Though i have read lots of post on __repr__ along with the python document . so i have decided to ask this Question here . The code bellow explains my confusion . If a normal function can also perform similar task then what is the extra advantage of __repr__ over print_class ( ) ( in my case a normal function ) function .
"The problem : I 've got a situation with 2 dataframes : What I 'd like to do is merge test2 with test1 where id_A == id_a and id_B == id_b OR where id_A == id_b and id_B == id_a , resulting in the following dataframe : Current Solution : My solution works but seems messy , and I 'd like to see if I 'm overlooking some more clever way to do things . It involves concatenating test2 with itself , but reversing the 2 columns of interest ( id_a becomes id_b and vice-versa ) , and then merging from there.Question : Does anyone know a neater way to do this ? I feel like I 'm probably overlooking some amazing pandorable way of doing things.Thanks for your help !"
I have a code in python to render a few spheres in python that looks like this : The default view of the figure adds distortion based on the camera position ( farther spheres smaller ) . I 'd like to set the to parallel projection ( farther spheres same size ) by some command so it renders like this automatically.I did n't find a straightforward solution with google or the documentation . Thanks !
"I ran into example code that uses a statement with the same variable : I have tried : and all four variables ( a , b , c and d ) all become 10 . Like : This is an Amazon code example so I doubt my understanding of Python rather than the code example . The page can be found here : AWS Kinesis exampleWhat is likely happening here ? Some Python voodoo I do n't understand or just a typo ?"
"I was using asyncio for a project , and encountered this strange behavior.For some reason , storing the resulting task when you call asyncio.async ( ) stops exceptions from doing anything.Could someone shed some light on this situation ? I need to have a way to catch exceptions in my current project ."
"I want to debug a python projectThe problem is , I do n't know where to set a break point , what I want to do , is be able to call a method and have the debugger be fired right awayHow do I do that ? I tried pdb.run ( string_command ) but it does n't seem to work right"
"I am finding it difficult to tackle the following bugs with the program , I would greatly appreciate some input.The goal of the program is to perform a SMTP scan . The user inputs the target IP address , usernames , passwords and numbers of threads to allocate to the scanning process.code is below : Thanks"
"I was playing around with NumPy and Pillow and came across an interesting result that apparently showcases a pattern in NumPy random.random ( ) results . Here a sample of the full code for generating and saving 100 of these images ( with seed 0 ) , the above are the first four images generated by this code.The above are four different images created using PIL.Image.fromarray ( ) on four different NumPy arrays created using numpy.random.random ( ( 256 , 256 , 3 ) ) * 255 to generate a 256 by 256 grid of RGB values in four different Python instances ( the same thing also happens in the same instance ) .I noticed that this only happens ( in my limited testing ) when the width and height of the image is a power of two , I am not sure how to interpret that.Although it may be hard to see due to browser anti-aliasing ( you can download the images and view them in image viewers with no anti-aliasing ) , there are clear purple-brown columns of pixels every 8th column starting from the 3rd column of every image . To make sure , I tested this on 100 different images and they all followed this pattern.What is going on here ? I am guessing that patterns like this are the reason that people always say to use cryptographically secure random number generators when true randomness is required , but is there a concrete explanation behind why this is happening in particular ?"
"For the following pandas dataframeI want to increment the column `` Expected output '' only if `` servo_in_position '' changes from 0 to 1 . I want also to assume `` Expected output '' to be 0 ( null ) if `` servo_in_position '' equals to 0.I tried but it gives output as in `` second_servo_in_position '' column , which is not what I wanted.After that I would like to group and calculate mean using :"
"Is it possible to assign to a numpy array along the lines of how the take functionality works ? E.g . if I have a an array a , a list of indices inds , and a desired axis , I can use take as follows : This is extremely useful when the indices / axis needed may change at runtime.However , numpy does not let you do this : It looks like there is some limited ( 1-D ) support for this via numpy.put , but I was wondering if there was a cleaner way to do this ?"
"How could I use a multidimensional Grouper , in this case another dataframe , as a Grouper for another dataframe ? Can it be done in one step ? My question is essentially regarding how to perform an actual grouping under these circumstances , but to make it more specific , say I want to then transform and take the sum . Consider for example : Then , the expected output would be : Where columns a and b in df1 have been grouped by columns a and b from df2 respectively ."
"There already exist some posts discussing python profiling using cProfile , as well as the challenges of analyzing the output due to the fact that the output file restats from the sample code below is not a plain text file . The snippet below is only a sample from docs.python.org/2/library/profile , and not directly reproducible.There is one discussion here : Profile a python script using cProfile into an external file , and on the docs.python.org there are more details on how to analyze the output using pstats.Stats ( still only a sample , and not reproducible ) : I may be missing some very important details here , but I would really like to store the output in a pandas DataFrame and do further analysis from there . I thought it would be pretty simple since the output in iPython from running cProfile.run ( ) looks fairly tidy : Any suggestions on how to get this into a pandas DataFrame in the same format ?"
I would like to create a scheduler class that uses aiohttp to make API calls . I tried this : but this just results in an error : RuntimeError : Session is closed . A second approach for the __aenter__ function : works well . Is this a good construct ? It does n't adhere to examples of how to use aiohttp . Also wondering why the first approach is n't working ?
"I know that this could be dismissed as opinion-based , but googling is n't finding the resources I was hoping for , and I am looking for links to any established and agreed best practices in the Python community.I am an intermediate Python programmer in an organization with a pretty terrible history of writing obfuscated code in every language ever invented . I would really like to set an example of good programming styles and practices . To that end , I am following PEP 8 , running pylint on everything I write , and thinking deeply about each of its suggestions rather than simply dismissing them . I have broken up longer , complex methods into shorter ones , in part due to its advice . I also write detailed docstrings following this style : http : //sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.htmlOne challenge for me is that , while I am not the only Python programmer in my organization , I seem to be the only one who takes any of this stuff seriously , and my colleagues do n't seem to mind undocumented , repetitive code with naming that does n't follow any particular schema , for example . So I do n't think getting them to review my code or learning from them is my best option.I just got my first `` Too many lines in module '' message from pylint . I am not done writing the module - I wanted to add at least one more class and several methods to the existing classes . I know that the idea is that a module should `` do one thing '' but that `` thing '' is not yet fully implemented.Here are some statistics that pylint gives me : I really do n't think that 266 lines of code is too many for a module . My docstrings are 75 % of the lines in the module - is this standard ? My docstrings are pretty repetitive , since my methods are smallish operations on data . Each docstring will tend to state , for example , that one argument is a pandas dataframe and list the required and optional columns of the dataframe with their meanings , and that is repeated in each method or function that does anything to the dataframe.Is there some sort of systematic error that it seems I might be making here ? Are there guidelines for what to read in order to improve my code ? Are my docstrings too long ? Is there such a thing as a too-long docstring ? Should I simply disable the pylint module-too-long message and get on with my life ?"
"I 'm trying to figure out if there 's a way to nest Sphinx extension directives . I have a few custom node types that are working great , but I 'd like a little more flexibility in the HTML I 'm generating.Here 's an example of what I 'm trying to accomplish : Say I have a directive that creates a textbox and button that triggers some JavaScript . This works fine - I can put this in my reST file : Now , I want the ability to create a tabbed interface with jQuery in the output . I have a node type that creates the raw HTML/JavaScript needed for this to happen.What I would like to be able to do is give the tab node 1 or more instances of myDirective ( or other directives ) , and have it put each instance into another tab . Something like this : Obviously this is a pretty specific case , but this is generally what I 'd like to accomplish - nested directives ."
"How is the sort ( ) working in matlab ? Code in pure matlab : q is an array : After q = sort ( roots ( q ) ) , I got : q = 0.3525 0.3371 - 0.1564i 0.3371 + 0.1564i 0.2694 - 0.3547i 0.2694 + 0.3547i 1.3579 - 1.7880i 1.3579 + 1.7880i 2.4410 - 1.1324i 2.4410 + 1.1324i 2.8365Well , seems to work fine ! Then in python , I use ( q is the same as above , it is an np.array ) : And I got : Well ... These two results seem different in that they sort differently , so what are the reasons ? did I make something wrong ? thank you in advance ! My answer : Then call it in the python code , works fine : p"
"I would like to implement an XorShift PRNG in both Java , Python and JavaScript . The different implementations must generate the exact same sequences given the same seed . So far , I 've have not been able to do this.My implementation in Javahave the following implementation of an XorShift PRNG in Java ( where x is a long field ) : If I seed x to 1 , the first four calls to randomLong ( ) will generate : My implementation in PythonI have tried both with and without numpy . Below is the version that uses numpy.With the same seed , the Python function will generate : My JavaScript implementationI 've not attempted one yet , since JavaScript 's only number type seems to be doubles based on IEEE 754 , which opens up a different can of worms.What I think the cause isJava and Python have different number types . Java has 32 and 64-bit integers , while Python has funky big int types.It seems that the shift operators have different semantics . For example , in Java there is both logical and arithmetic shift , while in Python there is only one type of shift ( logical ? ) .QuestionsI would be happy with an answer that lets me write a PRNG in these three languages , and one that is fast . It does not have to be very good . I have considered porting C libs implementations to the other languages , although it is not very good.Can I fix my above implementations so they work ? Should I switch to another PRNG function that is easier to implement across prog.langs ? I have read the SO where someone suggested using the java.util.Random class for Python . I do n't want this , since I 'm also going to need the function in JavaScript , and I do n't know that this packages exists there ."
"I am writing a program in Python , that reads the YV12 Frame data from a IP Camera produced by Hikvision Ltd.In the SDK , they provided a functional call that allows me to setup a Callback to retrieve the Frame Data.My Callback function is like this : $ The structure of pFramInfo is defined like this : $ I use the following code to setup the callback function : Then the callback function is being called by the SDK and it prints out the following : Simple types like c_long ( lPort and lSize ) are being read correctly , but the pFrameInfo Structure does not have the fields that I have defined . I can not read pFrameInfo.nWidth as it saids there is no such attribute ... My question is : How can I read the attributes in the structure returned from the dll through ctypes . I can not see any examples on google doing this , and I found a topic in python.org http : //bugs.python.org/issue5710 saying this just can not be done , but the message is written in 2009 . I believe that as pFrameInfo is already being read from the dll , is there any way that I can get back the c_long data stored in the pFrameInfo Structure by cutting and reading the bytes in the memory space of pFrameInfo ? Since all attributes of pFrameInfo are c_long , may be reading the structure byte by byte can reconstruct the value of the c_long variables . I am just guessing . Please help , this is a really big problem for me ..."
"I have a vector class : which works fine , i.e . I can write : However if the order of operation is reversed , then it fails : I understand the error : the addition of an int object to an Vector object is undefined because I have n't defined it in the int class . Is there a way to work around this without defining it in the int ( or parent of int ) class ?"
"In Python , I 'd like to be able to create a function that behaves both as a class function and an instance method , but with the ability to change behaviors . The use case for this is for a set of serializable objects and types . As an example : I know that given the definition of classmethod ( ) in funcobject.c in the Python source , this looks like it 'd be simple with a C module . Is there a way to do this from within python ? Thanks ! With the hint of descriptors , I was able to do it with the following code : Thank you Alex !"
"By default , plotting a set of points ( or whatever ) in 3D with matplotlib , locates the z axis vertically , as seen here ( code below ) : I need to interchange the z and y axis , so that that y axis is shown vertically.I 've looked around but found no way to tell matplotlib to do this.Add : I do not want to have to resort to a hack where I interchange the data and labels . This is a simple 3 points 3D plot , but I have to plot much more complicated surfaces . I 'm looking for a general solution , not just something that works with scatter plots . A simple way to tell matplotlib to put the y axis vertically instead of the z axis is the clean way to do it.MWE"
I have looked for ages and have found no actual answers because I ca n't see any that start with an ascending key and then by a descending value.An example to make it clearer : After doing some research I arrived at something like : This is because it 's reverse-sorting both the value and the key . I need the key to be un-reversed.I 'd really appreciate some help here . Thanks in advance !
"I am using docopt module to handle python script options : Traceback ( most recent call last ) : File '' /home/ajn/Converter-yaml-to-html-blocks/convert.py '' , line 66 , inargs = docopt ( doc , version='v0.1 ' ) File `` /usr/local/lib/python3.4/dist-packages/docopt.py '' , line 558 , indocoptDocoptExit.usage = printable_usage ( doc ) File `` /usr/local/lib/python3.4/dist-packages/docopt.py '' , line 466 , inprintable_usageusage_split = re.split ( r ' ( [ Uu ] [ Ss ] [ Aa ] [ Gg ] [ Ee ] : ) ' , doc ) File `` /usr/lib/python3.4/re.py '' , line 196 , in splitreturn _compile ( pattern , flags ) .split ( string , maxsplit ) TypeError : expected string or bufferAny hint ?"
"I 'm a geologist and have a bunch of boreholes of varying depth.I 've crudely set the number , width and height of the subplots to vary according to the number of boreholes and the number of samples in those boreholes.In each borehole there is a zone i 'd like to highlight , which I 've done with axhspan.What i 'd like to do is correlate between the boreholes ( the subplots ) , drawing a line connecting the top and bottom of all the zoned areas across all the boreholes.I 've tried using annotate but have n't progressed very far.I 'm not really sure how to approach this and would appreciate any advice . Here is some example code , and a pic of what it might produce :"
"I am trying to create a 'mask ' of a numpy.array by specifying certain criteria . Python even has nice syntax for something like this : But if I have a list of criteria instead of a range : I ca n't do this : I have to do something based on list comprehensions , like this : Which is correct.Now , the problem is that I am working with large arrays and the above code is very slow . Is there a more natural way of doing this operation that might speed it up ? EDIT : I was able to get a small speedup by making crit into a set.EDIT2 : For those who are interested : Jouni 's approach:1000 loops , best of 3 : 102 µs per loopnumpy.in1d:1000 loops , best of 3 : 1.33 ms per loopEDIT3 : Just tested again with B = randint ( 10 , size=100 ) Jouni 's approach:1000 loops , best of 3 : 2.96 ms per loopnumpy.in1d:1000 loops , best of 3 : 1.34 ms per loopConclusion : Use numpy.in1d ( ) unless B is very small ."
"Problem : When exceptions are raised in slots , invoked by signals , they do not seem to propagate as usual through Pythons call stack . In the example code below invoking : on_raise_without_signal ( ) : Will handle the exception as expected.on_raise_with_signal ( ) : Will print the exception and then unexpectedly print the success message from the else block.Question : What is the reason behind the exception being handled surprisingly when raised in a slot ? Is it some implementation detail/limitation of the PySide Qt wrapping of signals/slots ? Is there something to read about in the docs ? PS : I initially came across that topic when I got surprising results upon using try/except/else/finally when implementing a QAbstractTableModels virtual methods insertRows ( ) and removeRows ( ) ."
"For example , this code is Python : throws AttributeError : 'object ' object has no attribute ' b'But , this piece of code : is just fine . Why can I assign property b , when class x does not have that property ? How can I make my classes have only properties defined ?"
"I 'm using google app engine , and am having trouble writing querys to filter ReferenceProperties.eg.And I have tried writing something like this : and various other things that do n't work . Hopefully someone can give me a prod in the right direction ..."
"Generating some random Gaussian coordinates , I noticed the TSP-solver returns horrible solutions , however it also returns the same horrible solution over and over again for the same input.Given this code : For example , for 10 points I get a nice solution : For 20 It 's worse , some obvious optimizations still exist ( where one only would need to swap two points.And for 200 it 's horrible : I 'm wondering whether the code above actually does some LNS , or just returns the initial value , especially since most first_solution_strategy options suggest deterministic initialization.Why does the TSP-solver above return consistent solutions for the same data , even though tabu-search and simulated annealing and such are stochastic . And , why is the 200-point solution so bad ? I played with several options in SearchParameters , especially enabling 'use_ ... ' fields in local_search_operators . This had no effect , the same very sub-optimal solutions were returned ."
"I am using the python standard logging configuration , combined with yaml to load a config file with the dictConfig ( ) function : Since an incremental logging configuration in python limits the capabilities , every logging file has to contain a minimum amount of information , something like this : Which either forces one to learn this by head , store it somewhere , or look it up each time . Since neither of these really work for me , i would like to find a way to generate it . This brings me to the question : Is there a way to get the current ( or basic ) logging config as a dictionary ? This would make it easy to make an initial config file by running the following code once , and just remove/edit what you want : Yaml is just my personal preference of course , the same question could be posted for something like JSON ."
Consider the following pseudo code : How to implement this in easiest way ?
"I 'm seeing weird issues when trying to use tf.data ( ) to generate data in batches with keras api . It keeps throwing errors saying it 's running out of training_data.TensorFlow 2.1After creating the dataset , I 'm shuffling and adding the repeat parameter to the num_of_epochs , i.e . 50 in this case.This works , but it crashes after the 3rd epoch , and I ca n't seem to figure out what I 'm doing wrong in this particular instance . Am I supossed to declare the repeat and shuffle statements at the top of the pipeline ? Here is the error : Update : So model.fit ( ) should be supplied with model.fit ( x=data , y=labels ) , when using tf.data ( ) because of a weird problem.This removes the list out of index error.And now I 'm back to my original error.However it looks like this could be a tensorflow problem : https : //github.com/tensorflow/tensorflow/issues/32So when I increase the batch size from 6 to higher numbers , and decrease the steps_per_epoch , it goes through more epochs without throwing the StartAbort : Out of range errorsUpdate2 : As per @ jkjung13 suggestion , model.fit ( ) takes one parameter when using a dataset , model.fit ( x=batch ) . This is the correct implementation.But , you are supposed to supply the dataset instead of an iterable object if you 're only using the x parameter in model.fit ( ) .So , it should be : model.fit ( dataset , epochs=50 , steps_per_epoch=46 , validation_data= ( v , v_labels ) ) And with that I get a new error : GitHub IssueNow to overcome this , I 'm converting the dataset to a numpy_iterator ( ) : model.fit ( dataset.as_numpy_iterator ( ) , epochs=50 , steps_per_epoch=46 , validation_data= ( v , v_labels ) ) This solves the problem , however , the performance is appaling , similar to the old keras model.fit_generator without multiprocessing . So this defeats the whole purpose of 'tf.data ' ."
"Example : BeautifullSoup codein exit we have : Lxml code : in exit we have : because lxml think `` Text2 '' it 's a tail of < b > < /b > If we need only text line from join of tags we can use : But , how do that without changing text , but remove tags without tail ?"
"I have a middleware that will [ raise IgnoreRequests ( ) ] if url contains `` https '' .Is there a way to completely prevent scrapy from performing a GET request to the HTTPS url ? I get the same values for response_bytes/response_count without [ IgnoreRequests ( ) ] and with it my code snippet . I 'm looking for zero values and skip crawling the url . I do n't want scrapy to crawl/download all the bytes from the https page , just move on to the next url . Notes : MUST be a middleware , do not want to use rules embedded in spider . Have hundreds of spiders and want to consolidate the logic ."
"I would like to create a list of dicts from a list of keys and with the same value for each dict.The structure would be : I begin with a list of keys : Let 's say the value would be [ 'dog ' , 'cat ' ] .Here is what the final result should look like :"
PyCharm 's code editor shows the popup message Template file 'index.html ' not found when passing `` index.html '' to render_template . The template templates/index.html exists . Accessing http : //localhost:5000/ renders the template . How do I tell PyCharm that the template exists ?
"I have a dictionary containing some floating-point values.When I print the dictionary as is , I get these values printed in full accuracy.However , when I attempt to print each value separately , I get them truncated.A simple coding example to illustrate the problem : Now , of course I can solve this using format , i.e. , in the example above : However , I wish to avoid this kind of solution.First and foremost , because I do not know the exact precision required for each entry in the dictionary . Second , because I prefer to avoid the type of cumbersome coding , which involves specifying print format of every field to begin with.In other words , I want to do something like : Or : And get the value of d [ ' x ' ] printed in its full precision.Why does the Python interpreter chooses different print formats for d and for d [ ' x ' ] , and what approach can I take here in order to achieve my purpose ?"
"I 'm trying to use channels ( v2.1.7 ) in django to send messages from server to client . When i execute the celery task below , my message is not being fetched in consumers.py ( so not being sent to client ) and surprisingly no error occures.I 'm able to send message from consumers to client directly . But i could n't manage to send from outside of consumers using async_to_sync ( ) . ( I tried to use async_to_sync method in standard django views.py and i had same problem ) wololo/tasks.pywololo/consumers.pythe result that i have in celery terminal click to see celery terminalThanks in advance"
"I am facing a strange performance issue when parsing a lot of dates with Pandas 0.17.1 . For demonstration , I created CSV files with exactly one column , containing datetimes in the format `` 2015-12-31 13:01:01 '' . The sample files contain 10k , 100k , 1M , and 10M records . I am parsing it like this : The elapsed times are : 10k : 0.011 s 100k : 0.10 s 1m : 1.2 s 10m : 300 s You see , the time scales linearly to the number of records until 1 million , but then there is a huge drop.It 's not a memory issue . I have 16GB , and I work with dataframes of this size without any problems in Pandas , only parsing of dates appears to be slow.I tried to use infer_datetime_format=True , but the speed was similar . Also a huge drop for 10m records.Then I tried to register my own naive date parser : And the times are now : 10k : 0.045 s 100k : 0.36 s 1m : 3.7 s 10m : 36 s The routine is slower than the default pandas parser on smaller files , but it scales perfectly linearly for the larger one . So it really looks like some kind of performance leak in the standard date parsing routine.Well , I could use my parser , but it 's very simple , stupid , and apparently slow . I would prefer to use the intelligent , robust , and fast Pandas parser , only if I could somehow solve the scalability issue . Would anyone have any idea , if it could be solved , possibly by some esoteric parameter or something ? UPDATEThank all of you for your help so far.After all , it seems that there is a reproduceable performance problem with dates parsing , but it has nothing to do with scalability . I was wrong in my original analysis . You can try to download this filehttps : //www.dropbox.com/s/c5m21s1uif329f1/slow.csv.tar.gz ? dl=0and parse it in Pandas . The format and everything is correct , all the data are valid . There are only 100k records , but it takes 3 seconds to parse them - while it takes 0.1s to parse 100k records from the generated regular sequence . What happened : I did not generate my original testing data as a regular sequence , as @ exp1orer did . I was taking samples of our real data , and their distribution is not that regular . The sequence is overall growing by a constant pace , but there are some local irregularities and unordered pieces . And , apparently , in my 10M sample , there happened to be one section , which made pandas particularly unhappy and parsing took so long . It 's only a tiny fraction of the file content that is responsible for all the slowness . But I was not able to spot any principal differences between that fraction and the rest of the file.UPDATE 2So , the cause of slowness was that there were some weird dates , like 20124-10-20 . Apparently , I will need to do some more pre-processing before importing the data to Pandas ."
"Is there a similar mechanism in Python , to the effect set -x has on bash ? Here 's some example output from bash in this mode : I 'm aware of the Python trace module , however its output seems to be extremely verbose , and not high level like that of bash ."
"I am writing a Python wrapper class for a C # API accessed through Pythonnet.As I want to extend the API with my own methods I decided to wrap it using the composition approach outlined here : The C # API makes heavy use of properties which I want to mimic in my Python code . The following minimal example shows my current approach for the example of the C # Surface class with the two properties width and height : In total , I have to deal with about 50 properties . For several groups of properties I want to add my own error checks , type conversions , etc.What I am looking for is a Pythonic way of defining the properties , e.g . through a factory or using descriptors . Thanks for your help ! Edit : I want to be able to use tab completion within a python shell , i.e . surface . { hit tab } should propose surface.width and surface.height . This does not seem to be possible with the getattr approach outlined by Greg ."
"After a thorough search I could find how to delete all characters before a specific letter but not before any letter.I am trying to turn a string from this : To this : I have tried the following code , but strings such as the first example still appear.Not only does this fail to delete the double quote in the example for some unknown reason but also would n't work to delete the leading whitespace as it would delete all of the whitespace.Thank you in advance ."
Imagine I have a dataframe that looks like : As you can see this is panel data with multiple entries on the same date for different IDs . What I want to do is fill in missing dates for each ID . You can see that for ID `` 1 '' there is a jump in months between the second and third entry.I would like a dataframe that looks like : I have no idea how to do this since I can not index by date since there are duplicate dates .
"ContextI am trying to get an encryption status for all of my buckets for a security report . However , since encryption is on a key level basis , I want to iterate through all of the keys and get a general encryption status . For example , `` yes '' is all keys are encrypted , `` no '' if none are encrypted , and `` partially '' is some are encrypted.I must use boto3 because there is a known issue with boto where the encryption status for each key always returns None . See here.ProblemI am trying to iterate over all the keys in each of my buckets using boto3 . The following code works fine until it runs into buckets with names that contain periods , such as `` my.test.bucket '' .When it hits a bucket with a period in the name , it throws this exception when bucket.objects.all ( ) is called , telling me to send all requests to a specific endpoint . This endpoint can be found in the exception object that is thrown.Things I have triedSetting the endpoint_url paramter to the bucket endpoint specified in the exception response like s3_resource = session.resource ( 's3 ' , endpoint_url='my.test.bucket.s3.amazonaws.com ' ) Specifying the region the bucket is located in like s3_resource = session.resource ( 's3 ' , region_name='eu-west-1 ' ) I believe the problem is similar to this stackoverflow question in boto , which fixes the problem by setting the calling_format parameter in the s3Connection constructor . Unfortunately , I ca n't use boto though ( see above ) .UpdateHere is what ended up working for me . It is not the most elegant approach , but it works = ) ."
"I 'm on Python 2.7 . I have a dataframe with 200 columns and need to drop a few.I can use the below to drop the last n columns . How do I write it so i can drop the first 10 , then column 22 , then 26 , 10th from the last , and last 5 . All in one line ."
I 'm trying to insert multiple values into my postgres database using Pony ORM . My current approach is very inefficient : This is just a short example but the structure of the input is the same : a list of lists.I 'm extracting the data from several xml files and insert one `` file '' at a time.Does anyone has an idea on how to put several rows of data into one insert query in Pony ORM ?
"I am trying to figure out the best way ( fast ) to extract entities , e.g . a month . I have come up with 5 different approaches using spaCy.Initial setupFor each solution I start with an initial setupSolution : using extension attributes ( limited to single token matching ) 76.4 µs ± 169 ns per loop ( mean ± std . dev . of 7 runs , 10000 loops each ) Solution : using phrase matcher via entity ruler 131 µs ± 579 ns per loop ( mean ± std . dev . of 7 runs , 10000 loops each ) Solution : using token matcher via entity ruler 72.6 µs ± 76.7 ns per loop ( mean ± std . dev . of 7 runs , 10000 loops each ) Solution : using phrase matcher directly 115 µs ± 537 ns per loop ( mean ± std . dev . of 7 runs , 10000 loops each ) Solution : using token matcher directly 55.5 µs ± 459 ns per loop ( mean ± std . dev . of 7 runs , 10000 loops each ) ConclusionThe custom attributes with is limited to single token matching and the token matcher seems to be faster so that seems to be preferable . The EntityRuler seems to be the slowest which is n't surprising since it is changing the Doc.ents . It is however quite convenient that you have your matches in Doc.ents so you might want to consider this method still . I was quite surprised that the token matcher outperforms the phrase matcher . I thought it would be opposite : If you need to match large terminology lists , you can also use the PhraseMatcher and create Doc objects instead of token patterns , which is much more efficient overallQuestionAm I missing something important here or can I trust this analysis on a larger scale ?"
"When I generate LLVM IR Code from C++ , I can use the console command clang++ -emit-llvm –S test.cpp to get a test.ll file which is the LLVM IR I want . To get an executable these are the steps to follow : llvm-as test.ll - > gives me the test.bc file.llc test.bc -- o test.s - > gives me the test.s file.clang++ test.s -o test.native - > gives me a native file that i can execute . For C++ this works just fine . In theory , should the same steps apply when I write Rust or Python Code ? I take my Rust code and get the LLVM IR by typing rustc test.rs -- emit llvm-ir . This gives me the test.ll file again . For Python , I use `` Numba '' and get the LLVM IR by typing numba -- dump-llvm test.py > test.llwhich also gives me the test.ll file.The steps to generate an executable from those .ll files should be the same . They work up until the last step that creates the native executable : Python Error Rust Error What I get from this is that clang does n't understand the Rust/Python specific parts of the LLVM IR file ( e.g . `` PyObject '' in Python or `` panic '' from Rust ) that were used to generate the .bc , .s and in theory the .native files . But why are those even in the IR in the first place ? Should n't the LLVM IR be uniform and those parts be transformed so the LLVM toolchain can work with them ? As far as I know LLVMs modularity should allow those steps by using LLVM IR . Is there maybe another way to do this i do n't know about ? Can I generate the IRs from those languages in some other way that gives `` pure '' LLVM IR that clang understands , or could I still generate executables from those files , but in some other way without clang ?"
"I have two dataframes A , B with NxM shape . I want to multiply both such that each element of A is multiplied with respective element of B. I searched but could n't get exactly the solution ."
"I have a list What is the fastest way to check if any list in a is present in another list of lists b , where If any list in a is present in b , I would like to remove it . I 'm currently using this code : This works but is quite slow when working with large lists so was wondering if there 's a better way . The above code gives me the following result : I am not worried about order , for example if a list in ais [ 1,2,3,4,5,6 ] I want it to be removed if there exist a list [ 1,2,3,4,5,6 ] or [ 3,4,1,6,2,5 ] etc in list b ."
I was going through some tutorial about the sentiment analysis using lstm network.The below code said that its stacks up the lstm output . I Do n't know how it works .
"I am trying to run a Tensorflow model on my Android application , but the same trained model gives different results ( wrong inference ) compared to when it is run on Python on desktop.The model is a simple sequential CNN to recognize characters , much like this number plate recognition network , minus the windowing , as my model has the characters already cropped into place.I have : Model saved in protobuf ( .pb ) file - modeled and trained in Keras on Python/Linux + GPUThe inference was tested on a different computer on pure Tensorflow , to make sure Keras was not the culprit . Here , the results were as expected.Tensorflow 1.3.0 is being used on Python and Android . Installed from PIP on Python and jcenter on Android.The results on Android do not resemble the expected outcome.The input is a 129*45 RGB image , so a 129*45*3 array , and the output is a 4*36 array ( representing 4 characters from 0-9 and a-z ) .I used this code to save the Keras model as a .pb file.Python code , this works as expected : Android code , based on this example ; this gives seemingly random results : Any help is greatly appreciated !"
I am new to using numpy and one thing that I really do n't understand is indexing arrays.In the tentative tutorial there is this example : I have no idea why it does that last thing . Can anyone explain that to me ? Thanks !
"I switched from matplotlib to plotly mainly to plot smooth animations in 2D/3D . I want to plot the motion of robots consisting of multiple circles/spheres.The different body parts of the robot have different sizes and the circles need to represent that accurately.Is there a way in plotly to specify the size of the markers in data units ? For example I want to draw a 5m x 5m ( x5m ) section in which circles/spheres with radius 0.1m are moving along different trajectories.In matplotlib I know two alternatives . One is to use patches ( matplotlib.patches.Circle ) . The second option is plot the points and to scale their marker_size correctly by taking the dpi into account ( See the answers to this question for matplotlib ) .A ) Is it possible to animate shapes in plotly ( 3D ) ? orB ) Is there a way to specify the size of the markers in data units or scale the sizeref attribute correctly ? A problem with scaling the marker_size is that the image is only correct as long as you do n't zoom in or out , because the marker_size stays constant.Therefore the cleaner approach would be to specify the size of the circles directly in data units ."
"I want to construct a value in a list comprehension , but also filter on that value . For example : I want to avoid calling expensive_function twice per iteration.The generator may return an infinite series , and list comprehensions are n't lazily evaluated . So this would n't work : I could write this another way , but it feels right for a list comprehension and I 'm sure this is a common usage pattern ( possible or not ! ) ."
"I have a dataframe with all categorical columns which i am encoding using a oneHotEncoder from sklearn.preprocessing . My code is as below : As seen inside the OneHotEncoder the handle_unknown parameter takes either error or ignore . I want to know if there is a way to selectively ignore unknown categories for certain columns whereas give error for the other columns ? In this case for all the columns that is Country , Fruits and Flowers if there is a new value which comes the model would still be able to predict an output . I want to know if there is a way to ignore unknown categories for Fruits and Flowers but however raise an error for unknown value in Country column ?"
"I recently asked a question regarding missing values in pandas here and was directed to a github issue . After reading through that page and the missing data documentation.I am wondering why merge and join treat NaNs as a match when `` they do n't compare equal '' : np.nan ! = np.nanHowever , NaNs in groupby are excluded : Of course you can dropna ( ) or df [ df [ 'col1 ' ] .notnull ( ) ] but I am curious as to why NaNs are excluded in some pandas operations like groupby and not others like merge , join , update , and map ? Essentially , as I asked above , why does merge and join match on np.nan when they do not compare equal ?"
"During web scraping and after getting rid of all html tags , I got the black telephone character \u260e in unicode ( ☎ ) . But unlike this response I do want to get rid of it too.I used the following regular expressions in Scrapy to eliminate html tags : Then I tried to match \u260e and I think I got caught by the backslash plague . I tried unsuccessfully this patterns : None of this worked and I still have \u260e as an output.How can I make this disappear ?"
"Good day , I 'm writing a Python module for some numeric work . Since there 's a lot of stuff going on , I 've been spending the last few days optimizing code to improve calculations times.However , I have a question concerning Numba.Basically , I have a class with some fields which are numpy arrays , which I initialize in the following way : So , the code is vectorized , and using Numba 's JIT results in some improvement . However , sometimes I need to access the calculate_vibr_energy function from outside the class , and pass a single integer instead of an array in place of i.As far as I understand , if I use Numba 's JIT on the calculate_vibr_energy , it will have to always take an array as an argument.So , which of the following options is better:1 ) Create a new function calculate_vibr_energy_single ( i ) , which will only take a single integer number , and use Numba on it too2 ) Replace all usages of the function that are similar to this one : with this : Or are there other , more efficient ( or at least , more Python-ic ) ways of doing that ?"
"UPDATED QUESTION FOR CLARITY : suppose I have 2 processing generator functions : I can chain them with itertoolsand then I can create another generator function object with it , or simply if I just want to next ( mix ) , it 's there.My question is , how can I do the equivalent in asynchronous code ? Because I need it to : return in yield ( one by one ) , or with next iteratorthe fastest resolved yield first ( async ) PREV . UPDATE : After experimenting and researching , I found aiostream library which states as async version of itertools , so what I did : but I still ca n't do next ( a_mix ) or next ( await a_mix ) Although I still can make it into a list : so one goal is completed , one more to go : return in yield ( one by one ) , or with next iterator- the fastest resolved yield first ( async )"
"THE QUESTIONAll of my app 's routes are defined via flask-restful Resources.How can I find the resource object/class that is processing current request ? WHY I WANT THISI wanted to log all exceptions raised while processing requests . I connect to flask.got_request_exception , as described in http : //flask.pocoo.org/docs/1.0/api/ # signals and something like this works well : The only problem is that I want to log some of the request data , but not all the data - e.g . I 'd like to hide passwords . I think it would be a good idea to have logging-data-logic together with request processing logic , like this : and than use it in my log_exception : But maybe this should be done other way ?"
"I have a method that generates a PDF file using Reportlab library : when I call that method , it opens a save dialog , where I can specify where the file should be saved.How shoud I do to save the generated pdf file in the Datastore ? Thanks in advance !"
"I 'm trying to use the multiprocessing module in python 2.6 , but apparently there is something I do not understand . I would expect the class below to add up the numbers sent to it by add ( ) and return the sum in the get_result ( ) method . The code below prints `` 0 '' , I 'd like it to print `` 2 '' . What have I missed ? PS . This problem has been solved . Thanks for the answers ! Just to make it easier for any readers , here 's the complete working version :"
"This is an algorithmic problem . To keep it simple , say I have two doubles , A and B. I want to construct a function that will give me the difference until the next multiple of A or the next multiple of B , if that makes sense.For instance , say A is 3 and B is 5.Consider the multiples : ( 3,6,9,12,15 ) and ( 5,10,15 ) .I 'd want the function to output : ( 3 , 2 , 1 , 3 , 1 , 2 , 3 ) , since it takes 3 units to get to 3 , then 2 more to get to 5 , then 1 to 6 , then 3 to 9 , etc ... I hope this makes sense . Ideally it 's a Python-esque generator ( although I 'm writing this in Arduino ~ C++ ) . I need it to be fast - really fast.Any help would really be appreciated . My pseudocode is below , but it 's not that great.EDIT : How would this look with multiple values ? Not just a and b , but also c , d , e , etc..I 'd imagine I 'd just do some more if statements , but the cost is higher ( more operations per branch ) ."
"When a missing key is queried in a defaultdict object , the key is automatically added to the dictionary : However , often we want to only add keys when they are assigned explicitly or implicitly : One use case is simple counting , to avoid the higher overhead of collections.Counter , but this feature may also be desirable generally.Counter example [ pardon the pun ] This is the functionality I want : But Counter is significantly slower than defaultdict ( int ) . I find the performance hit usually ~2x slower vs defaultdict ( int ) .In addition , obviously Counter is only comparable to int argument in defaultdict , while defaultdict can take list , set , etc.Is there a way to implement the above behaviour efficiently ; for instance , by subclassing defaultdict ? Benchmarking exampleTest code : Note Regarding Bounty Comment : @ Aran-Fey 's solution has been updated since Bounty was offered , so please disregard the Bounty comment ."
"For example I have the following code : After d is created , will the stream opened in the list comprehension get closed automatically ?"
"Given a 4D array M : ( m , n , r , r ) , how can I sum all the m * n inner matrices ( of shape ( r , r ) ) to get a new matrix of shape ( r * r ) ? For example , I expect the result should be"
"In the tornado.web module there is a function called _time_independent_equals : It is used to compare secure cookie signatures , and thus the name.But regarding the implementation of this function , is it just a complex way to say a==b ?"
"I made a LSTM ( RNN ) neural network with supervised learning for data stock prediction . The problem is why it predicts wrong on its own training data ? ( note : reproducible example below ) I created simple model to predict next 5 days stock price : The correct results are in y_test ( 5 values ) , so model trains , looking back 90 previous days and then restore weights from best ( val_loss=0.0030 ) result with patience=3 : Prediction result is pretty awesome , is n't it ? That 's because algorithm restored best weights from # 5 epoch . Okey , let 's now save this model to .h5 file , move back -10 days and predict last 5 days ( at first example we made model and validate on 17-23 April including day off weekends , now let 's test on 2-8 April ) . Result : It shows absolutely wrong direction . As we see that 's because model was trained and took # 5 epoch best for validation set on 17-23 April , but not on 2-8 . If I try train more , playing with what epoch to choose , whatever I do , there are always a lot of time intervals in the past that have wrong prediction.Why does model show wrong results on its own trained data ? I trained data , it must remember how to predict data on this piece of set , but predicts wrong . What I also tried : Use large data sets with 50k+ rows , 20 years stock prices , adding more or less featuresCreate different types of model , like adding more hidden layers , different batch_sizes , different layers activations , dropouts , batchnormalizationCreate custom EarlyStopping callback , get average val_loss from many validation data sets and choose the bestMaybe I miss something ? What can I improve ? Here is very simple and reproducible example . yfinance downloads S & P 500 stock data ."
"I 'm new to django and bokeh.I was trying to render a simple bokeh plot supported by a few select options that essentially allow me to tweak my plot 's content in a django web application.The plots are rendered when the script and div elements obtained from the bokeh.embed.components ( ) are passed to the template as context variables . The same did n't work when i had a widget and a plot held in a bokeh.io.vform object . I get the output right when i perform a bokeh.io.show ( ) , by specifying the bokeh.plotting.output_file ( ) , but I 'm trying to get this running in my web application.Am I missing anything ? Or is there any other approach that serves my intent ? my code to just render a bokeh widget is as follows : views.py test.htmlI would expect a select form element to be rendered when test.html is launched from test ( ) in django . but does n't happen ."
"I 've been using scipy.optimize.minimize ( docs ) and noticed some strange behavior when I define a problem with impossible to satisfy constraints . Here 's an example : Resulting output : There is no solution to this problem that satisfies the constraints , however , minimize ( ) returns successfully using the initial condition as the optimal solution.Is this behavior intended ? If so , is there a way to force failure if the optimal solution does n't satisfy the constraints ?"
"I 'm having difficulty getting Google Cloud Endpoints working . I have an Python endpoints project running on GAE and it works perfectly using the api explorer . However I 'm struggling to properly generate the client library and use it in my android app . I 've tried a number of sample projects and have the same problem every time ; I ca n't import and use the model classes from the client libraries.Here 's what I 'm doing ( for this example I 'll use the helloworld api python sample at https : //github.com/GoogleCloudPlatform/appengine-endpoints-helloendpoints-python ) Unzip the sample codeGenerate the client library by navigating to folder and running Unzip the generated folder and copy into root of project in eclipseIn Eclipse add `` your_app_id_appspot_com-helloworld-v1-20140310110152-java-1.17.0-rc-sources.jar '' to build path ( Right click the JAR > Build Path > Add to Build Path ) At this stage , I can import com.appspot.your_app_id.helloworld.model . *but I can not import com.appspot.your_app_id.helloworld.model.GreetingCan anyone shed any light on what 's happening here ? I have tried many different ways to get this to work but have the same problem every time . Many thanks , Tom"
I am developing a long-running python script which makes many connections to different serial ports . The script crashes a few hours into its execution citing `` Too many open files '' . I have tracked the issue to the serial module where the .close ( ) method does not seem to reduce the number of file descriptors python is using . I am checking this using lsof | grep python | wc . Using Debian 7.2 & Python 2.7.3The example below slowly uses up more and more file descriptors until it hits the limit . Why is this and how can I avoid it ? ? Thanks
"The DRF documentation provides clear instructions on how to create a custom permission , offering the following code sample : By default this gives the following response when the permission check function return False . HTTP 403 FORBIDDEN Content-Type : application/json Vary : Accept Allow : GET , POST , HEAD , OPTIONS { `` detail '' : `` You do not have permission to perform this action . '' } I would like to change the `` detail '' section above , offering a more developer-friendly error message . How could I do this , ensuring the message is presented each time the permission check fails ?"
"This question has been SOLVED by myself after better looking at it . Please read my answer below.I am getting a `` The fields option must be a list or tuple . Got str . '' when running my Django app.Running exactly the same code with the debugger , and if I have a breakpoint in the line of the error , then it wo n't fail and what should be a tuple seems to be a tuple.The problem seems to be located in the following code inside a DRF ModelSerializer : In the views.py I just do : ... And this is the traceback : If I remove the print ( `` fields to be included : `` , self.fields ) then I got the same error but it will point to the line serializer = serializers.ChHiveLevel1Serializer ( hives , fields=fields , many=True ) in the views.pyOk , I am still a noob in Python and I could be doing something very wrong , but what I ca n't understand is why if I insert a breakpoint in the print I 've just mentioned , and I do the same api request with the debug on , then the code just works : I get my response just as I wanted , and it does n't give any error ( if I remove the breakpoint it will again give the error even if launching with the debugger ) .Do you guys have any idea of what could be wrong ? Thanks a lot in advance . Ask me for any extra info if you need it ! EDIT : Further explanations : The whole serializer is this : I know its confusing to have 3 different 'fields ' so I clarify this : in the views I am passing a param 'fields ' to the serializer , this is a tuple that contain names of fields to be dynamically removed from the serializer . Then inside the init I take pop out this param ( so it is not sent to the superclass ) and assign it the local tuple 'fields ' . Finally , inside the 'if fields is not None ' I am removing from self.fields ( these are the fields defined in the serializer ) the fields with name matching those inside the local tuple . I hope I could explain it better now.Here is a video showing how when debugging and stopping at the breakpoint it just works : http : //youtu.be/RImEMebBGLY"
"In Python , I have a a variable var of type gdb.Value that corresponds to a C++ struct.The struct has a method void foo ( ) . I can evaluate this expression var [ 'foo ' ] . But var [ 'foo ' ] \ ( ) will complain sayingI believe the value type will be gdb.TYPE_CODE_METHOD ( not sure , but var [ 'foo ' ] .type.code returns 16 ) in my case.So I guess the question is : Does python API support calls to class methods , and if not , is there a workaround ? Thanks !"
I have this DataFrame : df : And I would like to make it look like this : df : I have tried different solutions combining .groupby ( 'sensor ' ) and .unstack ( ) but not successful . Any good suggestions ? Thanks !
"How can you make your local Django development server think it 's running inside your AWS network using SSH tunneling ? My scenario , I 'm running a local Django server i.e . python manage.py runserver and Redis as cache backend ( Elasticache ) . When my app runs in the AWS environment it has access to Elasticache , but , Locally it wo n't ( and that 's a good thing ) . If for some reason I want to test my local environment with Elasticache I need to somehow use SSH tunneling to make AWS think it 's running inside the VPC network.I 've tried to get this working by using below . I 've confirmed I can connect locally using SSH tunneling with Redis Desktop Manager so 100 % I know AWS supports this , my problem is now doing the same thing with Django.This is what I 've tried : I get the message `` Error 60 connecting to '' message when I visit http : //127.0.0.1:8000/.What I 'm I doing wrong here ? Notes : ec2-user @ myRandomEC2.com is not the Redis server , just anotherEC2 instance on AWS that has access Elasticache that I want to use as atunnel . The mykey.pem has access and the correct permission.The ec2 instance has all the correct permissions and ports for access.Tested SSH tunneling with Redis Desktop Manager and this works forthat software.Elasticache and the EC2 instances are all in the same region and can connect to each other ."
"When inserting a huge pandas dataframe into sqlite via sqlalchemy and pandas to_sql and a specified chucksize , I would get memory errors . At first I thought it was an issue with to_sql but I tried a workaround where instead of using chunksize I used for i in range ( 100 ) : df.iloc [ i * 100000 : ( i+1 ) :100000 ] .to_sql ( ... ) and that still resulted in an error . It seems under certain conditions , that there is a memory leak with repeated insertions to sqlite via sqlalchemy.I had a hard time trying to replicate the memory leak that occured when converting my data , through a minimal example . But this gets pretty close . This was run on Google Colab CPU enviroment . The database itself is n't causing the memory leak , because I can restart my enviroment , and the previously inserted data is still there , and connecting to that database does n't cause an increase in memory . The issue seems to be under certain conditions repeated insertions via looping to_sql or one to_sql with chucksize specified . Is there a way that this code could be run without causing an eventual increase in memory usage ? Edit : To fully reproduce the error , run this notebookhttps : //drive.google.com/open ? id=1ZijvI1jU66xOHkcmERO4wMwe-9HpT5OSThe notebook requires you to import this folder into the main directory of your Google Drivehttps : //drive.google.com/open ? id=1m6JfoIEIcX74CFSIQArZmSd0A8d0IRG8The notebook will also mount your Google drive , you need to give it authorization to access your Google drive . Since the data is hosted on my Google drive , importing the data should not take up any of your allocated data ."
I thought I 'd like to learn the new python async await syntax and more specifically the asyncio module by making a simple script that allows you to download multiple resources at one.But now I 'm stuck.While researching I came across two options to limit the number of concurrent requests : Passing a aiohttp.TCPConnector ( with limit argument ) to a aiohttp.ClientSession orUsing a asyncio.Semaphore.Is there a preferred option or can they be used interchangeably if all you want is to limit the number of concurrent connections ? Are the ( roughly ) equal in terms of performance ? Also both seem to have a default value of 100 concurrent connections/operations . If I use only a Semaphore with a limit of lets say 500 will the aiohttp internals lock me down to 100 concurrent connections implicitly ? This is all very new and unclear to me . Please feel free to point out any misunderstandings on my part or flaws in my code.Here is my code currently containing both options ( which should I remove ? ) : Bonus Questions : How do I handle ( preferably retry x times ) coros that threw an error ? What is the best way to save the returned data ( inform my DataHandler ) as soon as a coro is finished ? I do n't want it all to be saved at the end because I could start working with the results as soon as possible.s
"I have a web application that runs long jobs that are independent of user sessions . To achieve this , I have an implementation for a thread-local Flask-SQLAlchemy session . The problem is a few times a day , I get a MySQL server has gone away error when I visit my site . The site always loads upon refresh . I think the issue is related to these thread-local sessions , but I 'm not sure.This is my implementation of a thread-local session scope : And here is my standard Flask-SQLAlchemy session : Then I use both session context managers like this : EDIT : Engine configurations"
"I used the interactive grabcut.py from the OpenCV samples to segment an image and saved the foreground and background models . Then I used these models to segment more images of the same kind , as I do n't want to retrain the model each time.After running the grabcut algorithm , the mask is all zeros ( all background ) and therefore it does n't segment anything.I tried to initialize the algorithm with a mask or a rectangle but this produces an error because the models are not empty ( which is what I actually want ) .How do I have to pass the pre-trained models to the algorithm , such that they are not retrained from scratch each time I 'm segmenting an image ? EDITAfter rayryeng 's comment I implemented following code : It seems to work but the first call now changes my model . In the source code it calls learnGMMs without checking whether a pretrained model is provided ."
"A quick question about Python logging format.This will give me logging messages like : What I want now is to add bracket to the level name , like : How should I modify the format argument ?"
"I am using seaborn for create plot according to this example . However , when I change the last line of code tothe background color does not change correctly : How I can fix the background color so that it fills the whole plot ?"
Is there a way to run unittests in pycharm in verbose mode . I am looking for a way to see the docstring in the test functions so that i can see some info of the ran test.It returns : I want to get :
In my forms.py I have the following : But I am using the same view to add and edit the item . So I want to change the button text ( Add Thing above ) based on some condition in my views.py . How can I do that ?
"Using Flask-Admin + Flask-SQLAlchemy I have defined three models : User , Apikey , Exchange.When an authenticated user creates a new Apikey through the web admin interface I 'd like that the user_id on this new row that 's inserted in the database is set to the current user_id that is logged in.With my current implementation the user can choose any user she likes ( which is not desired ) .This is my definition of the models :"
"There seem to be more than one way to install eggs into a buildout.Way 1 : Way 2 : Both ways work . ( variation on way 2 would be to install each requirement as a separate part . ) What is the difference between these 2 methods ? For my projects , I 'm using buildout with djangorecipe and mr.developer ."
"I have an awkward CSV file which has multiple delimiters : the delimiter for the non-numeric part is ' , ' , for the numeric part ' ; ' . I want to construct a dataframe only out of the numeric part as efficiently as possible.I have made 5 attempts : among them , utilising the converters argument of pd.read_csv , using regex with engine='python ' , using str.replace . They are all more than 2x slower than reading the entire CSV file with no conversions . This is prohibitively slow for my use case.I understand the comparison is n't like-for-like , but it does demonstrate the overall poor performance is not driven by I/O . Is there a more efficient way to read in the data into a numeric Pandas dataframe ? Or the equivalent NumPy array ? The below string can be used for benchmarking purposes.Checks : Benchmarking results : UpdateI 'm open to using command-line tools as a last resort . To that extent , I have included such an answer . My hope is there is a pure-Python or Pandas solution with comparable efficiency ."
"I have the following list : On each element of the list I apply a function and put the results in a dictionary : This gives the following result : What I would like to do is separate the keys from initial_list based on their values in the next_dict dictionary , basically group the elements of the first list to elements with the same value in the next_dict : ' A ' and ' C ' will stay in the same group because they have the same value ' C ' , ' B ' and 'D ' will also share the same group because their value is 'D ' and then 'D ' will be in it 's own group.How can I achieve this result ?"
"I 've looked at several of the Stack Overflow posts with similar titles , and none of the accepted answers have done the trick for me.I have a CSV file where each `` cell '' of data is delimited by a comma and is quoted ( including numbers ) . Each line ends with a new line character . Some text `` cells '' have quotation marks in them , and I want to use regex to find these , so that I can escape them properly.Example line : I want to match just the `` in E 60 '' and in AD '' 8 , but not any of the other `` .What is a ( preferably Python-friendly ) regular expression that I can use to do this ?"
"Why is a number in Scientific notation always read as a float , and how can i convert a string like '1e400 ' to an int ( which is too large for a float ) ? i know , i can make a function like : But this not a very pythonic way , is there a better way ?"
"I have enabled Warming Requests to my app , adding the following directive in app.yaml.Looking at app 's log I see several entries of this kind : 1 . 01-05 02:49PM 50.037 /_ah/warmup 404 300ms 280cpu_ms 1kb See details 2 . I 01-05 02:49PM 50.336 This request caused a new process to be started for your application , and thus caused your application code to be loaded for the first time . This request may thus take longerand use more CPU than a typical request for your application.This makes sense because the Warming Requests documentation says : This causes the App Engine infrastructure to issue GET requests to /_ah/warmup . You can implement handlers in this directory to perform application-specific tasks , such as pre-caching application data.AFAIK ah is a reserved URL , i.e . script handler and static file handler paths will never match these paths ! Should I simply add the ah/warmup route associating it to an empty web handler for example ? Is this correct ?"
"This is probably something stupid I am missing but it has really got me hung up on a larger project ( c extension ) that I am writing.Why is print `` Hello , World ! '' passing None and an extra \n to sys.stdout here ?"
"Suppose I have a DataFrame , on which I want to calculate rolling or expanding Pearson correlations between two columnsWith the inbuilt pandas functionality it 's very fast to calculate thisHowever , if I wish to get the p-values associated with these correlations the best I could do was define a custom rolling function and pass apply to groupby objectc_df now contains the appropriate correlations and importantly their associated p-values.However , this method is extremely slow compared to the inbuilt pandas method , which means it is not suitable , as practically I am calculating these correlations thousands of times during an optimisation process . Furthermore , I am unsure how to extend the custom_roll function to work for expanding windows . Can anyone point me in the direction of leveraging numpy to get the p-values over expanding windows at vectorised speeds ?"
"I have created 3 virtual GPU 's ( have 1 GPU ) and try to speedup vectorization on images . However , using provided below code with manual placement from off docs ( here ) I got strange results : training on all GPU two times slower than on a single one . Also check this code ( and remove virtual device initialization ) on machine with 3 physical GPU 's - work the same.Environment : Python 3.6 , Ubuntu 18.04.3 , tensorflow-gpu 1.14.0 . Code ( this example create 3 virtual devices and you could test it on a PC with one GPU ) : Provide view of output ( from list with 100 images ) : What I tried : split list with images into 3 chunks and assign each chunk to GPU ( see commited lines of code ) . This reduce multiGPU time to 17 seconds , which a little bit faster than single GPU run 18 seconds ( ~5 % ) .Expected results : MultiGPU version is faster than singleGPU version ( at least 1.5x speedup ) .Ideas , why it maybe happens : I wrote calculation in a wrong way"
"Given a couple of simple tables in sqlalchemy which have a simple one to many relationship , I am trying to write a generalized function to add children to the relationship collection . The tables look like this : So with the given setup , I can add items to the images collection for a given StockItem : Ok. so far so good . In reality , my app is going to have several tables with relationships . My issue arises when I try to generalize this out to a function to handle arbitrary relationships . Here ' e what I wrote ( note , add_item ( ) is just a method which wraps an object construction with a try/except to handle IntegrityError 's ) : I call the function like : which obviously errors because the parent table has no collection attribute . Traceback : I think it would throw an error anyway since the collection name is passed as a string.So my question , is how can I add an item to the collection by key word instead of using 'dot ' notation ?"
"I am building a python web app hosted on pythonanywhere following this tutorial loosely . I am modifying the resulting application to fit my own goal.Here is my python code that I am using to pass variables to a HTML document in order for them to be added to a table using a for loop : Using the following code within my HTML , I am looping through that list and adding each item to the table : And this works , however , I am trying to iterate through multiple lists and found various statements saying that the zip function was what I was looking for so I changed my HTML code to the following segment and it no longer works.From python anywhere , the error page says `` Error code : Unhandled Exception '' , and the error log through the pythonanywhere dashboard says : How do I get this to work ? Any help appreciated , thank you ."
"I 'm comparing the libraries dtaidistance , fastdtw and cdtw for DTW computations . This is my code : This is the output I get : dtw.distance ( x , y ) time : 22.16925272245262dtw.distance ( x , y ) distance : 1888.8583853746156dtw.distance_fast ( x , y ) time : 0.3889036471839056dtw.distance_fast ( x , y ) distance : 1888.8583853746156fastdtw ( x , y ) time : 0.23296659641047412 fastdtw ( x , y ) distance : 27238.0 pydtw.dtw ( x , y ) time : 0.13706478039556558pydtw.dtw ( x , y ) distance : 17330.0My question is : Why do I get different performances and different distances ? Thank you very much for your comments.// edit : The unit of the time measurements is seconds ."
"Say I have two lists , and I run the following commandBut if I were to run the following commandHave to admit , I 'm not too familiar with the PY3 codebase . What exactly is going on in the __lt__ , __le__ , __gt__ , __ge__ , __ne__ , __eq__ methods ?"
"I want to make pydev enter into an interactive console mode whenever my program raises an unhandled exception but I ca n't figure out how to do it . As it behaves now , the exception is reported and the process is terminated immidiately.After some searching around I found this : http : //sourceforge.net/tracker/index.php ? func=detail & aid=3029746 & group_id=85796 & atid=577332which suggests to use pydevd.set_pm_excepthook ( ) However , when i addto my code , I get an exception : But : Does n't seem to work , GetGlobalDebugger ( ) does not exist in the global namespace ."
I wrote a Fibonacci series using Python . Could not figure out why second program is giving wrong answer and first one right when both look same.Below program gives right answerBelow program gives wrong answer :
Consider the following Python 2 code : The error message I get is : This seems to imply that -- bar is an unrecognized argument for test . But in fact it is an unrecognized argument for the foo subcommand.I think the error message should be : Is this a bug in argparse ? Can I get configure argparse to give the correct error message ?
My scripting is python and cassandra is data stax community edition.this is the error I 'm getting while trying to insert into a cassandra column family.the code is like : x is a simple array in the form of `` { key : value } '' The error log suggests : There seems to be something very minute I 'm missing here ... well thats why I came here to ask experts !
"Following this guide , I am able to use models outside of Django with the following file structure by calling python main.py.where main.py looks like this : What I want to do is turn this into a `` package '' called db that looks like this : And in the __init__.py of the db package , I want to do this : So I can call the db package from test.py ( which is in the same directory as db ) like this : or like this : Is there a way I can use Django 's models without having to call : os.environ.setdefault ( 'DJANGO_SETTINGS_MODULE ' , 'settings ' ) django.setup ( ) on every page that uses a model ? What am I missing here ?"
"So this is as much a theoretical question as a language-specific one , but consider this : I need PHP to execute a rather system-intensive process ( using PHP exec ( ) ; ) that will be running in the background , but then when a user leaves that specific page , the process will be killed . I quickly realized that a dead man 's switch would be an easy way to implement this since I 'm not making use of any session variables or other server-side variables , which could end up looking like : In any case , a while loop in PHP , resetting a timer in a Python script or re-calling said script every 15 seconds so that it does n't get to the end and kill the process . However , when the user leaves the page , the script will have been called but not able to reset before killing the process.Are there any gaping holes in this idea ? If not , how would the implementation in PHP/JS look ? The order I see it working in would be : Page is hit by user < ? php exec ( 'killer.py ' ) ? > killer.py : Listen for 20 seconds - If no response ... os.system ( 'pkill process ' ) < ? php while ( true ) { sleep ( 15 ) ; exec ( 'killer.py no_wait_dont ' ) ; } ? > Any thoughts you guys have would be greatly appreciated ! Mason"
"I 'm writing a Python program with a GUI built with the Tkinter module . I 'm using a class to define the GUI because it makes it easier to pass commands to buttons and makes the whole thing a bit easier to understand.The actual initialization of my GUI takes about 150 lines of code . To make this easier to understand , I 've written the __init__ function like so : where _init_menu ( ) , _init_connectbar ( ) , and so on do all the initialization work . This makes my code easier to follow and prevents __init__ from getting too big.However , this creates scope issues . Since an Entry widget that I defined in _init_connectbar ( ) is in the function scope and is not a class attribute , I ca n't refer to it in other methods in the class.I can make these problems go away by doing most of the initialization in __init__ , but I 'll lose the abstraction I got with my first method.Should I expand __init__ , or find another way to bring the widgets into class scope ?"
"I have build a RNN language model with attention and I am creating context vector for every element of the input by attending all the previous hidden states ( only one direction ) .The most straight forward solution in my opinion is using a for-loop over the RNN output , such that each context vector is computed one after another . Note : The forward method here is only used for training.However this solution is not very efficient as the code is not well parallelizable with computing each context vector sequently . But since the context vectors are not dependent on each other , I wonder if there is a non-sequential way of calculating them.So is there is a way to compute the context vectors without for-loop so that more of computation can be parallelized ?"
"I am handling what I assume is a common issue : I 've realized that an existing model field of the model Foo would be better as a completely seperate model Bar with a foreign key to Foo . So , we need to make a schema migration . But what 's more , since there is already existing data in the model field of Foo , we need to make a data migration before we delete that field.So we have identified that there are three distinct steps to take : Create the new table BarMigrate the existing data in Foo to the new table BarDelete the existing field in FooFirst , I make all the needed model changes in models.py , and then auto-generate a migration . Everything looks good , except we 're going to lose all the data in the field , so I need to add one extra operation to handle the data migration ( RunPython ) . I would end up with something like the following : Is it safe to run a data migration as one of several operations in a migration ? My worries are that there is any sort of locking going on , or if perhaps the app registry that RunPython passes to do_data_migration wo n't be up to date with the preceding operations ? I am aware the I could create three migrations : one for CreateModel and AddField , the second for RunPython , and the last for RemoveField . The question is if it is functionally equivalent to do all four steps in a single migration ( which provides the added benefit of making the entire migration easier to understand . )"
"I am trying to bind hosts to specified ips in my python program . Just make it affect in the python program , so I am not going to modify the /etc/hosts file.I tried to add a bit code to the create_connection function in socket.py for host-ip translation , like this : I found it works fine.And now I want the host-ip translation only works in this python program.So my question is : how can I make my python program import this socket.py not the build-in one when using import socket ? To make it clear , here is an example . Suppose 'test ' is my work directory : In this case : How can I make main.py use test/socket.py by import socket ? How can I make another modules use test/socket.py when they are using import socket ? I think changing the module find path order may help . But I found that even if the current path ( `` ) is in the first place of sys.path already and import socket still imports the built-in scoket module ."
"Given a list of strings , return a list with the strings in sorted order , except group all the strings that begin with ' x ' first . e.g . [ 'mix ' , 'xyz ' , 'apple ' , 'xanadu ' , 'aardvark ' ] yields [ 'xanadu ' , 'xyz ' , 'aardvark ' , 'apple ' , 'mix ' ] . This code works as long as two list elements which start with x dont come together.i.e I get proper output for list a1 but not for a can you help me understand why ! obtained output ."
Is there a more concise way to split a list into two lists by a predicate ? I understand that this can be turned into an ugly one-liner using reduce ; this is not what I 'm looking for.Update : calculating success_condition only once per element is desirable .
I 've got my pie chart working but I noticed that the text boxes for the actual chart does n't seem to be working correctly . They are just clustered so I was wondering is there any way for me to move the labels into the middle where the white circle is and have the matching colour beside it or not ?
"I am attemptting to get a CRC32c checksum on my local file so I can compare it to the blob.crc32c provided by the gcloud library . Google says I should be using the crcmod module in order to actually calculate CRC32c hashes of my data . modifiedFile.txt has already been downloaded from a Google Cloud Storage bucket onto my local filesystem . The goal here is to set should_download to true only if modifiedFile.txt has a different CRC32c on my local client vs my remote server . How do I get them to generate matching CRC32c in the event that my local filesystem and my gcloud Blob both have the same content ? Unfortunately , it currently always fails as I do n't actually know how to compare the checksum I build with crcmod to the attribute I am seeing in the matching Blob object ."
"I have a Python application which orchestrates calls to an underlying process . The processes are called using subprocess.check_output and they make SNMP calls to remote network devices.For performance monitoring , I would like to count the number of sent SNMP packets which are transmitted . I am primarily interested in the count of the packets . Packet size of request/response would be interesting too , but less important . The aim is to have an idea on the firewall stress this application causes.So , for the sake of argument , let 's assume the following silly application : This would cause a new UDP packet to be sent out on port 161.How can I count them in such a case ? Here 's another version with stubbed functions ( could also be a context manager ) : In this contrived example , it will obviously be 3 calls , as I manually execute the SNMP calls . But in the practical example , the number of SNMP calls is not equal to calls to the subprocess . Sometimes one or more GETs are executed , sometimes it 's simple walks ( that is , a lot of sequential UDP requests ) sometimes it 's bulk walks ( an unknown amount of requests ) .So I ca n't simply monitor the amount of times the application is called . I really have to monitor the UDP requests.Is something like that even possible ? If yes , how ? It 's likely important to know that this runs on Linux as non-root user . But all subprocesses run as the same user ."
"Got a question regarding to the underlying data structure of float ( and precision ) in Python : it seems the values of b and c are machine dependent , they are the numbers that closest to the target values but not exactly the same numbers . I was supervised that we get the 'right ' numbers with 'Print ' , and someone told me that it was because print 'lies ' while Python chose to tell us the truth i.e . showing exactly what they have stored . And my questions are : 1 . How to lie ? e.g . in a function we take two values and return if they are the same , how I could have a best guess if the number of decimal ( precision ) is unknown ? like b and c mentioned above ? is there a well defined algorithm to do that ? I was told that every language ( C/C++ ) will have this kind of issue if we have floating point calculation involved , but how do they 'solve ' this ? 2 . why we can not just store the actual number instead of storing the closest number ? is it a limitation or trading for efficiency ? many thanksJohn"
"I am trying to implement the following ( divisive ) clustering algorithm ( below is presented short form of the algorithm , the full description is available here ) : Start with a sample x , i = 1 , ... , n regarded as a single cluster of n data points and a dissimilarity matrix D defined for all pairs of points . Fix a threshold T for deciding whether or not to split a cluster.First determine the distance between all pairs of data points and choose a pair with the largest distance ( Dmax ) between them.Compare Dmax to T. If Dmax > T then divide single cluster in two by using the selected pair as the first elements in two new clusters . The remaining n - 2 data points are put into one of the two new clusters . x_l is added to the new cluster containing x_i if D ( x_i , x_l ) < D ( x_j , x_l ) , otherwise is added to new cluster containing x_i.At the second stage , the values D ( x_i , x_j ) are found within one of two new clusters to find the pair in the cluster with the largest distance Dmax between them . If Dmax < T , the division of the cluster stops and the other cluster is considered . Then the procedure repeats on the clusters generated from this iteration.Output is a hierarchy of clustered data records . I kindly ask for an advice how to implement the clustering algorithm.EDIT 1 : I attach Python function which defines distance ( correlation coefficient ) and function which finds maximal distance in data matrix.EDIT 2 : Pasted below are functions from Dschoni 's answer.EDIT 3 : When I run the code provided by @ Dschoni the algorithm works as expected . Then I modified the create_distance_list function so we can compute distance between multivariate data points . I use euclidean distance . For toy example I load iris data . I cluster only the first 50 instances of the dataset.The result is as follows : [ [ 24 ] , [ 17 ] , [ 4 ] , [ 7 ] , [ 40 ] , [ 13 ] , [ 14 ] , [ 15 ] , [ 26 , 27 , 38 ] , [ 3 , 16 , 39 ] , [ 25 ] , [ 42 ] , [ 18 , 20 , 45 ] , [ 43 ] , [ 1 , 2 , 11 , 46 ] , [ 12 , 37 , 41 ] , [ 5 ] , [ 21 ] , [ 22 ] , [ 10 , 23 , 28 , 29 ] , [ 6 , 34 , 48 ] , [ 0 , 8 , 33 , 36 , 44 ] , [ 31 ] , [ 32 ] , [ 19 ] , [ 30 ] , [ 35 ] , [ 9 , 47 ] ] Some data points are still clustered together . I solve this problem by adding small amount of data noise to actual dictionary in the sort function : Any idea how to solve this problem properly ?"
"When I use dateutil.parser to parse an incomplete date that 's missing the day , I get the day set to 10 for some reason : Is there a way to changing this so that the day gets automatically set to 1 instead for such incomplete cases ?"
"I am building an app on Google App Engine using Flask . I am implementing Google+ login from the server-side flow described in https : //developers.google.com/+/web/signin/server-side-flow . Before switching to App Engine , I had a very similar flow working . Perhaps I have introduced an error since then . Or maybe it is an issue with my implementation in App Engine.I believe the url redirected to by the Google login flow should have a GET argument set `` gplus_id '' , however , I am not receiving this parameter.I have a login button created by : In the javascript code for the page I have a function to initiate the flow : This initiates the flow at `` /connect '' ( See step 8. referenced in the above doc ) : However , the flow stops at if result [ 'user_id ' ] ! = gplus_id : , saying `` Token 's user ID does n't match given user ID. '' . result [ 'user_id ' ] is a valid users ID , but gplus_id is None.The line gplus_id = request.args.get ( 'gplus_id ' ) is expecting the GET args to contain 'gplus_id ' , but they only contain 'state ' . Is this a problem with my javascript connectServer function ? Should I include 'gplus_id ' there ? Surely I do n't know it at that point . Or something else ?"
I have a class that has an output ( ) method which returns a matplotlib Figure instance . I have a decorator I wrote that takes that fig instance and turns it into a Django response object.My decorator looks like this : and this is how it was being used : The only reason I has it that way instead of using the `` @ '' syntax is because when I do it with the `` @ '' : I get this error : I 'm trying to 'fix ' this by putting it in the `` @ '' syntax but I ca n't figure out how to get it working . I 'm thinking it has something to do with self not getting passed where it 's supposed to ...
"More specifically , I have a list of rows/columns that need to be ignored when choosing the max entry . In other words , when choosing the max upper triangular entry , certain indices need to skipped . In that case , what is the most efficient way to find the location of the max upper triangular entry ? For example : I need to find the index of the min element among all elements in the upper triangle except for the entries a [ 0,1 ] , a [ 0,2 ] , and a [ 1,2 ] ."
Is there a way to figure out the names of the positional arguments to a python function ?
"The code below does n't seem to run concurrently , and I 'm not sure exactly why : The config variable is just a dictionary defined outside of the _run ( ) function . All of the processes seem to be created - but it is n't any faster than if I do it with a single process . Basically what 's happening in the run_**_normalizers ( ) functions is reading from a queue table in a database ( SQLAlchemy ) , then making a few HTTP requests , and then runing a 'pipeline ' of normalizers to modify data and then save it back into the database . I 'm coming from the JVM land where threads are 'heavy ' and often used for parallelism - i 'm a bit confused by this as i thought the multiprocess module was supposed to get around the limitations of Python 's GIL ."
"I have a .py pipeline using apache beam that import another module ( .py ) , that is my custom module.I have a strucutre like this : I import myothermodule.py in mymain.py like this : When I run locally on DirectRuner , I have no problem.But when I run it on dataflow with DataflowRunner , I have an error that tells : So I want to know what should I do if I whant this module to be found when running the job on dataflow ?"
"I have a base template for when a user is logged in , and on that base template , I need to add user specific options in a drop down menu . This drop down menu with options must be constant across all handlers , i.e. , any time the base template is invoked ( extended ) with a child template.Other than performing the necessary DB query , assigning the query results to a variable , and passing that variable to every handler ( there are many ) , how can I consolidate this into one query and one variable , which gets passed directly to the base template ? I am using jinja2 templates as well.I would hate to do something so cumbersome in exchange for something far more simple and maintainable.Any ideas ? Thanks.EDIT So I still have n't found anything that 's exactly what I 'm looking for ; however , I decided to at least make some headway in the interim . So , I made a custom decorator that takes a view 's returned dict ( ) and appends the appropriate data to it . For example : Now , I can at least decorate each method very concisely and simply by putting :"
"Is there a concise way of formatting a number , that on occasion can also be a string ? The number would normally be a float , but occasionally it 's also denoted as the string `` n/a '' .I would like to format the float with a fixed number of decimals , but print the entire string in case it is not a number.For instance : ,butI am not surprised by the ValueError , but wonder if there is a concise way around it , ideally without an explicit if-statement ."
"Following my answered question : R or Python - loop the test data - Prediction validation next 24 hours ( 96 values each day ) I want to predict the next day using H2o Package . You can find detail explanation for my dataset in the same above link.The data dimension in H2o is different . So , after making the prediction , I want to calculate the MAPEI have to change training and testing data to H2o format The upper code works well for `` Non-H2o '' prediction validation for the day-ahead and it calculates the MAPE for every day.I tried to convert the H2o predicted model to normal format but according to to : https : //stackoverflow.com/a/39221269/9341589 , it is not possible.To make a prediction in H2O : for instance , let say we want to create a Random Forest ModelThen we can get the prediction for complete dataset as shown below.But in my case I am trying to get one-day prediction using mape_calc NOTE : Any thoughts in R or Python will be appreciated.UPDATE2 ( reproducible example ) : ** Following @ Darren Cook steps : I provided a simpler example - Boston housing dataset . This is the error I am getting now : Error in .h2o.doSafeREST ( h2oRestApiVersion = h2oRestApiVersion , urlSuffix = page , : ERROR MESSAGE : Provided column type POSIXct is unknown . Can not proceed with parse due to invalid argument ."
"Can you define the type hint for a variable defined with the with syntax ? I would like to type hint the above to say that x is a str ( as an example ) .The only work around that I 've found is to use an intermediate variable , but this feels hacky.I ca n't find an example in the typing documentation ."
"Two days ago , I was given a sudoku problem that I tried to solve with Python 3 . I 've been informed that a solution does exist , but I 'm not certain if there exists multiple solutions.The problem is as following : A 9x9 grid of sudoku is completely empty . It does however contain colored boxes , and inside these boxes , the sum of the numbers has to be a square number . Other than that , normal sudoku rules apply.The issue here is not solving a sudoku puzzle , but rather generating a viable puzzle , that satisfies the rules of the colored boxes . My strategyUsing numpy arrays , I have divided the grid into 81 indices , which can be rearranged to a 9x9 grid.Here is a list containing all the blocks of indices.As you can see from the picture , or from the array above , the boxes are arranged into blocks of 2 , 3 , 4 , or 5 ( 8 twos , 12 threes , 3 fours , 1 fiver ) . I 've also noticed that a box can contain multiple numbers without breaking any rules of sudoku , but only 2 of one number is possible . Given that information , the biggest possible square would be 36 , as 9+9+8+7+6 = 39 , and thus no sum of a block could ever reach 49 . To find out if the sum of a list contains a square number , I 've made the following function : To find out if a list contain the correct amount of duplicates , that is , more than one duplicate of only one number , I 've made the following function : Now , given the digits 1-9 , there are limited ways solutions to a list , if the list has to sum into a square number . Using itertools , I could find the solutions , dividing them into an array , where index 0 contains blocks of twos , index 1 contains blocks of threes , and so on.However , any permutation of these lists are viable solutions to the `` square problem '' . Using itertools again , the total amount of possible boxes ( without the sudoku rules ) sums to 8782.This should be enough to implement functionality that decides if a board is legal , that is , rows , columns and boxes only contains one each of the digits 1-9 . My implementation : Difficulties with runtimeA straightforward approach would be to check every single combination of every single block . I have dones this , and produced several viable problems , however the complexity of my algorithm makes this take far too long time.Instead , I tried to randomize some of the properties : The order of the blocks and the order of the solutions . Using this , I limited the number of tries , and checked if a solution was viable : In the code above , the variable score refers to how many blocks the algorithm could find during an attempt . The variable correct refers to how many of the generated sudoku boards could be completed . If you are interested in how well it did in 700 attempts , here are some stats ( This is a historgram , the x-axis represents the scores , and the y-axis represents how many of each score was present during these 700 attempts ) .What I need help withI am struggling to find a feasible way to find a solution to this problem , that can actually run in a finite amount of time . I would greatly appreciate any tips regarding making some of my code faster or better , any ideas of a different approach to the problem , any solutions to the problem , or some useful tips about Python/Numpy relevant to this problem ."
Python allows the multiplication of strings by integers : How is this implemented in CPython ? I would particularly appreciate a pointer to the source code ; the Mercurial repository is a labyrinth beyond my abilities to navigate .
"According to PEP 468 : Starting in version 3.6 Python will preserve the order of keyword arguments as passed to a function . To accomplish this the collected kwargs will now be an ordered mapping . Note that this does not necessarily mean OrderedDict.In that case , why does this ordered mapping fail to respect equality comparison with Python 's canonical ordered mapping type , the collections.OrderedDict : Although iteration order is now preserved , kwargs seems to be behaving just like a normal dict for the comparisons . Python has a C implemented ordered dict since 3.5 , so it could conceivably have been used directly ( or , if performance was still a concern , a faster implementation using a thin subclass of the 3.6 compact dict ) .Why does n't the ordered mapping received by a function respect ordering in equality comparisons ?"
"I need a Python function iterate ( f , x ) that creates an iterator returning the values x , f ( x ) , f ( f ( x ) ) , f ( f ( f ( x ) ) ) , etc ( like , e.g. , Clojure 's iterate ) . First of all , I was wondering : Does this already exist somewhere in the standard library and I 'm only missing it ? Of course it 's easy enough to implement with a generator : Just out of curiosity : Is there a more functional way to do this in Python , e.g . with some itertools or functools magic ? In Python 3.3 this would workbut looks like an abuse to me . Can I do this more nicely ?"
"I have a matplotlib hexbin embedded in a GTK.Window that graphs some data ( x , y ) . I want the plot to update when new data is received ( via UDP ) . I am having some trouble though . I can get it to work in several different ways , but none have been `` efficient '' ( Meaning - redrawing the plot takes too long ) . I looked here and attempted to model my hexbin after the suggested answer but could not get this to work at all . I keep receiving the following error : TypeError : 'PolyCollection ' object is not iterable.I 'm guessing that hexbins can not be update in the same way as standard plots . Sample Code : The code is used like this : This is just a very small example of how I 'm using the code . I do n't have much experience with matplotlib so there is chance I could be going about this completely wrong . ( which is most likely what I am doing ) So my ultimate question is - How do you update a matplotlib hexbin plot ? Edit : Thanks to danodonovan 's answer , I altered my code and removed the ' , ' after self.graph = self.ax.hexbin ( ... ) The new error thrown is : AttributeError : 'PolyCollection ' object has no attribute 'set_xdata '"
"I am writing a small job scheduler in Python . The scheduler can be given a series of callables plus dependencies , and should run the callables , making sure that no task is run before any of its predecessors.I am trying to follow a test-driven approach , and I have run into an issue testing dependency handling . My test code looks like this : The problem is that this test ( sometimes ) worked even before I added the dependency handling code . This is because the specification does not state that tasks have to be run in a particular order . So the correct order is a perfectly valid choice even if the dependency information is ignored.Is there a way of writing tests to avoid this sort of `` accidental '' success ? It seems to me that this is a fairly common sort of situation , particularly when taking the test-driven `` do n't write code without a failing test '' approach ."
"In Python , I 've seen the recommendation to use holding or wrapping to extend the functionality of an object or class , rather than inheritance . In particular , I think that Alex Martelli spoke about this in his Python Design Patterns talk . I 've seen this pattern used in libraries for dependency injection , like pycontainer.One problem that I 've run into is that when I have to interface with code that uses the isinstance anti-pattern , this pattern fails because the holding/wrapping object fails the isinstance test . How can I set up the holding/wrapping object to get around unnecessary type checking ? Can this be done generically ? In some sense , I need something for class instances analogous to signature-preserving function decorators ( e.g. , simple_decorator or Michele Simionato 's decorator ) .A qualification : I 'm not asserting that all isinstance usage is inappropriate ; several answers make good points about this . That said , it should be recognized that isinstance usage poses significant limitations on object interactions -- -it forces inheritance to be the source of polymorphism , rather than behavior.There seems to be some confusion about exactly how/why this is a problem , so let me provide a simple example ( broadly lifted from pycontainer ) . Let 's say we have a class Foo , as well as a FooFactory . For the sake of the example , assume we want to be able to instantiate Foo objects that log every function call , or do n't -- -think AOP . Further , we want to do this without modifying the Foo class/source in any way ( e.g. , we may actually be implementing a generic factory that can add logging ability to any class instance on the fly ) . A first stab at this might be : There are may reasons why you might want to do things exactly this way ( use decorators instead , etc . ) but keep in mind : We do n't want to touch the Foo class . Assume we 're writing framework code that could be used by clients we do n't know about yet.The point is to return an object that is essentially a Foo , but with added functionality . It should appear to be a Foo -- -as much as possible -- -to any other client code expecting a Foo . Hence the desire to work around isinstance.Yes , I know that I do n't need the factory class ( preemptively defending myself here ) ."
"I ran into this issue while making a practice script to teach myself some Python and about the mysql.connector library . When I perform a query with a single column and print the values , I get results like : ( 'tech-pc-1 ' , ) # Python 3.4.3 ( u'tech-pc-1 ' , ) # Python 2.7.6However , when I perform a query with multiple columns and I print the values I get the results I want.tech-pc-1 jdoeI 'm doing this on a server running Ubuntu 14.04 . I have a couple of questions : Why does this happen ? How do I fix this ?"
"I 'm new to SVMs , and I 'm trying to use the Python interface to libsvm to classify a sample containing a mean and stddev . However , I 'm getting nonsensical results.Is this task inappropriate for SVMs or is there an error in my use of libsvm ? Below is the simple Python script I 'm using to test : The domain seems fairly simple . I 'd expect that if it 's trained to know a mean of 2.5 means label 1 , then when it sees a mean of 2.4 , it should return label 1 as the most likely classification . However , each kernel has an accuracy of 0 % . Why is this ? A couple of side notes , is there a way to hide all the verbose training output dumped by libsvm in the terminal ? I 've searched libsvm 's docs and code , but I ca n't find any way to turn this off.Also , I had wanted to use simple strings as the keys in my sparse dataset ( e.g . { 'mean':2.5 , 'stddev':3.5 } ) . Unfortunately , libsvm only supports integers . I tried using the long integer representation of the string ( e.g . 'mean ' == 1109110110971110 ) , but libsvm seems to truncate these to normal 32-bit integers . The only workaround I see is to maintain a separate `` key '' file that maps each string to an integer ( 'mean'=0 , 'stddev'=1 ) . But obviously that 'll be a pain since I 'll have to maintain and persist a second file along with the serialized classifier . Does anyone see an easier way ?"
"I continue to investigate about pipeline . My aim is to execute each step of machine learning only with pipeline . It will be more flexible and easier to adapt my pipeline with an other use case . So what I do : Step 1 : Fill NaN ValuesStep 2 : Transforming Categorical Values into NumbersStep 3 : ClassifierStep 4 : GridSearchStep 5 : Add a metrics ( failed ) Here is my code : I am aware that it is not ideal to print a roc curve but that 's not the problem right now.So , when I execute this code I have : I 'm interested in all ideas ..."
"I 'm running into an issue when combining multiprocessing , requests ( or urllib2 ) and nltk . Here is a very simple code : A bit more details on what this piece of code does : Import a few required modulesStart a child processIssue an HTTP GET request to 'api.github.com ' from the child processDisplay the resultThis is working great . The problem comes when importing nltk : After having imported NLTK , the requests actually silently crashes the thread ( if you try with a named function instead of the lambda function , adding a few print statement before and after the call , you 'll see that the execution stops right on the call to requests.get ) Does anybody have any idea what in NLTK could explain such behavior , and how to get overcome the issue ? Here are the version I 'm using : I 'm running Mac OS X v. 10.9.5.Thanks !"
"So one of my major pain points is name comprehension and piecing together household names & titles . I have a 80 % solution with a pretty massive regex I put together this morning that I probably should n't be proud of ( but am anyway in a kind of sick way ) that matches the following examples correctly : The regex matcher looks like this : ( wtf right ? ) For convenience : http : //www.pyregex.com/So , for the example : the regex results in a group dict that looks like : I need help with the final step that has been tripping me up , comprehending possible middle names . Examples include : Is this possible and is there a better way to do this without machine learning ? Maybe I can use nameparser ( discovered after I went down the regex rabbit hole ) instead with some way to determine whether or not there are multiple names ? The above matches 99.9 % of my cases so I feel like it 's worth finishing . TLDR : I ca n't figure out if I can use some sort of lookahead or lookbehind to make sure that the possible middle name only matches if there is a last name after it.Note : I do n't need to parse titles like Mr. Mrs. Ms. , etc. , but I suppose that can be added in the same manner as middle names . Solution Notes : First , follow Richard 's advice and do n't do this . Second , investigate NLTK or use/contribute to nameparser for a more robust solution if necessary ."
"So , I looked up information about the weights parameter in the polyfit ( numpy.polynomial.polynomial.polyfit ) function in Python and it seems like it has something to do with the error associated with the individual points . ( How to include measurement errors in numpy.polyfit ) However , what I am trying to do has nothing to do with the error , but weights . I have an image in the form of a numpy array which indicates the amount of charge deposited in the detector . I convert that image to a scatter plot and then do a fit . But I want that fit to give more weight to the points which have more charge deposited and less to the ones that have less charge . Is that what the weights parameter is for ? Here 's an example image : Here 's my code : Let me explain to you the code : The first line assigns the charged deposited in a variable called weights . The next two lines get the points where the charge deposited is > 0 , so there 's some charge deposited to capture the coordinates for the scatter plot . Then I get the size of the entire image to later convert to just a one dimensional array for plotting . I then go through the image and try to get the coordinates of the points where there 's some charge deposited ( remember that the amount of charge is stored in the variable weights ) . I then reshape the y coordinates to get a one dimensional array and get the x coordinates for all the corresponding y coordinates from the image , then change the shape of the weights too to be just one dimensional . Edit : if there 's a way of doing this using the np.linalg.lstsq function , that would be ideal since I 'm also trying to get the fit to go through the vertex of the plot . I could just reposition the plot so the vertex is at zero and then use np.linalg.lstsq , but that would n't allow me to use the weights ."
Suppose that I have a DataFrame : output : I wan na group by a dict : output : but what I want is : Is there any idea ? Thanks !
"I 'm new to programming and have spent the past few weeks studying python on my own . Most of the instruction has been from the resource `` Learn Python the Hard Way '' .Now to my problem , I just started working on a quiz that basically has you complete and install your own setup.py file . However , after spending some time understanding the file and trying to run it ; I keep getting a 'Invalid Syntax ' error on the second single quote here : 'packages ' : [ ] , .I 've tried everything that I can think of such as removing all the single quotes from the variables on the left side , replacing the colons with equal signs , renaming certain files and folders , or a combination of the three . I 've compared the code to other sites offering similar tutorials . which is where the previous ideas arose , and have searched in several places including Google and stackoverflow.com for solutions to this problem . However , so far I have n't found any posts related to this particular situation.I 'm using Ubuntu 12.04 LTS as my operating system.I have a screen shot of the problem , but i lack the reputation to post it here . : ( This code should be exactly like the code from `` Learn Python the Hard Way '' and I have only altered the variables as instructed , such as the name or email address.Heres the code from 'setup.py'Heres the file-tree for 'setup.py ' I 'm sure its probably a very simple solution , but i just ca n't seem to figure this one out.Thanks in advance for any help ."
"I often find I have class instances that are descendants of other class instances , in a tree like fashion . For example say I 'm making a CMS platform in Python . I might have a Realm , and under that a Blog , and under that a Post . Each constructor takes it 's parent as the first parameter so it knows what it belongs to . It might look like this : I typically add a create method to the parent class , so the linkage is a bit more automatic . My Realm class might look like this : That allows the user of the API to not import every single module , just the top level one . It might be like : The problem is those create methods are redundant and I have to pydoc the same parameters in two places.I wonder if there 's a pattern ( perhaps using metaclasses ) for linking one class instance to a parent class instance . Some way I could call code like this and have the blog know what it 's parent realm is :"
"I trained a basic FFNN on a example breast cancer dataset . For the results the precision_recall_curve function gives datapoints for 416 different thresholds . My Data contains 569 unique prediction values , as far as I understand the Precision Recall Curve I could apply 568 different threshold values and check the resulting Precision and Recall.But how do I do so ? is there a way to set the number of thresholds to test with sklearn ? Or at least an explanation of how sklearn selects those thresholds ? I mean 417 should be enough , even for bigger data sets , I am just curious how they got selected ."
"I tried the code below . The f is of type _io.TextIOWrapper , but I can not find any help info about this type . While there does exsist another similar type io.TextIOWrapper . My questions are : If the name _io is not defined , how can I use it ? What 's the difference between _io and io modules ?"
"I have some code that worked very well a year or so ago using pyplot ; I made a plot using plt.plot ( x , y ) using a logarithmic y-axis and replaced the y-axis ticks and tick labels with a custom set as follows : After recently updating my python installation to the current version of miniconda , I now find that while the new labels still appear , they are partly overwritten by matplotlib 's default labels in scientific notation . So it appears that whereas the above code used to replace the default ticks and labels , it now merely adds to them.What do I have to do regain the desired behavior ? And why did it change in the first place ?"
"In a Python tutorial , I 've learned that Like functions , generators can be recursively programmed . The following example is a generator to create all the permutations of a given list of items.I can not figure out how it generates the results . The recursive things and 'yield ' really confused me . Could someone explain the whole process clearly ?"
"I have a template in which you can pass a text variable . I want to include this template into another one but with a translated text as it 's variable . How can you achieve this ? I would like something like this : I tough about writing my own template tag that will perform a ugettext but then when creating the .po file , the text variable will not be taken automatically.I do n't want to do this work in the view since all our translations take place in the templates ."
"I 'm attempting to link to the apply ( ) autodoc documentation at : http : //pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.apply.html.I can link to the release page at : http : //pandas.pydata.org/pandas-docs/dev/release.html by using ... ... but I ca n't seem to get the proper link to the autodoc pages . My intersphinx mapping is configured as ... ... and I 've tried the following variations of links to the apply ( ) method : AnswerAs mzjn answered below , the link is case sensitive so any of the following work in my situation :"
"I 'm new to UI developement with Gtk and ran into something I did n't expect . The FileChooser automatically sorts by name , regardless to if it 's a file or directory . I like having directories listed first , and people are used to/expect it.Is there some way I can get FileChooser to behave this way ? EDIT : In most of the major visual file managers , it is the default behavior to list the directories before the files . These links show what people typically see in their file managers : konqueror , nautilus , thunar , windows , osx and this is what I 'm seeing with my Gtk FileChooser . Is there a way I can get it to look like the rest of the file managers by default , using code ? EDIT2 , the code I open it with :"
"I do n't really understand how yield statement works in this situation . The problem says that given an expression without parentheses , write a function to generate all possible fully parenthesized ( FP ) expressions . Say , the input is ' 1+2+3+4 ' which should be generated to 5 FP expressions : ( 1+ ( 2+ ( 3+4 ) ) ) ( 1+ ( ( 2+3 ) +4 ) ) ( ( 1+2 ) + ( 3+4 ) ) ( ( 1+ ( 2+3 ) ) +4 ) ( ( ( 1+2 ) +3 ) +4 ) My code is as follows.If I use return statement ( the commented out lines ) , then the code works as expected . However , when I change to yield statement as the code shows , I only get the first 4 results . If the number of operands of the input expression is increased , then of course more results will be lost . For example , for the input ' 1+2+3+4+5 ' , I only get 8 instead of 14.I finally figure out the way to make the code work by commenting out the line firstG , secondG = f ( first ) , f ( second ) and replace the linefor e in ( ' ( ' + e1 + op + e2 + ' ) ' for e1 in firstG for e2 in secondG ) : byfor e in ( ' ( ' + e1 + op + e2 + ' ) ' for e1 in f ( first ) for e2 in f ( second ) ) : That means some 'information ' of the generator is lost because of the line firstG , secondG = f ( first ) , f ( second ) but I ca n't figure out the real reason . Could you guys give me some ideas ?"
"I 've read PEP 572 about assignment expressions and I found this code to be a clear example where I could use it : But I am confused , from what I read , it is supposed to work just like normal assignment but return the value . But it does n't appear to work like that : Now after tinkering with it I realised the following works : But it feels so unpythonic . It is the only operator that requires parentheses : Is there a reason for it to be treated by the parser differently than literally anything else in Python ... ? I feel like I am missing something . This is not just an operator.It would be super useful to use : = in the REPL to assign variables as the value would be displayed . ( Update : I do not encourage opinionated discussion on this sensitive topic . Please avoid posting comments or answers other than useful ones . )"
I am trying to use the retro module and I jupyter notebooks I seemed to install it with a ! pip install retro where it went thru the download/install ok.But when I try to import retro I get an error ` Would anyone have any ideas ? Thank you
"When discussing the question Exponentials in python x . **y vs math.pow ( x , y ) , Alfe stated that there would be no good reason for using math.pow instead of the builtin ** operator in python . timeit shows that math.pow is slower than ** in all cases . What is math.pow ( ) good for anyway ? Has anybody an idea where it can be of any advantage then ? We tried to convince each other with some timeit arguments an he is the winner so far ; - ) -- At least the following timeit results , seem to verify that math.pow is slower than ** in all cases.Output : ( ideone-shortcut ) Is there a simple explanation for the difference [ 1 ] we observe ? [ 1 ] The performances of math.pow and ** differ by one order of magnitude.Edits : literal arguments instead of variables in titlefootnote that explicitly points on the magnitude of difference"
"Issue : All labels are getting printed when setting the rotation for the x-axis label using set_xticklabels.CodeScreenshotFix that I triedI do n't have any idea how to fix it and googled but no answer , so I took the labels separately in a list and feed inside the set_xticklabels but still no luck.Thanks in advance : )"
`` A '' is instance of metaclass `` B '' and `` B '' have __slots__ defined - why I have __dict__ at class A ? How I can create class object without __dict__ ?
I have to plot and saveplot in a loop from 1 to 500 with different data but gives a memory leak due to Matplot lib.Has someone any idea on how to deal with that ? Simple case here : And the output is ( memory error as of i=6 ) : Graph of the memory for the computer from launch the script to breack and to kill the console .
"In order to avoid time consuming and costly exact database count queries , I 'd like to override the count ( ) method inside a Django admin class like so : But I don # t know how to access the original queryset within my CountProxy class . Any idea ? I know I can overwrite the whole changelist view through get_changelist . But that involves a lot of duplicating code from Django 's repo ."
Update to more general case : How can I prevent a PytestCollectionWarning when testing a Class Testament via pytest ? Simple example for testament.py : And the test_testament.pyThis creates a PytestCollectionWarning when run with pytest . Is there a way to suppress this warning for the imported module without turning all warnings off ?
I have following DFI want to collapse rows into one as followsI do not want to iterate over columns but want to use pandas to achieve this .
"Consider the following hdfstore and dataframes df and df2I want to first write df to the store.At a later point in time I will have another dataframe that I want to update the store with . I want to overwrite the rows with the same index values as are in my new dataframe while keeping the old ones.When I doThis is n't at all what I want . Notice that ( 0 , ' X ' ) and ( 1 , ' X ' ) are repeated . I can manipulate the combined dataframe and overwrite , but I expect to be working with a lot data where this would n't be feasible.How do I update the store to get ? You 'll see that For each level of ' A ' , ' Y ' and ' Z ' are the same , ' V ' and ' W ' are new , and ' X ' is updated.What is the correct way to do this ?"
would result in TypeError : tuple ( ) takes at most 1 argument ( 2 given ) Why ? What should I do instead ?
"I am running Python 2.6.5 on Mac OS X 10.6.4 ( this is not the native version , I installed it myself ) with Scipy 0.8.0 . If I do the following : I get an IndexError . Then I do : I suspect the negative value is due to bad floating point precision . Then I do the first one again : Now it works ! Can someone explain this ? Are you seeing this behavior too ?"
"Say I have the following reST input : What I would like to end up with is a dict like this : I tried to use this : It does parse the field list , but I end up with some pseudo XML in tree [ `` whole '' ] ? : Since the tree dict does not contain any other useful information and that is just a string , I am not sure how to parse the field list out of the reST document . How would I do that ?"
"I am chaining nodes to a Q object the following way : What means the q object might or might not have been chained a node . Later on I am trying to apply the q object in a filter , but only if this is not empty : How can I check that q is not the same than it was when defined ?"
"When using builtin types as fixture parameters , pytest prints out the value of the parameters in the test report . For example : Running this with py.test -- verbose will print something like : Note that the value of the parameter is printed in square brackets after the test name.Now , when using an object of a user-defined class as parameter , like so : pytest will simply enumerate the number of values ( p0 , p1 , etc . ) : This behavior does not change even when the user-defined class provides custom __str__ and __repr__ implementations . Is there any way to make pytest display something more useful than just p0 here ? I am using pytest 2.5.2 on Python 2.7.6 on Windows 7 ."
"Note : I also posted this on the PyQt mailinglist -- I 'll answer my own question here if a good answer comes up there.I 'm having problems with occasional segfaults when executingQApplication.quit ( ) , possibly related to libQt5Network.so and/orQtWebkit.First of all , the 3 test systems I 'm using : Arch Linux , PyQt 5.2 , Qt 5.2.0 , Python 3.3.3Ubuntu 13.10 , PyQt 5.0.1 , Qt 5.0.2 , Python 3.3.2 in a VMWindows 7 , PyQt 5.2 , Qt 5.2.0 , Python 3.3.3These crashes never happened on Arch for me so far , quite often onUbuntu , and from time to time in Windows . ( though Windows is just aguess , I just get this python.exe is not working anymore foo . ) Original crashI first noticed the problem in a big ( ger ) project , qutebrowser , where it gave me this stacktrace when typing : quit ( on Ubuntu ) : Core dump here ( 15MB , gzip ) .Minimal exampleThen I tried again with a minimal example which quits itselfautomatically after a second with a QTimer . I had to run it in a loopfor about a minute or so before it happened : This gave me this very similiar stacktrace ( on Ubuntu ) : Coredump here ( 15MB , gzip ) .Anyone has an idea what 's going wrong there ? Some magic with stuffbeing garbage-collected the wrong way around ? I also tried someworkarounds* for similiar sympthoms on PyQt4 , but that didn'thelp either . * ca n't find the StackOverflow-answer where it was described -- basically setting QtWidgets.qApp to the QApplication instance beforerunning exec_ ( ) , and to None afterwards ."
"I have a dataframe like this : I would like to get this : I tried : but that is not quite correct . The first issue I have is ' A ' is repeated , I end up getting A:1 only ( 0 gets overwritten ) . How to fix ?"
"I just spent way too much time banging my head against an SMTP server because it did n't like the base64 encoded credentials I was using . Turns out that when I chose NOT to use perl like so many instructions on the internet say to use , I was making a big mistake . Why is this ? I thought base64 was a single standard.Consider : So , perl is unique in its output and my server requires it that way.Why am I getting different results ? How do you get the MIME/SMTP friendly output with something other than perl ?"
"I want to plot the names of cities onto a map of germany using the Basemap package.I have specified the longitude and latitide values with : furthermore , and with : I want to plot the city names but it isnt working but returns the exception ValueError : object too deep for desired array"
"Here is my setup.py : Then I create package : Then I try to install it : pip install file : //path/package-0.0.1.tar.gzAnd get this in terminal : And in pip.log messages like this : And I do n't have anywhere in my project that exact name `` gedthriftstubs '' , if it matters.But this works fine :"
"I believe it 's standard practice when using Python 's built-in logging module to have the logger in the main module be the root logger . Assuming this is correct , it seems to me that for any module that may or may not be run as main I need to explicitly check . The reason is that if I follow the standard practice of calling logging.getLogger ( __name__ ) I 'll get a logger named __main__ rather than the root logger : Is the best practice always to check ? This is not so bad because I 'll always have other code that only runs if __name__ == `` __main__ '' ( often including a call to logging.basicConfig ) but it would be nice to need only one line instead of more ."
"So I need to match strings that are surrounded by | . So , the pattern should simply be r '' \| ( [ ^\| ] * ) \| '' , right ? And yet : It 's only matching on strings that begin with | ? It works just fine on regex101 and this is python 2.7 if it matters . I 'm probably just doing something dumb here so any help would be appreciated . Thanks !"
Why use extend when you can just use the += operator ? Which method is best ? Also what 's the best way of joining multiple lists into one list
"I was reading official python documentation.In the mentioned link , the second line states that : Using this decorator requires that the class ’ s metaclass is ABCMeta or is derived from it.But , I was successfully able to define the below given class.So , the code above worked fine.And also , I was able to create a subclassWithout overriding the abstract method defined above.So , basically does this mean that if my base class 's metaclass is not ABCMeta ( or derived from it ) , the class does not behave like an abstract class even though I have an abstract method in it ? That means , the documentation needs more clarity ? Or , is this behaviour useful somehow and I 'm missing the point ."
"I have some data ( X-ray diffraction ) that looks like this : I want to fit a Gaussian to this data set to get the FWHM of the 'wider ' portion . The double peak at around 7 degrees theta is not important information and coming from unwanted sources . To make myself more clear I want something like this ( which I made in paint : ) ) : I have tried to script something in python with the following code : and I get the following output : Clearly , this is way off from what is desired . Does anyone have some tips how I can acheive this ? ( I have enclosed the data here : https : //justpaste.it/109qp )"
"Raymond Hettinger gave a talk on concurrency in python , where one of examples looked like that : Essentially we go after these links and print amount of received bytes and it takes about 20 seconds to run.Today I found trio library that has quite friendly api . But when I am trying to use it with this rather basic example I am failing to do it right.first try ( runs around the same 20 seconds ) : and second one ( same speed ) : So how is this example should be dealt with using trio ?"
"Consider the following python code for plotting a matplotlib figure : Giving me a nice plotI would like to find out how to disable one of the axes for panning / zooming , so when I use the pan / zoom tool , only ax2 will rescale for example . Is there a way to do this ? I want to do it programmatically ."
"In a Python script using RDFLib 3.0 , I get the following XML-Output when serializing my triples : How can I define specific namespace prefixes for those anonymous _x-prefixes automatically assigned by RDFLib ( or it 's XML-Serializer respectively ) ? Many thanks in advance for your responses !"
Please bear with me . I am quite new at Python - but having a lot of fun . I am trying to code a web crawler that crawls through election results from the last referendum in Denmark . I have managed to extract all the relevant links from the main page . And now I want Python to follow each of the 92 links and gather 9 pieces of information from each of those pages . But I am so stuck . Hope you can give me a hint . Here is my code :
"I 've had a big problem with debugging in VSCODE lately . I have tried to fix it my self by searching the site and reinstalling some of my extensions . Instead of showing my results in the debug console it writes the following output to my terminal : and the results from my script show up below that . The results also show up in the debug console but I would like them to only show up there.I am debugging with the Python : Current file . I have tried changing the console to none in the external and integrated terminal function , but I need those to be default . What can I do to make it debug in the debug console when I use Python : Current File ? I 've seen one post with this question but they changed the console to none and debug in the Python : Integrated Terminal instead of Current FileThe problem occurred when I made a virtualenv in my folder ."
"I had this test earlier today , and I tried to be too clever and hit a road block . Unfortunately I got stuck in this mental rut and wasted too much time , failing this portion of the test . I solved it afterward , but maybe y'all can help me get out of the initial rut I was in.Problem definition : An unordered and non-unique sequence A consisting of N integers ( all positive ) is given . A subsequence of A is any sequence obtained by removing none , some or all elements from A . The amplitude of a sequence is the difference between the largest and the smallest element in this sequence . The amplitude of the empty subsequence is assumed to be 0.For example , consider the sequence A consisting of six elements such that : A subsequence of array A is called quasi-constant if its amplitude does not exceed 1 . In the example above , the subsequences [ 1,2 ] , [ 6,6 ] , and [ 6,6,7 ] are quasi-constant . Subsequence [ 6 , 6 , 7 ] is the longest possible quasi-constant subsequence of A.Now , find a solution that , given a non-empty zero-indexed array A consisting of N integers , returns the length of the longest quasi-constant subsequence of array A . For example , given sequence A outlined above , the function should return 3 , as explained.Now , I solved this in python 3.6 after the fact using a sort-based method with no classes ( my code is below ) , but I did n't initially want to do that as sorting on large lists can be very slow . It seemed this should have a relatively simple formulation as a breadth-first tree-based class , but I could n't get it right . Any thoughts on this ? My class-less sort-based solution :"
"Can someone tell me if my current code is even possible ? I have to create Pascal 's Triangle with an input without using any loops . I am bound to recursion.I have spent 3 days on this , and this is the best output that I can come up with ."
"Does anyone have any tips on using azure-storage directly with Django ? I ask because currently I 'm trying to set up Azure Cloud Storage for my Django app ( hosted on Azure VM with Ubuntu OS ) , and django-storages does n't seem to be interfacing with Azure Storage SDK correctly ( known issue : see here ) . The fix listed there wo n't work for me given my Django version is < 1.6.2.Thus I 'll need to use Azure-storage directly with Django . Has anyone set that up before ? I need to save images and mp3s on the Cloud Storage.Currently , in my models.py , I have : And then django-storages and boto take care of the rest . However , when I hook django-storages up with Azure Cloud Storage , I get the following error : And the relevant snippet of the code is : Seems that the connection to the Azure container is failing . In my settings.py , I have : As described earlier , I need a solution that bypasses django-storages entirely , and just relies on Azure Storage SDK to get the job done.Note : ask me for more information in case you need it ."
"What I am trying to achieve is to design a model which has nested levels of categories.the levels are like this.category0 > category1 > category2 > category3 > category4 > category5Posts can have levels from 0 - 5 , so a post can have category 0 - 1 , while other post may have0 - 4 or 0 - 5 , The category on the given highest level ( 0 is the lower while 5 is the highest ) should inherit from the one just below it , ( 1 > 2 > 3 > 4 > 5 ) How can I achive this ? My current categories looks like thisAnd the post model hereWhat would be the best method ? any suggestions or changes welcome.UpdateThe backend is PostgrSQL.Thanks"
"I migrated a ForeignKey to a GenericForeignKey , using the contrib.contenttypes framework . To access the ContentType object I need to migrate the data , I used this code : The migration works when I run manage.py migrate , and I can then play with the updated model in the shell without problems.However , when I attempt to run manage.py test , I get the following error in the ContentTypes.object.get ( ) line : Querying for ContentType.objects.all ( ) at that time returns an empty queryset.I have tried ( as directed by another answer here in SO ) to run this before my query , but to no avail : How can I ensure that the ContentType rows exist at that point in the test database migration ?"
"I am new to Python ( and dont know much about programming anyway ) , but I remember reading that python generally does not copy values so any statement a = b makes b point to a . If I rungives the result 1 . Should that not be 2 ?"
"Today I came across this expression : ... and I 'm wondering what is the order of assignment.For example x , x , x = 1,2,3 set x to 3 from my test , does it actually set x to 1 , 2 , than 3 ? What 's the rule it follows ? And what happens in more complex conditions like the first code snippet ?"
"I use openpyxl to open a file , edit some cells and save the changes.Here 's an example : The problem is , when I save the file , other cells are modified . In fact , the cells in my sheet that contains formulas are `` corrupted '' , the file size is greatly reduced and when I use other scripts to read the sheet , the cells containing formulas are reported as empty . But when I open the sheet , everything looks normal and when I save , everything is repaired and the file size is back to normal . I think the problem comes from openpyxl not `` calculating '' the formulas when saving . This would reduce the file size and require a manual opening/saving in order to get the real cell values . I ca n't find any way to resolve this issue without completely changing the library I use.Any help would be appreciated , thanks !"
"So I 'm taking another stab at the asyncio module now that 3.8 is out . However , I am getting unexpected results when trying to do a graceful shutdown of the event loop . Specifically I am listening for a SIGINT , cancelling the running Tasks , gathering those Tasks , and then .stop ( ) ing the event loop . I know that Tasks raise a CancelledError when they are cancelled which will propagate up and end my call to asyncio.gather unless , according to the documentation , I pass return_exceptions=True to asyncio.gather , which should cause gather to wait for all the Tasks to cancel and return an array of CancelledErrors . However , it appears that return_exceptions=True still results in an immediate interruption of my gather call if I try to gather cancelled Tasks.Here is the code to reproduce the effect . I am running python 3.8.0 : Output : I 'm guessing there is still something about the event loop that I do n't understand , but I would expect all the CancelledErrors to come back as an array of objects stored in results and then be able to continue on rather than to see an error immediately ."
"Why are future imports limited to only certain functionality ? Is there no way to get the walrus operator in Python 3.7 ? I thought this would work , but it does n't : It does n't work because walrus is n't in the list of supported features : Are there any other alternatives short of using python 3.8 ?"
"Trying to load a file into python . It 's a very big file ( 1.5Gb ) , but I have the available memory and I just want to do this once ( hence the use of python , I just need to sort the file one time so python was an easy choice ) . My issue is that loading this file is resulting in way to much memory usage . When I 've loaded about 10 % of the lines into memory , Python is already using 700Mb , which is clearly too much . At around 50 % the script hangs , using 3.03 Gb of real memory ( and slowly rising ) . I know this is n't the most efficient method of sorting a file ( memory-wise ) but I just want it to work so I can move on to more important problems : D So , what is wrong with the following python code that 's causing the massive memory usage : EDIT : In case anyone is unsure , the general consensus seems to be that each variable allocated eats up more and more memory . I `` fixed '' it in this case by 1 ) calling readLines ( ) , which still loads all the data , but only has one 'string ' variable overhead for each line . This loads the entire file using about 1.7Gb . Then , when I call lines.sort ( ) , I pass a function to key that splits on tabs and returns the right column value , converted to an int . This is slow computationally , and memory-intensive overall , but it works . Learned a ton about variable allocation overhad today : D"
"I 'm rusty in SQL and completely new to SQL Alchemy , but have an upcoming project which uses both . So I thought I write something to get comfortable . Suffering from a hangover I decided to write something to keep track of alcohol levels.I have events where users participate and consume drinks . Those are my three basic tables ( with one helper table guestlist , for a m : n relationship between users and events ) .drinks list the drinks available at all events to all users all the time ( no need to map anything ) . users are created from time to time , so are events . all users can join all events , so I use the guestlist table to map those.Now to the heart of the question : I need to keep track at what time which user consumes which drink on which event . I try to solve this whit another table shots ( see below ) but I 'm not sure if this a good solution . ( source : anyimg.com ) In SQL Alchemy it might look somewhat like this ( or not , but this is what I came up with so far ) : :I struggle to find a good way to build a table that maps events , users and drinks together : How should I formulate the relationships and how do I query it ? The thing is I kinda feel I overlooked something . And frankly I 'm absolute lost on how to query it ? Here are the queries I would make most of the time : I at least need to get all the shots consumed on a event ( probably sorted by user ) I also sometimes need all shots for a specific user ( probably sorted by event ) And a lot of counting : Number of shots per eventNumber of shots per userNumber of shots a user downed at a eventIs the shots table a okay way to manage this ?"
"I have a comments section on some pages on my site that I build with a { % for ... % } loop ( and another nested loop for comment replies . The section was hacked together , and I am still learning web development and Django , so please forgive any frustrating sloppiness or weirdness . I am not concerned with efficiency at the moment , only efficacy , and right now it is not working quite right.For each comment I have a Bootstrap dropdown button that will bring up the options Edit and Delete . Edit will open a modal to edit the comment . The modals are rendered with an { % include % } tag . Below I have included part of my code unmodified , rather than trying to simplify my example and risk leaving something crucial out : Here is the edit modal : and the reply modal : The issue I am having is that my reply and edit modals ( e.g . { % include 'topspots/editmodal.html ' with edit_type='reply ' % } or { % include 'topspots/replymodal.html ' % } seem to be only rendered once with the context of the first iteration of my for loop . So even though all the questions are correctly rendered on the page , when I click reply , edit or delete , regardless of which button I click ( i.e. , whether I click the button for the first comment , or the fifth comment , etc . ) I can only reply to , edit , or delete the very first comment . I have a feeling that this has something to do with closures and scope in a way I am not quite understanding ( I have gotten into trouble in the past with unexpected results using lambda in Python loops because of this or this ) , but I am not sure.I did a test with the following view : and templates : and And it printed out a list of unique spot names , so why the difference with the modals ?"
"I 'm using the struct module , and things are n't going as I expected . Its due to some misunderstanding I have with the module I 'm sure.The output of this is : What am I missing here ? Why are the second and third different sizes , and why is the fourth not 16 ?"
"I have a Select widget that should give a different list of options whenever another Select widget is changed , so it updates whenever this other Select widget changes . How do I this in the example code below ?"
"Currently I was learning about generators and list comprehension , and messing around with the profiler to see about performance gains stumbled into this cProfile of a sum of prime numbers in a large range using both.I can see that in the generator the :1 genexpr as cumulative time way shorter than in its list counterpart , but the second line is what baffles me . Is doing a call which I think is the check for number is prime , but then is n't supposed to be another :1 module in the list comprehension ? Am I missing something in the profile ?"
I get the following example from Effective Python item 31 : I ca n't think of any reason to have if instance is None : return self in __get__ . How can an Exam ( or other potential classes using Grade ) instance be None ?
"I am trying to create a dictionary with two strings as a key and I want the keys to be in whatever order.I want this piece of code to print 'something else ' . Unfortunately , it seems that the ordering matters with tuples . What would be the best data structure to use as the key ?"
"I have sets of x , y , z points in 3D space and another variable called charge which represents the amount of charge that was deposited in a specific x , y , z coordinate . I would like to do a weighted ( weighted by the amount of charge deposited in the detector , which just corresponds to a higher weight for more charge ) for this data such that it passes through a given point , the vertex.Now , when I did this for 2D , I tried all sorts of methods ( bringing the vertex to the origin and doing the same transformation for all the other points and forcing the fit to go through the origin , giving the vertex really high weight ) but none of them were as good as the answer given here by Jaime : How to do a polynomial fit with fixed pointsIt uses the method of Lagrange multipliers , which I 'm vaguely familiar with from an undergraduate Advanced Multi variable course , but not much else and it does n't seem like the transformation of that code will be as easy as just adding a z coordinate . ( Note that even though the code does n't take into consideration the amount of charge deposited , it still gave me the best results ) . I was wondering if there was a version of the same algorithm out there , but in 3D . I also contacted the author of the answer in Gmail , but did n't hear back from him.Here is some more information about my data and what I 'm trying to do in 2D : How to weigh the points in a scatter plot for a fit ? Here is my code for doing this in a way where I force the vertex to be at the origin and then fit the data setting fit_intercept=False . I 'm currently pursuing this method for 2D data since I 'm not sure if there 's a 3D version out there for Lagrange multipliers , but there are linear regression ways of doing this in 3D , for instance , here : Fitting a line in 3D : image_array is a numpy array and vertexX and vertexY are the x and y coordinates of the vertex , respectively . Here 's my data : https : //uploadfiles.io/bbhxo . I can not create a toy data as there is not a simple way of replicating this data , it was produced by Geant4 simulation of a neutrino interacting with an argon nucleus . I do n't want to get rid of the complexity of the data . And this specific event happens to be the one for which my code does not work , I 'm not sure if I can generate a data specifically so my code does n't work on it ."
"I 'd like to extract the designator and ops from the string designator : op1 op2 , in which there could be 0 or more ops and multiple spaces are allowed . I used the following regular expression in PythonThe problems is that only des1 and op2 is found in the matching groups , op1 is not . Does anyone know why ?"
"Forgive me if this question has been asked before but I could not find any related answer.Consider a function that takes a numerical type as input parameter : This works with integers , floats and complex numbers.Is there a basic type so that I can do a type hinting ( of a real existing type/base class ) , such as : Furthermore I need to use this in a collection type parameter , such as :"
"I have two polygons , P and Q , where the exterior linear ring of a polygon is defined by two closed sets of points , stored as numpy arrays , connected in a counterclockwise direction . P and Q are in the following format : The exterior ring of P is formed by joining P [ 'x_coords ' ] [ 0 ] , P [ 'y_coords ' ] [ 0 ] - > P [ 'x_coords ' ] [ 1 ] , P [ 'y_coords ' ] [ 1 ] etc . The last coordinate of each array is the same as the first , indicating that the shape is topologically closed.Is it possible to calculate a simple minimum distance between the exterior rings of P and Q geometrically using numpy ? I have searched high and low on SO without finding anything explicit , so I suspect this may be a drastic oversimplification of a very complex problem . I am aware that distance calculations can be done with out-of-the-box spatial libraries such as GDAL or Shapely , but I 'm keen to understand how these work by building something from scratch in numpy.Some things I have considered or tried : Calculate the distance between each point in both arrays . This does n't work as the closest point between P and Q can be an edge-vertex pair . Using the convex hull of each shape , calculated using scipy.spatial has the same problem.An inefficient brute force approach calculating the distance between every pair of points , and every combination of edge-point pairsIs there a better way to go about this problem ?"
"i create my first article in restructured text.http : //s.yunio.com/ ! LrAsuplease download it and untar it on your computer , cd into /rest/build/html , open index.rst with your chrome.i found that in restructured text search function:1.can not search chinese character2.can not search short words please see attatchment 1 , it is my target article to be searchedyou can see is and 标准 in the text.please see attatchment 2 , can not search chinese character 标准 which is in the text . please see attatchment 3 , can not search short words is which is in the text . how can i solve the problem ?"
"Having a weird problem with Python 's unittest and PyMongo . The test randomly succeeds or fails : Class being tested : Random behavior : ... These two results randomly happen for the same test as I rerun the test without changing anything neither in the class nor in the test.All of this runs on my machine and I know for sure that while running the test , nobody else tinkers neither with MongoDB nor with the code.What gives ?"
"My goal is to write a small library for spectral finite elements in Python and to that purpose I tried extending python with a C++ library using Boost , with the hope that it would make my code faster . I compared the performance of three different methods to calculate the integral of two functions . The two functions are : The function f1 ( x , y , z ) = x*xA function that is more difficult to evaluate : f2 ( x , y , z ) = np.cos ( 2*x+2*y+2*z ) + x*y + np.exp ( -z*z ) +np.cos ( 2*x+2*y+2*z ) + x*y + np.exp ( -z*z ) +np.cos ( 2*x+2*y+2*z ) + x*y + np.exp ( -z*z ) +np.cos ( 2*x+2*y+2*z ) + x*y + np.exp ( -z*z ) +np.cos ( 2*x+2*y+2*z ) + x*y + np.exp ( -z*z ) The methods used are : Call the library from a C++ program : Call the library from a Python script : Use a for loop in Python : Here are the execution times of each of the method ( The time was measured using the time command for method 1 , and the python module time for methods 2 and 3 , and the C++ code was compiled using Cmake and set ( CMAKE_BUILD_TYPE Release ) ) For f1 : Method 1 : 0.07s user 0.01s system 99 % cpu 0.083 totalMethod 2 : 0.19sMethod 3 : 3.06sFor f2 : Method 1 : 0.28s user 0.01s system 99 % cpu 0.289 totalMethod 2 : 12.47sMethod 3 : 16.31sBased on these results , my questions are the following : Why is the first method so much faster than the second ? Could the python wrapper be improved to reach comparable performance between methods 1 and 2 ? Why is method 2 more sensitive than method 3 to the difficulty of the function to integrate ? EDIT : I also tried to define a function that accepts a string as argument , writes it to a file , and proceeds to compile the file and dynamically load the resulting .so file : It 's quite dirty and probably not very portable , so I 'd be happy to find a better solution , but it works well and plays nicely with the ccode function of sympy.SECOND EDIT I have rewritten the function in pure Python Using Numpy.Somewhat surprisingly ( at least to me ) , there is no significant difference in performance between this method and the pure C++ implementation . In particular , it takes 0.059s for f1 and 0.36s for f2 ."
"This is an extension of my recent question Avoiding race conditions in Python 3 's multiprocessing Queues . Hopefully this version of the question is more specific.TL ; DR : In a multiprocessing model where worker processes are fed from a queue using multiprocessing.Queue , why are my worker processes so idle ? Each process has its own input queue so they 're not fighting each other for a shared queue 's lock , but the queues spend a lot of time actually just empty . The main process is running an I/O-bound thread -- is that slowing the CPU-bound filling of the input queues ? I 'm trying to find the maximal element of the Cartesian product of N sets each with M_i elements ( for 0 < = i < N ) under a certain constraint . Recall that the elements of the Cartesian product are length-N tuples whose elements are are elements of the N sets . I 'll call these tuples 'combinations ' to emphasize the fact that I 'm looping over every combination of the original sets . A combination meets the constraint when my function is_feasible returns True . In my problem , I 'm trying to find the combination whose elements have the greatest weight : sum ( element.weight for element in combination ) .My problem size is large , but so is my company 's server . I 'm trying to rewrite the following serial algorithm as a parallel algorithm.My current multiprocessing approach is to create worker processes and feed them combinations with an input queue . When the workers receive a poison pill they place the best combination they 've seen on an output queue and exit . I fill the input queue from the main thread of the main process . One advantage of this technique is that I can spawn a secondary thread from the main process to run a monitoring tool ( just a REPL I can use to see how many combinations have been processed so far and how full the queues are ) .I originally had all the workers reading from one input queue but found that none of them were hitting the CPU . Figuring that they were spending all their time waiting for queue.get ( ) to unblock , I gave them their own queues . That increased pressure on the CPU , so I figured the workers were active more often . However , the queues spend most of their time empty ! ( I know this from the monitoring REPL I mentioned ) . This suggests to me that the main loop filling up the queues is slow . Here is that loop : I 'm guessing the bottleneck is worker.in_q.put ( ) . How do I make that faster ? My first instinct was to make the workers slower , but that just does n't make sense ... Is the problem that the monitor thread is stopping the loop too often ? How would I be able to tell ? Alternatively , is there another way to implement this that does n't involve so much waiting on locks ?"
"In my django app , this is my validator.py which is used to validate this : When I enter URL like google.co.in , and print the value right before returning it from validate_url , it prints http : //google.co.in but when I try to get the cleaned_data [ 'url ' ] in my views , it still shows google.co.in . So where does the value returned by my validator go and do I need to explicitly edit the clean ( ) functions to change the url field value ? ? The doc says the following : The clean ( ) method on a Field subclass is responsible for running to_python ( ) , validate ( ) , and run_validators ( ) in the correct order and propagating their errors . If , at any time , any of the methods raise ValidationError , the validation stops and that error is raised . This method returns the clean data , which is then inserted into the cleaned_data dictionary of the form.I am still not sure where the validator return value goes and if it is possible to change cleaned_data dict using the validator ."
"I would like to convert all the values in a pandas dataframe from strings to floats . My dataframe contains various NaN values ( e.g . NaN , NA , None ) . For example , I have found here and here ( among other places ) that convert_objects might be the way to go . However , I get a message that it is deprecated ( I am using Pandas 0.17.1 ) and should instead use to_numeric.Output : But to_numeric does n't seem to actually convert the strings.Output : Should I use convert_objects and deal with the warning message , or is there a proper way to do what I want with to_numeric ?"
I 've just updated Stripe API library to the latest version and it stopped working on Google AppEngine due to fact that GAE is blocking imports of some packages such as sockets and SSL.Are there any chances to make it working on Google AppEngine ?
"I 'm using the latest release of the PyGTK All-in-One installer ( 2.24.2 ) for Python 2.7 which includes Cairo 1.10.8 , Pango 1.29.4 , PyGTK 2.24.0 , and PyGobject 2.28.3 ( I think ) .The following code leaks ~55 MB of memory : Note : The for loop is in my test script just so I could see the memory consumption increase in Task Manager . It 's also essentially what 's going on in my real application , except the label text changes at least once per second instead of just being redrawn with the same text each time.The problem line is label.set_markup ( ) , which leaks about 0.5kB per call so I suspect the problem is in GTK or Cairo somewhere . It 's possibly this bug ( 685959 ) , as pointed out by a commenter.I tried using objgraph to see if any extra Python objects are showing up in proportion to the number of calls to gtk.Label.set_markup ( ) but there are no excess objects . It follows that calls to gc.collect ( ) do n't help , and I tried it to be sure . Python does n't seem to be aware of the objects which are responsible for the memory consumption.How do I find this memory leak , and/or work around it ? I need to use markup to style some text for this application , but I tried using gtk.Label.set_text ( ) as a workaround and it leaks memory too.I should note that this application targets Windows so using PyGObject to get GTK 3 is not an option -- GObject introspection is still not available on Windows ."
"I 'm trying to plot multiple series with two measurements ( so it 's actually num_of_time_series x 2 graphs ) in one figure using pygal.For instance , suppose mt data is : and the graph rendering code is that : The Current result is that.The problem in the code above is that it 's unclear which graph represents measurement 1 and which represents measurement 2 . Second , I would like to see each component in a different color ( or shape ) . This graph aims to compare one component versus the two others , and to see the correlation between measurement 1 and 2.Thanks for the help guys !"
"I have code where I show a figure , for exampleHow can I test that the pyplot.show ( ) shows a figure without actually showing it ? If it does n't work it gives an exception , so that 's alright , but I do n't want to have the figure shown every time I run a unittest when it does work ."
"I am parsing a web page with BeautifulSoup , and it has some elements like the following : The structure always seems to be a < td > with the first part surrounded by < font > < b > , and the text after the < /font > tag can be empty . How can I get that text that is after the font tag ? In this example I would want to get `` 16043646 '' . If the html was insteadI would want to get `` ''"
I have a DataFrame like : What I want to get is This is my approach as of now.Is there any efficient way to achieve this ? apply Here is way to slow .Thank you for your assistant ! : ) My real data size
"When I run in my Ubuntu terminal : it works fine.If I run it through Pythons subprocess.Popen ( ) : it does n't work . The Error I get is : dd : failed to open '~/disk_benchmark_file ' : No such file or directoryIf I change in the Popen ( ) call the tilde ~ to /home/user , then it works ! Why is it like that ? And more important to me : How can I make it work ? I do n't know what the user name will be in production ."
"I 'm attempting to verify the SHA1 signature of a message by downloading a certificate from a website and extracting its public key . There 's a few bits of sample code elsewhere on SO ( here and here ) , however I have n't yet figured out what I 'm doing wrong.This gives me the following assertion error : However , if I use openssl from the command line along with the files generated from the above Python script , it works fine : I 've hit a brick wall here ; any suggestions would be greatly appreciated . Thanks !"
"TaskI have a pandas dataframe where : the columns are document namesthe rows are words in those documentsnumbers inside the frame cells are a measure of word relevance ( word count if you want to keep it simple ) I need to calculate a new matrix of doc1-doc similarity where : rows and columns are document namesthe cells inside the frame are a similarity measure , ( 1 - cosine distance ) between the two documentsThe cosine distance is conveniently provided by script.spatial.distance.cosine.I 'm currently doing this : use itertools to create a list of all 2-combinations of the document names ( dataframe columns names ) loop over these and create a update a dictionary of { doc1 : { doc2 : similarity } } after the loop , create a new frame using pandas.DataFrame ( dict ) ProblemBut it takes a very very long time . The following shows current speed on a MacBook Pro 13 with 16GB ram and 2.9GHz i5cpu running latest anaconda python 3.5 ... plotting time taken against combinations of docs.You can see that 100,000 combinations takes 1200 seconds . Extrapolating that to my corpus of 7944 documents , which creates 31,549,596 combinations , would take 5 days to calculate this similarity matrix ! Any ideas ? I previously was dynamically creating the dataframe df.ix [ doc1 , doc2 ] = similarity .. which was very very much slower.I 've considered numba @ git but it fails with pandas data structures.I ca n't find a built in function which will do all the work internally ( in C ? ) What I have to do tactically is to randomly sample the documents to create a much smaller set to work with ... currently a fraction of 0.02 leads to about 20 minutes of calculation ! Here 's the code ( github ) Simple Example @ MaxU asked for an illustrative example.Relevance matrix ( wordcount here , just to keep it simple ) : calculated similarity matrix based on 2-combinations ( doc1 , doc2 ) , ( doc2 , doc3 ) , ( doc1 , doc3 ) Take that top left value 0.889 .. thats the dot product ( 2*3 + 2*2 + 0 + 0 ) = 10 but normalised by the lengths of the vectors ... so divide by sqrt ( 8 ) and sqrt ( 14 ) gives 0.9449 . You can see that there is no similarity between doc1 and doc3 .. the dot product is zero.Scale this from 3 documents with 4 words ... to 7944 documents , which creates 31,549,596 combinations ..."
I 'm resizing images in python using PillowWhy does resized_image.format Hold a None Value ? And How can i retain the format when resizing using pillow ?
"works , andworks , butfails withWhy ? EDIT My workaround : ( Luckily it worked for my real code too . )"
What is the most efficient way to get a fixed number of items from a generator ? What I currently do is using zip and range . In this example I takechunks of size 3 from the generator.The background is that the database I use provides a generator object for query results . Than I fill a fixed size numpy array with data and process it as one batch .
"I want to get the latest non null value across all the variables . For example , in this data set , we have 3 service dates . Example DatasetAs an output I need something like this : Expected OutputMy original data set has thousands of patients and 100+ covariates . Currently I am using a triple loop to achieve this task , which is very inefficient . I am looking for more efficient solutions ."
"Is it possible to add to existing model auto_now and auto_now_add DateTime fields ? We ca n't add this fields without default , but adding the default value gives an error : ./manage.py makemigrationsreturns :"
"I have an array like so : I 'd like to have a running counter of instances of 1.0 that resets when it encounters a 0.0 , so the result would be : My initial thought was to use something like b = np.cumsum ( a [ a==1.0 ] ) , but I do n't know how to ( 1 ) modify this to reset at zeros or ( 2 ) quite how to structure it so the output array is the same shape as the input array . Any ideas how to do this without iteration ?"
"I want to write a wrapper class which takes a value and behaves just like it except for adding a 'reason ' attribute . I had something like this in mind : However , the double-underscore functions do n't seem to be captured with __getattribute__ , for example : I would think the two above should end up doing this : Why is this not happening ? How can I make it happen ? ( This might be a horrible idea to actually use in real code but I 'm curious about the technical issue now . )"
"I frequently have simple classes which I 'll only ever want a single instance of . As a simple example : Is there a way that I could somehow combine the definition and instantiation into a single step and achieve the same results ? As another example , just as something that is simple enough to understand.I googled around and found nothing ( lots of people throwing around the words one-off and anonymous but nobody seems to be talking about the same thing I am ) . I tried this , but it did n't work : I realize I do n't gain much , just one line I do n't have to type plus one fewer things in my namespace , so I understand if this does n't exist ."
"I want to transform a string such as following : into a list of non-empty elements : My solution is this list comprehension : Just wonder , is there a nice , pythonic way to write this comprehension without calling el.strip ( ) twice ?"
"From this answer I have a flattened list.Now I want to remove duplicates and sort the list . Currently I have the following : Is there a better way of accomplishing this ? In my case x is of size ~32,000 and y ends up being of size ~1,100 . What I have works , but I 'd like to see if there 's anything better ( faster , more readable , etc )"
"As an example , the chat site Omegle always displays on its homepage the current number of users online , which I am able to extract with this python script using the headless HTMLUnit Webdriver in Selenium : The output is like : This number is dynamically generated and updated periodically by a script , and I want to read just this dynamically updated content at intervals without repeatedly loading the entire page with driver.get . What Selenium Webdriver method or functionality will let me do that ? This article seems like a relevant lead , though it led me nowehere ."
"Looking at kismet 's source code in packet_ieee80211.h is the section I understand this as shifting bits but that 's about it . Suppose I have the int 706 , how do I break this number up into the cryptset as defined above i.e . how can I extract which crypts are used give 706 especially ported to pythonThanks"
"I 've been scripting something that has to do with scanning directories and noticed a severe memory leak when calling os.path.isdir , so I 've tried the following snippet : Within a few seconds , the Python process reached 100MB RAM.I 'm trying to figure out what 's going on . It seems like the huge memory leak is in effect only when the path is indeed a valid directory path ( meaning the 'return False ' is not executed ) .Also , it is interesting to see what happens in related calls , like os.path.isfile.Thoughts ? Edit : I think I 'm onto something.Although isfile and isdir are implemented in the genericpath module , on Windows system - isdir is being imported from the builtin nt.So I had to download the 2.7.3 source ( which I should 've done long time ago ... ) .After a little bit of searching , I found out posix__isdir function in \Modules\posixmodule.c , which I assume is the 'isdir ' function imported from nt.This part of the function ( and comment ) caught my eye : It seems that it all boils down to Unicode/ASCII handling bug.I 've just tried my snippet above with path argument in unicode ( i.e . u 'D : \Downloads ' ) - no memory leak whatsoever . haha ."
Since the octal prefix is now 0o in Python 3 it 's not legal to write 0777 any more . Okay.So why is it legal to write 00 which evaluates properly to 0 whereas other digits trigger a syntax error ?
"I have a reasonably large number n=10000 of sorted lists of length k=100 each . Since merging two sorted lists takes linear time , I would imagine its cheaper to recursively merge the sorted lists of length O ( nk ) with heapq.merge ( ) in a tree of depth log ( n ) than to sort the entire thing at once with sorted ( ) in O ( nklog ( nk ) ) time.However , the sorted ( ) approach seems to be 17-44x faster on my machine . Is the implementation of sorted ( ) that much faster than heapq.merge ( ) that it outstrips the asymptotic time advantage of the classic merge ?"
"I 'm training an LSTM network with Tensorflow in Python and wanted to switch to tf.contrib.cudnn_rnn.CudnnLSTM for faster training . What I did is replacedwithI 'm experiencing significant training speedup ( more than 10x times ) , but at the same time my performance metric goes down . AUC on a binary classification is 0.741 when using LSTMCell and 0.705 when using CudnnLSTM . I 'm wondering if I 'm doing something wrong or it 's the difference in implementation between those two and it 's that 's the case how to get my performance back while keep using CudnnLSTM.The training dataset has 15,337 sequences of varying length ( up to few hundred elements ) that are padded with zeros to be the same length in each batch . All the code is the same including the TF Dataset API pipeline and all evaluation metrics . I ran each version few times and in all cases it converges around those values.Moreover , I have few datasets that can be plugged into exactly the same model and the problem persists on all of them.In the tensorflow code for cudnn_rnn I found a sentence saying : Cudnn LSTM and GRU are mathematically different from their tf counterparts.But there 's no explanation what those differences really are ..."
"I 'm doing web scraping using selenium webdriver in Python with Proxy.I want to browse more than 10k pages of single site using this scraping.Issue is using this proxy I 'm able to send request for single time only . when I 'm sending another request on same link or another link of this site , I 'm getting 416 error ( kind of block IP using firewall ) for 1-2 hours.Note : I 'm able to do scraping all normal sites with this code , but this site has kind of security which is prevent me for scraping.Here is code.Any solution ? ?"
"Sometimes ( in customer 's PCs ) I need a python script to execute in the Windows shell like a .CMD or .BAT , but without having the .py or .pyw extensions associated with PYTHON / PYTHONW.I came out with a pair of 'quick ' n dirty ' solutions:1 ) 2 ) Do you know a better solution ? ( ie : more concise or elegant ) Update : based on @ van answer , the more concise way I found ( without setting ERRORLEVEL )"
"So I 've been following Google 's official tensorflow guide and trying to build a simple neural network using Keras . But when it comes to training the model , it does not use the entire dataset ( with 60000 entries ) and instead uses only 1875 entries for training . Any possible fix ? Output : Here 's the original google colab notebook where I 've been working on this : https : //colab.research.google.com/drive/1NdtzXHEpiNnelcMaJeEm6zmp34JMcN38"
"Imagine that I have an order list of tuples : Given a parameter X , I want to select all the tuples that have a first element equal or greater than X up to but not including the first tuple that has -1 as the second element.For example , if X = 3 , I want to select the list [ ( 3,0 ) , ( 4,0 ) ] One idea I had is : Get the cut-off key withThen select elements with keys between the X and E : That gives me what I want in R , but it seems really inefficient , involving two table scans . I could do it in a for loop , discarding tuples with keys too small , and break when I hit the first blocking tuple . But for runs like a dog compared to list selection.Is there a super-efficient , python-esque ( 2.7 ) way of doing this ?"
"Given this input : [ 1,2,3,4 ] I 'd like to generate the set of spanning sets : Every set has all the elements of the original set , permuted to appear in unique subsets . What is the algorithm that produces these sets ? I 've tried Python generator functions using choose , permutation , combination , power set , and so on , but ca n't get the right combination.20 Jan 2009This is not a homework question . This is an improved answer I was working on for www.projecteuler.net problem # 118 . I already had a slow solution but came up with a better way -- except I could not figure out how to do the spanning set.I 'll post my code when I get back from an Inauguration Party.21 Jan 2009This is the eventual algorithm I used : @ Yuval F : I know how to do a powerset . Here 's a straightforward implementation :"
"I want to create a service on Google App Engine ( Python ) that will receive a URL of an image and store it on Google Storage . I managed to upload from a local file using boto or gsutil command line , but not by retrieving the file via URL . I tried doing it using the HTTP requests ( PUT ) and I 'm getting error responses for wrong signatures . Obviously I 'm doing something wrong , but unfortunately I have no idea where . So my question is : How can I retrieve a file from a URL and store it on Google Storage using Python for Google App Angine ? Here is what I 've done ( using another answer ) : And I 'm getting as a response :"
"I 'm trying to experiment some with cluster computing on AWS . I 'm completely new at this and having some issues . I 'm trying to follow the tutorial found here : http : //star.mit.edu/cluster/docs/latest/plugins/ipython.html # using-the-ipython-cluster . I use starcluster to start a cluster instance with the following : Everything comes up as expected and it shows that the ipython plugin has loaded . I then try to execute the following command as shown in the tutorial : The connection fails , however , and tells meI am able to log in usingso I attempted to continue the tutorial logged in to the master but when I try to create the Client I receive and error : The only thing that I saw that seemed out of the ordinary is when the cluster was starting up this appeared : Any thoughts ?"
"For tf.layers.dropout ( ) the documentation for the training arg is not clear to me.The documentation states : My interpretation is that depending on if training = True or training = False the dropout will be applied . However , it 's not clear to me if True or False will apply the dropout ( ie . which is in training mode ) . Given that this is an optional argument , I expected that tf.layers.dropout ( ) would apply by default , but the default is False which intuitively training=False would suggest that the default is not training.It appears that in order for tf.layers.dropout ( ) to actually apply , one would need something like : tf.layers.dropout ( input , 0.5 , training = mode == Modes.TRAIN ) This is not very obvious to me from the documentation as training is an optional argument.Does this appear to be the correct implementation of tf.layers.dropout ? Why would the training flag just not automatically be tied to Modes.TRAIN as the default and then need to be adjusted for other cases ? The default being training=False seems to be very misleading"
"The issue I am referring to is the indentation behavior of lists and other things in Python when on two lines . The result I am looking for is for Sublime to automatically indent like this example , making the code a little prettier : But in Sublime , when you press enter after line 1 , and then type the remaining arguments , this happens : Obviously , this is n't very readable ( and uncompliant with PEP 8 style conventions ) .I Googled around and found a few unresolved threads , no solutions . Running latest version of Sublime Text 2 , on a Mac . Any help would be appreciated ."
"Is it possible to asynchronously transfer memory from/to GPU with cupy ( or chainer ) ? I 'm training a relatively small network with very large data that does not fit into the GPU memory.This data should be kept on CPU memory and provided to GPU for its minibatch calculation sequentially.The memory transfer time is the dominant bottleneck of this application.I think the asynchronous memory transfer solves this problem , i.e . during the calculation of one minibatch , another minibatch is transferred to GPU in the background.I 'm wondering it would be possible with cupy.cuda.Stream class , but I have no idea yet.I would appreciate any comments/advice.EDIT : I thought the following codes makes asynchronous memory transfer , but not.The nvvp shows the memory transfer takes place sequentially ."
"Inspired by my own answer , I did n't even understand how it worked myself , consider the following : I expected a StopIteration to be raised , since upon reaching 2 , next ( it ) would be advancing a consumed iterator . However it appears that this behavior has been completely disabled , for generator expressions only ! The generator expression seems to immediately break once this happens.Even this works ! So I guess my question is , why is this behavior enabled for generator expressions ? Note : Same behavior in 3.x"
Trying to get my django web app to execute python command line application with arguments . Not sure if this is the best way to go around it ? If its not any advice would be great . Trying to take input from user via web form and use it as the arguments to execute cmd application . Help would be fantasticThanksWilliam
"I 'm using the following function to approximate the derivative of a function at a point : As a test I 'm passing f as fx and x as 3.0 . Where fx is : Which has exp ( x ) * ( sin ( x ) +cos ( x ) ) as derivative . Now , according to Google and to my calculator exp ( 3 ) * ( sin ( 3 ) +cos ( 3 ) ) = -17.050059.So far so good . But when I decided to test the function with small values for h I got the following : Why does the error increase when h decreases ( after a certain point ) ? I was expecting the contrary until f ( x+h ) was equal to f ( x ) ."
"I 'm trying to run some old code from gaussian filter when I find out that python launcher gets stuck trying to do the imshow function.I tried : Used Matplotlib to display a graph to see if the python launcher was the problem but no , graph showed up fine.Remove process in between just to have the image read and display in fear that something in my code was breaking the launcher but no success.Reinstalled opencv-python but no success.Also saw one question like this in the google search but OP deleted it.Has anyone encounter this issue or has any fix for this ? Example code : OS : MacOS Big Sur ( 11.0.1 )"
"I 'm trying to develop a Python library which will eventually be put on PyPI . It 's a library I use in another project which pulls it from PyPI.I have unit-tests for the library in its own project repository . But I mainly test the library in use through the main application.I was previously `` publishing '' the library locally , using so that the main project in another repository can pull it from local packages and I can test it in context.But now I 'm moving to pipenv . And I want to be able to do the same . But if I put the dependency in the Pipenv file , it seems to try to pull from the real PyPI , not my local store.How do I set up this workflow with Pipenv ?"
"I 'm trying to plot some histogram using pandas . Here 's some sample code that does something similar to what I 'm doing : The data shows in a histogram fine , with one slight problem : My histogram labels at the top overlap the X-axis labels at the bottom of the next row.I would love to be able to add some spacing between these , but I have n't found any information about how to do this.How can I increase the padding between my plots using pandas and matplotlib ?"
"Greeting ! I 'm trying to make a build using PyInstaller . Config : Python 3.6.5 pip 10.0.1 , OS : Ubuntu 18.04 . Using virtualenv ( also tried with python -m venv ) .My app using an apscheduler , websocket , _thread and it seems like some related modules have import problems.Tried pyinstaller -- onefile mymain.spec & pyinstaller -- onedir mymain.spec . Problem persists in both cases . Program working without errors if not frozen.here is the error i 'm getting if i try to run the generated executable : Modules imports warnings : requirements.txt : i need to make a -- onefile build.Note the for example using hiddenimports with these : and any other modules do n't help - they still appear in log with missing module flag"
"Ok , I 've been crawling google and Django documentation for over 2 hours now ( as well as the IRC channel on freenode ) , and have n't been able to figure this one out.Basically , I have a model called Room , which is displayed below : I 've also got a forms.py which has the following ModelForm to represent my Room model : I 'm creating a view which allows administrators to edit a given Room object . Here 's my view ( so far ) : Now , this view works fine when I view the page . It shows all of the form attributes , and all of the default values are filled in ( when users do a GET ) ... EXCEPT for the default values for the ManyToMany field 'bans'.Basically , if an admins clicks on a Room object to edit , the page they go to will show all of the Rooms default values except for the 'bans ' . No matter what I do , I ca n't find a way to get Django to display the currently 'banned users ' for the Room object . Here is the line of code that needs to be changed ( from the view ) : There must be some other syntax I have to use to specify the default value for the 'bans ' field . I 've really been pulling my hair out on this one , and would definitely appreciate some help.Thanks ! UPDATElazerscience actually helped me find the solution in one of his comments . Basically , the way it works is if you pass a list of primary keys . To make it work I had to change : toAnd bam , it instantly started working ( when I view the page , it will show a selectable list of Callers , with the already banned callers already highlighted ( selected ) ."
"I am trying to hit web-service which has Thread limit , i.e . if too many requests are made within seconds it will throw exception assuming it 's an attack.To resolve same we are using retrying module from pythonHere is sample code This works however there is no messages given which indicate it 's working . Is there a parameter / way by which we could use to log number of retries made , something like waiting for 10 sec before next attempt"
"I have been working on a Python project that has grown somewhat large , and has several layers of functions . Due to some boneheaded early decisions I 'm finding that I have to go a fix a lot of crashers because the lower level functions are returning a type I did not expect in the higher level functions ( usually None ) . Before I go through and clean this up , I got to wondering what is the most pythonic way of indicating error conditions and handling them in higher functions ? What I have been doing for the most part is if a function can not complete and return its expected result , I 'll return None . This gets a little gross , as you end up having to always check for None in all the functions that call it . I guess another solution might be to throw exceptions . With that you still get a lot of code in the calling functions to handle the exception . Nothing seems super elegant . What do other people use ? In obj-c a common pattern is to return an error parameter by reference , and then the caller checks that ."
Looking for a fast vectorized function that returns the rolling number of consecutive non-zero values . The count should start over at 0 whenever encountering a zero . The result should have the same shape as the input array.Given an array like this : The function should return this :
Here is my first geodatframe : City1and my second geodataframe : City2 : i would like third dataframe with the nearest city from city1 to city2 with the distance like : Here is my actual solution using geodjango and dict ( but it 's way too long ) : Here are my tryings : here : Regards
"Working in the context of creating a django model : In factory boy we also have an analogy for this DjangoismIn rest framework v3.3.2 , I ca n't find the analogy . Is it possible ? I can write my own , something like this : But it 's a drag not to have it on the base serializer , am I missing something ?"
"I 've written a program to check if my thought about solution on paper is right ( and it is ) .The task : how many zeros is in the back of multiplication of all numbers from 10 to 200.It is 48 and it is a simple to calculate manually.I never write on python seriously and this is what I get : I bet it is possible to write the same more elegant in such language like a python.ps : yes , it is a homework , but not mine - i just helped a guy ; - )"
"I am trying to modify Brandon Rhodes code Routines that examine the internals of a CPython dictionary so that it works for CPython 3.3.I believe I have translated this struct successfully.I think the following looks good now : This line of code now works , where d == { 0 : 0 , 1 : 1 , 2 : 2 , 3 : 3 } : Here is my translation from the C struct PyDictObject :"
"I want to test my views using data from postgres localhost database ( with already loaded data ) . I 'm using tox with pytest and pytest-django.My question : How to set up / connect to local database to get all the data model schema and data itself ? Or maybe it is better to use factory_boy ? Or to load whole data from .sql script ( if yes , how ) ? Example of my test : But instead of getting status code 200 I get 404 , which points that no data is in test database . But when I lunch runserver and go to that view ( 'localhost:8000/foo/bar/123/ ' ) I will get status 200 and html webpage with some data.Please help ! I 'm using : Django==1.7.11pytest==3.0.6pytest-django==3.1.2tox==2.6.0"
"In Python 3 , Unicode strings are supposed to kindly give you the number of Unicode characters , but I ca n't figure out how to get the final display width of a string given that some characters combine.Genesis 1:1 -- בְּרֵאשִׁית , בָּרָא אֱלֹהִים , אֵת הַשָּׁמַיִם , וְאֵת הָאָרֶץBut the string is only 37 characters wide . Normalization does n't solve the problem because the vowels ( dots underneath the larger characters ) are distinct characters.As a side note : the textwrap module is totally broken in this regard , aggressively wrapping where it should n't . str.format seems similarly broken.Similar question that was marked as a duplicate : Display width of unicode strings in PythonThe question it was marked as a duplicate of only addresses normalization : Normalizing Unicode"
"Consider a typical function with default arguments : This is compact and easy to understand . But what if we have another function g that will call f , and we want to pass on some arguments of g to f ? A natural way of doing this is : The problem with this way of doing things is that the default values of the optional arguments get repeated . Usually when propagating default arguments like this , one wants the same default in the upper function ( g ) as in the lower function ( f ) , and hence any time the default changes in f one needs to go through all the functions that call it and update the defaults of any of their arguments they would propagate to f.Another way of doing this is to use a placeholder argument , and fill in its value inside the function : Now the calling function does n't need to know what f 's defaults are . But the f interface is now a bit more cumbersome , and less clear . This is the typical approach in languages without explicit default argument support , like fortran or javascript . But if one does everything this way in python , one is throwing away most of the language 's default argument support.Is there a better approach than these two ? What is the standard , pythonic way of doing this ?"
"I am writing C extension for python . All I want to do is to take size as input , create an object of that size and return the reference of that created object . My code looks like : How can I do this ? I am able to allocate a memory using PyMem_Malloc ( ) but confused about returning a reference of an object ."
"I want to check if the From header contains the email address noreply @ youtube.com : This does not add one to the count because the header actually contains 'YouTube noreply @ youtube.com'.How can I adapt my code so it adds one to the counter , checking that it contains 'noreply @ youtube.com ' anywhere in the header ?"
"If a string contains foo , replace foo with bar . Otherwise , append bar to the string . How to write this with one single re.sub ( or any other function ) call ? No conditions or other logic.For those curious , in my application I need the replacement code to be table-driven - regexes and replacements are taken from the database ."
"I 'm trying to create a command line tool ( let 's call it ' X ' ) that wraps another tool ( let 's call it ' Y ' ) .I handle some cases specially , and add some options of myself , but I want to redirect whatever I do n't want to handle to the tool Y.So far I managed to redirect the arguments that do n't come with dashes , for example X Y option1 option2 option3 will just call Y option1 option2 option3 . I did this by adding a subparser Y and an argument any to itHere 's the code ( x.py ) : Then in my handler code , i do this : When I call python x.py y asdf zxcv qwer it works.When I call python x.py y asdf zxcv qwer -option I get the error x.py : error : unrecognized arguments : -optionI realize that if stuff just gets too complicated with argparse i can always fall back to using sys.argv , but if you know this to be doable , please share.I 've also been looking through the argparse code , which is a little dense , and where it seems that ArgumentParser._parse_known_args does everything ( 300 lines ) . But before I go deeper , I thought maybe someone knows how to do this - if not , i 'll post my discoveries here if someone else has the same problem ."
I 'm using python regex library to parse some strings and currently I found that my regex is either too complicated or the string I 'm searching is too long . Here 's an example of the hang up : I 'm not sure what 's going on . Any help appreciated ! EDIT : Here 's a link with examples of what I 'm trying to match : Regxr
"Defining a font inside a function and in the main body of the script seems to behave differently , and I ca n't seem to figure out how it 's supposed to work.For example , the Label in this example ends up being in a larger font , as expected : However , if I try to define styles inside a function , it seems like the font gets deleted or garbage collected and is not available by the time the widget needs to use it : Printing out tkFont.names ( ) in the first version just before the main_frame.pack ( ) lists the custom font as font < id > , but printing the same in the second version does not list the custom font outside the define_styles function . Do I have to do something special to save them ? Why ca n't I put that code in a function ? Am I fundamentally misunderstanding something about how Fonts are supposed to be used ? tkFont seems to have some kind of font registry , why are n't mine sticking around ?"
"I was reading a presentation on Pythons ' Object model when , in one slide ( number 9 ) , the author asserts that Pythons ' functions are descriptors . The example he presents to illustrate is similar to this one I wrote : Now , I understand that the point is made , since the function defines a __get__ it is a descriptor as I described in the description section of the Python documentation . What I do n't understand is how exactly the call results in the output provided ."
"I 'm comparing two time objects with different timezones , and looks like it 's actually ignoring the timezone , testing only the hour/minute/second components.Let 's create two time objects : Printing them , we see that they 're pretty different : Now , let 's compare them : Ehm , what ? How is it possible that Python treats them equal ?"
"I am running my neural network on ubuntu 16.04 , with 1 GPU ( GTX 1070 ) and 4 CPUs.My dataset contains around 35,000 images , but the dataset is not balanced : class 0 has 90 % , and class 1,2,3,4 share the other 10 % . Therefore I over-sample class 1-4 by using dataset.repeat ( class_weight ) [ I also use a function to apply random augmentation ] , and then concatenate them.The re-sampling strategy is : 1 ) At the very beginning , class_weight [ n ] will be set to a large number so that each class will have the same amount of images as class 0 . 2 ) As the training goes , number of epochs increases , and the weights will drop according to the epoch number , so that the distribution becomes closer to the actual distribution . Because my class_weight will vary epoch by epoch , I ca n't shuffle the whole dataset at the very beginning . Instead , I have to take in data class by class , and shuffle the whole dataset after I concatenate the over-sampled data from each class . And , in order to achieve balanced batches , I have to element-wise shuffle the whole dataset . The following is part of my code . To make it clear , let me describe why I am facing this issue step by step : Because classes in my dataset are not balanced , I want to over-sample those minority classes.Because of 1. , I want to apply random augmentation on those classes and concatenate the majority class ( class 0 ) with them . After doing some research , I find that repeat ( ) will generate different results if there is a random function in it , so I use repeat ( ) along with my_random_augmentation_func to achieve 2.Now , having achieved 2. , I want to combine all the datasets , so I use concatenate ( ) After 4 . I am now facing an issue : there are around 40,000 - 180,000 images in total ( because class_weight changes epoch by epoch , at the beginning there will be 180,000 images in total , and finally there will be about 40,000 ) , and they are concatenated class by class , the dataset will look like [ 0000-1111-2222-3333-4444 ] , therefore with batch size 100 , without any shuffling , there will almost always be only one class in each batch , which means the distribution in each batch will be imbalanced.In order to solve the `` imbalanced batch '' issue in 5. , I come up with the idea that I should shuffle the whole dataset , thus I use shuffle ( 180000 ) .And finally , boom , my computer freeze when it comes to shuffle 180000 items in the dataset.So , is there a better way that I can get balanced batches , but still keep the characteristics I want ( e.g . changing distribution epoch by epoch ) ? -- - Edit : Issue solved -- -It turned out that I should not apply the map function at the beginning . I should just take in the filenames instead of the real files , and just shuffle the filenames , then map them to the real files.More detailedly , delete the map ( _parse_csv_train ) part after d0 = tf.data.TextLineDataset ( train_csv_0 ) and other 4 lines , and add a new line dataset = dataset.map ( _parse_csv_train ) after shuffle ( 180000 ) .I also want to say thank you to @ P-Gn , the blog link in his `` shuffling '' part is really helpful . It answered a question that was in my mind but I did n't ask : `` Can I get similar randomness by using many small shuffles v.s . one large shuffle ? '' ( I 'm not gon na give an answer here , check that blog ! ) The method in that blog might also be a potential solution to this issue , but I have n't tried it out ."
"In Matlab , the builtin isequal does a check if two arrays are equal . If they are not equal , this might be very fast , as the implementation presumably stops checking as soon as there is a difference : Is there any equavalent in Python/numpy ? all ( A==B ) or all ( equal ( A , B ) ) is far slower , because it compares all elements , even if the initial one differs : Is there any numpy equivalent ? It should be very easy to implement in C , but slow to implement in Python because this is a case where we do not want to broadcast , so it would require an explicit loop.Edit : It appears array_equal does what I want . However , it is not faster than all ( A==B ) , because it 's not a built-in , but just a short Python function doing A==B . So it does not meet my need for a fast check ."
"Four-way logarithmic plot is a very often used graph for vibration control and earthquake protection . I am quite interesting in how this plot can be plotted in Matplotlib instead of adding axes in Inkscape . A sample of Four-way logarithmic plot is here.A quick and dirty Python code can generate main part of the figure , but I can not add the two axes onto the figure . http : //matplotlib.org/examples/axes_grid/demo_curvelinear_grid.html provides an example of adding axes , but I fails to make it working . Anyone has similar experience on adding axes to Matplotlib figure ? Update : Thank you all ! I use Inkscape to modify the figure above . I think the result is just fine . However , I am still looking for methods to draw this figure in Matplotlib ."
"I want the most Pythonic way to round numbers just like Javascript does ( through Math.round ( ) ) . They 're actually slightly different , but this difference can make huge difference for my application.Using round ( ) method from Python 3 : Using Math.round ( ) method from Javascript* : Thank you ! References : Math.round ( ) explanation at Mozilla Developers Network ( MDN )"
It appears that Mock.call_count does not work correctly with threads . For instance : This code produced the following output : Is there a way I can use call_count ( or similar ) within a multithreaded portion of code ? I 'd like to avoid having to rewrite MagicMock myself ...
I have a dataframe : How do I stack all levels of the MultiIndex without knowing how many levels columns has.I expect the results to look like this :
"Since this question is about inheritance and super , let 's begin by writing a class . Here 's a simple everyday class that represents a person : Like every good class should , it calls its parent constructor before initializing itself . And this class does its job perfectly well ; it can be used with no problems : But when I try to make a class that inherits from both Person and another class , things suddenly go wrong : Because of diamond inheritance ( with the object class at the top ) , it 's not possible to initialize Centaur instances correctly . The super ( ) .__init__ ( ) in Person ends up calling Horse.__init__ , which throws an exception because the fur_color argument is missing.But this problem would n't exist if Person and Horse did n't call super ( ) .__init__ ( ) .This raises the question : Should classes that inherit directly from object call super ( ) .__init__ ( ) ? If yes , how would you correctly initialize Centaur ? Disclaimer : I know what super does , how the MRO works , and how super interacts with multiple inheritance . I understand what 's causing this error . I just do n't know what the correct way to avoid the error is.Why am I asking specifically about object even though diamond inheritance can occur with other classes as well ? That 's because object has a special place in python 's type hierarchy - it sits at the top of your MRO whether you like it or not . Usually diamond inheritance happens only when you deliberately inherit from a certain base class for the goal of achieving a certain goal related to that class . In that case , diamond inheritance is to be expected . But if the class at the top of the diamond is object , chances are that your two parent classes are completely unrelated and have two completely different interfaces , so there 's a higher chance of things going wrong ."
"I have a cloud of points in three-dimensional space , and have estimated some distribution over those points ( also in 3D space ; using kernel density estimation although that 's irrelevant for this question ) . I would like to plot the projection of that distribution as a contour plot onto all three axes ( x , y , and z ) . It is straightforward to do this for the z-axis ( i.e . project onto plane with same z-coordinate everywhere ) : However , doing this for the other axes seems to be not implemented in Matplotlib . If I use the method outlined in this example , and specify a zdir keyword argument : the generation of the contour is done 'along another slice ' , so to say : Whereas I want something like this ( bad Paint skills ; hope the idea is clear ) : One option I had in mind was to generate the contour along the default zdir= ' z ' and then rotate the resulting curves in 3D space , but I have no idea how to approach this . I would be very grateful for any pointers !"
"I have created trial account on twilio , and installed twilio using on ubuntu 14.04 LTS.below is my python code to send the smsWhile running this python file I am getting below log error from twilio.Traceback ( most recent call last ) : File `` /home/software/ws/extra/scripts/test.py '' , line 40 , in from_= '' +1MY_TWILIO_NUMBER '' ) File `` /usr/local/lib/python2.7/dist-packages/twilio/rest/resources/sms_messages.py '' , line 167 , in create return self.create_instance ( kwargs ) File `` /usr/local/lib/python2.7/dist-packages/twilio/rest/resources/base.py '' , line 365 , in create_instance data=transform_params ( body ) ) File `` /usr/local/lib/python2.7/dist-packages/twilio/rest/resources/base.py '' , line 200 , in request resp = make_twilio_request ( method , uri , auth=self.auth , **kwargs ) File `` /usr/local/lib/python2.7/dist-packages/twilio/rest/resources/base.py '' , line 164 , in make_twilio_request uri=resp.url , msg=message , code=code ) twilio.rest.exceptions.TwilioRestException : HTTP 400 error : The From phone number +1MY_TWILIO_NUMBER is not a valid , SMS-capable inbound phone number or short code for your account.I have checked my number has Capabilities for Voice , SMS & MMS.I have checked this error on here . then i tried with +15005550006 number , It is running but the i never receive the sms on my cell phone.the message went in queue which never processes.What I am missing ? why I am not getting the sms ?"
"I just switched to Python from Matlab , and I want to use lambda function to map function f1 ( x , y ) with multiple arguments to one argument function f2 ( x ) for optimization . I want that when I map the function f2 ( x ) < - f1 ( x , y=y1 ) then y will stay constant no matter what y1 changes , in Matlab this is true by default but if I try in Python , it keeps changing as the following examplesI expect f2 ( 1 ) stays 3 even if I change y1 , however if I change y1 , the whole f1 ( 1 ) also changes as followsI wonder is there a way that when I declare f2 = lambda x : f1 ( x , y1 ) then f1 will take the value of y1 at that time and fix it to f2 . The reason for this because I want to dynamically create different functions for different scenarios then sum them all.I 'm still new to Python , please help , much appreciate ."
"I 'm trying to use wav2vec to train my own Automatic Speech Recognition System : https : //github.com/pytorch/fairseq/tree/master/examples/wav2vecFirst of all how can I use a loaded model to return predictions from a wav file ? Second , how can I pre-train using annotated data ? I do n't see any text mention in the manifest scripts ."
in windows file explorer create a new txt file name it Ń.txt ( note the accent over the N ) .hold shift and right click the folder where you created Ń.txt and select open command window here ( or alternatively open cmd.exe and cd into the directory where you created the filerun python terminalI have tried many decodings but os.listdir is not returning the bytestring of the actual filename at all ... so encoding/decoding the incorrect bytes is still the incorrect bytes ...
"So I 'm trying to implement my own layer in Keras , using the provided example : I noticed that what 's in call gets called at build time when I do : This happens without calling fit , train , predict , etc ... Why ?"
I 'm doing a work for school in which I 've got to implement rsa generating of public/private keys and encryption/decryption of a binary message . I already generate public/private keys but my encrypt/decrypt functions are n't working . I do n't get any execution errors but the message I encrypt is n't the same when I decrypt.My code for encrypting a block : My encryption code : And my decryption method : So for example if my message is : I get this encrypted message ( with 64 or plus bits for key size ) : And when I decrypt I get this : Does anyone have any idea why this is n't working ?
"How to make all forms in django formset required ? I tried to validate presence of all fields in cleaned_data overriding formset 's clean ( ) method but it just fails silently without any error displayed.Thanks ! Source code : There are 7 forms each for one day and all of them must be filled to be valid . In example above I just tried to raise error in clean . is_valid ( ) became False , but no errors were displayed ."
"What on earth am I doing wrong ? I have recently found an awesome django template called django-skel . I started a project with it because it made it very easy to use heroku with django . It was all going great until I tried to get celery working . No matter what I tried I could n't get my tasks to run . So I started a new bare bones app just to see if I could get it working without any of my other craziness preventing things.This is my bare-bones app . I have this up and running on heroku . Django admin is working , I have my databases sync 'd up and migrated . I am using CloudAMQP Little Lemur for my RabbitMQ . I see the requests queued up in the RabbitMQ interface , nothing happens . How I queue up the tasks is manually run in the shell : I make sure that I have all 3 dynos running , and still nothing . Also I have it working locally.I am sure there are tons of questions , and I 'm willing to give them . Just please ask ."
"I 'm overring the __new__ ( ) method of a class to return a class instance which has a specific __init__ ( ) set . Python seems to call the class-provided __init__ ( ) method instead of the instance-specific method , although the Python documentation athttp : //docs.python.org/reference/datamodel.htmlsays : Typical implementations create a new instance of the class by invoking the superclass ’ s __new__ ( ) method using super ( currentclass , cls ) .__new__ ( cls [ , ... ] ) with appropriate arguments and then modifying the newly-created instance as necessary before returning it . If __new__ ( ) returns an instance of cls , then the new instance ’ s __init__ ( ) method will be invoked like __init__ ( self [ , ... ] ) , where self is the new instance and the remaining arguments are the same as were passed to __new__ ( ) .Here 's my test code : which outputsIt seems the only way to get myinit ( ) to run , is to call it explicitly as self.__init__ ( ) inside myclass.__init__ ( ) ."
I 'm wondering where to configure and initialize stuff related with logging module ? For example I writing some class and I want to log some info while method will be executed . Should I configure logging in init or above class on the top of module : What is the best practice ?
"I try to make my code fool-proof , but I 've noticed that it takes a lot of time to type things out and it takes more time to read the code.Instead of : I tend to write this : Is my python coding style too paranoid ? Or is there a way to improve readability while keeping it fool-proof ? Note 1 : I 've added the comments between < and > just for clarification.Note 2 : The main fool I try to prevent to abuse my code is my future self ."
"Now I 'm creating a web app which allows user to upload images using Django Rest Framework.I 'd like to rotate those images according to EXIF tag and save.At first , I found this way , and it works in local environment.But , now I use Amazon S3 for deployment , then this way does n't work.So , I 'm trying to rotate image before save and struggling ... Recent code is below , raising TypeError at /api/work/ 'str ' object is not callable when I post new Work object.How to fix it ? Or is there other nice way ? [ models.py ] [ serializers.py ]"
"I have 3 numpy arrays and need to form the cartesian product between them . Dimensions of the arrays are not fixed , so they can take different values , one example could be A= ( 10000 , 50 ) , B= ( 40 , 50 ) , C= ( 10000,50 ) .Then , I perform some processing ( like a+b-c ) Below is the function that I am using for the product.The output is the same as with itertools.product . However , I am using my custom function to take advantage of numpy vectorized operations , which is working fine compared to itertools.product in my case . After this , I do So this is the final expected result.The function works as expected as long as my array fits in memory . But my usecase requires me to work with huge data and I get a MemoryError at the line np.empty ( ) since it is unable to allocate the memory required.I am working with circa 20GB data at the moment and this might increase in future . These arrays represent vectors and will have to be stored in float , so I can not use int . Also , they are dense arrays , so using sparse is not an option.I will be using these arrays for further processing and ideally I would not like to store them in files at this stage . So memmap / h5py format may not help , although I am not sure of this.If there are other ways to form this product , that would be okay too.As I am sure there are applications with way larger datasets than this , I hope someone has encountered such issues before and would like to know how to handle this issue . Please help ."
"Code : Output : Doc : Several built-in types such as list and dict do not directly support weak references but can add support through subclassing : ... Other built-in types such as tuple and int do not support weak references even when subclassed ( This is an implementation detail and may be different across various Python implementations . ) .This is n't expressive enough to explain : Why some built-in types do n't support weak references ? What are exactly those types that support weak references ? To add some thoughts : For the above example you can wrap the int within a user-defined wrapper class , and that wrapper class supports weak references ( Those who are familiar with Java will recall int and Integer ) : I 'm not sure why Python does n't provide auto-wrapping for commonly used built-in types ( int , str , etc . ) but instead simply say they do n't support weak references . It might be due to performance issues , but not being able to weakref these built-in types greatly reduced its usage ."
"I need to create a wide table , like the example below , that will span the whole pdf page after it 's rendered in latex.The problem I 'm having is that the column header text is not wrapping to fit the width of the column.Shows up like this : I 've tried adjusting column widths using the .. tabularcolumns : : |p { 0.1 \textwidth| ... directive , but it did n't seem to fix the encroaching header text problem.If I remove the `` Common column Header 3 '' ( second example below ) I get the desired wrapping header text behavior , so I 'm guessing I 'm doing something wrong with that part : Looks like this : Any help would be greatly appreciated !"
"I manage all my static files with Grunt in my Django project . I have setup a gruntfile for my project that gets all js and css files and then concats and minifies them into one file . Those files are copied to a directory named /static/source and in my settings I have configured static files like this : All this works fine and I do n't have any problem with this . The problem is when I try to use the admin app for example . If I add to STATICFILES_FINDERS the django.contrib.staticfiles.finders.AppDirectoriesFinder , when I do the collectatic process , it also copies all static files from my apps ( the source files which I have concatenated and minified with Grunt ) . Is there any way to collect the static files only for a certain app ?"
"I have a list : And I want to change the first element to the list , so a list inside a list.My first try : gives me my second trygives meAlthough I want"
"I am trying to find a workaround to the following problem . I have seen it quasi-described in this SO question , yet not really answered . The following code fails , starting with a fresh graph : In short , I wish , in one batch transaction , to update existing indexed node properties . The failure is occurring at the batch.set_properties line , and it is because the BatchRequest object returned by the previous line is not being interpreted as a valid node . Though not entirely indentical , it feels like I am attempting something like the answer posted hereSome specificsUpdateIf I split the problem into separate batches , then it can run without error : This works for many nodes as well . Though I do not love the idea of splitting the batch up , this might be the only way at the moment . Anyone have some comments on this ?"
"Let say I have 3 classes : A , B and C. A is a base class for B and B is for C. Hierarchy is kept normally here , but for one method it should be different . For C class it should act like it was inherited from A.For example like this : So basically it should work like this : But it is not going to work for C class , because I get this error : To solve this for C class I could inherit from class A , but I want to inherit everything from B and for that specific method m call super for base class A. I mean that method is one exception . Or should I call it somehow differently for class C in order to work ? How can I do that ?"
"I 'm trying to find out , at runtime , where an object has been instantiated , as this would enable providing a very useful error message to users of my library.Suppose we have the following code : obj is then passed to an instance of another class from mylib , and proceeds on a wonderful journey . Somewhere along the line , obj causes something bad to happen , and I 'd like to point the user to where obj was instantiated.I was hoping I could use the inspect module to find out in which file and at what line number obj was instantiated . Unfortunately , the inspect.getsourcefile and inspect.getsourcelines do not support instances . Is there a technical reason why this is not supported ? Is there another way I can obtain the data I 'm looking for ?"
This is my code to save web_cam streaming . It is working but the problem with output video file .
"I am currently running some endless tasks using asyncio.waitI need a special function to run when all the others are on awaitHow can I get the special_function to only be run when all main_tasks are on await ? Edit : What I mean by `` all main_tasks are on await '' : all main_tasks are not ready to continue , e.g . are in asyncio.sleep ( 100 ) or I/O bound and still waiting for data.Therefore the main_tasks can not continue and the event loop runs the special_function while the tasks are in this state , NOT every iteration of the event loop.Edit 2 : My use case : The main_tasks are updating a data structure with new data from web-sockets.The special_function transfers that data to another process upon an update signal from that process . ( multiprocessing with shared variables and data structures ) It needs to be the most up to date data it can be when it transfers , there can not be pending updates from main_tasks.This is why I only want to run special_function when there are no main_tasks with new data available to be processed . ( i.e . all waiting on await )"
"I would like to create vectors of NumPy datetime64 objects from 1-D vectors of years , months , and days , and also go the reverse direction , that is extracting vectors of years , months , or days from a daily datetime64 vector . I 'm using NumPy 1.7.0b2.For example , supposeNow I want to create a np.datetime64 vector of length 4 using these years , months , and days . Is there a way without using a Python loop ? Going the other direction , suppose dates is a vector of datatype np.datetime64 and the frequency is daily . Then I would to be able to something like x.DAYS ( ) and get back a vector [ 3 , 20 , 14 , 27 ] ."
"I have a 4x3 boolean numpy array , and I 'm trying to return a same-sized array which is all False , except for the location of the first True value on each row of the original . So if I have a starting array of then I 'd like to returnso indices 1 , 0 and 2 on the first three rows have been set to True and nothing else . Essentially any True value ( beyond the first on each row ) from the original way have been set to False.I 've been fiddling around with this with np.where and np.argmax and I have n't yet found a good solution - any help gratefully received . This needs to run many , many times so I 'd like to avoid iterating ."
First of all python is an awesome language . This is my first project using python and I 've made a ridiculous amount of progress already.There 's no way that this code below is the best way to do this . What 's the most idiomatic way write a class definition ? Thanks !
"I think ( hope ) this question differs substantially from What is the equivalent of `` zip ( ) '' in Python 's numpy ? , although it might just be my ignorance.Let 's say I have the following : and I want to turn it intoIn python I can do : But how can I do this with numpy arrays ( without using zip ( * ) ) ?"
Is there any difference between the following ?
"Suppose I have some kind of context manager ( from a third-party library ) that I am using like so : But , suppose if there is no value for test_dt , the context manager should not run , but all of the remaining code should run , like so : Assume that lines_of_code here is 2-3 lines of code which are exactly identical , is there a cleaner way of writing this ? I 'm aware that I could write something like this : But I 'm not crazy about this formatting . Also , I do n't want to have to litter this pattern all over my code.There is one final possibility , but I 'm not certain it will work : subclassing the context manager and skipping the __enter__ and __exit__ functions if the test_dt given is empty , like so : I tested it out with a blank context manager class , and it seemed to behave correctly . However , I 'm worried whether a real context manager will behave correctly if I do this ( I 'm not very familiar with the internals of how it works ) ."
"So lists are unhashable : The following page gives an explanation : A list is a mutable type , and can not be used as a key in a dictionary ( it could change in-place making the key no longer locatable in the internal hash table of the dictionary ) .I understand why it is undesirable to use mutable objects as dictionary keys . However , Python raises the same exception even when I am simply trying to hash a list ( independently of dictionary creation ) Does Python do this as a guarantee that mutable types will never be used as dictionary keys ? Or is there another reason that makes mutable objects impossible to hash , regardless of how I plan to use them ?"
"The Google AppEngine NDB Documentation for map ( ) states that : `` All query options keyword arguments are supported . `` However , I have tried to use produces_cursors=True on map ( ) and I 'm not getting a cursor back.I 'd like to use map ( ) as I can set the callback to a tasklet.https : //developers.google.com/appengine/docs/python/ndb/queryclass # kwdargs_optionsEdit - Providing code sample :"
"I have a shared python library that I use in multiple projects , so the structure looks like this : Now in each project 's main.py I use the following hack to make it work : Is there a way to do it without using this hack ? Or is there a better way to organize the projects structure ?"
"In numpy , why does subtraction of integers sometimes produce floating point numbers ? This seems to only occur when using multiple different integer types ( e.g . signed and unsigned ) , and when no larger integer type is available ."
"I have 3D numpy array and I want only unique 2D-sub-arrays.Input : Output : I tried convert sub-arrays to string ( tostring ( ) method ) and then use np.unique , but after transform to numpy array , it deleted last bytes of \x00 , so I ca n't transform it back with np.fromstring ( ) .Example : Output : I also tried view , but I realy do n't know how to use it.Can you help me please ?"
"I have a web scraper in Scrapy that gets data items . I want to asynchronously insert them into a database as well . For example , I have a transaction that inserts some items into my db using SQLAlchemy Core : I understand that it 's possible to use SQLAlchemy Core asynchronously with Twisted with alchimia . The documentation code example for alchimia is below.What I do n't understand is how can I use my above code in the alchimia framework . How can I set up process_item to use a reactor ? Can I do something like this ? How do I write the reactor part ? Or is there an easier way to do nonblocking database insertions in a Scrapy pipeline ? For reference , here is the code example from alchimia 's documentation :"
"Consider the following piece of code , which generates some ( potentially ) huge , multi-dimensional array and performs numpy.tensordot with it ( whether we multiply the same or two different arrays here , does not really matter ) .Now we can compare the performance for different data types , e.g . : Since the array only consists of small integer numbers , I would like to save some memory by using dtype=int8 . However , this slows down the matrix multiplication A LOT.Here are some test casesThe first one , is the important one for my use case . The others are just for reference.Using Numpy 1.13.1 and Python 3.4.2Large arraySame array with different data types . Memory decreases as expected . But why the large differences in the CPU time ? If anything I would expect int to be faster than float.Large array with different shapeThe shape does n't seem to have a large effect.Not so large arraySmaller values , but same weird trend.small array , lots of repetitionsL , N = 2 , 4 ; A.size = 4**4 = 256 ; repeat=1000000Other than float16 being much slower , everything is fine here.QuestionWhy is int8 for very large arrays so much slower ? Is there any way around this ? Saving memory becomes increasingly important for larger arrays !"
"Please run the following codeThe output of the code is [ 2*CRootOf ( 109*x**5 - 4157*x**4 + 50498*x**3 - 184552*x**2 - 527136*x + 3507840 , 0 ) , 2*CRootOf ( 109*x**5 - 4157*x**4 + 50498*x**3 - 184552*x**2 - 527136*x + 3507840 , 1 ) , 2*CRootOf ( 109*x**5 - 4157*x**4 + 50498*x**3 - 184552*x**2 - 527136*x + 3507840 , 2 ) , 2*CRootOf ( 109*x**5 - 4157*x**4 + 50498*x**3 - 184552*x**2 - 527136*x + 3507840 , 3 ) , 2*CRootOf ( 109*x**5 - 4157*x**4 + 50498*x**3 - 184552*x**2 - 527136*x + 3507840 , 4 ) ] Can someone explain what the answer represent and how to get the output in conventional form i.e . say if the answer is 0.1,0.2,0.3,0.1,0.4 sympy usually outputs the answer as [ 0.1,0.2,0.3,0.1,0.4 ]"
"I am an aerospace student working on a school project for our python programming course . The assignment is create a program only using Pygame and numpy . I decided to create a wind tunnel simulation that simulates the airflow over a two dimensional wing . I was wondering if there is a more efficient way of doing the computation from a programming perspective . I will explain the program : I have attached an image here : The ( steady ) flow field is modeled using the vortex panel method . Basically , I am using a grid of Nx times Ny points where at each point a velocity ( u , v ) vector is given . Then using Pygame I map these grid points as circles , so that they resemble an area of influence . The grid points are the grey circles in the following image : I create N particles and determine their velocities by iterating as follows : create a list of particles.create a grid list.for each gridpoint in grid list : for each particle in list of particles : if particle A is within the area of influence of grid point n ( xn , yn ) : particle A its velocity = velocity at grid point n.Visualize everything in Pygame.this basic way was the only way I could think of visualizing the flow in Pygame . The simulation works pretty well , but If I increase the number of grid points ( increase the accuracy of the flow field ) , the performance decreases . My question is if there is a more efficient way to do this just using pygame and numpy ? I have attached the code here : The code requires another script that computes the flow field itself . It also reads datapoints from a textfile to get the geometry of the wing.I have not provided these two files , but I can add them if necessary . Thank you in advance ."
"Is there any reason why pandas changes the type of columns from int to float in update , and can I prevent it from doing it ? Here is some example code of the problem"
"I ca n't figure out why this program is failing.I used Boost.Python to create libui , which exposes the class QPoint . I aso included PyQt4 , which has a sip-exposed QPoint . I 'm trying to accomplish a mapping between the two types.I checked that p is a new-style class , so why is n't __getattr__ being called for p.x ( ) ?"
"I 'm bulding an app with Flask and Angular hosted on heroku . I have a problem with migrating heroku postgresql . I 'm using flask-migrate which is a tiny wrapper around alembic . Locally everything is fine . I got an exception when I run heroku run upgrade which runs alembic upgrade command . Simply , alembic is trying to run from the first migration which creates the db . I tried to set explicitly the proper revision with heroku run python manage.py db upgrade +2 or revision number but the exception is the same.My guess was that due to ephemeral filesystem of Heroku revision in not stored , but that should n't be a problem if I explicitly set the revision , right ? : ) How can I set the current head of revision ? Here is relevant code : Procfile : models.py"
"In Python 2.4 , you can pass a custom comparer to sort.Let 's take the list -To sort with the even numbers first , and then odds , we can do the following -In Python 3 , you can only pass key ( which is also supported in Python 2.4 ) .Of course , the same sorting can be achieved in Python 3 with the right key : I am curious about the decision of not supporting custom comparers anymore , especially when it seems something that could be implemented easily enough.Is it true that in all , or most of the cases , a desired sort order has a natural key ? In the example above for example , such a key exists - and actually the code becomes more succinct using it . Is it always the case ? ( I am aware of this recipe for converting comparer to key , but ideally , one should not have to take such workarounds if it could be built into the language . )"
"How can I find all the abstract base classes that a given class is a `` virtual subclass '' of ? In other words , I 'm looking for a magic function virtual_base_classes ( ) that does something like this : ( I do n't know all the abc classes that list is registered with , so the above example may not be complete . ) Note that not every abstract base class will be defined in collections.abc . There is a module abc ( distinct from collections.abc ) which provides the metaclass ABCMeta . Any class that is an instance of ABCMeta supports registration of `` virtual subclasses '' using the standard interface ( the register method ) . There 's nothing that stops someone ( whether a programmer or Python library ) from creating an instance of ABCMeta that does not belong in collections.abc ."
"My project has the following structure : Here is a.py in part : Here is tb.py in part : When run directly , a.py produces no errors , and it is easy to verify there were no SyntaxErrors . However , running tb.py causes the following error : How should I rewrite the import of a from tb so that tb can be run directly without causing errors ?"
"I use Python 's lru_cache on a function which returns a mutable object , like so : If I call this function , mutate the result and call it again , I do not obtain a `` fresh '' , unmutated object : I get why this happens , but it 's not what I want . A fix would be to leave the caller in charge of using list.copy : However I would like to fix this inside f. A pretty solution would be something likethough no copy argument is actually taken by functools.lru_cache.Any suggestion as to how to best implement this behavior ? EditBased on the answer from holdenweb , this is my final implementation . It behaves exactly like the builtin functools.lru_cache by default , and extends it with the copying behavior when copy=True is supplied ."
This might be a subquestion of Passing arguments to process.crawl in Scrapy python but the author marked the answer ( that does n't answer the subquestion i 'm asking myself ) as a satisfying one.Here 's my problem : I can not use scrapy crawl mySpider -a start_urls ( myUrl ) -o myData.jsonInstead i want/need to use crawlerProcess.crawl ( spider ) I have already figured out several way to pass the arguments ( and anyway it is answered in the question I linked ) but i ca n't grasp how i am supposed to tell it to dump the data into myData.json ... the -o myData.json partAnyone got a suggestion ? Or am I just not understanding how it is supposed to work.. ? Here is the code :
"Background : I need to run automatic tasks every first and third monday of the month for a server.This should be realised via python not crontab.I found the python module `` schedule '' but its documentation is not detailed.https : //pypi.org/project/schedule/https : //schedule.readthedocs.io/en/stable/Does anybody know how to do this ? Will this be executed on the first monday of the year , month or every monday ?"
"I have a huge dataset on which I wish to PCA . I am limited by RAM and computational efficency of PCA . Therefore , I shifted to using Iterative PCA.Dataset Size- ( 140000,3504 ) The documentation states that This algorithm has constant memory complexity , on the order of batch_size , enabling use of np.memmap files without loading the entire file into memory.This is really good , but unsure on how take advantage of this.I tried load one memmap hoping it would access it in chunks but my RAM blew.My code below ends up using a lot of RAM : When I say `` my RAM blew '' , the Traceback I see is : How can I improve this without comprising on accuracy by reducing the batch-size ? My ideas to diagnose : I looked at the sklearn source code and in the fit ( ) function Source Code I can see the following . This makes sense to me , but I am still unsure about what is wrong in my case . Edit : Worst case scenario I will have to write my own code for iterativePCA which batch processes by reading and closing .npy files . But that would defeat the purpose of taking advantage of already present hack . Edit2 : If somehow I could delete a batch of processed memmap file . It would make much sense.Edit3 : Ideally if IncrementalPCA.fit ( ) is just using batches it should not crash my RAM . Posting the whole code , just to make sure I am not making a mistake in flushing the memmap completely to disk before.Surprisingly , I note out.flush does n't free my memory . Is there a way to using del out to free my memory completely and then someone pass a pointer of the file to IncrementalPCA.fit ( ) ."
"I 'm parsing a log with python and need quick fetch some values from itthis is the simple equivalent regex and usage exampleIt does n't work as expected , only the last match group is returned by pat.match ( ) .groups ( ) What is the simplest solution for such problems ? updated ( as wiki engine says to use edit rather than creating new post ) : I need repeated matches , of course.regex find need to be applyed twice recursively . I can bear it , but is there any options ?"
"I 've produced a python egg using setuptools and would like to access it 's metadata at runtime . I currently got working this : but this would probably work incorrectly if I had multiple versions of the same egg installed . And if I have both installed egg and development version , then running this code from development version would pick up version of the installed egg . So , how do I get metadata for my egg not some random matching egg installed on my system ?"
"I have some really weird behaviour that I just do n't understand and therefore can not explain , so I hope someone here can help me out . First thing I noticed was ipdb not letting me define variables any more : And a bit later I found ipdb returning this on my input ( after running the code again ) : To me this looks as I have two interleaving debug sessions , which I have access to in some strange alternating pattern . How can I get rid of that ? edit : Killing all python processes and re-running the code did help . Everything is back to normal now . But as I just do n't understand what was going on , I would be very interested in an answer to what happened , and how to reproduce the behaviour ."
"Given a Python list , I want to remove consecutive 'duplicates ' . The duplicate value however is a attribute of the list item ( In this example , the tuple 's first element ) .Input : Desired Output : Can not use set or dict , because order is important. Can not use list comprehension [ x for x in somelist if not determine ( x ) ] , because the check depends on predecessor.What I want is something like : What is the preferred way to solve this in Python ?"
"I am trying to scrape the title and price of a product . I am facing a problem where the website has a class that varies . This is an example , When i use another computer , it then shows this instead , I realized that they change their classes to a random letter . I am currently using BeautifulSoup4 and requests library.Are there any ways to get the class , other than the thought of making a whole long `` if-elif '' classes ? The website I am trying to scrape is carousell.comI am currently using an lxml parser , if that would be of any help . Thank you for your time ."
"This awesome code , shows memory leak in tornado 's gen module , when connections are closed without reading the response : and now , run a simple test client , multiple timesNow , server output shows , incremental memory usage : If you set CHUNK_COUNT to 1 , the 10KB of data can be written to OS connection buffer , and 'finished ' and 'finally ' texts will be printed to console , and because generator is completed , no memory leak occurs.But the strange part is that if your remove the try/finally block , the problem disappears ! ! ( even with CHUNK_COUNT set to 100 ) Is this a bug on CPython or tornado or ... ? !"
"I have a large number of cross-correlations to calculate and I 'm looking for the fastest way to do it . I 'm assuming vectorizing the problem would help rather than doing it with loopsI have a 3D array labelled as electrode x timepoint x trial ( shape : 64x256x913 ) . I want to calculate the max cross-correlation of the timepoints for every pair of electrodes , for every trial.Specifically : for every trial , I want to take each of the pair combination of electrodes and calculate the max cross-correlation value for every pair . That will result in 4096 ( 64*64 ) max cross-correlation values in a single row/vector . That will be done for every trial , stacking each of the rows/vectors on top of each other resulting in a final 2D array of shape 913*4096 containing max cross-correlation valuesThats a lot of computation but I want to try to find the fastest method to do it . I mocked up some proto-code using lists as containers that may help explain the problem a bit better . There may be some logic errors in there , but either way the code does n't run on my computer because theres so much to calculate that python just freezes . Here is it : Obviously loops are really slow for this type of thing . Is there a vectorized solution to this using numpy arrays ? I 've checked out things like correlate2d ( ) and the like , but I do n't think they really work in my case as I 'm not multiplying 2 matrices together"
"I 'm working in a python library that is going to be used by Brazilians , the official speak language here is Portuguese ! Basically , I want to get the Exception messages in portuguese , for example : Instead of get ... I wan na get this ... Is there any way to do it ?"
"I 'm currently using a basic LSTM to make regression predictions and I would like to implement a causal CNN as it should be computationally more efficient.I 'm struggling to figure out how to reshape my current data to fit the causal CNN cell and represent the same data/timestep relationship as well as what the dilation rate should be set at.My current data is of this shape : ( number of examples , lookback , features ) and here 's a basic example of the LSTM NN I 'm using right now.I then created a new CNN model ( although not causal as the 'causal ' padding is only an option for Conv1D and not Conv2D , per Keras documentation . If I understand correctly , by having multiple features , I need to use Conv2D , rather than Conv1D but then if I set Conv2D ( padding='causal ' ) , I get the following error - Invalid padding : causal ) Anyways , I was also able to fit the data with a new shape ( number of examples , lookback , features , 1 ) and run the following model using the Conv2D Layer : However , from my understanding , this does not propagate the data as causal , rather just the entire set ( lookback , features , 1 ) as an image.Is there any way to either reshape my data to fit into a Conv1D ( padding='causal ' ) Layer , with multiple features or somehow run the same data and input shape as Conv2D with 'causal ' padding ?"
"I have a list of tuples like this : list = [ ( 1 , ' q ' ) , ( 2 , ' w ' ) , ( 3 , ' e ' ) , ( 4 , ' r ' ) ] and i am trying to create a update function update ( item , num ) which search the item in the list and then change the num.for example if i use update ( w,6 ) the result would be i tried this code but i had errorPushheap is a function that push tuples in the heapany ideas ?"
"Is it possible to check the version of a package if only a module is imported ? When a package is imported like ... I use : to print the version number.How do I check the version number if only a module is imported , likeorAny suggestions ?"
"This is the practice example : Write a function ( list1 , list2 ) that takes in two lists as arguments and return a list that is the result of removing elements from list1 that can be found in list2.Why does this function return [ 1 , 3 , 4 ] and not [ 4 ] as I thought it would ? I think it has something to do with list1.remove ( ) . I guess it 's something obvious , but I ca n't see it.It works on these examples : but fails on this : Also i noticed it removes only even numbers from the list.Here is the code : I have solved this practice with :"
"I have my project structure as followingWhere the module 'Country ' folder is what I tried to rename it to 'Countries ' and every occurrence it is used , and it is also imported in Customer/views.py as well.According to this tutorial Refactoring Python Applications for Simplicity , I tried it as below : After executing the script , the module folder 'Country ' was changed to 'Countries ' but its instance where it is used in Customer/views.py does not change accordingly , the import statement in Customer/views.py is still I expected it should change to from app.Countries.views import * after refactoring , but it did not.Is there anything else I should do to refactor this successfully ? Thanks ."
"So say that I have a dictionary with a default value of another dictionaryThe problem is that the default dictionary that I pass into defaultdict ( attributes ) is passed as a reference . How can I pass it as a value ? So that changing the values in one key does n't change the values in other keysFor example - I want each of them to print 1 , since I only incremented their respective values for 'calls ' once ."
"I am using grequests python module to call some APIs . I want to make two functions . A single request ( use requests module ) A multiple request ( use grequests module ) When I use two modules in two different files , it runs normally , but when I import two modules in the same file , requests module fall in infinity recursive.If I call MultiRequest ( ) - > do Well ! but if I call SingleRequest ( ) ... .. ↓ Is it possible to use requests and grequests in one file ?"
"I just downloaded Python 3.4 and I 'm wondering how you would go about finding the first directory of a relative path ? I.e . given the path a/b/c/d I would like to print a.The closest I 've gotten is : or in both cases the -2 part is a bit magical . I 've read the docs and the PEP , and have n't found any better way.. did I miss something obvious ?"
"Im new in this Pandas and Matplotlib , I follow an example from a book and apparently it give me a warning '' MatplotlibDeprecationWarning : The epoch2num function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.base = dates.epoch2num ( dt.asi8 / 1.0e9 ) '' and the X value of axis change from years to some random numbers"
I 've been refactoring some rather crufty code and came across the following rather odd construct : ... and I was wondering if this would ever make any conceivable sense.I changed it to something like : I did fire up an interpreter and actually try the first construct ... it only seems to work if the values are all false and the last of these false values is None . ( In other words CPython 's implementation seems to return the first true or last false value from a chain of or expressions ) .I still suspect that the proper code should use either the any ( ) or all ( ) built-ins which were added 2.5 ( the code in question already requires 2.7 ) . I 'm not yet sure which are the preferred/intended semantics as I 'm just starting on this project.So is there any case where this original code would make sense ?
"TL ; DR : I want a drop-in replacement for PyQt5 's loadUiType ( ) function from its uic module that works with PySide2 and Python 3.6+.I want to migrate a PyQt5 application to PySide2 . A common pattern I use is , I create the user-interface design in Qt Designer and dynamically load the resulting .ui file as a mix-in class extending a Qt widget in the Python code , such as the main window itself : This means I can forgo compiling the .ui design to a .py Python module on the command line . More importantly , the mix-in pattern lets me access all Qt widgets defined in the design via self.name within the scope of the importing widgets , where name is assigned as such within Qt Designer.For the sake of providing a reproducible example , here is a minimal Qt design file to go along with the above Python code , in which it is referenced as design.ui : I would like to accomplish the same , but with PySide2 , and with the fewest code changes possible . The problem is , PySide2 does not provide an equivalent of PyQt5 's uic.loadUiType ( ) function , which , importantly , returns the design 's form class to be used as the mix-in.There is a related question , `` PyQt5 to PySide2 , loading UI-Files in different classes '' , but its premise is that the loaded objects be usable from a separate class , which is not my concern per se . Plus , the ( currently ) only answer to it is not the solution I am looking for . Other questions and their answers ( 1 , 2 ) establish that design files can be dynamically loaded in PySide2 , via QtUiTools.QUiLoader ( ) .load ( 'design.ui ' ) , but that method returns the widget object , not the required form class.The latter approach , without mixing in the imported class , would require me to change many lines of code for the migration , as it results in a different object hierarchy of the Python instance variables . In the above example , self.label would then have to be renamed to something like self.ui.label throughout the code base.What I want is a drop-in replacement for PyQt5 's loadUiType ( design ) function from its uic module that works with PySide2 and Python 3.6+ , wherein design designates the path to a .ui file.This answer , from 2013 , perfectly demonstrates that , but for PySide ( based on Qt4 ) and ( legacy ) Python 2 . How do I adapt that code to PySide2 ( based on Qt5 ) running on ( modern ) Python ?"
There are multiple iterator classes depending on what you 're iterating over : Two questions : Is there any meaningful distinction between them ? Why is the first one called a callable-iterator ? You definitely can not call it .
I want to find all possible partitions of a str without empty strs and must contain ever char ( should not contain the original str ) For example : EDIT : Can be in any orderWhy My Question Is Not a Duplicate : I do n't want permutations that is : But I want the the string partitioned into many lengths ( Please Have a Look at the First Code ) Thanks !
"I have a list of lists such that the length of each inner list is either 1 or n ( assume n > 1 ) .I want to transpose the list , but instead of truncating the longer list ( as with zip ) or filling the shorter lists with None , I want to fill the shorter lists with their own singular value . In other words , I 'd like to get : I can do this with a couple of iterations : But is there a better approach ? Do I really need to know maxlist in advance to accomplish this ?"
"In python I can use a template with how can I define a default value in the template ? And call the template without any value.EditAnd when I call without parameters get the default value , example"
"I have two sets : I want to find if there are any ( b ) in ( a ) including substrings . normally I would do : However , in this example it would return an empty set as 'apple ' ! = 'apple ! 'Assuming I can not remove characters from ( a ) and hopefully without creating loops , is there a way for me to find a match ? Edit : I would like for it to return a match from ( b ) e.g . I would like to know if 'apple ' is in set ( a ) , I do not want it to return 'apple ! '"
"I have the following entries in my database : I want to sort these objects by the status of the object : Default , then WIP , then Complete . The correct ordering would then be : How would I do the following db call ?"
"When I loop over the lines of a set of gzipped files with the module fileinput like this : Then those lines are byte strings and not text strings.When using the module gzip this can be prevented by opening the files with 'rt ' instead of 'rb ' : http : //bugs.python.org/issue13989Is there a similar fix for the module fileinput , so I can have it return text strings instead of byte strings ? I tried adding mode='rt ' , but then I get this error :"
"I was writing a code that finds `` unbound methods '' of a class using introspection and was surprised to see two different kinds of descriptors for builtin types : Searching the docs turned up very limited but interesting results : A note in the inspect module that inspect.getattr_static does n't resolve descriptors and includes a code that can be used to resolve them.an optimization made in python 2.4 claiming that method_descriptor is more efficient than wrapper_descriptor but not explaining what they are : The methods list.__getitem__ ( ) , dict.__getitem__ ( ) , and dict.__contains__ ( ) are now implemented as method_descriptor objects rather than wrapper_descriptor objects . This form of access doubles their performance and makes them more suitable for use as arguments to functionals : map ( mydict.__getitem__ , keylist ) .The difference in performance quite intrigued me , clearly there is a difference so I went looking for additional information.Neither of these types are in the module types : using help does n't provide any useful information : Searching the internet only came up with results about `` what is a descriptor '' or vague references to the specific types involved.So my question is : What is the actual difference between < class 'method_descriptor ' > and < class 'wrapper_descriptor ' > ?"
"I would like client admins to be able to edit the various status emails that their site is sending . The emails are very simple django templates , stored in the database . I would like to verify that they do n't have any syntax errors , missing variables , etc. , but I ca n't figure a simple way to do so.For unknown block tags , it 's easy : I do n't know how I would detect an empty variable though ? I 'm aware of the TEMPLATE_STRING_IF_INVALID setting , but that is a site-wide setting.missing closing tags/values do n't cause any exceptions either..Do I have to write a parser from scratch to do basic syntax validation ?"
"UPDATE : After ensuring my commands , serial config , and terminator ( '\r ' ) were correct I got this working on 1 of 5 computers . This leads me to believe it is an adapter issue . I plan on calling the company to see about ordering a USB/RJ11 adapter ( I had been using a Keyspan USB- > DB9- > RJ11 adapter on my mac ) I 've read this but I am still unable to communicate with this pump . This is the python script I modified ( source ) , tty ports : I 'm not even sure if it is sending the commands . Not getting any errors or feedback . Nothing is happening on the pump and nothing is getting returned ( out string is always empty ) This is my output : My ultimate goal : set up pumps parametersthere are three phases that are specified : phase 1 : push liquid to end of tubephase 2 : dispense liquid in specific rate and volumephase 3 : pull liquid back upthe liquid is pulled back up ( phase 3 ) so that it wo n't drip from the manifold , and so the subject ca n't suck it out . As such , phase 1 is needed to push theliquid back to the outflow point.volume and dispense rate can be changed . Use the following formula : rate= volume/sec * 60example : .5/4 x 60 ( deliver .5 ml over a 4 sec duration ) =7.5"
"Is there a way to detect anonymous enumerations using libclang without relying on the text in the spelling name ? The python bindings to libclang include functionality to detect whether C/C++ structs or unions are anonymous using clang.cindex.Cursor.is_anonymous , which ends up calling clang_Cursor_isAnonymous . The following sample demonstrates the issue.Which when run on sample.cppGives :"
"I have set up an optimization problem with linear equality constraints as followsThe objective is a weighted sum functions , whose coefficients/weights are to be optimized to make it minimized . As I have boundaries on the coefficients as well as constraints , I used the trust-constr method within scipy.optimize.minimize.The minimization works out , but I do not understand the termination criteria . According to the trust-constr documentation it should terminate on xtol The algorithm will terminate when tr_radius < xtol , where tr_radius is the radius of the trust region used in the algorithm . Default is 1e-8.However , the verbose output shows , that the termination is indeed triggered by the barrier_tol parameter , as you can see in the listing belowIt is obvious that once , tr_radius < xtol , the tr_radius is reset to its default value 1 and the barrier param is reduced . Once barrier param < barrier_tol ( i.e . 1e-8 ) and tr_radius < xtol , the optimization terminates successfully . The documentation says regarding barrier_tol When inequality constraints are present the algorithm will terminate only when the barrier parameter is less than barrier_tol.which would explain the behaviour in case of inequality constraints , but all my constraints are equality constraints defined as dictionaryIs anyone deep enough into trust-constr to explain this to me ?"
"I 'm working with the PyFacebook package in Python , and I 've seen people mention numerous times that you can write an import statement as follows : However , it does not work . It states that facebook.method_name exists in the facebook module , rather than the djangofb module . I assume I 'm importing the facebook.method_name as facebook , not that I 'm receiving it from the facebook package itself.I 'm using Python 2.6.How can I alias facebook.djangofb as facebook ?"
"I have multiple projects that shares child apps with other projects.When working within the project directory I want to be able to make changes to the app , update it , and pull those updates into the second project.Requirement : No use of symbolic links ( my IDE 's debugger does n't work well with them ) No compiling/rerun a script . I would like to make changes to the app without having to rerun a script/buildout . Apps must be within the project folder.Here 's the structure : Currently I 'm using git-submodules for this ; the downside is there is no way to link to a subfolder of a repo . I recently read about subtree , would this work better ? Ideally I would like to use buildout , but I have n't found a good way to accomplish this without the use of symbolic links . If there 's a way to to do this please let me know . Any suggestions would be greatly appreciated ."
"ProblemI have a target variable x and some additional variables A and B. I want to calculate averages ( and other statistics ) of x when certain conditions for A and B are met . A real world example would be to calculate the average air temperature ( x ) from a long series of measurements when solar radiation ( A ) and wind speed ( B ) fall into certain pre-defined bin ranges.Potential solutionsI have been able to accomplish this with loops ( see example below ) , but I 've learned that I should avoid looping over dataframes . From my research on this site I feel like there is probably a much more elegant / vectorized solution using either pd.cut or np.select , but I frankly could n't figure out how to do it.ExampleGenerate sample datadf.head ( ) output : Calculate bin averagesStore the result in a new dataframebinned.head ( ) output :"
"I 'm trying to setup python-ldap on macOS Sierra.When I try use the module ( which works in my live env running on CentOS ) I get the below error , which upon searching looks to be something to do with the install of OpenLDAP or python-ldap on macOS , but I 'm yet to find an article that explains how to fix it.Thus far I have installed OpenLDAP via homebrew which has not fixed the issue : error : I have installed openldap via brew as per the belowand i have installed python-ldap with pip"
"I use __init__.py to run checks when I do from myprojects.something import blabla.Today I started using pyzmq and I wanted to see what 's going on behind the scenes . So I browsed the code in github and I find ( for me ) some strange usage of __init__.py there that I can not explain myself.For example zmq/core/__init__.py . What 's the point of adding in zmq.core.__all__ the __all__ 's value of zmq.core.constants , zmq.core.error , zmq.core.message , etc . ? In zmq/__init__.py I see at the endwhere get_includes is a function which basically returns a list with the directory of the module and the utils directory in the parent directory.What 's the point of that ? What has __init.py__ achieved by doing that ?"
"tl ; drIs there a simple alternative to multi-table inheritance for implementing the basic data-model pattern depicted below , in Django ? PremisePlease consider the very basic data-model pattern in the image below , based on e.g . Hay , 1996 . Simply put : Organizations and Persons are Parties , and all Parties have Addresses . A similar pattern may apply to many other situations . The important point here is that the Address has an explicit relation with Party , rather than explicit relations with the individual sub-models Organization and Person.Note that each sub-model introduces additional fields ( not depicted here , but see code example below ) .This specific example has several obvious shortcomings , but that is beside the point . For the sake of this discussion , suppose the pattern perfectly describes what we wish to achieve , so the only question that remains is how to implement the pattern in Django.ImplementationThe most obvious implementation , I believe , would use multi-table-inheritance : This seems to match the pattern perfectly . It almost makes me believe this is what multi-table-inheritance was intended for in the first place.However , multi-table-inheritance appears to be frowned upon , especially from a performance point-of-view , although it depends on the application . Especially this scary , but ancient , post from one of Django 's creators is quite discouraging : In nearly every case , abstract inheritance is a better approach for the long term . I ’ ve seen more than few sites crushed under the load introduced by concrete inheritance , so I ’ d strongly suggest that Django users approach any use of concrete inheritance with a large dose of skepticism.Despite this scary warning , I guess the main point in that post is the following observation regarding multi-table inheritance : These joins tend to be `` hidden '' — they ’ re created automatically — and mean that what look like simple queries often aren ’ t.Disambiguation : The above post refers to Django 's `` multi-table inheritance '' as `` concrete inheritance '' , which should not be confused with Concrete Table Inheritance on the database level . The latter actually corresponds better with Django 's notion of inheritance using abstract base classes.I guess this SO question nicely illustrates the `` hidden joins '' issue.AlternativesAbstract inheritance does not seem like a viable alternative to me , because we can not set a foreign key to an abstract model , which makes sense , because it has no table . I guess this implies that we would need a foreign key for every `` child '' model plus some extra logic to simulate this.Proxy inheritance does not seem like an option either , as the sub-models each introduce extra fields . EDIT : On second thought , proxy models could be an option if we use Single Table Inheritance on the database level , i.e . use a single table that includes all the fields from Party , Organization and Person.GenericForeignKey relations may be an option in some specific cases , but to me they are the stuff of nightmares.As another alternative , it is often suggested to use explicit one-to-one relations ( eoto for short , here ) instead of multi-table-inheritance ( so Party , Person and Organization would all just be subclasses of models.Model ) .Both approaches , multi-table-inheritance ( mti ) and explicit one-to-one relations ( eoto ) , result in three database tables . So , depending on the type of query , of course , some form of JOIN is often inevitable when retrieving data . By inspecting the resulting tables in the database , it becomes clear that the only difference between the mti and eoto approaches , on the database level , is that an eoto Person table has an id column as primary-key , and a separate foreign-key column to Party.id , whereas an mti Person table has no separate id column , but instead uses the foreign-key to Party.id as its primary-key.Question ( s ) I do n't think the behavior from the example ( especially the single direct relation to the parent ) can be achieved with abstract inheritance , can it ? If it can , then how would you achieve that ? Is an explicit one-to-one relation really that much better than multi-table-inheritance , except for the fact that it forces us to make our queries more explicit ? To me the convenience and clarity of the multi-table approach outweighs the explicitness argument.Note that this SO question is very similar , but does not quite answer my questions . Moreover , the latest answer there is almost nine years old now , and Django has changed a lot since . [ 1 ] : Hay 1996 , Data Model Patterns"
"I am trying to run test with nose and here is project structure : test_suite.py contains : and test_runner.py contains next : when I run test_runner.py I have next content in output : Nose runs tests twice . If I connect myself plugin , I can see , for example , that mehtod addSuccess ( ) calls 4 times . Could you explain me , why do I get this behaviour and how can I prevent it ?"
How can I create a timewheel similar to below with logon/logoff event times ? Specifically looking to correlate mean login/logoff time correlated to the day of the week in a time wheel fashion ? The Picture below is an example but I am looking for times going around the clock with days of the week where the times are now in the picture . I have python available to me and data sets that include login times . I would also like to correlate colors to user types such as admins vs regular users or something of that nature . Any thoughts on how to accomplish this would be great . Some sample data is below in a pandas dataframe df :
"Using Python 2.6.6So I just learned that the following : can be replaced with : My quandry is that with the former code I could unittest that the lock was being used to protect the doing-of-stuff by mocking out the Lock . But with the latter my unittest now ( expectedly ) fails , because acquire ( ) and release ( ) are n't being called . So for the latter case , how do I verify that the lock is used to protect the doing-of-stuff ? I prefer the second method because it is not only more concise , but there is no chance that I 'll write code that forgets to unlock a resource . ( Not that I 've ever done that before ... )"
I am using sqlalchemy and marshmallow in my RESTful flask application to serialize my models . I have a hybrid_property that comes from one of my relationships on that model . I would like to now serialize that hybrid_property in my schema using the schema from the related model.Has anyone done this before ? Here are my relevant pieces of code . It does n't seem to be including the last_assessment in the serialized version of the model when I check the response
"Is it possible to detect the upper side of a dice ? While this will be an easy task if you look from the top , from many perspectives multiple sides are visible.Here is an example of a dice , feel free to take your own pictures : You usually want to know the score you have achieved . It is easy for me to extract ALL dots , but how to only extract those on the top ? In this special case , the top side is the largest , but this might not always be true . I am looking for someting which evaluates the distortion of the top square ( or circle in this case , which I can extract ) in relation to the perspective given by the grid in the bottom.Example program with some results is given below.Some resulting images :"
I 'm running a laplacian pyramid program on Colab that is not vectorized so it takes it 's time and when it gets to def vizpyramid ( ) it just exits per below : Most of those are from print statements except the 100 % ... and ^C . I did not push control-C so what is happening ? I have checked the function and it works fine on my local machine . Edit : I tried switching to a None no-GPU runtime and now it wo n't even run with a ! python main.py command .
"I 'm trying to use voluptuous to validate JSON input from HTTP request . However , it does n't seem to handle unicode string to well.The above code generates the following error : However , if I remove the u notation from the URL , everything works fine . Is this a bug or am I doing it wrong ? ps . I 'm using python 2.7 if it has anything to do with it ."
"I am doing binary classification using multilayer perceptron with numpy and tensorflow.input matrix is of shape = ( 9578,18 ) labels has the shape = ( 9578,1 ) Here 's the code : After running my model for 100 epochs , the cost decreases after each epoch which means that the network is working okay , but the accuracy is coming out to be 1.0 everytime and I have no clue why as I am sort of a beginner when it comes to neural networks and how they function . So any help will be much appreciated . Thanks ! Edit : I tried checking the predictions matrix after each epoch and I am getting all zeros in that , everytime . I used the following code in my for loop with epochs to check the predictions matrix : Here 's output of 1 epoch :"
"I have strings like '12454v ' , '346346z ' . I want to delete all letters from strings.Re works fine : Is there a way to do this without using regular expressions ?"
"Python newbie here , running 2.7.I am trying to create a program that uses a function to generate text , and then outputs the function-generated text to a file.When just printing the function in powershell ( like this : http : //codepad.org/KftHaO6x ) , it iterates , as I want it to : When trying to output the function into a file ( like this : http : //codepad.org/8GJpp9QY ) , it only gives 1 value , i.e . does not iterate : Why is this , and how can I make the output function iterate ( like it does with print ) ?"
"When using try/except blocks in Python , is there a recommendation to delegate it to any methods that might raise an exception , or to catch it in the parent function , or both ? For example , which of the following is preferred ? orPEP 8 seems to be quiet on the matter , and I seem to find examples of both cases everywhere ."
"I am accessing a postgresql table with serialization transaction isolation . I am doing something like this ( with an existing psycopg2 connection conn and cursor in that connection , cur : The point of this is to retry in case of serialization contention . Now , this works fine much of the time . But often I get this error after it has spent one iteration in the TransactionRollbackError trap : current transaction is aborted , commands ignored until end of transaction block . Apparently , spinning this way to avoid serialization contention is not appropriate ? Should I be doing this a different way ? Some notes : I am accessing the table with different processes ( that are all the same and doing the same things : selecting , incrementing , and updating / inserting into the table . ) Each of these processes has their own connection conn , they are not sharing a connection.Another note : it seems like after going through the TransactionRollbackError exception block once , in the next spin of the while loop it ends up in the Exception exception block instead.Still another note : The number of processes running at the same time has a direct effect on the frequency of errors , in that more processes tends to produce more errors . So there is some kind of contention going on . I am under the impression that using serialized transaction isolation with retries ( as in my demo code ) would fix this ."
I have a Python Serverless project that uses a private Git ( on Github ) repo . Requirements.txt file looks like this : Configurations of the project mainly looks like thisWhen I deploy using this command : I get this error : What am I doing wrong ? I 'm suspecting either the way I configured my SSH key for Github access or the configurations of the serverless package .
"Lets assume a simple method : To time this method using a decorator , a simple decorator would be : Now if I want to time specific lines of test_method say line 4 sum1 = sum ( range ( a , b ) ) , the current implementation involves inline coding like : The intention is to use the decorator to time lines M to N of a specific method without modifying the code in the method.Is it possible to inject such logic using a decorator ?"
"I know how to get a ctypes pointer to the beginning of a numpy array : however , I need to pass the pointer to , let 's say , element 100 , without copying the array.There must be an easy way to do it but can not find it.Any hint appreciated ."
"I 'd like to vectorize calls like numpy.arange ( 0 , cnt_i ) over a vector of cnt values and concatenate the results like this snippet : Unfortunately the code above is very memory inefficient due to the temporary arrays and list comprehension looping.Is there a way to do this more efficiently in numpy ?"
"I am new to Python and trying to learn it as much as possible . I am stuck with a silly problem where I want to remove certain dictionary items of a list based on selective key-value pairs . For ex , I have : And the output I want is removal of dictionaries based on two keys A and C values :"
"I want to fit a function , defined as follows , to a time series data : Here , t represents the time at which a measurement is made , and the rest of the arguments are the parameters of the function . The problem is that when I feed it into curve_fit , Python complains about the ambiguity in the t < T comparison . I believe this happens because t becomes a list of data points when func is called inside curve_fit , whereas T is a number ( not a list ) : where t1 is a list of times and d1 is a list of the data values measured at the corresponding times . I have attempted a number of ways to get around this problem , but to no avail . Any suggestion ? Thanks a lot !"
"From Project Euler , problem 45 : [ http : //projecteuler.net/problem=45 ] Now to solve them I took three variables and equated the equations to A.A = number at which the threee function coincide for values of n , a , bResultant we get 3 equations with n and A . Solving with quarditic formula , we get 3 equations.So my logic is to test for values of A at which the three equation give a natural +ve value . So far it works correct for number 40755 but fails to find the next one upto 10 million . ( Edit ) : Here is my code in pythonHow is my logic wrong ? ( Apologies for a bit of maths involved . : ) )"
"As mention in documentation , i have followed below steps -In my settings.pyMy locale directory is in my Django project 's root folderIn urls.pyIn base.htmlI am able to see translation done by trans tag , but when m trying to translate variables in javascript using gettext method m getting this particular error ReferenceError : gettext is not definedFYI - djangojs.po and djangojs.mo files are there in my locale directory and i have compiled the file after putting translations on.I tried hard on google but still same error ."
"I 'm running uwsgi+flask application , The app is running as a k8s pod.When i deploy a new pod ( a new version ) , the existing pod get SIGTERM.This causes the master to stop accepting new connection at the same moment , what causes issues as the LB still pass requests to the pod ( for a few more seconds ) . I would like the master to wait 30 sec BEFORE stop accepting new connections ( When getting SIGTERM ) but could n't find a way , is it possible ? My uwsgi.ini file : [ uwsgi ]"
"I am trying to augment an existing python source with a cython .pxd , as Stefan Behnel illustrates in slides 32 to 35 of `` Using the Cython Compiler to write fast Python code '' .As part of the exercise , I keep hitting a wall with the __eq__ ( ) method in my metaclass . I wish I could choose a simpler case to start Cython , but my production code is n't that simple . I cooked up a `` minimal , complete example '' to illustrate the problem ... see the code at the bottom of the question.Short story ... If I use cdef inline __richcmp__ ( Japan_Car_ABC self , Japan_Car_ABC other , int op ) : , cython complains that Special methods must be declared with 'def ' , not 'cdef'If I use def __richcmp__ ( Japan_Car_ABC self , Japan_Car_ABC other , int op ) : , cython complains that function definition in pxd file must be declared 'cdef inline'So cython gives me confusing guidance ... Questions : I know that pure-python .pxd files have restrictions ; is defining __richcmp__ ( ) in my .pxd a valid way to use a .pxd to augment pure python ? If this is a valid way to use a .pxd , how can I fix this to compile correctly ? If this is wrong , can a .pxd augment my pure python metaclass , without re-writing the whole metaclass in a .pyx file ? ( example : class Card in this project ) This is my .pxd ... Informational : car_abc.py : car.py : setup.py :"
"Suppose that class A has a member whose type is class B , and class B has a member whose type is class A.In Scala or Kotlin , you can define the classes in any order without worries in this case because the firstly-defined class can use the secondly-defined class as usual , even in case/data classes.However in Python , the following codethrows a compile error because class B is not defined when class A is being defined.You can work around this simple case , like in this answerHowever , this way does not work for data classes in Python because assigning members after the definition of data classes will not update the auto-generated methods of the data classes , which makes the usage of `` data class '' useless.How can I avoid this problem ?"
"I 'm looking for a more elegant way of declaring a variable value where the function may return None and there are chained methods following the function call . In the example below I am using BeautifulSoup to pass an HTML doc and if the element I am looking for is not found , the initial function call returns None . The chained methods then break the code because .string is not a method of None object . Which all makes sense , but I 'm wondering if there 's a cleaner way to write these variable declarations that wo n't break on a None value.Any better way ?"
"If I generate the same graph multiple times using NetworkX and Matplotlib it 's rotated randomly on every generation : Run 1 : Run 2 : Without changing the script or input data , the graph is randomly rotated every time it is generated . Is it possible to specify an orientation ? As the graph becomes more densely populated ( above are just samples but ultimately I will have thousands of nodes and edges ) , it will be difficult to see the newly added nodes or edges if they are moved because the graph picture is rotated.A second less important question is why are the edges/lines from R1 to R2 and R1 to R5 so much longer ? Update : I had n't set the `` length '' attribute in the JSON data source ."
I have a list of numbers from which I 'd like to return a list of mantissas : All help greatly appreciated .
"I try to write a script in Python that saves the file in each user directory.Example for user 1 , 2 and 3.How can I do this ?"
"I have one situation and I would like to approach this problem with Python , but unfortunately I do n't have enough knowledge about the graphs . I found one library which seems very suitable for this relatively simple task , networkx , but I am having issues doing exact things I want , which should be fairly simple . I have a list of nodes , which can have different types , and two `` classes '' of neighbors , upwards and downwards . The task is to find paths between two target nodes , with some constraints in mind : only nodes of specific type can be traversed , i.e . if starting nodes are of type x , any node in the path has to be from another set of paths , y or zif a node has a type y , it can be passed through only onceif a node has type z , it can be passed through twicein case a node of type z is visited , the exit has to be from the different class of neighbor , i.e . if its visited from upwards , the exit has to be from downwardsSo , I tried some experimentation but I , as said , have struggled . First , I am unsure what type of graph this actually represents ? Its not directional , since it does n't matter if you go from node 1 to node 2 , or from node 2 to node 1 ( except in that last scenario , so that complicates things a bit ... ) . This means I ca n't just create a graph which is simply multidirectional , since I have to have that constraint in mind . Second , I have to traverse through those nodes , but specify that only nodes of specific type have to be available for path . Also , in case the last scenario happens , I have to have in mind the entry and exit class/direction , which puts it in somewhat directed state.Here is some sample mockup code : The output is fairly nice , but I need these constraints . So , do you have some suggestions how can I implement these , or give me some more guidance regarding understanding this type of problem , or suggest a different approach or library for this problem ? Maybe a simple dictionary based algorithm would fit this need ? Thanks !"
"I have a dataframe containing some data , which I want to transform , so that the values of one column define the new columns.The values of the column A shall be the column names of the new dataframe . The result of the transformation should look like this : What I came up with so far did n't work completely : Besides this being incorrect , I guess there probably is a more efficient way anyway . I 'm just really having a hard time understanding how to handle things with pandas ."
"I 'm trying to create a writable nested serializer . My parent model is Game and the nested models are Measurements . I am trying to post this data to my DRF application using AJAX . However , when try to post the data , the nested Measurements are empty OrderedDict ( ) .Here are my models : Here are my serializers : My view for Game is the following : I followed this tutorial for the structure.I am trying to post to this API via AJAX , the code below : On posting this data , I find in the create method within the GameSerializer that the validate_data.pop ( 'measurements ' ) contains a list of 3 ordered dictionaries ( OrderedDict ( ) ) that are empty . UPDATE : I 've found that that the initial_data coming in via request.data is structured like so : Has anyone encountered this issue before ? Thanks ! UPDATE # 2I was able to resolve this ( although I believe it is more of a workaround than a solution ) by adding the following to my MeasurementSerializer : The Measurement data coming in was a QueryDict when I believe I needed a Dict . There were also some extra brackets around the key and values so I had to remove those as well.Still seeking a better answer than this !"
"I would like to write a program with a Text-based User Interface ( TUI ) that consists of several forms.The first form contains a `` list '' . Each list element represents a button . If the respective button is pressed , another form should appear in which one can enter the data for the list entry.Then the first form is displayed again ( with updated list entries ) .Here is my attempt , which uses the library npyscreen but does not return to the first form . The code does also not contain the logic to change the list item ."
"The output is : So , the order is not preserved even with the use of OrderedDict . I know the dictionaries do n't preserve the initial order by default , and all those things . But I want to learn why the OrderedDict is not working ."
"When using solve to compute the roots of a quadratic equation , SymPy returns expressions which could be simplified but I ca n't get it to simplify them . A minimal example looks like so : Here , SymPy just returns sqrt ( -24-70*I ) while Mathematica or Maple will answer with the equivalent of 5-7*I.I 'm aware that there are two square roots , but this behavior entails that SymPy will , for example , return pretty complicated solutions fromwhile , again , Maple and Mathematica will both happily give me the two Gaussian integers that solve this equation.Is there an option or something that I 'm missing ?"
"In this code , I 'm using Python 2.7.13 , OpenCV 2.4.13 and PyAutoGUI 0.9.36 . The objective is to move the cursor according to the facial movement , but the cursor movement is inverted . For example , if my face goes to right , the cursor moves to left and if my face goes to left , the cursor goes to right . Also , I want the cursor to move right , left , up and down in the whole screen of my PC , whose size is x=1920 , y=1080 . The purpose of this program is to show that it is possible to get a new way to acquire more independence and access so that people with tetraplegia are capable of doing the simple activities , which are part of the routine of millions of individuals , such as turning the light on and off and turning TV on and off ."
"Up to now I do simple logging to files , and if I log a multiline string , then the result looks like this : Emitting log : Logfile : barUp to now all lines which do not contain `` INFO '' or `` DEBUG '' get reported to operators.This means the line bar gets reported . This is a false positive.Environment : Linux.How to set up logging in Python to keep the INFO foo\nbar in one string and ignore the whole string since it is only `` INFO '' ? Note : Yes , you can filter the logging in the interpreter . Unfortunately this is not what the question is about . This question is different . First the logging happens . Then the logs get parsed.Here is a script to reproduce it : After thinking about it again , I think the real question is : Which logging format is feasible ? Just removing the newlines in messages which span multiple lines makes some output hard to read for the human eyes . On the other hand the current 1:1 relation between logging.info ( ) and a line in the log file is easy to read . ... I am unsure"
"Things used to work . Then it started working occasionally , until it totally stopped working.Following is my subscription code : Following is my view for handling the GET and POST requests from instagram : Following is my urls.pyThis is used to work beautifully . However , it stopped working since two days . Whenever the subscription function is called , it throws the error : I tried hitting the Callback URL manually and got the response `` test '' which is what you would expect from the function I have written . I tried a requests.get ( ) manually to that url before calling the subscription function and that returned a 200 response.Why ca n't Instagram find my callback url when everyone else can access it ?"
"I have a tricky and interesting question to You.While working on I/O tasks such as protocol implementation via some transport layer in Twisted , Tornado , I found a similar scenario or pattern . The pattern is rather generic than abstract . For example , when you are working with MODEM-like device , you send him commands and receive the results.However , sometimes you need to react on the response of a modem on last command with new command ( s ) . For example , suppose that modem is M , - > is communication operator that takes one parameter , message key , and server is S.So , it looks like FSM behaviour . It would be nice to implement this scenario in tornado , while working with non-blocking I/O ( via streams objects ) . By simply providing tracking scenario as an input and overriding handlers to the states ( events ) described in input we can reach nice finite state machine behaviour.Input may have the following notation : where all these alphanumeric signs are state names.each key-value pair is state name and possible set of state transitions.What is possible implementation of FSM using introduced in tornado coroutines and futures ? Please share your minds and code ."
I have a 5gb text file and i am trying to read it line by line . My file is in format- : Reviewerid < \t > pid < \t > date < \t > title < \t > body < \n > This is my code But i get Memory error when i try to run it . I have an 8gb ram and 1Tb space then why am i getting this error ? I tried to read it in blocks but then also i get that error .
Here an example : You can see that the file with dtype=object is about half the size . How come ? I was under the impression that properly defined numpy dtypes are strictly better than object dtypes .
"I would like to determine , if there 's a translation to current language for a given string . I 'd like to write something like : I did n't find anything appropriate . The ugettext function just returns the translation or the original string ( if the translation is n't available ) but without any option to determine if the translation is there or is n't . Thanks ."
"I have a QTableView that dynamically loads data from a custom model that inherits QAbstractItemModel . The model implements both fetchMore and canFetchMore . The problem is that I would like to be able to select all rows for small datasets , but if I hit ctrl-a in the view it only will select the rows that are currently loaded . Is there some mechanism to force the QTableView to fetch more rows ? Ideally I would like to show a progress bar indicating the fraction of data that has been loaded from the model . Every few seconds I would like to force the model to load a bit more of the data , but I still want to let the user interact with the data that has been loaded so far . This way when the progress bar is complete the user can press ctrl-a and be confident that all data is selected . Edit : I have another motivating use case . I want to jump to a specific row , but if that row is not loaded my interface does nothing . How can I force a QAbstractItemModel to fetch more ( or up to a specific row ) and then force the QTableView to show it ? If I do n't implement fetchMore and canFetchMore , the previous functionality works , but loading the tables is very slow . When I implement those methods the opposite happens . Not having an answer to this problem is causing issues with the usability of my qt interface , so I 'm opening a bounty for this question . Here is a method I 'm using to select a specific row . If the user has manually scrolled past the row in question then this function works . However , if the user has not seen the specific row this function just scrolls back to the top of the view ."
"I am reading Fluent Python and trying to get a deeper understanding of dictionaries.So when I run the below , the results are easy to understand in that both get ( ) and dunder getitem ( ) return the same resultWhen I subclass dict with get ( ) , I get a working instanceNow if I replace get ( ) with dunder getitem ( ) I get an error and I am unsure why.errorSo the question is , what is the difference between get and dunder getitem in this situation and why does this cause a recursion error ?"
"I 'd like to use SqlSoup with an existing database that contains views . Accessing a table goes swimmingly , but accessing a view results in `` PKNotFoundError : table ' [ viewname ] ' does not have a primary key defined ... '' Do I correctly infer that SqlSoup does not work with database views ( by default , at least ) ? I 've been unable to find anything directly relevant on Google , SO , or the SqlAlchemy mailing list . If you were faced with this , how would you proceed if you wanted to access non-updatable views ? I 'm new to SQLAlchemy and SQLSoup.Here 's a specific example : This is a public database . You can run the equivalent queries using psql :"
"I was recently completing a CNN implementation using TensorFlow from an online course which I would prefer not to mention to avoid breaking the platform rules . I ran into surprising results where my local implementation diverged significantly from the one on the platform server.After further investigation , I nailed down the problem to a change in tf.contrib.layers.fully_connected ( ) behaviour between versions 1.3 and 1.4 of TensorFlow.I prepared a small subset of the source code to reproduce the issue : When running tensorflow 1.3- ( tested 1.2.1 as well ) , the output for Z3 is : When running tensorflow 1.4+ ( tested up to 1.7 ) , the output for Z3 is : A detailed review of all the tensors in forward_propagation ( ) ( i.e.Wx , Ax , Px , etc . ) points to tf.contrib.layers.fully_connected ( ) since Z3 is the only diverging tensor.The function signature did not change so I have no idea what happens under the hood.I get a warning with 1.3 and before which disappears with 1.4 and beyond : I was wondering if maybe something changed in the default initialization of the parameters ? Anyway , this is where I am right now . I can go ahead with the course but I feel a bit frustrated that I ca n't get a final call on this issue . I am wondering if this is a known behaviour or if a bug was introduced somewhere.Besides , when completing the assignment , the final model is expected to deliver a test accuracy of 0.78 on an image recognition task after 100 epochs . This is precisely what happens with 1.3- but the accuracy drops to 0.58 with 1.4+ , everything otherwise equal.This is a huge difference . I guess that a longer training might erase the difference but still , this is not a slight one so it might be worth mentioning.Any comment / suggestion welcome.Thanks , Laurent"
"I have a data frame input that looks like : How to collapse all rows while concatenating the data in the rows with ' , ' ? The desired data frame output : Sample input code :"
"I 'm using Django adaptors to upload a simple CSV . It seems to work perfectly when I 'm importing 100 or 200 contacts . But when I try to upload a 165kb file with 5000 contacts , it never completes . I let let it keep trying , and when I came back after 1 hour it was still trying.What 's wrong with this ? There is no way a 165kb file should take over an hour to import with Django adaptors . Is there something wrong with the code ? CsvModel"
"I am using python to sequence some numbers . I would like to create a function which allows me to input a value ( 4 , 8 , 16 , 32 , 64 , etc . ) , create an array of numbers , and rearrange their sequence . I 've added figures which detail how to determine the sequence for value = 4 , and 8 . For value = 4 the array ( x = [ 0 , 1 , 2 , 3 ] ) should be split in two ( [ 0,1 ] and [ 2,3 ] ) and then combined based on the first number in each array ( [ 0 , 2 ,1 ,3 ] ) . For value = 8 the array ( x = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 ] ) should be split into two ( [ 0 , 1 , 2 , 3 ] and [ 4 , 5 , 6 , 7 ] ) . Both arrays should be split in two again ( [ 0 , 1 , 2 , 3 ] into [ 0,1 ] and [ 2,3 ] and [ 4 , 5 , 6 , 7 ] into [ 4,5 ] and [ 6,7 ] ) . Then the arrays should be combined based on the first number in each array and the sequence of 2nd set of arrays ( [ 0 , 4 , 2 , 6 , 1 , 5 , 3 , 7 ] ) .I do n't know how to handle the recursion ( dynamically nested for loops ) . I am trying to loop through each brach that is created by spliting the array . I 've looked into itertools and recursion ( Function with varying number of For Loops ( python ) ) , but I could not make it work . Below , I 've added code to illustrate my approach thus far.Any help is much appreciated . I am also open to other ideas to determine the sequence.I am using python 2.7.6 and numpy.Code : Output : Code : Output : The output for value = 16 should be [ 0 , 8 , 4 , 12 , 2 , 10 , 6 , 14 , 1 , 9 , 5 , 13 , 3 , 11 ,7 15 ] ."
"In case one task of gather raises an exception , the others are still allowed to continue.Well , that 's not exactly what I need . I want to distinguish between errors that are fatal and need to cancel all remaining tasks , and errors that are not and instead should be logged while allowing other tasks to continue.Here is my failed attempt to implement this : ( the finally await sleep here is to prevent the interpreter from closing immediately , which would on its own cancel all tasks ) Oddly , calling cancel on the gather does not actually cancel it ! I am very surprised by this behavior since it seems to be contradictory to the documentation , which states : asyncio.gather ( *coros_or_futures , loop=None , return_exceptions=False ) Return a future aggregating results from the given coroutine objects or futures. ( ... ) Cancellation : if the outer Future is cancelled , all children ( that have not completed yet ) are also cancelled . ( ... ) What am I missing here ? How to cancel the remaining tasks ?"
"I am trying to zip a file using the python module gzip , and then hash the gzipped filed using hashlib . I have the following code : which I thought should simply zip the file , hash it and display the same output hash three times in a row . However , the output I get is : If I leave out the gzip step , and just hash the same gzipped file three times in a row , I do indeed get the same output three times : Can anyone explain what is going on here ? The issue must be that the gzip process is different each time . But as far as I knew , the DEFLATE algorithm is Huffman coding followed by LZ77 ( a form of run-length-encoding ) or LZ77 followed by Huffman , and therefore given identical input should produce identical output ."
I 'm trying to write my own awaiatbale function which could use in asyncio loop such as asyncio.sleep ( ) method or something like these pre-awaitable implemented methods.Here is what I 've done so far : What I got as a result : What I expect as a result : [ NOTE ] : I 'm not looking for a multithread or multiprocess method.This Question is almost similar to my question which has not resolved yet.I 'm using Python3.6
"Let 's say I am having a list as : Here I want to remove every 'no ' which is preceded by every 'yes ' . So my resultant list should be like : I 've found that in order to remove an element from a list by its value , we may use list.remove ( .. ) as : But it gives me result with only removing first occurrence of 'no ' as : How can I achieve the desired result by removing all the occurrence of 'no ' which are preceded by all 'yes ' in my list ?"
"If I have two lists , each 800 elements long and filled with integers . Is there a faster way to compare that they have the exact same elements ( and short circuit if they do n't ) than using the built in == operator ? Anything better than this ? I 'm curious only because I have a giant list of lists to compare ."
"I think there is a memory leak in the ndb library but I can not find where.Is there a way to avoid the problem described below ? Do you have a more accurate idea of testing to figure out where the problem is ? That 's how I reproduced the problem : I created a minimalist Google App Engine application with 2 files.app.yaml : main.py : I uploaded the application , called /create once.After that , each call to / increases the memory used by the instance . Until it stops due to the error Exceeded soft private memory limit of 128 MB with 143 MB after servicing 5 requests total.Exemple of memory usage graph ( you can see the memory growth and crashes ) : Note : The problem can be reproduced with another framework than webapp2 , like web.py"
"I 'm trying to access the rows of a CategoricalIndex-based Pandas dataframe using .loc but I get a TypeError . A minimum non working example would beThen , in trying to access the row corresponding to label 13 asI getdespitebeing True . I know I can ultimately obtain the index of 13 withbut , why is the above easier approach not working ? What am I missing ? Cheers ."
The following code raises an UnboundLocalError : Is there a way to accomplish this ?
"I recently changed a date field to an integer field ( the data was specified in number of months remaining rather than a date ) . However all though the make migrations command works fine when I attempt to migrate this fails with a typecast error ( note using postgres 9.5 ) . Note there was only 1 instance/entry of the model , which I have also now deleted and the migrate still fails for some reason ?"
"I am using pip 20.0.2 on Ubuntu , and installing a bunch of requirements from a requirements file . For some reason , pip is deciding to install idna==2.9 ( link ) , even though that is not a compatible version with one of my directly listed dependencies . So I used python -m pipdeptree -r within the virtualenv that I 'm installing everything to , and I see this listed for idna : As we can see , my two direct dependencies ( cryptography and requests ) , are what require idna . According to those , it looks like pip should decide to install 2.8 , because it is the latest version that will fulfill the constraints.Why is pip instead installing idna 2.9 , as indicated by the top line of that output , and this error message when running pip install -r requirements.txt : EDIT : the contents of requirements.txt and it 's children , as requested in the comments : Edit 2 : I 've created a minimally viable example . For this example , here is requirements.txt : And here are the commands I ran from start to finish in a fresh directory : And the full output :"
"I have a project with the following structure : Now when I run python3 driver.py ( from project 's directory ) , I get the following error : This happens only when both conditions are met : When I do import lib.core.common as abc instead of import lib.core.common.When the project/lib/core/__init__.py contains from .util import Worker import.The thing is that I would like to keep the import lib.core.common as abc import form.Could anybody explain what 's going on here , please ?"
"I have a package installed in development mode with pip install -e ./mylocalpkg.This package defines an entry_points.console_scriptThis script can be called with either wayHowever , I can not debug this script : How can I start a debugging session with pdb while calling entry_point scripts ?"
"I am trying to convert a dataframe to long form.The dataframe I am starting with : The number of columns is not specified , there may be more than 4 . There should be a new row for each value after the keyThis gets what I need , however , it seems there should be a way to do this without having to drop null values :"
"With the following package structureContents of setup.pyContents of setup.cfgI can build wheel or a source distribution for my_package like thisBut according to maintainer of setuptools , a declarative build configuration is ideal and using an imperative build is going to be a code smell . So we replace setup.py with pyproject.toml : Contents of pyproject.tomlAnd you can still build a wheel the same way as before , it works . But sdist does n't work : So how should you actually build the .tar.gz file using setuptools ? What 's the user-facing tool to create sdist ? I do not want to change the build backend . It looks like other packaging tools all write their own build entry points , but I thought the whole point of defining a declarative build system in the metadata was so that you did n't have to get hands-on with the build system , learning how each different packaging tool expects to be invoked or having to go into the interpreter and calling a Python API manually . But the PEP for build system requirements is over 2 years old now . Am I missing something obvious here ? How to build a source distribution without using setup.py file ?"
"In Python , I 'd like to write a terminal program using both cmd and curses together , ie . use cmd to accept and decode full input lines , but position the output with curses.Mashing together examples of both curses and cmd like this : I find that I 'm not seeing anything when I type . curses is presumably waiting for a refresh before displaying anything on the screen . I could switch to using getch ( ) but then I 'd lose the value of cmd.Is there a way to make these work together ?"
How can I trigger the changing of an icon when the user clicks the Icon only for the items/rows in the list which are of type File . Each row in the treeview contains an object in the UserRole called TreeItem which stores if the item is marked as a favorite and also the filepath.In short is there a way to know if a the Decoration 'icon ' is clicked by the user ? The tool simply just goes through a directory recursively and collects the files and folders.Dropbox links to iconshttps : //www.dropbox.com/s/3pt0ev2un7eoswh/file_off.svg ? dl=0https : //www.dropbox.com/s/xext3m9d4atd3i6/file_on.svg ? dl=0https : //www.dropbox.com/s/6d750av0y77hq0g/folder.svg ? dl=0Be sure to change the directory path for testingTool Code :
"I want this code to `` just work '' : Of course , the easy way out is to write int ( c ) /3 , but I 'd like to enable a simpler perl-ish syntax for a configuration mini-language.It 's notable that if I use an `` old-style '' class ( do n't inherit from object ) I can do this quite simply by defining a __coerce__ method , but old-style classes are deprecated and will be removed in python3.When I do the same thing with a new-style class , I get this error : I believe this is by design , but then how can I simulate the old-style __coerce__ behavior with a new-style class ? You can find my current solution below , but it 's quite ugly and long-winded.This is the relevant documentation : ( i think ) Coercion RulesNew-style Special Method LookupBonus points :"
"New way of type/hinting/assignments is cool , but I do n't know how to make such a simple thing work : It throw an error : Using variable 'MyItem ' before assignment.The best but super ugly workaround I found so far is this : Please tell me that language with # 1 principle Beautiful is better than ugly has something better to solve this common typing issue"
"I 'm trying to login to Instagram using requests library . I succeeded using following script , however it does n't work anymore . The password field becomes encrypted ( checked the dev tools while logging in manually ) .I 've tried : I found using dev tools : How can I login to Instagram using requests ?"
"I am using an adapted LeNet model in keras to make a binary classification . I have about 250,000 training samples with ratio 60/40 . My model is training very well . The first epoch the accuracy reaches 97 percent with a loss of 0.07 . After 10 epochs the accuracy is over 99 percent with a loss of 0.01 . I am using a CheckPointer to save my models when they improve.Around the 11th epoch the accuracy drops to around 55 percent with a loss of around 6 . How , could this be possible ? Is it because the model can not be more accurate and it tries to find better weights but completely fails to do so ? My model is an adaptation on the LeNet model :"
"There is a multiple checkbox in template , if value contain in render the choice will checked by default . It works well with 1.10.form.py : template : views.py : When I upgrade to Django 1.11 , { { p.name } } and { { p.choice_value } } return nothing . I know 1.11 has removed choice_value , but how to solve this problem ? 1.10 https : //docs.djangoproject.com/en/1.10/_modules/django/forms/widgets/1.11 https : //docs.djangoproject.com/en/1.11/_modules/django/forms/widgets/"
"I am new to selenium , and I am trying to use Selenium IDE ( 2.9.0 ) to create a first click-and-record script as a basic , which I then refine with Selenium WebDriver ( 2.48.0 ) . I recorded a working script ( see attached at the end of this question ) , and exported it as `` python 2 / unittest / WebDriver '' . However , the source code made it clear , that there are some problems with it ( Commented lines with discomforting `` ERROR '' statements ) : Running this code also did not work . The error is : Do I have to manually fix all the errors ( if so , how ? ) ? Ca n't `` Selenium IDE '' export a working version of the script for the WebDriver ? Do I need to install something else ? Is my general approach completely wrong ? Maybe I should use something else than Selenium ? Here is the original working Selenium IDE Testcase Script ."
"I 'm writing some code which evaluates different sklearn models against some data . I am using type hints , both for my own education and to help other people who will eventually have to read my code . My question is how do I specify the type of a sklearn predictor ( such as LinearRegression ( ) ) ? For example : I see the typing library can make new types or I can use TypeVar to do : but I would n't want to use this if there was already a conventional type for an sklearn model.Checking the type of LinearRegression ( ) yields : and this is clearly of use , but only if I am interested in the LinearRegression model ."
"consider the array aI could doBut this requires finding all np.nan just to find the first.Is there a more efficient way ? I 've been trying to figure out if I can pass a parameter to np.argpartition such that np.nan get 's sorted first as opposed to last.EDIT regarding [ dup ] .There are several reasons this question is different.That question and answers addressed equality of values . This is in regards to isnan.Those answers all suffer from the same issue my answer faces . Note , I provided a perfectly valid answer but highlighted it 's inefficiency . I 'm looking to fix the inefficiency.EDIT regarding second [ dup ] .Still addressing equality and question/answers are old and very possibly outdated ."
"I 'm trying interface a small C function I made into python using SWIG and the Numpy typemapsThis function is defined as followsAnd my interface file is as followsTo test it , I used the following inputBut when I run it , I get the followingI am really confused about how these arguments are defined . Does anyone here have an idea why there are 6 arguments and what these arguments are ? Thanks !"
"This question is a followup to my previous question here : Multi-feature causal CNN - Keras implementation , however , there are numerous things that are unclear to me that I think it warrants a new question . The model in question here has been built according to the accepted answer in the post mentioned above.I am trying to apply a Causal CNN model on multivariate time-series data of 10 sequences with 5 features.What should filters and kernel be set to ? What is the effect of filters and kernel on the network ? Are these just an arbitrary number - i.e . number of neurons in ANN layer ? Or will they have an effect on how the net interprets the time-steps ? What should dilations be set to ? Is this just an arbitrary number or does this represent the lookback of the model ? According to the previously mentioned answer , the input needs to be reshaped according to the following logic : After Reshape 5 input features are now treated as the temporal layer for the TimeDistributed layerWhen Conv1D is applied to each input feature , it thinks the shape of the layer is ( 10 , 1 ) with the default `` channels_last '' , therefore ... 10 time-steps is the temporal dimension1 is the `` channel '' , the new location for the feature mapsAccording to the mentioned answer , the model needs to be reshaped , according to the following logic : Stack feature maps on top of each other so each time step can look at all features produced earlier - ( 10 time steps , 5 features * 32 filters ) Next , causal layers are now applied to the 5 input features dependently.Why were they initially applied independently ? Why are they now applied dependently ? SUMMARYWhat should filters and kernel be set to ? Will they have an effect on how the net interprets the time-steps ? What should dilations be set to to represent lookback of 10 ? Why are causal layers initially applied independently ? Why are they applied dependently after reshape ? Why not apply them dependently from the beginning ? ===========================================================================FULL CODE===========================================================================EDIT : Daniel , thank you for your answer.Question : If you can explain `` exactly '' how you 're structuring your data , what is the original data and how you 're transforming it into the input shape , if you have independent sequences , if you 're creating sliding windows , etc . A better understanding of this process could be achieved.Answer : I hope I understand your question correctly.Each feature is a sequence array of time-series data . They are independent , as in , they are not an image , however , they correlate with each other somewhat.Which is why I am trying to use Wavenet , which is very good at predicting a single time-series array , however , my problem requires me to use multiple multiple features ."
"I have connected to Amazon S3 and am trying to retrieve data from the JSON content from multiple buckets using the below code.But I have to read only specific JSON files , but not all . How do I do it ? Code : Bucket structure example.From the above list , I want to only read *-Account.json files.How can I achieve this ?"
"I am running checkinstall to compile Python on debian.I experienced a bunch of errors before this that would cause it to fail due to not being able to create a directory . At that point , I would just mkdir it myself and it would get past this point . But now I get this error : Does anyone know of anything I might be doing wrong with checkinstall ?"
"A program in both Python and C++ is given below , which performs the following task : read white-space delimited words from stdin , print the unique words sorted by string length along with a count of each unique word to stdout . The format for a line of the output is : length , count , word.For exmaple , with this input file ( 488kB of a thesaurus ) http : //pastebin.com/raw.php ? i=NeUBQ22TThe output , with formatting , is this : Here is the program in C++Here is the Program in Python : For the C++ program , the compiler used was g++ 4.9.0 with the -O3 flag.The version of Python used was 2.7.3Time taken for the C++ program : Time taken for the Python program : The Python program is much faster than the C++ program , and is relatively even faster with larger input sizes . What 's going on here ? Is my use of the C++ STL incorrect ? Edit : As suggested by a comment and an answer , I changed the C++ program to use std : :unordered_set and std : :unordered_map.The following lines were changedThe compilation command : This improved performance only marginally : Edit2 : A much faster program in C++This is a combination of NetVipeC 's solution , Dieter Lücking 's solution , and the top answer to this question . The real performance killer was cin using an unbuffered read by default . Solved with std : :cin.sync_with_stdio ( false ) ; . This solution also uses a single container , taking advantage of the ordered map in C++.RuntimeEdit3 : A nice and concise version of the Python program was provided by Daniel , it runs in about the same time as the version above : Runtime :"
"The following doctest fails : These pagesdoctest & logging use doctest and logging in python programseem to suggest logging.StreamHandler ( sys.stdout ) and logger.addHandler ( handler ) but my attempts failed in this respect . ( I am new to python , if it was n't obvious . ) Please help me fix the above code so that the test passes.Update on Jun 4 , 2017 : To answer 00prometheus ' comments : The accepted answer to use doctest and logging in python program , when I asked this question , seemed unnecessarily complicated . And indeed it is , as the accepted answer here gives a simpler solution . In my highly biased opinion , my question is also clearer than the one I already linked in the original post ."
"Python has a range method , which allows for stuff like : What I ’ m looking for is kind of the opposite : take a list of numbers , and return the start and end.This is easy enough to do for the above example , but is it possible to allow for gaps or multiple ranges as well , returning the range in a PCRE-like string format ? Something like this : Edit : I ’ m looking for a Python solution , but I welcome working examples in other languages as well . It ’ s more about figuring out an elegant , efficient algorithm . Bonus question : is there any programming language that has a built-in method for this ?"
How can I add an argument that is optional and must not be specified multiple times ? Valid : Invalid : If I add an argument like : The invalid example results in a string ThisIsNotValid . I would expect a parser error .
"I 'm trying to use multiple inputs in custom layers in Tensorflow-Keras . Usage can be anything , right now it is defined as multiplying the mask with the image . I 've search SO and the only answer I could find was for TF 1.x so it did n't do any good ."
"Below is a significantly simplified version on my code . After the __init__ ( ) there are several functions . I am trying to use functools.partial to create different versions of a basic comparison function , which references a function created earlier in the class , calculation . One version of this comparison function might be the grade_comparison seen below.When I run my code , there is an error , NameError : global name 'self ' is not defined . I have tried adding self in many combinations that seemed logical - one example below.This change resulted in this error , NameError : name 'self ' is not definedWhen I add self to the comparison function ( see below ) : I get this error , TypeError : comparison ( ) takes at least 2 arguments ( 1 given ) . Please let me know if you need more context ! As mentioned , this is the barebones of the code ."
"I 've written a server using Python and the Twisted library that communicates via UDP . This all works well.What I would like to do is extend that server so that it can accept messages on multiple UDP ports simultaneously ( I use the different ports to segregate the information returned , and it 's less about the source of the request ) . I 've tried to do the simplest thing first as a test - I wrote code that looks like this : ( The first line is the one originally in my server ; the second line is the 2nd port to listen to . ) When I run that , I get the following : I take from this that I ca n't add a second UDP listener , but that 's exactly what I 'd like to do ( actually , to make a total of 18 listeners - it 's a long story ) . Any thoughts on how to do this ?"
"In Python , is it safe to give keyword arguments that are not Python identifiers to a function ? Here is an example : I am asking this because it is more convenient for me to format with names that contain a dash ( because the values correspond to command-line argument with dashes in their name ) . But is this behavior reliable ( i.e . can it vary between version of Python ) ? I am not sure that using non-identifiers as keyword arguments is officially supported : in fact , the documentation reads : If the syntax **expression appears in the function call , expression must evaluate to a mapping , the contents of which are treated as additional keyword arguments.… where `` keyword arguments '' are defined as having a name which is an identifier : keyword_arguments : := keyword_item ( `` , '' keyword_item ) * keyword_item : := identifier `` = '' expressionwhere identifiers are restricted in what characters they can use ( - is for instance forbidden ) : identifier : := ( letter| '' _ '' ) ( letter | digit | `` _ '' ) *So , the documentation indicates that the mapping given to ** in a function call should only contain valid identifiers as keys , but CPython 2.7 accepts more general keys ( for format ( ) and functions with a ** argument , which do not put values in variables ) . Is this a reliable feature ?"
"I want to perform principal component analysis for dimension reduction and data integration.I have 3 features ( variables ) and 5 samples like below . I want to integrate them into 1-dimensional ( 1 feature ) output by transforming them ( computing 1st PC ) . I want to use transformed data for further statistical analysis , because I believe that it displays the 'main ' characteristics of 3 input features.I first wrote a test code with python using scikit-learn like below . It is the simple case that the values of 3 features are all equivalent . In other word , I applied PCA for three same vector , [ 0 , 1 , 2 , 1 , 0 ] .CodeOutputIs taking 1st PCA after dimension reduction proper approach for data integration ? 1-2 . For example , if features are like [ power rank , speed rank ] , and power have roughly negative correlation with speed , when it is a 2-feature case . I want to know the sample which have both 'high power ' and 'high speed ' . It is easy to decide that [ power 1 , speed 1 ] is better than [ power 2 , speed 2 ] , but difficult for the case like [ power 4 , speed 2 ] vs [ power 3 , speed 3 ] . So I want to apply PCA to 2-dimensional 'power and speed ' dataset , and take 1st PC , then use the rank of '1st PC ' . Is this kind of approach still proper ? In this case , I think the output should also be [ 0 , 1 , 2 , 1 , 0 ] which is the same as the input . But output was [ -1.38564065 , 0.34641016 , 2.07846097 , 0.34641016 , -1.38564065 ] . Are there any problem with the code , or is it the right answer ?"
"I have written python code that scrapes all the data from the PDF file . The problem here is that once it is scraped , the words lose their grammer . How to fix these problem ? I am attaching the code.and here is the screenshot of PDF ."
"I have downloaded 5MB of a very large json file . From this , I need to be able to load that 5MB to generate a preview of the json file . However , the file will probably be incomplete . Here 's an example of what it may look like : From here , I 'd like to `` rebuild it '' so that it can parse the first two objects ( and ignore the third ) . Is there a json parser that can infer or cut off the end of the string to make it parsable ? Or perhaps to 'stream ' the parsing of the json array , so that when it fails on the last object , I can exit the loop ? If not , how could the above be accomplished ?"
"I have a numpy array : I want the top 3 items . Callingreturns Notice values foo [ 1 ] and foo [ 4 ] are equal , so numpy.argsort ( ) handles the tie by returning the index of the item which appears last in the array ; i.e . index 4.For my application I want the tie breaking to return the index of the item which appears first in the array ( index 1 here ) . How do I implement this efficiently ?"
"I have a code which look like this : I am trying to add type annotations to the some_callback function , but I ca n't fully understand hoe to annotate the result variable . Should it be Coroutine ? Or maybe Awaitable ? When I am using the reveal_type of mypy , the output about the result variable is Any.The output of this program is : How should I document this function properly ?"
"I building SPA on Django and I have one huge function with many if statement for checking state name of my object field . Like this : and so on . I reading nice book `` Fluent python '' now , and I mention about @ singledispatch decorator , it looks so great , but it can overide function only with diferent type of parametres like str , int , etc.Question is , if there in python or Django way to separate logic like in my huge function with overided function like singledispatch do ?"
In /tmp/spam.py : pep8 utility complains about this conditional : the first suggestion is wrong/ '' worse '' according to pep8 itselfthe second suggestion changes the behaviour of the codeWhat is the best practice for a case where you actually do want to check equality with True ? Is identity checking with True using is OK ? Why does pep8 utility offer an alternative which is explicitly discouraged in pep8 itself ?
"I noticed that in theano , when one creates a shared variable based on 1D numpy array , this becomes a vector , but not a row : The same goes for a 1xN matrix , it becomes a matrix but not a row : This is troublesome when I want to add a M x N matrix to a 1 X N row-vector , because the shared vector is not broadcastable in the first dimension . First of all , this will not work : With the error : Ok , that 's clear , we ca n't assign vectors to rows . Unfortunately , this is also not fine : With the error : So we ca n't just use a 1 x N matrix as a row as well ( because the first dimension of a 1 x N matrix is not broadcastable ) .The question remains , what 'can ' we do ? How can I create a shared variable of type row , such that is i broadcastable using matrix-row addition ?"
I am trying to create an ordered category from the above dataframe using the following code -However it gives the error : astype ( ) got an unexpected keyword argument 'categories ' .
"I am kind of new to Python , so I am trying to read over existing code . I am a little confused on the syntax of this though.For example : I understand that select.select ( ) takes 3 lists ( and I assume [ ] just means empty list ) , but is the _ used to denote a placeholder of some sort ?"
"I 'm trying to create an install package for a Python project with included unit tests . My project layout is as follows : My setup.py looks like this : The file tests/disttest/testcore.py contains the line from disttest.core import DistTestCore.Running setup.py test now gives an ImportError : No module named core.After a setup.py install , python -c `` from disttest.core import DistTestCore '' works fine . It also works if I put import core into src/disttest/__init__.py , but I do n't really want to maintain that and it only seems necessary for the tests.Why is that ? And what is the correct way to fix it ?"
"I 'm developing an application on Python Google App Engine and I 'm using the BlobStore to store image data.I 've backed up all my database information to my local host to set up a local development environment but I want to use the Blobs Images from my Production server ( so I do n't have to copy all files to my local computer ) .When I call images.get_serving_url ( ) to get the image url it returns a local reference to it that does n't exist ( since all the images are in the prod server ) .Is there a way to configure the images class to point to my prod server ? If the answer is no , how can I redirect the calls from my dev server that hit /_ah/img/ to my prod server ? I wish the output from this call was my prod server url ."
"I 've got a fun one ! And I 've tried to find a duplicate question but was unsuccessful ... My dataframe consists of all United States and territories for years 2013-2016 with several attributes.I want to groupby year and state , and show the top 3 states ( by 'enrollees ' or 'utilizing ' - does not matter ) for each year.Desired Output : So far I 've tried the following : df.groupby ( [ 'year ' , 'state ' ] ) [ 'enrollees ' , 'utilizing ' ] .sum ( ) .head ( 3 ) Which yields just the first 3 rows in the GroupBy object : I 've also tried a lambda function : Which yields the absolute largest 3 in the GroupBy object : I think it may have to do with the indexing of the GroupBy object , but I am not sure ... Any guidance would be appreciated !"
I have a executable that accepts string and outputs another string . Now I need to feed a file to it as input and write output to another file . The standard command for that is like the following executable_path < input > output . Now I wrap this in python . But I get errors . invalid argument : < I also tried joining the cmd arguments : invalid argument : < tmp/input.txt > tmp/output.txtPassing the command as string did n't work either . OSError : [ Errno 2 ] No such file or directoryWhat am I missing here ?
"I am working with data from pharmaceutical labels . The text is always structured using the verb phrase 'indicated for'.For example : I have already used SpaCy to filter down to only sentences that contain the phrase 'indicated for ' . I now need a function that will take in the sentence , and return the phrase that is the object of 'indicated for ' . So for this example , the function , which I have called extract ( ) , would operate like this : Is there functionality to do this using spacy ? EDIT : Simply splitting after 'indicated for ' wo n't work for complicated examples.Here 's some examples : ' '' buprenorphine and naloxone sublingual tablets are indicated for the maintenance treatment of opioid dependence and should be used as part of a complete treatment plan to include counseling and psychosocial support buprenorphine and naloxone sublingual tablets contain buprenorphine a partial opioid agonist and naloxone an opioid antagonist and is indicated for the maintenance treatment of opioid dependence '' ' '' 'ofloxacin ophthalmic solution is indicated for the treatment of infections caused by susceptible strains of the following bacteria in the conditions listed below conjunctivitis gram positive bacteria gram negative bacteria staphylococcus aureus staphylococcus epidermidis streptococcus pneumoniae enterobacter cloacae haemophilus influenzae proteus mirabilis pseudomonas aeruginosa corneal ulcers gram positive bacteria gram negative bacteria staphylococcus aureus staphylococcus epidermidis streptococcus pneumoniae pseudomonas aeruginosa serratia marcescens '' 'where I just want the bold parts ."
"I am trying to fill in the blank using a bidirectional RNN and pytorch . The input will be like : The dog is _____ , but we are happy he is okay.The output will be like : I discovered this idea here : https : //medium.com/ @ plusepsilon/the-bidirectional-language-model-1f3961d1fb27I am trying to reimplement the code above found at the bottom of the blog post . I am new to pytorch and nlp , and ca n't understand what the input and output to the code is.Question about the input : I am guessing the input are the few words that are given . Why does one need beginning of sentence and end of sentence tags in this case ? Why do n't I see the input being a corpus on which the model is trained like other classic NLP problems ? I would like to use the Enron email corpus to train the RNN.Question about the output : I see the output is a tensor . My understanding is the tensor is a vector , so maybe a word vector in this case . How can you use the tensor to output the words themselves ?"
"I 'm dealing with data input in the form of json documents.These documents need to have a certain format , if they 're not compliant , they should be ignored . I 'm currently using a messy list of 'if thens ' to check the format of the json document.I have been experimenting a bit with different python json-schema libraries , which works ok , but I 'm still able to submit a document with keys not described in the schema , which makes it useless to me.This example does n't generate an exception although I would expect it : My question is twofold : Am I overlooking something in the schema definition ? If not , is there another lightweight way to approach this ? Thanks , Jay"
"I am trying to perform mutation on User models declared using SQL ALCHEMY.Here is the code for my models.py file This is Schema.py fileWhen i try performing following mutation , this is the error i get :"
"I want to implement Karatsuba 's 2-split multiplication in Python . However , writing numbers in the formwhere x is a power of the base ( let x=b^m ) close to sqrt ( A ) .How am I supposed to find x , if I ca n't even use division and multiplication ? Should I count the number of digits and shift A to the left by half the number of digits ? Thanks ."
"I 'm trying to use mrjob for running hadoop on EMR , and ca n't figure out how to setup logging ( user generated logs in map/reduce steps ) so I will be able to access them after the cluster is terminated.I have tried to setup logging using the logging module , print and sys.stderr.write ( ) but without luck so far . The only option which works for me is to write the logs to a file then SSH the machine and read it , but its cumbersome . I would like my logs to go to stderr/stdout/syslog and be automatically collected to S3 , so I can view them after the cluster is terminated.Here is the word_freq example with logging :"
"Keywords have to be stringsBut by some black magic , namespaces are able to bypass thatWhy ? And how ? Could you implement a Python function that can receive integers in the kwargs mapping ?"
"I know this is a borderline case whether it really belongs to stackoverflow or superuser , but as it seems there are quite a few 'editing code ' questions over here , I am posting it on SO.I have a pile of XML files that someone in their infinite wisdom have decided to explode to a multiple files using the tags , which in result makes debugging/editing them a huge P-i-t-A . Therefore I am looking for : A way in VIM to open them in a single buffer ( preferably so that the changes are saved in correct external entity files ) , OR ; A way to expand the files in VIM so that the external entities are read and replaced in the buffer , OR ; an easy bash/sed/python way of doing this on a command line ( or in .vimrc ) The files included on top level might include new files and so on on who knows on how many levels so this needs to be recursive ... Here 's a mockup sample on what the top level file looks like : EDIT : The list is in order of preference - if no 1. or 2. solutions are available , the bounty goes for the best # 3 ... EDIT 2 : Looks like @ Gaby 's answer works , but unfortunately only partially , unless I am doing something wrong - I 'll write some sort of tool using his answer and post it here for improvements . Of course , a # 1 or # 2 solution would be appreciated ... : ) EDIT 3 : Ok , the best non-Emacs -answer will get the bounty ; ) Conclusion : Thanks to @ hcayless I now have a working # 2 solution , I added : to my .vimrc and everything is hunky dory ."
I 'm trying to plot a line graph of ten values with error ranges over each point : I get the error messageCan someone please advise me the best way to plot a line graph with error bars over each point ? http : //matplotlib.org/api/pyplot_api.html # matplotlib.pyplot.errorbarthx
"I have a list of items stored in a remote database which may be unsorted , and I want to sort them . The database accepts commands of them form : So , given a list of the form : ... how can I get the sequence of moves : I assume a modification of the bubblesort algorithm would work , but I 'm specifically looking for the most efficient implementation that is still pythonic , and that generates the fewest move commands.UPDATE : the list is 1000-10000 long , and all items are unique - no repeats . Only a very small number of items - 1-10 - will be in the wrong place at any given time . Time is a concern - it should take seconds , not minutes - but it does not have to be extremely fast.UPDATE 2 : I would also like to move each item only once"
"Given a numpy array of size ( n , ) how do you transform it to a numpy array of size ( n,1 ) .The reason is because I am trying to matrix multiply to numpy arrays of size ( n , ) and ( , n ) to get a ( n , n ) but when I do : It says that you ca n't do it . I know as a fact that transposing a ( n , ) does nothing , so it would just be nice to change the ( n , ) and make them ( n,1 ) and avoid this problem all together ."
"I am struggling to parse nested structures with PyParsing . I 've searched many of the 'nested ' example uses of PyParsing , but I do n't see how to fix my problem.Here is what my internal structure looks like : and here is what my external structure looks like , but it can contain zero or more of the internal structures.I am successfully parsing the internal structure . Here is my code for that.Here is my attempt to parse the outer structure , When I say : pass_.parseString ( testPassStr ) I see errors in the console that `` } '' was expected.I see this as very similar to the C struct example , but I 'm not sure what is the missing magic . I 'm also curious how to control the resulting data structure when using the nestedExpr ."
"Is it possible to perform simple math on the output from Python regular expressions ? I have a large file where I need to divide numbers following a `` ) '' by 100 . For instance , I would convert the following line containing ) 75 and ) 2 : to ) 0.75 and ) 0.02 : My first thought was to use re.sub using the search expression `` \ ) \d+ '' , but I do n't know how to divide the integer following the parenthesis by 100 , or if this is even possible using re.Any thoughts on how to solve this ? Thanks for your help !"
"I am using mypy to check my Python code.I have a class where I set dynamically some attributes and mypy keep on complaining about it : This is my code : Obviously , there could be 3 ways to solve the issueIgnoring the issue with # type : ignore : toto.age = 10 # type : ignore # ... Using setattr to set the age of toto : setattr ( toto , `` age '' , 10 ) Setting the attributes explicitly ( self.age = 0 ... ) However , I am looking for a more elegant and systematic way at the class level.Any suggestion ?"
"I have a 2D array v , v.shape= ( M_1 , M_2 ) , which I want to reshape into a 3D array with v.shape= ( M_2 , N_1 , N_2 ) , and M_1=N_1*N_2 . I came up with the following lines which produce the same result : andfor reshape_tuple= ( M_2 , N_1 , N_2 ) .Which one is computationally better and in what sense ( comp time , memory , etc . ) if the original v is a huge ( possibly complex-valued ) matrix ? My guess would be that using the transpose is better , but if reshape does an automatic ravel then maybe the ravel-option is faster ( though reshape might be doing the ravel in C or Fortran and then it 's not clear ) ?"
"I have a python script run by jenkins . logging module is used.If the above config removed , I can not find the log generated by the following statement : I did not find it in syslog of the jenkins machine . I did not find it in the jenkins console output.Any hints ? Thanks"
"Background : As a short project over winter break , I 'm trying to implement a programming language called Axe ( designed for graphing calculators ) using Python and PLY . A brief note : the language allows only global variables and makes heavy use of pointers.I 'm trying to implement goto in this language , but have no idea how to do it.My general method is to first use PLY to parse the code into an ast , then walk through it executing as I go.For example , the statement ... would turn into ... ... which I would execute recursively ( I added indents for readability ) .Because the ast is a tree , I 'm not sure how to jump between different nodes . I 've considered perhaps converting the tree to a flat-ish array [ 'IF ' , [ 'CONDITION ' , 3 ] , [ 'DISP ' , 4 ] , [ 'DISP ' , 6 ] ] so that I can use the indices of the flat-ish array to go to specific lines in the code , but this seems to lack a certain elegance and almost feels like a step backwards ( although I could be wrong ) .I 've looked at this , but was unable to understand how it worked.Any help or hints would be appreciated ."
"I have a list of lists , which looks likeI want to count the number of lists which have a particular element . For Example , my output should be As you can see , I do n't need the total count of an element . In the case of `` c '' , though its total count is 5 , the output is 3 as it occurs only in 3 lists.I am using a counter to get the counts . The same can be seen below.So , when I print count , I getI want to know if there 's a much better way to accomplish my goal ."
"I 'm having trouble constructing a basic BNN in TFP . I 'm new to TFP and BNNs in general , so I apologize if I 've missed something simple.I can train a basic NN in Tensorflow by doing the following : However , I have trouble when trying to implement a similar architecture with tfp DenseFlipout layers : I get the following Value error : I 've done some googling , and have looked around the TFP docs , but am at a loss so thought I would share the issue . Have I missed something obvious ? Thanks in advance ."
"[ Updated ] : Answer inline below questionI have an inspecting program and one objective is for logic in a decorator to know whether the function it is decorating is a class method or regular function . This is failing in a strange way . Below is code run in Python 2.6 : Then on execution : Any clue on what 's going wrong , and if it is possible for @ decorate to correctly infer that test_call is a method ? [ Answer ] carl 's answer below is nearly perfect . I had a problem when using the decorator on a method that subclasses call . I adapted his code to include a im_func comparison on superclass members :"
"I have a sklearn pipeline performing feature engineering on heterogeneous data types ( boolean , categorical , numeric , text ) and wanted to try a neural network as my learning algorithm to fit the model . I am running into some problems with the shape of the input data . I am wondering if what I am trying to do is even possible and or if I should try a different approach ? I have tried a couple different methods but am receiving these errors : Error when checking input : expected dense_22_input to have shape ( 11 , ) but got array with shape ( 30513 , ) = > I have 11 input features ... so I then tried converting my X and y to arrays and now get this errorValueError : Specifying the columns using strings is only supported for pandas DataFrames = > which I think is because of the ColumnTransformer ( ) where I specify column namesI think I understand the problem here however not sure how to solve it . If it is not possible to integrate sklearn ColumnTransformer+Pipeline into Keras model does Keras have a good way for dealing with fixed data types to feature engineer ? Thank you !"
"You are confronted with an enemy within a rectangular shaped room and you 've got only a laser beam weapon , the room has no obstructions in it and the walls can completely reflect the laser beam . However the laser can only travels a certain distance before it become useless and if it hit a corner it would reflect back in the same direction it came from.That 's how the puzzle goes and you are given the coordinates of your location and the target 's location , the room dimensions and the maximum distance the beam can travel . for example If the room is 3 by 2 and your location is ( 1 , 1 ) and the target is ( 2 , 1 ) then the possible solutions are : I tried the following approach , start from the source ( 1 , 1 ) and create a vector at angle 0 radians , trace the vector path and reflections until either it hits the target or the total length of the vectors exceeds the max allowed length , repeat with 0.001 radians interval until it completes a full cycle . This the code I have so far : The code works somehow but it does n't find all the possible solutions , I have a big precision problem in it , I dont ' know how many decimals should I consider when comparing distances or angles . I 'm not sure it 's the right way to do it but that 's the best I was able to do.How can I fix my code to extract all solutions ? I need it to be efficient because the room can get quite large ( 500 x 500 ) . Is there a better way or maybe some sort of algorithm to do this ?"
"I 'm developing with Python 3.4 on Ubuntu 14.04 . I was trying to do recursive Pool.map ( ) . After I invoke g ( ) , it hangs there and never returns ."
I have the following text : What I want to do with this is remove the html tags and encode it into unicode . I am currently doing : Which only strips the tag . How would I correctly encode the above for database storage ?
"If I have an expression that I wish to evaluate in Python , such as the expression for r in the code snippet below , will the Python interpreter be smart and reuse the subresult x+y+z , or just evaluate it twice ? I 'd also be interested to know if the answer to this question would be the same for a compiled language , e.g . C.It has been suggested that this question is similar to this question . I believe it is similar . However , I believe the linked question is less of a 'minimal example ' . Also in the linked question there is no ambiguity as to the order of operations , for example , in examples similar to this question , where there is no defined order for operations ( mathematically ) , depending on the order of the individual function calls ( which are ambiguous ) , a worse or better job of optimisation may be done . Consider ( abba ) ( abba ) ( baa*b ) , there are nested repeated substrings , and depending on the order and amount of preprocessing many different optimisations could be performed ."
Please help me to understand : what is a view in Pandas . I know that if we change something in a view we always make changes in the original object.But a view of an object and the original object have different id 's for example . Does it mean that the view is another object with reference to original object ? What is the mechanism ? I tried but ca n't find an explanation .
"I have two lists in python list_a and list_b . The list_a have some images links , and the list_b too . 99 % of the items are the same , but i have to know this 1 % . The all surplus items are in list_a , that means all items in list_b are in list_a . My initial idea is subtract all items : list_a - list_b = list_c , where the list_c are my surplus items . My code is : I think the logic is right , if i have some items , the code is run fast . But i dont have 10 items , or 1.000 , or even 100.000 . I have 78.514.022 items in my list_b.txt and 78.616.777 in my list list_a.txt . I dont't know the cost of this expression : if a not in arq_b . But if i execute this code , i think wont finish in this year.My pc have 8GB , and i allocate 15gb for swap to not explode my RAM.My question is , there 's another way to make this operation more efficiently ( Faster ) ? The list_a is ordinate but the list_b not.Each item have this size : images/00000cd9fc6ae2fe9ec4bbdb2bf27318f2babc00.pngThe order doesnt matter , i want know the surplus ."
"Is there any way in argparse to parse flags like [ +- ] a , b , c , d ? should store True in the dest of s and False in the dest of b , much like done by the Windows attrib or the Linux chmod.Currently , I am using 2 separate arguments +s and -s with store_true and store_false , respectively . But it creates an ugly help with it listing each flag twice ( +a & -a ) Another workaround would be to manually parse the extended arg with regex ( which somehow seems a lot easier and use custom description , but before doing that I just wanted to look around if there was anything using which I could perform the same thing using argparse itself ."
"I want to run multiple train_op in parallel in a tensorflow session . The answer here says that tensorflow sess.run ( ) can release the GIL of python . I try the example in that anwser , but it seems that we still have a GIL . I have 8 GPUs available . When num_threads is 4 , it takes 24 seconds . When num_threads is 8 , it takes 54 seconds . Here is the code : my question is whether it is because I did not implement the method correctly . If this way can not release the GIL , then how to release the GIL ? I know distributed tensorflow via gRPC can release the GIL , but gRPC is expensive comparing to multithreading ( like pthread in C ) . I want each thread communicating with each other , and I want to reduce the communication overhead as much as possible . Any answer or hint would be really appreciated ! If there is no way to release GIL , is it possible to write a c++ extension to do multithreading . If not , is it possible to use other language which does not have GIL other than python . Thanks !"
"Trying to plot observations respectively to multiple scales per observation , I 've managed to produce the following plot : However I would like to add a tick presenting the y-max value in each scale , regardless of the gap between it and the previous tick . An example of a such plot is presented below . It is produced when the y-max is a multiple of the ticking interval . Thanks , F.Here is the code used to produce these example ."
"Is it possible to get the `` current element '' inside scipy.ndimage.filters.generic_filter 's filter function ? If , for example , A [ 0 ] always contained the current element ( which does n't seem to be the case ) something like the following might find local maxima"
"The documentation discusses using numba 's cfuncs as LowLevelCallable argument of scipy.integrate.quad . I need the same thing with additional parameter.I 'm basically trying to do something like this : However , it does not work , because params are supposed to be voidptr/void* and they can not be transformed to double . I have the following error message : I did n't find any information on how to extract values from void* in Numba . In C , it should be something like a = * ( ( double* ) params ) — is it possible to do the same thing in Numba ?"
"I am playing around with an xml file found @ http : //www.jsphylosvg.com/examples/source.php ? example=2 & t=xml I would like to insert a node if the value of the node name= '' Espresso '' . E.g . I would like to change from : to : Based on the research I have done thus far I can use xpath to find the node that contains espresso ( this should work , but it does not ? ) At this point , it should be possible to use use lxml.etree.Element to make the xml node , and use insert to attach them into xml documentHowever , while this sounds great in theory , I am unable to get it to work.I would really appreciate any help/suggestions"
"I 'm scraping a website to export the data into a semantic format ( n3 ) .However , I also want to perform some data analysis on that data , so having it in a csv format is more convenient.To get the data in both formats I can doHowever , this scrapes the data twice and I can not afford it with big amounts of data.Is there a way to export the same scraped data into multiple formats ? ( without downloading the data more than once ) I find interesting to have an intermediate representation of the scraped data that could be exported into different formats . But it seems there is no way to do this with scrapy ."
"I 'm looking at this library , which has little documentation : https : //pythonhosted.org/parsec/ # examplesI understand there are alternatives , but I 'd like to use this library.I have the following string I 'd like to parse : While I 'd like to parse the whole thing , I 'd settle for just grabbing the < tags > . I have : OK , looks good . Now to use it : This fails . I 'm afraid I do n't even understand what it meant about my lambda expression giving two arguments , it 's clearly 1 . How can I proceed ? My optimal desired output for all the bonus points is : The output I 'd settle for understanding in this question is using functions like those described above for start and end tags to generate : And simply be able to parse arbitrary xml-like tags out of the messy mixed text entry ."
i am opening a csv file : does the file stay in memory if i quit the program inside the WITH loop ? what happens to the variables in general inside the program when i quit the program without setting all variables to NULL ?
"I was surprised to know that Python 3.5.2 is much slower than Python 2.7.12 . I wrote a simple command line command that calculates the number of lines in a huge CSV-file.Python 2.7.12 took 15 seconds , Python 3.5.2 took 66 seconds . I expected that the difference may take place , but why is it so huge ? What 's new in Python 3 that makes it much slower towards such kind of tasks ? Is there a faster way to calculate the number of lines in Python 3 ? My CPU is Intel ( R ) Core ( TM ) i5-3570 CPU @ 3.40GHz.The size of huge.csv is 18.1 Gb and it contains 101253515 lines.Asking this question , I do n't need exactly to find the number of lines of a big file at any cost . I just wrote a particular case where Python 3 is much slower . Actually , I am developing a script in Python 3 that deals with big CSV files , some operations do n't suppose of using csv library . I know , I could write the script in Python 2 , and it would be acceptable towards the speed . But I would like to know a way to write similar script in Python 3 . This is why I am interested what makes Python 3 slower in my example and how it can be improved by `` honest '' python approaches ."
"I have a code like this : In the line with a comment I actually want not exactly the Derived object , but any object of class that self really is.Here is a real-life example from Mercurial.How to do that ?"
"I 'm having trouble installing python packets using pip in a virtualenv . After some investigations , it turns out there seems to be a problem with PyYaml . I downloaded the last version and tried to build it . When using the `` system '' python , there is no problem . However , when I try to run the same command ( python setup.py install ) after activating my virtualenv , I get the following error : I have no idea where it comes from ... Any clue ?"
"I create a class whose objects are initialized witha bunch of XML code . The class has the ability to extract various parameters out of that XML and to cache them inside the object state variables . The potential amount of these parameters is large and most probably , the user will not need most of them . That is why I have decided to perform a `` lazy '' initialization . In the following test case such a parameter is title . When the user tries to access it for the first time , the getter function parses the XML , properly initializes the state variable and return its value : This looks nice and works fine for me . However , I am disturbed a little bit by the fact that the getter function is actually a `` setter '' one in the sense that it has a very significant side effect on the object . Is this a legitimate concern ? If so , how should I address it ?"
Say I have a list with one or more tuples in it : What 's the best way to get rid of the tuples so that it 's just an int list ?
"I am trying to pick up python and as someone coming from Javascript I have n't really been able to understand python 's regex package reWhat I am trying to do is something I have done in javascript to build a very very simple templating `` engine '' ( I understand AST is the way to go for anything more complex ) : In javascript : In Javascript that will result in : `` One To Rule All testing this . { { _thiswillNotMatch } } One To Rule All '' And the function will get called twice with : and : Now , in python I have tried looking up docs on the re packageHave tried doing something along the lines of : But it really does n't make sense to me and does n't work . For one , I 'm not clear on what this group ( ) concept means ? And how am I to know if there is match.group ( n ) ... group ( n+11000 ) ? Thanks !"
"The Python 2.7 docs state that assertItemsEqual `` is the equivalent of assertEqual ( sorted ( expected ) , sorted ( actual ) ) '' . In the below example , all tests pass except for test4 . Why does assertItemsEqual fail in this case ? Per the principle of least astonishment , given two iterables , I would expect that a successful assertEqual implies a successful assertItemsEqual . Here is the output on my machine :"
"If I create a setup.py using requires , Pip does n't install my dependencies.Here 's my setup.py : I wrote a simple sample.py : I then try to install it : Note that requests , my dependency is n't installed . If I now try to use my test project : What am I doing wrong ?"
"I have a Python 3 project.On install , I would like to move my latex files to known directory , say /usr/share/mkc/latex , so I 've told setuptools to include data filesNow when I runorI get the following error : Running just ./setup.py bdist works fine , so the problem must be in package creation ."
"I 'm looking to loop over an array with Go templates and I want to add an extra string to the last item in the loop.In python , I can do Looking to achieve same thing with Go , below is the snippet of the Go equivalent.Thanks ."
"This works as expected : I can also call the base class explicitly : I was wondering why I ca n't omit the first argument to super , like this : when the result of the super call without a second argument seems to be a class type inside a super type : I guess I 'm just wondering about the design here . Why would I need to pass the class object into the super call to get a reference to the base class type Foo ? For a normal method , it makes sense to pass self to the function , since it needs to bind the base class type to an actual instance of the class . But a classmethod does n't need a specific instance of the class.EDIT : I get the same error in Python 3.2 as I do above in 2.7 for super ( Bar ) .hello ( ) . However , I can simply do super ( ) .hello ( ) and that works fine ."
"Why does D.__class__ return the name of the class , while D ( ) .__class__ returns the defined attribute in class D ? And from where do builtin attributes such as __class__ and __name__ come from ? I suspected __name__ or __class__ to be simple descriptors that live either in object class or somewhere , but this ca n't be seen.In my understanding , the attribute lookup rule as follows in Python , omitting the conditions for descriptors etc.. : Instance -- > Class -- > Class.__bases__ and the bases of the other classes as wellGiven the fact that a class is an instance of a metaclass , type in this case , why D.__class__ does n't look for __class__ in D.__dict__ ?"
"Following a remark here : How to define a new string formatter , I tried subclassing string.Formatter . Here is what I 've done . Unfortunately I seem to have broken it in the processAs if as in the last line , I do n't refer to the variables by position , I get a KeyError . It is obviously expecting a key which is optional in the original class but I do n't understand why and I am not sure what I 've done wrong ."
I often find myself doing this : Is there a more concise way to do this in Python ? I am thinking of something along the lines of
"I ’ m working with geo-located social media posts and clustering their locations ( latitude/longitude ) using DBSCAN . In my data set , I have many users who have posted multiple times , which allows me to derive their trajectory ( a time ordered sequence of positions from place to place ) . Ex : I have derived trajectories for my entire data set , and my next step is to cluster or aggregate the trajectories in order to identify areas with dense movements between locations . Any ideas on how to tackle trajectory clustering/aggregation in Python ? Here is some code I 've been working with to create trajectories as line strings/JSON dicts : EDITI 've come across a python package , NetworkX , and was debating the idea of creating a network graph from my clusters as opposed to clustering the trajectory lines/segments . Any opinions on clustering trajectories v.s . turning clusters into a graph to identify densely clustered movements between locations . Below is an example of some clusters look like :"
"I 'm using Flask to expose a local directory of HTML files on a web page.I am also using a jinja2 to generate the sitemap in the lefthand div of my main endpoint.I am unable to correctly specify the URL to the endpoint of my subfolders.As mentioned in the code below , how would I dynamically build a relative link from /docs ( i.e . /docs/folder1/subfolder1/SubFolder1Page.html ) ? The way I am currently setting the value for href obviously does not work.Folder structure example : How it looks overall displaying the contents of file1.html :"
"I have an array : I want to add some other array into each index of a , while the index can appear more than one times . I want to get the some of each index . I write : but get a to be : But what I want is to get : How to implement this in numpy without for loop ?"
"I 'm currently building an application where I need to iterate over a series of steps that do largely the same thing , save a very small amount of code ( ~15 lines ) . The number of steps will vary depending on how the project is configured , so it seems kind of silly for me to create a separate function for each potential instance.In JavaScript , I would do something like this : Is there a way to do something similar to this in python ? The only thing I can think of is something like this : This just seems terribly inefficient if I have any significant number of steps . Is there any better way to go about this ?"
"I am trying to script a HTTP POST request with python.When trying it with curl from bash , everything is working . With python , using either the requests or the urllib3-library , I am getting an error response from the API . The POST request contains information in headers and as json in the request body.What I noticed , when I intercept the packets with Wireshark , the curl-request ( which is working ) is one single packet of length 374 bytes . The python-request ( no difference between requests and urllib3 here ) is splitted into 2 separate packets of 253 and 144 bytes length . Wireshark reassembles these without problems and they both seem to contain the complete information in header and POST body . But the API I am trying to connect to answeres with a not very helpful `` Error when processing request '' .As the 253 bytes ca n't be the limit of a TCP-packet , what is the reason for that behavior ? Is there a way to fix that ? EDIT : bash : python :"
"We are upgrading django-rest-framework from 3.1.3 to 3.5.3 . After the upgrade all of our ModelViewSet and viewsets.GenericViewSet views that utilize DefaultRouter to generate the urls no longer allow HEAD method calls . I 've searched through the release notes and docs and have n't been able to find any setting or change that caused HEAD to stop being allowed.I 'm able to resolve this issue by subclassing the DefaultRouter and altering the route defaults , but I do n't think this is the best or correct solution . From reading within django-rest-framework issues and documentation , it appears that django-rest-framework should handle HEAD and OPTIONS methods automatically . @ detail_route , @ list_route , and views derived from ApiView which allow the GET method are automatically gaining the HEAD and OPTION methods.Why has the HEAD method disappeared after this upgrade and what is the correct method of ensuring HEAD methods are allowed on our routes ? Our route and ModelViewSet definitions are very standard , here is a non working route : And the view : Postman response to a HEAD call :"
"Suppose I have a Python list of arbitrary length k. Now , suppose I would like a random sample of n , ( where n < = k ! ) distinct permutations of that list . I was tempted to try : But , naturally , this code becomes unusably slow when k gets too large . Given that the number of permutations that I may be looking for n is going to be relatively small compared to the total number of permutations , computing all of the permutations is unnecessary . Yet it 's important that none of the permutations in the final list are duplicates.How would you achieve this more efficiently ? Remember , mylist could be a list of anything , I just used list ( range ( 0 , k ) ) for simplicity ."
"Can anyone see why this is n't working ? Its trying to do ; if Column Name Contains the text 'Andy ' , then make a column called Andy and set that row = to 1"
"In both Python2 and Python3 , in the stack trace the __name__ of a function is not used , the original name ( the one that is specified after def ) is used instead.Consider the example : The output is : Why so ? How do I change the name that is used in the stack trace ? Where is the __name__ attribute used then ?"
"I 'm wondering if anyone can think up a way to check if a function needs to return a meaningful value in Python . That is , to check whether the return value will be used for anything . I 'm guessing the answer is no , and it is better to restructure my program flow . The function in question pulls its return values from a network socket . If the return value is not going to get used , I do n't want to waste the resources fetching the result.I tried already to use tracebacks to discover the calling line , but that did n't work . Here 's an example of what I had in mind : The function `` knows '' that its return value is being assigned.Here is my current workaround :"
"I 'm using pytest.mark.parametrize to feed increasingly long inputs into a rather slow test function , like so : Because compressing large amounts of data takes a long time , I 'd like to skip all the remaining tests after one fails . For example if the test fails with the input b'ab ' ( the first one ) , b'xyz'*1000 and b'12345'*1024**2 and all other parametrizations should be skipped ( or xfail without being executed ) .I know it 's possible to attach marks to individual parametrizations like so : But I do n't know how I could conditionally apply those marks depending on the status of the previous test case . Is there a way to do this ?"
"I 'm not sure if the issue I 'm experiencing is my lack of experience with Python or if it 's a bug in the interpreter , but I think I 'm getting a TypeError message on the wrong line.Please explain to me why this is happening if it 's not a bug . My code is as follows : This is my outputIf I change the following linetoIt works . Why is Python complaining about the line 3 lines below ?"
"I need to consume a service that sends JSON responses containing JSON-serialized nested structures , which I would like to deserialize and store in my database - my application uses Django.Business rules are the following : The query returns objects which always have an id property which is a unique integer , often a createdAt property and an updatedAt property , both with datetime data , and then several other properties which are primitive types ( int , float , str , datetime , etc . ) , and several properties that can be another object or an array of objects.In case the property value is an object , then the parent relates to it through a 'foreign key ' . In case it 's an array of objects , then we have two scenarios : either the objects of the array relate to the parent through a 'foreign key ' , or the parent and each member of the array are related through a 'many-to-many ' relation.I need to mirror each of those objects in my database , so each model has an id field which is the primary key , but it 's not autogenerated , because the real ids will be provided with the imported data.The relations between all those entities are already mirrored in my model schema . I adopted this approach ( mirroring data structure ) because if I flatten the received data to save it all into a single table , there will be horrendous replication , defying all data normalization rules.For every root object , I need to do this : check whether there is already a record in database for that idcreate a new record in case there isn'tupdate the existing record in case there is already one ( update might be skipped if updatedAt values are the same for both the record and the incoming datarecursively repeat these same steps for each nested object that is the provided value for one of its parent 's properties.Below I 'm reproducing a very simplified sample of the data I receive from the service and the models I in which I want to store it . The real thing is much , much more bulky and complex than that , and that 's why I 'm so wanting to learn a way of letting the ORM take care of the problem , should it be able to . Hard-coding the whole thing is taking forever , aside of being pretty error-prone and creating a maintenance hell should the data schema change in the future.EDIT : A link to a previous simplified version of the following JSON and Models*JSON sample : Models.py sample : EDIT ( ref . DRF Serializers ) : I 'm trying to follow Max Malysh I Reinstate Monica 's suggestion , and I started to work on a recursive serializer : However , it does a weird thing : when first run , against an empty database , it only creates the last and most deeply nested object . In the second run , it does nothing and returns a code='unique ' validation error saying that such object already exists.Now I must say I 'm quite new to Python and Django ( I come from .NET development ) and the difficulties I 'm facing about this task begin to look very awkward for me . I 've been reading docs about Django and DRF , which helped me less than I expected . Yet I refuse to believe aforementioned language and framework lack resources for performing such a trivial operation . So , If I 'm missing something very obvious , as it seems , for lack of knowledge of mine , I 'll be grateful if someone teaches me what I seem not to know here ."
"I would like to be able to import a python module which is actually located in a subdirectory of another module.I am developing a framework with plug-ins.Since I 'm expecting to have a few thousands ( there 's currently > 250 already ) and I do n't want one big directory containing > 1000 files I have them ordered in directories like this , where they are grouped by the first letter of their name : Since I would not like to impose a burden on developers of other plugins , or people not needing as many as I have , I would like to put each plugin in the 'framework.plugins ' namespace.This way someone adding a bunch of private plugins can just do so by adding them in the folder framework.plugins and there provide a __init__.py file containing : however , currently this setup is forcing them to also use the a-z subdirectories.Sometimes a plugin is extending another plugin , so now I have a and I would like to haveIs there any way to declare a namespace where the full name space name actually does n't map to a folder structure ? I am aware of the pkg_resources package , but this is only available via setuptools , and I 'd rather not have an extra dependency.The solution should work in python 2.4-2.7.3update : Combining the provided answers I tried to get a list of all plugins imported in the __init__.py from plugins . However , this fails due to dependencies . Since a plugin in the ' c ' folder tries to import a plugin starting with 't ' , and this one has not been added yet.I 'm not sure If I 'm on the right track here , or just overcomplicating things and better write my own PEP302 importer . However , I ca n't seem to find any decent examples of how these should work.Update : I tried to follow the suggesting of wrapping the __getattr__ function in my __init__.py , but this seems to no avail ."
"I 've been using Django for several years , but have recently decided to try out Flask for a new API . Thanks to Carl Meyers excellent presentation on testing Django at PyCon , I 've been using the following technique to prevent touching the database in my Django unit tests : My question is can anyone tell me how to do this same basic technique with SQLAlchemy ? In other words , I want any time I actually run a query against the database to produce a runtime error ."
"I 'm receiving the following warnings in my GTK 3 application : Gtk-WARNING ** : Allocating size to __main__+MCVEWindow 0000000004e93b30 without calling gtk_widget_get_preferred_width/height ( ) . How does the code know the size to allocate ? The warnings occurs when Gtk.ScrolledWindow containing Gtk.TreeView is attached to the grid , the grid itself is attached to the gtk.ApplicationWindow and there are enough elements for the scrollbar to actually appear . If there are n't enough elements to make it scrollable , the warning does n't appear.You should be able to copy , paste and run this code if you have environment set up.The warnings do n't follow any specific pattern , sometimes there is one warning , sometimes two or more . The warrnings also pop up whenever I expand all tree items.GTK version is 3.22.18What could cause these warnings ?"
"[ EDIT : I 'm running Python 2.7.3 ] I 'm a network engineer by trade , and I 've been hacking on ncclient ( the version on the website is old , and this was the version I 've been working off of ) to make it work with Brocade 's implementation of NETCONF . There are some tweaks that I had to make in order to get it to work with our Brocade equipment , but I had to fork off the package and make tweaks to the source itself . This did n't feel `` clean '' to me so I decided I wanted to try to do it `` the right way '' and override a couple of things that exist in the package* ; three things specifically : A `` static method '' called build ( ) which belongs to the HelloHandler class , which itself is a subclass of SessionListenerThe `` ._id '' attribute of the RPC class ( the original implementation used uuid , and Brocade boxes did n't like this very much , so in my original tweaks I just changed this to a static value that never changed ) .A small tweak to a util function that builds XML filter attributesSo far I have this code in a file brcd_ncclient.py : And then in my file netconftest.py I have : Whenever I run my netconftest.py file , I get timeout errors because in the log file ncclient.log I can see that my subclass definitions ( namely the one that changes the XML for hello exchange - the staticmethod build ) are being ignored and the Brocade box does n't know how to interpret the XML that the original ncclient HelloHandler.build ( ) method is generating** . I can also see in the generated logfile that the other things I 'm trying to override are also being ignored , like the message-id ( static value of 1 ) as well as the XML filters.So , I 'm kind of at a loss here . I did find this blog post/module from my research , and it would appear to do exactly what I want , but I 'd really like to be able to understand what I 'm doing wrong via doing it by hand , rather than using a module that someone has already written as an excuse to not have to figure this out on my own . *Can someone explain to me if this is `` monkey patching '' and is actually bad ? I 've seen in my research that monkey patching is not desirable , but this answer and this answer are confusing me quite a bit . To me , my desire to override these bits would prevent me from having to maintain an entire fork of my own ncclient . **To give a little more context , this XML , which ncclient.transport.session.HelloHandler.build ( ) generates by default , the Brocade box does n't seem to like : The purpose of my overridden build ( ) method is to turn the above XML into this ( which the Brocade does like :"
"Everybody knows that in Python assignments do not return a value , presumably to avoid assignments on if statements when usually just a comparison is intended : For the same reason , one could suspect that multiple assignments on the same statement were also syntax errors.In fact , a = ( b = 2 ) is not a valid expression : So , my question is : why a = b = 2 works in Python as it works in other languages where assignment statements have a value , like C ? Is this behavior documented ? I could not found anything about this in the assignment statement documentation : http : //docs.python.org/reference/simple_stmts.html # assignment-statements"
I wonder why this is a SyntaxError in Python 3.4 : It works when removing the trailing comma after **kwargs .
"For example , having the string : should result in something like this : How do I even go about it ? I thought of something not very efficient , such as : Is there any better way ? Some way that is more efficient or more general so I could also go about it with 3 letters more easily ?"
I am new to python and trying to create a bloomFilter based on Bit torrent BEP 33.I have created Bloom Filter but it is not exactly what I am looking for . Here 's what I need and I have not completely understood this situation . If someone here can explain ... And this is what I have created
"When I run an interactive Python inside an Emacs shell buffer ( M-x shell ) , it does two surprising things to the TTY . First , it turns on input echo , which persists after Python exits , until I do stty -echo . Secondly , it does n't accept C-d ( or C-q C-d , i.e . ^D ) as EOF : I have to type quit ( ) to leave the Python . How can I stop these two behaviours ? I know that I could run python-shell , but I do n't want to : I 'm noodling about in the shell and I want to do five lines of Python and then C-d out of it . So `` run python-shell '' is not an answer to my question.Python running in a Terminal window is fine : ^D keeps working and echo does n't change.Python 2.7.5 , GNU Emacs 24.3.1 , OS X 10.8.5Edited to add this snippet from a shell buffer :"
"I noticed in the Pyramid + SQLAlchemy + URL Dispatch Wiki Tutorial that the database is initialized in the main function when the application is run.where initialize_sql is defined as follows : which essentially creates all of the tables ( if they do n't exist ) and populates it with some initial values . Easy enough to understand , BUT ... This is just a tutorial to demonstrate a small application , so how it is typically done in production may differ ( or not ... ) . This brings me to my question : When using Pyramid with SQLAlchemy , is it a typical pattern in production for a database to be initialized this way , or is it typical to use something equivalent to a manage syncdb command in Django that is invoked manually ?"
"I 'm running into some trouble with memory management related to bytes in Python3.2 . In some cases the ob_sval buffer seems to contain memory that I can not account for.For a particular secure application I need to be able to ensure that memory is `` zeroed '' and returned to the OS as soon as possible after it is no longer being used . Since re-compiling Python is n't really an option , I 'm writing a module that can be used with LD_PRELOAD to : Disable memory pooling by replacing PyObject_Malloc with PyMem_Malloc , PyObject_Realloc with PyMem_Realloc , and PyObject_Free with PyMem_Free ( e.g . : what you would get if you compiled without WITH_PYMALLOC ) . I do n't really care if the memory is pooled or not , but this seems to be the easiest approach.Wraps malloc , realloc , and free so as to track how much memory is requested and to memset everything to 0 when it is released.At a cursory glance , this approach seems to work great : The errant \x13 at the end is odd but does n't come from my original value so at first I assumed it was okay . I quickly found examples where things were not so good though : Here the last three bytes , ous , survived.So , my question : What 's going on with the leftover bytes for bytes objects , and why do n't they get deleted when del is called on them ? I 'm guessing that my approach is missing something similar to a realloc , but I ca n't see what that would be in bytesobject.c.I 've attempted to quantify the number of 'leftover ' bytes that remain after garbage collection and it appears to be predictable to some extent.Edit 1I had originally expressed concern about the fact that if the bytes object is used in a function it does n't get cleaned up at all : It turns out that this is an artificial concern that is n't covered by my requirements . You can see the comments to this question for details , but the problem comes from the way the hello_forever.__code__.co_consts tuple will contain a reference to Hello , World ! even after a is deleted from the locals.In the real code , the `` secure '' values would be coming from an external source and would never be hard-coded and later deleted like this.Edit 2I had also expressed confusion over the behaviour with strings . It has been pointed out that they likely also suffer the same problem as bytes with respect to hard-coding them in functions ( e.g . : an artifact of my test code ) . There are two other risks with them that I have not been able to demonstrate as being a problem but will continue to investigate : String interning is done by Python at various points to speed up access . This should n't be a problem since the interned strings are supposed to be removed when the last reference is lost . If it proves to be a concern it should be possible to replace PyUnicode_InternInPlace so that it does n't do anything.Strings and other 'primitive ' object types in Python often keep a 'free list ' to make it faster to get memory for new objects . If this proves to be a problem , the *_dealloc methods in the Objects/*.c can be replaced.I had also believed that I was seeing a problem with class instances not getting zeroed correctly , but I now believe that was an error on my part.ThanksMuch thanks to @ Dunes and @ Kevin for pointing out the issues that obfuscated my original question . Those issues have been left above in the `` edit '' sections above for reference ."
"Running Django v1.10 on Python 3.5.0 : Expected output : Actual output : How do I correctly pass the ending character ? I currently use a workaround of setting explicitly : But this hack means you do n't get all the features of the print function , you must use self.stdout.write and prepare the bytes manually ."
"I have the following class : How can I rewrite this class as a dataclass ? Specifically , how should the id field be declared ? It has a generated value , and is not a field that the code creating instances would provide ."
"I need an efficient way to row standardize a sparse matrix.GivenI need to produce ... Where , I 'd like to find a way to do this without loops ( i.e . Vectorized ) and using Scipy.sparse matrices . W could be as large at 10mil x 10mil ."
"Basically , the logic of my problem is : That is : Find the first index for which the cumulative sum of entries in my_array is above a threshold.Now the problem is : I know that my_array will be large , but that the condition will be met fairly early . Of course that means I could just do a simple while loop to manually figure out when the cumulative sum is larger than the threshold , but I am wondering if there 's a numpythonic way , i.e. , a way to test for some condition without having the entire array evaluated ."
"I am attempting to modify a value in a class __dict__ directly using something like X.__dict__ [ ' x ' ] += 1 . It is impossible to do the modification like that because a class __dict__ is actually a mappingproxy object that does not allow direct modification of values . The reason for attempting direct modification or equivalent is that I am trying to hide the class attribute behind a property defined on the metaclass with the same name . Here is an example : This is example shows a scheme for creating an auto-incremented ID for each instance of Class . The line __class__.__dict__ [ ' x ' ] += 1 can not be replaced by setattr ( __class__ , ' x ' , __class__.x + 1 ) because x is a property with no setter in Meta . It would just change a TypeError from mappingproxy into an AttributeError from property.I have tried messing with __prepare__ , but that has no effect . The implementation in type already returns a mutable dict for the namespace . The immutable mappingproxy seems to get set in type.__new__ , which I do n't know how to avoid.I have also attempted to rebind the entire __dict__ reference to a mutable version , but that failed as well : https : //ideone.com/w3HqNf , implying that perhaps the mappingproxy is not created in type.__new__.How can I modify a class dict value directly , even when shadowed by a metaclass property ? While it may be effectively impossible , setattr is able to do it somehow , so I would expect that there is a solution.My main requirement is to have a class attribute that appears to be read only and does not use additional names anywhere . I am not absolutely hung up on the idea of using a metaclass property with an eponymous class dict entry , but that is usually how I hide read only values in regular instances.EDITI finally figured out where the class __dict__ becomes immutable . It is described in the last paragraph of the `` Creating the Class Object '' section of the Data Model reference : When a new class is created by type.__new__ , the object provided as the namespace parameter is copied to a new ordered mapping and the original object is discarded . The new copy is wrapped in a read-only proxy , which becomes the __dict__ attribute of the class object ."
"I have some code like this : How could i implement RandomPerson , with the following requirements : calling person = RandomPerson ( ) must return a RandomPerson object.RandomPerson should subclass either John or Kyle randomly ."
I keep getting this error : time zone `` Eastern Standard Time '' not recognizedhere is the code : In my template : And in my settings : What I would like to do is get some kind of an archive system going . Where it is possible to get the different months and years there was any posts .
"Debugging differences between Python 's zlib and golang 's zlib . Why do n't the following have the same results ? compress.go : compress.py : ResultsThe Python version has 0 for the fifth byte , but the golang version has 4 -- what 's causing the different output ?"
I have looked at some common tools like Heapy to measure how much memory is being utilized by each traversal technique but I do n't know if they are giving me the right results . Here is some code to give the context.The code simply measures the number of unique nodes in a graph . Two traversal techniques provided viz . count_bfs and count_dfsI want to know by what factor is the total memory utilization different in the two traversal techniques viz . count_dfs and count_bfs ? One might have the intuition that dfs may be expensive as a new stack is created for every function call . How can the total memory allocations in each traversal technique be measured ? Do the ( commented ) hpy statements give the desired measure ? Sample file with connections :
"Good morning all , I have a pandas dataframe containing multiple series . For a given series within the dataframe , the datatypes are unicode , NaN , and int/float . I want to determine the number of NaNs in the series but can not use the built in numpy.isnan method because it can not safely cast unicode data into a format it can interpret . I have proposed a work around , but I 'm wondering if there is a better/more Pythonic way of accomplishing this task . Thanks in advance , Myles"
"When converting a float to a str , I can specify the number of decimal points I want to displayBut when simply calling str on a float in python 2.7 , it seems to default to 12 decimal points maxWhere is this max # of decimal points defined/documented ? Can I programmatically get this number ?"
I installed Python 3 on a new Mac using Miniconda and have the following setup : macOS Catalina 10.15.1Conda 4.7.12Python 3.7.5iPython 7.9.0When I try to run iPython in the terminal I receive the following error : The error appears to be related to an __init__ ( ) method keyword argument 'inputhook ' . Any suggestions on how to fix this ?
"I have a data that looks something like this : numpy array : its like a user-item matrix . I want to construct a sparse matrix with shape : number_of_items , num_of_users which gives 1 if the user has rated/bought an item or 0 if he has n't . So , for the above example , shape should be ( 5,6 ) . This is just an example , there are thousands of users and thousands of items.Currently I 'm doing this using two for loops . Is there any faster/pythonic way of achieving the same ? desired output : where rows : abc , def , ghi , fg , f76and columns : a , b , c , d , e , f"
"I 've gone through the getting started tut for python27 and app engine : https : //developers.google.com/appengine/docs/python/gettingstartedpython27/By the end of the tut , all the the classes are in the same file ( helloworld.py ) and you and you configure the router to point a url path to a class at the bottom of the file : What the tut did not cover is how do I orginise my classes / files as my app grows . For example , would I put MainPage in a separate file and then call 'import MainPage ' in the helloworld.py file and add the route to the WSGIApplication ? Is there anything more automated than this ? What should I call the MainPage file and where should I store it ?"
I 've read some conflicting advice on the use of assert in the setUp method of a Python unit test . I ca n't see the harm in failing a test if a precondition that test relies on fails . For example : This seems like a reasonable thing to do to me i.e . make sure the test is able to run . When this fails because of the setup condition we get :
"I need to find the extent of a plot including its related artists ( in this case just ticks and ticklabels ) in axis coordinates ( as defined in the matplotlib transformations tutorial ) .The background to this is that I am automatically creating thumbnail plots ( as in this SO question ) for a large number of charts , only when I can position the thumbnail so that it does not obscure data in the original plot . This is my current approach : Create a number of candidate rectangles to test , starting at the top-right of the original plot and working left , then the bottom-right of the original plot and move left.For each candidate rectangle : Using code from this SO question convert the left and right hand side of the rect ( in axis coordinates ) into data coordinates , to find which slice of the x-data the rectangle will cover.Find the minimum / maximum y-value for the slice of data the rectangle covers.Find the top and bottom of the rectangle in data coordinates.Using the above , determine whether the rectangle overlaps with any data . If not , draw the thumbnail plot in the current rectangle , otherwise continue.The problem with this approach is that axis coordinates give you the extent of the axis from ( 0,0 ) ( bottom-left of the axes ) to ( 1,1 ) ( top-right ) and does not include ticks and ticklabels ( the thumbnail plots do not have titles , axis labels , legends or other artists ) . All charts use the same font sizes , but the charts have ticklabels of different lengths ( e.g . 1.5 or 1.2345 * 10^6 ) , although these are known before the inset is drawn . Is there a way to convert from font sizes / points to axis coordinates ? Alternatively , maybe there is a better approach than the one above ( bounding boxes ? ) .The following code implements the algorithm above : And the output of this is :"
I am developing a package containing Cython extensions.According to https : //github.com/pypa/pip/issues/1958 I shall use setup_requires and postpone import of Cython.The best solution I came up with is to call setup ( ) twice in setup.py : However I have a feeling that the name of setup ( ) suggests it shall be called only once . Is it safe to call it several times like I do ? I can not just distribute wheels because the package shall be available also for Linux users . [ EDIT ] Also I see the question as more general than dealing with compiler-dependencies . One may want to import some package ( eg . sphinx or pweave ) to preprocess the description of ones package .
"I 've been reading about itertools , which seems to be a very powerful module . I am particularly interested in itertools.product ( ) which appears to give me all of the combinations of the iterable inputs.However , I would like to know which of the input iterables each of the outputs are coming from . For example , a simple standard example is : If the user provided the inputs of [ 1,2,3 ] , [ 1 , 2 ] I wo n't know which order they came in , so getting a result ofis n't much help , as I do n't know which way round they will be . Is there some way of providing input like : and then getting outputs like : or"
"I have n't found how to give type hints indication when using weakrefs.Is there a way to say that my_list is a list of weakref to MyObject , something like : ?"
"I 'm currently reading chapter 5.8 of Dive Into Python and Mark Pilgrim says : There are no constants in Python . Everything can be changed if you try hard enough . This fits with one of the core principles of Python : bad behavior should be discouraged but not banned . If you really want to change the value of None , you can do it , but do n't come running to me when your code is impossible to debug . I tried this in the interpreterI get a SyntaxError : assignment to NoneJust out of curiosity how do you change None ? EDIT : Interestingly : Also So I guess 'None = ' just does n't work in any context"
how to find property color and change value for Text element in my qtquick project ? content on my.qml file .
I can extract vocabulary from CountVecotizerModel by the following waythe above code will print list of vocabulary with index as it 's ids.Now I have created a pipeline of the above code as following : it will throw the following error AttributeError : 'PipelineModel ' object has no attribute 'vocabulary'So how to extract the Model attribute from the pipeline ?
"I 'm trying to scrap an e-commerce web site , and I 'm doing it in 2 steps.This website has a structure like this : The homepage has the links to the family-items and subfamily-items pagesEach family & subfamily page has a list of products paginatedRight now I have 2 spiders : GeneralSpider to get the homepage links and store themItemSpider to get elements from each pageI 'm completely new to Scrapy , I 'm following some tutorials to achieve this . I 'm wondering how complex can be the parse functions and how rules works . My spiders right now looks like : GeneralSpider : ItemSpider : Wich is the best way to make the spider follow the pagination of an url ? If the pagination is JQuery , meaning there is no GET variable in the URL , Would be possible to follow the pagination ? Can I have different `` rules '' in the same spider to scrap different parts of the page ? or is better to have the spiders specialized , each spider focused in one thing ? I 've also googled looking for any book related with Scrapy , but it seems there is n't any finished book yet , or at least I could n't find one . Does anyone know if some Scrapy book that will be released soon ? Edit : This 2 URL 's fits for this example . In the Eroski Home page you can get the URL 's to the products page.In the products page you have a list of items paginated ( Eroski Items ) : URL to get Links : Eroski HomeURL to get Items : Eroski FruitsIn the Eroski Fruits page , the pagination of the items seems to be JQuery/AJAX , because more items are shown when you scroll down , is there a way to get all this items with Scrapy ?"
"I try to calculate how often a state is entered and how long it lasts . For example I have the three possible states 1,2 and 3 , which state is active is logged in a pandas Dataframe : For example the state 1 is entered two times ( at index 3 and 12 ) , the first time it lasts three hours , the second time two hours ( so on average 2.5 ) . State 2 is entered 3 times , on average for 2.66 hours.I know that I can mask data I 'm not interested in , for example to analyize for state 1 : but from there on I ca n't find a way to go on ."
"By reading the doc : https : //docs.djangoproject.com/en/dev/topics/db/transactions/ # django.db.transaction.atomicI know that However , my question is that for a code structure like following : If B and C both succeed , and A goes wrong after them , then B and C will right back , right ? What if B succeed , but C messed up , will B get roll back ? And about the memory usage for maintaining this roll back functionality , is there any difference between the one above and the one following : I know these two structure handle different case . I am just asking , assume they both succeed ( completely ) , what is the difference at the memory usage level ? Thanks in advance ."
"I have two Django models as shown below , MyModel1 & MyModel2 : MyModel2 has a ManyToMany field to MyModel1 entitled model1Now look what happens when I add a new entry to this ManyToMany field . According to Django , it has no effect : Why ? It seems definitely like a caching issue because I see that there is a new entry in Database table myapp_mymodel2_mymodel1 for this link between m2 & m1 . How should I fix it ? ?"
"Suppose I have a Pandas data frame as follows : I 'd like to get the row with Parameter value 3 . That is the row with the last increasing value before the first drop . Notice that later on we might have higher values ( eg row 22 ) . Essentially , I 'm trying to get the `` last '' number before the `` first '' decrease value.Also note that there are multiple Tests , so I probably need to do something like :"
I have the following image which is a receipt image and a lot of white space around the receipt in focus . I would like to crop the white space . I ca n't manually crop it so I 'm looking for a way that I could do it.Cropped one : Tried this code from the following post : How to remove whitespace from an image in OpenCV ? it 's cropping a tiny part of the white space .
"Basically , I have a list like : [ START , 'foo ' , 'bar ' , 'spam ' , eggs ' , END ] and the START/END identifiers are necessary for later so I can compare later on . Right now , I have it set up like this : This works fine , but it suffers from the problem of not working with pickling . I tried doing it the following way , but it seems like a terrible method of accomplishing this : Could anybody share a better means of doing this ? Also , the example I have set up above is just an oversimplification of a different problem ."
"I ’ m trying to load into memory a few 2 000 FITS using astropy.io.fits : However , when reaching the 1015th file , OSError : [ Errno 24 ] Too many openfiles is raised . I have the same issue with : I suspect that astropy.io.fits does not properly close the file . Is there away I can force the files to be closed ?"
One of the columns in DataFrame is an array . How do I flatten it ? After flattening it should be : I seemed unstack could help me but I could n't understand how exactly .
"First question on stackoverflow : D ( because I found almost everything until now ) .I try to deploy my python app to Heroku , but the following error appears : On my virtual box everything works fine if I do I tried the following with no avail : changing functools32 versionRemoving functools from requirements text , which made me realize it is a dependency by Flask for decorators : ("
"I have configured Django to use a file backend for email sending on my local machine . This seemed to work fine earlier on , and all mails were recorded in the directory I had specified in my settings.py file : However , this suddenly stopped working . I have checked the permissions of the folder , and this seems to be fine . There is no error that I can see . I 'm using docker , and when I start the Python server I have the logs shown in my terminal . Normally when there is an error I see it there . But nothing appears . To test things , I have renamed the folder and tried sending a mail . This time , no error appears either . In production , where my settings.py are different but all else is the same , the emails are sent out just fine . So the code seems to be working , but the local filebased backend seems to be a problem . Anybody any idea ? I have configured these log settings : The logs seem to work fine and provide detailed logging ; but none of it related to the e-mail error ."
"First post here , although i already spent days of searching for various queries here . Python 3.6 , Pillow and tiff processing.I would like to automate one of our manual tasks , by resizing some of the images from very big to match A4 format . We 're operating on tiff format , that sometimes ( often ) contains more than one page . So I wrote : But the very ( not ) obvious is that only first page of tiff is being converted . This is not exactly what I expect from this lib , however tried to dig , and found a way to enumerate each page from tiff , and save it as a separate file . Now I could use imagemagick , or some internal commands to convert multiple pages into one , but this is not what I want to do , as it drives to code complication.My question , is there a unicorn that can help me with either :1 ) resizing all pages of given multi-page tiff in the fly2 ) build a tiff from few tiffs I 'd like to focus only on python modules . Thx ."
"I 'm teaching my self python and I was translating some sample code into thisWhich is pretty much my intended class definitionLater in the main method I create an instance and call it a That 's how I find out that the variable a declared in main is available to the method average and to make that method work , I have to type self.a + self.b + self.c instead What 's the rationale of this ? I found related questions , but I do n't really know if they are about the same"
"I am using yahoo_finance in python to pull stock data and for some reason , the get_prev_close ( ) method is not returning the same data with every call.Here is a simple example : For some reason this will print two different numbers seemingly at random . So for today I am getting 69.3 and 69.71 printed out . But since this is yesterdays close data , there should only be a single value.Is this is known bug and is there a way around this ?"
"I 'd like to know how to get the number of lines in a Tkinter Text widget that has word wrap enabled . In this example , there are 3 lines in the text widget : But methods that work for non-wrapped text , like : will return 1 instead of 3 . Is there another method that would properly count wrapped lines ( and return 3 in my example ) ? Thanks for your help !"
"If I were to have a list , say : with a character of ! , how would I return a list given : I 'm having some difficulty finding a solution for this . Here 's one approach I 've tried : Any help would be greatly appreciated ."
"I am trying to remove the last character of a string in a `` right-to-left '' language . When I do , however , the last character wraps to the beginning of the string.e.g.ותֵיהֶם ] ׃becomesותֵיהֶם ] I know that this is a fundamental issue with how I 'm handling the R-T-L paradigm , but if someone could help me think through it , I 'd very much appreciate it.CODE"
"I 'm new to Python and I 'm wondering if I can build enums with complex structures , not just primitive types . For instance ( in pseudo-code ) : So far , I could only find Python documentation that mentions enums with strings or ints ."
"I 'm having difficulty loading a pickle in Python that was dumped in IronPython.When I pickle something basic like `` [ 1,2,3 ] '' in IronPython , the pickle loads fine in Python . But , when I pickle results from the DB query below in IronPython , I get the error `` No module named clr '' when trying to load the pickle in Python.What might be going wrong ? ( And is there a better way to share data between Python and IronPython ? )"
"I am trying to write different implementations for a fractional knapsack problem.For this I have 2 arrays : ValuesWeightsThe elements value [ n ] corresponds to element weights [ n ] . So we can calculate value_per_unit as : I now need the 2 arrays ( values and weights ) to be sorted according to the value_per_unit arrayeg : Ifvalues = [ 60 , 100 , 120 ] weights = [ 20 , 50 , 30 ] Thenvalues_per_unit = [ 3.0 , 2.0 , 4.0 ] and so values_per_unit_sorted will be [ 2.0 , 3.0 , 4.0 ] I need the values and weights arrays to become : values_sorted = [ 100,60,120 ] weights_sorted = [ 50,20,30 ] Is there a way to achieve this using simple lambda functions ? I can still do something like this , but it seems highly inefficient every-time I need to access the elements :"
"I 'm running some hibernation tests using python + microsoft 's pwrtest utilityAlso I 'm using sqlalchemy ( orm ) to work with database ( ms sql server 2008 r2 ) .I 'm connected to the remote sql server and everything works fines , however after computer goes into the hibernation mode ( S4 ) sql server drops the connection ( I see it as the `` Activity monitor '' at management studio ) . When my pc gets back for hibernation and continues with the script I get the error `` DBAPIError : ( Error ) ( '08S01 ' , ' [ 08S01 ] [ Microsoft ] [ ODBC SQL Server Driver ] Communication link failure ( 0 ) ( SQLExecDirectW ) ' ) '' I 've tried to use the pool_recycleHowever , as far as I understand sqlalchemy does not realize that the connection does not exist anymore.I 've also tried to use engine.dispose ( ) and according to the documentation it should drop the current pool : Dispose of the connection pool used by this Engine . A new connection pool is created immediately after the old one has been disposed . This new pool , like all SQLAlchemy connection pools , does not make any actual connections to the database until one is first requested.But that also did n't workHow to reconnect to the Database ? Thanks ! The code : actual call :"
"I want to store huge data into a dictionary in python . Huge data may be around 21 GB . I wrote a snippet to do so . Storing the integer values inside the dictionary . Code : During runtime , when I reached the size using **getsizeof ( dicts ) ** around 1.2GB , it fails to store the values in dictionary but does n't show any error . Does Dictionary has some capacity to store the data ? So , the question is how can I store huge data into the dictionary ? NOTE : Does n't require to store the data in files or databases . because I want to retrieve the key , value pair very fast ."
"In Python ( 2.7.2 ) , why does works as expected whereasraises : Note that this does n't affect Python3 ."
I have tried an example with PolyCollection from matplotlib tutorials and noticed one strange thing . I could n't remove this points from axes origin see fig . How do I manage this ?
I have a treeview in the left side of an hpaned but when I try to move the bar to the left to make the treeview smaller than its automatic size instead of resizing the treeview it expands the entire program window to the right . Any ideas on how to fix this ? The relevant portions of the source are the following : For the hpaned.And for the tree View .
"This is the model I am using : However , when I try to add a comment from the admin site I get : Am I doing something wrong with my model ? I feel like django-mptt is trying to get the DateTimeField while it is still `` None '' , before it has been set at the db level ."
"Is there any way to assign an interval in a variable ? For exampleI wish to return bc . However , the second line does not work ."
"I 'm using Python Flask + nginx with FCGI.On some requests , I have to output large responses . Usually those responses are fetched from a socket . Currently I 'm doing the response like this : The problem is I actually do not need the data . I also have a way to determine the exact response length to be fetched from the socket . So I need a good way to send the HTTP headers , then start outputing directly from the socket , instead of collecting it in memory and then supplying to nginx ( probably by some sort of a stream ) .I was unable to find the solution to this seemingly common issue . How would that be achieved ? Thank you !"
"I have two lists , looking like this : which I want to subtract from each other element by element for an Output like this : In order to do so I convert each of a and b to arrays and subtract them I use : The Output just gives me the error : Unsupported Operand type for- : 'list ' and 'list'What am I doing wrong ? Should n't the np.array command ensure the conversion to the array ?"
"If we have a numpy array like : and we want to set one element of it , given by another How can we do that ? is setting every element of the FIRST dimension of Array to 5"
"I answered several questions here by using this to `` flatten '' a list of lists : it works fine and yields : although I was told that the sum operator does a = a + b which is not as performant as itertools.chainMy planned question was `` why is it possible on lists where it is prevented on strings '' , but I made a quick benchmark on my machine comparing sum and itertools.chain.from_iterable on the same data : I did that several times and I always get about the same figures as below : To my surprise , chain - recommended over sum for lists by everyone in several comments on my answers - is much slower.It 's still interesting when iterating in a for loop because it does n't actually create the list , but when creating the list , sum wins.So should we drop itertools.chain and use sum when the expected result is a list ? EDIT : thanks to some comments , I made another test by increasing the number of listsnow I get the opposite :"
"I have experimental data : And the formula f ( x ) = m1 + m2 / ( 1 + e ^ ( -m3* ( x - m4 ) ) ) . I need to find m1 , m2 , m3 , m4 with least square method , where 0.05 < m1 < 0.3 0.3 < m2 < 0.8 0.05 < m3 < 0.5 100 < m4 < 200.I use curve_fit and my function is : But the program return the error : RuntimeError : Optimal parameters not found : Number of calls to function has reached maxfev = 1000.What to do ?"
"I am writing a django south migration that depends on the model in another app , so I have included -- freeze OTHERAPPNAME when I ran python manage.py datamigration …However in the forwards ( ) function I can access the other model fine ( with orm [ 'otherappname.MyModelName ' ] ) , however in the backwards ( ) step ( which also depends on the model in otherappname ) , if I try to access orm [ 'otherappname.MyModelName ' ] , I get an errorI can see the frozen model details in the bottom of the file . Why ca n't I access it ? NB : This model is created in another migration inside otherappname of which this datamigration depends on ."
"While trying to write unittests that check whether a concrete subclass of an Abstract base class really does raise a TypeError upon instantiation if one of the required methods is not implemented , I stumbled upon something which made me wonder when the check if the required methods is defined by the concrete subclass is actually performed.Until now I would have said : upon instantiation of the object , since this is the time when the Exception is actually raised when running the program.But look at this snippet : As expected , trying to instantiate MyConcreteSubclass raises a TypeError : But what happens if I declare a valid subclass at first and then afterwards delete this method surprises me : This is certainly not what I expected . When inspecting MyConcreteSubclass.foo after deletion , we see that through the method Resolution order the Abstract method of the base class is retrieved , which is the same behaviour as if we have n't implemented foo in the concrete subclass in the first place.But after instantiation the TypeError is not raised.So I wonder , are the checks whether the required methods are implemented already performed when the body of the concrete subclass is evaluated by the Interpreter ? If so , why are the TypeErrors only raised when someone tries to instantiate the subclass ? The Tests shown above were performed using Python 3.6.5 ."
"I run the following code with Python 2.7.5. under Windows : I expect both timestamps ( =floats ) to be equal ( as their string representations suggest ) , so why does t1 == t2 evaluate to False ? Also , I was unable to reproduce this behaviour with less code , i.e . without comparing the timestamps retrieved via os.lstat from two different files . I have the feeling , I am missing something trivial here ... Edit : After further testing I noticed , that it does print True once in a while , but not more often than once every 10 runs.Edit 2 : As suggested by larsmans : This raises two new questions : Why are the timestamps not equal after calling shutil.copystat ? print rounds floats by default ? !"
"Currently I am trying to run Stardew Valley from python by doing this : However , this fails and only opens a CMD window . I have a basic understanding of how to launch programs from python , but I do not understand how to specifically open a program that is located not only in a different location , but also on a different drive.Any help would be appreciated . Thanks ! Edit : This is on windows 10Stardew Valley version is the beta and is located on the D : / drive ( windows is on C : / of course )"
"I am scraping the following webpage using scrapy-splash , http : //www.starcitygames.com/buylist/ , which I have to login to , to get the data I need . That works fine but in order to get the data I need to click the display button so I can scrape that data , the data I need is not accessible until the button is clicked . I already got an answer to this that told me I can not simply click the display button and scrape the data that shows up and that I need to scrape the JSON webpage associated with that information but I am concerned that scraping the JSON instead will be a red flag to the owners of the site since most people do not open the JSON data page and it would take a human several minutes to find it versus the computer which would be much faster . So I guess my question is , is there anyway to scrape the webpage my clicking display and going from there or do I have no choice but to scrape the JSON page ? This is what I have got so far ... but it is not clicking the button ."
"I have a dataframe like this : I would like to drop all rows where the value of column A is duplicate but only if the value of column B is 'true'.The resulting dataframe I have in mind is : I tried using : df.loc [ df [ ' B ' ] =='true ' ] .drop_duplicates ( ' A ' , inplace=True , keep='first ' ) but it does n't seem to work.Thanks for your help !"
"First off , the versions : gevent - v0.13.7 gunicorn - v0.14.2 requests - 0.11.2We recently upgraded our servers that are running behind gunicorn to use the gevent asynchronous workers instead of just normal sync workers . Everything works great , but we 're now experiencing an issue when attempting to access a 3rd party service over http and I just have no idea how to track down what might be the issue.A brief stack trace looks like the following : Another different stack trace but we think it 's the same issue : At first , I thought it could be potentially something related to a libevent-dns , from this google groups issue . I checked our /etc/resolv.conf , and there is only one dns resolution service : I looked up what ERRNO66 is : https : //github.com/libevent/libevent/blob/master/include/event2/dns.h # L162 , '' /** An unknown error occurred */ '' . I 'm not having much luck finding that helpful..sounds like it could n't talk to the dns server ? I thought it might have to do something with python-requests , see how enable requests async mode ? since python-requests depends on urllib3 , which is implemented in terms of httplib ; but , it turns out the author of gevent removed the httplib patch in this commit earlier this year without any comments as to why.Does anyone have any ideas on how to approach debugging this issue or might shed some light on what 's happening here ? Thanks in advance ! Update - 12:50PM PDTAfter some conversations on freenode , the # gevent and the # gunicorn channel seem to shed some more insight : # geventgevent v0.13.7 still supports the patch_all with httplib=TrueI asked if `` it make sense to patch it ? `` , the response was no.Recommendation to use gevent 1.0 ( even if it 's beta ) .quote from @ schmir : `` patch httplib uses libevent http client library . I do n't trust libevent . my advice would have been to turn it off , if you used it '' # gunicorn < Damianz > What 's your platform ? I 've seen that issue appear on windows boxes where it tries ipv6 and just fails life.. ( I 'm on CentOS 5 ) < dmishe > I 've seen similar on mac , looks like gevent beta fixed itSounds like the general advice is to ditch gevent v0.13.7 and upgrade to gevent 1.0b . I 'll follow up on if that fixes this issue . Meanwhile , anyone that can shed advice , I 'd much appreciate it.Update # 2 - 4 days in production , 1:15PM PDTLooks like the upgrade to gevent has solved this issue -- I 'll add my answer and accept it if no one else chimes in , but only after a week without incidents in production ."
"I am using PyCharm with iPython on Windows . The only Python I have installed on the box is the latest Anaconda distribution , Python 3.4 flavor . Very often , while using the console , I get numerous instances of the following warning message : This is mixed in with the normal output . Has anyone else experienced and/or fixed this ? I have dug through both the iPython and PyCharm documentation and have not found anything related ."
"I am trying to find whether Date falls in PromoInterval in a data frame.Struck at below error : dset1.apply ( func , axis=1 , args = ( dset1 [ 'Date ' ] .dt.month , > dset1 [ 'PromoInterval ' ] ) ) ( 'func ( ) takes exactly 2 arguments ( 3 given ) ' , u'occurred at index 1760 ' ) Data set :"
"I have two arrays , and I want to append them into a new array but I need the masked information to be kept . I tried numpy.append ( ) , but it lose the masked information ."
"In my Django app I create a User from django.contrib.auth.models , and I am using request.user in multiple view functions without a problem . In one of my view functions I change the user password , save the user , and redirect the client to another view function . Once I try to get the user from the request in that function , the user is Anonymous . After using User.set_password ( ) or redirecting , does it take the user out of the session ? views.pyforms.py"
"I have a list of lists A of length m. Every list of A contains positive numbers from { 1 , 2 , ... , n } . The following is an example , where m = 3 and n = 4.I represent every number x in A as a pair ( i , j ) where A [ i ] [ j ] = x. I would like to sort the numbers in A in non-decreasing order ; breaking ties by lowest first index . That is , if A [ i1 ] [ j1 ] == A [ i2 ] [ j2 ] , then ( i1 , j1 ) comes before ( i2 , j2 ) iff i1 < = i2.In the example , I would like to return the pairs : which represents the sorted numbersWhat I did is a naive approach that works as follows : First I sort every list in A.Then I iterate the numbers in { 1 , 2 , ... , n } and the list A and add the pairs . Code : I think this approach is not good . Can we do better ?"
"I am trying to use the TRE-library in python to match misspelled input.It is important , that it does handle utf-8 encoded Strings well.an example : The German capital 's name is Berlin , but from the pronunciation it is the same , if people would write `` Bärlin '' It is working so far , but if a non-ASCII character is on the first or second position of the detected String , neither the range nor the detected string itself is correct.outputNot that for the regex ' . *Berlin ' it works fine , while for the regex 'Berlin'are not working , while work as expected.Is there something I do wrong with the encoding ? Do you know any trick ?"
"I am having trouble understanding why the following happens . I am having a decorator which does nothing except it checks whether a function is a method . I thought I have understood what method in Python is , but obviously , this is not the case : Now , running the following : I would expect for this code to print True two times.So , decorating the function manually as in the Adder2 is not an exact equivalent to decorating via the @ deco function ? Can someone be so glad and explain why this happens ?"
"I 'm getting what looks like a label positioning error when setting a label for a logit scale axis using matplotlib , here 's my example : Here 's the trace : Works great when I omitIt does n't seem to matter of the label is set before or after the axis scale.Anyone know how to work around this ?"
Let 's say I got this logging.logger instance : Problem comes when I try to use it like the builtin print with a dynamic number of arguments : How could i emulate the print behaviour still using logging.Logger ?
"I want to find out from inside the script -- the exact command I used to fire it up . I tried the following : But it loses info : You see -- it has already lost the info as to wither I 've used double quotes , single quotes or there have been no quotes at all -- in the command . Edit : Here 's what I 'm using . All args in my script have default values , and after args are parsed with argparse : I log them or if there 's a log -- overwrite them : defuns mentioned :"
"Following this tutorial I 've just setup nginx with uWSGI to serve my website which I built in Flask , and things work fine for now . I sometimes want to debug something for which I normally use basic print statements in the code . Unfortunately I have no idea where the result of these print 's go ? I 've tailed the following log files , but I do n't see the print 's in there : Does anybody know where I can see the result of the prints ?"
"I 've read all related posts and scoured the internet but this is really beating me.I have some text containing a date.I would like to capture the date , but not if it 's preceded by a certain phrase.A straightforward solution is to add a negative lookbehind to my RegEx.Here are some examples ( using findall ) .I only want to capture the date if it is n't preceded by the phrase `` as of '' . 19-2-11 something something 15-4-11 such and such as of 29-5-11 Here is my regular expression : Expected results : [ '19-2-11 ' ] [ '15-4-11 ' ] [ ] Actual results : [ '19-2-11 ' ] [ '15-4-11 ' ] [ ' 9-5-11 ' ] Notice that 's 9 not 29 . If I change \d { 1,2 } to something solid like \d { 2 } on the first pattern : Then I get my expected results . Of course this is no good because I 'd like to match 2-digit days as well as single-digit days.Apparently my negative lookbehind is quity greedy -- moreso than my date capture , so it 's stealing a digit from it and failing . I 've tried every means of correcting the greed I can think of , but I just do n't know to fix this.I 'd like my date capture to match with the utmost greed , and then my negative lookbehind be applied . Is this possible ? My problem seemed like a good use of negative lookbehinds and not overly complicated . I 'm sure I could accomplish it another way if I must but I 'd like to learn how to do this.How do I make Python 's negative lookbehind less greedy ?"
"I 'm experiencing the same problem as with : django - `` manage.py test '' fails `` table already exists '' The schemamigration / migration worked fine ( although did have some problems that required me to -- fake , but all subsequent migrations with south work ) . But when I run a unit test I get : I 'm just curious how I can get round this , and why this happens . The only answer given in the question linked above was that south could be excluded from unit tests , if I do this does it mean I ca n't unit test with tables managed by south ? Explanations much appreciated : ) Adam"
I 'm working with a dataset with timestamps from two different time zones . Below is what I am trying to do:1.Create a time object t1 from a string ; 2.Set the timezone for t1 ; 3.Infer the time t2 at a different timezone.Any answers/comments will be appreciated.. !
I tried the following code and It gave me different output.I am using python 2.7.2 . Why id function return different value in case of float but same value in case of integers ?
"I 'm working on image color recognition , so I 'm converting the RGB image to Lab because it 's the closest color space to human vision . After that , I get each one of the Lab 's 3 channels and I want to plot in the 3D graphic the color variations that I identified in the converted image . How do I plot the graphic with the colors of the image ? Exit :"
"I am currently trying to understand the fft-function from numpy . For that I tested the following assumption : I have two functions , f ( x ) = x^2 and g ( x ) = f ' ( x ) = 2*x . According to the fourier transformation laws and wolfram alpha it should be that G ( w ) = 2pi*i*F ( w ) ( prefactors can vary , but there should only be a constant factor ) . When implementing that in python , I writeNow I am expecting a nearly constant value for c , but I getWhere is my mistake , and what can I do to use the fft as intended ?"
"This is a bit difficult to explain without a direct example . So let 's put the very simplistic ideal-gas law as example . For an ideal gas under normal circumstances the following equation holds : This means that if we know 3 of the 4 variables ( pressure , volume , specific gas constant and temperature ) we can solve for the other one.How would I put this inside an object ? I want to have an object where I can just insert 3 of the variables , and then it calculates the 4th . I wonder if this can be achieved through properties ? My current best guess is to insert it like : Though this is quite cumbersome , and error prone ( I have to add checks to see that exactly one of the parameters is set to `` None '' ) .Is there a better , cleaner way ? I see this `` problem '' happening quite often , in all kinds of various ways , and especially once the number of variables grows ( adding density , reynolds number , viscosity to the mix ) the number of different if-statements grows quickly . ( IE if I have 8 variables and any 5 make the system unique I would need 8 nCr 5 = 56 if statements ) ."
"I 'm trying to create a point class which defines a property called `` coordinate '' . However , it 's not behaving like I 'd expect and I ca n't figure out why . It seems that p.x and p.y are not getting set for some reason , even though the setter `` should '' set those values . Anybody know why this is ?"
"I have several apps running on uWSGI . Most of them grow in memory usage over time . I 've always attributed this to a memory leak that I had n't tracked down . But lately I 've noticed that the growth is quite chunky . I 'm wondering if each chunk correlates with a process being started.Does uWSGI start all processes at boot time , or does it only start up a new one when there are enough requests coming in to make it necessary ? Here 's an example config : update : this looks relevant : http : //uwsgi-docs.readthedocs.org/en/latest/Cheaper.htmlDoes `` worker '' mean the same thing as `` process '' ( answer seems to be yes ) ? If so then it seems like if I want the number to remain constant always , I should do :"
I would like to implement an exception handler for my Flask application that displays a custom error page when an Exception is thrown . I can get this working easily withbut this has the side effect of catching all exceptions before they hit the debugger ( either the Werkzeug debugger or my IDE 's ) so that debugging is effectively disabled.How can I implement a custom exception handler that still allows be to debug exceptions and errors ? Is there a way to disable my custom handler when in debug mode ?
"I am trying to bring a Django project from version 1.8 to 1.11 . Pretty much everything seems to work fine except unit tests . We have a base test class inheriting from Django TestCase with a Tastypie mixin . The base class has some code in the setUp ( ) like thisAnd the app specific tests would inherit the base test and do something likeSo , under Django 1.8.x this works fine . But under 1.11.x all of these give me an error on the User.objects.create_superuser ( ) line.django.db.utils.InterfaceError : connection already closed I have been going through the release notes , but there is just too much stuff that has happened between 1.8 and 1.11 . Is there something simple that I am missing ?"
"I have a csv file which is ~40gb and 1800000 lines . I want to randomly sample 10,000 lines and print them to a new file.Right now , my approach is to use sed as : Where $ vars is a randomly generated list of lines . ( Eg : 1p ; 14p ; 1700p ; ... ; 10203p ) While this works , it takes about 5 minutes per execution . It 's not a huge time , but I was wondering if anybody had ideas on how to make it quicker ?"
"In Graphene Python , how should one go about setting cookies in the schema.py when there is no access to the HttpResponse object to set the cookie on ? My current implementation is to set the cookie by overriding the GraphQLView 's dispatch method by catching the data.operationName . This involves hard-coding of the operation names / mutations that I need cookies to be set on . In views.py : Is there a cleaner way of setting cookies for specific Graphene Python mutations ?"
"Our code takes 10 minutes to siphon thru 68,000 records when we use : However when we do the following it takes just 1 second : Here is the code : All code I 've ever written in python uses the first option . This is just basic string operations ... we are reading input from a file , processing it and outputting it to the new file . I am 100 % certain that the first method takes roughly 600 times longer to run than the second , but why ? The file being processed is a csv but uses ~ instead of a comma . All we are doing here is taking this csv , which has a column for country , and adding a column for the countries region , e.g . LAC , EMEA , NA , etc ... cmdbre.regions is just a dictionary , with all ~200 countries as the key and each region as the value.Once I changed to the append string operation ... the loop completed in 1 second instead of 10 minutes ... 68,000 records in the csv ."
"In Python 2 you could do the following to get the current locale 's character set : However , in Python 3 the string module 's locale-dependent constants ( e.g . string.letters , string.lowercase , string.uppercase , etc . ) were removed.How can I get the current locale 's character set using Python 3 ?"
I 'm trying to run a little program that should save my 3D scatterplot instead of opening it in a GUI . The problem is that it does both ! This is the piece of code I 'm talking about : I would very much like to know how I can get a saved image of my plot without the plot being opened in a gui .
"I would like to launch an ncurses based application from python using subprocess module.The ncurses based application is TABARI , an event extraction system . The result of event extraction is saved to a file . I would like to launch it from a python script , wait for it to terminate and then read the results file . A code sample is shown bellow : The result of this code when running the program is PyCharm is : When I run the same code from a terminal initiated python interpreter ( the same as is used within PyCharm ) , the output is : I tried several things , including using shell=False , setting the bufsize to -1 , and investigating os.environ variables . One suspicious difference between the os.environ output from PyCharm and the terminal is the 'TERM ' variable , which does not exist in PyCharm and equals 'xterm ' in terminal.I would appreciate any help ."
"In Python3.4 , is it possible to open an SQLite3 database from an io.BytesIO stream ? Something akin to : The short story is : I have a stream ( byte_stream ) that is the sqlite database file . I ca n't do the following for security reasons ( ca n't create an unencrypted file ) : Is there some lower-level API for sqlite3 that I have n't been able to find ? I assume that sqlite3.connect simply calls open ( ) at some point and opens the file as a byte stream anyway . I 'm simply trying to skip that open ( ) step ."
I have a column in my data frame which has values like ' 3.456B ' which actually stands for 3.456 Billion ( and similar notation for Million ) . How to convert this string form to correct numeric representation ? This shows the data frame : This is a sample value : I tried this : But unfortunately there are also values with 'M ' at the end which denotes Millions . It returns error as follows : How can I replace both B and M with appropriate values in this column ? Is there a better way to do it ?
"Let 's say I have a matrix like so : and I want to make a function that will take in two matrices and do pointwise multiplication . ( not using numpy ) I 've seen some things on using zip but that does n't seem to be working for me . I think its because my list is of lists and not a single list.My code : Matrix1 could be plugged in as both arguments here . a second function called display_matrix would take this function in and display each element of the lists on new lines , but that 's beyond on the scope of this question . my guess is that i 'll need some list comprehensions or lambda functions but I 'm just too new to python to full grasp them ."
"I am trying to write a deeply nested set of classes , attributes , bound methods , etc . to a HDF5 file using the h5py module for long-term storage . I am really close . The only wrinkle I have that I ca n't seem to resolve is to programmatically , at run-time , figure out a way to determine if something is a class instance type , rather than a list , int , etc . I need to recurse into the class instance , but obviously should n't recurse into an int , float , etc . This needs to work for both old- and new-style classes . Things that I researched that do n't work/ I ca n't get to work : Using inspect moduleThis is n't helpful , I need a function like inspect.isclassinstance ( _R ) to return TrueUsing the types moduleIf you use old-style classes , there is a type called InstanceType that matches instances of old-style classes as in the code belowBut if you use new-style classes there is no corresponding type in types"
"I have a question about the software design necessary to schedule an event that is going to be triggered once in the future in Heroku 's distributed environment.I believe it 's better to write what I want to achieve , but I have certainly done my research and could not figure it out myself even after two hours of work.Let 's say in my views.py I have a function : so what I want to achieve is to be able to run after_6_hours function exactly 6 hours after create_game has been invoked . Now , as you can see , this function is defined out of the usual clock.py or task.py or etc etc files . Now , how can I have my whole application running in Heroku , all the time , and be able to add this job into the queue of this imaginary-for-now-scheduler library ? On a side note , I ca n't use Temporizer add-on of Heroku . The combination of APScheduler and Python rq looked promising , but examples are trivial , all scheduled on the same file within clock.py , and I just simply do n't know how to tie everything together with the setup I have . Thanks in advance !"
"I am mostly writing a small tools for tech savvy people , e.g . programmers , engineers etc . As those tools are usually quick hacks improved over time I know that there are going to be unhandled exceptions and the users are not going to mind . I would like the user to be able to send me the traceback so I can examine what happened and possibly improve the application.I usually do wxPython programming but I have done some Java recently . I have hooked up the TaskDialog class to the Thread.UncaughtExceptionHandler ( ) and I am quite happy with the result . Especially that it can catch and handle exceptions from any thread : I was doing something similar in wxPython for a long time . However : I had to write a decorator-hack to be able to print exceptions from another thread well.Even when functional , the result is quite ugly.Here is the code for both Java and wxPython so you can see what I have done : Java : wxPython : Now the question : Can I do easily something similar to Java solution in wxPython ? Or maybe , is there a better way in either Java or wxPython ?"
"I have ~30 methods ( ~6 logical groupings with ~5 methods per group ) which only do calculations based on parameters passed , they do not save state or need anything else beside parameter values.What is the more pythonic and better way of grouping this methods , using modules , or classes with static methods ? Difference will be : and : This are just example class , module and method names . Grouping with classes seems more logical to me . Is there any drawback to this way if this methods are accessed very frequently , or any other caveat ?"
"Is there an easy way to index a numpy multidimensional array along the last dimension , using an array of indices ? For example , take an array a of shape ( 10 , 10 , 20 ) . Let 's assume I have an array of indices b , of shape ( 10 , 10 ) so that the result would be c [ i , j ] = a [ i , j , b [ i , j ] ] . I 've tried the following example : However , this does n't work because it then tries to index like a [ b [ i , j ] , b [ i , j ] ] , which is not the same as a [ i , j , b [ i , j ] ] . And so on . Is there an easy way to do this without resorting to a loop ?"
"I compare phase and amplitude spectrum in Matlab and numpy . I think Matlab work correct , but numpy compute correct amplitude spectrum , but phase spectrum is strange . How i must change python code for correct computing fft by numpy ? Matlab : Python : matlab outputnumpy output"
"Here is the code : I was hoping to see the output like thisHowever , instead I see the standard representation : Why ?"
"I 'd like to generate a defaultdict that contains a deque . For example : The above work fine , but I 'd like to make the deque a fixed length by passing in an argument : How can I pass arguments such as this to defaultdict ?"
"So I may have a rather unique use case here , but I 'm thinking it should work- But it 's not working correctly.Basically , I have a class that uses a static factory method ( create ) that returns a shared_ptr to the newly createdinstance of the class . This class also has a virtual function that I 'd like to override from python and call from C++.Maybe my code can express the thought more clearly than my words : And here 's the contents of test.py : And the output : Testing ... quacking like a Wrapper ... ... no override found ! quacks like a ClassA Base quacking like a Wrapper ... ... no override found ! quacks like a ClassA BaseSo both the base and the class derived in python are acting the same . It looks like it 's not finding the override for some reason . I 'm not sure but this may have something to do with the create ( ) function . Any ideas would be greatly appreciated ! EDIT : Added pythonquack to the py script - This works as expect : Calling it for Ainst and Dinst says 'Quacks like a Base ' , and 'Quacks like a Derived ' , as I would expect . So for some reason the overrides are n't getting passed back to C++ ."
"Consider this simple dataframe : I perform a .apply as such : Why is pandas printing out junk each time ? I 've verified this happens in v0.20 . Edit : Looking for an answer , not a workaround ."
Are you able to use open id to log into the local development server with google app engine sdk version 1.4.1 and python 2.5 ? When I execute thisI get redirected to http : //localhost/_ah/login rather than the openid url.The openid url and continue url are valid.My app.yaml looks like thisIf I browse to http : //localhost/users/ I am also redirected to http : //localhost/_ah/login rather than http : //localhost/_ah/login_requiredIs there a config issue or does openid not work locally ?
"I 'm using Django 's markup package to transform restructuredText into html . Is there a way to customize the HTML writer to add a class attribute to each < p > tag ? I could use the class directive for each paragraph , but I 'd like to automate this process.For example , I want this restructured text : To be converted to this html.The reason I want to insert classes is because I 'm using the hyphenator library which works by adding hyphens to all tags with a `` hyphenate '' class . I could add the hyphenate class to the container tag , but then all the children would inherit the hyphenate class . I could use javascript to dynamically add the class , but I thought there might be a simple way to do it with restructuredText.Thanks for the help , Joe"
"I have an output file from a legacy piece of software which is shown below . I want to extract values from it , so that , for example , I can set a variable called direct_solar_irradiance to 648.957 , and target ground pressure to 1013.00.So far , I have been extracting individual lines and processing them like below ( repeated many times for the different values I want to extract ) : However , I have now found that extra lines are added to the middle of the output when certain parameters are selected . This means , of course that the 97th line will no longer have the values I need on it.Is there a good Pythonic way to extract these values , given that there may be extra lines added into the output under certain circumstances ? I guess I need to search for known pieces of text in the file , and then extract the numbers referred to by them , but the only ways I can think of doing that are very clunky.So : Is there a nice Pythonic way to search for these strings and extract the values that I want ? If not , is there some other way to sensibly do this ? ( for example , some kind of cool text-file parsing library that I know nothing about ) ."
"I 've encountered an overflow warning as a result of multiplying the output of Numpy products that I 'm looking to understand . A simplified version of their actual use within my larger project is detailed below : Including the use with the class structure for completeness , though heavily edited down to the bare minimum . If I run this code ( on Python 3.6.0 ) I get the following output : Clearly I can get around the problem using the regular multiplication , but I would like to understand why there is a problem and if it can be fixed as is . I think there is some dtype=X subtlety that I have missed , so my question is what is causing these overflow errors ?"
"I 'm using bokeh 1.0.1 version inside a Django application and I would like to display microscopic surface images as zoomable image plots with a color-encoded height and colorbar . In principle this works , but I have problems to get plots with the correct aspect ratio only showing the image without space around.Here is an example for what I want to achieve : The resulting plot shouldshow an image of random data having a width of sx=10 and a height of sy=5 in data space ( image size ) have axes limited to ( 0 , sx ) and ( 0 , sy ) , on initial viewand when zoominga square on the screen should match a square in data space , at least in initial viewFor the image I just use random data with nx=100 points in x direction and ny=100 points in y direction.Here is my first approach : Attempt 1I 've also added blue squares to the plot in order to see , when the aspect ratio requirement fails.Unfortunately , in the resulting picture , the square is no square any more , it 's twice as high as wide . Zooming and panning works as expected.Attempt 2When leaving out the ranges by usingI 'll get this picture . The square is a square on the screen , this is fine , but the axis ranges changed , so there isnow space around it . I would like to have only the data area covered by the image.Attempt 3Alternatively , when providing a plot_height and plot_width to the figure , with a pre-defined aspect ratio e.g . byI 'll get this picture . The square is also not a square any more . It can be done almost , but it 's difficult , because the plot_width also comprises the colorbar and the toolbar.I 've read this corresponding blog postand the corresponding bokeh documentation , but I can not get it working.Does anybody know how to achieve what I want or whether it is impossible ? Responsive behaviour would also be nice , but we can neglect that for now.Thanks for any hint.UpdateAfter a conversation with a Bokeh developer on Gitter ( thanks Bryan ! ) it seems that it is nearly impossible what I want . The reason is , how match_aspect=True works in order to make a square in data space look like a square in pixel space : Given a canvas size , which may result from applying different sizing_mode settings for responsive behaviour , the data range is then changed in order to have the matching aspect ratio . So there is no other way to make the pixel aspect ratio to match the data aspect ratio without adding extra space around the image , i.e . to extend the axes over the given bounds . Also see the comment of this issue.Going without responsive behaviour and then fixing the canvas size beforehand with respect to the aspect ratio could be done , but currently not perfectly because of all the other elements around the inner plot frame which also take space . There is a PR which may allow a direct control of inner frame dimensions , but I 'm not sure how to do it.Okay , what if I give up the goal to have tight axes ? This is done in `` Attempt 2 '' above , but there is too much empty space around the image , the same space that the image plot takes.I 've tried to use various range_padding* attributes , e.g.but it does n't reduce the amount of space around the plot , but increases it only . The padding in percent should be relative to the image dimensions given by dh and dw . Does anybody know how to use the range_padding parameters to have smaller axis ranges or another way to have smaller paddings around the image plot in the example above ( using match_aspect=True ) ? I 've opened another question on this ."
"I 'm using two different decorators from two different libraries . Lets say : @ decorator1 ( param1 , param2 ) and @ decorator2 ( param3 , param4 ) . That I often use in many functions as : Since it happens every time , I would like to create a custom decorator that handle both of them . Something like : How do I achieve that ?"
"I am reading first line of all the files in a directory , on local it works fine but on EMR this test is failing at stuck at around 200-300th file.Also ps -eLF show increase of childs to 3000 even print in on 200th line.It this some bug on EMR to read max bytes ? pydoop version pydoop==0.12.0"
"I have plotted my text data in the word cloud.this is the data frame I havethen I transformed it as a string like this ( Actually , I have coppied the number of the times each word happened in my data frame and then feed to the function ) : then I have used this code to visualize text data : and then the result is like this : as you see most words are repeated 2 or 3 times but their size in the word cloud does not show this . even for the words of the same size , there is a big difference in sizes ! for example : for example look at `` tinnitu '' and `` dysuria '' in this data frame which both has the frequency of 3 , tinnitu quite big but dysuria you 'll find it very hard as it is very small.Thanks : )"
"Give an input sentence , that has BIO chunk tags : [ ( 'What ' , ' B-NP ' ) , ( 'is ' , ' B-VP ' ) , ( 'the ' , ' B-NP ' ) , ( 'airspeed ' , ' I-NP ' ) , ( 'of ' , ' B-PP ' ) , ( 'an ' , ' B-NP ' ) , ( 'unladen ' , ' I-NP ' ) , ( 'swallow ' , ' I-NP ' ) , ( ' ? ' , ' O ' ) ] I would need to extract the relevant phrases out , e.g . if I want to extract 'NP ' , I would need to extract the fragments of tuples that contains B-NP and I-NP . [ out ] : ( Note : the numbers in the extract tuples represent the token index . ) I have tried extracting it using the following code : But when I have adjacent chunk of the same type : It outputs this : Instead of the desired : How can this be resolved from the above code ? Other than how it 's done from the code above , is there a better solution to extract the desired chunks of a specific chunk_type ?"
"I am getting a system error ( shown below ) while performing some simple numpy-based matrix algebra calculations in parallel using Multiprocessing package ( python 2.73 with numpy 1.7.0 on Ubuntu 12.04 on Amazon EC2 ) . My code works fine for smaller matrix sizes but crashes for larger ones ( with plenty of available memory ) The size of the matrices I use is substantial ( my code runs fine for 1000000x10 float dense matrices but crashes for 1000000x500 ones - I am passing these matrices to/from subprocesses by the way ) . 10 vs 500 is a run-time parameter , everything else stays the same ( input data , other run-time parameters etc . ) I 've also tried to run the same ( ported ) code using python3 - for larger matrices the subprocesses go to a sleep/idle mode ( instead of crashing as in python 2.7 ) and the program/subprocesses just hang in there doing nothing . For smaller matrices the code runs fine with python3.Any suggestions would be highly appreciated ( I am running out of ideas here ) Error message : The Multiprocessing code I use : Below is the `` proc '' that gets executed for each subprocess . Basically , it solves many systems of linear equations using numpy ( it constructs required matrices inside the subprocess ) and returns the results as another matrix . Once again , it works fine for smaller values of one run-time parameter but crashes ( or hangs in python3 ) for larger ones ."
"My goal is to detect that 2 string are same but in different order . What i already tried is splitting both string into list and compare it within loop.It return what i intended , but is this actually correct and efficient way ? Maybe there is another approach . Any suggestion of journal about that topic would be really nice.Edit 1 : Duplicate words are importantIt must return false"
"I am using Numpy and Python in a project where a 2D map is represented by an ndarray : An object has a tuple location : and a view range : How do I write the function actor.view_map ( map ) , such that the map returns the area surrounding the actor 's location up to a range . For example ( using the above map ) , but if the actor 's range extends too far I want the map filled with -1 : the easiest case is a range of 0 , which returns the current square : How do I slice my map up to a certain boundary ?"
"I have a 2D array containing grayscale image created from .png as follows : What I would like to do is to extract a subarray containing only the rectangle containing the data - ignoring all zeros surrounding the picture.For example , if input is : then output should be : I could iterate over the rows in forward direction to find the first nonzero row and then iterate over the rows backwards to find the last nonzero row remembering indices - and then repeat the same for the columns and then extract a subarray using that data but I am sure there are more appropriate ways of doing the same or there even might be a NumPy function designed for such a purpose.If I were to choose between shortest code vs fastest execution I 'd be more interested in fastest code execution.EDIT : I did not include the best example because there could be zero rows/columns in the middle as here : Input : Output :"
My Flask app has url routing defined asProblem is one of the application making queries to this app adds additional / in url like /api/1//accounts/id . It 's not in my control to correct the application which makes such queries so I cant change it.To resolve this problem currently I have added multiple rulesThere are number of such routes and it 's ugly workaround . Is there a way in flask to modify URL before it hits the routing logic ?
"I am wondering how matrix multiplication can be supported in numpy with arrays of dtype=object . I have homomorphically encrypted numbers that are encapsulated in a class Ciphertext for which I have overriden the basic math operators like __add__ , __mul__ etc.I have created numpy array where each entry is an instance of my class Ciphertext and numpy understands how to broadcast addition and multiplication operations just fine . However , numpy wo n't let me do matrix multiplicationsI do n't quite understand why this happens considering that addition and multiplication works . I guess it has something to do with numpy not being able to know the shape of the object , since it could be a list or something fance . Naive Solution : I could write my own class that extends ndarray and overwrite the __matmul__ operation , but I would probably lose out on performance and also this approach entails implementing broadcasting etc. , so I would basically reinvent the wheel for something that should work as it is right now . Question : How can I use the standard matrix multiplication provided by numpy on arrays with dtype=objects where the objects behave exactly like numbers ? Thank you in advance !"
"I 'm trying to determine the cycles in a directed graph using Tarjan 's algorithm , presented in his research paper `` Enumeration of the elementary circuits of a directed graph '' from Septermber 1972.I 'm using Python to code the algorithm , and an adjacency list to keep track of the relationships between nodes.So in `` G '' below , node 0 points to node 1 , node 1 points to nodes 4,6,7 ... etc.Tarjan 's algorithm detects the following cycles : [ 2 , 4 ] [ 2 , 4 , 3 , 6 , 5 ] [ 2 , 4 , 3 , 7 , 5 ] [ 2 , 6 , 5 ] [ 2 , 6 , 5 , 3 , 4 ] [ 2 , 7 , 5 ] [ 2 , 7 , 5 , 3 , 4 ] [ 3 , 7 , 5 ] I 've also done Tiernan 's algorithm , and it ( correctly ) finds 2 extra cycles : [ 3,4 ] [ 3,6,5 ] I 'd appreciate any help in finding out why Tarjan skips over those 2 cycles . A problem in my code perhaps ?"
"Is there a better way in numpy to tile an array a non-integer number of times ? This gets the job done , but is clunky and does n't easily generalize to n-dimensions : this yields : EDIT : Performance resultsSome test of the three answers that were generalized to n-dimensions . These definitions were put in a file newtile.py : Here are the bash lines : Here are the results with small arrays ( 2 x 3 x 5 ) : Here are the results with larger arrays ( 2 x 3 x 5 x 7 x 11 ) : So the method using np.pad is probably the most performant choice ."
"Can I use a python object in a desired boolean context ? By default any object is True in boolean context.Like bool ( 0 ) returns False and bool ( 1 ) returns True.Can i have any way to define an object to have it 's boolean value either True or False.Correct me if i 'm wrong anywhere , thanks ."
"I have a class which has two methods that raise NotImplementedError and also inherits from an abstract class ( a class that contains abstract methods , from the abc package . This parent class in turn inherits from a class marked as abstract through __metaclass__ = ABCMeta ) . Due to this a R0921 warning is raised when running pylint on my code . If I remove the NotImplementedErrors pylint does not give that warning . Now , I 've tried disabling the R0921 for the class like this : But it does not seem to work . I still get the warning `` Abstract class not referenced '' . What am I missing ?"
"I am exploring different concepts in python and I happened to read upon an example of coroutines which can be used for the chain of responsibility design pattern . I wrote the following code : What I have tried to do here is try to create a chain of responsibility pattern solution for handling different types of customers ( Platinum , Gold , Diamond , Silver ) . For that Customer has a pipeline where I have mentioned the order in which the different customers will be handled . Customer ( ) .HandleCustomer will send an instance of itself through the pipeline which will check whether its custtype matches and then process it accordingly or it will send it across to its successor ( if available ) PROBLEM : The problem is that when I run the above script , it will handle the first platinum customer but not the gold or the undefined . I am assuming this is because he has reached the end of the generator . How do I modify the code so that everytime it is a new instance of a customer , it will go through the pipeline from its beginning ?"
Hello I 'm trying to calculate the first 10000 prime numbers.I 'm doing this first non threaded and then splitting the calculation in 1 to 5000 and 5001 to 10000 . I expected that the use of threads makes it significant faster but the output is like this : There is in fact no big difference except that the threaded function is even a bit slower.What is wrong ? This is my code :
"I have an abstract base class : I 'd like to use this class as a spec for a mock.This works partially . For example , mock_a.x results in AttribureError ( `` Mock object has no attribute ' x ' ) . However mock_a.f is not speced based on the abstract method from A.f . It returns a mock regardless of the number of arguments passed to f.Mock can create a mock speced from A.f with create_autospec ... ... but does n't do so for the attributes of AHow can I create a mock that accurately implements the abstract base class ?"
"I would like to display an image in Python using gstreamer bindings , but without using GTK+ ( I 'm on ARM ) . I know how to listen to music with python and gstreamer : I know how to display an image with gstreamer in command line : What I would want is the exact same thing , but using Python . I tried some things , the code runs without errors , but nothing shows on screen.Would you have any idea that could help me ? Thanks by advance ! By the way , I also have audiotest and videotest working . Here is an example that runs fine :"
"What is the fastest performing regular expression that does not match any string ? It may seem like a useless thing , but consider a program that takes a mandatory regex as a filter for instance ( this is actually my scenario ) . I 've tried a few and found b ( ? < ! b ) to be the best performer given that b occurs rarely in the input.Here is a python code I wrote to test different patterns for their speed : On my machine , I get the following times : update : added benchmark for some of suggested regexes ."
"Coming from JavaScript , if a `` class '' prototype was augmented with a certain attribute . It is known that all instances of that `` class '' would have that attribute in its prototype chain , hence no modifications has to be done on any of its instances or `` sub-classes '' .In that sense , how can a Class-based language like Python achieve Monkey patching ?"
"EDIT2 : Github link below contains possible solutions to the problem of calling TF model from process . They include eager execution and dedicated server process , serving TF model predictions via http requests . I wonder if with custom server and requests I win any time compared to initializing global variables each time and calling tf.train.Server , but it seems to be more elegant way.I will investigate memory leak , and if it is gone , close this question . EDIT : Added simple reproducible example of the problem : https : //github.com/hcl14/Tensorflow-server-launched-from-child-processBackground : I am running Tensorflow server , and connecting to it from 'forked ' processes . Creating ( and destroying ) processes dynamically is essential for me - I moved highly loaded part of code there because of a weird memory leak , not visible to Python profilers ( threads do not solve the issue ) . Therefore , I want processes to be initialized fast and immediately start working . Memory is freed only when process is destroyed.Doing experiments , I found a solution when loaded model and graph are saved into global variable , then taken by child process ( which uses 'fork ' mode by default ) and then server is called.Problem : The strange thing for me is that , after loading keras models , I can not lock graph which I do not expect to modify , and I need to run tf.global_variables_initializer ( ) each time I open new session in child process . However , dummy run in the main flow without any session creation works Ok . I know that in this case tensorflow uses default session , but all the variables on a graph should be initialized after model run , so I expected new session to work Ok with previously defined graph.Thus , I think that modifying model makes Python to pickle a lot to the child process ( 'fork ' mode ) , which creates computational and memory overhead.Please , excuse me for a lot of code . The model I use is legacy and black box for me , so it is possible that my problem is related to it . Tensorflow version is 1.2 ( I can not upgrade it , model is not compatible ) , Python 3.6.5.Also , maybe my solution is inefficient and there is better one , I would be grateful for your advice.My setup is the following:1.Tensorflow server started in main process : Initialize the server : In main process:2.init_interests_for_process ( ) is a model initializer , which loads my legacy model and shares it in the global variable . I do one dummy model pass to have everything initialized on the graph , and then want to lock the graph . But it is not working:3.Now I spawn the process ( actually , the process is spawned from another process - hierarchy is complicated ) :4.And inside this process I am calling the model : Everything works Ok if I do n't lock the graph and run variable initializer each time new process is spawned . ( Except , there is a memory leak of about 30-90 MB for each call , not visible to python memory profilers ) . When I want to lock the graph , I get errors about uninitialized variables : Thanks in advance !"
"While there are several posts on StackOverflow that are similar to this , none of them involve a situation when the target string is one space after one of the substrings.I have the following string ( example_string ) : < insert_randomletters > [ ? ] I want this string.Reduced < insert_randomletters > I want to extract `` I want this string . '' from the string above . The randomletters will always change , however the quote `` I want this string . '' will always be between [ ? ] ( with a space after the last square bracket ) and Reduced.Right now , I can do the following to extract `` I want this string '' .This eliminates the ] and that always appear at the start of my extracted string , thus only printing `` I want this string . '' However , this solution seems ugly , and I 'd rather make re.search ( ) return the current target string without any modification . How can I do this ?"
"I have a list : Target number : I want to find between which pair of timestamp does ptime lie in . For e.g . in this case , it lies between the first and the second timestamp . So I want to return the value 1377091800 as the desired output.Similarly , if ptime was 1377091520 , then I want the third timestamp to be returned i.e . 1377091500 since it lies between third and the fourth timestamp.My code : Output : 1377091800I wanted to know is there any elegant solution to this ? Since I am just starting with python , I am not yet familiar with all the functions in python.Any help is appreciated.EDIT : SOLUTION USED : OUTPUT :"
"I encounter an issue with plotly . I would like to display different figures but , somehow , I ca n't manage to achieve what I want.I created 2 sources of data : Then , I created a subplot with 1 row and 2 columns and tried to add this data to the subplot : That resulted in the following error : I mist be doing something wrong . What is weird is that my data seems correctly shaped since I can run the following code without any issue : I hope you can help me find a solution.Thanks !"
"According to Python documentation , both dir ( ) ( without args ) and locals ( ) evaluates to the list of variables in something called local scope . First one returns list of names , second returns a dictionary of name-value pairs . Is it the only difference ? Is this always valid ?"
"For the first time , I 'm using Python to create a library , and I 'm trying to take the opportunity in this project to learn unit testing . I 've written a first method and I want to write some unit tests for it . ( Yes , I know that TDD requires I write the test first , I 'll get there , really . ) The method is fairly simple , but it expects that the class has a file attribute set , that the attribute points to an existing file , and that the file is an archive of some sort ( currently only working with zip files , tar , rar , etc. , to be added later ) . The method is supposed to return the number of files in the archive.I 've created a folder in my project called files that contains a few sample files , and I 've manually tested the method and it works as it should so far . The manual test looks like this , located in the archive_file.py file : All I do then is make sure that what 's printed is what I expect given the contents of test.zip.Here 's what file_count looks like : Directly translating this to a unit test seems wrong to me for a few reasons , some of which may be invalid . I 'm counting on the precise location of the test files in relation to the current script file , I 'll need a large sample of manually created archive files to make sure I 'm testing enough variations , and , of course , I 'm manually comparing the returned value to what I expect because I know how many files are in the test archive.It seems to me that this should be automated as much as possible , but it also seems that doing so is going to be very complicated.What 's the proper way to create unit tests for such a class method ?"
"I 'm reading in some gzipped data from s3 , using dask ( a replacement for a SQL query ) . However , it looks like there is some caching of the data file , or unzipped file somewhere that keeps in system memory . NB this should be runnable , the test data here is used from the pandas test suite in a public s3 bucket.Which gives me : Naively , I would expect the 'after function call ' to be the 'before function call ' plus the dataframe and a bit of overhead . Here , the gzip is 43mb , and results in an overhead of about 90mb , in my real example , this extra part is about 50gb of extra memory for a 10gb dataframe.You can see that the memory is freed up if you rerun on another , smaller file - uncomment the rerun on the smaller file to see it . This also shows that the increase is due to the file size - you can switch the order and run 'tips ' first and the memory stays at ~90mb.I am guessing dask , s3fs or pandas is holding the file or the unzipped contents in a buffer somewhere , but I have n't been able to track it down to clear it . Any ideas on how to reduce this memory use , or free the buffer ? EDIT : An example of the above output for some of my real data - 32 gzipped files : I understand dask will have a higher peak memory usage than a pandas loop over the same 32 files , but I still do n't get why it does n't get freed up ."
"I 'm trying to transform a pandas dataframe with three columns ( Date , Start , End ) into a frequency matrix . My input dataframe look like this : The values of 'Start ' and 'End ' are integers between 0 and 23 inclusive . The 'Date ' is a datetime . The frequency matrix I 'm trying to create is a 24 by 24 csv , where row i and column j is the number of times 'End'=i and 'Start'=j occurs in the input . For example , the above data would create : For extra help , could this be done in a way that creates a separate matrix for every 15 minutes ? That would be 672 matrices as this date range is one week.I 'm a self taught beginner , and I really ca n't think of how to solve this in a pythonic way , any solutions or advice would be greatly appreciated ."
While trying to configure the TEMPLATE_CONTEXT PROCESSORS in Django settings.py I am getting the following error : My TEMPLATE_CONTEXT_PROCESSORS is as follows : ( Django Version : 1.5.1 )
"I am using MySQL ( running InnoDB ) , and wrapped the entire thing using sqlalchemy . Now , I would like to generate changes in my database by using ( see docs ) Generally the above function does what it is supposed to . The only exception being the generation of unique indexes.Say , I define a table like this : when I call create_database I will get sqlalchemy to create the table 'my_table ' with all columns as specified . The foreign key is also setup fine , but no unique index can be found on the database side . I then tried using a Index ( unique=True ) instead . So instead of I putMy impression was this logically produces a similar result . This time sqlalchemy indeed created the unique index on the db.Maybe I am miserably misunderstanding something about the difference between UniqueConstraint and Index ( unique=True ) , or the way sqlalchemy uses them to automate generation of databases . Can anyone shed some light on this ?"
"I have two Pandas dataframes , namely : habitat_family and habitat_species . I want to populate habitat_species based on the taxonomical lookupMap and the values in habitat_family : The nested for loops above do the job . But in reality the sizes of my dataframes are massive and using for loops are not feasible . Is there a more efficient method to achieve this using maybe dataframe.apply ( ) or a similar function ? EDIT : The desired output habitat_species is :"
"Let 's say that I 've got a situation like this in Python : I know that the order of my_dict.keys ( ) is undefined , but what I 'm wondering is whether the initialization via a literal like this is guaranteed to happen in a particular order . Will the value of my_dict [ 'key1 ' ] always be 1.0 as asserted ?"
"In another Q+A ( Can I perform dynamic cumsum of rows in pandas ? ) I made a comment regarding the correctness of using prange about this code ( of this answer ) : The comment was : I would n't recommend parallelizing a loop that is n't pure . In this case the running variable makes it impure . There are 4 possible outcomes : ( 1 ) numba decides that it can not parallelize it and just process the loop as if it was cumsum instead of prange ( 2 ) it can lift the variable outside the loop and use parallelization on the remainder ( 3 ) numba incorrectly inserts synchronization between the parallel executions and the result may be bogus ( 4 ) numba inserts the necessary synchronizations around running which may impose more overhead than you gain by parallelizing it in the first placeAnd the later addition : Of course both the running and cumsum variable make the loop `` impure '' , not just the running variable as stated in the previous commentThen I was asked : This might sound like a silly question , but how can I figure out which of the 4 things it did and improve it ? I would really like to become better with numba ! Given that it could be useful for future readers I decided to create a self-answered Q+A here . Spoiler : I can not really answer the question which of the 4 outcomes is produced ( or if numba produces a totally different outcome ) so I highly encourage other answers ."
"First Note : They wont let me embed images until i have more reputation points ( sorry ) , but all the links are images posted on imgur ! : ) thanksI have replicated a method to animate any single path ( 1 closed path ) using fourier transforms . This creates an animation of epicylces ( rotating circles ) which rotate around each other , and follow the imputed points , tracing the path as a continuous loop/function.I would like to adopt this system to 3D . the two methods i can think of to achieve this is to use a Spherical Coordinate system ( two complex planes ) or 3 Epicycles -- > one for each axis ( x , y , z ) with their individual parametric equations . This is probably the best way to start ! ! 2 Cycles , One for X and one for Y : Picture : One Cycle -- > Complex Numbers -- > For X and Y Fourier Transformation Background ! ! ! : • Eulers formula allows us to decompose each point in the complex plane into an angle ( the argument to the exponential function ) and an amplitude ( Cn coefficients ) • In this sense , there is a connection to imaging each term in the infinite series above as representing a point on a circle with radius cn , offset by 2πnt/T radians• The image below shows how a sum of complex numbers in terms of phases/amplitudes can be visualized as a set of concatenated cirlces in the complex plane . Each red line is a vector representing a term in the sequence of sums : cne2πi ( nT ) t• Adding the summands corresponds to simply concatenating each of these red vectors in complex space : Animated Rotating Circles : Circles to Animated Drawings : • If you have a line drawing in 2D ( x-y ) space , you can describe this path mathematically as a parametric function . ( two separate single variable functions , both in terms of an auxiliary variable ( T in this case ) : • For example , below is a simple line drawing of a horse , and a parametric path through the black pixels in image , and that path then seperated into its X and Y components : • At this point , we need to calculate the Fourier approximations of these two paths , and use coefficients from this approximation to determine the phase and amplitudes of the circles needed for the final visualization.Python Code : The python code used for this example can be found here on guithubI have successful animated this process in 2D , but i would like to adopt this to 3D.The Following Code Represents Animations in 2D -- > something I already have working : [ Using JavaScript & P5.js library ] The Fourier Algorithm ( fourier.js ) : The Sketch Function/ Animations ( Sketch.js ) : And Most Importantly ! THE PATH / COORDINATES : ( this one is a triangle )"
"How do I create a special method __repr__ where I can print , for example , ' 6 of spades ' or ' Q of diamonds ' ? How do I access the data from the namedtuple , keeping in mind that I have a list of namedtuples in self._cards ?"
"My task is to detect the cracks on the soil surface and calculate crack 's total area . I used Canny edge detection for this purpose . Input image Result My next step is to convert canny edges to contours since I want to filtrate the cracks using cv2.mean and calculate their area using cv2.contourArea functions . On this step , I faced the problem . When I used : It does not convert properly because of the holes the ends of edges . See problem hereMy question is how can I connect the ends of edges in order to close the hole between them ? Note : I used contours detection without applying Canny edges . The problem is that contours detection gives a lot of noise and does n't detect all cracks well . Or maybe I do not know how to find contours as canny edges do ."
"So I 've wrote a small script to download pictures from a website . It goes through a 7 alpha charactor value , where the first char is always a number . The problem is if I want to stop the script and start it up again I have to start all over.Can I seed itertools.product somehow with the last value I got so I do n't have to go through them all again.Thanks for any input.here is part of the code :"
"Is there a good way to add/remove a neuron and its associated connections into/from a fully connected PyBrain network ? Say I start with : How would I go about making it a ( 2,4,1 ) or a ( 2,2,1 ) network WHILE maintaining all the old weights ( and initializing any new ones to be random as is done when initializing the network ) ? The reason I want to do this is because I am attempting to use an evolutionary learning strategy to determine the best architecture and the 'mutation ' step involves adding/removing nodes with some probability . ( The input and output modules should always remain the same . ) edit : I found NeuronDecomposableNetwork which should make this easier , but it still seems that I have to keep track of neurons and connections separately ."
"I have a dict containing lists and need a fast way to dedupe the lists.I know how to dedupe a list in isolation using the set ( ) function , but in this case I want a fast way of iterating through the dict , deduping each list on the way.I 'd like it to appear like ; Though I do n't necessarily need to have the original order of the lists preserved.I 've tried using a set like this , but it 's not quite correct ( it 's not iterating properly and I 'm losing the first key ) EDIT : Following PM 2Ring 's comment below , I 'm now populating the dict differently to avoid duplicates in the first place . Previously I was using lists , but using sets prevents dupes to be appended by default ; As you can see , the final output is a deduped set , as required ."
"I want to define a class that is able to populate itself reading from the serialized data of another instance . Here 's the simplified code : This is the output I obtain : The problem I have is that the instance returned is always empty , even though I am able to read all the elements inside __init__ ( ) What can be causing this ?"
"lastly I 'm working on google colab I get this dataset colled celeba and it is into a google drive accout and this account is not mine but I have the access to go through it now because the internet problems and drive capacity I can not dounload the dataset then upload it to my drive ... so the question is : is there any way to let google colab get access to this dataset or such a way to import the path ... I have this function definition below where I have tried to put the sharablelink of the dataset but , it does not work please help"
"I 'm building a CGI script that polls a SQLite database and builds a table of statistics . The source database table is described below , as is the chunk of pertinent code . Everything works ( functionally ) , but the CGI itself is very slow as I have multiple nested SELECT COUNT ( id ) calls . I figure my best shot at optimization is to ask the SO community as my time with Google has been relatively fruitless.The table : ( Yes , I know the table is n't normalized but it 's populated with extracts from a mail log ... I was happy enough to get the extract & populate working , let alone normalize it . I do n't think the table structure has a lot to do with my question at this point , but I could be wrong . ) Sample row : And , the Python code that builds my tables : I 'm not sure if there are any ways I can combine some of the queries , or approach it from a different angle to extract the data . I had also thought about building a second table with the counts in it and just updating it when the original table is updated . I 've been staring at this for entirely too long today so I 'm going to attack it fresh again tomorrow , hopefully with some insight from the experts ; ) Edit : Using the GROUP BY answer provided below , I was able to get the data needed from the database in one query . I switched to Perl since Python 's nested dict support just did n't work very well for the way I needed to approach this ( building a set of HTML tables in a specific way ) . Here 's a snippet of the revised code : What once executed in about 28.024s now takes 0.415s !"
"Im trying to refactor my application ( with 1000+ lines of GUI code ) to an MVC style pattern.The logic code is already seperate from the GUI so that is not a problem . My concern is seperation of the view from the controller . I understand the basic principal of MVC and this tutorial in the wxpython wiki has been very helpfull but the code example is a little simplistic and leaves me with doubts when I try to apply the principal to my own project which is quite a bit more complex.A snippet of the structure..I have a MainWindow with a number of widgets including a noteBook ( tabbed section ) , the noteBook has a number of tabs one of the tabs ( which I call FilterTab ) holds two instances of a class ( that I call a FilterPanel ) that is a panel with a listbox , three buttons , one to clear , one to remove and one to add items to/from the list . Depending on flags passed to the class on instantiation the add buttons event can create different types of dialogs , eg a text entry dialog or directoryPicker etc..Thats just one part of the GUI which is quite layered -with event handlers buried in the FilterPanel class . If I were to convert that part to MVC I would have to bind the button events for each instance of the FilterPanel in my controller ( instead of in the filterPanel class ) -in this case there are two ( filterPanel instances ) So I would have something like this for each button , ( 3 buttons per filterPanel * number of panel instances ) plus the handlers..Which adds alot of extra code , ( double the amount of event handlers if I have only two filterPanel instances ) So Id like to know am I taking the right approach ?"
"When optimising slow parts of my code , I was surprised by the fact that A.sum ( ) is almost twice as fast as A.max ( ) : I had expected that A.any ( ) would be much faster ( it should need to check only one element ! ) , followed by A.max ( ) , and that A.sum ( ) would be the slowest ( sum ( ) needs to add numbers and update a value every time , max needs to compare numbers every time and update sometimes , and I thought adding should be slower than comparing ) . In fact , it 's the opposite . Why ?"
Maybe groupby is the wrong approach . Seems like it should work but I 'm not seeing it ... I want to group an event by it 's outcome . Here is my DataFrame ( df ) : Here is my desired result : I 'm trying to make a grouped object but I ca n't figure out how to call it to display what I want .
"I am using Spark cluster and I would like to implement linear regression by executing this code : IllegalArgumentException : u'requirement failed : Column features must be of type org.apache.spark.ml.linalg.VectorUDT @ 3bfc3ba7 but was actually org.apache.spark.mllib.linalg.VectorUDT @ f71b0bce.After googling this error I found an answer that says : use from pyspark.ml.linalg import Vectors , VectorUDTinstead of orand a function : With example data : But still I have the same error : here some data : cause , '' weight0 '' , '' dbp0 '' , '' gfr0m '' 1 , '' 90 '' , '' 10 '' , '' 22.72 '' 5 , '' 54 '' , '' 10 '' , '' 16.08 '' 6 , '' 66 '' , '' 9 '' , '' 25.47 '' 3 , '' 110 '' , '' 11 '' , '' 32.95 '' 5 , '' 62 '' , '' 11 '' , '' 20.3 '' 5 , '' 65 '' , '' 8 '' , '' 28.94 '' 1 , '' 65 '' , '' 8 '' , '' 15.88 '' 5 , '' 96 '' , '' 8 '' , '' 38.09 '' 5 , '' 110 '' , '' 8 '' , '' 41.64 '' 4 , '' 68 '' , '' 8 '' , '' 25.85 '' 5 , '' 68 '' , '' 7 '' , '' 37.77 '' 1 , '' 82 '' , '' 9.5 '' , '' 16.25 '' 5 , '' 76 '' , '' 10 '' , '' 37.55 '' 5 , '' 56 '' , '' '' , '' 37.06 '' 1 , '' 93 '' , '' 8 '' , '' 18.26 '' 5 , '' 80 '' , '' 7.5 '' , '' 48.49 '' 1 , '' 73 '' , '' 8 '' , '' 38.37 '' 4 , '' 76 '' , '' 8 '' , '' 31.09 '' 1 , '' 68 '' , '' 8 '' , '' 39.62 '' 1 , '' 82 '' , '' 8 '' , '' 40.08 '' 1 , '' 76 '' , '' 9.5 '' , '' 28.2 '' 5 , '' 81 '' , '' 10 '' , '' 36.66 '' 2 , '' 80 '' , '' '' , '' 47.1 '' 5 , '' 91 '' , '' 10 '' , '' 16.59 '' 2 , '' 58 '' , '' 8 '' , '' 49.22 '' 1 , '' 76 '' , '' 7 '' , '' 38.98 '' , '' 61 '' , '' 8 '' , '' 21.8 '' 5 , '' 50 '' , '' 6 '' , '' 26.97 '' 1 , '' 83 '' , '' 7 '' , '' 27.81 '' 1 , '' 86 '' , '' 8 '' , '' 48.62 '' , '' 77 '' , '' 6 '' , '' 46.78 '' 5 , '' 64 '' , '' 6 '' , '' 34.17 '' 5 , '' 58 '' , '' 6 '' , '' 38.95 '' 1 , '' 73 '' , '' 6 '' , '' 7.63 '' 5 , '' 86 '' , '' 8 '' , '' 32.46 '' 1 , '' 50 '' , '' 6 '' , '' 35.98 '' 5 , '' 90 '' , '' 7 '' , '' 32.26 '' 5 , '' 42 '' , '' 7 '' , '' 17.3 '' 1 , '' 88 '' , '' 7 '' , '' 25.61 '' 5 , '' 110 '' , '' '' , '' '' 1 , '' 84 '' , '' 6 '' , '' 31.4 '' 5 , '' 68 '' , '' 8 '' , '' 53.25 '' 1 , '' 96 '' , '' 8 '' , '' 52.65 '' 6 , '' 74 '' , '' 8 '' , '' 40.77 '' 1 , '' 70 '' , '' 9.5 '' , '' 22.35 '' 6 , '' 54 '' , '' 8 '' , '' 20.16 '' 1 , '' 52 '' , '' 13 '' , '' 32.61 '' , '' 84 '' , '' 8 '' , '' 52.98 '' 5 , '' 90 '' , '' 9 '' , '' 28.67 ''"
I am trying to numerically Solve an ODE that admits discrete jumps . I am using the Euler Method and was hoping that Numba 's jit might help me to speed up the process ( right now the script takes 300s to run and I need it to run 200 times ) .Here is my simplified first attempt : I do n't understand why I got this error . My suspicion is that numba does not recognize the input field fdot ( which is a python function which btw is already compiled with Numba ) .Since I am so new to Numba I have several questionsWhat can I do to make Numba understand the input field fdot is a function ? Using JIT on the function fdot `` only '' leads to a decrease in 50 % . Should I expect more ? or is this normal ? Does this script look like a reasonable way to simulate an ODE with discrete jumps ? Mathematically this is equivalent at solving an ODE with delta functions.Numba version is 0.17
"In C/C++ , comparison operators such as < > have higher priority than == does . This code will evaluate to true or 1 : But in Python , it seems wrong : In Python , does every comparison operator have the same priority ?"
"I would like to ask how I could embed a seaborn figure in wxPython panel.Similarly to this post , I want to embed an external figure in a wxPython panel . I would like a specific panel of my wxPython GUI to plot the density contours of my data based on bandwidth values of a Gaussian kernel , according to Seaborn 's kdeplot function , along with a scatter plot of the data points . Here is an example of what I would like to be plotted in the panel : Until now , I have managed to get what I want in a separate figure out of the wxPython panel.Is it possible to embed a seaborn plot in a wxPython panel or should find an alternative way to implement what I want ? Below is the specific part of my code that generates the plot in case it is needed : This part of the code plots in the wxPython panel the scattered data points and creates an external figure for the density contours . But , if I try ax.sns.kdeplot ( ... ) I get the error Attribute Error : AxesSubplot object has not attribute .snsI do n't know if I can embed a Seaborn figure in wxPython panel or I should try implement it in another way . Any suggestions ? Thanks in advance ."
I wrote a small example of the issue for everybody to see what 's going on using Python 2.7 and Django 1.10.8Exits with the following output + tracebackHas somebody an idea what 's going on here ?
"I 'd like to be able to detect trigger pulls independently , but the triggers appear to share a single axis . I did n't see anything on Xbox One controllers in the MSDN docs , but for Xbox 360 controllers , DirectInput triggers share the axis , and XInput gives separate axes to each trigger . This suggests that pygame is using DirectInput rather than Xinput , but I do n't know how to force pygame to use Xinput instead . ( How ) can I force pygame to use xinput instead of direct input ? If this is not really feasible , I 'd be willing to use a different language.I am using this script to read input from the controller : EDIT : I do n't have time at the moment to write this out in full , but in the meantime , I found this code which appears to work for me ( on an Xbox One Spectra controller ) : https : //github.com/r4dian/Xbox-360-Controller-for-Python"
I try to reuse HTTP-session as aiohttp docs advice Don ’ t create a session per request . Most likely you need a session per application which performs all requests altogether.But usual pattern which I use with requests lib doesn ` t work : Then I try toI got errorAny workarounds with async_timeout did n't help.Another way is working : But it seems like creating session per request.So my question : how to properly reuse aiohttp-session ? UPD : minimal working example . Sanic application with following view
"I 'm reaching back to my CLOS ( Common Lisp Object System ) days for this abstract question . I 'm augmenting the question to clarify : It appears to me that a Python decorator is sort of like an `` around '' method in CLOS.From what I remember , an `` around '' method in CLOS is a method/function that wraps around the primary method/function of the same name . It traverses up and down sub-classes too . Here 's some syntax ( I just grabbed my book ) .All of these methods This would be inside a class : There can be before and after methods too ( which I 'm throwing in for completeness ) : And finally the around method ( Notice here that this method seemed to be like a decorator ) : I believe the output would be : I have always used this understanding of classes and their methods as a reference point for developing code . And unfortunately few languages seem to get this far with their method parameterization and power.I 'm pretty new to Python and am trying to see how decorators fit in . They seem a little looser in that a decorator can be a completely external function which yet has the ability to manipulate information within the calling information and even modifying the instance and class variables of the object called , and further that it seems to preform the role of the around method as shown here . But I was hoping somebody could help explain the relationship between decorators and around methods . I thought somebody would really like the opportunity to do that.What makes CLOS powerful to me is that you can have multiple inheritance with these methods . Thus a class can be made up of superclasses that contain distinct functionalities and attributes which handle themselves . Thus an around method on one of the superclasses might terminate flow ( if `` call-next-method '' is not done ) , just as the way a decorator can apparently work . So is this the same as a decorator , or different ? In an around method , you 're passing in the same arguments , but to a decorator , you 're passing in the `` function '' in a strict definition which gets augmented . But is the outcome the same ? Thanks much ! Maybe somebody could show closes approximation to the above in Python.done calling next method.So the issue is not about implementing the CLOS methods in Python , but showing how close Python gets to that system in a pythonic way . Or showing how Python is actually better than that.This is more of the kind of example I was thinking of : If an instance of triangle has visible=false , then the render : around will not call the triangle 's primary method.In other words the calling chain of the render method is ( a ) renderable : around , ( b ) triangle primary , ( c ) finish renderable : around . If triangle had an : after method , it would be called after primary , and then the around method would finish up.I understand the difficulties of using inheritance versus considering newer design patterns but here I 'm trying to bridge my CLOS knowledge . If there 's a design pattern that matches decorators ( more accurately than the `` decorator '' design pattern ) , that would be great to understand also.ConclusionsI 'm getting the hang of decorators . But I wanted to present where I 'm at with trying to emulate the CLOS method traversal . Everybody inspired me to try it since I 've got the book and I remember it pretty well . Thanks all for all the great suggestions , they 're all a piece of the puzzle . In terms of implementing the actual structure in a single decorator , Will got close and that 's what worked for moving it forward with dynamic method finding ( see below ) . I 've created a single decorator that does what I 'm looking for and can operate on any class . I 'm sure it could be cleaner and there 's a problem that it only looks up one superclass and it 's doing around methods weirdly , but it does work.Here is the output so you can see what I 'm trying to do . I hope that creates some understand of the CLOS calling and also sparks ideas on how to improve that decorator , or how to lambast me for even trying to do it . : - )"
"When calling a test suite in the Python unittest framework , it is possible to give a -v for higher level of verbosity , like : How can a test case get access to the verbosity level ?"
I am trying to send an image from my local computer to a computer in the cloud using asyncio with TCP protocol . Sometimes I get the entire image being sent and sometimes only part of the image gets sent.client codeserver code : I did n't give the ip address and port number on purpose but it should n't matter.Here is the output : server outputclient outputI am using Python 3.6.I do n't know if I am supposed to have a checking mechanism or send data in chunks ? I would assume all that would happen automatically under the read function.I adjusted the code from this website : http : //asyncio.readthedocs.io/en/latest/tcp_echo.html
"If I ask SymPy to row-reduce the singular matrixthen it returns the identity matrixwhich it should n't do , since the matrix is singular . Why is SymPy giving me the wrong answer and how can I get it to give me the right answer ? I know SymPy knows the matrix is singular , because when I ask for A3.inv ( ) , it givesFurthermore , when I remove lamb from the matrix ( equivalent to setting lamb = 0 ) , SymPy gives the correct answer : which leads me to believe that this problem only happens with more than one variable.EDIT : Interestingly , I just got the correct answer when I pass rref ( ) the argument `` simplify=True '' . I still have no idea why that is though ."
"I 've done a research over similar questions on this subject , but did n't find a duplicate.It is stated that an object is iterable if it implements __iter__ protocol . iterator.__iter__ ( ) : Return the iterator object itself . This is required to allow both containers and iterators to be used with the for and in statements . iterator.__next__ ( ) : Return the next item from the container . If there are no further items , raise the StopIteration exception.From my understanding this applies to all iterator objects . I 've encountered a code that implements a binary-tree container . The container only has __iter__ and so does the node objects resides in it.The __iter__ implementation of the Node objects returns a generator . It yields objects and seems to do all the logic , without an implementation of __next__.How this code actually works ? It seems to function just as a regular iterator , but this one has no __next__ . Ofcourse if I manually do iter ( obj ) and then next ( obj ) it works . Here is the code snippet : An example of running code"
"I am trying to write a convenience function based on the multiprocessing library , that takes any function and argument , and runs that function using multiple processes . I have the following file `` MultiProcFunctions.py '' that I am importing : Here is the code I run : I get the following error : PicklingError : Ca n't pickle < function g at 0x01BD83B0 > : it 's not found as MultiProcFunctions.gIf I use the following definition for g instead , everything is fine : Why is are the two definitions of g different , and is there any thing I can do to get the original g definition to `` work '' ?"
"How can you tell whether python has been started with the -i flag ? According to the docs , you can check the PYTHONINSPECT variable in os.environ , which is the equivalent of -i . But apparently it does n't work the same way.Works : Does n't work : The reason I ask is because I have a script that calls sys.exit ( -1 ) if certain conditions fail . This is good , but sometimes I want to manually debug it using -i. I suppose I can just learn to use `` PYTHONINSPECT=1 python '' instead of `` python -i '' , but it would be nice if there were a universal way of doing this ."
What is the difference between numpy.rint and numpy.round/numpy.around ? They both seem to perform the same function :
"I would like to subclass list and trigger an event ( data checking ) every time any change happens to the data . Here is an example subclass : Here , I am overriding methods __init__ , __setitem__ and __append__ to perform the check if data changes . I think this approach is undesirable , so my question is : Is there a possibilty of triggering data checking automatically if any kind of mutation happens to the underlying data structure ?"
How do I sort a list of strings by key=len first then by key=str ? I 've tried the following but it 's not giving me the desired sort : I need to get :
"I have a NumPy array : I want to create a new array that contains powers of arr up to a power order : My current approach uses for loops : Is there a faster ( i.e . vectorized ) approach to creating powers of an array ? BenchmarkingThanks to @ hpaulj and @ Divakar and @ Paul Panzer for the answers . I benchmarked the loop-based and broadcasting-based operations on the following test arrays.The loop_based function is : The broadcast_based function using hstack is : The broadcast_based function using reshape is : The broadcast_based function using cumulative product cumprod and reshape : On Jupyter notebook , I used the timeit command and got these results : Small arrays ( 2x2 ) : Large arrays ( 100x100 ) : Conclusions : It seems that the broadcast based approach using reshape is faster for smaller arrays . However , for large arrays , the cumprod approach scales better and is faster ."
"Is there a way to express an symbolic expression involving vectors and operations on them without evaluating them ? This will evaluate the expression to : But instead I would like it to evaluate to something like this , while still being able to substitute the symbolic Vectors later on ."
"I 'm trying set ships on a Battleship board . The function in the code below should do this with a number of ships and an board ( an array of arrays ) as arguments . When I run this in Terminal , I get : What does [ ... ] mean ? How to replace [ ... ] with a random integer from 0 to 2 in this output ?"
"Forgive me if I have the incorrect terminology ; perhaps just getting the `` right '' words to describe what I want is enough for me to find the answer on my own.I am working on a parser for ODL ( Object Description Language ) , an arcane language that as far as I can tell is now used only by NASA PDS ( Planetary Data Systems ; it 's how NASA makes its data available to the public ) . Fortunately , PDS is finally moving to XML , but I still have to write software for a mission that fell just before the cutoff.ODL defines objects in something like the following manner : I am attempting to write a parser with pyparsing , and I was doing fine right up until I came to the above construction.I have to create some rule that is able to ensure that the right-hand-value of the OBJECT line is identical to the RHV of END_OBJECT . But I ca n't seem to put that into a pyparsing rule . I can ensure that both are syntactically valid values , but I ca n't go the extra step and ensure that the values are identical.Am I correct in my intuition that this is a context-sensitive grammar ? Is that the phrase I should be using to describe this problem ? Whatever kind of grammar this is in the theoretical sense , is pyparsing able to handle this kind of construction ? If pyparsing is not able to handle it , is there another Python tool capable of doing so ? How about ply ( the Python implementation of lex/yacc ) ?"
"I would like to test the following codeI thought about using mock from the standard library to check if the __call__ method calls the required arguments . The problem I face is that if I create a Mock object for a bound method , it does not have a __self__ or __func__ attribute . So I tried the following code : It works , but I feel like I 'm not testing properly . I 'm using too much knowledge on how the class WeakBoundMethod is working , instead of testing the actual result.Is there a better way to mock a bound method ? Should I make a Dummy class with a dummy method instead ?"
"I 'm parsing the JSON from the web api , and Python seems to shuffle the keys when I iterate over them.Original JSON on screenshot ( it 's right original ordering . No , it 's not just alphabetically sorted ) : My code : And output : I 've also tried to do this via urllib & json.loads ( ) — it gives the same result.How can I can achieve the original ordering ?"
"I 'm a Ruby/Rails developer now working at a Python/Django shop . I 've started to warm up to Python , however , I 'm still struggling to find Django comparable to Rails in certain aspects I find important . A lot of my current and future work will focus on making AJAX requests to our API . As a Rails developer , I 'd have used unobtrusive javascript and in particular on form submissions added a data-remote tag , as shown below.I 'd then write a method in the controller to handle the request and would have written a JavaScript/jQuery function using event delegation in a JS file located in the /assets/js directory to handle the response on the client-side . I assumed coming over to Django there would be a similar way of implementing this sort of functionality.What I guess I 'm really trying to say is I assumed Django would offer similar `` magic '' to Rails in terms of not having to write out jQuery AJAX functions every time I wanted to make an AJAX request . I wrote a rough comparison ( very rough ) of how I 'd write both of these out . I 'm looking to learn if this is an incorrect approach to what I would do in Rails in Django . I know StackOverflow is n't meant for opinions , but I think breaking principles that apply no matter what language/framework you 're using , i.e . DRYing up code by not writing out AJAX functions over and over , is n't really going against an opinion , its more like breaking an accepted rule.My current approach to working with AJAX requests in Django feels wrong , or maybe I 'm just used to the `` magic '' Rails offers via the data-remote= '' true '' attribute . Would love some guidance on the topic to help me determine a solid approach , thanks . RAILSviews/some_controller/form.html.erbassets/javascripts/some_model.jscontrollers/some_controller.rbDJANGOsome_app/templates/form.htmlsome_app/static/assets/js/some_app.js"
"In the python docs ( yeah , I have this thing with the docs ) it says that : User-defined classes have __cmp__ ( ) and __hash__ ( ) methods by default ; with them , all objects compare unequal ( except with themselves ) and x.__hash__ ( ) returns id ( x ) .But the following code shows another thing : So where is __cmp__ or what am I missing ?"
"So I have this python thing that needs to process a file.First it was : And I would simply run it with python script.py file.csv.Then it grew and became : ( There is an empty __init__.pyin every directory ) But now my_service.py would like to use string_util.py and it 's so damn not straightforward how to do this nicely.I would like to do from ..util import string_util in my_service.py ( which is imported into script.py with from services import my_service ) , but that does not work with python script.py since my_service 's __name__ is then only services.my_service ( and I get the Attempted relative import beyond toplevel package ) I can do cd .. and python -m my_project.script , but that seems so unnatural and would be really bad to put it in the README for the instructions how to run this.Right now I 'm solving it with the ugly sys.path.append ( ) hack . What other options do I have ?"
"I 'm trying Heroku with Python , I ran the `` hello word '' example with Flask successfully.I now want to deploy a very basic application , using sqlite3 and Flask , and I know the application was working . But I have trouble getting it to work , and I suspect the problem is with sqlite.When I started the Python shell that Heroku provides , here the import error log : Do I need to add something to the requirements.txt , the file used for dependencies ? It only contains Flask==0.8 so far . Import datetime in examples works as expected . I looked with heroku logs and this message appears as well , without any other important messages.Do I have any way to use some sqlite3 on Heroku ? Thanks for help ."
"I have a numpy array which holds 4-dimensional vectors which have the following format ( x , y , z , w ) The size of the array is 4 x N. Now , the data I have is where I have ( x , y , z ) spatial locations and w holds some particular measurement at this location . Now , there could be multiple measurements associated with an ( x , y , z ) position ( measured as floats ) .What I would like to do is filter the array , so that I get a new array where I get the maximum measurement corresponding with each ( x , y , z ) position.So if my data is like : where w1 is greater than w2 and w3 , the filtered data would be : So more concretely , say I have data like : This should return"
"I 'm struggling to figure out how to remove rows from a pandas dataframe in which two specified columns have the same value across a row . For example , in the below examples I would like to remove the rows which have duplicate values in the columns 2 and 4 . For example : Would turn into : Any help is appreciated , thank you !"
"I 've been learning Rust for about two weeks now and today , I got into its FFI . I used Python to play with Rust , using ctypes and libc . I passed integers , strings and even learned to pass a list of integers ( thanks to this wonderful answer ) .Then , I tried to pass a list of strings ( following the reasoning behind the that answer ) , but I failed , as I could n't get a lead on it . In Python , I have something like this to pass the array of strings.In Rust , I thought that there should be something ( like a STRING_RECEIVER ) to collect the incoming strings , but I ca n't find one.Is there any alternative way to achieve this ?"
"Suppose I have a pandas dataframe and a function I 'd like to apply to each row . I can call df.apply ( apply_fn , axis=1 ) , which should take time linear in the size of df . Or I can split df and use pool.map to call my function on each piece , and then concatenate the results.I was expecting the speedup factor from using pool.map to be roughly equal to the number of processes in the pool ( new_execution_time = original_execution_time/N if using N processors -- and that 's assuming zero overhead ) .Instead , in this toy example , time falls to around 2 % ( 0.005272 / 0.230757 ) when using 4 processors . I was expecting 25 % at best . What is going on and what am I not understanding ? I saved the code above and ran it using python3 my_filename.py.PS I realize that in this toy example new_df can be created in a much more straightforward way , without using apply . I 'm interested in applying similar code with a more complex apply_fn that does n't just add columns ."
"I have the following python test code : If I run this code using a 32 bit python or a 64 bit one I obtain the following output ( as expected ) : My purpose is to build two standalone executable ( 32bit and 64bit ) : in order to achieve that I 'm using pyinstaller and the following command ( test.py is the name of the file containing the python code shown above ) If I run the 64 bit exe I obtain the following output ( as expected ) : Instead , if I run the 32 bit exe I obtain the following output : Does anyone know what is going on ? Thanks , Daniele"
"Currently I 'm working on a Flask project and need to make some tests.The test I 'm struggling is about Flask Sessions.I have this view : And have this test : My currently conftest.py is : However , when I execute the test , I 'm getting a 302 status code instead of the expected 200 status code.So my question is how I can pass properly the session value ? OBS : Running normally the application the if statement for session is working properly ."
"I 'm currently working with a big image dataset ( ~60GB ) to train a CNN ( Keras/Tensorflow ) for a simple classification task.The images are video frames , and thus highly correlated in time , so I shuffled the data already once when generating the huge .hdf5 file ... To feed the data into the CNN without having to load the whole set at once into memory I wrote a simple batch generator ( see code below ) .Now my question : Usually it is recommended to shuffle the data after each training epoch right ? ( for SGD convergence reasons ? ) But to do so I 'd have to load the whole dataset after each epoch and shuffle it , which is exactly what I wanted to avoid using the batch generator ... So : Is it really that important to shuffle the dataset after each epoch and if yes how could I do that as efficiently as possible ? Here is the current code of my batch generator :"
I 'm trying to access Google Analytics API using the code provided by their documentation : https : //developers.google.com/analytics/solutions/articles/hello-analytics-apiBut the problem is that this code opens a browser and asks a user to allow access to analytics service . Does anyone know how to access Google Analytics API ( =obtain that token ) without oauth2 ?
"Using tf.maximum with negative inf inputs as follows : gives the expected result -infHowever , tf.reduce_max , on the same inputs : gives : -3.40282e+38 which is the min float32.For positive infinity inputs , both functions result in inf . Is this a bug ?"
"So I think I basically understand how floating-point works and why we ca n't have `` precise '' results for some operations.I got confused by this SO-question , where @ MikeMüller suggests rounding.My understanding is the following . If we write decimal places it would look like this:1000 100 10 1 . 1/10 1/100 1/1000It would look like this in binary:8 4 2 1 . 1/2 1/4 1/8So we store 0.5 or 0.25 or 0.125 precisely in memory but not e.g . 0.3So why does python output the following : I think it should output Where am I wrong ? My Question is NOT a duplicate of Is floating point math broken ? because OP does not understand why 0.1+0.2 ! = 0.3 . This is not topic of my question !"
"ProblemWhile trying to generate sphinx documentation for a re-usable Django application I struck upon the following snafoo . When sphinx parses the model.py code it is thwarted by the code therein trying to access the Django project settings . As this is a standalone/reusable application there is not a primary project providing these settings i.e . there is no ROOT/PROJECT/PROJECT/settings.py file . SetupIn the interest of clarity here is what I 've done . Traverse to what would be the usual project folder cd ROOT/PROJECT and create an application django-admin startapp APPLICATION which produces the following structureNote : There are no /ROOT/PROJECT/PROJECT/*.py files because I did not navigate to the root folder cd root and create a project using django-admin createproject as one normally might . Next one creates the sphinx documentation spinx-quickstart docs producing the following additional structure.That is the docs are built next to the APPLICATION.QuestionWhat do I place within conf.py to properly load the application without there being a settings.py file ? HomeworkIn trying to resolve this I have perused a number of SO Questions , Blogs and the Django Docs and not found a succinct solution . As this has been asked a few times before on SO I 'd like to motivate that it not be closed as a duplicate , if the answer in the proposed duplicate uses one of these snippets as it 's solution.Fails with AppRegistryNotReadySimilar to the first method of failure i.e . emits AppRegistryNotReadyFails with ImproperlyConfiguredThere is a really old solution mentioning the deprecated setup_environThis is also a favourite answer but fails if there is no settings.py file.I 've also made this question rather verbose as the related questions on SO are rather terse and not especially helpful . If it 's any help I 'm using Django 1.10.UpdateI have since found that if one imports their setup function from setuptools versus distutils.core that one may invoke the setup script to compile their documentation as in python setup.py build_sphinx -b BUILDER It 's probably best to re-ask this when invoking setup.py over docs/conf.py via either the make.bat or the MakeFile provided by Sphinx.I suspect the outcome would be similar though , namely include the provided answers within docs/conf.py or alternatively within setup.py , both must be called within the same Python session after all ."
"I 'm facing a strange issue , using Django Haystack and ElasticSearch , preventing me from indexing content.ElasticSearch is properly running on the machine : But when I try to build my index using Haystack : I get a ProtocolError ( ( 'Connection aborted . ' , error ( 111 , 'Connection refused ' ) ) ) error : The strange part is : by this time ( after the error ) , elasticsearch stays unreachable , even the way I did earlier : If I want it to be up and running again , I need to restart it : This occurs on a Raspbian distribution . On my MacOX development machine , everything is OK , indexing works well.Have you got some ideas ? Memory informations $ free $ cat /proc/meminfo $ vmstat -sAccessing ES via raw PythonI also tried to access ES via raw Python ( without Haystack ) . And it seems to work : So OK , it does n't find anything ( because nothing has been indexed , hey ! ) , but it seems to be able to query the engine.ES logsThe default log file doens't tell me anything useful : So I tried to get more informations , editing /etc/elasticsearch/logging.yml configuration file , and asking TRACE level ( es.logger.level : TRACE ) . Everything still looks OK for ES in its log file , even after it crashes ! Extended logs ( last lines , copied just after ES crash ) : System logsI also tried to explore system logs , to get informations about ES crash ( as it does n't seem to log anything by itself about this point… ) ./var/log/daemon.log : /var/log/messages : ARGH ! One step further , I changed StandardOutput=null to StandardOutput=journal in /usr/lib/systemd/system/elasticsearch.service , executed systemctl daemon-reload and then journalctl -u elasticsearch.service -f. This way , I got more details on the error :"
"I 've recently been trying to write a function in C++ that converts a vector of doubles into a vector of strings . I want to run this from a python interpreter so I 'm using Pybind11 to interface C++ and Python . This is what I have so far , Now this compiles into a shared library , using the follow command from command line : where the ../include is where the pybind11 includes are . After compilation start up python and use , And I get the following error , `` Incompatible function arguments . The following argument types are supported '' . Where it then gives me a list of possible types , this is strange because I 'm just trying to use a vector as my function argument which according to the documentation for pybind11 this is a supported data type : http : //pybind11.readthedocs.io/en/latest/basics.html # supported-data-typesIs it that I have a vector of vectors ( a 2d vector ) and that is unsupported ?"
"Following is the simple database model i have : Now in the url handler , i send all the notes to the template as follows : In the template i am using jinja2 templating engine , i want the id of each of the notes to be printed , so that i can embed an edit link , somewhat like this : But the trouble is , i dont see anything printed , in place of note.key.idAs per the docs over here key class represents a unique key for a database entity and this has a method id , its a numeric value . for a single note from the collection of notes i want the id of the note . If i use the django templating engine i get the values { { notes.key.id } } printed , but with jinja2 i dont see it . How can i do this ?"
I am trying to group by hospital staff working hours bi monthly . I have raw data on daily basis which look like below . How I want to group by is . I am trying to do the same with grouper and frequency in pandas something as below . I also tried resamplingBut this is giving data of 15 days interval not like 1 to 15 and 15 to 31 . Please let me know what I am doing wrong here .
"I am using Flask to build a very small one-page dynamic website . I want to share a list of variables and their values across functions without using a class.I looked into Flask 's View class , however I feel as if my application is n't large enough nor complex enough to implement a class-based version of my project using Flask . If I 'm stating correctly I would also lose the ability to use the route decorator and would have to use its surrogate function add_url_rule.Which would also force me to refactor my code into something such as this : For variable sharing I thought of two techniques ."
"I am attempting to connect to Neo4j but I keep getting this error . I tried but I get this error when I try to connect SecurityError : Failed to establish secure connection to 'EOF occurred in violation of protocol ( _ssl.c:841 ) ' I can connect to the browser when I type http : //localhost:7474/browser/Here is the full error log : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - SSLEOFError Traceback ( most recent call last ) ~\AppData\Roaming\Python\Python36\site-packages\neobolt\direct.py in _secure ( s , host , ssl_context , **config ) 853 try : -- > 854 s = ssl_context.wrap_socket ( s , server_hostname=host if HAS_SNI and host else None ) 855 except SSLError as cause : c : \program files\python36\lib\ssl.py in wrap_socket ( self , sock , server_side , do_handshake_on_connect , suppress_ragged_eofs , server_hostname , session ) 406 server_hostname=server_hostname , -- > 407 _context=self , _session=session ) 408 c : \program files\python36\lib\ssl.py in init ( self , sock , keyfile , certfile , server_side , cert_reqs , ssl_version , ca_certs , do_handshake_on_connect , family , type , proto , fileno , suppress_ragged_eofs , npn_protocols , ciphers , server_hostname , _context , _session ) 813 raise ValueError ( `` do_handshake_on_connect should not be specified for non-blocking sockets '' ) -- > 814 self.do_handshake ( ) 815 c : \program files\python36\lib\ssl.py in do_handshake ( self , block ) 1067 self.settimeout ( None ) - > 1068 self._sslobj.do_handshake ( ) 1069 finally : c : \program files\python36\lib\ssl.py in do_handshake ( self ) 688 `` '' '' Start the SSL/TLS handshake . '' '' '' -- > 689 self._sslobj.do_handshake ( ) 690 if self.context.check_hostname : SSLEOFError : EOF occurred in violation of protocol ( _ssl.c:841 ) The above exception was the direct cause of the following exception : SecurityError Traceback ( most recent call last ) in 1 -- -- > 2 driver = GraphDatabase.driver ( uri= '' bolt : //localhost:7687 '' , auth= ( `` neo4j '' , `` 12345 '' ) ) ~\AppData\Roaming\Python\Python36\site-packages\neo4j__init__.py in driver ( cls , uri , **config ) 118 : class : .Driver subclass instance directly . 119 `` '' '' -- > 120 return Driver ( uri , **config ) 121 122 ~\AppData\Roaming\Python\Python36\site-packages\neo4j__init__.py in new ( cls , uri , **config ) 159 for subclass in Driver.subclasses ( ) : 160 if parsed_scheme in subclass.uri_schemes : -- > 161 return subclass ( uri , **config ) 162 raise ValueError ( `` URI scheme % r not supported '' % parsed.scheme ) 163 ~\AppData\Roaming\Python\Python36\site-packages\neo4j__init__.py in new ( cls , uri , **config ) 233 234 pool = ConnectionPool ( connector , instance.address , **config ) -- > 235 pool.release ( pool.acquire ( ) ) 236 instance._pool = pool 237 instance._max_retry_time = config.get ( `` max_retry_time '' , default_config [ `` max_retry_time '' ] ) ~\AppData\Roaming\Python\Python36\site-packages\neobolt\direct.py in acquire ( self , access_mode ) 713 714 def acquire ( self , access_mode=None ) : -- > 715 return self.acquire_direct ( self.address ) 716 717 ~\AppData\Roaming\Python\Python36\site-packages\neobolt\direct.py in acquire_direct ( self , address ) 606 if can_create_new_connection : 607 try : -- > 608 connection = self.connector ( address , error_handler=self.connection_error_handler ) 609 except ServiceUnavailable : 610 self.remove ( address ) ~\AppData\Roaming\Python\Python36\site-packages\neo4j__init__.py in connector ( address , **kwargs ) 230 231 def connector ( address , **kwargs ) : -- > 232 return connect ( address , **dict ( config , **kwargs ) ) 233 234 pool = ConnectionPool ( connector , instance.address , **config ) ~\AppData\Roaming\Python\Python36\site-packages\neobolt\direct.py in connect ( address , **config ) 970 raise ServiceUnavailable ( `` Failed to resolve addresses for % s '' % address ) 971 else : -- > 972 raise last_error ~\AppData\Roaming\Python\Python36\site-packages\neobolt\direct.py in connect ( address , **config ) 961 host = address [ 0 ] 962 s = _connect ( resolved_address , **config ) -- > 963 s , der_encoded_server_certificate = _secure ( s , host , security_plan.ssl_context , **config ) 964 connection = _handshake ( s , address , der_encoded_server_certificate , **config ) 965 except Exception as error : ~\AppData\Roaming\Python\Python36\site-packages\neobolt\direct.py in _secure ( s , host , ssl_context , **config ) 857 error = SecurityError ( `` Failed to establish secure connection to { ! r } '' .format ( cause.args [ 1 ] ) ) 858 error.cause = cause -- > 859 raise error 860 else : 861 # Check that the server provides a certificate SecurityError : Failed to establish secure connection to 'EOF occurred in violation of protocol ( _ssl.c:841 ) '"
"I 'm trying to map a dataset to a blank CSV file with different headers , so I 'm essentially trying to map data from one CSV file which has different headers to a new CSV with different amount of headers and called different things , the reason this question is different is since the column names are n't the same but there are no overlapping columns either . And I ca n't overwrite the data file with new headers since the data file has other columns with irrelevant data , I 'm certain I 'm overcomplicating this.I 've seen this example code but how do I change this since this example is using a common header to join the data.Sample Dataa.csv ( blank format file , the format must match this file ) : b.csv : Expected output file :"
"After upgrading to Flask 0.10 , I get this error : What the self.save_session ( ctx.session , response ) for flask/app.py line 1693 gives is this : flask.sessions.SecureCookieSession ( { '_id ' : 'iB\rOU # \xf7BO\x08^\xa6\xd1 ) v\xad ' , '_flashes ' : [ ( 'message ' , 'Please log in to access this page . ' ) ] } ) So , if I read this correctly , the session gives an id that flask tries to unicode ( and fails at ) . I have no clue on how to correct this ( emptying my browsers cache to hope for a new session did n't help either ) .Can anyone give me any suggestions on what I should do ? Kind regards , Carstedit : some extra info . My setup uses flask , flask_mail , flask.ext.mongoengine and flask.ext.security ; i use MongoEngine : db = flask.ext.mongoengine.MongoEngine ( app ) and flask-security : user_datastore = flask.ext.security.MongoEngineUserDatastore ( db , User , Role ) security = flask.ext.security.Security ( app , user_datastore ) Edit : it appears to be a double question : Flask Login : TypeError : decoding Unicode is not supported ( had n't seen it , sorry )"
"I am developing a web and am using tornado server with motor . I use the generator to find the document from the collection . When the code is executed I get an error saying @ gen is not defined . Motor , tornado.ioloop and tornado.web has been imported.One more thing is the web server could not be closed using Ctrl+C . I have to close the terminal every time and then start from beginning . Is there a way to stop the service in the terminal itself ."
"Got a bit problem with ZeroMQ socket on trying to .bind ( ) on 0.0.0.0:5555 address , when tried to run it in Docker container via a Rancher Cattle.Every time I try to run it , I 'm getting the same error : zmq.error.ZMQError : Address already in use.Tried to do EXPOSE 5555 and EXPOSE 5555/tcp in my Dockerfile , but it did not help me.Here is a part of my code : Maybe somebody had the same problem . How to solve it ?"
"Attempted to do simple movement in tkinter : Where self.momentum is an array containing 2 integers : one for the x movement , and another for the y movement . However , the actual movement of the rectangle is really slow ( about 5 movements per second ) , with the self.window.master.after ( ) time not seeming to have an effect.Previously on another tkinter project I had managed to get really responsive tkinter movement , so I 'm just wondering if there is a way I can minimize that movement updating time in this case , by either using a different style of OOP , or just different code altogether.UPDATE : Turns out the time in the .after ( ) method does matter , and it actually stacks onto the real time of the method . After using timeit to time calling the method , I got this output : So I guess the real question is : Why is that .after ( ) method taking so long ? UPDATE 2 : Tested on multiple computers , movement is still slow on any platform ."
Is there a way to use refresh_from_db and automatically propagate it on ForeignKey ? model.py : shell : The value returned at this point is still the initial one . Is there an option to make refresh_from_db automatically cascade on elements referenced by ForeignKey ? It is possible to do it by explicitly using refresh_from_db on the referenced item - shown below . But I would like to only have to do refresh_from_db on container_product itself.shell ( continued ) :
"I have installed pytz ( v2013.8 , but it happens in 2013.b , 2011k ) in a virtualenv . The first call to takes about 4 seconds . In a regular environment this is essentially instantaneous . Does anyone have a trick to get this to run faster ?"
"Numpy allows matrices of different sizes to be added/multiplied/divided provided certain broadcasting rules are followed . Also , creation of temporary arrays is a major speed impediment to numpy . The following timit results surprise me ... what is going on ? I thought that f_dot would be slower since it had to create the temporary array denominator , and I assumed that this step was skipped by f_no_dot . I should note that these times scale linearly ( with array size , up to length 1 billion ) for f_no_dot , and slightly worse than linear for f_dot ."
"I have created a video player in Gtk3 using Gstreamer in Python3 . It works except when I add a GtkMenuBar ( place 2 ) . It will then either show a black screen , or fail with an exception . The exception references the XInitThreads , which I am calling ( Place 1 ) ( I took this from the pitivi project ) but this does not seem to make a diffrence.Question : How do I make this work ? Other things I would like to know : Why would the menubar break this ? This will clearly break on anything not X , is there some prebuilt component the abstracts this logic and is crossplatform that I am missing ? System : python3Gtk3Ubuntu 16.04The exception : The code ( in as small a form as possible to demonstrate the concept ) :"
"Given a list of unsorted numbers , I want to find the smallest number larger than N ( if any ) .In C # , I 'd do something like this ( checks omitted ) : What 's a short , READABLE way to do this in Python ?"
"I use : MongoDB 1.6.5Pymongo 1.9Python 2.6.6I have 3 types of daemons . 1st load data from web , 2nd analyze it and save result , and 3rd group result . All of them working with Mongodb.At some time 3rd daemon throws many exceptions like this ( mostly when there are big amount of data in DB ) : Can anyone tell what cause this exeption and how to fix this.Thanks ."
"I 'm trying to submit a POST method form using lxml and I 'm getting a TypeError . This is a minimal example that raises this Error : I 've found the exact error elsewhere online , but I have n't seen it generated from inside lxml like this . Does anyone know if this is a bug , or expected behaviour and how to work around it ?"
"I have a simple sklearn class I would like to use as part of an sklearn pipeline . This class just takes a pandas dataframe X_DF and a categorical column name , and calls pd.get_dummies to return the dataframe with the column turned into a matrix of dummy variables ... Now using this transformer on it 's own to fit/transform , I get output as expected . For some toy data as below : ... my dummy encoder produces the correct output : However , when I call the same transformer from an sklearn pipeline as defined below : All 's well until I fit the pipeline , at which point I get an error from the dummy encoder : In [ 101 ] : cv_model_search.fit ( X , y=y ) Fitting 3 folds for each of 4 candidates , totalling 12 fits None None None None [ CV ] dummy_vars__column_to_dummy=category_1 , clf__penalty=l1 ... ... ... Traceback ( most recent call last ) : File `` '' , line 1 , in cv_model_search.fit ( X , y=y ) File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/sklearn/model_selection/_search.py '' , line 638 , in fit cv.split ( X , y , groups ) ) ) File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py '' , line 779 , in call while self.dispatch_one_batch ( iterator ) : File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py '' , line 625 , in dispatch_one_batch self._dispatch ( tasks ) File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py '' , line 588 , in _dispatch job = self._backend.apply_async ( batch , callback=cb ) File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.py '' , line 111 , in apply_async result = ImmediateResult ( func ) File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.py '' , line 332 , in init self.results = batch ( ) File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py '' , line 131 , in call return [ func ( *args , **kwargs ) for func , args , kwargs in self.items ] File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/sklearn/model_selection/_validation.py '' , line 437 , in _fit_and_score estimator.fit ( X_train , y_train , **fit_params ) File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/sklearn/pipeline.py '' , line 257 , in fit Xt , fit_params = self._fit ( X , y , **fit_params ) File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/sklearn/pipeline.py '' , line 222 , in _fit **fit_params_steps [ name ] ) File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/sklearn/externals/joblib/memory.py '' , line 362 , in call return self.func ( *args , **kwargs ) File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/sklearn/pipeline.py '' , line 589 , in _fit_transform_one res = transformer.fit_transform ( X , y , **fit_params ) File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/sklearn/base.py '' , line 521 , in fit_transform return self.fit ( X , y , **fit_params ) .transform ( X ) File `` '' , line 21 , in transform dummy_matrix = pd.get_dummies ( X_DF [ column ] , prefix=column ) File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/pandas/core/frame.py '' , line 1964 , in getitem return self._getitem_column ( key ) File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/pandas/core/frame.py '' , line 1971 , in _getitem_column return self._get_item_cache ( key ) File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/pandas/core/generic.py '' , line 1645 , in _get_item_cache values = self._data.get ( item ) File `` /home/max/anaconda3/envs/remine/lib/python2.7/site-packages/pandas/core/internals.py '' , line 3599 , in get raise ValueError ( `` can not label index with a null key '' ) ValueError : can not label index with a null key"
"In this following code , I only see , an image is read and written again . But how do the image pixel values get changed so drastically ? Apparently , converting the PIL image object to numpy array causes this but do n't know why . I have read the doc for PIL images but did n't see any reasonable explanation for this to happen.Input image is , Here is the output , Note : The value at the red area in the input image is [ 128,0,0 ] and in the output image it 's [ 1,1,1 ] .The actual source of the code is here.Edit : As @ taras made it clear in his comment , Basically , palette is a list of 3 * 256 values in form 256 red values , 256 green values and 256 blue values . Your pil_image is an array of greyscale pixels each taking a single value in 0..255 range . When using ' P ' mode the pixel value k is mapped to a color ( pallette [ k ] , palette [ 256 + k ] , palette [ 2*256 + k ] ) . When using ' L ' mode the color is simply k or ( k , k , k ) in RGBThe segmentation image annotations use a unique color for each object type . So we do n't need the actual color palette for the visualization , we get rid of the unnecessary color palette ."
How to use StringIO with imghdr to determine if valid imageI loaded a image into
"I have a square matrix with > 1,000 rows & columns . In many fields at the `` border '' there is nan , for example : Now I want to eliminate all rows and columns where I only have nan . This would be the 1. and 2. row and the last column . But I also want to receive a square matrix , so the number of the eliminated rows must be equal to the number of eliminated columns . In this example , I want to get this : I 'm sure I could solve this with a loop : check every column & row if there is only nan inside and in the end I use numpy.delete to delete the rows & columns I found ( but only the minimal number , because of getting a square ) . But I hope anyone can help me with a better solution or a good library ."
"In Pylons 1.0 I could go into config/routing.py and addto route m.mydomain.com to a different controller , but still use the same app . Can I do the same in Pyramid ?"
"I have a structure something like this : This does n't work because the original foobar function ca n't handle self being passed to it -- it was n't part of any class or object to begin with . Python wo n't let me add the @ staticmethod decorator either.I have no control over the definition of foobar , and I may have to override the value of foo in subclasses . How can I call foo , without passing the object calling it ?"
"I am attempting to convert a collection of Flask apps to a single app with several Blueprints.In one of my apps , I have a task that runs periodically in the background , not related to a request . It looks something like this : When I convert this app to a Blueprint , I lose access to the app object and I ca n't figure out how to create an application context when I need one . This is what I tried : The error I get is : So , how can I get the application context in a blueprint but outside a request ?"
I would like to know if I can display a view inside another view with django.This is what I tried to do : The output looks like what I was expecting but I have had to replace the request value with an empty string . Is that fine or is there a better way to do it ?
"I 'm using pgmagick to generate a circular thumbnail . I 'm using a process similar to the one discussed here , which does indeed produce a nice circular thumbnail for me . However , I need a white border around the radius of the circle . My initial approach was to create a new image of a slightly larger white circle with a transparent background and composite the thumbnail over that , letting the white circle `` peak out '' from under the thumbnail and create a border effect . Here 's the pgmagick code I used to achieve that : This 'works ' , but the surrounding white border is fairly distorted with choppy edges -- not production ready . If I take drawer.stroke_antialias ( False ) out , it 's even worse . Any ideas on making this border smoother using pgmagick ?"
"Below is a figure I created with matplotlib . The problem is pretty obvious -- the labels overlap and the whole thing is an unreadable mess.I tried calling tight_layout for each subplot , but this crashes my ipython-notebook kernel.What can I do to fix the layout ? Acceptable approaches include fixing the xlabel , ylabel , and title for each subplot , but another ( and perhaps better ) approach would be to have a single xlabel , ylabel and title for the entire figure.Here 's the loop I used to generate the above subplots : Update : Okay , so ImageGrid seems to be the way to go , but my figure is still looking a bit wonky : Here 's the code I used :"
Which is the best pattern to make a query with psycopg2 ? This one :
"In python , if you need a module from a different package you have to import it . Coming from a Java background , that makes sense.What does n't make sense though , is why do I need to use the full name whenever I want to use bar ? If I wanted to use the full name , why do I need to import ? Does n't using the full name immediately describe which module I 'm addressing ? It just seems a little redundant to have from foo import bar when that 's what import foo.bar should be doing . Also a little vague why I had to import when I was going to use the full name ."
"I came across this code : I 'm unsure as to : Why the list comprehension does n't need to catch a StopIteration how does while iters work as I 've tried : and it still prints `` Still True '' in both cases.The author of the code also said because map returns a `` one-shot iterable '' in 3.X and `` as soon as we ’ ve run the list comprehension inside the loop once , iters will be exhausted but still True ( and res will be [ ] ) forever '' . He suggested to use list ( map ( iters , args ) instead if we 're using 3.X.I 'm unsure of how this change actually helps it to work as I thought that even if iterators are at the StopIteration point it still is True ( based on what I tried earlier ) . Edit : The author gave this as an example"
my User class does not inherit from UserMixin but rather defines its own members and functions necessary for flask-login.the login process only works if i set get_id ( ) to return the instance email . if i attempt to do this on the user 's name or actual id i do n't get an error but i notice i am not logged ( i am unable to access url 's that require me to be logged in using @ required_login ) EDIT : at the time asking the question i did not understand get_id ( ) needs to match load_user ( )
"I fell on a weird case . I tried either of the three solutions posted here from Pyson : Increment a python floating point value by the smallest possible amount . All three solutions display a weird behavior when I land on this floating point : 1.15898324042702949299155079643242061138153076171875.Let say I have the following code : For some reason , instead of incrementing b by the smallest amount possible , it is decremented.Why is that ? Here 's some quick results that I got from playing around :"
"I want to make a speaking mouth which moves or emits light or something when a playing wav file emits sound . So I need to detect when a wav file is speaking or when it is in a silence between words . Currently I 'm using a pygame script that I have foundI guess I could make some checking at the while loop to look the sounds output level , or something like that , and then send it to one of the gpio outputs . But I do n't know how to achieve that.Any help would be much appreciated"
I would like to make changes ( and possibly contribute if its any good ) to a public project on GitHub . I 've forked and cloned the module but Im unclear how to get my program to import the local library instead of the 'official ' installed module.I tried cloning it into my project folder but when I imported it and tried to use it things got weird calmap\calmap.plot ( ) I also tried doing sys.path.append and the folder location . But it seems to still import the official one instead of the forked.I 'm assuming that I could put my program inside the module folder so that module would be found first but I ca n't image thats the 'correct ' way to do it .
"I 'm writing a mapping class which persists to the disk . I am currently allowing only str keys but it would be nice if I could use a couple more types : hopefully up to anything that is hashable ( ie . same requirements as the builtin dict ) , but more reasonable I would accept string , unicode , int , and tuples of these types.To that end I would like to derive a deterministic serialization scheme.Option 1 - Pickling the keyThe first thought I had was to use the pickle ( or cPickle ) module to serialize the key , but I noticed that the output from pickle and cPickle do not match each other : Is there any implementation/protocol combination of pickle which is deterministic for some set of types ( e.g . can only use cPickle with protocol 0 ) ? Option 2 - Repr and ast.literal_evalAnother option is to use repr to dump and ast.literal_eval to load . I have written a function to determine if a given key would survive this process ( it is rather conservative on the types it allows ) : The question for this method is if repr itself is deterministic for the types that I have allowed here . I believe this would not survive the 2/3 version barrier due to the change in str/unicode literals . This also would not work for integers where 2**32 - 1 < x < 2**64 jumping between 32 and 64 bit platforms . Are there any other conditions ( ie . do strings serialize differently under different conditions in the same interpreter ) ? Edit : I 'm just trying to understand the conditions that this breaks down , not necessarily overcome them.Option 3 : Custom reprAnother option which is likely overkill is to write my own repr which flattens out the things of repr which I know ( or suspect may be ) a problem . I just wrote an example here : http : //gist.github.com/423945 ( If this all fails miserably then I can store the hash of the key along with the pickle of both the key and value , then iterate across rows that have a matching hash looking for one that unpickles to the expected key , but that really does complicate a few other things and I would rather not do it . Edit : it turns out that the builtin hash is not deterministic across platforms either . Scratch that . ) Any insights ?"
"Consider we have this function under test located in the module.py : And , we have the following test case in the test_module.py : If we run tests with pytest with the enabled `` branch coverage '' with the HTML output reporting : The report is going to claim 100 % branch coverage with all the `` partial '' branches covered : But , we clearly have not covered the else 1 / 0 part at all . Is there a way to improve reporting to see the non-covered parts of the ternary operators ?"
"For specific debugging purposes I 'd like to wrap the del function of an arbitrary object to perform extra tasks like write the last value of the object to a file.Ideally I want to write monkey ( x ) and it should mean that the final value of x is printed when x is deletedNow I figured that del is a class method . So the following is a start : However if I want to monkey specific objects only I suppose I need to dynamically rewrite their class to a new one ? ! Moreover I need to do this anyway , since I can not access del of built-in types ? Anyone knows how to implement that ?"
"I am running python nosetests on a foreign module . The dots mean that a test passed . What does S stand for ? This seems so obvios that it is hardly mentioned , could not find it in the docs , in the man page and not using google . Sorry I am not a python native ."
"I have a dictionary that needs to be sorted . I want to sort the dict based on the values . And as a next step , if two keys have same value , I want to sort based on the lexicographic values of keys.I tried this ."
"I have a similarity matrix between four users . I want to do an agglomerative clustering . the code is like this : the print result of label is like : Does this mean it gives a lists of possible cluster result , we can choose one from them ? like choosing : [ 0,0,2,1 ] . If is wrong , could you tell me how to do the agglomerative algorithm based on similarity ? If it'ss right , the similarity matrix is huge , how can i choose the optimal clustering result from a huge list ? Thanks"
"I am wondering if there is a standard library function in Python which will rearrange the elements of a list like below : It should get one element from beginning of original list , then one from end , then second from beginning and so on . Then rearrange the list.Regards ,"
"For example , I have aI do not want everyone that subclasses BaseHandler and also wants to implement prepare to have to remember to callIs there a way to ensure the superclass method is run even if the subclass also implements prepare ?"
"I 'm trying to integrate some numerical code written in C into a Python library using numpy and ctypes . I 've already got the actual computations working but would now like to report the progress of the intermediate steps of my algorithm to a callback function in my Python code . While I can call the callback function successfully , I 'm not able to retrieve the data in the x array passed to the callback . In the callback , x is an ndpointer object that I ca n't seem to dereference.Current CodeConsider this minimal example : test.h : test.c : test.py : Current OutputAfter compiling and running the script , I get the following output : I 've also tried the subsetting operator x [ 0 : n ] ( TypeError : 'ndpointer_x.value ( returns the pointer as a number ) .Hackish SolutionIf I use the following alternative definition of callback_func : and the following alternative callback function : I get the desired results : My QuestionIs there a more more numpy-ish way of accessing x in the callback ? Rather than subscripting and then converting back to a numpy.array , I would prefer to access the data pointed to by the ndpointer since I would like to limit the amount of copies of x ( and for the sake of elegant code ) I 've uploaded a gist of the whole mini-example if you want to experiment on my code ."
"Why did the creators of python prefer this syntax ( instruction ) over this ( method ) ? It seems to me that del belongs to the same `` cathegory '' as append , remove , find , e.t.c . and therefore should have the same syntax ( be a method ) , but due to some reason the creators of python implemented it as an instruction . Why did they do it ?"
"Assume the following html snippet , from which I would like to extract the values corresponding to the labels 'price ' and 'ships from ' : Which is part of a larger html file . Assume that in some files the 'Ships from ' label is present , sometimes not . I would like to use BeautifulSoup , of a similar approach , to deal with this , because of the variability of the html content . Multiple div and span are present , which makes it hard to select without id or class nameMy thoughts , something like this : However , this returns an empty list ."
"Understanding that the below are not true constants , attempting to follow PEP 8 I 'd like to make a `` constant '' in my @ dataclass in Python 3.7.My code used to be : Are these two equivalent in there handling of the seed ? Is the seed now part of the init/eq/hash ? Is there a preferred style for these constants ?"
"I have a matrix a.shape : ( 80000 , 38 , 38 ) . I want to check and see if there are any duplicates or similar ( 38,38 ) matrices along the first dimension ( in this case , there are 80000 of these matrices ) .I could run through two for loops : But that seems incredibly inefficient . I know there is numpy.unique , but I 'm not sure I understand how it works when you have a set of 2-dimensional matrices.Suggestions for an efficient way to do this ? Is there a way to get broadcasting to find the differences of all of the elements in all the matrices ?"
"I have the file named `` names.txt '' having the following contents : Problem statement : File `` names.txt '' contains some student records in the format - { `` number '' : [ year of birth , `` name rank '' ] } Parse this file and Segregate them according to year and then sort the names according to rank . First segregation and then sorting.Output should be in the format - So the expected output is -First How to store this file content in a dictionary object ? Then Grouping by year & then ordering names by rank ? How to achieve this in Python ? Thanks.."
"I was reading the code for the timeit module and I noticed this segment : This just stores the state of garbage collection ( on or off ) and then turns it off . The function inner executes the statement being timed . It then reverts the garbage collector to its old state.So I 'm curious about what the point of this is . If the code being tested works the garbage collector , then should n't that be reflected in the testing ? What am I missing ?"
"I 'm currently doing a integration with WSDL , and such decided to go with Python using the Zeep library . I 'm trying to model the response with mypy , so that it works with VSCode 's Intellisense , as well as some giving me hints when I 'm doing careless assignments or modifications . But I hit a roadblock when the WSDL responses is in a nested object , and I ca n't figure a way to type-hint it.Sample response from WSDL : I 'm using the following snippet to type-hint : For Attempt 1 , the reason is obvious . But after trying Attempt 2 , I do n't know how to proceed anymore . What am I missing here ? Update : Following @ Avi Kaminetzky 's answer , I tried with following ( playground ) : But I 'm getting error message from mypy :"
"I have a Rust function that returns an array and I want to use this array in Python , it could be a list or numpy.array it does not really matter.My Rust function looks like this : And I am trying to call it in Python like this : What am I doing wrong ? Why is my output not [ 1,2,3,4 ] ? Why are my first two elements right and other two are populated with garbage ? I was not able to find any good documentation on ctypes.ARRAY , so I just went with what looked right , so that is likely the problem ."
"I 'm using YAML as a configuration file format for a Python project.Recently I found Rx to be the only schema validator available for Python and YAML . : -/ Kwalify works with YAML , but it 's only for Ruby and Java . : ( I 've been reading their lacking documentation all day and just ca n't seem to write a valid schema to represent my file structure . Help ? I have the following YAML config file : I 'm failing at representing a nested structure . What I want is for the outer-most item ( cmd , load and echo , in this case ) to be an arbitrary string that in turn contains other items . 'exec ' is a fixed string and required item ; 'aliases ' and 'filter ' are also fixed , but should be optional . Filter in turn has another set of required and optional items . How should I represent this with Rx ? So far I have the following schema ( in YAML ) , which Rx fails to compile : Testing this in IPython gives me this : Which leads me to believe I 'm not specifying `` type '' somewhere . : -SAny ideas ? I 'm pretty tired fighting with this thing ... Is there some other way I can write a schema and use it to validate my configuration files ? Thanks in advance , Ivan"
"Is there any neat way to check is numpy array surrounded by zeros . Example : I know I can iterate it element wise to find out but I wonder is there any nice trick we can use here . The numpy array is of floats , n x m of arbitrary size.Any ideas are welcome ."
"How can I split each value which is between ( ; and ~ ) ? The result will be like CREATEDBY , CREATEDBYNAME , CREATEDBYYOMINAME , ... I have tried the below , but it 's giving the first occurrence.How do I get the list of strings by using the split ?"
"The code is : The output is : But , the desired output is : Could this be done with pprint or is there another way ?"
"I have a column in my database called coordinates , now the coordinates column contains information on the range of time an object takes up within my graph . I want to allow the user to filter by the date , but the problem is I use a function to determine the date normally . Take : Now if I wanted to filter by date , I would want to is a like : But the problem is I first need to apply get_shape_range to coordinates in order to receive a string . Is there any way to do ... I guess a transform_filter operation ? Such that before the like happens , I apply some function to coordinates ? In this case I would need to write a get_time_range function that returned only time , but the question remains the same.EDIT : Here 's my database classand I 'm using sqlite 3.0 . The reasoning why behind most things are strings is because most of my values that are to be stored in the database are sent as strings , so storing is trivial . I 'm wondering if I should do all this parsing magic with the functions before , and just have more database entries ? for stuff like decimal time_begin , time_end , latitude_begin instead of having a string containing the range of time that I parse to find time_begin and time_end when i 'm filtering"
"I am currently offloading a long running job to a TaskQueue to calculate connections between NDB entities in the Datastore . Basically this queue handles several lists of entity keys that are to be related to another query by the node_in_connected_nodes function in the GetConnectedNodes node : Here a Node has a repeated property connections that contains an array with other Node key ids , and a matching sources array to that given connection.The yielded results are stored in a blobstore.Now the problem I 'm getting is that after an iteration of connection function the memory is not cleared somehow . The following log shows the memory used by AppEngine just before creating a new GetConnectedNodes instance : Apart from some fluctuations the memory just keeps increasing , even though none of the previous values are accessed . I found it quite hard to debug this or to figure out if I have a memory leak somewhere , but I seem to have traced it down to that class . Would appreciate any help ."
"Grouping by a single dimension works fine for xarray DataArrays : However , this is only supported for a single dimension , grouping by multiple dimensions does thus not work : I only have an inefficient solution which does the for loops manually : This is however horribly slow for larger arrays.Is there a more efficient / straight-forward work-around ?"
"Here is my simple codeIt 's absolutely obvious , but ... WHY interpreter is executing blockForever ? I specified in generator , that I want only elements < 10IMHO , it 's a little bit misunderstandingAny way to prevent infinite execution without re-writing `` series '' ?"
"As expected , 1 is not contained by the empty tuplebut the False value returned is not equal to FalseLooking at it another way , the in operator returns a bool which is neither True nor False : However , normal behaviour resumes if the original expression is parenthesizedor its value is stored in a variableThis behaviour is observed in both Python 2 and Python 3.Can you explain what is going on ?"
"Answered : It appears that this datatype will not be suited for adding arbitrary strings into hdf5store.BackgroundI work with a script which generates single rows of results and appends them to a file on disk in an iterative approach . To speed things up , I decided to use HDF5 containers rather than .csv . A benchmarking then revealed that strings slow HDF5 down . I was told this can be mitigated when converting strings to categorical dtype . IssueI have not been able to append categorical rows with new categories to HDF5 . Also , I do n't know how to control the dtypes of cat.codes , which AFAIK can be done somehow . Reproducible example:1 - Create large dataframe with categorical data2 - Create one row to append3 - Save ( 1 ) to HDF and try to append ( 2 ) This results in the following Exception ValueError : invalid combinate of [ values_axes ] on appending data [ name- > Dummy_Data , cname- > Dummy_Data , dtype- > int8 , kind- > integer , shape- > ( 1 , ) ] vs current table [ name- > Dummy_Data , cname- > Dummy_Data , dtype- > int32 , kind- > integer , shape- > None ] My fixing attemptsI tried to adjust the dtypes of cat.catcodes : When I do this , the error disappears , but so does the categorical dtype : In addition , df_small.info ( ) does not show the dtype of cat.codes in the first place , which makes it difficult to debug . What am I doing wrong ? Questions1 . How to properly change dtypes of cat.codes ? 2 . How to properly append Categorical Data to HDF5 in python ?"
"I am slowly trying to understand the difference between views and copys in numpy , as well as mutable vs. immutable types.If I access part of an array with 'advanced indexing ' it is supposed to return a copy . This seems to be true : Since c is just a copy , it does not share data and changing it does not mutate a . However , this is what confuses me : So , it seems , even if I use advanced indexing , assignment still treats the thing on the left as a view . Clearly the a in line 2 is the same object/data as the a in line 6 , since mutating c has no effect on it.So my question : is the a in line 8 the same object/data as before ( not counting the diagonal of course ) or is it a copy ? In other words , was a 's data copied to the new a , or was its data mutated in place ? For example , is it like : or like : I do n't know how to check for this because in either case , a.flags.owndata is True . Please feel free to elaborate or answer a different question if I 'm thinking about this in a confusing way ."
I 'm trying to generate a list quickly with content from two different arrays of size n and n/2 . As an example : I wish to generate something likeI understand the second for statement is the nested for loop after the `` x '' one . I 'm trying to get the contents of the new array to beCould anyone point me in the right direction ?
"I tried the following on Codecademy 's Python lessonWith this , it works fine but if instead I tryI get [ u'Hobby1 ' , u'Hobby2 ' , u'Hobby3 ' ] . Where are the extra us coming from ?"
I am writing a small program which uses Qt5 QML as the GUI layer and Python3-PyQt5 to implement the data model.I now want to display a ComboBox in QML and set its model to a list of enums . How would I export the enum as a property of the python class so that I can reference it in QML ? Preferably I would write this in QML :
"I 'm working on a project just for fun , and my goal is to play online poker and have the program identify the cards that are on the table . I am using OpenCV with python to isolate the area where the cards are going to be . I have been able to take an image of that area , grayscale and threshold it , and draw a contour around the edge of the card . I am now stuck on how to move forward . This is my code so far : This is the result of what I have so far : I need to be able to take the inside of the contour , and remove anything outside of it . Then the resulting image should only be the card , which I need to scale to 49x68 pixels . Once I can get that to work my plan is to get the contours of the rank and suit , and fill it in with white pixels , which I would then compare to a set of images to determine the best fit.I am very new to OpenCV and image processing , but I find this stuff incredibly fascinating ! I 've been able to get this far with Google , but I ca n't find anything this time . This is the image that I am using as a way to replace the game for now : This is one of the images that I will use to compare the table cards to :"
"I 'm working on a window manager written using python 's xlib bindings and I 'm ( initially ) attempting to mimic dwm 's behavior in a more pythonic way . I 've gotten much of what I need , but I 'm having trouble using X 's built in window border functionality to indicate window focus . Assuming I 've got an instance of Xlib 's window class and that I 'm reading the documentation correctly , this should do what I want to do ( at least for now ) - set the window border of a preexisting window to a garish color and set the border width to 2px . However , I get nothing from this - I can add print statements to prove that my program is indeed running the callback function that I associated with the event , but I get absolutely no color change on the border . Can anyone identify what exactly I 'm missing here ? I can pastebin a more complete example , if it will help . I 'm not exactly sure it will though as this is the only bit that handles the border ."
"I 'm trying to use elements from a numpy array as an input for the Fraction module and I 'm getting the following error : `` TypeError : both arguments should be Rational instances '' For example , if I do : And then : I 'll get the same error . I also tried to do X=X.astype ( 'int ' ) , or X=X.astype ( 'int32 ' ) , without success . What do I have to do to convert the numpy array in a `` Rational instance '' that is needed for Fraction module ?"
"In the python and/or ipython interactive interpreter , how can I get name bound on the last unhandled exception ? I.e . the equivalent ofMust be something like ..."
"I 've been trying to get Python3s ast.literal_eval ( ) function to work ( in an interactive Python session ) but -- and I read it 's doc description -- I can not .My goal is to replace this : with the safer alternative , this : but the latter does n't work ( at least not how I am using it ) . Note that I also tried triple-quoted and raw-string variations , too.I believe I 'm simply missing a nuance here ( e.g . it 's possible that it may not like evaluating constants in a module ) . From the Python-3 documentation on ast.literal_eval ( ) : Safely evaluate an expression node or a string containing a Python literal or container display . The string or node provided may only consist of the following Python literal structures : strings , bytes , numbers , tuples , lists , dicts , sets , booleans , and None . This can be used for safely evaluating strings containing Python values from untrusted sources without the need to parse the values oneself . It is not capable of evaluating arbitrarily complex expressions , for example involving operators or indexing.Any help appreciated . = : ) Thank you in advance !"
"I have a scrip that automatically generates equations.The equations are constructed using sympy symbols.I would like to know whether or not these is a way to check if the equations are linear in terms of certain variables.eg.check for the following : is eq1 linear in terms of a , d ?"
The grammar needs to reject from : mary and only accept from : mary i.e . without any interleaving spaces . How can I enforce this in pyparsing ? Thanks
So . We have a messy data stored in a TSV file that I need to analyse.This is how it looks And the problem is that some of the rows have different column order / some of them missing values and I need to get rid of that with high performance ( since the datasets I am working with are up to 100 Gigabytes ) .Now I got rid of missing values but one problem still remains ! This is how the data looks : And this is how the problem looks : As you can see some of columns are offset.I made a very low-performance solutionSo now the data looks good ... at least I can work with it ! But what is the High-Performance ALTERNATIVE to the method I made above ? Update on unutbu code : traceback errorPandas version : 0.18.0-np110py27_0UpdateEverything worked ... Thanks everybody !
When I do something like ( totally random example dont read into variable names ) : In this case does read_file ( ) get excuted twice ? If so is there a way to do it to only execute once but keep it within one line ?
I have a string likeI also have val=1 for example . Is there a clean way to split the foo/bar/baz into a multi-dimensional dict with the last item in the dict to equal 1 . So it would look like
"Suppose I have a string such asI want to remove the second occurrence of duplicate phrase without removing other occurrences of its constituent parts , such as the other use of duplicate.Moreover , I need to remove all potential duplicate phrases , not just the duplicates of some specific phrase that I know in advance.I have found several posts on similar problems , but none that have helped me solve my particular issue : Removing duplicate wordsRemoving duplicate linesRemoving repeated words within a phraseI had hoped to adapt the approach from the last link there ( re.sub ( r'\b ( .+ ) ( \s+\1\b ) + ' , r'\1 ' , s ) ) for my purposes , but could not figure out how to do so.How do I remove all arbitrary duplicate phrases of two or more words from a string in Python ?"
"I 'm trying out google customsearch api to search image and but the weird thing is my search through api returns different result than regular search through browser . for examplerunning this code gives totally different resulthere is result from running above codehere is a snapshot of google search from chromequeries are same , anyone knows why ?"
I have an numpy array in Python and i need to classify between a range of value ( > = 2 to < 5 = 100 ) . I got an error message and I do n't understand the use of a.any ( ) or a.all ( )
"I 've been Googling quite some time for e.g . 'typeof ' and 'performance ' , but I have n't been able to find a satisfactory answer to the following problem.I am trying to implement complex numbers for the Transcrypt Python to JavaScript compiler , using local operator overloading . Since we 're dealing with a dynamically typed language , it can not be predicted what type of data will be in a variable.If I translate x + y to JavaScript , having operator overloading switched on , it will translate e.g . as __add__ ( x , y ) In order to do the right thing , the __add__ function has to check for both x and y whether they are 'ordinary ' JavaScript numbers or if one of them or both of them are of type 'complex ' , since that requires special operations.The most obvious way to do that is to test for typeof x == 'number ' . However , coming from a C/C++ background , it seems ridiculously inefficient to test for equality with a string with six characters which on top of that first has to be retrieved from memory , only to possible add two integers , which for many processors , once parsed , would be only one instruction.What amazes me most is that checks like this are advised everywhere around the internet as the normal thing to do . Does anyone know if x == 'number ' or possible x === 'number ' is somehow cleverly optimized to prevent a full string comparison.To further clarify the problem , here 's my current code for the __add__ operator , using the string comparison.If not can anyone hint me on a quicker way to distinguish between a number and an arbitrary non-numerical object.Thanks for the tips . The source is now : translated by : to :"
Is it possible to call Tcl procedures that have function pointers ( or callback functions ) from Python ? I am using Tkinter to call Tcl procedures from Python.Python Snippet : Tcl Snippet : Note I can not modify Tcl code as its an external library to my application.//Hemanth
"Given a list of decorator methods , how would one apply those to a callable ? For example , since : ... is the same as : ... one would assume that with a list of decorators ( [ foo , bar ] ) they could be applied to baz dynamically ."
How to get count of values greater than current row in the last n rows ? Imagine we have a dataframe as following : I am trying to get a table such as following where n=3.Thanks in advance .
"I am using recurive feature elimination with cross validation ( rfecv ) as the feature selection technique with GridSearchCV.My code is as follows.Now , I want to get the optimal number of features and selected features from the above code . For that I ran the below code.However , I got the following error : AttributeError : 'RFECV ' object has no attribute 'n_features_ ' . Is there any other way of getting these details ? I am happy to provide more details if needed ."
"This code below best illustrates my problem : The output to the console ( NB it takes ~8 minutes to run even the first test ) shows the 512x512x512x16-bit array allocations consuming no more than expected ( 256MByte for each one ) , and looking at `` top '' the process generally remains sub-600MByte as expected.However , while the vectorized version of the function is being called , the process expands to enormous size ( over 7GByte ! ) . Even the most obvious explanation I can think of to account for this - that vectorize is converting the inputs and outputs to float64 internally - could only account for a couple of gigabytes , even though the vectorized function returns an int16 , and the returned array is certainly an int16 . Is there some way to avoid this happening ? Am I using/understanding vectorize 's otypes argument wrong ? I 'm using whichever versions of Python/numpy are current on an amd64 Debian Squeeze system ( Python 2.6.6 , numpy 1.4.1 ) ."
"I know it is possible to force sdist to produce .zip from command line : But how to make this option a default for my setup.py ? I 'd like to get consistency for running setup.py sdist on both Windows and Linux , and I choose .zip format , because I can turn .zip into executable ."
"Given the following list of lists : I need each list within a to have the same number of elements.First , I need to get the longest length of any list in a.Then , I need to ensure all lists are at least that long.If not , I want to add a zero ( 0 ) to the end until that is true . The desired result is : Thanks in advance ! P.S . I also need to apply this to a Pandas Data Frame like this one :"
"I have a System model , and an Interface model . An Interface is the combination between two systems . Before , this interface was represented as an Excel sheet ( cross table ) . Now I 'd like to store it in a database.I tried creation an Interface model , with two foreign keys to System . This does n't work because : It creates two different reverse relationships on the target modelIt does n't avoid having duplicates ( first and second rel swapped ) I used this code : Is n't there a better way to do this ? I 'd need to have symmetrical relations : it should'nt matter is a System is the `` first '' or `` second '' in an Interface ."
"I want to remove all spaces from a string. `` as fa sdf sdfsdf `` The result would be : `` asfasdfsdfsdf '' There are several ways I can think of to achieve this , and I 'm wondering which one is the best.1.2.And I assume that there are more.Which one is preferred ?"
"I 'd like to align two lists in a similar way to what difflib.Differ would do except I want to be able to define a match function for comparing items , not just use string equality , and preferably a match function that can return a number between 0.0 and 1.0 , not just a boolean.So , for example , say I had the two lists : and I want to be able to write a match function like this : and then do : and have it diff using the match function . Like difflib , I 'd rather the algorithm gave more intuitive Ratcliff-Obershelp type results rather than a purely minimal Levenshtein distance ."
"I have an ndarray , and I want to replace every value in the array with the mean of its adjacent elements . The code below can do the job , but it is super slow when I have 700 arrays all with shape ( 7000 , 7000 ) , so I wonder if there are better ways to do it . Thanks !"
"If I have an d dimensional np.array , how can I get the indicies of the boundary ? For example , in 2d , Now I would like to get the boundaries Great if efficient and works for arbitrary number of dimensions , but it has to work at least 3 . The array is not a necessarily a hypercube , but potentially a hyperrectangle : the number of grid points in all dimension are not necessarily the same , unlike in the example.For an array of shape ( 4 , 5 , 6 ) , the expected output is"
Essentially a 3d version of this : Plot two histograms at the same time with matplotlib Though I do n't know how to do it since I am using Axes 3d .
"I have a set of data ( X , Y ) . My independent variable values X are not unique , so there are multiple repeated values , I want to output a new array containing : X_unique , which is a list of unique values of X. Y_mean , the mean of all of the Y values corresponding to X_unique . Y_std , the standard deviation of all the Y values corresponding to X_unique ."
"I 've got a question which I 'll ask two ways : short & generic , so future generations of StackOverflow readers will benefit , and Long & Detailed , so I can get my work done without screwing anything up.Short & Generic Version : How do I make Django generate a table-like form where some info in the table is from the database , and the user fills in the rest ? On form submission , each row in the table should become a record in the database ( after it 's validated , of course ) .What 's the cleanest way to do this ? What 's this mode of interaction cannonically called ? Example Form Resulting Database Records Long VersionI 'm writing a web app to track gas cylinder usage at my company . We have a bunch of gas plumbing in our building , and we need to know which gas cylinder was hooked up to which gas line at what time . I 'd like two forms for the technicians to fill out : Daily Inventory : Every morning , the stockroom guy needs to look at each gas line and record the line 's pressure , and the reference number of the bottle . This generates bunch of 4-tuple records ( time , line , bottle , psi ) ; one for each line , every morning . As-Needed Bottle Change : After doing the daily inventory , if a bottle is almost out it needs to be changed , and that change needs to be logged . This should add another entry to the table of bottles for the new bottle , and another 4-tuple with the new ( time , line , bottle , psi ) info for the new connection . This happens to a random line a few times a week , but not every day.So to keep track of this I 'm writing a Django application . I 've got the following models : I 've populated the database with our back-log of data using some scripts , and I 've written a few views to pull data out of the databse ; I 'm happy with them so far , and the results look very promising -- at least for displaying stored data.But I 'm not sure how to cleanly populate the database using HTML forms . I 'd like the forms to be basically two separate `` worksheets '' -- like the kind the DMV gives you , with nice clear instructions # justkidding.Form 1 : Daily InventoryThe form would list all the lines in a given farm , display what bottle should be on each line ( based on previous readings/updates ) , and then prompt the user to enter a value . This would require that the technician update the pressure of every bottle on every line each time they submit the form -- we want a global snapshot of the whole gas system . In a perfect world , the form would pre-fill the current time and each line 's most recent pressure reading into the Reading Time and Pressure fields to ease data entry.After reading through the Django docs on Forms , ModelForms , and Formsets , I 've written some code that does almost everything I want -- but the Line and Bottle information are editable form fields , and I need them to be static guideposts for filling in the rest of the form . They do need to be present in the generated database records , though.I am dimly aware of the readonly and disabled attributes , and of what appear to be kludgy solutions to clean data from the POST variable in the response when you want to have read-only stuff in forms , but I 'm still not clear on how those work or why they 're necessary . I 'm wondering if there 's a cleaner way to get what I '' m after ? Perhaps forms with programmatically generated headings , or annotations ? That 's all I really want : an auto-generated guide to filling out the form.Form 2 : Change a Gas BottleI 'd like a list of all the gas lines , with the current bottle & pressure ( easy -- this is done elsewhere ) , and then a button that makes a pop-up window where you can submit a new bottle , much like you find in the admin interface . How do I make pop-up windows ? How do I make buttons ? I 'm not even sure where to start with this one yetI 'm still very new to Django , and I 've searched high and low , but have n't found anything that answers my question -- maybe I 'm just not using the right keywords ? Thanks for your help.-Matt"
"In Python3 and pandas I have the dataframe : The column `` EmentaMateria '' has in each row a series of sentences . I plan to create a new dataframe from rows that contain any or several of these words ( or group of words ) in this column : So I did like this : The newly generated dataframe has collected multiple sentences that have one or more of these words . However , many lines do not have these words , as `` ENCAMINHA AO SENADO FEDERAL , UM ADENDO AS SUGESTOES DE EMENDAS A PROPOSTA ORCAMENTARIA DO DISTRITO FEDERAL , REFERENTE A ALTERACAO DO PROGRAMA DE TRABALHO DO FUNDEPE - FUNDO DE DESENVOLVIMENTO DO DISTRITO FEDERAL , VISANDO A ACRESCENTAR MAIS CZ 3.453.977.000,00 ( TRES BILHOES , QUATROCENTOS E CINQUENTA E TRES MILHOES , NOVECENTOS E SETENTA E SETE MIL CRUZADOS ) AO PROJETO DE EXECUCAO DE OBRAS E EQUIPAMENTOS DO SISTEMA DE EDUCACAO E CULTURA . `` Please , is this because similar words are also being searched for in sentences ? Or because many sentences have too much white space or line breaks between some words ? Edit 7/12/2019Thanks so much all friends for your attention . After you wrote I went back to reviewing the database and again the suggested codes . I got the original database , with accents in Brazilian Portuguese . I think this is the original problem - I did n't know the original base had been changedI found that the database I was working on had gone through unidecode to remove accents from Portuguese . So I repeated the tests with a str_choice with accents and with the original database , and then it worked - I have n't checked all the lines yet , but all I 've seen are correct so farSo the new str_choice ( I used the name search_list ) , I used was this :"
"Can I accomplish a rank/sort using Counter.most_common ( ) functionality , thus avoiding this line : d = sorted ( d.items ( ) , key=lambda x : ( -x [ 1 ] , x [ 0 ] ) , reverse=False ) ? ? Challenge : You are given a string.The string contains only lowercase English alphabet characters.Your task is to find the top three most common characters in the string.Output Format : Print the three most common characters along with their occurrence count each on a separate line . Sort output in descending order of occurrence count . If the occurrence count is the same , sort the characters in ascending order.In completing this I used dict , Counter , and sort in order to ensure `` the occurrence count is the same , sort the characters in ascending order '' . The in-built Python sorted functionality ensures ordering by count , then alphabetical . I 'm curious if there is a way to override Counter.most_common ( ) default arbitrary sort/order logic as it seems to disregard the lexicographical order of the results when picking the top 3 ."
"I would like to write multiple functions in the same Python module , each of which is a separate profiling test using timeit , so that I can use command line argument to specify which one to run . A naive example ( profiling.py ) would be : However , when I try python profiling.py foo , I get error like the following : I have searched for the usual space/tab indention error in the code , but did n't find any . Hence I wonder if it is because I wrap a timeit test inside a function , and this is not allowed ?"
"I want to see how precision and recall vary with the threshold ( not just with each other ) Returns : I can , therefore , not plot them together . Any clues as to why this might be the case ?"
"( Or a list of lists ... I just edited ) Is there an existing python/pandas method for converting a structure like thisinto a pivot table like this ? Naively , I would probably just loop through the dictionary . I see how I can use a map on each inner list , but I do n't know how to join/stack them over the dictionary . Once I did join them , I could just use pandas.pivot_table"
"I am new to the world of SAT solvers and would need some guidance regarding the following problem.Considering that : ❶ I have a selection of 14 adjacent cells in a 4*4 grid ❷ I have 5 polyominoes ( A , B , C , D , E ) of sizes 4 , 2 , 5 , 2 and 1 ❸ these polyominoes are free , i.e . their shape is not fixed and can form different patternsHow can I compute all the possible combinations of these 5 free polyominoes inside the selected area ( cells in grey ) with a SAT-solver ? Borrowing both from @ spinkus 's insightful answer and the OR-tools documentation I could make the following example code ( runs in a Jupyter Notebook ) : The problem is that I have hard-coded 5 unique/fixed polyominoes and I do n't know to how define the constraints so as each possible pattern for each polyomino is taken into account ( provided it is possible ) ."
"For testing purposes on a project I 'm working on , I have a need to , if given a regular expression , randomly generate a string that will FAIL to be matched by it . For instance , if I 'm given this regex : Then I should be able to generate strings such as : ... each of which does NOT match the regex , but NOT generate : ... each of which DO . In other words , I want something like an anti-Xeger.Does such a library exist , preferably in Python ( if I can understand the theory , I can most likely convert it to Python if need be ) ? I gave some thought to how I could write this , but given the scope of regular expressions , it seemed that might be a much harder problem than what things like Xeger can tackle . I also looked around for a pre-made library to do this , but either I 'm not using the right keywords to search or nobody 's had this problem before ."
"I have a question that is puzzling me recently about which is the best way to retrieve attributes from outside.Let say I have a class : Now I know that if I want to retrieve whatever attribute I can do this : I have the habit ( probably because I come from other oo languages ) to define methods to retrieve class attributes as needed and use them insted of retrieve them directly , like : In my little experience I 've found that using this approach make things easier to mantain in the long term because if I edit the structure of data attributes I have to edit only the specific method.But since I am not really a python veteran I 'd love to know if I am doin ' it right or if some other approaches are better and more pythonic . Thoughts ?"
"I have a function that takes a string-like argument.I want to decide if I can safely store the argument and be sure that it wo n't change . So I 'd like to test if it 's mutable , e.g the result of a buffer ( ) built from an array.array ( ) , or not.Currently I use : Is there a better way to do it ? ( copying the argument is too costly , that 's why I want to avoid it )"
"A code illustration as an intro to my questions : It seems that the only way for me to find the signature of datetime.datetime.replace while I code is to look it up in the doc : date.replace ( year , month , day ) .The only introspection part that seems to work : I 've examined how the Jupyter function arglist tool-tip works , they have the exact same problem , i.e . no arglist available for datetime.datetime.replace.So here are the questions : Is it still possible to get the argument list somehow ? Maybe I could install the C sources for datetime and connect them via the __file__ attribute ? Is it possible to annotate a < type 'method_descriptor ' > with the arglist information ? In that case , I could parse the linked doc 's markdown definition and automatically annotate the built-in module functions ."
"I have a pandas Series that is composed of intsnow I want to cluster the series into groups that in each group , the differences between two neighbour values are < = distance . For example , if the distance is defined as 1 , we haveif the distance is 2 , we haveif the distance is 3 , we havehow to do this using pandas/numpy ?"
"This is a bit of a ( very basic ) language-lawyer kind of question . I understand what the code does , and why , so please no elementary explanations.In an expression , in has higher precedence than and . So if I writeit is interpreted just likeHowever , the in of a for loop has lower precedence than and ( in fact it has to , otherwise the following would be a syntax error ) . Hence if a Python beginner writes ... , it is equivalent to this : ( which , provided `` seq1 '' is truthy , evaluates to for n in `` something '' ) .So , the question : Where is the precedence of the for-loop 's in keyword specified/documented ? I understand that n in ... is not an expression in this context ( it does not have a value ) , but is part of the for statement 's syntax . Still , I 'm not sure how/where non-expression precedence is specified ."
"There 's a surprise here : It seems like the mere mention of __class__ is explicitly checked by the parser ? Otherwise we should get something likeIndeed , if you modify to only check the key instead , i.e . check for '__class__ ' in locals ( ) , then we only have self in scope as expected . How does it happen that this variable gets magically injected into scope ? My guess is this is something to do with super - but I did n't use super , so why does the compiler create an implicit closure reference here if it is n't needed ?"
"I 'm close to getting the map that I want . Matplotlib 's Basemap is great , but the coastlines are too coarse when I zoom in . I can read the Natural Earth shapefiles and plot them , which are much better ... but when I try and fill the polygons , I think it 's treating all of the points as belonging to a single polygon . How can I iterate through the polygons and display the map correctly ? Thanks in advance ! Here 's the code :"
I have a list with an order of insertion . I want to paginate the results using the same order . As you can see currently the output will be a different order . related question
I 'm trying to append a number of NaN rows to each group in a pandas dataframe . Essentially I want to pad each group to be 5 rows long . Ordering is important . I have : I want :
"Does Parquet support storing various data frames of different widths ( numbers of columns ) in a single file ? E.g . in HDF5 it is possible to store multiple such data frames and access them by key . So far it looks from my reading that Parquet does not support it , so alternative would be storing multiple Parquet files into the file system . I have a rather large number ( say 10000 ) of relatively small frames ~1-5MB to process , so I 'm not sure if this could become a concern ?"
"I first runSo I should have a .coverage file with all the default settings.Within folder_1 , I have file_1.py , file_2.py , and file_3.pyWhen I cd into folder_1 and runIt outputs : It does n't generate anything for file_3.py ! But then when I run : it says : Does it skip files with no coverage in the report ? How can I change it so the report shows me the results of every *.py file ?"
I was following the instruction by VS code 's website but it seemed that nothing that I tried worked.I created a new configuration as required but whenever I put the path it refuses to work in VS code although the path VS code complains about in the integrated terminal window works fine when I call it manually.The error the debugger throws is the following : then I tried running the file it complains manually and it runs it just fine ... even when I hover over the path name and click it with command + click then it takes me to the path from within VS code . Which seems bizzare . So somehow only when I run it in debugger mode does it not work . Why ? Launch.jsonCross-posted : Quora : https : //qr.ae/TzkO4Lreddit : https : //www.reddit.com/r/vscode/comments/f3hm9r/how_to_correctly_set_specific_module_to_debug_in/gitissue : https : //github.com/microsoft/ptvsd/issues/2088
I was wondering if there is a way to groupby consecutive index numbers and move the groups in different columns . Here is an example of the DataFrame I 'm using : And my idea is to gruoup by sequential index numbers and get something like this : Ive been trying to split my data by blocks of 3 and then groupby but I was looking more about something that can be used to group and rearrange sequential index numbers.Thank you !
"The usual coreference resolution works in the following way : Providedit figures out that refers to There are plenty of tools to do this.However , is there a way to do it backwards ? For example , givenI want to do the pronoun resolution `` backwards , '' so that I get an output likeMy input text will mostly be 3~10 sentences , and I 'm working with python ."
"I have the following folder structure : Inside each of the modules ( some_module1.py for example ) there is a class that extends from a base class , in my case , Producer.What I am trying to do is dynamically load in this class . To do that , I have a list of `` installed apps '' that looks like this : I am trying to write a function that will check each `` app '' package for a particular producer class and ensure it extends from the producer base class . Something like this : I 've tried experimenting with imp and importlib , but it does n't seem to handle this kind of import . Is there anyway for me to be able to achieve this ?"
I do n't want to create real files in my unittest . So i find myself often tinkeringwith nonsense boilerplate and creating mock-factories for pseudo-files . I was wonderingif it would n't be nicer to avoid ( in this instance ) these pointless efforts and try something like in the scope of a local unittest method : Would this be ok ? Or are there major caveats/do n't issue in this approach and be betterof with mock-factories ?
"I have a csv-file with time series data , the first column is the date in the format % Y : % m : % d and the second column is the intraday time in the format ' % H : % M : % S ' . I would like to import this csv-file into a multiindex dataframe or panel object.With this code , it already works : It returns the data in the following format:1st question : I would like to show the second index as a pure time-object not datetime . To do that , I have to declare two different date-pasers in the read_csv function , but I ca n't figure out how . What is the `` best '' way to do that ? 2nd question : After I created the Dataframe , I converted it to a panel-object . Would you recommend doing that ? Is the panel-object the better choice for such a data structure ? What are the benefits ( drawbacks ) of a panel-object ?"
"I have a website that dynamically creates rules that have cron events attached . All of these rules are associated with and call a single lambda function.I am using python and boto3 to generate the rules and apply them to the Lambda Function . ( If seeing my python code that generates the rules and events would help , I 'd be happy to include it here . ) This all works , however after using my website and creating about 68 rules I got this error : PolicyLengthExceededException : An error occurred ( PolicyLengthExceededException ) when calling the AddPermission operation : The final policy size ( 20642 ) is bigger than the limit ( 20480 ) .Every-time I create a rule and its event , a permission needs to be added to the lambda 's Function Policy , and after about 68 rules , the Function Policy gets too large.How can I fix this ? Here is an example permission : The AWS : SourceArn is the unique value of the rule , so I 've figured I could use * to give permission to all rules . So I tried putting as the value for the AWS : SourceArn : arn : aws : events < some_arn_data > : rule/*But on the function dashboard , down where the list of CloudWatch Events is at , all it said was : The rule * could not be found.Is there a way to make a permission that applies to all rules ? If not , is there a different way to solve this ? And if there is no way to directly solve this issue , I could create a separate lambda for each record on the website rather than create separate rules for each record that all point to one lambda . Is there any reason why creating a separate lambda would this would be a bad idea ? Such as is there a limit to the amount of Lambda functions you can have ?"
"Is this even possible ? Basically , I want to turn these two calls to sub into a single call : What I 'd really like is some sort of conditional substitution notation like : I only care about the capitalization of the first character . None of the others ."
"When you do something like this : What does Python do ? Does it first generate an array with [ 0,1,2,3,4 ] and then goes over each item printing it ? Similar to : Or does it print each number as it generates them ? Something like : Generate 0 assign 0 to i print i Generate 1 - > assign 1 to i - > print i Generate 2 - > assign 2 to i - > print i Generate 3 - > assign 3 to i - > print i Generate 4 - > assign 4 to i - > print iUpdateI have added the tag for Python 2.7 . I did n't think my question was version specific , but it seems it is ! Right now I am working with Python 2.7 so my question refers to that . But I find very valuable the information comparing Python 2 range against Python 3 range ."
"I am doing a value_counts ( ) over a column of integers that represent categorical values . I have a dict that maps the numbers to strings that correspond to the category name . I want to find the best way to have the index with the corresponding name . As I am not happy with my 4 lines solution.My current solutionNow how I solve the problem : QuestionI am not happy with that solution that is very tedious , do you have a best practice for that situation ?"
"I use pdfkit and wkhtmltopdf to generate pdf documents . When i generate the first pdf all is well . When i quickly ( within 5 seconds ) generate an other i get the error [ Errno 9 ] Bad file descriptor . If i close the error ( step back in browser ) and open again , it will create the pdf . my views.pyMaybe important note : i run this site on IIS8 , when running from commandline ( python manage.py runserver ) the error is not present . Any guidelines on how to handle this error would be great ."
"I 'm running a Flask app on Heroku using gunicorn with eventlet workers . A particular route on my app frequently receives POST data ( x-www-form-urlencoded ) with some fairly chunky fields -- on the order of 500KB at most.This works fine when running locally , but on Heroku , requests to that route takes anywhere from 5 to 30 seconds to complete -- and almost 100 % of the time is spent in the first access to request.form : This is confirmed by Newrelic slow request tracing as well . There 's a few milliseconds here or there for the database operations , and then a huge chunk of time in Python code , apparently spent waiting on some i/o , since the reported CPU time is usually less than a millisecond.I have been totally unable to reproduce this in a local environment using the same gunicorn/eventlet setup I 'm using in production . Even the built-in debug WSGI server is lightning-fast on these requests.Does anybody have any idea what might be going wrong ? Is it a problem with Flask , or something I just need to contact Heroku support about ?"
"I am trying to determine if an SSL certificate is self signed or not . Currently I have the following code which compares the issuer CN and the subject CN and if they are the same , marks the result as self signed.This comparison is very simplistic , but works in a lot of cases . I 'm not trying to verify SSL certs or reimplement OpenSSL . How can I make this better and be roughly 95 % sure if a certificate is self signed or not ? My one requirement is that I would like to do this in Python and not call other processes or use shell commands ."
"I am trying to setup a `` off-brand '' touchscreen on a Raspberry Pi3 and I am having some trouble when I run my python/kivy program , visually everything is fine , but whenever I need to move a slider , push a button or whatever . the input y-axis is inverted , note that it works fine when i run this on my windows machine As you can see the input for the buttons are inverted on the y axisI do know this exists and I have tried it , but it did nothing to solve the problem ( In my case ) .https : //stackoverflow.com/a/34344458/7522859And I have tried to fix it in the config file under ( in my case ) ( .kivy/config.ini ) , and as you can see it reads the invert_y option but does nothing with both 1 and 0 as its values.So the question is how do I invert the Y-axis for the input in the kivy config file . I have also taken a look at this and I did n't really understand it that well , maybe I am just stupid.Sorry if this post is bad . Its my first post and English is not my native language ."
"Give the following dfI want the result with new column with grouped values as listThis is similar to these questions : grouping rows in list in pandas groupbyReplicating GROUP_CONCAT for pandas.DataFrameHowever , it is apply the grouping you get from df.groupby ( 'Id ' ) [ 'concat ' ] .apply ( list ) , which is a Series of smaller size than the dataframe , to the original dataframe.I have tried the code below , but it does not apply this to the dataframe : I know that transform can be used to apply groupings to dataframes , but it does not work in this case ."
"[ Edit : as someone pointed out I have used improperly the palindrom concept , now I have edited with the correct functions . I have done also some optimizations in the first and third example , in which the for statement goes until it reach half of the string ] I have coded three different versions for a method which checks if a string is a palindrome . The method are implemented as extensions for the class `` str '' The methods also convert the string to lowercase , and delete all the punctual and spaces . Which one is the better ( faster , pythonic ) ? Here are the methods:1 ) This one is the first solution that I thought of : I think that this one is the more faster because there are n't transformations or reversing of the string , and the for statement breaks at the first different element , but I do n't think it 's an elegant and pythonic way to do so2 ) In the second version I do a transformation with the solution founded here on stackoverflow ( using advanced slicing string [ : :-1 ] ) But I think that the slicing and the comparision between the strings make this solution slower.3 ) The thirds solution that I thought of , use an iterator : which I think is way more elegant of the first solution , and more efficient of the second solution"
"I have a large collection of data , about 10 million entries and part of my program required very many membership checks ... right now I have data as dictionary entries with all their values equal to ' 1 ' I also have a program that uses an algorithm to figure out the same information , but for now it is slower then the dictionary method however I expect the size of data to continue growing ... For my current dictionary solution , would type ( data ) as a frozenset , or set ( or something else ? ) be faster ? And for the future to find out when I need to switch to my program , does anyone know how the speed of checking membership correlated with increasing the size of a type that 's hashable ? Is a dictionary with 1 billion entries still fast ?"
"I 'm running Python 3.1 and you would call me an advanced novice : ) My question is simple : I 'm trying to make a simple program which asks the users for a URL ( or multiple URLs ) and then goes to the website and takes a screenshot ( of the whole page , not just what can be seen in the browser without scrolling all the way down ) .It 's simpler then it sounds , I want to use an existing platform on the web , similar to this : Although this website does not work : ( , I 'm wondering is it possible to do it with this website and if so , how ? If it is not possible , are there any alternatives ?"
"I have dict of nested lists : I need create list of tuples like : I tried : I think my solution is a bit over-complicated . Is there some better , more pythonic , maybe one line solution ?"
"I 'm trying to merge two dataframes in pandas , using read_csv . But one of my dataframes ( in this example d1 ) is too big for my computer to handle , so I 'm using the iterator argument in read_csv . Let 's say I have two dataframesI need to merge them so that each row captures all data for each person , so the equivalent of doing : but since I ca n't fit d1 into memory , I 've been using read_csv ( I 'm using read_csv because I already processed a huge file and saved it into .csv format , so imagine my dataframe d1 is contained in the file test.csv ) .But when I domy output is the first dataframe appended by the second dataframe . My output looks like this : Hope my question makes sense : )"
"I am learning about descriptors in python . I want to write a non-data descriptor but the class having the descriptor as its classmethod does n't call the __get__ special method when I call the classmethod . This is my example ( without the __set__ ) : And here is how I call it : The __get__ of the descriptor class gets no call . But when I also set a __set__ the descriptor seems to get activated : Now I create a C instance : and both of __get__ , __set__ are present . It seems that I am missing some basic concepts about descriptors and how they can be used . Can anyone explain this behaviour of __get__ , __set__ ?"
"I wanted to test the performance of vectorizing code in python : The code gives the following output : The performance difference in the first and second functions are not surprising . But I was surprised that the 3rd function is significantly slower than the other functions . I am much more familiar in vectorising code in C than in Python and the 3rd function is more C-like - running a for loop and processing 4 numbers in one instruction in each loop . To my understanding numpy calls a C function and then vectorize the code in C. So if this is the case my code is also passing 4 numbers to numpy each at a time . The code should n't perform better when I pass more numbers at once . So why is it much more slower ? Is it because of the overhead in calling a numpy function ? Besides , the reason that I even came up with the 3rd function in the first place is because I 'm worried about the performance of the large amount of memory allocation to x in func1 . Is my worry valid ? Why and how can I improve it or why not ? Thanks in advance . Edit : For curiosity sake , although it defeats my original purpose for creating the 3rd version , I have looked into roganjosh 's suggestion and tried the following edit.The output : There is an improvement , but still a large gap compared with the other functions.Is it because x [ i : i+4 ] still creates a new array ? Edit 2 : I 've modified the code again according to Daniel 's suggestion.The output : There is another speedup . So the declaration of numpy arrays are definitely a problem.Now in func3 there should be one array declaration only , but yet the time is still way slower . Is it because of the overhead of calling numpy arrays ?"
NumPy is really helpful when creating arrays . If the first argument for numpy.array has a __getitem__ and __len__ method these are used on the basis that it might be a valid sequence.Unfortunatly I want to create an array containing dtype=object without NumPy being `` helpful '' .Broken down to a minimal example the class would like this : and if the `` iterables '' have different lengths everything is fine and I get exactly the result I want to have : but NumPy creates a multidimensional array if these happen to have the same length : Unfortunatly there is only a ndmin argument so I was wondering if there is a way to enforce a ndmax or somehow prevent NumPy from interpreting the custom classes as another dimension ( without deleting __len__ or __getitem__ ) ?
"The output of np.c_ differs when its arguments are lists or tuples . Consider the output of the three following linesWith a list argument , np.c_ returns a column array , as expected . When the argument is a tuple instead ( second line ) , it returns a 2D row . Adding a comma after the tuple ( third line ) returns a column array as for the first call.Can somebody explain the rationale behind this behavior ?"
"Given the following dataframe : HAVE : I want to convert it to another format , that is , `` unfold '' the intervals and make them into a DatetimeIndex , and resample the data . The result should look like this : WANT : Any help is very much appreciated !"
"I have a pandas dataframe that looks like the following : I want to remove any rows that overlap . Overlapping rows is defined as any row within X days of another row . For example , if X = 365. then the result should be : If X = 50 , the result should be : I 've taken a look at a few questions on here but have n't find the right approach . For example , Pandas check for overlapping dates in multiple rows and Fastest way to eliminate specific dates from pandas dataframe are similar but do n't quite get me what I need.I have the following ugly code in place today that works for small X values but when X gets larger ( e.g. , when X = 365 ) , it removes all dates except the original date . Any help/pointers would be appreciated ! Clarification : The solution to this needs to look at every row , not just the first row ."
"So what I want to do seems relatively simple , but for the life of me , I just ca n't quite get it . I have a .txt file likeAnd I want its information to be available to me like so ( i.e . I do not need to write a new .txt file unless it would be necessary . ) ... or , 1 is subtracted from every number but the formatting remains the same . There will never be a number greater than 1 in the original , so negatives wo n't be possible . This whole headache is due to converting indexing to begin with 0 instead of 1 . What may complicate things is that the original file prints likeWhat I 've DoneWell its a mishmash of different things I 've found on StackOverflow , but I think I 'm going about it in the most cumbersome way possible . And this one did n't make sense when I implemented it.. although it may be on the same track with the issue with spaces..My thought process was , I need to remove the `` /n '' and to convert these strings into integers so I can subtract 1 from them . And somehow keep the formatting . It 's important to have the formatting be the same since each line contains information on the similarly indexed line of another file . I do n't want to see the `` /n '' in the end result ( or print statement ) but I still want the effect of a new line beginning . The above code however , wont work for two reasons ( that I know of ) . int ( n [ : ] ) throws an error since it does n't like the spaces and when I put a value ( say 0 ) in there , then the code prints the first number on each of the lines and subtracts one.. and puts it all on one line . So , it seems redundant to take out a carriage return and have to throw another in , but I do need to keep the formatting , as well as have a way to get all the numbers ! This also did n't work : but instead of just a wrong output it was an error : Does anyone have any ideas on what I 'm doing wrong here and how to fix this ? I am working with Python 2.6 and am a beginner ."
Compose map function in pythonHello Today I use two map call to transform a mask on 0 1 string to boolean : How to do that with only one map ?
"I would like to create a square plot using multiple axes using make_axes_locateable as demonstrated in the matplotlib documentation . However , while this works on plots where the x and y data have the same range , it does not work when the ranges are orders of magnitude different . Although this code uses the set_aspect answer as in How do I make a matplotlib scatter plot square ? the axes are not modified correctly as shown here : I attempted to repair this with : But this resulted in the following : Setting the aspect after calling scatter and before creating the two histogram axes seemed to have no effect , even though it appears that was done in the documentation example . This code does work when the data range is the same : Update : One of the key constraints of this question is to use make_axes_locateable and not GridSpec as discussed in the comments below . The problem I 'm working on involves creating plotting functions that accept an Axes object to work on and modify it without having knowledge of the figure or any other Axes in the plot as in the following code : This question extends questions such as Set equal aspect in plot with colorbar and python interplay between axis ( 'square ' ) and set_xlim because of the Axes-only constraint ."
"i 'm making a crawler to get text html inside , i 'm using beautifulsoup.when I open the url using urllib2 , this library converts automatically the html that was using portuguese accents like `` ã ó é õ `` in another characters like these `` a³ a¡ a´a§ '' what I want is just get the words without accentscontrã¡rio - > contrarioI tried to use this algoritm , bu this one just works when the text uses words like these `` olá coração contrário ''"
"I 'm attempting to write a custom Theano Op which numerically integrates a function between two values . The Op is a custom likelihood for PyMC3 which involves the numerical evaluation of some integrals . I ca n't simply use the @ as_op decorator as I need to use HMC to do the MCMC step . Any help would be much appreciated , as this question seems to have come up several times but has never been solved ( e.g . https : //stackoverflow.com/questions/36853015/using-theano-with-numerical-integration , Theano : implementing an integral function ) . Clearly one solution would be to write a numerical integrator within Theano , but this seems like a waste of effort when very good integrators are already available , for example through scipy.integrate.To keep this as a minimal example , let 's just try and integrate a function between 0 and 1 inside an Op . The following integrates a Theano function outside of an Op , and produces correct results as far as my testing has gone.However , attempting to do integration within an Op appears much harder . My current best effort is : Which gives the following error : I 'm surprised by this , especially the TypeError , as I thought I had converted the output_storage variable into a tensor but it appears to believe here that it is still an ndarray ."
"When I run flask run on Windows , I get the following errorThis was working previously , the issue started after I created a new env recently ."
"Suppose I have the code : The question is : how to keep b updated on each change in a ? E.g. , after the above code I would like to get : print ( b ) to be 5 , not 4.Of course , b can be a function of a via def , but , say , in IPython it 's more comfortable to have simple variables . Are there way to do so ? Maybe via SymPy or other libraries ?"
"The problem I am trying to solve is a bit like the employee scheduling one here : https : //github.com/google/or-tools/blob/master/examples/python/shift_scheduling_sat.pyHowever , there are a few things that I am stuck on and have no idea how to incorporate in to the code . I will explain the issue below.ProblemI have a fleet of 47 trains that I want to assign to 49 routes each day . The trains should be assigned with the following constraints : Every train must be used at least once during the day ( no train must be idle for the whole day ) Every train must be assigned to at least one route ( and max two routes ) and every route MUST be coveredThe trains final mileage , once assigned to a route , must not exceed 24,800 ( i.e . the previous day 's cumulative mileage + assigned route mileage < = 24,800 ) . This is probably best understood by looking at the total_km_day_end column in the 3rd table belowWhere a train is assigned to two routes in a day , the times of these routes must not overlap A further constraint I would like to have , but am not precious about is this ( let 's say it 's a soft constraint ) : trains with high mileage for the previous day should be assigned to short routes and trains with low mileage for the previous day should be assigned to long routesI have a data frame for the trains that looks like this . I can pick a date at random and see the cumulative mileage up to the end of the previous day ( i.e . 18/9/2018 ) for each of the 47 trains : And a data frame for the routes that looks like this . Note that a route above 100 km is defined as being long , below this it 's short . Of the 49 routes , there are only 6 routes that are short ( 10 km ) - note that only 5 of the short routes are shown below : What I want to end up with is something like this where the trains have been assigned 1 or 2 routes and the total mileage is shown for the end of the day ( assuming the assigned routes are completed by the train ) : EDIT/UPDATE ( 2/8/19 ) : ( NOTE : the code below shows a pared down version of the problem with 6 trains assigned to 8 routes . I have also included constraint 5 in the code . ) Thanks so much to Stradivari and Laurent for their help with this one . Output :"
"I am working on a tool to model wave energy converters , where I need to couple two software packages to each other . One program is written in Fortran , the other one in C++ . I need to send information from the Fortran program to the C++ program at each time step . However , the data first needs to be processed in Python before it is sent to the C++ program . I have received a tip to use MPI to transfer the data between the programs . I am now trying to send a simple string from the Fortran code to Python , but the Python code gets stuck at the receive command.My Fortran code looks like this : My Python code is the following : The Python code never gets past the MPI receive command and does not finish . The Fortran code does finish and properly prints the `` MPI FINALIZED '' message.I do n't see where I am doing something wrong , the message gets sent from process 0 to process 0 with a tag 22 and uses MPI_COMM_WORLD in both codes ."
I have a list : I want to sort it in next order : The following method does not produce a result :
"Inspired by this other question , I 'm trying to wrap my mind around advanced indexing in NumPy and build up more intuitive understanding of how it works.I 've found an interesting case . Here 's an array : if I index it a scalar , I get a scalar of course : with a 1D array of integers , I get another 1D array : so if I index it with a 2D array of integers , I get ... what do I get ? Oh no ! The symmetry is broken . I have to index with a 3D array to get a 2D array ! What makes numpy behave this way ? To make this more interesting , I noticed that indexing with numpy arrays ( instead of lists ) behaves how I 'd intuitively expect , and 2D gives me 2D : This looks inconsistent from where I 'm at . What 's the rule here ?"
"I have 5 sets of request 's categories defined as python dicts , for example : And I need to handle requests using their category , for instance : and I need to dispatch a request using the request type to a different function to process it.I already know there are ways of dispatching this requests in Python without the need of using if-elif , but my question is : what 's the best way to do it while maintaining the code clean and simple ?"
"I have camera set up , which works fine . The thing is , there is an ModuleNotFoundError when I am trying to import pygame . ( Note : I am using windows ) This is a test project , and I have to make a camera out of pygame . I 've tried some youtube tutorials and I messed with pygame but it always causes an Error.This is what I have so far : I resulted in the same error every time I tried.The Error message is : Any Advice ?"
"In Python , how can I map from a range of values to one concrete value ? Basically , I want a dictionary , which I can fill with ranges and index with numbers :"
"In the Cython docs there is an example where they give two ways of writing a C/Python hybrid method . An explicit one with a cdef for fast C access and a wrapper def for access from Python : And one using cpdef : I was wondering what the differences are in practical terms.For example , is either method faster/slower when called from C/Python ? Also , when subclassing/overriding does cpdef offer anything that the other method lacks ?"
"I 'm trying to read the access for files and directories in Windows using this code ( patterned after Tim Golden 's proposed patch to os.access to make it read from ACLs on Windows ) : However , every time I run this , I get this : How am I supposed to pass the SecurityDescriptor to AccessCheck ? EDIT : Changing the DACL_SECURITY_INFORMATION to DACL_SECURITY_INFORMATION | GROUP_SECURITY_INFORMATION | OWNER_SECURITY_INFORMATION gives me this :"
"Is it possible to add a datashader image to a set of matplotlib subplots ? As a concrete example , Where I have a two by two array of matplotlib subplots and would like to replace the [ 0,0 ] plot ax_r [ 0 ] in the above example with the datashader image a . Is this possible , and if so , how ? Thanks !"
"My CMake project compiles a Python .so/.dylib extension module linked with a big static library . I want to compile the subproject 's static library with hidden visibility for symbols : it would allows the linker ( or LTO optimizer ) to discard symbols unused by my python module.The best way to do this would be using CXX_VISIBILITY_PRESET and friends on the subproject 's static library target.However , the subproject use policies from 3.1.0 by declaring : The policy CMP0063 NEW : Honor visibility properties for all target types . is only introduced from version 3.3 and thus , the set_target_properties have no effects.My project requires CMake 3.3 , but I have no control on the subproject.I would like to avoid patching the subproject CMakeLists.txt , but currently I see no other way.Any idea ?"
"I use jupyter notebook with anaconda . I use kerast firstly , and i ca n't do tutorial . About this issues are two themes in stackoverflow , but solve not found.My code : And I have error , it 's some random and sometimes one or two epoch competed : Epoch 1/5 4352/17500 [ ====== > ... ... ... ... ... ... ... .. ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - ValueError Traceback ( most recent call last ) in ( ) 2 # of 32 samples 3 # sleep ( 0.1 ) -- -- > 4 model.fit ( X_train , Y_train , nb_epoch=5 , batch_size=32 ) 5 # sleep ( 0.1 ) C : \Anaconda3\envs\py27\lib\site-packages\keras\models.pyc in fit ( self , x , y , batch_size , nb_epoch , verbose , callbacks , validation_split , validation_data , shuffle , class_weight , sample_weight , **kwargs ) 395 shuffle=shuffle , 396 class_weight=class_weight , -- > 397 sample_weight=sample_weight ) 398 399 def evaluate ( self , x , y , batch_size=32 , verbose=1 , C : \Anaconda3\envs\py27\lib\site-packages\keras\engine\training.pyc in fit ( self , x , y , batch_size , nb_epoch , verbose , callbacks , validation_split , validation_data , shuffle , class_weight , sample_weight ) 1009 verbose=verbose , callbacks=callbacks , 1010 val_f=val_f , val_ins=val_ins , shuffle=shuffle , - > 1011 callback_metrics=callback_metrics ) 1012 1013 def evaluate ( self , x , y , batch_size=32 , verbose=1 , sample_weight=None ) : C : \Anaconda3\envs\py27\lib\site-packages\keras\engine\training.pyc in _fit_loop ( self , f , ins , out_labels , batch_size , nb_epoch , verbose , callbacks , val_f , val_ins , shuffle , callback_metrics ) 753 batch_logs [ l ] = o 754 -- > 755 callbacks.on_batch_end ( batch_index , batch_logs ) 756 757 epoch_logs = { } C : \Anaconda3\envs\py27\lib\site-packages\keras\callbacks.pyc in on_batch_end ( self , batch , logs ) 58 t_before_callbacks = time.time ( ) 59 for callback in self.callbacks : -- - > 60 callback.on_batch_end ( batch , logs ) 61 self._delta_ts_batch_end.append ( time.time ( ) - t_before_callbacks ) 62 delta_t_median = np.median ( self._delta_ts_batch_end ) C : \Anaconda3\envs\py27\lib\site-packages\keras\callbacks.pyc in on_batch_end ( self , batch , logs ) 187 # will be handled by on_epoch_end 188 if self.verbose and self.seen < self.params [ 'nb_sample ' ] : -- > 189 self.progbar.update ( self.seen , self.log_values ) 190 191 def on_epoch_end ( self , epoch , logs= { } ) : C : \Anaconda3\envs\py27\lib\site-packages\keras\utils\generic_utils.pyc in update ( self , current , values ) 110 info += ( ( prev_total_width - self.total_width ) * `` `` ) 111 -- > 112 sys.stdout.write ( info ) 113 sys.stdout.flush ( ) 114 C : \Anaconda3\envs\py27\lib\site-packages\ipykernel\iostream.pyc in write ( self , string ) 315 316 is_child = ( not self._is_master_process ( ) ) -- > 317 self._buffer.write ( string ) 318 if is_child : 319 # newlines imply flush in subprocesses ValueError : I/O operation on closed file"
"I 'm trying to use ltree extension in PostgreSQL to build a full-text address search engine.My model looks like this ( it 's slightly simplified ) : So , data in this table will look like this : I want to do a full-text search on the aggregated name of each entity.Basically I need to convert each row in table to the next format to do a search : I 'm doing that in such way , so user can perform queries like 'los ang cali ' or similar.I have no problems to do that with raw PostgreSQL query : That works fine , but while using RawQuerySet , I ca n't use things like .filter ( ) , .group_by ( ) , pagination , etc.The main constraint to reproduce it in Django is this JOIN : it 's used to join all ancestors of each element and then aggregate them using array_agg ( ) , array_to_string functions , so the output of these functions can be used further in full-text search.If anyone have better ideas how to implement such kind of thing using Django ORM , please advise ."
"When I open verb.exc , I can seeWhile I use lemmatization in codeHow can this happen ? Do I misunderstand in revising wordNet ?"
"I am using pytest framework and want to skip testcase based on some condition . Below code is not skipping the test case . When running , it is saying 0 testcases executed . This func_fixture is very crucial for the testsuite . It performs many pre-requisites before starting the test . If I remove class , and add rest of the functions with same syntax ( after removing self ) , it works . Not sure why it is failing in class"
I was learning this and this to understand class attributes . But got confused with the output of following code snippet . Python Interpreter : 2.7.9And output is And I was expecting something like this as an output .
"When aiming for reproducibility in Python code using random number generators , the recommended approach seems to be to construct separate RandomState objects . Unfortunately , some essential packages like scipy.stats can not ( to the best of my knowledge ) be set to use a specific RandomState and will just use the current state of numpy.random.My current workaround is to use a context manager which saves the state of the RNG and then resets it upon exiting as follows : There are a lot of warnings in the documentation about changing the state in any way - is the above approach safe in general ? ( in the sense that the change is local to the context and that the rest of my code will be unaffected )"
I am trying to change the displayed length of the axis of matplotlib plot . This is my current code : Which generates the following plot : I would like my plot to look like the following with axis line shortened : I was n't able to find this in ax [ axis ] .spines method . I also was n't able to plot this nicely using ax.axhline method .
"I have some generators doing some searching stuff , and I use another generator wrapping them up : So now I am wondering how I can rewrite it into async version , that can yield multiple values in searching_stuff_1 and searching_stuff_2 . I was trying : However , in this version , I have to append all results into a list and wrap it in a Future instance . I am wondering if there is better way to handle multiple results from coroutine function . Is it possible that I can still yield those numbers ?"
Is it possible to use df.apply to get an attribute as opposed to running a function ? I want to retrieve the year from a date to perform a groupby . For example ..This works ..This also works ..But this does not ..Nor does this ..What is the most efficient way of doing this ?
"I 'm having the same problem as this guy and possibly this guy , but I am around to share some code and respond to questions ! I have some code in a batch job that reads fields from a Microsoft Access database via pyodbc and prepares the output for display.Here is a snippet . Note the assert.When I run it , it successfully processes 100,000 rows and then fails : Note the aberrant colon . It occurs rarely - about one time in 300,000 records.When I try to isolate it , of course it works. $ 54871.00The type of the field in Access is : Data Type : CurrencyDecimal Places : 2Input Mask : Default Value : Validation Rule : Text Align : GeneralMy vague finger-pointing suspicion based on insufficient evidence : pyodbc is poking with the internals of Decimal , perhaps confused by an Access corruption . As @ ecatmur points out : ' : ' is ' 9 ' + 1 in ASCIIAnyone seen this and solved it ? Versions : Python 2.7.4pyodbc 3.0.6 ( latest ) Access 2010Windows 7Digging further : The decimal module is implemented in Python . From my reading , the values are described by four attributes : _exp , _int , _sign , _is_specialSuspecting corruption , I printed out the values of these fields.Surprisingly , for both the faulty and the working version , I get : That 's weird.In the decimal module , the __float__ function is defined fairly simply : But when I do this with the bad data : I get : Str 54871.0000Float 54870. : The more I learn , the less weird it does n't get ."
"Example code : [ ] How do I get a copy of the x , y data , or the indices from the selection ?"
Why does foo function bellow work and bar one does not ? What am I missing here ?
"I have a large amount of auto-generated classes in python that essentially represent enums for part of a communication protocol , they look like soDefining things this way makes it easy for the auto-generator and also for humans to modify . The ParamEnum class provides all the functionality , get/setting , comparison , conversion and creation from the incoming network data and etc.However , these classes require some extra meta-data . I do not want to add this into the source definition of each class though , as it makes it less readable and will break the autogeneratorAt the moment I am doing it like thishowever that strikes me as somewhat inefficient , since this will happen every time we instantiate one of these Enums ( which happens often ) , not just on definition ( the metadata does not change , so it only needs to be set once ) I tried added this to the bottom of the definition file , but ran into problems there too.Is there a way in python of overriding/adding functionality to when the class is defined , that can be inherited to subclasses ? Ideally I 'd like something like"
"I have a simple model class that represents a battle between two characters : I have a method which constructs a CTE which projects the battles into a series of appearences ( each battle has two appearences - the winner and the loser ) : I then have a query which utilises this view to determine the characters which have seen the most battles : This generates the following SQL : This works perfectly fine when executing against a SQLite database , but when executing against a Postgres SQL database the following error is given : There are a few things to notice at this point : The sub-select is redundant , we should simply using the sub-select as the main select statement.You could resolve this by aliasing the sub-select and using < alias > . < column > in the main select statement - Postgres requiring an alias on the sub-selects is well documented elsewhere.My first question is how would I alias this sub-select seeing that SQLalchemy decides to introduce it despite not being explicitly instructed to ( as far as I can tell ) ? I found a solution to the problem was to add .alias ( `` foo '' ) to the query : Which casuses the following SQL to be generated ( one that weirdly resolved the whole redundant sub-select issue as well ! ) : My second question is why did adding the alias prevent the sub-select from being created and why is the alias not used ! The `` foo '' alias was seemingly disregarded yet had a substantial effect on the generated query ."
"I train a multi label classification model with XGBBoost and want to code this model in another system.Is it possible to see the text output of my XGBClassifier model as dump_model in XGB Booster.Edit : I found that model._Booster.dump_model ( outputfile ) returns a dump file as below . However , there is nothing that specifies the class . In my model , there are 10 classes , however in the dumpfile there is only a booster . So , I 'm not sure if it 's the model of all classes or just one of them ."
"I would like to add a check in a python 2.7.x script in the form ofHow would the __check_freebsd__ function look like ? I have the following code for __check_debian__ already : So you do n't have to bother with it ( suggestions for improvements are welcome , of course ) ."
"I have a regex to detect invalid xml 1.0 characters in a unicode string : On Linux/python2.7 , this works perfectly . On windows the following is raised : Any ideas why this is n't compiling on Windows ?"
"Using PyQt4 4.8.6 the code below produces the errorQObject : :startTimer : QTimer can only be used with threads started with QThreadwhen a is used as the variable for QApplication , but it does not produce the error if cpp ( or most anything else ) is used for the variable . Is this a bug in PyQt4 or is there something I am missing ?"
"I 'm working on a django web application and i 'm in the process of minimizing the amount of individual database hits by using the prefetch_related and select_related methods , i have a certain method in my User model that pulls a couple of different related objects from it.And then i use this method in my view.The problem is , since request.user is not retrieved by the normal query methods , i do n't get to use the prefetch_related and select_related along with pulling the user , and i ca n't find any way to retrieve the related data along with the model of that user.Is there a way to , say , override the retrieving of the user model so i can run the prefetch_related and select_related methods ?"
"I 'm trying to solve a Sudoku with cvxpy optimization package . I 'm really new to both optimization and cvxpy . The constraints are : all the values are between 1 to 9sum of all rows = 45sum of all columns = 45sum of all squares = 45the given numbers ( I 'm trying to solve a 17-clues Sudoku ) . So , here is my code : What I get from cvxpy is This is for sure a valid Sudoku , and I checked the numbers million times . Why am I not getting the answer ? Any help would be appreciated !"
I have the following dataframe using pandasI would like to add new column to get the date corresponding to the value presents in 'Last_age ' for each row to get something like that :
"I have a function that runs a tick ( ) for all players and objects within my game server . I do this by looping through a set every .1 seconds . I need it to be a solid .1 . Lots of timing and math depends on this pause being as exact as possible to .1 seconds . To achieve this , I added this to the tick thread : My question is , is this the best way to do this ? If the duration of my loop is 0.01 , then time_lapsed == 0.01 ... and then the sleep should only be for 0.09 . I ask , because it does n't seem to be working . I started getting the overloaded server message the other day , and the server was most definitely not overloaded . Any thoughts on a good way to `` dynamically '' control the sleep ? Maybe there 's a different way to run code every tenth of a second without sleeping ?"
"I am currently coding setup.py using setuptools.And I want to copy the static data ( which is not a Python module ) to site-packages.The thing is , the current folder hierarchy is structured like the following : I want to copy the skeleton directory to site-packages WHILE maintaining the folder structure/hierarchy , but how should I do this ?"
"I am working on a bot . I want the bot to change the proxy of the webdriver every 50 searches . I have an API that requests the proxy and the socket , i store those variables and so far i have been using firefox profiles to set it up but that doesnt work very well.So given the fact that i already have a viable source of proxies and ports , can you tell me any way i can change the proxy without crashing the webdriver and doing it on a single session ? Previous attempts : I tried setting up a firefox profile this way : This caused me some problems and i had to start a new session each time i wanted to swap a proxy . So i tried to change the proxy through the Firefox options ( options - general - connection settings ) but turns out the popup that appears on the screen after clicking the connection settings button is not accessible through selenium or javascript ( xul file ) ."
"I encounter a memory leak and decreasing performance when looping over a Keras model predict function when using a tf.data.Dataset to feed the model , but not when feeding it with a numpy array.Does anyone understand what is causing this and/or how to resolve the issue ? Minimal reproducible code snippet ( copy/paste runnable ) : Result : Predict loop timing starts around 0.04s per iteration , within a minute or two it 's up to about 0.5s and process memory continues to increase from a few hundred MB to close to a GB.Swap out the tf.data.Dataset for an equivalent numpy array and runtime is ~0.01s consistently.Working case code snippet ( copy/paste runnable ) : Related discussions : Memory leak tf.data + Keras - Does n't seem to address the core issue , but the question appears similar.https : //github.com/tensorflow/tensorflow/issues/22098 - Possibly an open issue in Keras/Github , but I ca n't confirm it , changing inter_op_paralellism as suggested in that thread has no impact on the results posted here.Additional info : I can reduce the rate of performance degradation by around 10x by passing in an iterator instead of a dataset object . I noticed in training_utils.py:1314 the Keras code is creating an iterator each call to predict.TF 1.14.0"
"I have ranges in a list like : I would like to find the longest ranges that can be constructed from these ( when they overlap with each other ) . Expected output : I have a solution , however it is way too complicated , and I am sure there must be an easier solution to this problemMy solution : Output :"
I have n't had a very thorough training in python and sometimes do n't know the correct way of doing things . One of these things is testing if my resultQuery returned a result or not . I find myself doing this a lot : I do n't know about python but try catches in other languages are supposed to be for exceptions and not for normal program flow . How would you do this with an if else ? I guess I want something similar to :
I do not see this in the SQL comparison documentation for Pandas . What would be the equivalent of this SQL in Pandas ? I have the merge code as follows : How do I incorporate the 'not equal ' portion ?
"There 's many examples on how to do these things to be found : 1 ) Communicate between diffrent processes in the same program . 2 ) Communicate between client/server over a network However , this question has no good example anywhere I 've looked : What is a canonical way to send a string from python program A to program B , which blocks and processes this string and then waits for another in a loop ? I feel like I 've come close to an answer many times , but never managed to create a working example . Additional implied requirements : Actually two diffrent programs : the example needs to actually have two diffrent programs ( i.e . two files progA.py , progB.py that may be separately run from the commandline in two screens on the same machine ) , not use any kind of forking or multiprocess to create the client and server . Please suggest a way of doing this that allows variable length delimited strings up to a reasonable length to be sent , instead of having to get the exact byte count of the data size correct . ( The latter is much more error-prone in implementation ) . Ideally do this without utilizing localhost internet connectionsFor example ; when the reader uses : and the writer uses : as is suggested at http : //www.python-course.eu/pipes.php the reader gets stuck in an infinite loop reading out the empty string . Interestingly , adding if ( action == 'enable ' ) : longFunction ( ) to the program.processLine function results in part of the code in the longFunction being executed before getting stuck reading out empty lines forever . On the other hand , all examples utilizing the more modern less low-level subprocess module only involve multi-threaded applications , not multiple applications . Other implementations involve sockets and networking . While I 've tried utilizing sockets , this results in the generic 'something went wrong'-type error with many possible causes Error 111 : “ connection refused ” showing up 'some of the time ' . As part of the python code that is executed upon receiving certain commands actually modifies the network config ( e.g . it calls commands such as ip , tc and iptables with various arguments ) utilizing a network connection to localhost is something that probably should be avoided , causing hard to debug and generally nasty issues . Besides the part where it 's unneccessary as the second program runs on the same machine , so any interprogram communication should not need to use network interfaces ."
"I want to pull all the items currently in a queue . There is another thread constantly putting items in the other end , and every period I want to get all the items currently in the queue.Is there some reason to prefer : orNow the docs specifically say that the qsize ( ) > 0 will not prevent the queue from blocking on a get , but is this true only in the case where multiple threads can take from the output ? Queue.qsize ( ) Return the approximate size of the queue . Note , qsize ( ) > 0 doesn ’ t guarantee that a subsequent get ( ) will not block , nor will qsize ( ) < maxsize guarantee that put ( ) will not block.Does this mean that the second form should always be preferred ? EAFP and all that ? Also , is there any cost to calling q.qsize ( ) ? Does it block the other end of the queue in order to count ? I guess I have talked myself into using the second form but it seems much less clean to me ."
"I 've been playing around with memoization and recursion in python 3.3Ignoring the fact that python is the wrong language to be doing this in , I 've found that I get inconsistent results between using functools.lru_cache to memoize , and not using functools.lru_cache I 'm not changing the recursion limit - it stays at the default , which for me is 1000.To test the problem , I 've written up a simple recursive function to sum numbers from 1 through iRunning this function normally , I can run sumtil ( 998 ) comfortably without hitting the recursion limit . sumtil ( 999 ) or above will throw an exception.However , if I try decorating this function with @ functools.lru_cache ( ) , the recursion limit exception is thrown 3 times earlier , when running sumtil ( 333 ) Being that 332*3 = 996 , but 333*3 = 999 , it appears to me that the lru_cache decorator is causing each level of recursion in my function to become three levels of recursion.Why do I get three times as many levels of recursion when using functools.lru_cache to memoize a function ?"
"I have a dictionary ( e.g . English - Croatian ) . It may contain sentences and phrases . I 'm translating a file of form `` english text '' = `` english text '' into form `` english text '' = `` croatian text '' and using python regex module to do so . The regex I 'm using looks like this ( given variable original which is text in English that should be translated : That way I'am able to capture exactly the english text inside the quotes on the right-hand side and substitute it with Croatian . However , the problem appears if the original text contains parenthesis inside . In example : In that case an error `` unbalanced parenthesis '' is raised . If original would be hard-coded , I could solve the problem by puttingHowever , there is a whole file full of *original * variables.Is there any solution to this problem other than changing original variable by preceeding all parenthesis in it with a backslash ?"
"What could possibly cause this weird python behaviour ? It gives the same output for 0.1 , 0.5 , 5.1 , 0.0 , etc.. Integers are echoed back at me correctly , but anything with a decimal point gives me the crazy numbers.This is a python binary compiled for ARM , installed via Optware on a Synology DiskStation 101j.Has anyone seen anything like this before ?"
"I want to get the original file/scriptname , etc of the function that is being decorated . How can I do that ? I tried using fn.__code__ but that gave me more stuff that I needed . I could parse that string to get the function name , but was wondering if there is a more elegant way to do it"
"I am seeing Python behavior that I do n't understand . Consider this layout : main.py : test1.py : test2.py : config.py : so , python main.py produce : I am confused by the last line . I thought that it would print initial_value again because I 'm importing config.py in test2.py again , and I thought that changes that I 've made in the previous step would be overwritten . Am I misunderstanding something ?"
"Essentially , I am training an LSTM model using Keras , but when I save it , its size takes up to 100MB . However , my purpose of the model is to deploy to a web server in order to serve as an API , my web server is not able to run it since the model size is too big . After analyzing all parameters in my model , I figured out that my model has 20,000,000 parameters but 15,000,000 parameters are untrained since they are word embeddings . Is there any way that I can minimize the size of the model by removing that 15,000,000 parameters but still preserving the performance of the model ? Here is my code for the model :"
"This prints 2 , 1 , 0.Now , is the order of the deleted elements defined somewhere in specification or is it implementation-specific ? ( or maybe I do n't understand the underlying mechanics ) Could the output , for example , be 0 , 1 , 2 ? I realize the 2 , 1 , 0 order is probably done to avoid the memory reallocations for the elements while deleting them , but still the question remains.And the last one - what 's the difference between del l and del l [ : ] statements ?"
"I am using flask-mwoauth to create a simple application in Flask using OAuth authentication on Mediawiki ( and Wikipedia in particular ) .flask-mwoauth is a blueprint that provides some convenience methods to interact with Mediawiki Extensions : OAuth and adds the following URIs : /login - runs the OAuth handshake and returns the user to //login ? next=/someurl will return the user to /someurl/logout - clears the users ' access tokens/logout ? next=/someurl will return the user to /someurl/oauth-callback - callback from MW to finish the handshakeThe users ' OAuth key and secret are stored in the session.I would like to be able to create custom responses for some of this custom URIs . Take for example /logout , the definition of the response in very simple ( __init__.py # L56 ) : I would like to define in my application the route /logout with a custom response ( for example , rendering a template ) , however if I use the blueprint then the route @ app.route ( `` /logout '' ) is ignored.What I would like to know if it is possible to `` extend '' the blueprint in the sense that I can define a route /logout in my app , call the original method from the blueprint and then serve a customized response ."
"I have a dataframe , and I want to calculate weighted cumulative sum of B group by A . But I do n't know how to apply the transformation ."
Is there a established way that i validate an object in the dispatch without making an extra database call when self.get_object ( ) is called later in get/post ? Here is what i have so far ( slightly altered for this question ) :
"To compare performance of Spark when using Python and Scala I created the same job in both languages and compared the runtime . I expected both jobs to take roughly the same amount of time , but Python job took only 27min , while Scala job took 37min ( almost 40 % longer ! ) . I implemented the same job in Java as well and it took 37minutes too . How is this possible that Python is so much faster ? Minimal verifiable example : Python job : Scala job : Just by looking at the code , they seem to be identical . I looked a the DAGs and they did n't provide any insights ( or at least I lack the know-how to come up with an explanation based on them ) . I would really appreciate any pointers ."
"I know that a Numba-jitted function calling another jitted function will recognize this and automatically use a fast C calling convention rather than going through the Python object layer , and therefore avoid the high Python function call overhead : My question is whether the same is true if I call a Cython function from Numba . So let 's say I have a Cython module , foo.pyx : As well as a standard Python module bar.py : Will Numba recognize foo.foo as a C-callable function automatically or do I need to tell it manually by , say , setting up a CFFI wrapper ? EDIT : Upon further reflection , Cython functions are just standard `` builtin '' functions from the view of the Python interpreter . So the question can be made more general : does Numba optimize calls to builtin functions and methods to bypass the Python calling overhead ?"
I want to load a Jupyter Notebook Server Extension within a local directory : extension.pyI run the following command from the directory containing server_ext : But I get the error `` No module named extension '' . Is there something I have to do to get Jupyter/python session to recognize the path to the module ?
"I am running Superset via Docker . I enabled the Email Report feature and tried it : However , I only receive the test email report . I do n't receive any emails after.This is my CeleryConfig in superset_config.py : The documentation says I need to run the celery worker and beat . I added them to the 'docker-compose.yml ' : Celery Worker is indeed working when sending the first email . The log file is also visible . However , the celery beat seems to not be functioning . There is also no 'celery_beat.log ' created.If you 'd like a deeper insight , here 's the commit with the full implementation of the functionality.How do I correctly configure celery beat ? How can I debug this ?"
"I 'm trying to use PyInstaller to create a standalone OSX app that runs a GUI I 've made . When I enter the following in my terminal : Everything seems to work until I get the following error : Which seems strange , since my name is not felipe ! I have a couple of questions:1 ) How is it possible that there is a directory under the name felipe on my computer ? ( I used anaconda to install qt , I do n't know whether that has something to do with it ? ) 2 ) Up until I get the error message , PyInstaller is looking in the correct folder . Why does it start looking in this vague ( vague to me that is ) directory I do n't know of ? 3 ) I 'm quite a novice regarding directories and I ca n't find mister felipe anywhere on my computer . When I look in the Users folder I just see my own user and an empty `` Shared '' folder . ( I do n't know what the shared folder is used for and why it 's there . ) 4 ) Based on what I read on the internet , I copied qt_menu-nib to the folder where the script that 's supposed to be turned into a standalone is located . What should I do in order to successfully create a standalone from here ?"
"I have the following codeThe problem is that lib2 refers to the original shared library , not the new one . If I delete mylib.so between the calls I get no error.Using ctypes._reset_cache ( ) does not help.How can I tell ctypes to actually reload the library from the hard disk ?"
"I 'm writing a simple IronWorker in Python to do some work with the AWS API.To do so I want to use the boto library which is distributed via PiPy . The boto library is not installed by default in the IronWorker runtime environment.How can I bundle the boto library dependancy with my IronWorker code ? Ideally I 'm hoping I can use something like the gem dependancy bundling available for Ruby IronWorkers - i.e in myRuby.worker specify In the Python Loggly sample , I see that the hoover library is used : However , I ca n't see where/how you specify which hoover library version you want , or where to download it from.What is the official/correct way to use 3rd party libraries in Python IronWorkers ?"
"I have a list comprehension that looks like this : C is a dict with keys that are tuples of arbitrary integers . All the tuples have the same length . Worst case is that all the combinations should be included in the new list . This can happen quite frequently.As an example , I have a dictionary like this : And I want the the result to be : So , by overlap I am referring to , for instance , that the tuples ( 1,2,3,4 ) and ( 2,3,4,5 ) have the overlapping section ( 2,3,4 ) . The overlapping sections must be on the `` edges '' of the tuples . I only want overlaps that have length one shorter than the tuple length . Thus ( 1,2,3,4 ) does not overlap with ( 3,4,5,6 ) . Also note that when removing the first or last element of a tuple we might end up with non-distinct tuples , all of which must be compared to all the other elements . This last point was not emphasized in my first example.The better part of my codes execution time is spent in this list comprehension . I always need all elements of cart so there appears to be no speedup when using a generator instead.My question is : Is there a faster way of doing this ? A thought I had was that I could try to create two new dictionaries like this : And somehow merge all combinations of elements of the list in aa [ i ] with the list in bb [ i ] for all i , but I can not seem to wrap my head around this idea either.UpdateBoth the solution added by tobias_k and shx2 have better complexity than my original code ( as far as I can tell ) . My code is O ( n^2 ) whereas the two other solutions are O ( n ) . For my problem size and composition however , all three solutions seem to run at more or less the same time . I suppose this has to do with a combination of overhead associated with function calls , as well as the nature of the data I am working with . In particular the number of different keys , as well as the actual composition of the keys , seem to have a large impact . The latter I know because the code runs much slower for completely random keys . I have accepted tobias_k 's answer because his code is the easiest to follow . However , i would still greatly welcome other suggestions on how to perform this task ."
"This question has been asked but not answered . The only difference is I am on a Arch Linux 64 bit . I am using python 2.7 and the package that got installed of bokeh is 0.10.0 I followed theconda install bokeh instructions from here and did the conda update conda and conda update anacondaStill it does not work . Not only is bokeh.plotting not working , but neither is bokeh.sampledata which leads me to believe none of it is working . Has any one else had this problem with this or any package and successfully solved it ? I do not know if this helps , but there are three versions of bokeh in my pkgs folder . Two of them are bokeh 0.9.0 and one of them is bokeh 0.10.0 which is the one that comes up when I call conda . In the site-packages/bokeh folder there is a plotting.py . I tried to install it in python 3.4 and this is what the terminal returned"
"I 'm new with python and having some trouble regarding inheritance.i have this class : I 'm using another class that is overriding it : the problem is that I want to get func1 with all its operations and add for example op3 ( ) so that when using this class I will have the option to use op1 , op2 or op3.I 've tried using : but it just gave me the option of op3 without op1 and op2.I saw that there is `` super '' option but I 'm not sure I understand how to use it ."
"EDIT2 : As @ ShadowRanger pointed out , this is a Numpy phenomenon , not Python . But when doing the calculations in Python with list comprehensions ( so x+y becomes [ a+b for a , b in zip ( x , y ) ] ) then all arithmetic operations still take equally long ( although more than 100x as long as Numpy ) . However , when I use integer division in my real simulations , they run way faster . So the main question remains : even in Python , why do these tests suggest integer division is n't faster than regular division ? EDIT1 : versions : Python 3.5.5 , Numpy 1.15.0.It seems that in PythonNumpy , integer division is more expensive than regular division ( of integers ) , which is counter-intuitive . When testing , I get this : addition ( + ) ~ 0.1ssubtraction ( - ) ~ 0.1smultiplication ( * ) ~ 0.1sdivision ( / ) ~ 0.35sinteger division ( // ) ~ 1s ( ! ) Any ideas why this is happening ? Why is integer division not faster ?"
"I have a list of indexes , e.g . 0 ... 365 and I want to select few , randomly selected without replacement , contiguous sub-regions of this list.Does anyone have suggestions for the implementation of get_random_contiguous_region ( )"
"I 'm running a website using Django , and I import ipdb at the beginning of almost all of my scripts to make debugging easier . However , most of the time I never use the functions from the module ( only when I 'm debugging ) .Just wondering , will this decrease my performance ? It 's just that when I want to create a breakpoint I prefer to write : as opposed to : But I 've seen the second example done in several places , which makes me wonder if it 's more efficient ... I just do n't know how importing python modules relates to efficiency ( assuming you 're not using the module methods within your script ) ."
"I noticed that the cv_values_ from RidgeCV is always in the same metric regardless of the scoring option . Here is an example : The output is : As you can see , the cv_values_ are identical even though I specify different scoring . I would have expected the cv_values_ to be in the r2 metric in the first case . Am I missing something obvious ? I am quite new to scikit-learn . This is scikit-learn 0.18.1 running on Python 3.5.2 ."
"I chose these numbers randomly , but these results seem to be consistent -- - a float exponent is 25 % -50 % faster than an integer one . How are these handled differently ?"
I am trying to make a 'put ' method with curl everything is working fine and I got the JSON back : But for some reason when using the python requests module as follow : I get the following error : 500 Internal Server ErrorInternal Server ErrorThe server encountered an internal error and was unable to complete your request . Either the server is overloaded or there is an error in the application.NB : The server is up and running .
I would like to read sample csv file shown in belowI tried But it did n't work well.How can I read this csv ?
Can you access pandas rolling window object . Can I get as groups ? :
"I 'm trying to build a sequence to sequence model in Tensorflow , I have followed several tutorials and all is good . Untill I reached a point where I decided to remove the teacher forcing in my model . below is a sample of decoder network that I 'm using : As per my understanding the TrainingHelper is doing the teacher forcing . Especially that is it taking the true output as part of its arguments . I tried to use the decoder without training help but it appears to be mandatory . I tried to set the true output to 0 but apparently the output is needed by the TrainingHelper . I have also tried to google a solution but I did not find anything related . ===================Update============= I apologize for not mentioning this earlier but I tried using GreedyEmbeddingHelper as well .The model runs fine a couple of iterations and then starts throwing a run time error . it appears that the GreedyEmbeddingHelper starts predicting output different that the expectected shape . Below is my function when using the GreedyEmbeddingHelper this is a sample of the error that gets thrown after a coupe of training iterations : I 'm not sure but I guess the GreedyEmbeddingHepler should not be used for training . , I would appreciate your help and thoughts on how to stop the teacher forcing . thank you ."
"I documented some functions using autodoc . Locally it works well.When I commit to GitHub , the documentation is built on ReadTheDocs , but there are no functions that I documented with `` automodule '' . I added in my conf.py : But it does not help.Could this be related to the fact that I use my own C library ? ( I have my .c file that I compile to get .so )"
"As the final step in building a custom python , I need to add a myproject.pth.Currently I 'm doing this in a Makefile : but I would like to encapsulate this in a setup.py . Unfortuntately the setup.py docs do n't seem to cover this trivial case ! Any help appreciated . I tried this but it does n't work :"
"I found this decorator that times out a function here on Stack Overflow , and I am wondering if someone could explain in detail how it works , as the code is very elegant but not clear at all . Usage is @ timeout ( timelimit ) ."
"I just started dwelling into the realm of Qt ( coming from PyGTK ) and I 'm using PySide . So I found this great example on another answer here on stack exchange.My question is as follows , how do I make the inspector show up in the same window instead of a new one ? I understand I need to add the QWebInspector to another widget inside the main window ( a vbox for example ) , what I want to know is how to connect that event to the signal the context menu `` Inspect '' triggers . In PyGTK I would need to use .connect ( ) but I ca n't find the right SIGNAL for this specific action.Thanks for your time guys/gals"
"Recently I am developing a device base on raspberrypi 2b+ which connected to mpu9250 ( welding by myself ) .I could read 9-axis data correctly , but I noticed that each data input with different time differential : the figure shows the time differential between each two data.But I have used QTimer to make sure my code every 10ms reading mpu9250 once.So I tried this code on RaspberryPi 2b+ : And result : even these simple code still shows peaks on diagram , and it 's put me down ... Could anyone helps me solve these issue or explains what 's going on ?"
"Is it possible to change format of date from ( YYYY , MM , DD ) to ( DD , MM , YYYY ) ..output : Converting date formats python - Unusual date formats - Extract % Y % M % Dcurrent output is in ( YYYY , MM , DD ) formate ."
"I am trying to print all valid combination of parentheses in python using my own intuition . It almost worked but just that it does n't print few combinations . The code looks like thisFor n = 3 , it prints ( ) ( ) ( ) ( ) ( ( ) ) ( ( ) ) ( ) ( ( ( ) ) ) It works like this.In the first iteration ( ) ( ) ( ) will be printed , when the call returns to the immediate parent , it will slice of the list where it started to append first which will now be ( ) and run the next iteration of the loop to print ( ) ( ( ) ) and so onThe problem is I am somehow not able to capture this combination using this logic ( ( ) ( ) ) While I am thinking on how to fix it , if any python guru can suggest a fix to this , then that will be great . There are alternate solutions , but since I came very close , I want to improve mine ."
"I use click like this : So the name of the option and the name of the function parameter is the same . When it is not the same ( e.g . when you want -- id instead of -- bar ) , I get : I do n't want the parameter to be called id because that is a Python function . I do n't want the CLI parameter to be called different , because it would be more cumbersome / less intuitive . How can I fix it ?"
"As per another question , i 've been doing this ( python ) to push my timestamps into bigquery ( they come from a node-js app in a nr-of-miliseconds format ) : But they end up as this : I 've been printing debug information , and this is their last form before being inserted with insertAll :"
"I was trying to find the quickest way to count the number of items in a list matching a specific filter.In this case , finding how many odd numbers there are in a list.While doing this , I was surprised by the results of comparing a list comprehension vs the equivalent generator expression : I have also tried with L being a regular list , and different sizes , but in all cases the list comprehension wins.What is the genexp doing that causes it to be slower compared to the listcomp that creates a new list with 1 million items ... ? ( Btw , the fastest way I found was : x = 1 ; len ( filter ( x.__and__ , L ) ) . And yes , I know writing code like that kills kittens , I 'm doing it for the fun of it )"
"I am developing an agent based model in which I use different type of agents classes whose instances are assigned to different types of objects such as schools , companies , homes , etc . The problem I have is that I can not enforce reproducibility of runs when debugging , which makes the task very hard because of the model complexity . After a long investigation , I realised that the problem is linked to the order of sets ( built-in random and numpy random seeds are of course applied ) . Even when I set PYHTONHASHSEED=0 , I observe that the order of sets is random at each run . This makes each run of my model different when agents move.Of course I know that sets are not meant to have an order . I want to use them to make the model as light and fast an possible when removing agents from objects . I want them to behave randomly , except when I need to debug a specific run that raises an exception.I add the following code so that my claims can be verified . I always set PYTHONHASHSEED from command line via export before launching the code . I print the PYTHONHASHSEED value from code to check that the value has indeed been updatedThe strange thing is that if I use integers as set members instead of class instances , I can not detect the randomness . Does the problem have to do with the fact that the set members are class instances ? Why ? Of course I could remodel the way agents are assigned to model objects and replace sets with lists , but if possible I would like to understand the problem . The version I use is python 3.5.4"
"( Now that Django 1.1 is in release candidate status , it could be a good time to ask this . ) I 've been searing everywhere for ways to extend Django 's comments app to support authenticated comments . After reading through the comments model a few times , I found that a ForeignKey to User already exists.From django.contrib.comments.models : I ca n't seem to get my head around setting user . If I use comments as is , even if I 'm authenticated , it still seems to require the other fields . I 'm guessing I should override the form and do it there ? On top of that , if I use user , I should ignore the fact that user_name , user_email and user_url will be empty and just pull that information from a related profile model , correct ? While the answers could be quite trivial in the end , I 'm just surprised that it has n't been written or even talked about ."
"I have got into an issue or might quite possibly feature in turn ! Not sure , wondering ! ! In python 's datetime library , to get difference in time , as in below snippet.I would like to understand why datetime.datetime.now ( ) - datetime.datetime.now ( ) is producing output as -1 days , 86399 seconds whereas assigning current time to some variable and computing difference gives desired output 0 days , 4 seconds.The results seems to be bit confusing , it would be helpful if someone could decode whats going behindNote : I 'm using Python 2.7"
"Taking this class as an example : should my_fun be allowed to access and change self.attribute ? I can see that my editor does not colour self inside my_fun.This is what I am trying to implement : my_method should call an external function that requires a callback taking two arguments , arg1 and arg2 . I would like to extend the functionality by allowing this callback to use variables declared inside my_method , and also make the my_fun callback write some debugging info to self.log . I am not quite sure if this is the right design for this type of problem ."
"Yesterday i started exploring the genetic algorithms , and when i ended up with some basic theory , i tried to write simple GA on Python , that solves Diophantine equation . I 'm new to Python and GAs , so please , do n't judge my code strictly.ProblemI cant get any result due to premature convergence ( there is some no-return point ( n-population ) , population [ n ] == population [ n+i ] , where i is any integer . even the random mutatuion element cant change this , the generation is degradating very quickly ) GA is using crossover to breed , and weighted choice of parents.Q1 : Is there any design mistakes in mycode ( below ) ? Q1.2 : Do i need to add elitism ? Q1.3 : Do i need to change breedlogic ? Q2 : Is there realy needed deep copy ? Code : I tried to change breed and weighted random choice logic but with no results . This GA supposed to be work , i dont know , what 's wrong.I know that there are some GA libraries on Python , i 'm trying to understand them at the moment - it seems that they are quite complex to me . Sorry for mistakes , english is not my native language . Thank you for your understanding.NECROUPDATE : Store chromosomes in Gray Code , not in integer ."
"Yesterday this code was working fine both in local and production servers : From yesterday to today I 've upgraded GAE Launcher and changed the billing options ( I was using a free trial and now a paid account ) ( not sure if it has anything to do , but just to give extra information ) But today the code stopped working in local ( works fine in production ) This is the beginning of the error logThe warning shows a bucket object , but as soon as I try to iterate in the list I get the exception on the identity service.What is hapening ? Seems that I need to authorize local devserver gcs mockup , but I 'm not sure how.Remember this is only happening in devserver , not in production.Thanks for your help"
Trying to take a df and create a new column thats based on the difference between the Value in a group and that groups max : End up with a new column `` from_max '' I tried this but a ValueError : Thanks in Advance
"I 'm referring to a similar question : Find indices of a list of values in a numpy arrayIn that case we have a master array that is sorted and another array of which we want to find the index in the master array.The suggested solution was : But what if master is not sorted ? for example if i have two arrays like this where the first one is not sorted : i get : But instead i would like : i.e . the indices of items in search in master.How do i get them possibly with a native function of numpy ? ( not using [ np.where ( master==i ) for i in search ] ) ThanksEDIT : In this case the search array is a permutation of master . Then i would like to find how the index of master are permuted to give a permuted array like search.As general case , search array contain some item that maybe contained or not in the master such as :"
I can not find any way to determine if an array is record array or not : the type is always numpy.ndarraythe element type is always numpy.voidnow I use dtype.fields to determine it : Is there any official way to determine if an array is record array ?
"I came up with a regex string that parses the given text into 3 categories : in parenthesesin bracketsneither.Like this : My intention is to use the outermost operator only . So , given a ( b [ c ] d ) e , the split is going to be : It works fine given parentheses inside brackets , or brackets inside parentheses , but breaks down when there are brackets inside brackets and parentheses inside parentheses . For example , a [ b [ c ] d ] e is split as Is there any way to handle this using regex alone , not resorting to using code to count number of open/closed parentheses ? Thanks !"
"Here is what I mean : Apparently python iterates through the whole argument , even if the result is known to be the empty set beforehand . Is there any good reason for this ? The code was run in python 2.7.6 . ( Even for nonempty sets , if you find that you 've removed all of the first set 's elements midway through the iteration , it makes sense to stop right away . )"
I am having trouble in managing the insert-text signal emitted by the Gtk.Entry widget . Consider the following example : The position attribute I am receiving on the signal handler is always 0 . Unless I am misunderstanding this should it not be the position where the next text should be inserted ? In the end what I want to do is to validate the entry of text in the widget to restrict the characters that will be accepted . The way I plan to do this is similar to the example provided in the documentation in which all characters are transformed to uppercase .
"I am trying to get all matches for a RegExp from a string but apparently it 's not so easy in R , or I have overlooked something . Truth be told , it 's really confusing and I found myself lost among all the options : str_extract , str_match , str_match_all , regexec , grep , gregexpr , and who knows how many others.In reality , all I 'm trying to accomplish is simply ( in Python ) : The problem of the functions mentioned above is that either they return one match , or they return no match at all ."
"Consider the following code : It is missing an import re line and would fail with a NameError without it.Now , I 'm trying to use PyCharm 's Auto-Import feature : focusing on re and hitting Alt+Enter , which opens up the following popup : Now , if I choose Import 're ' option , Pycharm would insert the new import line at the top of the script : Looks almost good , except that it does n't follow PEP8 import guidelines : Imports should be grouped in the following order : standard library imports related third party imports local application/library specific imports You should put a blank line between each group of imports.In other words , there is a missing blank line between the two imports : Question is : is it possible to tell Pycharm to follow the PEP8 guidelines and insert a new-line between the lines with different import types on auto-import ? As a workaround , I 'm calling Optimize Imports after that organizes the imports correctly ."
"I have been working through the following tutorial provided for Django Autocomplete Light : https : //django-autocomplete-light.readthedocs.io/en/master/tutorial.htmlI have successfully implemented autocompletion for one of the fields in my form , however I am unable to complete the following section : https : //django-autocomplete-light.readthedocs.io/en/master/tutorial.html # creation-of-new-choices-in-the-autocomplete-formThe documentation states that I should be able to add in a feature which allows the user to create a new choice in the form if their required choice is unavailable . However the tutorial is not particularly clear in explaining how to do this.I am trying to implement a form in which the user can create a new Feedback by : Selecting from an autocompleting list of CategoriesSelecting a Message corresponding to the chosen CategoryIf the Category or Message they wish to choose is not available , they should be able to add to the existing choicesI have this partly implemented , but it does not appear to work correctly as if no Category is selected , the drop down for the Messages displays the list of Categories . However , if a Category is selected , the correct Messages are displayed as required.models.pyforms.pyviews.pyurls.pyI have searched for an answer to this for a while and have struggled to find a solution.I am also aware that my forms.py in particular may not have the most efficient/clean code and am open to suggestions to improve this . I have tried defining an init method however I was unable to do this successfully.Thanks in advance"
"In a pandas dataframe , a function can be used to group its index . I 'm looking to define a function that instead is applied to a column.I 'm looking to group by two columns , except I need the second column to be grouped by an arbitrary function , foo : How would foo be defined to group the second column into two groups , demarcated by whether values are > 0 , for example ? Or , is an entirely different approach or syntax used ?"
"I have a sample script to make a radial contour plot : ... which gives me a plot that looks like : However , I would like to have a logarithmic scale on the radius-axis . Does anyone know a convenient way to do this ?"
"I have two dataFrames : And I would like to create a final DataFrame which looks like : I tried something with pivot , but it does n't seem to do the jobCould you help me ?"
"TL ; DRMy question is simple - where is the code responsible to raise ConnectionResetError on cpython3 following a call to self._sslobj.read ( len , buffer ) on ssl.py ? BackgroundI 'm getting sometimes ConnectionResetError when trying to connect to S3 with ssl . this error occurs rarely so its tricky to reproduce it.What i 've triedlooking at ssl.py:631 gives me no further clues - we have to go deeper ! : i 've tried searching it on CPython repo but AFAICS nothing seems to raise it , i suspect its hidden in SSL implementation or on some mapping between OSError to ConnectionError subclasses.my final goal is to write py2 & py3 compatible code for handling this exceptions ( ConnectionError is new on py3 ) by comparing the module 's py2 & py3 versions that raises this error.Update - py2 & py3 catch for ConnectionError subclassesmy question origins was to find a way to catch ConnectionError and its subclasses on python2 & python3 , so here it is :"
"I have a python program that opens several urls in seperate tabs in a new browser window , however when I run the program from the command line and open the browser using The stderr from firefox prints to bash . Looking at the docs I ca n't seem to find a way to redirect or suppress themI have resorted to using Where log is a tempfile & then opening the other tabs with webbrowser.open_new . Is there a way to do this within the webbrowser module ?"
"Whenever I export a fastai model and reload it , I get this error ( or a very similar one ) when I try and use the reloaded model to generate predictions on a new test set : Minimal reprodudeable code example below , you just need to update your FILES_DIR variable to where the MNIST data gets deposited on your system : Output : `` RuntimeError : Input type ( torch.cuda.FloatTensor ) and weight type ( torch.cuda.HalfTensor ) should be the same '' Stepping through the code statement by statement , everything works fine until the last line pred = ... which is where the torch error above pops up.Relevant software versions : Python 3.7.3fastai 1.0.57torch 1.2.0torchvision 0.4.0"
"I am trying to use ggplot and ggimage to create a 3D scatterplot with a custom image . It works fine in 2D : I 've tried two ways to create a 3D chart : plotly - This currently does not work with geom_image , though it is queued as a future request.gg3D - This is an R package , but I can not get it to play nice with custom images . Here is how combining those libraries ends up : Any help would be appreciated . I 'd be fine with a python library , javascript , etc . if the solution exists there ."
"Is there a `` lenient '' JSON Parser for Python ? I keep getting ( handwritten ) JSON files such as this : The program that actually consumed those monstrously malformed JSON files somehow does n't puke on those errors . That program is written using C # , by the way.I 'm writing some scripts in Python that will perform things based on those JSON files , but it keeps crashing ( correctly ) on those mistakes.I can manually edit those .json files to be standard-compliant ... but there are a LOT of them and thus it 's too effort-intensive -- not to mention that I will have to keep editing new incoming JSON files , urgh.So , back to my question , is there a lenient JSON parser that can consume those malformed JSON files without dying ? Note : This question concerns only trailing comma of last object ; it does NOT handle block-comments and/or inline comments.Edit : What the ... I just received a JSON file in which the creator decided to remove leading zero for 0 < numbers < 1 ... -_-And I discovered a file where the comment is embedded ... : fuming_red : I 'll update the example above to reflect my additional `` findings '' ..."
I use setuptools 'tests_require ' to specify dependencies required for testing my package.I have begun using wheel packagingand building a directory of the wheels for my current packages and all their dependencies.However I would like to also build wheels for all the packages listed in any of the packages tests_require . Obviously I could explicitly specify the requirements in a duplicate test_requirements.txt file : But then I am duplicating the dependencies in both the test requirements file and in the tests_require list . I could read the test requirements file into the tests_require but that seems to be misusing requirements files which to my understanding are intended to allow users to be in control of specifying an environment of packages that are known to work together .
"I found a similar question which is quite a bit outdated . I wonder if it 's possible without the use of another library . Currently , the forms.ValidationError will trigger the form_invalid which will only return a JSON response with the error and status code.I have an ajax form and wonder if the usual django field validations can occur on the form field upon an ajax form submit . My form triggering the error : The corresponding View 's mixin for ajax : The View : On the browser console , errors will come through as a 400 Bad Request , followed by the responseJSON which has the correct ValidationError message . Edit : Any way to get the field validation to show client-side ? edit : Additional code : Full copy of data received on front-end : The form in the template is rendered using Django 's { { as_p } } : Javascript :"
"I have two NxN matrices that I want to multiply together : A and B . In NumPy , I used : However , I happen to know that for matrix B only row n and column n are non-zero ( this comes directly from the analytical formula that produced the matrix and is without a doubt always the case ) .Hoping to take advantage of this fact and reduce the number of multiplications needed to produce C , I replaced the above with : Analytically , this should reduce the total complexity as follows : In the general case ( not using any fancy tricks , just basic matrix multiplication ) C = AB , where A and B are both NxN , should be O ( N^3 ) . That is , all N rows must multiply all N columns , and each of these dot products contains N multiplications = > O ( NNN ) = O ( N^3 ) . # Exploiting the structure of B as I 've done above however should go as O ( N^2 + N^2 ) = O ( 2N^2 ) = O ( N^2 ) . That is , All N rows must multiply all N columns , however , for all of these ( except those involving ' B [ : , n ] ' ) only one scalar multiplication is required : only one element of ' B [ : , m ] ' is non-zero for m ! = n. When n == m , which will occur N times ( once for each row of A that must multiply column n of B ) , N scalar multiplications must occur. # However , the first block of code ( using np.dot ( A , B ) ) is substantially faster . I 'm aware ( via information like : Why is matrix multiplication faster with numpy than with ctypes in Python ? ) that the low level implementation details of np.dot are likely to blame for this . So my question is this : How can I exploit the structure of matrix B to improve multiplication efficiency without sacrificing the implementation efficiency of NumPy , without building my own low level matrix multiplication in c ? This method is part of a numerical optimization over many variables , hence , O ( N^3 ) is intractable whereas O ( N^2 ) will likely get the job done.Thank you for any help . Also , I 'm new to SO , so please pardon any newbie errors ."
"The is operator is used test for identity.I was wondering if the is operator and id ( ) function call any __magic__ method , the way == calls __eq__.I had some fun checking out __hash__ : Think about dict b and the value of b [ a ] Every subsequent lookup of d [ a ] is either a KeyError or a random integer.But as the docs on the special methods state [ the default implementation of ] x.__hash__ ( ) returns id ( x ) .So there is relation between the two , but just the other way around . I 've seen many questions on is and id here , and the answers have helped many confused minds , but I could n't find an answer to this one ."
"I am reading a source-code which downloads the zip-file and reads the data into numpy array . The code suppose to work on macos and linux and here is the snippet that I see : This function is used in the following context : It is not hard to see what happens here , but I am puzzled with the purpose of newbyteorder ( ' > ' ) . I read the documentation , and know what endianness mean , but can not understand why exactly developer added newbyteorder ( in my opinion it is not really needed ) ."
I 'm working with some models that has to return a sum of model fields . Is it better to override the save method on the model or just create a custom method that returns the sum . Is there any performance issues with either of the solutions ? Option 1 : Overriding the save method.Option 2 : Custom method
"So I had a SQLAlchemy Table with a JSON column : And I tried to update the column with the dict # update method like so : However , that did n't work . Edit : What I meant is , the updates were n't being persisted unto the database.What did work , was this : I suspect it has something to do with `` immutability '' or the ORM only notices changes on direct assignment , or something . Does anyone know exactly why ?"
I have a list of list like belowAnd want grouping based on column1 and column2 . Does python provide anything in lists that i can get the below result
"Correct value : Incorrect value : I wonder if that _utcoffset attribute is used in utcoffset ( ) method , why the method is working while the attribute is wrong.Looks like a bug anyway.Nothing changes if you replace Asia/Tehran with IranOS : Linux Mint 15 ( Olivia ) Using Python 2.7"
"I 'm very new to Python , so sorry for the probably simple question . ( Although , I spent now 2 hours to find an answer ) I simplified my code to illustrate the problem : This yields : Why does the remove command also affects the list 'side ' ? What can I do to use a copy of 'side ' , without modifying the list ? Thank you very much Edit : Thank you very much for the good and comprehensible answers !"
"I have some code that needs to hash certain data , then later on , in another process , continue the hash with more data . Is there a way to create an object , either from the md5 or hashlib modules , which has a different initial value than 'd41d8cd98f00b204e9800998ecf8427e ' ? What I mean is something similar to : Note : the less desirable approach would be to save the original md5 object and restore it later , but afaik HASH objects are non-pickleable ."
"I have a NumPy array consisting of 0 's and 1 's like above . How can I add all consecutive 1 's like below ? Any time I encounter a 0 , I reset.I can do this using a for loop , but is there a vectorized solution using NumPy ?"
How can I cast an int32 tensor to float32 in tensorflow . I do n't understand what tf.cast does . It does not seem to do anything.outputs ;
"Given a directory with a large number of small files ( > 1 mio ) what 's a fast way to remember which files were already processed ( for a database import ) .The first solution I tried was a bash script : For a smaller sample ( 160k files ) this ran ~8 minutes ( without any processing ) Next I tried a python script : This runs in less than 2 mins . Is there a significantly faster way that I 'm overlooking ? Other solutions : Moving processed files to a different locations is not convenient since I use s3sync to download new filessince the files have a timestamp as part of their name I might consider to rely on processing them in order and only compare the name to a `` last processed '' datealternatively I could keep track of the last time a processing ran , and only process files that have been modified since ."
Running subprocess wo n't handle curly braces correctlyThe same program will work on a different machine with python 2.7.2 . Both systems use bash shells.Do you the reason and how can I fix it ? EDIT : Invoking the command directly from the command line returns the correct result :
"What is the best way to find the maximum number of consecutive repeated nan in a numpy array ? Examples : Input 1 : [ nan , nan , nan , 0.16 , 1 , 0.16 , 0.9999 , 0.0001 , 0.16 , 0.101 , nan , 0.16 ] Output 1 : 3Input 2 : [ nan , nan , 2 , 1 , 1 , nan , nan , nan , nan , 0.101 , nan , 0.16 ] Output 2 : 4"
"I have a Python package that I 've created and I 'm using setuptools.setup ( ) to install it . The package includes executable scripts , which use the scripts parameter of the setup ( ) function.I 'm installing like this : After installation , the executable scripts are located in /usr/local/bin . The only problem is that the permissions are : Instead of : Anybody know either how I can specify the permissions of the output executables or why the default is n't allowing anyone to execute ? FYI : My umask is 0027 and the permission of /usr/local/bin/ is drwxr-xr-x ( owner=root group=root ) . All executable scripts have -rwxr-xr-x permissions in the development area ."
"I 'm trying to use the user_registered signal in order to set up default roles for users when they register using flask-security as in the following link : Setting Default Role in Flask SecurityIn my searches I can see that there was a bug that was already addressed for this in flask-security : Not getting signal from flask-security , Fix - user_registered signal problemI 've tried the following to prove if the signal is received by the handler without any luck : This , however , never gets called even though the user gets registered and the signal should be sent . If it helps I 've set the flask-security configuration as follows : Signals from Flask-Login and Flask-Principal are working for me as I managed to confirm that the following code snippets successfully print when the signals are sent : For my setup I am using python 3.3 ( anaconda ) and using the following : Flask==0.10.1 , flask-login==0.2.11 , flask-principal==0.4.0 , flask-security==1.7.4 , blinker==1.3 . Having looked at the signals in both flask-login and flask-security I 'm not sure why the flask-security signals would not be working.EDIT : If I add print ( user_registered.receivers ) to a route in my app it will show that I have a receiver : { 139923381372400 : < function user_registered_sighandler at 0x7f42737145f0 > } . If I put this same print statement within the registerable.py of flask-security just before the user_registered.send ( app._get_current_object ( ) , user=user , confirm_token=token ) then it lists no receivers : { } EDIT2 : Problem appears to be related to using python 3.3 . I created a python 2.7 environment and the user_registered code worked as expected.Full code to reproduce : base.html template :"
"Given a corpus/texts as such : I could simply do this to get a dictionary with word frequencies : But if the aim is to achieve an ordered dictionary from highest to lowest frequency , I will have to do this : Imagine that I have 1 billion keys in the Counter object , iterating through the most_common ( ) would have a complexity of going through a corpus ( non-unique instances ) once and the vocabulary ( unique key ) . Note : The Counter.most_common ( ) would call an ad-hoc sorted ( ) , see https : //hg.python.org/cpython/file/e38470b49d3c/Lib/collections.py # l472Given this , I have seen the following code that uses numpy.argsort ( ) : Which is faster ? Is there any other faster way to get such an OrderedDict from a Counter ? Other than OrderedDict , is there other python objects that achieves the same sorted key-value pair ? Assume that memory is not an issue . Given 120 GB of RAM , there should n't be much issue to keep 1 billion key-value pairs right ? Assume an average of 20 chars per key for 1 billion keys and a single integer for each value ."
"Mercurial 's vast publishing options include HgWeb ( and formerly HgWebDir ) , which is almost perfect for my needs . HgWeb 1.6 supports multiple repositories , including collections.So hgweb.config could contain something like this : So , the above folders ( collection1 , collection2 ) might each contain about 10-20 repositories , as subfolders , in the above locations.My question is , is there any hg extension , or configuration technique that will allow me to push a clone up to a collection being served by hgweb ? My usual approach on Linux is to login remotely using SSH and then do an hg clone of my developer workstation copy , which I momentarily serve up using ` hg serve ' . I find that I work with some Linux-savvy developers , who have no problem ssh-ing into a box , and doing stuff with hg . But windows users ( a ) do n't grok ssh , and ( b ) do n't know Linux command line environments , and want a way either with their IDE version control plugins , or GUI tools , or with a CGI Page ( written by me in python ) , to remotely pull a clone , giving them effectively a way to push or publish their HG Repository up to a central repository without recourse to any ssh access to the remote server system . In the odd case that a centralized mercurial server was running on Windows , you might not even be ABLE to ssh into the remote server . It seems to me the answer is `` no you ca n't do this '' , unless I write my own extension for hgweb , which I am about to do . But if there is a way to do this already in hg , I would like to know it.Related questions : Setup of mercurial on shared hostingMultiple central repositoriess with mercurial"
"I 'm having trouble locating some problem images in a dataset.My model starts training , but I get the following error : So I 've written a small script that runs before I generate my TFRecords to try and catch any problem images . This is basically the tutorial code but with a batch size of 1 . This was the simplest way I could think of to try and catch the error.This duly crashes , but the exception is n't properly handled . It just throws the exception and crashes . So two questions : Is there a way I can force it to be handled correctly ? It looks like no Tensorflow , try and except does n't handle exceptionIs there a better way to look for corrupt inputs ? I 've isolated an image that fails , but OpenCV , SciPy , Matplotlib and Skimage all open it . For example , I 've tried this : I get four matrices printed out . I assume these libraries are all using libpng or something similar.Where image 1258 then crashes Tensorflow . Looking at the DecodePng source , it looks like it 's actually crashing the TF png library.I realise I could probably write my own dataloader , but that seems like a faff.EDIT : This also works as a snippet :"
"Good afternoon , I 'm very new to Python , but I 'm trying to write a code which will allow me to download all of the posts ( including the `` notes '' ) from a specified Tumblr account to my computer.Given my inexperience with coding , I was trying to find a pre-made script which would allow me to do this . I found several brilliant scripts on GitHub , but none of them actually return the notes from Tumblr posts ( as far as I can see , although please do correct me if anyone knows of one that does ! ) .Therefore , I tried to write my own script . I 've had some success with the code below . It prints the most recent 20 posts from the given Tumblr ( albeit in a rather ugly format -- essentially hundreds of lines of texts all printed into one line of a notepad file ) : However , I want the script to continuously print posts until it reaches the end of the specified blog . I searched this site and found a very similar question ( Getting only 20 posts returned through PyTumblr ) , which has been answered by the stackoverflow user poke . However , I ca n't seem to actually implement poke 's solution so that it works for my data . Indeed , when I run the following script , no output at all is produced.I should note that there are several posts on this site ( e.g.Getting more than 50 notes with Tumblr API ) about Tumblr notes , most of them asking how to download more than 50 notes per posts . I 'm perfectly happy with just 50 notes per post , it is the number of posts that I would like to increase.Also , I 've tagged this post as Python , however , if there is a better way to get the data I require using another programming language , that would be more than okay.Thank you very much in advance for your time !"
"I 've previously had trouble with trying to apply SWIG 's OUTPUT typemaps to class types , and asked this previous question.The answers I got there were helpful , but still require me to ask SWIG to do something like : This does n't seem to work for me on SWIG 3.0.6 , with the following message : Warning 453 : Ca n't apply ( exportedClassType & OUTPUT ) . No typemaps are defined.From looking at the documentation : Be aware that the primary purpose of the typemaps.i file is to support primitive datatypes . Writing a function like thisvoid foo ( Bar *OUTPUT ) ; may not have the intended effect since typemaps.i does not define an OUTPUT rule for Bar.It does n't look like this is supported . So I guess my question is , what combination of typemaps do I need to define in the interface.i file so that the generated wrapper code for class types goes from this : to something more like this ? I do n't get what in/out/argout typemaps are needed so that SWIG_Python_AppendOutput etc . is called for my exportedClassType . Can anybody please give me some pointers ? No pun intended ."
"I need a function that reads input into a buffer as raw_input ( ) would , but instead of echoing input and blocking until returning a full line , it should supress echo and invoke a callback every time the buffer changes.I say `` buffer changes '' instead of `` character is read '' because , as raw_input ( ) , I 'd like it to be aware of special keys . Backspace should work , for example.If I wanted to , for example , use the callback to simulate uppercased echo of input , the code would look like this : How can I achieve this ? NOTE : I 've been trying to use readline and curses to meet my ends , but both Python bindings are incomplete . curses can not be made to start without clearing the whole screen , and readline offers a single hook before any input begins ."
"I have a set of 3d coordinates that was generated using meshgrid ( ) . I want to be able to rotate these about the 3 axes.I tried unraveling the meshgrid and doing a rotation on each point but the meshgrid is large and I run out of memory.This question addresses this in 2d with einsum ( ) , but I ca n't figure out the string format when extending it to 3d.I have read several other pages about einsum ( ) and its format string but have n't been able to figure it out.EDIT : I call my meshgrid axes X , Y , and Z , each is of shape ( 213 , 48 , 37 ) . Also , the actual memory error came when I tried to put the results back into a meshgrid . When I attempted to 'unravel ' it to do point by point rotation I used the following function : I looped over the result with the following : After the rotation I will be using the points to interpolate onto ."
"I have a master class for a planet : I also have a few classes that inherit from Planet and I want to make one of them unable to be destroyed ( not to inherit the destroy function ) Example : So when this is run , it should produce an error like :"
"I currently have a project with the following .travis.yml file : Locally , tox properly executes and runs 35 tests , but on Travis CI , it runs 0 tests.More details : https : //travis-ci.org/neverendingqs/pyiterable/builds/78954867I also tried other ways , including : They also could not find any tests.My project structure is Like This : Are the project/folders structured incorrectly ?"
"The following code runs too slowly even though everything seems to be vectorized . The problem seems to be that the indexing operation is implemented as a python function , and invoking A [ i , j ] results in the following profiling outputNamely , the python function _get_single_element gets called 100000 times which is really inefficient . Why is n't this implemented in pure C ? Does anybody know of a way of getting around this limitation , and speeding up the above code ? Should I be using a different sparse matrix type ?"
"I have the following pandas dataframe : and am looking for the most efficient way to fill the missing years with a default value of 0 for the column numberThe expected output is : The dataframe that I have is relatively big , so I am looking for an efficient solution.Edit : This is the code that I have so far : Result"
I am using docopt in my simple Python program : If I run : The expected behaviour is to buy a random quantity of eggs between the values 100 and 115 at the price 0.25 . This works without problems at least when it comes to interpreting the arguments . In other words docopt gets everything as intended : However sometimes I do not want to buy a random amount of eggs but a specific amount . In this case the -- quantity option takes only one argument : But this fails as docopt interprets -- price 0.25 as a repeating element of -- quantity and loses the value of < price > : How can I get other options to work after repeating elements ?
"I am using the following code to open an existing Excel file in python 3.6 , Excel 2016 : sourceSo when I do : where : my_file is a str like filename.xlsx and sheet is a str with the sheet name.It bugs me with an error stating that the sheet name mentioned does not exist.I also tried to replace : self.ws = self.wb.get_sheet_by_name ( sheet ) withbut I still receive the same error ."
"My problem is the following : I have some python classes that have properties that are derived from other properties ; and those should be cached once they are calculated , and the cached results should be invalidated each time the base properties are changed.I could do it manually , but it seems quite difficult to maintain if the number of properties grows . So I would like to have something like Makefile rules inside my objects to automatically keep track of what needs to be recalculated.The desired syntax and behaviour should be something like that : So , is there something like this already available or should I start implementing my own ? In the second case , suggestions are welcome : - )"
I am writing python tests with unittest and running the tests from the command line withWhen I include numpy in one of my tests it tries to test the numpy packages as well . Example output : How can I correctly ignore the numpy tests with the -I -m or -e parameters ? I could pipe this to grep -v numpy but that is not cool .
"I am working with a .dll that contains a single call which returns an array of function pointers . GetMyApi ( ) returns a pointer to a struct , which is an array of function pointers . The functions themselves have different individual inputs and outputs . What I have tried so far : C code that I ca n't easily alter : Code in C : Python effort : At this point , I am just trying to get the first function of the function list returned to work . Any help pointing me in better direction is appreciated ."
"I am a bit suprised/confused about the following difference between numpy and PandasHowever : In other words , numpy does not include the stop index in start : stop notation , but Pandas does . I thought Pandas was based on Numpy . Is this a bug ? Intentional ?"
"I want to create a small NumPy integer to save memory . However , I noticed thatprints 12 , so it seems that numpy.int8 ( ) is generating 12 bytes instead of 1 byte of data . Why is this ?"
"I create a argparser like this : However , only for choice `` x '' and not for choices `` y , z '' , I want to have an additional REQUIRED argument . For eg.How can I accomplish that with ArgumentParser ? I know its possible with bash optparser"
Here 's some code from Richard Jones ' Blog : My question is : how the heck did he do this ? How can the context manager access the scope inside the with block ? Here 's a basic template for trying to figure this out :
"I have a NumPy array with integer values . Values of matrix range from 0 to max element in matrix ( in other words , all numbers from 0 to max data element presented in it ) . I need to build effective ( effective means fast fully-vectorized solution ) for searching number of elements in each row and encode them according to matrix values . I could not find a similar question , or a question that somehow helped to solve this.So if i have this data in input : desired output is : I know how to solve this by simply counting unique values in each row of data iterating one by one , and then combining results taking in account all possible values in data array . While using NumPy for vectorizing this the key problem is that searching each number one by one is slow and assuming that there are a lot of unique numbers presented , this can not be effective solution . Generally both N and unique numbers count is rather large ( by the way , N seem to be larger than unique numbers count ) .Has somebody have great ideas ? )"
"I want to create a wordcloud . When my string is in English , everything works fine : But when I 'm doing the same in Hebrew , it does n't detect the font , and I get only empty rectangles : Any ideas ?"
"I have a long running Twitter scraping script that occasionally hangs with stack traces ending inMy question is `` why ? '' and `` what can I do to fix it ? `` .It 'll typically last a week or so before this happens . This last time , 3 of the 4 threads that deal with tweepy hung ( the fourth was waiting for info from a hung thread ) . Strangely , there was quite a long delay between the threads hanging : firstly , the thread calling api.followers_ids ( ) hung , then about 12 minutes later the thread calling api.friends_ids ( ) hung , then 1 hour 12 minutes later ( ! ) the thread calling api.search ( ) hung . There were many api calls in between all of these.I have a little code in there to dump it 's stack traces when I send a QUIT signal , and I got something like the following for the hung threads . They are all identical from ( and including ) the second entry ( the tweepy/binder.py , line 185 , in _call part ) . The other two got there from tweepy/cursor.py , line 85 in next and tweepy/cursor.py , line 60 , in next : There were a few tweepy errors around the times the threads hung . That 's not too unusual , though the number is slightly more than normal . The fourth one looks interesting though.. it happened immediately before that thread hung . [ Errno 110 ] : Connection timed out about 7 mimutes before the last followers_ids ( ) call ( with many assorted api calls in between ) [ Errno 104 ] Connection reset by peer about 3 minutes after ( again , several successful calls between ) [ Errno 110 ] Connection timed out about 1.5 minutes before the last friends_ids ( ) call . This was in the api.search ( ) thread , which had been waiting since about 5 minutes before the first threaad hung - a total wait of about 15 minutes . [ Errno 104 ] Connection reset by peer about 2 milliseconds before the last news from the friends_ids ( ) thread , and was in the same thread . The pages of friends ids just collected appear all be ok and there was n't an error from those calls . [ Errno 104 ] Connection reset by peer in the search thread , about 17 minutes after the friends_ids thread hung and nearly an hour before the search thread hung.A Failed to send request TweepError with no reason about 1.5 minutes later.3 more reason-less Failed to send request 's and a [ Errno 104 ] Connection reset by peer over the next 45 minutes.About 15 error-free minutes with lots of search and lookup_users calls before the search thread finally hung ."
"I have a post-receive hook that is running as user 'git ' . I have a virtualenv /python/ve//bin/activate that is readable by git . Running : works fine for a user in the git group.When it runs as a post-receive hook after a push , I get the error `` source : not found '' .I 'm not sure where else to look - any hints much appreciated ."
"As I do not succeed in integrating boto3 to Google App Engine , I 'm trying to use APIs directly using the docs : as Polly ( text to speech API ) uses Signature Version 4 process , I refered to : http : //docs.aws.amazon.com/general/latest/gr/sigv4-signed-request-examples.htmlHere is the given script : And I get the error message : How can I debug that ? I suspect that it is not a secret keys problem as they work when in boto3"
"Normally I have been using GNU Octave to solve quadratic programming problems.I solve problems likeWith subject toWhere lb and ub are lower bounds and upper bounds , e.g limits for xMy Octave code looks like this when I solve . Just one simple lineThe square brackets [ ] are empty because I do n't need the equality constraints So my question is : Is there a easy to use quadratic solver in Python for solving problemsWith subject toOr subject to"
"I 'm writing a concolic engine for Python using the sys.settrace ( ) functionality.The main task during such kind of execution is to record the constraints on the input variables . The constraints are nothing else than the conditions of the if statements , that create two branches ( the 'then ' and the 'else ' branch ) .When an execution is complete , the engine chooses a constraint and finds appropriate values for the inputs so that the execution will go the down along the other branch ( at execution x it goes the 'then ' branch , at execution x+1 it goes along the 'else ' branch ) .This is to have a bit of context on why I doing what I 'm trying to do ... By combining settrace ( ) and the dis module , I get to see the bytecode of each source line , just before it is executed . This way I can easily record the if conditions as they appear during execution.But then I have the big problem . I need to know which way the if went , which branch the execution took . So if my code is something like : at a certain point my tracing thing will see : then the python interpreter will execute the if and jump ( or not ) somewhere . And I will see : So is the instruction t + 1 in the `` then '' branch or in the `` else '' one ? Keep in mind the trace function sees only some bytecode in the current block.I know of two way to do this . One is to evaluate the condition to see exactly whether it is true or false . This works only if there are no side-effects.The other way is to try to look and the instruction pointer at t + 1 and try to understand where we are in the code . This is the way I am using right now , but it very delicate because at t + 1 I could find myself somewhere completely different ( another module , a builtin function , etc ) .So finally , the question I have is this : is there a way to get from Python itself , or from a C module/extension/whatever , the result of the last conditional jump ? In alternative , are there more fine-grained tracing options ? Something like execute bytecode one opcode at a time . With the settrace ( ) functionality the maximum resolution I get is whole source code lines.In the worst case , I think I can modify the Python interpreter to expose such information , but I would leave that as last resort , for obvious reasons ."
"I am working on a problem statement that requires me to fill the rows of missing dates ( i.e dates in between two dates in columns of a pandas dataframe ) . Please see the example below . I am using Pandas for my current approach ( mentioned below ) .Input Data Example ( which has around 25000 rows ) : Output Expected : I know of more traditional way to achieve this ( my current approach ) : Iterate over each row.Get the days difference between two date columns.If the date is the same in both columns , just include one row for that month and year in output dataframeIf dates are different ( diff > 0 ) , then get all ( month , year ) combination for each date difference row and append to new dataframe Since the input data has around 25000 rows , I believe the output data will be extremely very large , so I am looking for more Pythonic way to achieve this ( if possible and faster than iterative approach ) !"
"My tensorflow 2.0.0beta1 runs normally , but I can not install tensorflow-text using the command pip install tensorflow-text ( as described on the tensorflow page ) . I can find it using pip search tensorflow-text but I am getting an errorThere are no requirements for this package ( i.e . a specific python version ) . I am running on windows , using conda , python 3.6.9"
"I have a csv which is generated in a format that I can not change . The file has a multi index . The file looks like this.The end goal is to turn the top row ( hours ) into an index , and index it with the `` ID '' column , so that the data looks like this.I have imported the file into pandas ... But that gives me three unnamed fields : My final step is to stack on hour : But I a missing what comes before that , where I can index the other columns , even though there 's a blank multiindex line above them ."
"Below is a partial class definition of mine : The last line of the partial class definition , target_dic = { let : font.render ( let , True , WHITE , BG ) for let in list ( `` ABCDEFGHJKLMNPRSTUVWX '' ) returns the error : global name 'font ' is not defined . Fair enough.However , I tried the following test case and got no error : Why should the first case not work ? Does n't the member font exist by the time the dictionary comprehension is reached ? Do I need to move these operations to __init__ or is it possible to define the list exactly once when the class object is created ? EDIT : For clarity , I 'd like to be able to populate the list at class object creation time to cut down on the time spent creating Trial objects ."
"I am using Anaconda 2.1.0 and Basemap installation is successfulI have geos version 3.3.3 and proj4 4.8.0-0 . However , I keep getting the following error when I try to project ( lat , lon ) degrees to a planar : My code looks like this :"
I used the following import : but got this error : What does this error mean ?
"I am writing a python command line interface tool on top of cement.Cement is working quite well for standard argument parseing . However , I want to be able to add a specific amount of non-flagged arguments . Let me explain.Typical command line tool : Now , lets say I want to add some arguments without flags , for example how cd worksmy/dir is an argument without a flag.Is there anyway to do this with cement ? My current example cement app : So lets say I wanted to do myapp command1 some/dir some_stringis there a way to parse those arguments ?"
I just noticed sys.argv is visible in a imported script.A.pyB.pyYieldsThis is nice since now I do n't have to do the argument parsing in the main script ( aka . manage.py ) .The question is : can I rely on this behavior ? Are there cases where this will not work ?
If I have the following code in a daemon thread and the main thread does not do invoke a join on the daemon . Will the file close safely since it is used inside `` with '' once the main thread exits or no ? Anyway to make it safe ? Thanks : D
"What is the easiest way to do cycles in c ++ like in python ? I mean something simple and one-line like thisbut not like this : Or thisOf course , it is not difficult to use these examples or just for ( ; ; ) but I hope there is a way to do it briefly and succinctly in python ."
The function works in python2 : But when I 'm trying in python3 it shows invalid syntax : Is it the bracket problem ? How can I fix this in py3 ?
"I have 20+ MySQL tables , prm_a , prm_b , ... with the same basic structure but different names , and I 'd like to associate them with Django model classes without writing each one by hand . So , feeling ambitious , I thought I 'd try my hand at using type ( ) as a class-factory : The following works : But if I try to generate the model classes as follows : I get a curious Exception on the type ( ) line ( given that __module__ is , in fact , in the prm_class_attrs dictionary ) : So I have two questions : what 's wrong with my second approach , and is this even the right way to go about creating my class models ? OK - thanks to @ Anentropic , I see that the items in my prm_class_attrs dictionary are being popped away by Python when it makes the classes . And I now have it working , but only if I do this : not if I set the Meta class as an attribtue withI do n't really know why this is , but at least I have it working now ."
"Imagine an app with the following url.py : And this views.py : Obviously , I want number to be always taken as an int . When querying /double/5/ , I 'll always expect to get 10 , not 55 . Are there any good way to handle the parameters typing within Django urls ?"
"The problemI 'm trying to save a serialized object ( using cPickle ) into a Cassandra 1.2 column , I 'm using the python cql library . I 've already tried defining the column as text ( utf8 string ) and blob , in both cases I 'm receiving the same error : The object is a Python dict : The error is this : And looking at the executed CQL statement I can see some '\ ' characters after pickling the object , for instance : Part of the pickled objectMy questionsWhat is the usual way of serializing a python dict ( including datetimes ) to save it into cassandra 1.2 using the cql library ? Is there a better or more straightforward way of doing this ? Thanks in advance !"
suppose I have an numpy array with structure like this : and I want to save it to a csv file that looks like thisthe columns with shorter length just fill with blank . How can I do that ?
"I am trying to save an Estimator and then load it to predict as required . Part where I train the model : I then save the model as follows : I now load the model and give the test set to it for prediction : This gives me predictions like : class and prob are two things that I am predicting . Now , if I predict the output with the same test set without saving and loading the model : then I get the output as follows : which is correct . Notice the difference between two outputs is that the class in the second one is increasing 1 by 1 while the class in the first case shows 0s at most places.Why is there a difference in the prediction ? Am I saving the model in a wrong way ? Edit 1 : From this question , I came to know that Estimator readily saved the checkpoints if the model_dir argument is given . And loads the same graph when the same model_dir is referred to . So I did this while saving the model : I checked and found that checkpoints have been stored at E : /models/ . Now , the part where I want to restore the model , I wrote : The logs gave me : which show that the model has been successfully reconstructed from the given model_dir . I then try to predict the output on the test data but only to get the same output as the previous one : Most of the classes are again 0 . Why is this happening ? Is there any alternative that could help me ?"
I am trying to install llvmpy on ubuntu 14.04. sudo pip install llvmpy fails . The end of the output isI have llvm version 3.4 installed . The full output is at http : //paste.ubuntu.com/8074574/ .
"I am using the fantastic Python social auth with Django.However , at the moment , everytime the process is invoked , a new user is created . I only need the tokens ( access_token and refresh_token ) from the process . How can this be achieved ? Via some sort of pipeline ? This is my pipeline.py code at the moment ( abbreviated ) : And the corresponding settings.py file :"
"I 'm trying to make a PATCH request using to the Django Rest Framework but get the following error : I understand that JSONField ( ) could give some issues so I have taken care of that by adding to_native and from_native , But , I 'm still running into this issue . I do n't think JSONField ( ) is the problem here at all , but still worth mentioning.I believe I 'm doing something fundamentally wrong in how I am trying to update the related field.Code below ... Models : Serializers : PATCH : I have also tried : But again giving the same error : [ { `` non_field_errors '' : [ `` Invalid data '' ] } ]"
"Let 's say I have a script like the following , and I ca n't edit it : later I call this script with : But it is n't populating the file.log file , but outputting the logs in console . Q : Is there a way to , despite configuring logging handler to StreamHandler , still output the logs into a file from command line ?"
"I have an algorithm that can generate a prime list as a generator : But if I put the lambda function into the filter function directly , like below : I can get only an odd list , not a prime list . It seems the filter function does n't work.What can I do ?"
"I have a data frame like this : I would like to take count of unique values in this column and add the count as a variable by itself . At the end , it should look like this : I am able to take the unique values of the column using the below code : However , I am not sure how to match these in a loop in python so that i can get the desired results in python . Any sort of help is much appreciated . I am not able to find a equivalent answer in stackoverflow . If there is anything please direct me there . Thank you ."
"I am looking for a way to naturally sort Django 's QuerySets . I found a similar question , but it did not focus on QuerySets . Instead they are doing it directly in Python.So here is my problem . Let 's say I have this model : In the Django Admin Interface , I want to use a filter , which sorts them alphanumeric . Currently , they are sorted this way : What I 'd expect is a list of [ `` BA 1 '' , `` BA 2 '' , ... ] . I found admin.SimpleListFilter in the official documentation , which sounds quite suitable . But what I get in the queryset ( ) function is a QuerySet , which can not be sorted in a natural way , because it does not contain the elements , but only the query to the database.The order_by method on QuerySet gives the same ordering as it can be seen in the image . Is there a way to manipulate the QuerySet to get it naturally sorted ? My code so far : How can I transform the QuerySet to get my desired output ? Or is there a different way ? The Django Admin Interface is really powerful , which is why I want to use it as long as it is possible . But this feature is really missing.I am currently using Django 1.11Any help , comments or hints are appreciated . Thanks for your help ."
I 've started using pipenv and installed the flask package.In my Pipfile.lock there is this entry : I wonder why there are two different hashes . Can anyone elaborate on this ? Thanks in advance !
"I 've managed to succesfully setup a Visual Studio Python project . I now want to share this project with other developers through source control ( the company I work at uses SVN ) .As I want to avoid each of my colleagues having to manually setup the same Python environment , I looked into using a Virtual Environment . In my head , this seems very similar as to how NPM modules are stored locally.Without too much hassle , I managed to setup a Virtual Environment , which works brilliantly.However , I was ready to exclude my `` Virtual Environment '' -folder from being checked into SVN , when I noticed that the `` pyproj '' -file contains a reference to my local virtual environment : If I remove the `` Virtual Environment '' -folder and open the Visual Studio solution , I do n't have any option to restore the environment based on the generated `` requirements.txt '' -file ( as I expected ) . Unless I remove the non-working `` Virtual Environment '' and add a completely new one.This leads me to believe that there is something wrong in my work-flow or assumptions.Should I not exclude the virtual environment from being checked-in ? Should I only exclude parts of the virtual environment and if so , which parts ? Side notes : As you can probably tell , I 'm still fairly new to using Python , so any advice is very welcome.The reason I want to use Visual Studio is because the company is primarily .NET focused , which makes it a very farmiliar environment for most developers.I did read Working with python in Visual Studio - Step 06 Working with Git , but it does n't mention Virtual Environments at all ."
"I have a gigantic dataframe with a datetime type column called time , and another float type column called dist , the data frame is sorted based on time , and dist already . I want to split the dataframe into several dataframes base on monotonic increase of dist.Splitinto"
"Which CPU information this code is trying to retrieve . This code is part of a larger package . I am not a Python programmer , and I want to convert this code to C # .If you are a Python programmer and knows what this code is doing , it will be a great help for me ."
"I want to multiply a lookup table ( demand ) , given for multiple commodities ( here : Water , Elec ) and area types ( Com , Ind , Res ) with a DataFrame ( areas ) that is a table of areas for these area types.Before : After : My attemptVerbose and incomplete ; does not work for arbitrary number of commodities.Almost there : In shortHow to join/multiply the DataFrames areas and demand together in a decent way ?"
"This question will no doubt to a piece of cake for a Python 2.7 expert ( or enthusiast ) , so here it is.How can I create a list of integers whose value is duplicated next to it 's original value like this ? It 's really easy to do it like this : But i 'd rather have it in one simple line starting a = desired output.Thank you for taking the time to answer.PS . none of the `` Questions that may already have your answer were what I was looking for ."
"I try to store a list of different shaped arrays as a dtype=object array using np.save ( I 'm aware I could just pickle the list but I 'm really curious how to do this ) .If I do this : it works.But this : Gives me an error : I guess np.save converts the list into an array first , so I tried : Which has the same effect ( first one works , second one doesn't.The resulting x behaves as expected : I also tried to force the 'object ' dtype : Without success . It seems numpy tries to broadcast the array with equal first dimension into the new array and realizes too late that their shape is different . Oddly it seems to have worked at one point - so I 'm really curious what the difference is , and how to do this properly.EDIT : I figured out the case it worked before : The only difference seems to be that the numpy arrays in the list have another data type.It works with dtype ( ' < f8 ' ) , but it does n't with dtype ( 'float64 ' ) , I 'm not even sure what the difference is.EDIT 2 : I found a very non-pythonic way to solve my issue , I add it here , maybe it helps to understand what I wanted to do :"
"I know that it is bad practice to use shell=True for subprocesses . However for this line of code , I 'm not sure how to execute it with shell=FalseWhere the command I want to run is : Where file_name is /path/to/file.log"
"I have a Heroku setup with django-compressor compressing my CSS and JS files . My default cache backend is set up with memcached ( actually Memcachier when on Heroku ) . My static files are being served on an Amazon S3 instance ( handled by django-storages and boto ) . Generally my setup is in line with this one from the django-compressor docs.In my pages , the links to my compressed files have an `` expires '' query that seems to be an hour from when the link is generated . I do n't want to set there to be no expiration for all CSS or JS requests because I have some CSS and JS that I do n't compress ( it gives me errors when I try ... probably a different question ) .However , once an hour the links break and the site has no more styles or JS . It looks like the link , or at least the expires header , is being cached and not regenerated , so after the time in the expires argument passes , Amazon no longer returns the file . If I flush the memcache , it changes the expires header in the link to an hour from then , which fixes the problem for an hour , until it expires again.Here is an example of a link that was generated around 1:39 PM EST today , Tuesday , September 18 : https : //zen180-static.s3.amazonaws.com/CACHE/css/68d31794a646.css ? Signature=u % 2FTxeF7LBBZTDV79YovOjoK2tcw % 3D & Expires=1347993542 & AWSAccessKeyId=AKIAIJ7VKLEX7HIMJZCA . After it is first generated , the page continues to serve that link without alteration . Around 2:39 EST ( i.e . the Unix time 1347993542 , from the expires argument in the URL ) , that link stopped working and sent back an `` Expired '' XML message ( the same one it does now ) . But the bad link was still there on my page until I flushed memcache.Here are the relevant settings : Here is the error :"
"In Pandas , there is a method DataFrame.shift ( n ) which shifts the contents of an array by n rows , relative to the index , similarly to np.roll ( a , n ) . I ca n't seem to find a way to get a similar behaviour working with Dask . I realise things like row-shifts may be difficult to manage with Dask 's chunked system , but I do n't know of a better way to compare each row with the subsequent one.What I 'd like to be able to do is this : in order to create a boolean series indicating the locations of sign changes in the data . ( I am aware that method would also catch changes from a signed value to zero ) I would then use the boolean series to index a different Dask dataframe for plotting ."
"I am trying to create a program that storesFruit NameFruit TypeFruit ColorFruit Sizeand show them back to the user upon request . The user will be given pre-defined choices to select from . Something like this : My database table will be like this : Now , I am trying to implement a filter function that lets the user select Fruit TypeFruit ColorFruit SizeAnd it will give out all the Fruit Names that have the above properties . But now , I have an additional option , `` All '' .Assuming that I have already queried out the data for all the fruits and stored them in dictionary like this : How do I get the list of fruit names that has the three properties that the user selected ? ( For example , if the user selected 'fleshy ' type , 'All ' color , 'All ' size -- it should return [ 'apple ' , 'orange ' ] . ) I have thought of using if statement , but as the number of properties grow , I would have to write so many lines of if and else which I do n't think is feasible.I am using Python 2.7 with PyQt 4 and the SQLite 3 database on Windows XP SP3 32-bit ."
"I 'm trying to use the metadata harvesting package https : //pypi.python.org/pypi/pyoai to harvest the data on this site https : //www.duo.uio.no/oai/request ? verb=IdentifyI tried the example on the pyaoi site , but that did not work . When I test it I get a error . The code is : This is the stack trace : I need to get access to all the files on the page I have linked to above plus generate an additional file with some metadata . Any suggestions ?"
I 've seen several examples from different languages that unambiguously prove that joining elements of a list ( array ) is times faster that just concatenating string . Unfortunately I did n't find an explanation why ? Can someone explain the inner algorithm that works under both operations and why is the one faster than another . Here is a python example of what I mean : Thank is advance )
Is there a way to have the `` Zoom to rectangle '' tool automatically activated by default when a matplotlib figure is shown ?
"I have a process that loops through two lists , one being relatively large while the other being significantly smaller . Example : I scaled the sized down of the lists to test performance , and I noticed there is a decent difference between which list is looped through first.At first glance , they look identical - however there is 16 second difference between the two functions.Why is that ?"
"In a pandas DataFrame , I have a series of boolean values . In order to filter to rows where the boolean is True , I can use : df [ df.column_x ] I thought in order to filter to only rows where the column is False , I could use : df [ ~df.column_x ] . I feel like I have done this before , and have seen it as the accepted answer . However , this fails because ~df.column_x converts the values to integers . See below . Basically , I can use c [ ~b ] , but not c [ ~c.Boolean ] Am I just dreaming that this use to work ?"
"I am using the django-subdomains package to create subdomains . The problem is that no matter how I configure the SUBDOMAIN_URLCONFS , the site always directs to whatever I have put in ROOT_URLCONF as a default . Any insight as to what I am doing incorrectly would be greatly appreciated ! EDIT : Added MIDDLEWARE_CLASSESmysite/settings.pymysite/urls.pymyapp/views.pymyapptwo/urls.pymyapptwo/views.py"
"So I know the way to make a variable `` private '' in python like this : This `` works '' and does n't , as shown below : Now , I understand this is the way to make private variables in python and I like this way . It allows you to mangle names so that no subclasses accidentally override this ( because it begins with the class 's name ) , and that nobody will accidentally use it . It also gives you the power to change the private variables if you know what you are doing . Also , it is the best way to do it , because truly private variables are impossible.Or so I thought.Recently , I was reading PEP 8 and I saw this line : We do n't use the term `` private '' here , since no attribute is really private in Python ( without a generally unnecessary amount of work ) .This quote is found in the Designing for Inheritance section of PEP 8.Note the phrase `` without a generally unnecessary amount of work '' . I am now sure that there must be a way to get truly private variables in python . How would I do that ? I have tried overriding __getattribute__ , but the problem is that there is no way to tell if the call is coming from inside the class or not ( that I am aware of ) .Also , the __dict__ attribute is annoying when trying to do this because it holds references to all instance variables.I also thought of metaclasses , but those seem to have the same problems as __getattribute__.Thoughts ? Note : I understand that any way to make truly private variables in python should never be done in productive code . I just want to know how it could be done ."
the result is 999In my test i seems += thread safeMy question is : is += really thread safe ?
"I wrote a python script to do all my tests automatically for me , and generate a HTML report . I discovered discover for unittests the other day which lets me run all the unittests in a given directory without explicitly naming them , and I 'd really like to be able to do my doctests the same way , rather than having to import each module explicitly.I found some info on how to do this at https : //docs.python.org/2/library/doctest.html but did n't really get it . Could you please help me with using discover with my doctests ? Python test discovery with doctests , coverage and parallelism is related , but still does n't answer my question.coverage_moduletest_module"
"I do n't intend to simply waste your time , but : has it occurred to you too , while using Python 's with statement that it really is contrary to the 5th line of `` The Zen of Python '' that goes `` Flat is better than nested '' ? Can any enlightened Python guru share me some of their insights on this ? ( I always find that one more level of indentation pops up in my code every time I use with instead of f.close ( ) ... and it 's not like I 'm not gon na use try : ... finally : ... anyways and thus the benefits of with still elude me , even as I grow to like and understand Python more and more ... ) @ glglgl ( sorry , I ca n't find a way to write code in comments ) : yes , but if you go the with way , your code becomes : ... and using just with without the try is what people end up doing in the type of hacky `` one use '' code where they use f.close ( ) instead of with anyways ( which is bad because the file may not be closed if an exception is thrown before their f.close ( ) ) , so for `` hacky '' code people just do n't use with because , I do n't know , I guess they just find it too `` fancy '' and for well structured code it does n't bring any benefits anyways , so it seems to me there 's no real world use case left for it ... that was my pondering about really ."
"I 'd like to shorten a string using textwrap.shorten or a function like it . The string can potentially have non-ASCII characters . What 's special here is that the maximal width is for the bytes encoding of the string . This problem is motivated by the fact that several database column definitions and some message buses have a bytes based max length.For example : It 's okay for the implementation to use a width greater than or equal to the length of the whitespace-stripped placeholder [ ... ] , i.e . 5.The text should not be shortened any more than necessary . Some buggy implementations can use optimizations which on occasion result in excessive shortening.Using textwrap.wrap with bytes count is a similar question but it 's different enough from this one since it is about textwrap.wrap , not textwrap.shorten . Only the latter function uses a placeholder ( [ ... ] ) which makes this question sufficiently unique.Caution : Do not rely on any of the answers here for shortening a JSON encoded string in a fixed number of bytes . For it , substitute text.encode ( ) with json.dumps ( text ) ."
"Is there some simple way of capturing and making assertions about logged messages with nose ? For example , I 'd like to be able to do something like :"
"Hi guys , so i 'm trying to use graphQL queries in django . Basically i have two apps , my 'api ' app which contains everything i need to make the queries and another one called 'frontend ' from which i call the api to use these queries.I can use the GraphQL view to type queries in it and it works perfectly , but whenever i try to make the query , i get this : `` OrderedDict ( [ ( 'users ' , None ) ] ) '' Result of my query in the GraphQl viewAnd for the code : In 'api ' my schema.py : And the views.py that calls the app 'api ' in my app frontend : The template : and finally : The web page resultand the error in the console : Any help is welcome guys , i just ca n't figure out why it is n't working since i 'm logged and everything works just perfectly in the graphql viewps : sorry if i made some mistake , english is n't my mother tongue"
"Ran into this problem ( in Python 2.7.5 ) with a little typo : Dang it , I accidentally exploded the Moon.My understanding is that E > F is equivalent to ( E ) .__gt__ ( F ) and for well behaved classes ( such as builtins ) equivalent to ( F ) .__lt__ ( E ) .If there 's no __lt__ or __gt__ operators then I think Python uses __cmp__.But , none of these methods work with function objects while the < and > operators do work . What goes on under the hood that makes this happen ?"
"When I click on customer count it works only once and to make it work I have to run the program again , what is the problem here ? And what I have noticed is once the link localhost:8000/graph is ( customer count ) clicked , works but makes busy to localhost:8000 and does n't work that link . How to solve it ? views.pyurls.pyhome.htmlAlternative source code location : click here"
"I have a tuple of zeros and ones , for instance : It turns out : I want a function f such that if s is a non-empty tuple of zeros and ones , f ( s ) is the shortest subtuple r such that s == r * n for some positive integer n.So for instance , What is a slick way to write the function f in Python ? Edit : The naive method I am currently using"
I am currently using a function to display a pandas dataframe in a spreadsheet style format . I would like to be able to add some functionality to format individual cells of the treeview based on their content e.g . if they contain substring ' X ' or if their value is higher than Y.The update function currently implemented is as follows : Can anyone confirm that you can in fact edit an individual cell in a treview ? If yes are there any ideas as to how this could be implemented ?
"I 'm developing a procedurally-generated game world in Python . The structure of the world will be similar to the MUD/MUSH paradigm of rooms and exits arranged as a directed graph ( rooms are nodes , exits are edges ) . ( Note that this is not necessarily an acyclic graph , though I 'm willing to consider acyclic solutions . ) To the world generation algorithm , rooms of different sorts will be distinguished by each room 's `` tags '' attribute ( a set of strings ) . Once they have been instantiated , rooms can be queried and selected by tags ( single-tag , tag intersection , tag union , best-candidate ) .I 'll be creating specific sorts of rooms using a glorified system of template objects and factory methods -- I do n't think the details are important here , as the current implementation will probably change to match the chosen strategy . ( For instance , it would be possible to add tags and tag-queries to the room template system . ) For an example , I will have rooms of these sorts : Finally , the question : what is a good strategy for instantiating and arranging these rooms to create a graph that might correspond to given rules ? Some rules might include : one plaza per 10,000 population ; main_street connects to plaza ; side_street connects to main_street or side_street ; hotel favors main_street or plaza connections , and receives further tags accordingly ; etc.Bonus points if a suggested strategy would enable a data-driven implementation ."
"I have a model saved in a pb file . I hope to calculate the flops of it . My example code is as follow : The print information is strange . My model has tens of layers , but it reports only 18 flops in the printed information . I 'm quite sure the model is correctly loaded because if I try to print the names of every layer as follows : The print information shows exactly the right network.What 's wrong with my code ? Thank you !"
"Well I 'm considering a case where I have a baseclass with ( several ) children . I have a function that takes a list of baseclass objects , and returns a new list with these objects in it.Now if I would use a child class obviously the return is a list of those child class objects : consider the following trivial case : Legal code : however pycharm ( and the standard type hints ) shows an error during static code analysis : '' unresolved attribute reference '' @ idx.sum_mul_data ( ) Now apparently this is due to pycharm thinking the return of the function is of type `` BaseClass '' - not a child . So how would I state : `` return same type as input '' ? I tried using a typevar : T = TypeVar ( `` T '' , BaseClass ) , though that gave an actual error , that a single contraint ca n't be used in TypeVar . Interestingly using T = TypeVar ( `` T '' , BaseClass , ChildClass ) did work , and pycharm correctly deduced the type ( hint ) for sum_div_data ."
"I 'm creating an app that downloads and installs its own egg plugins , but I have a problem loading the egg after easy_install extracts it into place . This is how it works now : App downloads egg into temp folderInstalls egg with setuptools.command.easy_install.main ( ) into ~/.app/plugins folder ( which is pointed by a pth on dist-packages ) At this point , the ~/.apps/plugins/easy-install.pth is updated with the new egg pathThe problem is that the pth is not reloaded until the python process is relaunched , which means the app has to be stopped and restarted ( app is a long-running process , and plugin installation must not require a restart ) .So the question is how to , either reload the pth programatically so that plugin entry-point discovery works for the new egg , or somehow have easy_install return the path it installed the egg into , so I can manually ( with pkg_resources ) load the new plugin ? I could create a function that tries to guess the easy_install'ed path or parse the pth on my own , but I prefer not to , if at all possible.Python 2.6 , setuptools 0.6c9Thanks to Marius Gedminas , what I 'm doing now basically is :"
"What is the proper syntax for a hanging indent for a method with multiple parameters and type hinting ? Align under first parameterIndent one level beneathPEP8 supports the Indent one level beneath case , but does not specify if Align under first parameter is allowed . It states : When using a hanging indent the following should be considered ; there should be no arguments on the first line and further indentation should be used to clearly distinguish itself as a continuation line ."
I am trying to get all IP Addresses for : earth.all.vpn.airdns.orgIn Python : ( fyi eprint is a function for printing error ) . The Output gives me 29 Addresses.But when i do : I get about 100 entrys.How to achieve this in python ? Why i am not getting all entrys with `` getaddrinfo '' ? This behaviour only appears using windows ( python 3 ) . When i am executing the code on my Linux ( python 2.7 ) it gives me the same result as using nslookup.info : As in the answer explained it does not depend on the system.Without changing anything the results of nslookup and getaddrinfo are the same now .
"Trying to make a POST request between a Python ( WSGI ) and a NodeJS + Express application . They are on different servers . The problem is that when using different IP addresses ( i.e . private network vs. public network ) , a urllib2 request on the public network succeeds , but the same request for the private network fails with a 502 Bad Gateway or URLError [ 32 ] Broken pipe.The urllib2 code I 'm using is this : Now , I have also coded the request like this , using requests : And get a 200 OK response . This alternate method works for both networks.I am interested in finding out if there is some additional configuration needed for a urllib2 request that I do n't know of , or if I need to look into some network configuration which might be missing ( I do n't believe this is the case , since the alternate request method works , but I could definitely be wrong ) .Any suggestions or pointers with this will be greatly appreciated . Thanks !"
"I am generating a large matrix ( 100x100 , let 's call it X ) with random numbers , with numpy.matrix ( ) so that I have a numpy.ndarray.I have been wondering if there are any difference between the two operations : numpy.transpose ( X ) X.TI have measured the time of each operation in a loop with a range of 1000 and it seems that X.T is significantly faster than numpy.transpose ( X ) Added Benchmarks : For a 100x100 matrix I got the following results with X.T and numpy.tranpose ( X ) In a 10.000 range loop:7421/10.000 : X.T fastest1256/10.000 : numpy.transpose ( X ) fastest1323/10.000 : Same computation time or difference too small to determineAdded the code belowBest regardsWhir"
"My question is about a specific array operation that I want to express using numpy.I have an array of floats w and an array of indices idx of the same length as w and I want to sum up all w with the same idx value and collect them in an array v.As a loop , this looks like this : Is there a way to do this with array operations ? My guess was v [ idx ] += w but that does not work , since idx contains the same index multiple times.Thanks !"
"Goal : Show data from server in wxPython GUI on clientNewcomer to Twisted . I have a wxPython GUI running on a Windows 7 client , and I have a program running on an Ubuntu server that produces a log . My current attempt is to tail -f the log , pipe the output to a twisted server , then serve any data that meets my regex conditions to the client . I already have a tunnel open , so I do n't need to complicate things with SSH . I 've gotten the following block of code running , but it only serves the first line in the input . I know I need to keep checking the input for a newline and then writing it to the transport , but I 'm not sure how to do that without breaking the connection . I have n't been able to find enough information to patch a full solution together . I have also tried various other methods using sockets and file IO , but I think Twisted seems to be a good tool for this issue . Am I on the right track ? Any recommendations appreciated . ThanksTo answer the first comment , I have also tried to just read the log from within Python , program hangs . Code follows :"
"The following minimal program reproduces the problem.If you run the program and send a SIGTERM to the process , the print statement in line 16 ( stopping event loop ) is called but the programm does not terminate and the print statement in line 13 ( event loop has stopped ) is never called . So it seems that the event loop is never stopped and self._event_loop.run_forever ( ) blocks indefinitely.Why is this ? Note : A modified version of the program , where a.stop ( ) is not called by a signal handler but by a seperate thread with a delay , works as expected . How can it make a difference how a.stop ( ) is called ?"
"I got a problem which is : receive a tuple with objects of any type , and separate it into two tuples : the first , with strings only ; the second , with numbers only.Alright . The standard algorithm would be something like : That way , we only iterate once.My question is : is there a way to do it in a more pythonic way ? A one-liner ? I have triedBut it is less efficient , as we iterate over the input tuple twice.Also , Has the same problem , as we do two iterations.Would that be possible to iterate only once and still get the result , or because i am dealing with separation into two tuples , it is n't possible ? Thanks !"
"I have the following django model : Is using `` type '' as an attribute name considered a bad practice ? Here the attribute is not shadowing `` type '' , so it 's not the same question as this one"
"MySQL Version : 5.5.37-0ubuntu0.14.04.1I am currently writing a python script which makes use of numerous MySQL tables and queries to obtain results from an inverted index stored in tables . I have noticed that choosing the right type of Cursor in the MySQLdb python module when executing a query has a really big effect on performance and was wondering if someone could explain or provide a reliable resource explaining which cursor to use when.As an example , executing this query 40 times with SSCursor takes 7 seconds : Running the same query 40 times with the default Cursor takes 0.004 seconds.Removing the calculation of weight ( Counter/LOG ( Length ) ) makes this query execute fast again using an SSCursor however.I was using SSCursor because it was proving to have vastly superior performance on a number of other queries and then suddenly became very slow for this one . Changing back to the default Cursor surprised me when it executed so fast.EDIT : Some more examples . Running the following with the default cursor 40 times takes ~3 seconds : Running it with SSCursor instead takes about 0.002 seconds ."
"I 'm trying to figure out how to use variables and setups across modules.Am I right when I think it 's smart to keep separate functions of a program in separate modules ? I have a main program module called main.py and in it I have this : I then import a module ( in that module I have a function called Relay ) and try to use the function with module1.Relay ( 1,1 ) But the function in module1 needs the GPIO from the main.py to Work . How do I go about with this ? I really do n't want the GPIO setting part in the module1 , I do n't want it to be run every time I run the module1.Relay ( 1,1 ) call..What is best practice for working across modules ? ( I 'm making a controller for my home 's heat system . )"
"I 'm using the Python interface for libsvm , and what I 'm noticing is that after selecting the best C and gamma parameters ( RBF kernel ) using grid search , when I train the model and cross validate it ( 5 fold , if it 's relevant ) , the accuracy that I receive is the same as the ratio of labels in my training data set.I have 3947 samples , and 2898 of them have label -1 , and the rest have label 1 . So that 's 73.4229 % of the samples.And when I train the model and cross validate it 5 folds , this is what I get -Does this mean that the SVM is not taking the features into account ? Or that it 's the data at fault here ? Are they both related at all ? I 'm just not able to get it past the 73.4229 number . Also , the number of support vectors is supposed to be much less than the size of the dataset , but in this case , it does n't seem so.In general , what does it mean when the cross validation accuracy is the same as the ratio of labels in the dataset ?"
"I 'm using the csv.DictWriter class , and I want to inherit it : But this type is an old-style object . Can MyObj be a new-style class but still inherit from csv.DictWriter ?"
"I 'm trying to write an ElementTree object to disk . Everything works , except that the output file looks like this : Since it 's got the html : namespace info , the browser ca n't render it.How can I make etree save some html to disk without the html : namespace info ? Here 's the code I 'm using to write : Thanks !"
"Q1 - Is the following a set ( ) of a generator expression or a set comprehension ? ( Or are they same ? If so , are list & dict comprehensions also corresponding type-cast on generators ? ) Q2 - Does the evaluation consider duplicate values & then remove them by applying set ( ) ? Does the comprehension perform ( speed-wise ) better than regular for loops ? Update - I tried using timeit for speed comparisons . Am not sure if I am being just ( fair ) about it.Now , using some conditionalsSo , there is quite some difference , is it due to the functionality being hardcoded in c ?"
"According to the python documentationIndeed , this error made sense when overflowing integers were not converted to long automatically . Similarly , floats overflow to inf . I do n't really see any situation where the standard interpreter may still raise OverflowError . Is there such a case somewhere ? Just a curiosity ."
"I 'm pretty sure there is a duplicate lying around , but could n't find it.When declaring a urlpatterns in urls.py on dev , I use the following successfully : Which understandably , works.But if I try the following : django server dies complaining : ? : ( urls.E004 ) Your URL pattern [ < URLPattern '^static\/ ( ? P < path > . * ) $ ' > ] is invalid . Ensure that urlpatterns is a list of path ( ) and/or re_path ( ) instances.Why are n't the two definitions equivalent ? The return of static ( ) should be the same : And thus valid , but only works if I concatenate the element to the list instead of defining it in the list directly.Why one method works but not the other ?"
"I 'm getting confused by using Mock in my python unittests . I 've made this simplified version of my problem : I have this dummy class and methods : Which is using this class : And then this test : I would expect that running this test would pass and that print statement , for debugging , would output something like < MagicMock name= ... > . Instead it prints out < class 'app.fetch.FetcherA ' > and I get : Why is n't FetcherA being patched ?"
"I have created a Django REST API using Django Rest Framework.I use 3 different clients to hit this app : Curl ( for debugging purposes ) Swagger Interface ( for debugging purposes ) My Angular applicationI have a perplexing problem that data returned by the API is being corrupted by Swagger and Angular but not by Curl.My Django model looks like this : The make_id ( ) method referenced above is described here . I recently implemented this change from the standard Django assigned and auto-incremented primary key . As part of that change , I converted id from an IntegerField to a BigIntegerField.I have some other Django view code ( which I have not shown here ) that creates an endpoint called GetMyModel1ByName . That endpoint returns a serialized instance of MyModel1 . Here is the curl showing what happens when I hit that endpoint . It works perfectly : Now here is what happens when I hit the same endpoint from Chrome 's Developer console : As you can see curl reports the ID as 10150133855458395 . That 's correct . That 's what is in the Database . However Angular reports it as 10150133855458396 . The final digit is wrong . The difference is 1 . It 's very surprising ! This is a truly perplexing error . My Django code is so simple that I 'm very confident that there is no mistake in it . Instead I feel that change the id field from IntegerField to BigIntegerField might have caused this problem . Why is it happening and what is the solution ? ? It 's ruining the functionality of my application . I 'm seeing the same corruption when I hit this endpoint through Swagger.EDIT : The other questioner is experiencing the same problem I am . It 's great to know that I 'm not the only one ! ! However , the answers on that question are not really answers . They do n't explain the cause . And they do n't tell me how I should solve the issue in my Angular JS code ."
"I 've stumbled over a seemingly simple problem while building two libraries and test programs for both.The Problem : I have two static libraries , libA and libB , and libB depends on libA . I do n't want to explicitly link all programs that use libB to libA , I want SCons to see that if a program links to library B it should link to library A as well.I 've built a simple example that illustrates this problem . Since I could n't find a suitable file hoster and this is programming related , I created a small SVN repository : or you can download a tarball here ."
"I have this subcode in Python and I can not understand what it is or what it does , especially this statement : The subcode is :"
"I was playing around with the OrderedDict type in Python 3.6 and was surprised by its behaviour . When I create a simple dict like this in IPython : I get : as an output , which does n't preserve the order of elements at instantiation for some reason . Now , when I create an OrderedDict from d like this : the output is : Now I ask myself , how can the OrderedDict-constructor know about the order of elements at instantiation of d ? And does it always behave the same , such that I can rely on the order of elements in the OrderedDict ? I was already reading the Python docs about dictionaries and OrderedDicts but I did n't find an answer to my question.The output from ( sys.version ) :"
Here is an example use case of itertools.groupby ( ) in Python : Is there any language construct or library support in Java that behaves or can achieve what itertools.groupby ( ) does above ?
"I wrote a script that I believe should produce the same results in Python and R , but they are producing very different answers . Each attempts to fit a model to simulated data by minimizing deviance using Nelder-Mead . Overall , optim in R is performing much better . Am I doing something wrong ? Are the algorithms implemented in R and SciPy different ? Python result : R result : I 've checked over my code and as far as I can tell this appears to be due to some difference between optim and minimize because the function I 'm trying to minimize ( i.e. , choiceProbDev ) operates the same in each ( besides the output , I 've also checked the equivalence of each step within the function ) . See for example : Python choiceProbDev : R choiceProbDev : I 've also tried to play around with the tolerance levels for each optimization function , but I 'm not entirely sure how the tolerance arguments match up between the two . Either way , my fiddling so far has n't brought the two into agreement . Here is the entire code for each.Python : R : UPDATE : After printing the estimates at each iteration , it now appears to me that the discrepancy might stem from differences in 'step sizes ' that each algorithm takes . Scipy appears to take smaller steps than optim ( and in a different initial direction ) . I have n't figured out how to adjust this.Python : R :"
"I just upgraded to IPython Notebook version 3.0 and it 's disabling the formatting for seaborn . Here 's some sample code that replicates the problemThis code works just fine in IPython Notebook V2.4.1 ( see http : //nbviewer.ipython.org/gist/anonymous/71733c24a68ee464ca40 ) , but in IPython Notebook v3.0 , the axes become invisible ( see http : //nbviewer.ipython.org/gist/anonymous/7525146b07709206908c ) . Strangely , in V3 , when I switch the order of the seaborn import and the matplotlib inline magic , the plot renders normally the first time I run , then if I re-run , the axes and gridlines disappear . So it seems to have something to do with the inline magic disabling seaborn properties.Any workarounds , other than not re-executing my imports after the first time ?"
I am working with OpenERP 7I want to modify my invoice report footer to show the current page and the total number of pages like this : page:1/2 in the first page and page:2/2 in the second page ... this is my code : but pageCount do n't return any number . What 's wrong ?
"Is there any easy way to read a Latex table , as generated by the DataFrame method to_latex ( ) , back into another DataFrame ? . In particular , I 'm looking for something that handles Multiindex . For instance if we have the following file 'test.out ' : my first attempt was to read it aswhich does not work correctly since read_csv ( ) picks up the empty fields as new levels of the Multiindex : Is there any way to do this ?"
"I 'm coding some backend software for a second-hand selling app using Django and DjangoRestFramework . Right now , I 'm trying to send a Response object that contains a list of products , but I seem not to be able to return an actual list of products , as I 'm getting an error sayingI 've tried both using the serializer constructor like this : And by creating a list of ProductoSerializer.data and then creating the Response object with that.Here 's the serializers that I 'm using : And here 's the views.py function that I 'm trying to code : I 'm using a request object because I also have to server a WebPage , and not only a mobile app , with the same function ( the webpage part is still not coded though ) .It should return all the products that contain at least one of the words from the user 's search , and it all should be structured based on the ProductoSerializer object , but for some reason , it 's outputting that error and I 'm not quite sure how to fix it.Thanks in advance , and if you need any extra information which I 've missed , please do ask for it ... It 's been a long day and I probably missed something ."
"I am trying to plot a great circle distance between two points . I have found an in the cartopy docs ( introductory_examples/01.great_circle.html ) : which makes the following image : great circle exampleThe thing is , in my own work , the two points are much closer together , and in a different projection ( though I think that is n't important here ) . If I change this code to be a line in a smaller area , like so : This makes the following image : shorter lineThe red great circle line in this case looks crappy and looks like it is due to being too low resolution . How do I increase the number of points making up the great circle line ?"
"If I run : Then pi is printed with 16 digits , However , according to : My precision is 15 digits.So , should I rely on the last digit of that value ( i.e . that the value of π indeed is 3.141592653589793nnnnnn ) ."
"I am trying to add manual labels to the contourplot in the code below . The labels are printed somewhat randomly . Does anyone have an idea how to fix this ? It seems to be a bug in Matplotlib.Regards , David"
"I 'm reading Fluent Python chapter 19 > A Proper Look at Properties , and I 'm confused about the following words : Properties are always class attributes , but they actually manage attribute access in the instances of the class.The example code is : From my previous experiences , class attributes are belong to the class itself and shared by all the instances . But here , weight , the property , is an instance method and the value returned by it is different between instances . How is it eligible to be a class attribute ? Does n't it that all the class attributes should be the same for any instances ? I think I misunderstand something , so I hope to get a correct explanation . Thanks !"
"I have set of objects : How to remove duplicates from set of objects ? Thanks for answers , it 's work : result : [ 2,8,3,4,5,6,7 ] but how to save order ? Sets not support order . Can i use list instead set for example ?"
"So let 's say i have a pandas data-frame as below : So my goal is to replace 0 value with [ ] ( empty list ) in this dataframe , but i did : But it gives me an error : I tried everything that 's possible i.e : etc ... But nothing works.Desired output ( in case for confusion ) :"
"I am using opencv 2.4.4 installed via macports with python 2.7.5 on a mac os x 10.7.5 . I want to train a cascade to look for male frontal faces . But I am getting the terminate called throwing an exceptionAbort trap : 6 error . I request the SO community help me figure out what might be going wrong.The negative ( background ) image are taken from google : googleImages_noFaces ( 293 images ) The positive images are taken from Karolinska database : trainingSet ( 70 images ) I created a text file which indicates the relative location of background images : bgDesc.txtI also created a text file indicating the relative location , number of positive instances in the image ( which is always 1 ) and bounding region of the object ( which is the entire image ) : maleDesc.txt All these files can be downloaded from here.The organization of the files is in this form : when I use opencv_createsamples a maleDesc.vec file is successfully created with the following line : if I use -show parameter I can see that 24x24 pixel images are created . I then tryWhich gives me an error.I have tried different values of -numPos such as 10 , 20 and so on up to 70 along with different values of -numNeg as 30 , 60 and so on up to 293 . I have tried to use numPos values that are less than numNeg values and even those which are greater than . I have also tried different -numStages values like 1 , 5 , 10 , 20 and 100 but in all of these attempts I get the same error . I have not tried different values of -minHitRate , -maxFalseAlarmRate , -weightTrimRate , -maxDepth , -maxWeakCount because I do n't really understand how they influence the behavior of opencv_traincascade algorithm . Any help is much appreciated : )"
"Is it possible to unpack a list of numbers in to list indices ? For example I have a lists with in a list containing numbers like this : I need to place them in a pattern so i did something like thisso instead of writing new_nums = [ lst [ 4 ] , lst [ 2 ] , lst [ 3 ] , lst [ 0 ] , lst [ 1 ] ] , i thought of defining a pattern as list called pattern = [ 4,2,3,0,1 ] and then unpack these in to those indices of lst to create new order of lst.Is there a fine way to do this ."
"I have a web server that is connecting to one of many serverlets . The web server might queue up to 40 jobs that can take 20 mins or 30 hours to run each.The web server connects to a serverlet using sockets and the serverlet runs the job sent through using threads.I want to put a cap on the number of threads ( jobs ) that can be run at once , say 3 and once that limit is reached it holds the main thread . When one of the threads ends it allows the main thread to continue and pickup another job . I 'm currently using a loop to make my main thread wait until a free thread is available . It works , but it feels like a quick and dirty solution . I wonder if there might be a better way to do it ? server.py"
I am beginner of multithreading in python.I want to use a Lock in threads . Does it have to be declared global in the thread ? My code looks like this :
"As far as I understand var is a class variable here : And thats an instance variable : I had the problem , that I was looking for a method to make type hinting possible for instance variables . I can of course typehint the parameter with def __init__ ( self , var : str ) : but that would not effect the instance variable itself.Then I noticed in some descriptions ( like here ) that they used the term instance variable for a var like this : That would be the solution indeed , but is that still an instance variable ? Because it is defined in the class body , it would be a class variable in my understanding . If you would use a list for var , all alterations to this list-var would be shared over the instances . But in this case there would be no problem , because the string is replaced and would not be shared for other instances . However , it seems wrong to me if you call it an instance variable and I do n't know if I should use it like this just to have the type hinting working ."
"I 'm trying to find the max since condition was true in a pandas dataframe . I 've searched for similar questions and read the documentation but have n't been able to find this problem discussed . To illustrate , I want a function that will return the maxsince column below.I 'm having trouble calculating this without resorting to looping . What would be the most efficient way ? Thanks ."
"I do n't understand why a so basic optimization has not yet be done : The whole array is scanned , even if the conclusion is an evidence at first item ."
"I have been trying to run the websocket chat example provided here on an Apple Mac.https : //github.com/unbit/uwsgi/blob/master/tests/websockets_chat_async.pyBut running this example results in a segfault in uwsgi . I have copied pasted this example in websocket.py and am running the server with the following commandWhen I access the URL , I get the proper message for connection establishing . But then the server crashes with this segfaultApparantly the crash is on this lineStumped here . Need help ."
"I want to duplicate ( copy ) an object mapped by SQLAlchemy . It should only copy the data created by me , not all the underliying stuff . It should n't copy the primary keys or unique values.This is usefull when creating new data entries which differ only a little from the last one . So the user does n't have to enter all data again.An important requirement is that this need to work when the column name in the table ( e.g . name ) and the memeber name ( e.g . _name ) in the python class are not the same.This ( simplified ) code work for all declarative_base ( ) derived classes BUT ONLY when the col-name and the member-name are the same.col.key is the name of the column in the table . When the member name in the python class is different this would n't work . I do n't know how SQLAlchemy connect the column-name with the member-name . How does SQLA know this connection ? How can I take care of it ?"
"Let 's say I have 5 columns . Is there a function to know the type of relationship each par of columns has ? ( one-to-one , one-to-many , many-to-one , many-to-many ) An output like :"
"Background informationI have a Python script which generates word documents with the docx module . These documents are generated based on a log and then printed and stored as records . However , the log can be edited retroactively , so the document records need to be revised , and these revisions must be tracked . I 'm not actually revising the documents , but generating a new one which shows the difference between what is currently in the log , and what will soon be in the log ( the log is updated after the revised file is printed ) . When a revision occurs , my script uses diff_match_patch to generate a mark-up of what 's changed with the following function : docx can take text either as strings , or by tuple if word-by-word formatting is required , so [ see second bullet in `` Some Things to Note '' ] produces Hello my name is BradThe Problemdiff_match_patch is a very efficient code which finds the difference between two texts . Unfortuanly , its a little too efficient , so replacing redundant with dune results in redunanteThis is ugly , but its fine for single words . However , if an entire paragraph gets replaced , the results will be entirely unreadable . That is not ok.Previously I addressed this by collapsing all the text into a single paragraph , but this was less than ideal because it became very cluttered and was still pretty ugly.The Solution So FarI have a function which creates the revision document . This function gets passed a list of tuples set up like this : So the document is set up as I assume in order to resolve the problem , I 'll need to do some sort of matching between paragraphs to make sure I do n't diff two completely separate paragraphs . I 'm also assuming this matching will depend if paragraphs are added or removed . Here 's the code I have so far : So far I 've planned on using something like difflib to do paragraph matching . But if there 's a better way to avoid this problem that is a completely different approach , that 's great too.Some Things to Note : I 'm running Python 2.7.6 32-bit on Windows 7 64-bitI 've made some changes to my local copy of docx ( namely adding the strike through formatting ) so if you test this code you will not be able to replicate what I 'm doing in that regardDescription of the Entire Process ( with the revision steps in bold ) :1 ) User opens Python script and uses GUI to add information to a thing called a `` Condition Report '' ( CR ) NOTE : A full CR contains 4 parts , all completed by different people . But each part gets individually printed . All 4 parts are stored together in the log2 ) When the user is finished , the information is saved to a log ( described below ) , and then printed as a .docx file3 ) The printed document is signed and stored4 ) When the user wants to revise a part of the CR , the open the GUI , and edit the information in each of the fields . I am only concerned about a few of the fields in this question , and those are the multiline text controls ( which can result in multiple paragraphs ) 5 ) Once the user is done with the revision , the code generates the tuple list I described in the `` Solution So Far '' section , and sends this to the function which generates the revision document6 ) The revision document is created , printed , signed , and stored with the original document for that part of that CR7 ) The log is completely rewritten to include the revised information The Log : The log is simply a giant dict which stores all the information on all of the CRs . The general format is The log does n't store past versions of a CR , so when a CR is revised the old information is overwritten ( which is what we want for the system ) . As I mentioned earlier , every time the log is edited , the whole thing is rewritten . To get at the information in the log , I import it ( since it always lives in the same directory as the script )"
Using the following dictionary : How can I create the following DataFrame with multiple column indices ? Any help is appreciated .
I have run into a strange difference between Python2 and Python3 . Printing the same list of characters yields an extra byte C2 when printed with Python3 . I would have expected the same behaviour . Python2 behaves as I expected . What am I missing here ?
"I am trying to check if the first 3 characters of a Charfield ( charfield_1 ) are similar to another Charfield ( charfield_2 ) of the same model.Tried : Tried using F and Func without any success.I keep getting : Any idea how to make this work ? I would like a solution using the ORM to avoid performance issues.Update : After checking the query generated by the ORM and the error message , it looks like the second Substr parameter is replaced by a non integer when I am using startswith or contains lookup expression.ex : Substr ( 'charfield_1 ' , 1 , 3 ) is replace by Substr ( 'charfield_1 ' , ' % 1 % ' , 3 ) I am using version 2.0.2.A ticket has been opened and accepted : https : //code.djangoproject.com/ticket/29155"
"I have the pandas DataFramefrom which I would like to select rows based on values of the index ( x ) in the selection arrayThe correct output can be obtained by using df.loc as followsThe problem I am running in to is df.loc is running pretty slow on large DataFrames ( 2-7 million rows ) . Is there a way to speed up this operation ? I 've looked into eval ( ) , but it does n't seem to apply to hard-coded lists of index values like this . I have also thought about using pd.DataFrame.isin , but that misses the repeat values ( only returns a row per unique element in selection ) ."
"I have a list of tuples like this : I want to generate a query in SQLAlchemy that the value field of my table are between 100 , 230 or 10 , 12or 7 , 1320 and so on . my table looks like this : in this case I want these ids : 1,3,4.I did something like this but I cant do the or part so it just filter the rows that matches all criteria I 'm using Python3.6 ."
"Why ar n't the following two scripts equivalent ? ( Taken from another question : Understanding Python Decorators ) and with a decorated decorator : Why do I want to know ? I 've written a retry decorator to catch MySQLdb exceptions - if the exception is transient ( e.g . Timeout ) it will re-call the function after sleeping a bit.I 've also got a modifies_db decorator which takes care of some cache-related housekeeping . modifies_db is decorated with retry , so I assumed that all functions decorated with modifies_db would also retry implicitly . Where did I go wrong ?"
"I guess these two questions are related , so I 'll post them together:1.- Is it possible to put type hint in chained assignments ? These two attempts failed:2.- Is it possible to put type hint in multiple assignments ? These were my attempts : I am aware that in both cases the type is inferred from the type hint of a , but I have a long variable list ( in the __init__ of a class ) and I want to be extra-explicit.I am using Python 3.6.8 ."
"I have created some program using python on Windows Vista . But I want to deploy it on Windows XP . Is it necessary to make new build on windows XP ? Or there is possibility to make build that will work on both of these systems ? EDIT ( EDIT 2 - very simple program does not work also ) : My setup : Using dependency explorer i checked that dependencies are : Almost solved : I figured out that installing : http : //www.microsoft.com/downloads/details.aspx ? FamilyID=9b2da534-3e03-4391-8a4d-074b9f2bc1bf & displaylang=en does the thing . But i tried to provide msvrc90.dll manually before and it did not work , is there any way to redistribute it automatically ? Or I must provide this install file to him.Last and the main problemNow i have problem with msvcrt.dll . Message that occured on windows xp : The procedure entry point wcsscpy_s could not be located in the dynamic link library msvcrt.dllOn vista i have version 7.0.6001.18000But on XP 7.0.2600.5512Is there a way to fix this ? Seems that i did not exclude few dll 's ... silly mistake : now it works !"
"I 'm currently lost deep inside the pandas documentation . My problem is this : I have a simple dataframe My aim is to apply something like : where square is a cleanly defined function.But this operation throws an error warning ( and produces incorrect results ) A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc [ row_indexer , col_indexer ] = value instead I ca n't make sense of this nor the documentation it points to . My workflow is linear ( in case this makes a wider range of solutions viable ) .Pandas 0.17.1 and Python 2.7All help much appreciated ."
I need a good explanation ( reference ) to explain NumPy slicing within ( for ) loops . I have three cases . A simple case : returns : It can be seen that the first result differs from the second and third .
"I want to concatenate two dataframes with category-type columns , by first adding the missing categories to each column.In theory the categories for both `` a '' columns are the same : However , when concatenating the two dataframes , I get an object-type `` a '' column : In the documentation it says that when categories are the same , it should result in a category-type column . Does the order of the categories matter even though the category is unordered ? I am using pandas-0.20.3 ."
"Say I have a dataframe of test scores of students , where each student studies different subjects . Each student can take the test for each subject multiple times , and only the highest score ( out of 100 ) will be retained . For instance , say I have a dataframe of all test records : ( 1 ) How do I keep only the test records ( rows ) with the maximum score ? That is , ( 2 ) How would I keep the average of all tests taken for the same subject , for the same student ? That is : I 've tried various combinations of df.sort_values ( ) and df.drop_duplicates ( subset= ... , keep= ... ) , to no avail . Actual DataAfter aggregation function is applied , only the column pct-similarity will be of interest . ( 1 ) Drop duplicate query+target rows , by choosing the maximum aln_length . Retain the pct-similarity value that belongs to the row with maximum aln_length . ( 2 ) Aggregate duplicate query+target rows by choosing the row with maximum aln_length , and computing the average pct-similarity for that set of duplicate rows . The other numerical columns are n't necessary and will be dropped eventually , so I really do n't care what aggregation function ( max or mean ) is applied to them ."
"I 'm having trouble while running embedded python . It turns out that I ca n't capture that SystemExit exception raised by sys.exit ( ) ; This is what I have so far : Also , my script is : Running it : I must execute a file , not a command ."
"firstly , let me quote a bit an essay from `` Expert Python Programming '' book : In the following example , a C class that calls its base classes using the __init__ method will make B class be called twice ! and finally , here is an explanation of what 's going on here : This happens due to the A.__init__ ( self ) call , which is made with the C instance , thus making super ( A , self ) .__init__ ( ) call B 's constructor . In other words , super should be used into the whole class hierarchy . The problem is that sometimes a part of this hierarchy is located in third-party code.i have no idea why `` super ( A , self ) .__init__ ( ) calls B 's constructor '' . Please explain this moment . Thanks a lot ."
"So in my Django stucture ( default setup ) , my folder structure goes like this : So to access the settings file , wsgi.py is , by default , set to mysite/settings.py . However , Django gave the mysite name to 2 folders , one within the other . So it looks in the top level one , does n't find it , and raises an error . I tried mysite/mysite/settings.py with the parent mysite 's directory in the system path , but that gave the same error.To get more info on this error , check How do I stop getting ImportError : Could not import settings 'mofin.settings ' when using django with wsgi ? I 'm just wondering why would Django make the default structure so difficult and if renaming stuff is the best way out ? Seems it could be sloppy and an error-prone way of doing things . Not sure how many places I 'd have to change the name if I did start renaming stuff . Seems dangerous ."
"I just watched Batch data processing with App Engine session of Google I/O 2010 , read some parts of MapReduce article from Google Research and now I am thinking to use MapReduce on Google App Engine to implement a recommender system in Python.I prefer using appengine-mapreduce instead of Task Queue API because the former offers easy iteration over all instances of some kind , automatic batching , automatic task chaining , etc . The problem is : my recommender system needs to calculate correlation between instances of two different Models , i.e. , instances of two distinct kinds.Example : I have these two Models : User and Item . Each one has a list of tags as an attribute . Below are the functions to calculate correlation between users and items . Note that calculateCorrelation should be called for every combination of users and items : But that calculateCorrelation is not a valid Mapper in appengine-mapreduce and maybe this function is not even compatible with MapReduce computation concept . Yet , I need to be sure ... it would be really great for me having those appengine-mapreduce advantages like automatic batching and task chaining.Is there any solution for that ? Should I define my own InputReader ? A new InputReader that reads all instances of two different kinds is compatible with the current appengine-mapreduce implementation ? Or should I try the following ? Combine all keys of all entities of these two kinds , two by two , into instances of a new Model ( possibly using MapReduce ) Iterate using mappers over instances of this new ModelFor each instance , use keys inside it to get the two entities of different kinds and calculate the correlation between them ."
"I am unable to understand the following behaviour . I am creating 2 strings , and using is operator to compare it . On the first case , it is working differently . On the second case , it works as expected . What is the reason when I use comma or space , it is showing False on comparing with is and when no comma or space or other characters are used , it gives TrueIs there a reliable information on why python interprets strings in different way ? I understand that initially , a and b refers to same object . And then b gets a new object , still b is a says True . It is little confusing to understand the behaviour . When I do it with 'string ' - it produces same result . What 's wrong when I use ' 1,2,3,4 ' - they both are strings . What 's different from case 1 and case 2 ? i.e is operator producing different results for different contents of the strings ."
I came across the following oddity in numpy which may or may not be a bug : I would have expected that both of the options below work.Opinions ?
"I need some help getting my brain around designing an ( efficient ) markov chain in spark ( via python ) . I 've written it as best as I could , but the code I came up with does n't scale.. Basically for the various map stages , I wrote custom functions and they work fine for sequences of a couple thousand , but when we get in the 20,000+ ( and I 've got some up to 800k ) things slow to a crawl . For those of you not familiar with markov moodels , this is the gist of it..This is my data.. I 've got the actual data ( no header ) in an RDD at this point.We look at sequences in tuples , so And I need to get to this point.. where I return a dictionary ( for each row of data ) that I then serialize and store in an in memory database.So in essence , each sequence is combined with the next ( HNL , LNH become 'HNLLNH ' ) , then for all possible transitions ( combinations of sequences ) we count their occurrence and then divide by the total number of transitions ( 3 in this case ) and get their frequency of occurrence . There were 3 transitions above , and one of those was HNLLNH.. So for HNLLNH , 1/3 = 0.333 As a side not , and I 'm not sure if it 's relevant , but the values for each position in a sequence are limited.. 1st position ( H/M/L ) , 2nd position ( M/L ) , 3rd position ( H , M , L ) . What my code had previously done was to collect ( ) the rdd , and map it a couple times using functions I wrote . Those functions first turned the string into a list , then merged list [ 1 ] with list [ 2 ] , then list [ 2 ] with list [ 3 ] , then list [ 3 ] with list [ 4 ] , etc.. so I ended up with something like this..Then the next function created a dictionary out of that list , using the list item as a key and then counted the total ocurrence of that key in the full list , divided by len ( list ) to get the frequency . I then wrapped that dictionary in another dictionary , along with it 's ID number ( resulting in the 2nd code block , up a above ) .Like I said , this worked well for small-ish sequences , but not so well for lists with a length of 100k+ . Also , keep in mind , this is just one row of data . I have to perform this operation on anywhere from 10-20k rows of data , with rows of data varying between lengths of 500-800,000 sequences per row.Any suggestions on how I can write pyspark code ( using the API map/reduce/agg/etc.. functions ) to do this efficiently ? EDITCode as follows.. Probably makes sense to start at the bottom . Please keep in mind I 'm learning this ( Python and Spark ) as I go , and I do n't do this for a living , so my coding standards are not great.."
"I 'm using the Python Parsimonious Parser to try to build an interpreter for a simple language I 'm designing . I watched this tutorial video which was very helpful , and now I 'm slowly modifying the code to match my own rules . I 'm stuck on an assignment rule originally defined as : I modified the rule slightly with the following grammar : I 'd like the parser to evaluate SET a , 7 for example , the same as a = 7 and bind the value 7 to the name a . However , when I attempt to parse it , I get this error from the Parsimonious library : I 'm fairly new to parsing/lexing and am not entirely sure if I defined the rule correctly . Was hoping someone with more parsing/lexing experience can help me properly define the rule and explain where I went wrong . Also perhaps explain the Parsimonious error to me ?"
"For example there is some folder : where file_a.py has something like : I know this is definitely not good practice , but what is the order of resolution behind ? i.e . How python decides which module to be imported for an `` absolute import '' ?"
"I 'm mainly working in Spyder , building scripts that required a pop-up folder or file Browse window.The code below works perfect in spyder.In Pycharm , the askopenfilename working well , while askdirectory do nothing ( stuck ) .But , if running in debug mode - the script works well.I tried to run the script from SAS jsl - same issue.Any Idea what should I do ? Python 3.6Pycharm 2017.2Thanks.The Code I 'm using includes : edit : seems like issue related to the pythonnet `` imoprt clr '' , but I do need it in the code.Similar question asked here : https : //github.com/pythonnet/pythonnet/issues/648"
"I 'm trying to use build OpenGL textures in python using numpy , but I 'm running into problems because I ca n't predict how numpy arrays will be organized in memory . The example program below ( which should run as-is ) illustrates my confusion : Although the two arrays are equivalent as far as python is concerned , the are laid out differently in memory and therefore displayed differently by OpenGL . Can someone explain why this is happening and how I might coerce both arrays to the same format ?"
"i have measurements in a dataframe . Columns are different objects . Index is a datetime64 index . Now for each date I have a measurement in total seconds ( int ) for each column.Everything plots quite nice , my only problem instead of showing 6000 seconds on the y axis i want to show 1:40 to indicate 1 hour and 40 minutes . How can I actually achieve this ? I wantCan you hint me on how to do it"
"I was sure that there would be a one liner to convert a list to a dictionary where the items in the list were keys and the dictionary had no values.The only way I could find to do it was argued against . `` Using list comprehensions when the result is ignored is misleading and inefficient . A for loop is better '' It works , but is there a better way to do this ?"
"I 've wrote a python script that need to pass millions of items to a C program and receive its output many times in a short period ( pass from 1 up to 10 millions of vertices data ( integer index and 2 float coords ) rapidly 500 times , and each time the python script call the C program , i need to store the returned values in variables ) . I already implemented a way reading and writing text and or binary files , but it 's slow and not smart ( why write files to hdd while you do n't need to store the data after the python script terminates ? ) . I tried to use pipes , but for large data they gave me errors ... So , by now i think the best way can be using the ability of ctypes to load functions in .dllSince i 've never created a dll , i would like to know how to set it up ( i know many ide have a template for this , but my wxdev-c++ crashes when i try to open it . Right now i 'm downloading Code : :Blocks ) Can you tell me if the solution i 'm starting to implement is right , or if there is a better solution ? The 2 functions i need to call in python are theseandEDIT : forgot to post the type defintion of vertexEDIT2 : I 'll redistribute the code , so i prefer not to use external modules in python and external programs in C. Alsa i want try to keep the code cross platform . The script is an addon for a 3D app , so the less it uses external `` stuff '' the better it is ."
"I have this code for calculating fibonacci number in python . It works and gives the expected result . but when I translated the same to Java , it fails . Any idea of what is going wrong here ? In python : fib3 ( 12 ) -- > 144In Java : fib2 ( 12 ) -- > 2048"
"I am learning Python and as an exercise I tried to make a program to make transactions on bitcoin market : https : //bitcurex.com . Here is an API reference : https : //bitcurex.com/reading-room/API . There is a PHP client example , so I tried to translate it to Python , so I 've got : These API keys are working - you can only make getFunds query with them.It keeps returning error `` Must me logged in '' . I tried to look on that request through Fiddler Proxy Debugger , and here you have the headers of that attempt : Fiddler is showing me an error : Any Ideas ? It seems like my Rest-Sign is too long or something like that . I think that my code should do exactly the same as PHP example . What I 'm doing wrong ?"
"I 'm trying to take the covariance of a large matrix using numpy.cov . I get the following error : It seems that this is not uncommon for 32-bit machines/builds ( I have a 64-bit mac os x 10.5 , but using a 32-bit python and numpy build as I had trouble building numpy+scipy+matplotlib on a 64-bit installation ) .So at this point what would be the recommended course of action that will allow me to proceed with the analysis , if not switching machines ( none others are available to me at the moment ) ? Export to fortran/C ? Is there a simple ( r ) solution ? Thanks for your suggestions ."
"I start by creating a string variable with some non-ascii utf-8 encoded data on it : Using unicode ( ) on it raises errors ... ... but if I know the encoding I can use it as second parameter : Now if I have a class that returns this text in the __str__ ( ) method : unicode ( r ) seems to use str ( ) on it , since it raises the same error as unicode ( text ) above : Until now everything is as planned ! But as no one would ever expect , unicode ( r , 'utf-8 ' ) wo n't even try : Why ? Why this inconsistent behavior ? Is it a bug ? is it intended ? Very awkward ."
"Is it `` good practice '' to create a class like the one below that can handle the memoization process for you ? The benefits of memoization are so great ( in some cases , like this one , where it drops from 501003 to 1507 function calls and from 1.409 to 0.006 seconds of CPU time on my computer ) that it seems a class like this would be useful . However , I 've read only negative comments on the usage of eval ( ) . Is this usage of it excusable , given the flexibility this approach offers ? This can save any returned value automatically at the cost of losing side effects . Thanks ."
"I would like to declare and share some simple , pure python functions betweentwo or more PL/Python functions . I am using Postgres 9.3 . For example , I have : I would like to use function is_float in some other PL/Python function.I understand I could create it as callable PL/Python function , but I find that much clunkier ( to execute SQL-based call to PL/Python ) than just making a straight call to a pure Python , custom utility function.Is it possible to create and expose through PL/Python reusable pure Python functions on Postgres ?"
"this is my first time using StackOverflow to ask a question , but you 've collectively saved so many of my projects over the years that I feel at home already.I 'm using Python3.5 and nltk to parse the Complete Corpus of Old English , which was published to me as 77 text files and an XML doc that designates the file sequence as contiguous segments of a TEI-formatted corpus . Here 's the relevant part of the header from the XML doc showing that we are , in fact , working with TEI : Right , so as a test , I 'm just trying to use NLTK 's MTECorpusReader to open the corpus and use the words ( ) method to prove that I 'm able to open it . I 'm doing all of this from the interactive Python shell , just for ease of testing . Here 's all I 'm really doing : When I try that , I get the following traceback : So , as I 'm a valiant StackOverflowsketeer , I 've determined that either one or more files is corrupted or there 's some character in the file ( s ) that contains a character that Python 's utf-8 decoder does n't know how to handle . I can be fairly certain of this file 's integrity ( take my word for it ) , so I 'm pursuingI tried the following to reformat the 77 text files with no apparent effect : So my questions are:1 ) Does my approach so far make sense , or have I screwed something up in my troubleshooting so far ? 2 ) Is it fair to conclude at this point that the issue must be with the XML doc , based on the fact that the UTF-8 error shows up very early ( at hex position 59 ) and the fact that my utf-8 error replacement script made no difference to the problem ? If I 'm wrong to assume that , then how can I better isolate the issue ? 3 ) If we can conclude that the issue is with the XML doc , what 's the best way to clear it up ? Is it feasible for me to try to find that hex byte and the ASCII it corresponds to and change the character ? Thank you in advance for your help !"
"Given an array of positive integers . How to find a subsequence of length L with max sum which has the distance between any two of its neighboring elements that do not exceed KI have the following solution but do n't know how to take into account length L. 1 < = N < = 100000 , 1 < = L < = 200 , 1 < = K < = Nf [ i ] contains max sum of the subsequence that ends in i ."
"I would like to have three values increment at different speeds . My overall goal is to emulate this pattern : The first two numbers are easy . I would solve it like this : This is the pattern that I want.The question is how do I increment the third number by one each time , while still having the first two numbers increment in the way that they are ? Basically , how can I make the third number increment by one each time the for loop goes ?"
"I have a list of sentences such as this : I want to find out count of longest phrases/part ( phrase must be more than 2 words ) of sentences in each element of list ? In following example , output will look closer to this ( longest phrase as key and count as value ) : Using re module is out of question since problem is close to sequence matching or perhaps using nltk or perhaps scikit-learn ? I have some familiarity with NLP and scikit but not enough to solve this ? If I solve this , I will publish it here ."
"I just started with python and very soon wondered if indexing a nested list with a tuple was possible . Something like : elements [ ( 1,1 ) ] One example where I wanted to do that was something similar to the code below in which I save some positions of the matrix that I will later need to access in a tuple called index . It seems like a useful feature . Is there any way of doing it ? Or perhaps a simple alternative ?"
"I have two different list of dictionaries , How to find an intersection of list_A with list_B based on `` count '' key by including rest of the key as like list_C ? I would like to retain the keys from list2 , but with missing values from list1 represented by `` - '' ."
"I am currently using a defaultdict of Counter to uniquely count several unpredictable values for unpredictable keys : This gives me the expected result : I now need to expand the structure of the values in the defaultdict and make it a dict with two keys : the previous Counter and an str : Is it possible to use a specific data structure ( like the above ) as the default_factory in defaultdict ? The expected result would be that for each nonexistent key in the defaultdict , a new key and value initialized with the structure above would be created ."
"I have an existing database that has two schemas , named schools and students , contained in an instance of declarative_base and through two different classes that inherit from that instanceandI can use the Base instance to as Base.metadata.create_all ( bind=engine ) to recreate it in a test DB I have in postgres . I can confirm this was done without problems if I query the pg_namespaceand from the psql CLIHowever , if I want to reflect that database in some other instance of declarative_base nothing is reflected.Something likeI understand that reflect accepts a schema as a parameter but I would like to obtain all of them at once during reflection . For some reason I can achieve this one at a time.Is there a way to do this ?"
"I 'm working on an automated webscrapper for a Restaurant website , but I 'm having an issue . The said website uses cloudlfare 's anti-bot security , which I would like to bypass , not the Under-Attack-Mode but a captcha test that only triggers when it detects a non-American IP or a bot . I 'm trying to bypass it as cloudflare 's security does n't trigger when I clear cookies , disable javascript or when I use an American proxy.Knowing this , I tried using python 's requests library as such : But this ends up triggering Cloudflare , no matter the proxy I use.HOWEVER when using urllib.request with the same headers as such : When ran with the same American IP , this time it does not trigger Cloudflare 's security , even though it uses the same headers and IP used with the requests library.So I 'm trying to figure out what exactly is triggering cloudflare in the requests library that is n't in the urllib library.While the typical answer would be `` Just use urllib then '' , I 'd like to figure out what exactly is different with requests , and how I could fix it , first off to understand how requests works and cloudflare detects bots , but also so that I may apply any fix I can find to other httplibs ( notably asynchronous ones ) EDIT N°2 : Progress so far : Thanks to @ TuanGeek we can now bypass the cloudflare block using requests as long as we connect directly to the host IP rather than the domain name ( for some reason , the DNS redirection with requests triggers cloudflare , but urllib does n't ) : To note : trying to access via http ( rather than https with the verify variable set to False ) will trigger cloudflare 's blockNow this is great , but unfortunately my final goal of making this work asynchronously with the httplib HTTPX still is n't met , as using the following code , the cloudflare block is still triggered even though we 're connecting directly through the Host IP , with proper headers , and with verify set to False : EDIT N°1 : For additional details , here 's the raw http request from urllib and from requestsREQUESTS : URLLIB :"
"Numpy.r_ , .c_ and .s_ are the only Python functions I 've come across that take arguments in square brackets rather than parentheses . Why is this the case ? Is there something special about these functions ? Can I make my own functions that use brackets ( not that I want to ; just curious ) ? For example , the proper syntax is : I would have expected it to be :"
"numpy.argsort docs state Returns : index_array : ndarray , int Array of indices that sort a along the specified axis . If a is one-dimensional , a [ index_array ] yields a sorted a.How can I apply the result of numpy.argsort for a multidimensional array to get back a sorted array ? ( NOT just a 1-D or 2-D array ; it could be an N-dimensional array where N is known only at runtime ) For me it 's not just a matter of using sort ( ) instead ; I have another array B and I want to order B using the results of np.argsort ( A ) along the appropriate axis . Consider the following example : It looks like this functionality is already an enhancement request in numpy ."
"I was reading how to check if a python module has been imported and the instructions seems clear , check for the module in the sys.modules . This works as I expected in Python 2 , but not with Python 3 ( 3.5 and 3.6 tested ) . For example : Python 3.6Python 2.7I note that , itertools is described as a 'built-in ' in the Python 3 sys.modules dict ( < module 'itertools ' ( built-in ) > ) , and not in Python 2 so maybe that 's why it 's in sys.modules prior to being imported , but it 's not listed as a built-in . Anyway , since itertools still needs importing in Python 3 , I 'd be grateful for an explanation ."
"Say I have the following class with a method returning a list : If I loop over the this method as follows : Inside the for loop , will c.f ( ) be executed multiple times ? If yes , in order to get it once , do I have to do assignment outside of the loop , or there is some trivial way ?"
"I 'm trying to optimize some code to process lists of lists , and I notice that when I try to specify a list within a list that I keep running into errors with syntax or with my output.My code is belowOne of my many attempts : Example input : Example output : Does anyone know any resources that can help me solve this ? I am coding in Python 3"
"When I create a document using the minidom , attributes get sorted alphabetically in the element . Take this example from here : The result is this : Which is all very well if you wanted the attributes in email/name/nickname/photo order instead of name/nickname/email/photo order as they were created.How do you get the attributes to show up in the order you created them ? Or , how do you control the order at all ?"
"I was trying to scrape some information from a website page by page , basically here 's what I did : Well , the above codes ended up throwing out `` Out of Memory '' error and in task manager it shows that the script used up almost 1GB memory after several hours running ... how come ? ! Would anybody tell me what went wrong ?"
"I am having some difficulty working with times/timezones . I have raw JSON data of the formThis data is then loaded into MongoDB , and this string representation of the date is transformed into a JavaScript Date object . This conversion to UTC time results in the following dateIt `` looks '' as though the date has actually been moved forward a day , I 'm assuming ( perhaps incorrectly ) that this is because my machine is set to Irish Standard Time.I then read this data from MongoDB and use it to create a pandas DatetimeIndexwhich gives mewhich is incorrect since the time has not been converted back correctly from UTC to local time . So I followed the solution given in this answerwhich gives me the right day backI then normalize the DatetimeIndex so the hours are removed , allowing me to group all entries by day . At this point , however , something strange happens . The dates end up getting grouped as followsbut this does n't reflect the dates in the frameCan anyone shed some light on where I might be going wrong ? Response to @ ptrjExplicitly using my timezone as a stringthis does n't work for me , it results in the following plotFor some reason the groupby is not properly grouping for 2014 , as shown belowIf instead , I useI get the same problemConvert to an objectThis approach seems to work correctly for me"
"I have a sample pyside demo which I created to see the webkit browser communication with python ... I have two buttons in webkitbutton 1 - when clicked it sleeps for 10 seconds and then prints a messagebutton2 - when clicked it prints a message immediately.When I clicked on button 1 , the whole apps freezes and waits for python to finish sleeping , this means I can not click on button 2 to do some other stuff . How can I implement an asynchronous method between function calls ? My python codes are belowQUESTIONHow can I click on button 1 in webkit and let python do something in the background when button 1 is clicked ? ( so that button 2 function does not need to wait for button 1 function to finish ) Kindly use this demo and improve on it ... much appreciated"
"I am using Qt for developing GUI applications.I get an error when I try to create another QApplication using multiprocessing : RuntimeError : A QApplication instance already existsI have a main window , which contains a button for spawning a new process so that I can create a new GUI instance . Basically like this : It works in Windows but gives the RuntimeError in Linux.Is this because of difference in multiprocessing mechanism between Windows and Linux ? How could I achieve the same thing in Linux ?"
"I am hitting a webservice with Python 's requests library and the endpoint is returning a ( very large ) CSV file which I then want to stream into a database . The code looks like this : Now when the database is a MongoDB database , the loading works perfectly using a DictReader : However , I am switching from MongoDB to Amazon RedShift , which I can already access just fine using psycopg2 . I can open connections and make simple queries just fine , but what I want to do is use my streamed response from the webservice and use psycopg2 's copy_expert to load the RedShift table . Here is what I tried so far : The error that I get is : file must be a readable file-like object for COPY FROM ; a writable file-like object for COPY TO.I understand what the error is saying ; in fact , I can see from the psycopg2 documentation that copy_expert calls copy_from , which : Reads data from a file-like object appending them to a database table ( COPY table FROM file syntax ) . The source file must have both read ( ) and readline ( ) method.My problem is that I can not find a way to make the response object be a file-like object ! I tried both .data and .iter_lines without success . I certainly do not want to download the entire multi-gigabyte file from the webservice and then upload it to RedShift . There must be a way to use the streaming response as a file-like object that psycopg2 can copy into RedShift . Anyone know what I am missing ?"
"I 'm trying to write the Dockerfile for a small python web project and there is something wrong with the dependencies . I 've been doing some search on the internet and it said that Librosa library requires libsndfile to work properly so I tried to install it using apt-get install libsndfile1 ( I 've also tried libsndfile-dev , ... ) . However , it does n't seem to solve my problem.This is how my Dockerfile looks like : However , when i try to build and run this , this error occured :"
"Is there a way to save a custom maplotlib colourmap ( matplotlib.cm ) as a file ( e.g Color Palette Table file ( .cpt ) , like used in MATLAB ) to be shared and then use later in other programs ? ( e.g . Panopoly , MATLAB ... ) ExampleBelow a new LinearSegmentedColormap is made by modifying an existing colormap ( by truncation , as shown in another question linked here ) .More detailI am aware of ways of loading external colormaps in matplotlib ( e.g . shown here and here ) . From NASA GISS 's Panoply documentation : Color Palette Table ( CPT ) indicates a color palette format used by the Generic Mapping Tools program . The format defines a number of solid color and/or gradient bands between the colorbar extrema rather than a finite number of distinct colors ."
In *nix systems one can use which to find out the full path to a command . For example : or whereis to show all possible locations for a given commandIs there an easy way to find out the location of a module in the PYTHONPATH . Something like :
"Pandas internals question : I 've been surprised to find a few times that explicitly passing a callable to date_parser within pandas.read_csv results in much slower read time than simply using infer_datetime_format=True.Why is this ? Will timing differences between these two options be date-format-specific , or what other factors will influence their relative timing ? In the below case , infer_datetime_format=True takes one-tenth the time of passing a date parser with a specified format . I would have naively assumed the latter would be faster because it 's explicit.The docs do note , [ if True , ] pandas will attempt to infer the format of the datetime strings in the columns , and if it can be inferred , switch to a faster method of parsing them . In some cases this can increase the parsing speed by 5-10x.but there 's not much detail given and I was unable to work my way fully through the source.Setup : I 'm interested in knowing a bit about what is going on internally with infer to give it this advantage . My old understanding was that there was already some type of inference going on in the first place because dateutil.parser.parser is used if neither is passed.Update : did some digging on this but have n't been able to answer the question . read_csv ( ) calls a helper function which in turn calls pd.core.tools.datetimes.to_datetime ( ) . That function ( accessible as just pd.to_datetime ( ) ) has both an infer_datetime_format and a format argument.However , in this case , the relative timings are very different and do n't reflect the above :"
"I 'm relatively new to the Python world , but this seems very straight forward.Google is yelling at me that this code needs to be optimized : The dashboard is telling me that this is using a ton of CPU.Where should I look for improvements ?"
"I am inspecting the logging.Logger.manager.loggerDict by doing : and the dict is as follows : My questions are : How come celery is involved in logging of various other non-celery apps ? Is it because logging is done in an async way and somehow logging framework detects presence of celery and uses it ? For two of my own files that are logging using logger = logging.getLogger ( __name__ ) , I see one is PlaceHolderObject and other two it is celery.utils.log.ProcessAwareLogger object - although these latter two are called in views and not in celery processes . How did it become this way thenThanks"
"I 'm sorry to ask this apparently simple question , but I am a python beginner and coul n't find an answer anywhere . I want to run a simple if statement , but python returns just : No matter which of the given alternatives I apply - it just does n't work . Do you have any ideas ?"
"I 'm trying to implement yielding results for a searching only a part of a word ( which is called autocomplete according to the Haystack docs if I 'm not mistaken ) . Example : Search `` gol '' Result `` goldfish '' What have I tried ? I did as asked in step 1 of the docs , I added the following line in my Index class : Then did python manage.py rebuild_index.Rebuilding the index however produced an error haystack.exceptions.SearchFieldError : The model ' < Person : Reginald > ' does not have a model_attr 'content ' . With Reginald being the first entry in my indexed table and Person being the model I indexed.Now indeed my model does n't have a field called content but as it is shown in the docs it should not need to have such a field.I am using Whoosh 2.4.1 , Django-haystack 1.2.7 and Django 1.4 ."
"I 'm trying to modify posts app from a django tutorial- https : //github.com/codingforentrepreneurs/Advancing-the-Blog/tree/master/src/postsI 'm creating a new field 'userc ' in a forms.py : I 've tried various methods but I 'm unable to display the selected user in the template . What should I add in views.py ? Edit : I 've tried { { obj.userc } } , { { instance.userc } } to display the selected user in templates.views.py"
Tensorflow provides ragged tensors ( https : //www.tensorflow.org/guide/ragged_tensor ) . PyTorch however does n't provide such a data structure . Is there a workaround to construct something similar in PyTorch ?
Say that I have this task : and I 'm using it like so : The problem I 'm facing is that there are a lot of repetitive tasks with the same arg param and it 's boggling down the queue.Is it possible to apply async only if the same args and the same task is not in the queue ?
"The problem is that I 'm losing a value after the zip call . This would be a bigger issue had it not been for the fact that gen is pure code.I do n't know whether or not it would be possible to create a function that behaves like this , it 's definitely possible if only one of the arguments to the zip function is a generator and the rest are `` normal '' iterators where all the values are known , and stored in memory . If that were the case you could just check the generator last.Basically what I am wondering is if there is any function in the python standard library that will act like I need it to in this case.Of course , in some cases one could just do something likeThen you only have to deal with a list.I could also add , that getting the last value that zip got from gen would also be a solution to this problem ."
"I have been following this URL to help me create template views using BrowserView . So far , it works OK and I am able to create a template with a view class.What I need to know is whether it is possible to pass arguments to methods inside the view class i.e.I need to add an extra argument to the still_dreaming function and process it inside the function like this : Then I need to call the function and pass an argument to it from my template . Something like this : Unfortunately , I do not know the correct way of passing arguments to the method . Any pointers will be greatly appreciated.EDIT : item/publication_date is just some variable and could be anything . It has just been defined previously ."
Given a list of classes inheriting from this base : ... and the following rules : Plugins can provide a list of plugins that they must run after.Plugins can provide a list of plugins that they must run before.The list of plugins may or may not contain all plugins that have been specified in ordering constraints.Can anyone provide a nice clean algorithm for ordering a list of plugins ? It will need to detect circular dependencies as well ... .I 've come up with a few versions but nothing particuarlly neat : I 'm sure some of you Art of Computer Programming types will relish the challenge : ) [ note : question given in Python but it 's clearly not only a Python question : pseudocode in any language would do ]
"I am learning Python for the past few days and I have written this piece of code to evaluate a postfix expression.Is there a way I can avoid that huge if else block ? As in , is there module that takes a mathematical operator in the string form and invokes the corresponding mathematical operator or some python idiom that makes this simple ?"
"I have a 3D surface plot . I would also like to plot slices of this plot in 2D , and somehow indicate on the 3D plot where the slices came from ( such as coloring the points along the slice to 'highlight ' the slice , or plotting an intersecting plane or something ) .Following is an example where I am simply setting a particular row to 0 so I can see where the slice is on the 3D plot ."
"I am using Pulp modeler with python to solve an integer programming problem . I am using IBM CPLEX as a solver . When I run my Python program , I have a lot of output messages in the console like : The CPLEX Optimizers will solve problems up to 1000 variables and 1000 constraints . IBM ILOG CPLEX Optimization Studio Preview Edition good for 48 more days ... I look for a solution on the internet and I fix the issue . So I disable the display by writing msg=0 as follows : Yesterday I removed some softwares from my computer and when I tried to run my Python program , Python says can not run cplex.exe . I find out that something went wrong with my environment variables ( all environment variable in path are erased ) . So I re-installed the CPLEX solver and I run exactly the same program but I still have the output messages now even with msg=0.What do you think is the problem ? And how can I disable the output messages ?"
"GO : Is there some way to communicate with a subprocess ( shell script / python script ) , which is waiting for input on stdin ? e.g . python script ( subprocess ) In the go program , I want to create a subprocess of this python script and provide it input on its stdin , whenever necessary and repeatedly , and take its output . Writing on stdout of Go program or reading/writing from a file will also do.This is roughly what I am trying , but nothing happens -"
"a in my_list returns true , but b in my_list returns `` ValueError : The truth value of an array with more than one element is ambiguous . Use a.any ( ) or a.all ( ) '' . I can get around this by converting the arrays to strings or lists first , but is there a nicer ( more Pythonic ) way of doing it ?"
"I am building a lemmatizer in python . As I need it to run in realtime/process fairly large amount of data the processing speedis of the essence . Data : I have all possible suffixes that are linked to all wordtypes that they can be combined with . Additionally I have lemmaforms that are linked to both their wordtype ( s ) and lemma ( s ) . The program takes a word as input and outputs its lemma.word = lemmafrom + suffixFor example ( Note : although the example is given in English I am not building a lemmatizer for English ) : word : forbiddinglemmaform : forbiddsuffix : inglemma : forbidMy solution : I have converted the data to ( nested ) dicts:1 ) Find all possible suffixes and word types that they are linked to.If the longest possible suffix is 3 characters long , the program tries to match 'ing ' , 'ng ' , ' n ' to the keys in suffixdict . If the key exists it returns a value ( a set of wordtypes ) .2 ) For each matching suffix search the lemmaform from the dict.If lemmaform exists it returns the wordtypes.3 ) Finally , the program tries to intersect the wordtypes produced in steps 1 ) ans 2 ) and if the intersection issucessful it returns the lemma of the word.My question : could there be a better solution to my problem from the prespective of speed ? ( Disregarding the option to keep frequent words and lemmas in the dictionary ) Help much appriciated ."
"Suppose I have the following pandas data frame : There are many userID 's and each one has 3 days and 4 features per day.What I want to do is for each feature , select 1 of the days at random and then cut down the matrix . So for instance , if feature 0 was day 1 , feature 1 used day 0 , feature 2 used day 0 and feature 3 used day 2 : And so forth.I 've come up with : I thought the this code works but it does not.But this seems slow . Is there a faster way to do it ?"
"It is very similar to this : How to tell if a string contains valid Python codeThe only difference being instead of the entire program being given altogether , I am interested in a single line of code at a time.Formally , we say a line of python is `` syntactically valid '' if there exists any syntactically valid python program that uses that particular line.For instance , I would like to identify these as syntactically valid lines : Because one can use these lines in some syntactically valid python programs.I would like to identify these lines as syntactically invalid lines : Because no syntactically correct python programs could ever use these linesThe check does not need to be too strict , it just need to be good enough to filter out obviously bogus statements ( like the ones shown above ) . The line is given as a string , of course ."
"To get to grips with PyTorch ( and deep learning in general ) I started by working through some basic classification examples . One such example was classifying a non-linear dataset created using sklearn ( full code available as notebook here ) This is then accurately classified using a pretty basic neural netAs I have an interest in health data I then decided to try and use the same network structure to classify some a basic real-world dataset . I took heart rate data for one patient from here , and altered it so all values > 91 would be labelled as anomalies ( e.g . a 1 and everything < = 91 labelled a 0 ) . This is completely arbitrary , but I just wanted to see how the classification would work . The complete notebook for this example is here.What is not intuitive to me is why the first example reaches a loss of 0.0016 after 1,000 epochs , whereas the second example only reaches a loss of 0.4296 after 10,000 epochsPerhaps I am being naive in thinking that the heart rate example would be much easier to classify . Any insights to help me understand why this is not what I am seeing would be great !"
"I am trying to produce a matrix of True and False values , which shows all the permutations for a given number of choices . So for 5 choices you would have the following output.I have been looking at using itertool 's permutations and combinations , but these work off position and not value which results in duplicates.I 'm sure there is a standard algorithm for this problem , but I 'm struggling to find its name ."
"I 'm attempting to install a custom build on heroku , so I 'm using a variety of ways to attempt a third part installing using the buildpacks . In my .buildpacks file I have : and in my Aptfile I have the following : libgeoip-dev which is a pre-requisite for geoip which is installed with the requirements.txt ( GeoIP==1.3.2 ) Here are my environment variables : The error message I am getting is : What is the smartest way to fix this ? I.e . I guess I can not change where the package manager installs . Is there a way around this ?"
"Until like one hour ago , I was convinced that in python Foo ( ) .bar ( ) was nothing more than a short hand for Foo.bar ( Foo ( ) ) which passes the instance as first parameter . In this example the last two lines do ( apparently ) the same thing : But now I have a class Animal that has a static method populate ( ) that returns a list of all animals known to man . Also each instance of Animal has a method populate ( ) that fills the properties of the instance with random values.The code works fine , but what made me suspicious was the fact that print Animal.populate ( qux ) called the static populate method ( and hence returned a list and did not populate poor qux ) . So apparently my conviction that Foo ( ) .bar ( ) was nothing more than a short hand for Foo.bar ( Foo ( ) ) is wrong.This raises various questions for me : What happens when I call Foo ( ) .bar ( ) ? What happens when I call Foo.bar ( Foo ( ) ) ? Is there an internal difference between the both ? Am I missing some fundamental concept of python ? If you had to write a class whose static populate method does something else than the populate method invoked over an instance of this class , which would be the way to go ? ( Yes it must be the same name . )"
"I have a string that looks like this : `` XaXbXcX '' . I 'm looking to match any lowercase letters surrounded by X on either side . I tried this in Python , but I 'm not getting what I 'm looking for :"
"I am researching and trying to understand the python GIL and best practices to use multithreading in python . I found this presentation and this videoI tried to reproduce the strange and crazy problems mentioned in the first 4 slides of the presentation . This problem also mentioned by the lecturer in the video ( first 4 minutes ) .I wrote this simple code to reproduce the problembut the results are completely different from the paper and video ! executing time with threading and without threading are almost the same . sometimes one of both case is a bit faster than the other.here is a result I got using CPython 3.7.3 under Windows 10 using a multicore architecture processor.also what I understand according to the video and the paper is GIL prevent real parallel execution of two thread at the same time in two core . so if this is true , Why the final count variable ( in multithreading case ) is not zero as expected and will be a different number at the end of each execution probably because of manipulation of threads at the same time ? does anything changes happen to GIL in newer pythons than the video and the paper ( which use python 3.2 ) cause these different ? thanks in advance"
"How do I generate all possible Newick Tree permutations for a set of species given an outgroup ? For those who do n't know what Newick tree format is , a good description is available at : https : //en.wikipedia.org/wiki/Newick_formatI want to create all possible Newick Tree permutations for a set of species given an outgroup . The number of leaf nodes I expect to process are most likely 4 , 5 , or 6 leaf nodes.Both `` Soft '' and `` hard '' polytomies are allowed.https : //en.wikipedia.org/wiki/Polytomy # Soft_polytomies_vs._hard_polytomieshttps : //biology.stackexchange.com/questions/23667/evidence-discussions-of-hard-polytomyShown below is the ideal output , with `` E '' set as the outgroupIdeal Output : However , any possible solutions that I 've come with using itertools , specifically itertools.permutations , have come across the problem of equivalent output . The last idea I came up with involved the equivalent output that is shown below.Equivalent output : Here is the start of my idea for a solution . However , I 'm not really sure what to about this problem besides itertools for now ."
I was wondering if it 's possible to use star unpacking with own classes rather than just builtins like list and tuple.And be able to write But I get : Which methods do I have to implement in order to make unpacking possible ?
"[ Note : Rereading this before submitting , I realized this Q has become a bit of an epic . Thank you for indulging my long explanation of the reasoning behind this pursuit . I feel that , were I in a position to help another undertaking a similar project , I would be more likely to get on board if I knew the motivation behind the question . ] I have been getting into Structure Synth by Mikael Hvidtfeldt Christensen lately . It is a tool for generating 3D geometry from a ( mostly ) context free grammar called Eisenscript . Structure Synth is itself inspired by Context Free Art . Context free grammars can create some stunning results from surprisingly simple rulesets.My current Structure Synth workflow involves exporting an OBJ file from Structure Synth , importing it into Blender , setting up lights , materials , etcetera , then rendering with Luxrender . Unfortunately , importing these OBJ files often brings Blender to a grinding halt as there can be thousands of objects with fairly complex geometry . I say 'fairly ' because Structure Synth only generates basic shapes , but a sphere represented by triangles still has many faces.Thus , generating the structures directly in Blender would be preferable to the current process ( Blender 's deep support for Python scripting should make this possible ) . An intelligent Python library could use Blender 's instancing abilities to use one mesh to generate myriad objects , thus saving memory . Plus Blender is a full-featured 3D suite and its ability to interpret a CFDG would provide creative possibilities far beyond what Structure Synth can offer.And so my question is how best to translate the Eisenscript grammar into a Python DSL . Here 's what a simple Eisenscript looks like : To explain , the first call to R1 ( line 2 ) will randomly invoke one of the two definitions of R1 . Each definition of R1 recursively calls R1 ( randomly invoking one of the two definitions ) and also creates a box . The first line kills generation after recursion has gone 2000 levels deep.Jeremy Ashkenas ( of CoffeeScript fame ) successfully implemented a context free DSL in Ruby using blocks . Internally , it works by creating a hash key for each rule 'name ' , and stores the blocks for each definition of that rule in an array , to be randomly chosen when the rule is invoked.The previous Eisenscript rule definitions would translate to the Ruby DSL like so : I am a novice Python user and so have been doing some research on Python 's functional programming capabilities . It seems like lambda is too limited to create something similar to Jeremy 's Ruby DSL , and , as far as I can tell , lambda is the only option for anonymous functions ? How might an experienced Pythonista approach the design ?"
"I have a python list of strings , let 's say : I want to create a new list whose elements are each element of elems repeated a fixed number of times ( let 's say twice ) , in a random order , but while making sure that two consecutive elements never have the same value.For example , [ `` D '' , `` B '' , `` A '' , `` B '' , `` D '' , `` C '' , `` A '' , `` C '' ] is a good result . [ `` D '' , `` B '' , `` A '' , `` B '' , `` D '' , `` C '' , `` C '' , `` A '' ] is not ( C is repeated in 6th and 7th position ) .The simplest idea is probbaly just : and then some code to take care of the repetitions , but all the solutions I can think of involve potentially infinite loops . Is there a simple and reliable way to do that ? Thanks ."
"How do you change the retrieval timeout for the Memcached/Elasticache caching backend in Django ? I 'm using Amazon 's Elasticache for caching content in Django , and I 'm frequently seeing errors like : I 've tried increasing the number of nodes in my Elasticache cluster , but that has had no effect . My next thought was to increase the timeout for the memcached retrieval , but the Django docs do n't seem to provide an option for this.There 's a `` TIMEOUT '' option , but that seems to define the default time after which the content expires , not the timeout of the HTTP request to the memcached server ."
"I have a GeoDjango model object that I want't to serialize to json . I do this in my view : The problem is that simplejson considers the a.area.geojson as a simple string , even though it is beautiful pre-generated json . This is easily fixed in the client by eval ( ) 'ing the area-string , but I would like to do it proper . Can I tell simplejson that a particular string is already json and should be used as-is ( and not returned as a simple string ) ? Or is there another workaround ? UPDATEJust to clarify , this is the json currently returned : The challenge is to have `` area '' be a json dictionary instead of a simple string ."
"I 'm not a SymPy expert , but I 've used it successfully in some of my lectures in the last years . However , sometimes it seems to be very slow with symbolic integration . Here 's an example that Mathematica computes almost instantly while SymPy needs a long time ( more than half a minute ) for it on my machine.Am I doing something wrong or is this the expected behavior ? Is there some trick to speed this up ? SymPy version is 1.0 , Python is 3.5.1 64-Bit ( Anaconda ) on Windows ."
"I have some large arrays ( ~100 million points ) that I need to interactively plot . I am currenlty using Matplotlib . Plotting the arrays as-is gets very slow and is a waste since you ca n't visualize that many points anyway.So I made a min/max decimation function that I tied to the 'xlim_changed ' callback of the axis . I went with a min/max approach because the data contains fast spikes that I do not want to miss by just stepping through the data . There are more wrappers that crop to the x-limits , and skip processing under certain conditions but the relevant part is below : This works pretty well and is sufficiently fast ( ~80ms on 1e8 points & 2k bins ) . There is very little lag as it periodically recalculates & updates the line 's x & y-data.However , my only complaint is in the x-data . This code duplicates the x-value of each bin 's left edge and does n't return the true x-location of the y min/max pairs . I typically set the number of bins to double the axis pixel width . So you ca n't really see the difference because the bins are so small ... but I know its there ... and it bugs me . So attempt number 2 which does return the actual x-values for every min/max pair . However it is about 5x slower . This takes roughly 400+ ms on my machine which becomes pretty noticeable . So my question is basically is there a way to go faster and provide the same results ? The bottleneck is mostly in the numpy.argmin and numpy.argmax functions which are a good bit slower than numpy.min and numpy.max.The answer might be to just live with version # 1 since it visually does n't really matter . Or maybe try to speed it up something like cython ( which I have never used ) . FYI using Python 3.6.4 on Windows ... example usage would be something like this :"
"I have a pandas data frame like this ; I want to make a new column roll_speed where it takes a rolling average speed of the last 5 positions . But I wan na put more detailed condition in it.Groupby leg ( it does n't take into account the speed of the rows in different leg.I want the rolling window to be changed from 1 to 5 maximum according to the available rows . For example in leg == 1 , in the first row there is only one row to calculate , so the rolling speed should be 10/1 = 10 . For the second row , there are only two rows available for calculation , the rolling speed should be ( 10+11 ) /2 = 10.5.My attempt : But it just returns NA for rows where less than five rows are available for calculation . How should I solve this problem ? Thank you for any help !"
"I have read-only access to a database that I query and read into a Pandas dataframe using pymssql . One of the variables contains dates , some of which are stored as midnight on 01 Jan 0001 ( i.e . 0001-01-01 00:00:00.0000000 ) . I 've no idea why those dates should be included – as far as I know , they are not recognised as a valid date by SQL Server and they are probably due to some default data entry . Nevertheless , that 's what I have to work with . This can be recreated as a dataframe as follows : The dataframe looks like : ... with the following dtypes : However , I routinely convert date fields in the dataframe to datetime format using : However , by chance , I 've noticed that the 0001-01-01 date is converted to 2001-01-01.I realise that the dates in the original database are incorrect because SQL Server does n't see 0001-01-01 as a valid date . But at least in the 0001-01-01 format , such missing data are easy to identify within my Pandas dataframe . However , when pandas.to_datetime ( ) changes these dates so they lie within a feasible range , it is very easy to miss such outliers.How can I make sure that pd.to_datetime does n't interpret the outlier dates incorrectly ?"
"In the following code , I want metaclass NameMeta to add attribute gender to MyName class in case this class does not declare that attribute.This is the output that I am getting : I know this error makes sense since name is a string.My question is , how can I access MyName class as an object in the metaclass so that I can add the attribute ?"
"I have seen many json reading problems in stackoverflow using pandas , but still I could not manage to solve this simple problem.DataMy attemptRequired outputPreferred methodhttps : //pandas.pydata.org/pandas-docs/stable/reference/api/pandas.io.json.json_normalize.htmlRelated linksPandas explode list of dictionaries into rowsHow to normalize json correctly by Python PandasJSON to pandas DataFrame"
"An n-dimensional array has 2n sides ( a 1-dimensional array has 2 endpoints ; a 2-dimensional array has 4 sides or edges ; a 3-dimensional array has 6 2-dimensional faces ; a 4-dimensional array has 8 sides ; etc. ) . This is analogous to what happens with abstract n-dimensional cubes.I want to check if all sides of an n-dimensional array are composed by only zeros . Here are three examples of arrays whose sides are composed by zeros : How can I check if all sides of a multidimensional numpy array are arrays of zeros ? For example , with a simple 2-dimensional array I can do this : While this approach works for 2D cases , it does not generalize to higher dimensions . I wonder if there is some clever numpy trick I can use here to make it efficient and also more maintainable ."
"I have a Django application that I am migrating from v1.8 to v1.10 . In the process of doing this work , I ran my application via : python -Wall manage.py runserverDoing so causes a number of Python warnings to appear in my console . I 'd like to have these warnings show up in my Django application log , so I can examine them later . I thought my application 's log handler would catch these warnings , but it does n't . The log handler looks like the following ( as taken from settings.py ) : How can I capture Python warnings ( with -Wall ) in my Django log for examination later ?"
"I was working on a project for class where my code was n't producing the same results as the reference code.I compared my code with the reference code line by line , they appeared almost exactly the same . Everything seemed to be logically equivalent . Eventually I began replacing lines and testing until I found the line that mattered.Turned out it was something like this ( EDIT : exact code is lower down ) : Now , this baffled me . I tried some experiments with the Python ( 2.7 ) interpreter , running tests using max on list comprehensions with and without the square brackets . Results seemed to be exactly the same.Even by debugging via PyCharm I could find no reason why my version did n't produce the exact same result as the reference version . Up to this point I thought I had a pretty good handle on how list comprehensions worked ( and how the max ( ) function worked ) , but now I 'm not so sure , because this is such a weird discrepancy.What 's going on here ? Why does my code produce different results than the reference code ( in 2.7 ) ? How does passing in a comprehension without brackets differ from passing in a comprehension with brackets ? EDIT 2 : the exact code was this : I do n't think this should be marked as duplicate -- yes , the other question regards the difference between comprehension objects and list objects , but not why max ( ) would provide different results when given a 'some list built by X comprehension ' , rather than ' X comprehension ' alone ."
"Two python interpreter sessions . The first is from python on CentOS . The second is from the built-in python on Mac OS X 10.7 . Why does the second session create strings of length two from the \U escape sequence , and subsequently error out ?"
"The following code adds tasks that perform some processing on files from the blobstore , it runs on a B2 backend so it has no timeout limit : tasks is a dictionary in the following form : x1 , y1 , z1 are all stringstools.debug is a function I wrote that sends messages to my local sever using urlfetch ( so I wo n't have to wait 20min to be able to read the logs ) : since tools.debug was n't in the code when it first failed , I know for sure it is n't the cause for the memory problems.I got this error : And right after it : again , I received it for the code above without the line : tools.debug ( `` add_tasks_to_process_files '' , `` adding_task '' ) Now , let me show you what I see in my debugger : full trace : http : //pastebin.com/CcPDU6s7Is there a memory leak in taskqueue.add ( ) ? Thanks"
"I have a list of 1500 emoji character dictionary in a json file , and I wanted to import those to my python code , I did a file read and convert it to a python dictionary but now I have only 143 records . How can I import all the emoji to my code , this is my code.This is my input sampleThis is my result , and the 143 denotes number of emoji . 143 word = word.replace ( `` �‍�‍�‍� '' , `` family '' ) word = word.replace ( `` Ⓜ '' , `` '' ) word = word.replace ( `` ♥ '' , `` '' ) word = word.replace ( `` ♠ '' , `` '' ) word = word.replace ( `` ⌛ '' , `` wait '' )"
"I have a function in a sub-module which needs to manipulate the variables from parent/interpreter Globals ( assertion , validation ) like this : If I do this , it works : But , If I dont pass explictely globals ( ) , how can I get access to interpreter/parent globals ( ) into my sub-module ? In IPython , it can be put in profile settings ."
"I want to understand the NumPy behavior.When I try to get the reference of an inner array of a NumPy array , and then compare it to the object itself , I get as returned value False.Here is the example : While on the other hand , with Python native objects , the returned is True.My question , is that the intended behavior of NumPy ? If so , what should I do if I want to create a reference of inner objects of NumPy arrays ."
"Python 3.6.5 and mypy 0.600I wrote the code : I do n't understand , why I have an error 'Incompatible types in assignment ' with a variable 'arr3 ' . Dog is a class which inherits from a Animal . For example , I do n't have an error with variable 'arr2 ' ."
"I have a frustrating problem that only manifests itself when plotting filled contour plots on 3D axes and only in certain situations.Here is an example of the issue I am experiencing : andThese are the same data at different contouring intervals.You 'll notice on the left side of the domain there is mis-filling occurring . This is a plot with the Z points squished into the Z=0 plane , via a plotting command likeThe miscontouring happens regardless of alpha level or colormap used , but is sensitive to the number of levels . The use of zdir and offset do not effect the mis-contouring ( the artifact just occurs on the Z surface . If I do not fill the contour , there is no mis-contouring . I can also alter the domain to sometimes make the issue better ( or worse ) , but I have many plots to make within the same domain so that is not a fix.This issue does not occur when the same data is plotted on 2D axes , e.g . : This plot has some extra data on it , but you can see that the filled contouring does not have the same artifact from mis-filling the contour that occurs on the 3d axes.Below is a script you can run to reproduce the issue . This code will produce the plot : In all of the cases I have observed this mis-contouring the bad triangle always has a vertex near the bottom left of the domain . My data is regularly gridded and for the domain in question is uniform in X and Y . In this case the mis-filling will go away if the number of contour levels is reduced . In some other cases this does not always help or just changes the visual appearance of the error . In any case , even at very coarse contouring I still get errors in a subset of my plots.Has anyone seen this before and found a fix for it ? Am I overlooking something ? I 'm open to workarounds that do n't involve lowering my contouring level ( which does reduce the errors overall ) . If others are in agreement that this could be a bug in the mplot3d , I will file a bug report with them ( Issue opened here ) . I have a feeling the problem lies with contouring very strong gradients when the levels option causes dense contours , but oddly only on 3d axes.Relevant version information : Python 3.4.1matplotlib 1.4.3numpy 1.9.0"
"I yould like to have generators that defer to other generators , e.g.Is the explicit loop in gz ( ) the only way to do this , or are there better alternatives ?"
"I have a pandas dataframe with a column of lists.df : I need the matrixAn efficient way to do this ? Note : When I try df.inputs.as_matrix ( ) the output iswhich has shape ( 4 , ) , not ( 4,3 ) as desired ."
"I work on a set of python unit tests that are currently built using pythons built in testing framework . I would like to write paramaterized tests that will execute multiple times based on the set of data I give it.ie . if my data set is [ 1,2,3,4 ] my test function would run four times using the input in my data set . From my understanding this is n't possible currently in the built in framework , unless I put a loop in my test function . I do n't want to do this because I need the test to continue executing even if one input fails.I 've seen that it 's possible to do using nose , or pyTest . Which is the best framework to use ? Is there another framework I could use that would be better than either of these ? Thanks in advance !"
"I ’ m new to programming and I ’ m reading a book called How To Think Like A Computer Scientist . In the fourth chapter , it talks about functions.At the end of the chapter , there ’ s an exercise that asks me to draw the following pattern using Python ’ s turtle module.I was examining this picture and decided to split it into two : 1 ) the lines in the middle and 2 ) the squares that go on top of each other like a spiral.I drew the first part using this code : When I run it , it draws this : Then , I created the draw_square function and managed to draw the first square : When I run it , it draws this : Now I ’ m stuck . I don ’ t know where to go from here . I don ’ t know how to go about drawing all the other squares . I can ’ t figure out where to place the turtle , and how many degrees to tilt the square ( probably 20 , like the lines , but I don ’ t know how to implement it ) … Anyways , you guys got any tips ? Any suggestions ? I ’ m trying not to skip any exercises on the book and this one got me ."
"Let 's say I have these two models : And I 'm using a simple TabularInline to show Components inside the Distribution admin form : So , my goal is to validate if the percentages of all the Components of the Distribution sum 100 before saving it . Sounds simple , so I did : But this will never work work , because in Django all objects are saved before saving its foreign-key or many2many related objects , this is not a flaw , it has a reason : it can not save the related objects first , because the object to which they are related does n't have an id defined yet ( id is None until the object is saved for the first time in the DB ) .I 'm sure I 'm not the first guy to run into this issue . So , is there a way to accomplish what I 'm trying to do ? I was thinking maybe a admin hack using TabularInline or ModelAdmin ... ?"
"I am pretty new to Bottle and Cork . I am working on a web app using Bottle as the framework and MongoDB as the backend . I am trying to figure out how to implement a user-login system using Cork . I read documents and examples of Cork , and I tried to set things up . I have successfully hooked up the cork instance with Mongodb because I saw the database name , which I passed into the Cork ( ) constructor as a parameter ( backend ) , showed up in mongo shell . But then I have no idea how to keep going.I read all the Cork methods ' source code and they all make sense to me . But a lot of them require to have a user with enough role level to be called like `` list_roles ( ) '' or `` create_users ( ) '' . I understand that they are designed to be called by admin . But my question is how can I get started to create some users and test things out when I initially have no users at all ? The following is what I got so far , and I just need some simple guidance or sample code to start . I would really appreciate any help ! filename : api.py"
"I have a large dataframe df ( ~100 columns and ~7 million rows ) and I need to create ~50 new variables / columns which are simple transformations of the current variables . One way to proceed would be with many .apply statements ( I 'm just using transform* as a placeholder for simple transformations such as max or squaring ) : Another way would be to first create a dictionaryand then write one .apply combined with .concat : Is one method preferred over the other , either in how 'Pythonic ' it is , or efficiency , scalability , flexibility ?"
"I 'm working with timeseries data that represents vectors ( magnitud and direction ) . I want to resample my data and use the describe function as the how parameter.However , the describe method uses a standard average and I want to use a special function to average direction . Because of this , I implemented my own describe method based on the implementation of pandas.Series.describe ( ) : The problem is that when I do : I get this exception ( last few lines are shown ) : The question is : how do I implement my own describe function so that it works with resample ?"
"I 'm sure there are answers to this simple question but I do n't really know how to formulate it in English ( ashamed ) , so decided to ask human beings.Suppose I have a list of lists : What would be the most pythonic way to obtain the following output ?"
a tuple is a comma-separated list of valuesso the valid syntax to declare a tuple is : But what I often see is a declaration like this : What is the benefit of enclosing tuples in parentheses ?
"I want to understand why : a = [ ] ; del a ; anddel a [ : ] ; behave so differently.I ran a test for each to illustrate the differences I witnessed : I did find Different ways of clearing lists , but I did n't find an explanation for the differences in behaviour . Can anyone clarify this ?"
"How to initialize backrefs of mappers without some queries through a session ? For example , I have two models , named `` Client '' and `` Subject '' in follow code : Then , somewhere in my code , I want to get the backref client of the class Subject like this , but that raises an exception : After a query to Client like : Attribute client was created after a query to the related model ( mapper ) .I do n't want to make such `` warming '' queries !"
"I understand that this could be argued as a non-issue , but I write software for HPC environments , so this 3.5x speed increase actually makes a difference.I used dis to have a look at the code , and I assume float ( ) will be slower as it requires a function call ( unfortunately I could n't dis.dis ( float ) to see what it 's actually doing ) .I guess a second question would be when should I use float ( n ) and when should I use n * 1.0 ?"
"I have a Flask RESTful API app that has the following SQLAlchemy class with a self-referential key representative of an adjacency list : I want nested JSON returned from the Medications class , along the lines ofAs per how-to-create-a-json-object-from-tree-data-structure-in-database I created the serializer classand subclassed this to my Medications class , as per class Medications ( db.Model , JsonSerializer ) : I then call Models.to_json ( ) to get my serialized JSON output , but alas , the object is empty : { 'parent_id ' : None , 'type ' : None , 'children ' : [ ] , 'name ' : None , 'id ' : None } However , as a test , if I create a Flask Restless endpoint , as perI get the following output : along with some pagination information . Am curious why I am getting an empty dictionary from the method using the JsonSerializer class . I would use the Flask Restless method , but since I am using Flask as a wsgi app , it would screw up my endpoints , plus , the nodes with children : [ ] are not desired in the output ."
"Let 's say I have a length 30 array with 4 bad values in it . I want to create a mask for those bad values , but since I will be using rolling window functions , I 'd also like a fixed number of subsequent indices after each bad value to be marked as bad . In the below , n = 3 : I would like to do this as efficiently as possible because this routine will be run many times on large data series containing billions of datapoints . Thus I need as close to a numpy vectorized solution as possible because I 'd like to avoid python loops . For avoidance of retyping , here is the array :"
"consider dfhow do I fill those NaN with ( 0 , 0 ) ? I 've put this hack together but I 'm assuming there is a more direct way . And this does n't work for a pd.Series"
"I 'm developing a small tool on Python which basically launches a set of scripts on a folder . I need to package this on a stand-alone binary and I 'm using py2exe for it.My current code use os.path.listdir ( ) to get all the .py files on a folder , and then launch some of them using execfile ( ) funcion based on user input on a PyQT interface.My code works as expected if executed through the main Python file , but fails when compiled with py2exe . The exception is : for the python files launched with execfile ( ) .I 'm currently bundling with `` bundle_files '' : 1 and zipfile = None . I tried to include these files messing with includes and packages but without luck . Can you help me to configure py2exe properly ? This is my current setup.py : And I 'm getting the following traceback :"
"I 'm trying to update a two dimensional tensor in a nested while_loop ( ) . When passing the variable to the second loop however , I can not updated it using tf.assign ( ) as it throws this error : Somehow it works fine if I create the variable outside the while_loop and use it only in the first loop.How can I modify my 2D tf variable in the second while loop ? ( I 'm using python 2.7 and TensorFlow 1.2 ) My code :"
"Eprime outputs a .txt file like this : I want to parse this and write it to a .csv file but with a number of lines deleted.I tried to create a dictionary that took the text appearing before the colon as the key andthe text after as the value : both of the scripts below produce garbage : If I could set up the dictionary , I can write it to a csv file that would look like this ! ! :"
"Summary : when a certain python module is imported , I want to be able to intercept this action , and instead of loading the required class , I want to load another class of my choice.Reason : I am working on some legacy code . I need to write some unit test code before I start some enhancement/refactoring . The code imports a certain module which will fail in a unit test setting , however . ( Because of database server dependency ) Pseduo Code : So , ideally , when python excutes the import line above in a unit test , an alternative class , says MockDataLoader , is loaded instead . I am still using 2.4.3 . I suppose there is an import hook I can manipulateEditThanks a lot for the answers so far . They are all very helpful.One particular type of suggestion is about manipulation of PYTHONPATH . It does not work in my case . So I will elaborate my particular situation here.The original codebase is organised in this wayMy goal is to enhance the Other class in the Other module . But since it is legacy code , I do not feel comfortable working on it without strapping a test suite around it first.Now I introduce this unit test codeThe content is simply : When the CI server runs the above test , the test fails . It is because class Other uses LegacyDataLoader , and LegacydataLoader can not establish database connection to the db server from the CI box.Now let 's add a fake class as suggested : Modify the PYTHONPATH to Now the test fails for another reasonIt has something to do with the way python resolves classes/attributes in a module"
"I have a Sphinx project with a TOC ( index.rst ) that includes : maxdepth : 2 . The problem is I want to reduce the depth to 1 for the release section so that it does n't include the list of release notes in the main TOC ( the list is too long ) .It seems that the TOC list can be modified using a doctree-resolved event handler , but I ca n't figure out how to modify the TOC tree in the event handler :"
"I have data ( counts ) indexed by user_id and analysis_type_id obtained from a database . It 's a list of 3-tuple . Sample data : where the first item of each tuple is the count , the second the analysis_type_id , and the last the user_id.I 'd like to place that into a dictionary , so i can retrieve the counts quickly : given a user_id and analysis_type_id . It would have to be a two-level dictionary . Is there any better structure ? To construct the two-level dictionary `` by hand '' , I would code : Where user_id is the first dict key level , analysis_type_id is the second ( sub- ) key , and the count is the value inside the dict.How would I create the `` double-depth '' in dict keys through list comprehension ? Or do I need to resort to a nested for-loop , where I first iterate through unique user_id values , then find matching analysis_type_id and fill in the counts ... one-at-a-time into the dict ?"
"Disabling a Pylint check or getting around one of its warnings , should not be without a clear reason . I would like to be able to comment these reasons at the place I 'm disabling it ; so far , without success.As an example , let a class with just a constructor and a single method . The kind of thing Pylint warns about with reasons , while there may be as much good reasons to disable this warning locally.With the above , Pylint not only still warns about “ too few public methods ” but even additionally complains about “ bad option value 'R0903 -- - Closure object ' ” .The question has a wider rational than this single example ( may be I 'm not aware of a better way to achieve closures in Python ) , and I would like to be able to comment most of these in‑line directives , on the same line , for clarity , and simplicity . By the way , may also be useful to remind about what an option is for . As an example , reminding # pylint : disable=R0903 -- - Too few public methods ( to stay on the same example ) .In less words : is there a way to comment Pylint in‑line directives ?"
"I 'm still learning Python , and I have a question I have n't been able to solve . I have a very long string ( millions of lines long ) which I would like to be split into a smaller string length based on a specified number of occurrences of a delimeter.For instance : In this case I would want to split based on `` // '' and return a string of all lines before the nth occurrence of the delimeter.So an input of splitting the string by // by 1 would return : an input of splitting the string by // by 2 would return : an input of splitting the string by // by 3 would return : And so on ... However , The length of the original 2 million line string appeared to be a problem when I simply tried to split the entire string and by `` // '' and just work with the individual indexes . ( I was getting a memory error ) Perhaps Python ca n't handle so many lines in one split ? So I ca n't do that . I 'm looking for a way that I do n't need to split the entire string into a hundred-thousand indexes when I may only need 100 , but instead just start from the beginning until a certain point , stop and return everything before it , which I assume may also be faster ? I hope my question is as clear as possible.Is there a simple or elegant way to achieve this ? Thanks !"
"I am learning all about Python classes and I have a lot of ground to cover.I came across an example that got me a bit confused.These are the parent classesChild classes are : Grandchild class is : Does n't Class M inherit Class Z through inheriting from Class B or what would the reason be for this type of structure ? Class M would just ignore the second time Class Z is inherited would n't it be , or am I missing something ?"
"Does Python have a naming convention for variables that are functions ? I could n't see anything specific for this in PEP-8 ( other than naming variables ) .Since functions are first-class objects in Python , is using a _fn suffix , or something similar , a recognized convention ? EDIT : Updated with more realistic exampleExample :"
"I 'm trying make a generalized ufunc using numpy API . The inputs are one ( n x m ) matrix and a scalar , and outputs are two matrix ( ( n x p ) and ( p x m ) ) . But I do n't knowing how to do it . Someone could help me ? In init function I 'm using PyUFunc_FromFuncAndDataAndSignature function with signature : I can read the inputs ( matrix and scalar ) , but I wanted to use the scalar input like the dimension p in signature . Is it possible ? Here a example code that just print the inputs : This code compiles and works . The python script is here : And the terminal output is : My doubt is how use the scalar input ( 2 in the case ) like dimension p of outputs matrices . In example p = 1 , and I do n't set it ."
"I tried to get an estimate of the prediction time of my keras model and realised something strange . Apart from being fairly fast normally , every once in a while the model needs quite long to come up with a prediction . And not only that , those times also increase the longer the model runs . I added a minimal working example to reproduce the error.The time does not depend on the sample ( it is being picked randomly ) . If the test is repeated , the indices in the for loop where the prediction takes longer are going to be ( nearly ) the same again.I 'm using : For my application I need to guarantee the execution in a certain time . This is however impossible considering that behaviour . What is going wrong ? Is it a bug in Keras or a bug in the tensorflow backend ? EDIT : predict_on_batch shows the same behavior , however , more sparse : y_pred = model ( sample , training=False ) .numpy ( ) shows some heavy outliers as well , however , they are not increasing.EDIT 2 : I downgraded to the latest tensorflow 1 version ( 1.15 ) . Not only is the problem not existent anymore , also the `` normal '' prediction time significantly improved ! I do not see the two spikes as problematic , as they did n't appear when I repeated the test ( at least not at the same indices and linearly increasing ) and are percentual not as large as in the first plot.We can thus conclude that this seems to be a problem inherent to tensorflow 2.0 , which shows similar behaviour in other situations as @ OverLordGoldDragon mentions ."
"Lots of bokeh examples use the hold command . I can see from the documentation that it 's been deprecated since version 0.8.0So I tried this : but no joy ... attribute 'hold ' does not existHow do I update such examples to work on recent versions of bokeh ? As pointed out below , the answer to this question : bokeh overlay multiple plot objects in a GridPlot says that hold is deprecated.I think ( from 3 seconds of experimentation ) , that hold commands can just be removed . Since I do n't know what it was meant to do , I ca n't verify this is correct : - ) .I would be good to have that confirmed here ."
"I have something which is an awful lot like a list comprehension in Python , except that it shares mutable state between iterations . Is there any way to do it with a list comprehension ? which returns [ 1 , 18 , 9 , 22 , 11 ] . Here the important thing is the batch function , not f ( x ) which is here just a simple example to illustrate the issue.Alternatively I could implement using a generator : But it smells a little awkward . What I 'm looking for is something like this ..."
"I have code like the following : What exactly happens inside PyArg_ParseTuple ? My guess is that callback gets the function pointer I passed to args ( also PyObject* ) . How does PyArg_ParseTuple convert the function pointer to PyObject* ? What I want to know is what happens if I pass in the same callback function pointer twice . I think callback gets allocated a new PyObject inside PyArg_ParseTuple , so it will get a different memory address each time , but will contain the same callback function pointer.But if I PyObject_Hash callback , it will produce a different value each time , right ? ( since address is different each time.. )"
I 'm using Sphinx to document my Python package . When I use the automodule directive on my module : It prints everything including the GPL notice in the docstring . Is there any way to tell Sphinx to ignore the docstring/GPL or should I leave it included in the documentation ?
"I have keywords that are all stored in lower case , e.g . `` discount nike shoes '' , that I am trying to perform entity extraction on . The issue I 've run into is that spaCy seems to be case sensitive when it comes to NER . Mind you , I do n't think that this is spaCy specific . When I run ... ... nothing is returned.When I run ... I get the following results ... Should I just title case everything ? Is there another workaround that I could use ?"
"I ran a comparison of several ways to access data in a DataFrame . See results below . The quickest access was from using the get_value method on a DataFrame . I was referred to this on this post.What I was surprised by is that the access via get_value is quicker than accessing via the underlying numpy object df.values.QuestionMy question is , is there a way to access elements of a numpy array as quickly as I can access a pandas dataframe via get_value ? SetupTesting 10000 loops , best of 3 : 108 µs per loop The slowest run took 5.42 times longer than the fastest . This could mean that an intermediate result is being cached . 100000 loops , best of 3 : 8.02 µs per loop The slowest run took 4.96 times longer than the fastest . This could mean that an intermediate result is being cached . 100000 loops , best of 3 : 9.85 µs per loop The slowest run took 19.29 times longer than the fastest . This could mean that an intermediate result is being cached . 100000 loops , best of 3 : 3.57 µs per loop"
"Since I 'm starting to get the hang of Python , I 'm starting to test my newly acquired Python skills on some problems on projecteuler.net.Anyways , at some point , I ended up making a function for getting a list of all primes up until a number ' n'.Here 's how the function looks atm : It seems to work fine , although one there 's one thing that bothers me.While commenting the code , this piece suddenly seemed off : If the candidate IS NOT divisible by the prime we examine the next candidate located at ' c + 1 ' . No problem with that.However , if the candidate IS divisible by the prime , we first pop it and then examine the next candidate located at ' c + 1'.It struck me that the next candidate , after popping , is not located at ' c + 1 ' , but ' c ' , since after popping at ' c ' , the next candidate `` falls '' into that index.I then thought that the block should look like the following : This above block seems more correct to me , but leaves me wondering why the original piece apparently worked just fine.So , here are my questions : After popping a candidate which turned out not be a prime , can we assume , as it is in my original code , that the next candidate is NOT divisible by that same prime ? If so , why is that ? Would the suggested `` safe '' code just do unnecessary checks on the candidates which where skipped in the `` unsafe '' code ? PS : I 've tried writing the above assumption as an assertion into the 'unsafe ' function , and test it with n = 100000 . No problems occurred . Here 's the modified block :"
"I 'm riffing from the information here : Metaclass not being called in subclassesMy problem is that I 'm unable to create an instance of an object using this class registry . If I use `` regular '' construction methods , then it seems to instantiate objects correctly ; but when I try to use the class object associated with registry , then I get an error that I 'm passing an incorrect number of arguments . ( Seems to be calling the metaclass new and not my constructor ... ? ? ) I 'm not clear why it 's failing , because I thought I should be able to create an instance from the class object by using `` callable '' syntax.Seems I 'm getting the metaclass put in the registry and not the class itself ? But I do n't see an easy way to access the class itself in the new call.Here is my code example , which fails to instantiate a variable 'd ' : Thanks for any help ."
"I have a custom function that authenticate a request . I 'm trying to mock this module during test but no luck so far This is my viewAnd in the test i 'm trying to mock authenticate_request so it would not raise an error Ca n't make it work . Any suggestions ? Thanks , Python 2.7 , Django 1.8 ."
"I am trying to upgrade from Django 1.7.1 to 1.8 on my dev env . I seem to be having an issue with one of my models , I think a core file got upgraded and its messing with my model . I cant seem to figure out what 's causing it to die.This is the only error I get when I attempt to run a manage.py testI tried changing the class name and looked around my model and cant seem to find anything that would be causing an override errorThis is the class :"
"I have a setup.py script which needs to probe the compiler for certain things like the support for TR1 , the presence of windows.h ( to add NOMINMAX define ) , etc . I do these checks by creating a simple program and trying to compile it with Distutils ' Compiler class . The presence/lack of errors is my answer.This works well , but it means that the compiler 's ugly error messages get printed to the console . Is there a way to suppress error messages for when the compile function is called manually ? Here is my function which tries to compile the program , which now DOES eliminate the error messages by piping the error stream to a file ( answered my own question ) : Here is a function which uses the above to check for the presence of a header ."
"Note : Do not mention Numpy for matrix creation , as I can not use that specific library.I 've been working on a Python program that checks if all the elements inside a board ( the board size can change ) are connected . For example , this board 's elements are all connected : However , the current program I have is faulty , as some specific cases return the opposite Boolean value . What should I be doing instead ? This is the current code I have :"
"I 'd like to add the 'http ' scheme name in front of a given url string if it 's missing . Otherwise , leave the url alone so I thought urlparse was the right way to do this . But whenever there 's no scheme and I use get url , I get /// instead of '// ' between the scheme and domain.How do I convert this url so it actually looks like :"
"So , I 'm not the most knowledgeable at math and was hoping to get some much needed feedback . My goal for starters is just to have an image chase my mouse . There are a few things happening that I did n't expect that I 'd like feedback on . 1 ) The chase is very rigid , and follows the mouse in a very ... `` angular '' way , only changes directions at what seems to be set points on the screen ( middle x-axis , middle y-axis and diagonals ) .2 ) I want the speed to be a constant , but the image moves faster as it gets closer to the mouse.3 ) When the image reaches the mouse , it `` slingshots '' passed the mouse creating a blurry frenzy instead of reaching the mouse position and stopping.Those are my main concerns , but if you see anything that I might be misunderstanding PLEASE tell me . I 'm dying to understand this whole vector/trigonometry ( to soon get into physics ) thing . ( The entire program can be seen here http : //ideone.com/6OxWLi ) Thanks in advance for any help !"
"Just curious , I tried from __future__ import * , but I received this error : Well , that makes sense . A __future__ import is a little special and does n't follow the normal rules , but it got me to thinking : how can I import all the future features ?"
"Relatively new to Python , and I saw the following construct in the PyFacebook library ( source here : http : //github.com/sciyoshi/pyfacebook/blob/master/facebook/init.py # L660 ) . I 'm curious what this does because it appears to be a class that inherits from itself.what is this doing ? Tangentially related , I 'm using PyDev in Eclipse and it 's flagging this as an error . I 'm guessing that 's not the case . Anyway to let Eclipse know this is good ?"
"Given a regexp , I would like to generate random data x number of time to test something.e.g.Of course the objective is to do something a bit more complicated than that such as phone numbers and email addresses.Does something like this exists ? If it does , does it exists for Python ? If not , any clue/theory I could use to do that ?"
"If I am retrieving an object with django , I can use .select_related ( ) to instruct django to get all foreign key objects as well , to wit : If I already have obj , without it having been .select_related ( ) , that is : is there any way to get django to fill in all of its foreign key relations ? Something like :"
"Say I have a function that runs a SQL query and returns a dataframe : I would like to : Be able to memoize my query above with one cache entry per value of query_string ( i.e . per query ) Be able to force a cache reset on demand ( e.g . based on some flag ) , e.g . so that I can update my cache if I think that the database has changed.How can I do this with joblib , jug ?"
"Here is my example : I first create dataframe and save it to fileThen df.col_1 [ 0 ] returns [ ' a ' , ' b ' , 's ' ] a listLater I read it from file : Now df_1 [ 'col_1 ' ] [ 0 ] returns `` [ ' a ' 's ' ] '' a string.I would like to get list back . I am experimenting with different read_csv settings , but so far no luck"
"I 've noticed that when certain Numpy float64 values are saved as Excel file ( via a Pandas DataFrame ) , they get changed . First I thought this has to do with some imprecision in Excel , but Excel seems to encode floating point numbers as double precision , so I am a bit confused about this observation.Why is it not possible to read the same float64 back from Excel that was previously written ? I also tried this with Openpyxl ( .xlsx format ) and Xlwt ( .xls format ) as engines . While the former produced the same erroneous result as xlsxwriter , Xlwt was actually working as expected and wrote the float according to the exact variable value . Is there perhaps a parameter that I miss for the .xlsx format writer engines ?"
"Consider the following numpy code : Here : A and B are 2D arrays with the same number of columns ; start and end are scalars ; mask is a 1D boolean array ; ( end - start ) == sum ( mask ) .In principle , the above operation can be carried out using O ( 1 ) temporary storage , by copying elements of B directly into A.Is this what actually happens in practice , or does numpy construct a temporary array for B [ mask ] ? If the latter , is there a way to avoid this by rewriting the statement ?"
"Is there a way to stop python from creating .pyc files , already in the shebang ( or magic number if you will ) of the Python script ? Not working :"
I 'm trying to add a QLabel to a QTreeWidgetItem but Python crashes on the setItemWidget call.Any ideas why it crashes ? Here is the code :
"In terms of I/O , I 'd expect Python and C to have similar performance , but I 'm seeing C being from 1.5 to 2 times faster than Python for a similar implementation.The task is simple : concatenate thousands of ~250 bytes text files , each containing two lines : The header is the same for all files , so it is read only once and the output file will look like : Here is my implementation in C : And in Python : And a benchmark of 10 runs each - the files are in a local network drive in a busy office , so I guess that explains the variation : Also , a profile run of the python implementation indeed says that 70 % of the time is spent with io.open , and the rest with readlines . Even if readlines is extremely slower than fgets , the time spent by python with io.open only is larger than total runtime in C. And also , in the end , both readlines and fgets will read the file line by line , so I 'd expect more comparable performance.So , into my question : in this particular case , why is python so much slower than C for I/O ?"
"The built-in int takes two parameters : However , ( in CPython 3.4.0 ) inspect.signature shows 0 : in contrast with a user-defined function : The docs for inspect.signature do say : Some callables may not be introspectable in certain implementations of Python . For example , in CPython , some built-in functions defined in C provide no metadata about their arguments.But they also say : Raises ValueError if no signature can be provided , and TypeError if that type of object is not supported.So I am surprised that I did not get a ValueError and instead got what appears to be an incorrect signature.Is there a way to reliably ( and programmatically ) determine when it is not possible to get the parameters for a callable with inspect ? That is , if I am given something like int , is there a way to distinguish between `` this thing does not have any parameters '' and `` it is not possible to determine what parameters this thing has '' ?"
"Here 's something I 've always wondered about . I 'll pose the question for Python , but I would also welcome answers which address the standard libraries in Java and C++.Let 's say you have a Python list called `` my_list '' , and you would like to iterate over its unique elements . There are two natural approaches : orThe tension is that iterating over a list is faster than iterating over a set , but it takes time to convert a set to a list . My guess is that the answer to this question will depend on a number of factors , such as : How many times will we need to iterate ? How big is the original list ? How many repetitions in the original list should we expect ? So I guess I 'm looking for a rule of thumb of the form `` If the list has x many elements with each element repeated no more than y times and you only need to iterate z times then you should iterate over the set ; otherwise you should convert it to a list . ''"
"I 'm facing a very strange issue with pyspark on macOS Sierra . My goal is to parse dates in ddMMMyyyy format ( eg : 31Dec1989 ) but get errors . I run Spark 2.0.1 , Python 2.7.10 and Java 1.8.0_101 . I tried also using Anaconda 4.2.0 ( it ships with Python 2.7.12 ) , but get errors too.The same code on Ubuntu Server 15.04 with same Java version and Python 2.7.9 works without any error.The official documentation about spark.read.load ( ) states : dateFormat – sets the string that indicates a date format . Custom date formats follow the formats at java.text.SimpleDateFormat . This applies to date type . If None is set , it uses the default value value , yyyy-MM-dd.The official Java documentation talks about MMM as the right format to parse month names like Jan , Dec , etc . but it throws a lot of errors starting with java.lang.IllegalArgumentException.The documentation states that LLL can be used too , but pyspark does n't recognize it and throws pyspark.sql.utils.IllegalArgumentException : u'Illegal pattern component : LLL'.I know of another solution to dateFormat , but this is the fastest way to parse data and the simplest to code . What am I missing here ? In order to run the following examples you simply have to place test.csv and test.py in the same directory , then run < spark-bin-directory > /spark-submit < working-directory > /test.py.My test case using ddMMMyyyy formatI have a plain-text file named test.csv containing the following two lines : and the code is the following : I get errors . I tried also moving month name before or after days and year ( eg : 1989Dec31 and yyyyMMMdd ) without success.A working example using ddMMyyyy formatThis example is identical to the previous one except from the date format . test.csv now contains : The following code prints the content of test.csv : The ouput is the following ( I omit the various verbose lines ) : UPDATE1I made a simple Java class that uses java.text.SimpleDateFormat : This code does n't work on my environment and throws this error : but works perfectly on another system ( Ubuntu 15.04 ) . This seems a Java issue , but I do n't know how to solve it . I installed the latest available version of Java and all of my software has been updated.Any ideas ? UPDATE2I 've found how to make it work under pure Java by specifying Locale.US : Now , the question becomes : how to specify Java 's Locale in pyspark ?"
Is there a way for Airflow to skip current task from the PythonOperator ? For example : And also marking the task as `` Skipped '' in Airflow UI ?
"You know , to turn list : into list : You simply do it like this : It 's fast and pythonic . But what if i need to turn this list : to : What 's the pythonic way to do it ?"
"I want to capture all urls beginning with the prefix /stuff , so that the following examples match : /users , /users/ , and /users/604511/edit . Currently I write multiple rules to match everything . Is there a way to write one rule to match what I want ?"
"I 'm a very experienced developer - done a lot of heavy duty work with Delphi , C # and C++ for years . I have always adhered very closely to the guidelines for structured programming , OOP , loosely coupled modular designs etc - and since all the languages I 've used have built-in ways of enforcing these concepts - access control , static types , interface and abstract class support etc - I rely on these to structure my code.Now , I have been doodling with Python for a few months . I am impressed by its many wonderful features - but I sorely miss the built-in constraints that make it easy to keep code modularized and organized . And , unfortunately , I see an awful lot of 'spaghetti code ' out there written in Python , even from very respectable sources . I wo n't single anyone out but I have a few books written by major league pythonistas with examples replete with designs ( better put - 'anti-designs ' ) that make me shudder . It seems to me that because Python is so easy to use , it 's also very easy to abuse . I do try to discipline myself when I code in Python , but I find it takes a lot of extra work to implement and often I have to set up and adhere to constraints simply based on my own memory of the design with no help from the language at all . And since there is no 'compile time ' checking , it 's doubly difficult - often you do n't discover a design flaw until you actually RUN that segment of code.So , I 'm looking for very specific information : some examples or better still a book of WELL STRUCTURED Python designs and design techniques - how to best implement encapsulation , indirection , very loosely coupled designs , etc . Bad design IMO from a prominent python book author - ( with obfuscation )"
I 'm writing a python script which I would like to be able to both call from the command line and import as a library function.Ideally the command line options and the function should use the same set of default values.What is the best way to allow me to reuse a single set of defaults in both places ? Here 's the current code with duplicate defaults .
"I see that the Python syntax for a namedtuple is : Why is n't it simpler like so : Its less verbose ,"
So I have the values : and I want to convert the above dictionary to be : my function : but I 'm getting this as my output instead :
"type.__setattr__ is used for classes , basically instances of metaclasses . object.__setattr__ on the other hand , is used for instances of classes . This is totally understood . I do n't see a significant difference between the two method , at least at Python level , I notice the two use the same procedures for attribute assignment , correct me if I 'm wrong : Suppose a is an instance of a user-defined class , just a normal class : then a.x = .. invokes type ( a ) .__setattr__ ( ... ) which performs the following steps : Note : type ( a ) .__setattr__ will find __setattr__ in object builtin class 1 ) Look for a data descriptor in type ( a ) .__mro__ . 2 ) If a data descriptor was found , call its __set__ method and exit . 3 ) If no data descriptor was found in type ( a ) .__mro__ , then add attribute to a.__dict__ , a.__dict__ [ ' x ' ] = ... With classes -- instances of metaclasses , the process is similar : then : A.x = ... is translated to type ( A ) .__setattr__ ( ... ) which performs the following steps : Note : type ( A ) .__setattr__ will find __setattr__ in type builtin class 1 ) Look for a data descriptor in type ( A ) .__mro__2 ) If a data descriptor was found , call its __set__ method and exit . 3 ) If no data descriptor was found in type ( A ) .__mro__ , then add attribute to A.__dict__ , a.__dict__ [ ' x ' ] = ... But object.__setattr__ does n't work for classes : and vice versa , type.__setattr__ does n't work for instances of A : Hmmm ! There must be something different between the two methods . This is subtle , but true nonetheless ! Presumably the two methods perform the same steps inside __setattr__ , what is the difference between type.__setattr__ and object.__setattr__ so that type.__setattr__ is limited to classes and object.__setattr__ is limited to instances of classes ?"
"I 've been reading about metaclasses and I got lost when it came to type and object classes.I understand that they are at the top of the hierarchy and they are implemented in C code . I also understand that type inherits from object and that object is an instance of type.In one of the answers I 've found on SO , someone said - in reagards to object-type relationship - that : This kind of mutual inheritance is not normally possible , but that 's the way it is for these fundamental types in Python : they break the rules.My question is why is it implemented this way , what is purpose of such implementation ? What problems does it solve/what are the benefits of this design ? Could n't it be just type or just object class that is at the top of the hierarchy that every class inherits from ? Finally , is there any difference between subclassing from object vs subclassing from type , and when would I want to use one over the other ? vs"
I would like to make a 3D scatterplot with with the 2D projections at each side . Something like the following : The scatterplot has been created with : And the projections with : And they have been brought together with inkscape . How would I do this completely with matplotlib ?
"I have defined this functionWhere a is a string containing the path of the file and seed is an integer seed.I want to parallelize a simple program in such a way that each core takes one of the available paths that I give in , seeds its random generator and write some random numbers on that files , so , for example , if I pass the vectorand the seeds it gives to the first available core the function and to the second one the same function with different arguments : I have looked through a lot of similar questions here on Stackoverflow , but I can not make any solution work . What I tried is : and gives me TypeError : writeonfiles ( ) takes exactly 2 arguments ( 1 given ) .I tried also But it gives me File `` /usr/lib/python2.7/random.py '' , line 120 , in seed super ( Random , self ) .seed ( a ) TypeError : unhashable type : 'list'Finally , I tried the contextmanagerand it results in File `` /usr/lib/python2.7/multiprocessing/pool.py '' , line 572 , in get raise self._valueTypeError : 'module ' object is not callable"
"I have a file with more than 160.000 urls , of which pages I want to scrape some information . The script looks roughly like this : Which works , but very , very slow . It would take more than four days to scrape all 160.000 pages . Any suggestions to speed it up ?"
"I 'm making and investigation for a seminar in retrieval information . I have a json file with a list of articles and i need to index them and after use a percolator with highlighting.The list of steps for do this in terminal is this:1 . Create a map with percolating.Index a new article : Percolate a documment : I have this code until now : UPDATE : Thank you for the answer , i have this now : It is working fine , but i have two goals now : I 'm getting the next error after indexing a randome number of items in the dict : I know i can solve this problem using this command : curl -XPUT -H `` Content-Type : application/json '' http : //localhost:9200/_all/_settings -d ' { `` index.blocks.read_only_allow_delete '' : null } ' UPDATEFinally i solved it deleting the data folder.But now i 'm making search in the index and i do n't get anything : And this is my trying with curl : I 'm getting variable stats but not results : Can anyone help me ?"
"I 've been playing with Spark and Python on this online jupyter notebook https : //tmpnb.org/ and tried 3 ways to pass python functions:1 ) using map2 ) parallelizing my_sqrt and call it3 ) parallelizing np.sqrt and call it ( 1 ) and ( 3 ) do work and ( 2 ) does n't . First I would like to understand why/how ( 1 ) and ( 3 ) work . Second , I would like to understand why ( 2 ) does n't and what could be done to make it work ."
Huh ... AttributeError : sqrt what 's going on here then ? math.sqrt does n't seem to have the same problem .
Why ca n't I pickle a typing.NamedTuple while I can pickle a collections.namedtuple ? How can I manage to do pickle a NamedTuple ? This code shows what I have tried so far : Output on the shell :
Recursive references work great in ruamel.yaml or pyyaml : However it ( obviously ) does not work on normal references : I would like is to explicitly create a reference : This will be very useful to generate YAML output of large data structures that have lots of common keysHow is it possible without disputable re.replace on the output ? Actually the result of ruamel.yaml.dump ( data ) isSo I need to replace ' < < ' with < < and maybe replace id001 with foo .
"I 'm authoring a set of python coding guidelines for a team of ~30 developers . As a basis for my document , so far I 've studied the Google python style guide and the PEP 8 style guide , and incorporated information from both.One place where the Google style guide is more restrictive than PEP 8 is with imports . The Google guide requests developers only import packages and modules only , and then refer to items within by a more-qualified name . For example : The justification is that the `` source of each identifier is indicated in a consistent way '' . For our project , we intend to organize with packages two or three levels deep , so to know the full source of the identifier , the reader will likely need to examine the import statement anyway . I 'd like to advocate this style of import as a `` preferred style '' : IMHO , the readability in python constructs such as list comprehensions is improved when the names are more succinct.What I 'm unclear on is what the python interpreter might do behind the scenes . For example , is MyClass now part of the global namespace for both this module , and all importers of this module ? ( This would be bad , could lead to some weird bugs ; if this were true , I 'd advocate the Google style ) .My python development experience is limited to about 6 months ( and there are not many experts on our project to consult ) , so I wanted to get more information from the community . Here are some items I 've researched already : effbot - discussion on importsstack overflow - import vs. from importpython documentation - modulespython documentation - importThank you for your responses !"
I 'm trying to figure out what the best way of doing this is : I think the code is quite simple ; if the rows have a value then add them to the list . Is this approach considered OK ? Is there any other approach that would be better ? The toPython method will return the string description of the contained object .
"Running into a small problem with some code coverage using nosetests and coverage with a Django web application . I have created a .coveragerc file to exclude a huge amount of code ( things like class declarations ) but I 'm still getting some weird results.Here is my .coveragerc file : This is an example of one of the models.py files : So when I run code coverage , the issue I run into is that despite me telling coverage to explicitly exclude anything with the string `` = models . `` , it still says the lines are missing in the report given through the command line . This is making it very hard to determine which lines I 'm actually failing to cover in my test cases . Can anyone offer some insight to this ?"
"I 'm writing what might not even be called a language in python . I currently have several operators : + , - , * , ^ , fac , @ , ! ! . fac computes a factorial , @ returns the value of a variable , ! ! sets a variable . The code is below . How would I go about writing a way to define functions in this simple language ? EDIT : i updated the code !"
"I have a coordinated storage list in python A [ row , col , value ] for storing non-zeros values.How can I get the list of all the row indexes ? I expected this A [ 0 : ] [ 0 ] to work as print A [ 0 : ] prints the whole list but print A [ 0 : ] [ 0 ] only prints A [ 0 ] .The reason I ask is for efficient calculation of the number of non-zero values in each row i.e iterating over range ( 0 , n ) where n is the total number of rows . This should be much cheaper than my current way of for i in range ( 0 , n ) : for j in A : ... . Something like : Over : EDIT : Using Junuxx 's answer , this question and this post I came up with the following ( for returning the number of singleton rows ) which is much faster for my current problems size of A than my original attempt . However it still grows with the number of rows and columns . I wonder if it 's possible to not have to iterate over A but just upto n ?"
"Which is better ? ( or ) var = var or get_var ( ) Also , How do I know the better of the two ? edit : One more option from steve ,"
"I have a Python class that stores some fields and has some properties , likeWhat changes do I need to make to the class so that I can dowhere I specify that I want to return y and z ? I ca n't just use a.__dict__ because it would contain x but not z. I would like to be able to specify anything that can be accessed with __getattribute__ ."
I ca n't quite figure out what 's going on with string templates : This prints : I thought that the braces handled arbitrary strings . What characters are allowed in braces and is there any way I can subclass Template to do what I want ?
"Here is an example of models : And here is the Admin class for exercise : I now have the filters book and chapter for exercise . When I click on a book in the filter book , it shows me all the exercises of the selected book accordingly . But in the list of filter chapter , it still shows all the chapters of all the books.Is there a way to only display , in the filter chapter , the chapters of the book that I selected in the first filter book ? How ?"
"Lets say I have this Python code in a setup.py script to build a C extension : Easy enough . Now I call the setup.py script with this line : Ok , but whats the question ? When distutils calls mingw32 and passes all the necessary and operating system independant flags and options to it , how does it figure those flags out ? Where does distutils keep the commands related to each platform , and how can I access them ?"
I have a dataframe ( or could be any RDD ) containing several millions row in a well-known schema like this : I need to load a dozen other datasets from disk that contains different features for the same number of keys . Some datasets are up to a dozen or so columns wide . Imagine : It feels like a fold or an accumulation where I just want to iterate all the datasets and get back something like this : I 've tried loading each dataframe then joining but that takes forever once I get past a handful of datasets . Am I missing a common pattern or efficient way of accomplishing this task ?
"I 'm currently working on a scraper-sort of program , which will enter a Wikipedia page , and in its current form , will scrape the references from the page.I 'd like to have a gui that will allow the user to input a Wikipedia page . I want the input to be attached to the selectWikiPage variable , but have had no luck as of far.Below is my current code.Many thanks in advance ."
"I 've problems integrating Bert Embedding Layer in a BiLSTM model for word sense disambiguation task , The whole scriptThe layer BertEmbeddingLayer ( ) is imported from strongio/keras-bert , as well as following the approach in the file to integrate my work however I always have this error , please check the traceback below ( exception is raised when building the model ) Please refer to my issue on their repo and for data examples being fed to the model please check this issue"
"Ok , so I 'm writing some python code ( I do n't write python much , I 'm more used to java and C ) .Anyway , so I have collection of integer literals I need to store . ( Ideally > 10,000 of them , currently I 've only got 1000 of them ) I would have liked to be accessing the literals by file IO , or by accessing there source API , but that is disallowed.And not ontopic anyway.So I have the literals put into a list : But when I try to run the file it comes up with an error because there are more than 255 arguments.So the constructor is the problem.How should I do this ? The data is intitally avaiable to me as a space deliminated textfile.I just searched and replaced and copied it in"
"Short version : Is there way to achieve in Python the same effect achieved by Perl 's Carp : :carp utility ? Long version ( for those unfamiliar with Carp : :carp ) : Suppose we are implementing some library API function ( i.e. , it is meant to be used by other programmers in their code ) , say spam , and suppose that spam includes some code to check the validity of the arguments passed to it . Of course , this code is supposed to raise an exception if any problem with these arguments is detected . Let 's say that we want to make the associated error message and traceback as helpful as possible to someone debugging some client code.Ideally , the last line of the traceback produced by this raised exception should pinpoint the `` offending code '' , namely the line in the client code where spam was called with invalid arguments.Unfortunately , this is not what would happen , at least by default , using Python . Instead , the last line of the traceback will refer to somewhere in the internals of the library code , where the exception was actually raise 'd , which would be quite obscure to the intended audience of this particular traceback.Example : When we run client.py , we get : whereas what we want would be closer to : ... with the offending code ( x = spam ( False , False ) ) as the last line of the traceback.What we need is some way to report the error `` from the perspective of the caller '' ( which is what Carp : :carp lets one do in Perl ) .EDIT : Just to be clear , this question is not about LBYL vs EAFP , nor about preconditions or programming-by-contract . I am sorry if I gave this wrong impression . This question is about how to produce a traceback starting from a few ( one , two ) levels up the call stack.EDIT2 : Python 's traceback module is an obvious place to look for a Python-equivalent of Perl 's Carp : :carp , but after studying it for some time I was not able to find any way to use it for what I want to do . FWIW , Perl 's Carp : :carp allows fine-adjusting of the initial frame for the traceback by exposing the global ( hence dynamically scoped ) variable $ Carp : :CarpLevel . Non-API library functions that may carp-out , local-ize and increase this variable on entry ( e.g . local $ Carp : :CarpLevel += 1 ; ) . I do n't see anything even remotely like this Python 's traceback module . So , unless I missed something , any solution that uses Python 's traceback would have to take a rather different tack ..."
I 'm using random.random ( ) to get a random float ( obviously ! ) . But what I really want to do is something like : Can you guys help me structure this ?
"Some days I just hate using middleware . Take this for example : I 'd like to have a lookup table that maps values from a set of inputs ( domain ) values , to outputs ( range ) values . The mapping is unique . A Python map can do this , but since the map is quite big I figured , why not use a ps.Series and its index , which has added benefit that I can : pass in multiple values to be mapped as a series ( hopefully faster than dictionary lookup ) the original series ' index in maintained in the resultlike so : Works as expected . But not . The above .map 's time cost grows with len ( domain2range ) not ( more sensibly ) O ( len ( query_vals ) ) as can be shown : facepalm . At n=10000000 its taken ( 0.01/3 ) second per mapped value.So , questions : is Series.map expected to behave like this ? Why is it so utterly , ridiculously slow ? I think I 'm using it as shown in the docs.is there a fast way to use pandas to do table-lookup . It seems like the above is not it ?"
"I 'm building an internal webapp for my company to use and want to use our Google Apps domain to manage access from our company domain usernames ( example.com for the rest of this question ) .I 'm using : From reading other SO questions I 've discovered the Goog `` hosted domain '' ( hd ) parameter that can be used via the following setting : The parameter is being successfully appended to the initial request , I can see it in the URL before granting access.However , it 's not working as I 'd expect . I 've been able to successfully authenticate with two non-company email addresses.Am I misunderstanding how the `` hd= '' parameter works or do I need to also limit access via the app somewhere else on the Google Admin dashboard ? Or is it just not supported within the OAuth2 flow ? Thanks in advance for any help ."
Possible Duplicate : In Python how do I sort a list of dictionaries by values of the dictionary ? I 'm writing a Python 3.2 app and I have a list of dictionaries containing the following : I want the list to be sorted by the values in the rating key . How do I accomplish this ?
"I have two after_request handlers . In my case , I need one to fire before the next . In my case , I want compress to go first , then check_something . But they are firing in reverse.If it matters , in my actual code , these two handlers are not consecutively declared like this . They are each in different modules that are installed at different times.How can I control the order of execution ?"
"The django tutorial part 4 has the following code : I 'm having trouble finding out where the error_message variable in the if conditional statement is defined . Searches on google , stack overflow and the django apis doesnt seem to give any answer on this ."
"I 'm trying to figure out how to make iterator , below is an iterator that works fine . However , when I try to pass 16 into the second argument in iter ( ) ( I expect the iterator will stop when return 16 ) It throws TypeError : iter ( v , w ) : v must be callableTherefore , I try to do so.It returns < main.DoubleIt object at 0x7f4dcd4459e8 > . Which is not I expected.I checked the website of programiz , https : //www.programiz.com/python-programming/methods/built-in/iterWhich said that callable object must be passed in the first argument so as to use the second argument , but it does n't mention can User defined object be passed in it in order to use the second argument.So my question is , is there a way to do so ? Can the second argument be used with the `` Self defined Object '' ?"
"I am trying to test my app but not sure how to configure the django-allauth in the test environment . I am getting : ImproperlyConfigured : No Facebook app configured : please add a SocialApp using the Django adminMy approach so far is to instantiate app objects inside tests.py with actual Facebook app parameters , an app which functions correctly locally in the browser : How can I get these tests running ? Thanks"
"I have a list that looks like this : [ ' a ' , ' b ' , ' c ' , `` , `` , `` ] This is the result of parsing a 'dirty ' csv file . I now want to get rid of the empty columns on the right . I can not just use counting , because the length is variable . I also can not just use simple filtering , because there are also rows that look like this : [ 'a1 ' , `` , 'c1 ' , `` , `` ] So I have to preserve the empty columns that are not at the very right . Is there an idiomatic way to do this ? I am hoping for something like a `` removeWhile '' function that I could apply on the reversed list.The best I 've come up with so far is the following :"
"There are , as far as I know , three ways to create a generator through a comprehension1.The classical one : The yield variant : The yield from variant ( that raises a SyntaxError except inside of a function ) : The three variants lead to different bytecode , which is not really surprising.It would seem logical that the first one is the best , since it 's a dedicated , straightforward syntax to create a generator through comprehension.However , it is not the one that produces the shortest bytecode.Disassembled in Python 3.6Classical generator comprehensionyield variantyield from variantIn addition , a timeit comparison shows that the yield from variant is the fastest ( still run with Python 3.6 ) : f3 is more or less 2.7 times as fast as f1 and f2.As Leon mentioned in a comment , the efficiency of a generator is best measured by the speed it can be iterated over.So I changed the three functions so they iterate over the generators , and call a dummy function.The results are even more blatant : f3 is now 8.4 times as fast as f1 , and 9.3 times as fast as f2.Note : The results are more or less the same when the iterable is not range ( 10 ) but a static iterable , such as [ 0 , 1 , 2 , 3 , 4 , 5 ] .Therefore , the difference of speed has nothing to do with range being somehow optimized.So , what are the differences between the three ways ? More specifically , what is the difference between the yield from variant and the two other ? Is this normal behaviour that the natural construct ( elt for elt in it ) is slower than the tricky [ ( yield from it ) ] ? Shall I from now on replace the former by the latter in all of my scripts , or is there any drawbacks to using the yield from construct ? EditThis is all related , so I do n't feel like opening a new question , but this is getting even stranger.I tried comparing range ( 10 ) and [ ( yield from range ( 10 ) ) ] .So . Now , iterating over [ ( yield from range ( 10 ) ) ] is 186 times as fast as iterating over a bare range ( 10 ) ? How do you explain why iterating over [ ( yield from range ( 10 ) ) ] is so much faster than iterating over range ( 10 ) ? 1 : For the sceptical , the three expressions that follow do produce a generator object ; try and call type on them ."
"I am using the following function to force a coroutine to run synchronously : However , intermittently , it will freeze upon simply trying to create a new loop : loop = asyncio.new_event_loop ( ) . Inspecting the stack traces shows me the exact location where it hangs : What can be causing such an issue , in a library as low level as socket ? Am I doing something wrong ? I am using Python 3.5.1.Edit : I filed a bug report here but Guido recommended me to continue seeking help on StackOverflow ."
"I 'm trying to display some results in a human-readable way . For the purposes of this question , some of them are numbers , some are letters , some are a combination of the two.I 'm trying to figure out how I could get them to sort like this : Desired Results : Actual results : I 'm having trouble coming up with how to do this ."
"Along the lines of my previous question , how can i join a list of strings into a string such that values get quoted cleanly . Something like : into : I suspect that the csv module will come into play here , but i 'm not sure how to get the output I want ."
"According to my understanding , circular variance has a range between 0 and 1 . This is also confirmed in wikipedia as well as here . But for some reasons , circular variance function from scipy.stats gives values above 1.Could somebody inform me why I am getting values above 1 from the function circvar"
"Say I make a list comprehension that looks something like this : for some function f. Will using a dummy name identical to the iterator ever yield unexpected results ? Sometimes I have variable names that are individual letters , and to me it is more readable to stick with the same letter rather than assigning a new one , like [ f ( x ) for x in x ] instead of [ f ( i ) for i in x ] ( for instance , if the letter of the iterator x is meaningful , I will wonder what the heck i is ) ."
"I am trying to patch a class that is instantiated by the class I am trying to test , but it does n't work . I have read the various docs but still have n't found what I am doing wrong . Here is the code snippet : In tests/Test.py : In module/ClassToPatch.py : In module/ClassToTest.py : I know in this case I could easily inject the dependency , but this is just an example . Also , we use a single class per file policy , with the file named like the class , hence the weird import naming ."
"Variable are not retained from one chunk to the next in notebook mode , but they are retained when knitting the markdown document to html.I made a sample document available as a gist called pythonvariables.Rmd , the content of this file is : In Rstudio version 1.1.453 , in notebook mode , when running one chunk after the other , the output of the print ( x ) python chunk is : However the issue does n't appear when the Rmd is compiled to html . The output of the print ( x ) python chunk is 1 as expected ."
"I have dataset which has more than 50k nodes and I am trying to extract possible edges and communities from them . I did try using some graph tools like gephi , cytoscape , socnet , nodexl and so on to visualize and identify the edges and communities but the node list too large for those tools . Hence I am trying to write script to exact the edge and communities . The other columns are connection start datetime and end datetime with GPS locations . Input : Id , starttime , endtime , gps1 , gps2I am trying to implement undirected weighted / unweighted graph ."
I have a DataFrame And I want to get the value differences based on each namelike this Can anyone show me the easiest way ?
Every example I 've seen of using widgets for interactive matplotlib plots in the notebook do something like this ( adapted from here ) : I suspect that the responsiveness of the plot could be sped up hugely if you did n't have to create a brand new figure with plt.subplots ( ) or plt.figure ( ) each time a widget was adjusted.I 've tried a few things to move figure creation outside of the function being called by interact ( ) but nothing has worked .
"I 'm running a python application ( flask + redis-py ) with uwsgi + nginx and using aws elasticache ( redis 2.8.24 ) .while trying to improve my application response time , I 've noticed that under high load ( 500 request per second/for 30 seconds using loader.io ) I 'm losing requests ( for this test i 'm using just a single server without load balancer , 1 uwsgi instance , 4 processes , on purpose for testing ) .I 've dug a little deeper and found out that under this load , some requests to ElastiCache are slow.for example : normal load : cache_set time 0.000654935836792heavy load : cache_set time 0.0122258663177this does not happen for all requests , just randomly occurres..My AWS ElastiCache is based on 2 nodes on cache.m4.xlarge ( default AWS configuration settings ) .See current clients connected in the last 3 hours : I think this does n't make sense as currently 14 servers ( 8 of them with high traffic of XX RPS use this cluster ) , I would expect to see a much higher client rate.uWSGI config ( Version 2.0.5.1 ) Nginx is just a web proxy to uWSGI using unix socket.This is how I open a connection to redis : This is how I set a value for example : This is how I get a value : Version : uWSGI : 2.0.5.1Flask : 0.11.1redis-py : 2.10.5Redis : 2.8.24So the conclude : Why AWS clients count is low if 14 servers are connected , each with 4 processes , and each of them opens a connection to 8 different database within the redis clusterWhat causes the requests response time to climb ? Would appreciate any advise regarding ElastiCache and/or uWSGI performance under heavy load"
"Do emojis occupy a well-defined unicode range ? And , is there a definitive way to check whether a code point is an emoji in python 2.7 ? I can not seem to find any information on this . A couple of sources have pointed to the range : But for example , has the code pointwhich lies outside this range.Thanks ."
"I decided to implement sleep sort ( https : //rosettacode.org/wiki/Sorting_algorithms/Sleep_sort ) using Python 's asyncio when I made a strange discovery : it works with negative values ( and returns immediately with 0 ) ! Here is the code ( you can run it here https : //repl.it/DYTZ ) : The code takes 5 seconds to execute , as expected , but the result is always [ 0 , -5 , -4 , -3 , -2 , -1 , 1 , 2 , 3 , 4 , 5 ] . I can understand 0 returning immediately , but how are the negative values coming back in the right order ?"
"When I run the basic script : forthe first time it all works fine . However , if I run it a second time I get : I am running the scripts in an Ubuntu machine . I get the same error in python2 and python3.Thanks !"
"EDIT : My question is not a duplicate as someone has marked . The other question is incorrect and does not even work.I have tried a few ways to group the results of itertools.combinations and been unable to come up with the correct output . It is needed to create matches in a game . Every team needs to play every day , but only once . Teams need to play different teams on the following days until everyone has played everyone.the result : But what I need is to group them without any duplicate list items.example : Any tips would be appreciated , I feel like there 's probably a simple one-liner to get this done ."
"I want to use difflib.SequenceMatcher to extract longest common substrings from two strings . I 'm not sure whether I found a bug or misunderstood the documentation of find_longest_match . This is the point that I find confusing : In other words , of all maximal matching blocks , return one that starts earliest in a , and of all those maximal matching blocks that start earliest in a , return the one that starts earliest in b . ( https : //docs.python.org/3.5/library/difflib.html # difflib.SequenceMatcher.find_longest_match ) Comparing the strings X this is a test and this is a test X , the substring X is in fact a maximal block : it ca n't be extended ( i.e. , it is inclusion-maximal ) . Also , it is the first such maximal block in text A . But it is certainly not a longest common substring . I strongly suspect this is not what find_longest_match is supposed to find.In fact , in this example , find_longest_match does find a longest common substring : However , it seems like with some other strings , I can provoke the `` find the first maximal block '' -behavious described above ( sorry for the long strings , if I shorten them , the example somehow breaks ) : In this case , it matches the first - in s1 [ 1 ] to the - in s2 [ 47 ] , and that 's it . A longest common substring would probably be something starting with graph visualization using…Did I find a bug , or is there another possible interpretation of the documentation that describes this behavior ? I 'm using Python 3.5.2 on Ubuntu ."
"We know in Python 3.6 dictionaries are insertion ordered as an implementation detail , and in 3.7 insertion ordering can be relied upon.I expected this to also be the case for subclasses of dict such as collections.Counter and collections.defaultdict . But this appears to only hold true for the defaultdict case.So my questions are : Is it true that ordering is maintained for defaultdict but not for Counter ? And , if so , is there a straightforward explanation ? Should ordering of these dict subclasses in the collections module be considered implementation details ? Or , for example , can we rely on defaultdict being insertion ordered like dict in Python 3.7+ ? Here are my rudimentary tests : dict : orderedCounter : unordereddefaultdict : ordered"
"Why is it that I ca n't have 008 or 009 be keys for a Python dict , but 001-007 are fine ? Example : Update : Problem solved . I was n't aware that starting a literal with a zero made it octal . That seems really odd . Why zero ?"
"I 'm reading Python for Data Analysis by Wes Mckinney , but I was surprised by this data manipulation . You can see all the procedure here but I will try to summarize it here . Assume you have something like this : tz means time zone and Not Windows and Windows are categories extracted from the User Agent in the original data , so we can see that there are 3 Windows users and 0 Non-windows users in Africa/Cairo from the data collected.Then in order to get `` the top overall time zones '' we have : So at that point , I would have thought that according to the documentation I was summing over columns ( in sum ( 1 ) ) and then sorting according to the result showing arguments ( as usual in argsort ) . First of all , I 'm not sure what does it mean `` columns '' in the context of this series because sum ( 1 ) is actually summing Not Windows and Windows users keeping that value in the same row as its time zone . Furthermore , I ca n't see a correlation between argsort values and agg_counts . For example , Pacific/Auckland has an `` argsort value '' ( in In [ 134 ] ) of 0 and it only has a sum of 11 Windows and Not Windows users . Asia/Harbin has an argsort value of 1 and appears with a sum of 3 Windows and Not Windows users.Can someone explain to me what is going on there ? Obviously I 'm misunderstanding something ."
"I 'm a novice programmer with basic Java experience , and currently learning Python.I 've stumbled across this blog post in another question thread : http : //dirtsimple.org/2004/12/python-is-not-java.htmland I 've got a couple of questions regarding the topic posted : 1 ) `` Oh , and all those Foo.Bar.Baz attribute chains do n't come for free , ... , so each dot counts . `` Is the solution to this particular problem is importing module and its method beforehand ? Such as : 2 ) Got a switch statement ? The Python translation is a hash table , not a bunch of if-then statments.There are several related answers regarding this topic , but they also raise a couple of questions : Use of if-else is cleaner , but it does n't have the advantage of constant time O ( 1 ) in switch statement.Use of hash for constant time O ( 1 ) Use of lambda function in hash for comparison ( not recommended ) Why is it not recommended ? Is it because the lambda function removes the constant factor of hash ? Use of bisect moduleDoes this method retain the constant time O ( 1 ) , or it is just another type of lambda function ? So what method in Python , that is equal to switch statement , with constant time O ( 1 ) , while at the same time allowing comparison statement ? 3 ) Getters and setters are evil . Evil , evil ... do n't write getters and setters ... This is what the 'property ' built-in is for ... In Python , this ( getter and setter ) is silly , because you can start with a normal attribute and change your mind at any time , without affecting any clients of the class.I do n't really quite understand this part . Also , it seems that in Python public and private method or variable can be easily accessed , in contrast of that in C++ and Java . Is there any design reason for this behavior ? Finally , is there any recommended further good read on Python vs. any other programming language ?"
"This is somewhat of a follow on to Why are mutable values in Python Enums the same object ? .If the values of an Enum are mutable ( e.g . lists , etc . ) , those values can be changed at any time . I think this poses something of an issue if Enum members are retrieved by value , especially if someone inadvertently changes the value of an Enum he looks up : I think given normal Python idioms this is okay , with the implication being that users can use mutables as their Enum values , but just to understand the can of worms they might be opening.However this brings up a second issue - since you can look up an Enum memeber by value , and the value can be mutable , it must be doing the lookup by a means other than a hashmap/dict , since the mutable can not be a key in such a dict.Would n't it be more efficient ( although , granted , less flexible ) to limit Enum values to only immutable types so that lookup-by-value could be implemented with a dict ?"
"Inspired by this nice answer , Here 's a benchmark : For me , test2 is sligtly faster ( ~10 % ) . Why is that the case ? I would expect it to be slower since : slice assignment must be able to accept iterables of any length and therefore must be more general.in slice assignment , we need to create a new list on the right hand side just to get it to work.Can anybody help me understand this ? ( using python 2.7 on OS-X 10.5.8 )"
Coming from primarily coding in Java and wanted to know if Python could use conditionals and different kinds of incrementing inside its for loops like Java and C can . Sorry if this seems like a simple question.i.e . :
I 'm getting an incomprehensible error message when using validation for dynamic option ( where options in one select field depend on the choice in the other selectfield . I can however not select a city once a region has been selected . Why not ? What must be done ? My entire code block for inserts and validation is as follows.My form class looks as follows .
"I have a model like : I need to get responses from this questionnaire grouped by months and count `` yes '' and `` no '' responses.Example , I have responses like this : I need a django queryset to do something like : The date is not important , in template I 'll just use the month value ( 01 , 02 , 03 , ... , 11 , 12 ) .I 'm searching for a pythonic way to do this , preferably with queryset in django , not dictionary ."
"I have two SQLite databases containing tables I need to join using SQLalchemy . For reasons I can not combine all the tables into one SQLite database . I am using SQLalchemy ORM . I have not been able to find any solution online that meets my specific case.My question is in principle the same as SQLAlchemy error query join across database , but the original poster 's problem was solved using a different solution that does not match my use case.My question to the wise people at Stackoverflow : I want to emulate the following SQL query : This query works fine using SQliteStudio having attached both database files.The code I am currently using to describe the metadata : The last print statement returns : I believe my troubles are over if I would somehow be able to bind both database engines to one session . But how ? For two databases attached to the same engine ( e.g . two databases in a MySQL database ) I could add __table_args__ = { 'schema ' : 'annotations ' } ( as per Cross database join in sqlalchemy ) , but I am unable to figure this one out in the case of SQLite.I would prefer a solution that would allow users of my code to construct queries without having to know in which database each table resides.Please help ! And many thanks in advance !"
How can I get the full stack trace from the Exception object itself ? Consider the following code as reduced example of the problem :
I 'm currently running my tests like this : Now I want to run a specific test knowing his name ( like test_valid_user ) but not knowing his class . If there is more than one test with such name than I would like to run all such tests . Is there any way to filter tests after discover ? Or maybe there are other solutions to this problem ( please note that it should n't be done from command line ) ?
"I am new to Theano , and I try to implement a numerical integrator of a reaction-diffusion system - FitzHugh–Nagumo model of this version : For now my expressions are : So I have n't implemented the finite-differences laplacian operator yet . My question is whether there is a smart way of doing it in Theano ?"
I 've already installed all necessary libraries even did apt-get build-dep uwsgi but I think I have problems with system libraries.No I ca n't use pre-built uwsgi package for Debian because some project libraries depends on pip version.Any suggestions ? Traceback
"I 'm working on an online store in Django ( just a basic shopping cart right now ) , and I 'm planning to add functionality for users to mark items as favorite ( just like in stackoverflow ) . Models for the cart look something like this : The favorites model would be just a table with two rows : user and product.The problem is that this would only work for registered users , as I need a user object . How can I also let unregistered users use these features , saving the data in cookies/sessions , and when and if they decides to register , moving the data to their user ? I guess one option would be some kind of generic relations , but I think that 's a little to complicated . Maybe having an extra row after user that 's a session object ( I have n't really used sessions in django until now ) , and if the User is set to None , use that ? So basically , what I want to ask , is if you 've had this problem before , how did you solve it , what would be the best approach ?"
"The ApplicationI 'm trying to build a python shell for my PyQt5 application using the stdlib InteractiveConsole so I can let users script live plots . I 'm using a QTextEdit to display the stdout from the shell.The ProblemWhen I do for loops in the shell , the application freezes because the insertPlainText ( ) to the QTextEdit is too fast . So I wrote a buffer that would delay the inserts by a few milliseconds . However , I noticed that as soon as I ran any blocking functions like time.sleep ( ) in the for loops , it would freeze . So the prints inside the for loops will only be displayed after the loop is done . This does not happen if the buffer is disabled.For eg , if i do this in the shell : This will only print after 10 seconds.CodeThis is the most minimal version I could write according to MVCE guidelines.Here is the main.ui file : Here is themain.py file : The class BaseSignals is needed for communication between the main thread and the interpreter . Here is a transcript as to why this was implemented.What I knowThis line is responsible for inserting the plain text self.output.signal_str.emit ( data ) . This emit ( ) happens inside a QThread . So until the multiple self.buffer.write ( ) is finished the emit ( ) wo n't be processed . I thought adding a QApplication.processEvents ( ) in DelayedBuffer.process ( ) would help . It does n't . I admit I could however be wrong about this.Any help appreciated.Thanks in advance ."
"This is more or less a follow up question to Two dimensional color ramp ( 256x256 matrix ) interpolated from 4 corner colors that was profoundly answered by jadsq today.For linear gradients the previous answer works very well . However , if one wants to have better control of the stop colors of the gradient , this method seems not to be very practical . What might help in this situation is to have some reference color points in a matrix ( lookup table ) which are used to interpolate color values for the empty position in the look-up table . What I mean might be easier read out of the below image.The whole idea is taken from http : //cartography.oregonstate.edu/pdf/2006_JennyHurni_SwissStyleShading.pdf page 4 to 6 . I 've read through the paper , I understand theoretically what is going on but failing miserably because of my low experience with interpolation methods and to be honest , general math skills . What might also be of interest is , that they use a sigmoid Gaussian bell as interpolation method ( page 6 ) . They argue that Gaussian weighting yielded the visually best results and was simple to compute ( equation 1 , with k=0.0002 for a table of 256 per 256 cells ) .Edit ( better illustrations ) : I have the other parts of their presented methods in place but filling the empty values in the matrix really is a key part and keeps me from continuing . Once again , thank you for your help ! What I have right now :"
"DescriptionNormally if you change your python code means , you need to restart the server in order to apply the new changes.If the -- auto-reload parameter is enabled means , you do n't need to restart the server . It enables auto-reloading of python files and xml files without having to restart the server . It requires pyinotify . It is a Python module for monitoring filesystems changes.Previous ProblemI got the error : But I followed the advice of this link and now I do n't get that error anymore : After this I got this in the server log : That means that 's working properly . And in fact I tested it with a physical addon path and it worked.Current ProblemI have all my modules en several folders but I only use one addons path : /opt/odoo_8/src/linked-addons . This folder contains all the links of the modules that I 'm using . All modules are working well when I run Odoo.But the problem is that pyinotify is not able to check the files beyond the links and it does n't reload the files well . What I should do to fix this ? Is there a way that pyinotify can recognise the content of the links ? PD : I do n't want to change my way of managing the modules folders in Odoo ."
"I have a method for extracting all the `` words '' from a string in javascript : I 'd like to convert that same logic ( create an array of `` words '' from my string ) , but in python . How can I achieve that ?"
"I would love to see the last 10 lines which were executed by the python interpreter before this exception occured : I want to see where check_perm ( ) returned True.I know that I could use interactive debugging to find the matching line , but I am lazy and want to find a easier way to the line where check_perm ( ) returned the return value.I use pyCharm , but a text based tool , would solve my need , too.BTW : Please do n't tell me how to use the debugger with step-over and step-into . I know this.Here is some code to illustrate it . There are several ways where check_perm ( ) could return True . If True was returned because of condition_1 , then I want to see something like thisThe output I have in mind is like set -x on the shell.Updatecgitb , pytest and other tools can show the lines before the line where the assertion failed . BUT , they only show the lines of the current python file . This question is about the lines which were executed before the assertion happens , but covering all files . In my case I want to know where the return value of check_perm ( ) was created . The tools pytest , cgitb , ... do n't show this.What I am searching is like set -x on the shell : help set -x Print commands and their arguments as they are executed ."
"I am using pandas.DataFrame.resample to resample a grouped Pandas dataframe with a timestamp index.In one of the columns , I would like to resample such that I select the most frequent value . At the moment , I am only having success using NumPy functions like np.max or np.sum etc.The previous code works because I have used NumPy functions . However , if I want to use resample by most frequent value , I am not sure . I try defining a custom function like However , if I now do : I get an empty dataframe ..."
"I am testing an async function that might get deadlocked . I tried to add a fixture to limit the function to only run for 5 seconds before raising a failure , but it has n't worked so far.Setup : Code : -- Edit : Mikhail 's solution works fine . I ca n't find a way to incorporate it into a fixture , though ."
"I am trying to remove the loop from this matrix multiplication ( and learn more about optimizing code in general ) , and I think I need some form of np.broadcasting or np.einsum , but after reading up on them , I 'm still not sure how to use them for my problem . I know I ca n't just do np.multidot by itself , because that results in a ( 5,5 ) array.I also found this : Multiply matrix by each row of another matrix in Numpy , but I ca n't tell if it 's actually the same problem as mine ."
"I can define a numba struct type with : Now that I have the type , how do I create an actual struct ?"
"I have a text file text_isbn with loads of ISBN in it . I want to write a script to parse it and write it to a new text file with each ISBN number in a new line.Thus far I could write the regular expression for finding the ISBN , but could not process any further : I tried to use the following but got an error ( I guess the list is not in proper format ... ) How to do the parsing and write it to a new file ( output.txt ) ? Here is a sample of the text in text_isbn"
"I am trying to automatically save a PDF file created with pdftohtmlEX ( https : //github.com/coolwanglu/pdf2htmlEX ) using the selenium ( chrome ) webdriver.It almost works except captions of figures and sometimes even part of the figures are missing.Manually saved : Automatically saved using selenium & chrome webdriver : Here is my code ( you need the chromium webdriver ( http : //chromedriver.chromium.org/downloads ) in the same folder as this script ) : Sometimes when I manually print this happens , too . But if I then change any of the printing options , the preview reloads and the image captions are there again and stay there no matter what options I further enable/disable.What I tried so far : different Chrome webdriver versions ( 71 , 72 , 73 ) from this site : http : //chromedriver.chromium.org/downloadsenable background graphics by adding ' '' isCssBackgroundEnabled '' : true ' to the appState"
"Consider the following code : The resulting graph is x - > z - > y . Sometimes I 'm interested in computing y all the way from from x but sometimes I have z to start and would like inject this value into the graph . So the z needs to behave like a partial placeholder . How can I do that ? ( For anyone interested why I need this . I am working with an autoencoder network which observes an image x , generates an intermediate compressed representation z , then computes reconstruction of image y. I 'd like to see what the network reconstructs when I inject different values for z . )"
"I am trying to embed a matplotlib graph that updates every second into a PyQt GUI main window.In my program I call an update function every second using threading.Timer via the timer function shown below . I have a problem : my program grows bigger every second - at a rate of about 1k every 4 seconds . My initial thoughts are that the append function ( that returns a new array in update_figure ) does not delete the old array ? Is it possible this is the cause of my problem ? This is my timer function - this is triggered by the click of a button in my PyQt GUI and then calls itself as you can see : EDIT : I cant post my entire code because it requires a lot of .dll includes . So i 'll try to explain what this program does.In my GUI I want to show the my CO2 value over time . My get_co22 function just returns a float value and I 'm 100 % sure this works fine . With my timer , shown above , I want to keep append a value to a matplotlib graph - the Axes object is available to me as self.axes . I try to plot the last 10 values of the data.EDIT 2 : After some discussion in chat , I tried putting the call to update_figure ( ) in a while loop and using just one thread to call it and was able to make this minimal example http : //pastebin.com/RXya6Zah . This changed the structure of the code to call update_figure ( ) to the following : but now the program crashes after 5 iterations or so ."
"I 'm working with cherrypy in a server that implements a RESTful like API.The responses imply some heavy computation that takes about 2 seconds for request . To do this computations , some data is used that is updated threetimes a day . The data is updated in the background ( takes about half hour ) , and once it is updated , the references of the new data are passed tothe functions that respond the requests . This takes just a milisecond.What I need is to be sure that each request is answered either with the old data or with the new data , but none request processing can take place while the data references are being changed . Ideally , I would like to find a way of buffering incoming request while the data references are changed , and also to ensure that the references are changed after all in-process requests finished.My current ( not ) working minimal example is as follows : If I run this script , and also open a browser , put localhost:8080 and refresh the page a lot , I get : Which means that some requests processing started before and ends after the data references start or end to being changed . I want to avoid both cases . Something like : I searched documentation and the web and find these references that do not completely cover this case : http : //www.defuze.org/archives/198-managing-your-process-with-the-cherrypy-bus.htmlHow to execute asynchronous post-processing in CherryPy ? http : //tools.cherrypy.org/wiki/BackgroundTaskQueueCherrypy : which solutions for pages with large processing timeHow to stop request processing in Cherrypy ? Update ( with a simple solution ) : After giving more thought , I think that the question is misleading since it includes some implementation requirements in the question itself , namely : to stop processing and start buffering . While for the problem the requirement can be simplified to : be sure that each request is processed either with the old data or with the new data . For the later , it is enough to store a temporal local reference of the used data . This reference can be used in all the request processing , and it will be no problem if another thread changes self.data . For python objects , the garbage collector will take care of the old data.Specifically , it is enough to change the index function by : And as a result we will see : [ 21/Sep/2015:10:06:00 ] ENGINE I started with 1 that changed to 2 but I am still using 1 I still want to keep the original ( more restrictive ) question and cyraxjoe answer too , since I find those solutions very useful ."
"I 'm trying to parallelize some calculations that use numpy with the help of Python 's multiprocessing module . Consider this simplified example : When I execute it , the multicore_time is roughly equal to single_time * n_par , while I would expect it to be close to single_time . Indeed , if I replace numpy calculations with just time.sleep ( 10 ) , this is what I get — perfect efficiency . But for some reason it does not work with numpy . Can this be solved , or is it some internal limitation of numpy ? Some additional info which may be useful : I 'm using OSX 10.9.5 , Python 3.4.2 and the CPU is Core i7 with ( as reported by the system info ) 4 cores ( although the above program only takes 50 % of CPU time in total , so the system info may not be taking into account hyperthreading ) .when I run this I see n_par processes in top working at 100 % CPUif I replace numpy array operations with a loop and per-index operations , the efficiency rises significantly ( to about 75 % for n_par = 4 ) ."
"Suppose I have some code likeHowever , I want to tell mypy that d should only contain certain keys ( only the `` x '' key for example ) . That way , if I make a mistake lower in the code trying to reference an invalid key of d , mypy will trigger an error.My question is : Is this possible ? Can mypy validate dictionary keys ? If yes , how is this done ? If no , is there a preferred workaround ?"
"I 'm trying to find the probability of a given word within a dataframe , but I 'm getting a AttributeError : 'Series ' object has no attribute 'columns ' error with my current setup . Hoping you can help me find where the error is . I 'm started with a dataframe that looks like the below , and transforming it to find the total count for each individual word with the below function . Function below : Resulting in the below df ( note 'foo ' is 16 since it appears 16 times in the whole df ) : The issue comes in when trying to find the probability of a given keyword within the df , which is currently does not append a column name . Below is what I 'm currently working with , but it is throwing the `` AttributeError : 'Series ' object has no attribute 'columns ' '' error . My hope is that calling _probability ( df , 'foo ' ) will return 0.421052632 ( 16/ ( 12+16+10 ) ) . Thanks in advance !"
"In the docs for wx.Slider ( wxPython for py2 , wxPython for py3 , wxWidgets ) , there is listed a widget control named wx.SL_SELRANGE , defined to allow `` the user to select a range on the slider ( MSW only ) '' . To me , this speaks of a twin-control , two sliders on the same axis in order to define a low/high range . I ca n't get it to show two controls.Basic code to get it started . I 'm not even worried yet about methods , events , or whatnot at this point , just to show something.With all of that , the most I 've been able to get it to show is a blue line spanning about where I would expect things to be.The wxPython docs all talk about it but how is the user supposed to be able to `` select a range on the slider '' , like shown here ( taken from shiny ) ? What am I missing ? Are there any reasonable public examples of a wxPython wx.Slider in the wild with this functionality ? PS : One page I found speaks of WinXP only , but since that page has n't been updated in seven years , I do n't consider it authoritative on the version restriction.I 've been using wxGlade for gui layout , but I 'm certainly willing/able to go into the code after export and muck around.System : win81_64 , python-2.7.10 , wxPython-3.0.2.0"
"I have a dataframe like the following . I would essentially like to do an operation like df.groupby ( 'ID ' ) .sum ( ) to get the sum of the Variable column , but I need to skip the first period observed for a particular ID . So , for ID=1 , I am dropping the observation at period 1 , but for ID=2 , I am dropping the observation at period 2 . How can I do this ?"
"I am currently looking for method in which i can interleave 2 numpy.ndarray . such thatprint c shoule be interleaving each row both matrices I have three in total which should be interleaved , but i guess it would be easier to do it two at a time.. but how do i do it easily.. I read some method which used arrays , but i am not sure to do it with ndarrays ?"
"We have a system with different types of jobs . Let 's call them for example : They all require different sets of parameters ( and optional parameters ) . I.e . we run job_1 ( x ) for different x= A , B , C ... . job_2 runs for a set of parameters that is dependent on the results of job_1 ( x ) and also job_2 loads data that job_A ( x ) stored . And so on.The result is a tree structure of dependencies . Now , these jobs occasionally fail for one reason or another . So , if job_A for x=B fails that branch of the tree will fail completely and should n't run . All the other branches should run though.All the jobs are written in Python and use parallelism ( based on spawning SLURM jobs ) . They are scheduled with cron . This is obviously not very and has two major drawbacks : It is very hard to debug . All the jobs run whether a job higher in the tree failed or not . It is hard to see where the problem is without a deep understanding of the dependencies.If higher job ( for example job_A ) is not finished job_B might be scheduled to run , and fails or runs based on stale date.In order to tackle that problem we were looking at airflow for scheduling or visualization because it is written in Python and it seems to roughly fit our needs . I see different challenges though : The dependency tree of jobs is either very general ( i.e . job_B depends on job_A ) or very wide ( i.e . job_B ( y ) for a 100 parameters depends on job_A ( x=A ) . The visualized tree in the first case would have roughly 10 leaves but would make debugging very hard because the job might just have failed for a certain parameter . The visualized tree in the latter case would be very wide and have roughly 300 leaves . It would be more accurate but the visualization might be hard to read . Can we filter failed jobs , and just look at their dependencies ? We have parallelism within the job ( and we need it otherwise the jobs run for more than a day , and we want to rerun the whole lot every day ) does that screw up our scheduling ? We want to change our jobs and data management as little as possible.Can we implement the rule system of what jobs to spawn next in a easily understandable way ? Is airflow a good choice for this ? I understand there are a few others ( luigi , Azkaban etc . ) out there which are somewhat related to the Hadoop stack ( which we are not using because it is not Big Data ) . How much hacking is required ? How much hacking is sensible ?"
"Let 's say that I have a function that requires that NumPy ndarray with 2 axes , e.g. , a data matrix of rows and columns . If a `` column '' is sliced from such an array , this function should also work , thus it should do some internal X [ : , np.newaxis ] for convenience . However , I do n't want to create a new array object for this since this can be expensive in certain cases.I am wondering if there is a good way to do it . For example , would the following code be safe ( by that I mean , would the global arrays always be unchanged like Python lists ) ? I am asking because I heard that NumPy arrays are sometimes copied in certain cases , however , I ca n't find a good resource about this . Any ideas ?"
"I am using ctypes to access a shared library written in C. The C source of the shared library contains an enum like On the Python side I was intending to just define integer constants for the various enum values , like : And then use these numerical `` constants '' in the Python code calling the C functions . This seems to work OK , however I would strongly prefer to get the numerical values for the enums directly from the shared library ( introspection ? ) ; however using e.g . nm on the shared library it does not seem to contain any of the symbols 'invalid ' , 'type1 ' or 'type2 ' . So my question is : Is it possible to extract the numerical values from enum definitions from a shared library - or is the whole enum concept 'dropped on the floor ' when the compiler is done ? If the enum values exist in the shared library - how can I access them from Python/ctypes ?"
"SummaryIn a Python Tkinter application , when using ttk.Notebook , how do I bind a keypress event so that it only fires when the tab containing the frame generating the event is active ( i.e. , for a button hotkey , how do I only capture the event while the button is on the active tab ) ? DetailI am writing a Tkinter application ( my first ) , which uses a ttk.Notebook object to manage multiple portions of the interface . I have multiple tabs , some of which have the `` same '' button on them , but which have different actions , depending on which tab is active ( i.e. , a `` Save '' button on one tab saves the items from that tab only , not from all tabs ) .The intuitive way to do this is to bind the event to the frame , then the frame containing the `` active '' objects would catch the event , but this does n't appear to work . However , if I bind the event to the root window , the same handler is called , regardless of the tab context.I would think that this would be a common requirement , however I am unable to find information regarding how to do this.I am using Python 3.4.3.MCVEHere is a minimum example , which demonstrates the behavior I have observed . It generates a main window with five tabs , each with an event binding for Alt-t , which should fire the event handler for the frame in that tab ."
"I 'm using the Sphinx autodoc extension to document a module , and I 'd like to get a flat list of the module 's members in the documentation output.I tried using the following : However , there are two problems with this : It includes the module 's docstring , which I do n't want here.The name of each entry is prefixed with `` modname . `` , which is completely redundant ( since this page is specifically for documenting this module ) However , I have n't been able to find any config options that would let me selectively disable these two aspects while still getting the automatic listing of all of the module members.My current plan is to just use autofunction ( etc ) and explicitly enumerate the members to be documented , but I 'd still like to know if I missed an easy way to achieve what I originally wanted.Update : I at least found a workaround for the second part : set add_module_names=False in conf.py . That 's a global setting though , so it does n't really answer my original question ."
"NOTE : IT DOES WORK IN PRODUCTION . I MEAN , WHEN I UPLOAD THE APPLICATION IT JUST WORKS FINE . THE PROBLEM IS IN THE DEVELOPMENT SERVER.Here is some code that can show you what i 'm trying to do : If i do it , and after some time i try to get itIt does n't work . e is None ! But , if i do this : It works fine.What am i missing ? Important Note : I wait some time to check if the object is created ! I do n't do get after the call put_async . But , it still does n't work , even a minute after . I 'm in the Development Server !"
"I 'm trying to subclass str - not for anything important , just an experiment to learn more about Python built-in types . I 've subclassed str this way ( using __new__ because str is immutable ) : It initializes right , as far as I can tell . but I cant get it to modify itself in-place using the += operator . I 've tried overriding __add__ , __radd__ , __iadd__ and a variety of other configurations . Using a return statement , ive managed to get it to return a new instance of the correct appended MyString , but not modify in place . Success would look like : Any thoughts ? UPDATETo possibly add a reason why someone might want to do this , I followed the suggestion of creating the following mutable class that uses a plain string internally : and tested with timeit : The difference increases rapidly at the number goes up - at 1 million interactions , StringInside took longer than I was willing to wait to return , while the pure str version returned in ~100ms.UPDATE 2For posterity , I decided to write a cython class wrapping a C++ string to see if performance could be improved compared to one loosely based on Mike Müller 's updated version below , and I managed to succeed . I realize cython is `` cheating '' but I provide this just for fun.python version : cython version : performance : writingreadingpopping"
"I am adding unit tests and to a kind of `` legacy '' Python package . Some of the modules contain their own doctests embedded in docstrings . My goal is to run both those doctests and new , dedicated unit tests.Following this Q & A ( `` How to make py.test run doctests as well as normal tests directory ? '' ) I 'm using the -- doctest-modules option to pytest . When running from the source repository , pytest indeed discovers the embedded doctests from Python modules under the src directory.However , my goal is to test that the source distribution builds and installs at all , and then test everything against the installed package . To do this I 'm using tox which automate the process of building a sdist ( source distribution ) tarball , installing it in a virtual environment , and running the tests against the installed version . To ensure that it is the installed version , rather than the one in the source repository , that is imported by the tests , I follow the suggestion in this article and the repository looks like this now : ( The test scripts under tests import the package as in import my_package , which hits the installed version , because the repository layout makes sure that the src/my_package directory is out of the module search paths . ) And in the tox configuration file , the relevant sections looks likeSo far , the tests run fine , and the doctests are picked up -- from the modules under src/my_package , rather than from the package installed in tox virtual environments.My questions related to this set-up is as follows : Is this actually a concern ? tox seems to ensure that what you install is what you have in the source repository , but does it ? How can I instruct pytest to actually run doctests from the installed modules in a sort of clean way ? I can think of a few solutions such as building a dedicated documentation tree in a doc directory and let pytest find the doctests in there with the -- doctest-glob option . But is there a method to do this without building the docs first ?"
"I 'm using NamedTuples to hold data , and I want to add a method that can be inherited by multiple NamedTuple based classes . But when I try using multiple inheritance or subclassing NamedTuple based classes , it does n't work . Specifically , I 'm trying to automatically give all of my data classes a method that can look at the classes annotations and then call some serializing code based on that . Here are some examples of what I 've tried : is there a way for me to use the NamedTuple class like this ?"
"I 'm just getting started with the factory_boy django library for test factories , and having an issue with a duplicate key constraint violation.test_member_programme.pyfactories.pyWhen running the first test passes successfully , but the second fails with the following error : My understanding is that every TestCase should run within a transaction , yet the creation of the foreign key does not appear to have rolled back before the second test runs . Clearly I 'm doing something fundamentally wrong , but I 'm a bit stumped ! Thanks ! I 've tracked down the problem , but unfortunately do n't know how to resolve it . The issue is that ROLLBACKs are occurring , but only on one database ( this app has 2 databases ) . For legacy reasons , we have a separate database for django auth , flatpages etc and another db for our app.Someone with a similar problem here ."
I saw this Python snippet on Twitter and was quite confused by the output : What is going on here ?
"In Django , when I request a resource that has a many-to-many relationship , I end up getting all the items in child part of the relationship , even those not directly related to the parent . It 'll be easier if I show you with code ( classes trimmed down to only show what 's necessary ) : ModelsResourcesA Ticker Report has many Wells , and these Wells are shared across all Ticker Reports . What is different is that you can tie Nodes to Wells ; for a given ticker report , the only nodes that should display are the ones that are related to that Ticker Report . So , for a given ticker report and a set of wells , only the nodes that share that GenericForeignKey to that Ticker report should be shown.Relationships : page_object_id , page_content_object , page_content_type is a GenericForeignKey relationship to ReportCurrently , all Nodes are shown ( this is a bug ) .In TastyPie , how do I tell it to only show the related objects and not all objects ? Here 's a short python console that shows the problem more succicintly : SQL Actual Output ( Modified to make it easier to read ) Expected SQL Output : Expected output : How can I filter out the child end of a many-to-many relationship with Django and TastyPie ( though this problem is apparent without TastyPie as well , leaving me to believe it 's a structural issue )"
"I am writing a Cloud Function to : Export a Cloud SQL ( postgresql ) DB to a file in a Cloud Storage bucketImport it back into another Cloud SQL instance/DB ( still postgresql ) Note : I want this code to run on its own every night to copy a production DB to a staging environment , so I 'm planning to trigger it using Cloud Scheduler.If you have a better/easier solution to pull this out within GCP I 'm all ears : ) Here 's my code ( the actual function is clone_db at the bottom of the file ) : Things work perfectly fine until I 'm trying to import the exported data into my target instance.As it calls import_ , the function fails with the following error : I have read about this error in many other Q & As here and on the web , but I ca n't figure out how to make things work.Here 's what I have done : The Cloud Function is run as my `` Compute Engine default service account '' , which has the Project Editor role set in IAMThe target Cloud SQL instance 's service account is added in the bucket 's permissions as a Storage Object Admin . I have tried various other roles combinations ( legacy reader/owner , storage object viewer , ... ) to no availAs you can see in the function 's code , I am specifically granting read access to the target instance 's service account for the exported file , and it is correctly reflected on the object 's permissions in cloud storage : I have tried disabling object-level permissions for this bucket and made sure the permissions of my first point above were correctly set , but it did not work eitherInterestingly , when I 'm trying to manually import the same file on the same instance from the GCP Cloud SQL console , things work perfectly well.After it 's done , I can see that my exported file 's permissions have been updated to include the instance 's service account as a Reader , just as I have done in my code in the end to try and reproduce the behaviour.So what am I missing here ? Which permissions should I set , for which service account , for this to work ?"
Is there a way to dynamically get the version tag from my __init__.py file and append it to the dockerrun.aws.json image name for example : :This when when I do eb deploy it will build the correct version . At the moment I have to keep modifying the json file with each deploy .
"I was trying to resolve the issue below , after some searching it seems to be an open bug in Django . I resolved the issue by adding a classmethod to the model child , although this solution works , it still requires another custom check on any ( Model ) Form using this child class . I 'm posting this for others to find a solution sooner than I did , other solutions are welcome too.raises :"
Is there a way of creating the tags dynamically with the E-factory of lxml ? For instance I get a syntax error for the following code : I get the following error :
"I am trying to make a directed graph or Sankey diagram ( any would work ) for customer state migration . Data looks like below , count means the number of users migrating from the current state to next state.I have written a code that builds a sankey , but the plot is not easily readable . Looking for a readable directed graph . Here is my code :"
"I am interfacing to an external library using ctypes . This library returns to me a binary buffer . The interface looks like this : The library also exports a deallocator so that I can free the buffer when I am done with it , but that aspect presents no problems to me , so I do n't think we need to cover it.In my ctypes code I am representing the buf argument as c_void_p . I would like to copy this buffer into a bytes object as efficiently as possible.At the moment I have : where buf is c_void_p and len is c_int.As I understand it , this performs two copies . Once to the bytearray object , and then again to the bytes object.How can I do this with only a single copy ? My current efforts have concentrated on Python 2 , but in due course I will need to support this for Python 3 as well ."
"I am trying to write some nose tests for a python project . It has been a while ( a year or so ) since I last wrote some nostests and it looks like nose2 is the suggested module to use for this now.I want to write a test to check that an exception is raised when the wrong value is sent into a def function . I know in nose is was used like this : I just ca n't find an equivalent use example for nose2 , none of these imports work ( there is a suggestion that nose2 is more like unittest than nose , which seems to use assertRaises ) : A search of the nose2 documentation website has no mention of an assert_raises or assertRaises"
I was trying to match the orthogonal polynomials in the following code in R : but in python . To do this I implemented my own method for giving orthogonal polynomials : though it does not seem to match it . Does someone know type of orthogonal polynomial it uses ? I tried search in the documentation but did n't say.To give some context I am trying to implement the following R code in python ( https : //stats.stackexchange.com/questions/313265/issue-with-convergence-with-sgd-with-function-approximation-using-polynomial-lin/315185 # comment602020_315185 ) : because it seems to be the only one that works with gradient descent with linear regression with polynomial features . I feel that any orthogonal polynomial ( or at least orthonormal ) should work and give a hessian with condition number 1 but I ca n't seem to make it work in python . Related question : How does one use Hermite polynomials with Stochastic Gradient Descent ( SGD ) ?
How do I check for a particular subparser ?
"If I try the following code ( in Python 3.2.2 ) , then it appears that x is untouched - it either remains undefined or keeps whatever value it had previously . Is this behaviour guaranteed whenever the right hand side of an assignment throws an exception ? I realise this is a very basic question , but I ca n't find much information about how exactly assignment works . More generally , is the entire right hand side always evaluated before anything relating to the assignment happens ? Is this even true when using setattr , assigning to an element of a list , or using tuple unpacking ( i.e . something like x , y = y , f ( ) ) ?"
"I have Python code split into a web frontend and consumer backend . The backend has to run under Jython for interoperability with some Java libraries , but we want to run the webserver in cpython/mod_wsgi because of memory problems we 've had running it in Jython through Jetty . We use virtualenv for development , and I 've currently got two virtualenv directories , one for cpython and one for jython . My question is whether both interpreters can happily coexist in one virtualenv ( primarily to save having to update installed libraries twice , but also from curiosity ) . I 've initialized the same environment with both interpreters : I can run both interpreters , and I can symlink environ/Lib/site-packages and environ/lib/python/site-packages so that they 're both looking at the same place . As long as there are no libraries relying on C extensions/optimizations I ca n't see why they would n't work in both interpreters . Anyone disagree or have any further things to look out for ?"
"I 'm overwriting the save method of a ModelForm and I do n't know why it would cause recursion : Causes this : Stacktrace shows this line repetitively calling itself : Now , the parsley decorator is like this : As @ DanielRoseman suggested that the Parsley decorator extending the AccountForm causes the super ( AccountForm , self ) to keep calling itself , what 's the solution ? Also I can not get my head around this why this would cause recursion ."
"Just learning django , I 'm reading this tutorial and getting confused at this part : Having searching its documentation , still ca n't figure out what does 'date published ' argument mean ? Anyone can explain ?"
"I have a CSV file containing feature values for items : each row is a triple ( id_item , id_feature , value ) representing the value of a specific feature for a specific item . The data is very sparse.I need to compute two item distance matrixes , one using Pearson correlation as metric and the other using the Jaccard index.At the moment I implemented an in-memory solution and I do something like this : it works well and it 's pretty fast but it is not scalable horizontally . I would like to be able to increase the performances just by adding nodes to a cluster and that everything could work even in a big data scenario , again just by adding nodes . I do n't care if the process takes hours ; distances need to be updated once a day.What 's the best approach ? 1 ) Sklearn pairwise_distances has a n_jobs parameter that allows to take advantage of parallel computing ( http : //scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html ) but as far as I know it supports multiple cores on the same machine and not cluster computing.This is a related question Easy way to use parallel options of scikit-learn functions on HPC but I did n't get what is the best solution in my specific case and if Joblib actually has issues . Also , the part that reads in memory the CSV would still be a bottleneck : I can store the CSV in HDFS and read it doing something like : and then loop through cat.stdout : but I am not sure it is a good solution.2 ) Store data in HDFS , implement computation in a map reduce fashion and run the job via mrjob3 ) Store data in HDFS , implement the computation in a SQL-like fashion ( I do n't know if it is easy and feasible , I have to think about it ) and run it using PyHiveOf course I would like to keep as much as possible the current code , so a variant of the solution 1 ) is the best one for me ."
"I am using python requests library and trying to persist the session.Since I have multiple IPs on my host , I created the following method in order to make session bind to a specific IP.The following code piece is used to invoke this class : After mounting http and https protocol to this adapter , I used pickle to persist the object into redis as follows : The problem occurred when I tried to unpickle the session object : It complained the SourceAddressAdapter does not have source_address attribute . Before I added this class SourceAddressAdapter to my session , the serialization worked well . So I guess this is a problem with customized class pickling/unpickling . UPDATE : It works after I added __getstate__ and __setstate__ method into SourceAddressAdapter"
"I 'm trying to decorate a function by replacing it with a instance of a callable class : This works as expected.However , as soon as I try to decorate a class method : I get a TypeError : dosomething ( ) takes exactly 1 argument ( 0 given ) .Now , I think I can answer the why : To support method calls , functions include the __get__ ( ) method for binding methods during attribute access . This means that all functions are non-data descriptors which return bound or unbound methods depending whether they are invoked from an object or a class.So , FunctionFaker , not being a function , does n't have the said descriptor , thus not mangling the arguments.How can I implement a callable class that is able to replace a instance method ?"
"I 'm trying my hand at asyncio in Python 3.6 and having a hard time figuring out why this piece of code is behaving the way it is . Example code : Output : Expected Output : My reasoning for expected output : While the compute_sum coroutine is correctly called before the compute_product coroutine , my understanding was that once we hit await asyncio.sleep ( 5 ) , the control would be passed back to the event loop which would start the execution of the compute_product coroutine . Why is `` Returning sum '' being executed before we hit the print statement in the compute_product coroutine ?"
"I 'd like to use multiprocessing.Value + multiprocessing.Lock to share a counter between separate processes . For example : This will throw the following exception : RuntimeError : Synchronized objects should only be shared between processes through inheritanceWhat I am most confused by is that a related ( albeit not completely analogous ) pattern works with multiprocessing.Process ( ) : Now , I recognize that these are two different markedly things : the first example uses a number of worker processes equal to cpu_count ( ) , and splits an iterable range ( 25 ) between themthe second example creates 25 worker processes and tasks each with one inputThat said : how can I share an instance with pool.starmap ( ) ( or pool.map ( ) ) in this manner ? I 've seen similar questions here , here , and here , but those approaches does n't seem to be suited to .map ( ) /.starmap ( ) , regarldess of whether Value uses ctypes.c_int.I realize that this approach technically works : Is this really the best-practices way of going about this ?"
"If I have a list of letters , such as : word = [ ' W ' , ' I ' , ' N ' , ' E ' ] and need to get every possible sequence of substrings , of length 3 or less , e.g . : W I N E , WI N E , WI NE , W IN E , WIN E etc.What is the most efficient way to go about this ? Right now , I have : This just gives me the below , rather than the sub-sequences : I just ca n't figure out how to create every possible sequence ."
"I have a simple time series plot in Pandas , that can be emulated by the following code below : Which generates the following graph : My problem is that the date displayed is not relevant to the graph itself and I wish to remove it and leave only the time series on the x axis.How do I do this ?"
"I am writing documentation for a project and I would like to make sure I did not miss any method . The code is written in Python and I am using PyCharm as an IDE.Basically , I would need a REGEX to match something like : but it should NOT match : I tried using PyCharm 's search with REGEX feature with the pattern ) : \s* [ ^ '' ' ] so it would match any line after : that does n't start with `` or ' after whitespace , but it does n't work . Any idea why ?"
"Context : I want to take a user uploaded file , attach it to a model , change some properties and then save the object to database.Here 's some relevant code : models.pyforms.pyviews.pyThis code prints exactly what I want , which is the file path after upload_to has been called and the file has been saved to said location ( eg . `` uploaded_files/b992e44e-6403-4c37-82b4-b3c403d07f79 '' ) .Now , with this views.py , things change : views.pyInstead of the path , I get the original filename ( eg . `` jnes.exe '' ) so I ca n't process the file so I can get its mime type , for example , and then update the model and save it do the database.Any ideas what I 'm doing wrong ? Thanks . How I got around it : I do the necessary model edits on the file that 's still in memory/temporarily on disk since Django does n't allow you to directly find a FileField 's location until you actually save the new model instance.forms.pyI get the hashes and mime type in FileObject 's constructor by reading data from Django 's UploadedFile temporary object . ( thanks Daniel )"
"I have read this Q & A , and already try to catch exception on my code that raise an IntegrityError exception , this way : But somehow my unit test still failed and stop with IntegrityError exception . I expect it to say OK as I already expect to have exception in my unit test.This was cause by code that tries to insert row having the same unique field values.Any idea ?"
"If you look at the following timings : There is a significant difference in execution speed betweenreduce ( int.__mul__ , range ( 10000 ) ) and reduce ( mul , range ( 10000 ) ) with the latter being faster.using the dis module to look at what was happening : Using int.__mul__ method : And the operator mul methodThey appear the same , so why is there a difference in execution speed ? I am referring to the CPython implementation of PythonThe same happens on python3 :"
"In Python , if I have a dict , and I want to get a value from a dict where the key might not be present I 'd do something like : where , if someKey is not present , then someDefaultValue is returned.In C # , there 's TryGetValue ( ) which is kinda similar : One gotcha with this though is that if someKey is null then an exception gets thrown , so you put in a null-check : Which , TBH , is icky ( 3 lines for a dict lookup ? ) Is there a more concise ( ie 1-line ) way that is much like Python 's get ( ) ?"
"I 'm using Python and Envoy . I need to delete all files in a directory . Apart from some files , the directory is empty . In a terminal this would be : Common sense dictates that in envoy , this translates into : However : Naturally there are alternatives to using envoy in this case , I am simply wondering why it does n't work.Any clues ?"
"We 've been having an issue on our production code for a little while and finally narrowed it down to the data coming back from SQLAlchemy . Running the same query multiple times will sometimes return an empty result . Under some conditions , we can get it to return an empty result every time the code is executed . This is despite the fact that the data in the database has n't changed at all and that pure SQL versions of the same query running directly on cx_Oracle always return the correct result.Here is the Declarative code for SQLAlchemy : And here is the test code : The first function , sqlalchemy_test , will error about 50 % of the time . The second function , cx_oracle_test , has not yet errored . Now here 's what 's interesting : the problem disappears if we introduce a pause for several seconds between cursor = session.query ( db.lot ) and results = cursor.first ( ) . So it looks like some sort of timing issue.Any clue what 's going on here ? EDIT : I 've simplified the code necessary to create the error . Here it is :"
"I am trying to produce a Mock of matplotlib so that I can compile my docs using ReadTheDocs , but have run into a problem.In my code , I import matplotlib using from matplotlib.pyplot import *.I am using the following code for my Mocks ( as suggested by the ReadTheDocs FAQ ) : However , when running from matplotlib.pyplot import * I get an error saying that TypeError : 'type ' object does not support indexing.Is there a way that I can change my Mock so that it allows me to import matplotlib using the from x import * style ? I do n't need any particular functions to be made available , I just need it to be able to be imported so that ReadTheDocs can import the code properly ."
"I come from a python background , where it 's often said that it 's easier to apologize than to ask permission . Specifically given the two snippets : Then under most usage scenarios the second one will be faster when A is usually an integer ( assuming do_something needs an integer as input and will raise its exception fairly swiftly ) as you lose the logical test from every execution loop , at the expense of a more costly exception , but far less frequently . What I wanted to check was whether this is true in C # , or whether logical tests are fast enough compared to exceptions to make this a small corner case ? Oh and I 'm only interested in release performance , not debug.OK my example was too vague try this one : Naive solution : Logic based solution : Exception based solution : Examples using FSOs , database connections , or stuff over a network are better but a bit long-winded for a question ."
"Consider following Python 2.x code snippet.Output of this script ( given file.txt has more than one line ) is : I 'm interested about __exit__ call specifically . What triggers execution of his method ? For what we know , code never left with statement ( it `` stopped '' after yield statement and never continued ) . Is it guaranteed that __exit__ will be called when reference count of generator drops to 0 ?"
"There are many situations where slicing operations in 2D arrays produce a 1D array as output , example : There are workarounds like : Is there any numpy built in function that transforms an input array to a given number of dimensions ? Like :"
"Good day everyone , I have the following code segment : That produces Voronoi plots such as this : My goal is to find where the 'rays ' ( the lines that go out of the plot , dashed or solid ) intersect with a given line ( for example x = 500 ) . How may I go about doing this ? I have already tried using ridge_vertices list in the Voronoi object , however , these 'rays ' are associated with only a single vertex in the list , so I can not figure out the line equation.Edit : My ultimate goal with this is , given the borders of the plane , to find the points that intersect with these borders for a given edge cell . For example , given the edge cell in the upper left , and the borders y = -50 and x = 525 , I would find the points marked with the red X's.So if you have any insights to this , they would be appreciated.Thank you ."
"This is a follow-up question to Combinatorics in PythonI have a tree or directed acyclic graph if you will with a structure as : Where r are root nodes , p are parent nodes , c are child nodes and b are hypothetical branches . The root nodes are not directly linked to the parent nodes , it is only a reference.I am intressted in finding all the combinations of branches under the constraints : A child can be shared by any number of parent nodes given that these parent nodes do not share root node.A valid combination should not be a subset of another combinationIn this example only two valid combinations are possible under the constraints : The data structure is such as b is a list of branch objects , which have properties r , c and p , e.g . :"
"I 'm using Python Networkx 2.1 to calculate Betweenness Centrality and Current Flow Betweenness Centrality on an undirected graph , with weighted edges.My concern is about the meaning of the parameter 'weight ' in the networkx functions . Please consider the graph given by the following exampleEdges weight in my case is strength of relationship , therefore something positive . The idea is that edges with higher weights should contribute more to betweenness.Given this idea , am I correct in saying that the right formulas to calculate node Weighted Betweenness Centrality and Weighted Current Flow Betweenness Centrality are the following ? Please note that in the first formula I used the reciprocal of edge weights as it is my feeling that these are interpreted by the algorithm as distances , so something 'bad'.In the second formula , on the other hand , I used the original weight , as in the algorithm of current flow betweenness it seems this gives more importance to nodes 1 and 2 , as in betweenness . Therefore here weights seems to be 'positive'.I 'm wondering if I do something wrong . In fact , on larger graphs the two measures correlate more if I use the same weight parameter , and not the reciprocal.How is weight treated in these two algorithms ?"
"ProblemAs part of python unittest , some input json files are to be loaded which exists under 'data ' directory which resides in the same directory of test py file . 'pkg_resources ' is used for this purpose.It works fine when the unittest are running with python . But it fails when running with twisted trial.My project has mixed testcases with both python unittest testcases as well as twisted.trial.unittest testcases . so , there is a need to run both type of testcases with twisted trial in general.The '_trial_temp ' directory is added in path when running testcases with twisted trial . please , let me know there is any way to handle this ? Example directory structure : trialTest.pyRunning test using python and its output : Running test using python and its output :"
"Recently I have been trying to get wagtail to work with my existing Django application . I was experiencing an error : -After much troubleshooting I managed to fix this , by copying the folder wagtail from : into hereHaving resolved this error , I received another about a different module , and another ... each time I copied the folder from /usr/local/lib/python2.7/dist-packages/ into /opt/django/src/ and it eventually resolved the issues I was having and uWSGI started . Now when I access the homepage of my app , I receive this errorI have checked the file referenced ( ./wagtail/wagtailadmin/urls/__init__.py ) and it looks like this : The offending line is the first wagtail 'from ' ... from wagtail.wagtailadmin.urls import pages as wagtailadmin_pages_urlsHow can I resolve this error ?"
"Original QuestionI have a working version of my web application that I am trying to upgrade at the moment , and I 'm running into the issue of having a task which takes too long to complete in during a single HTTP request . The application takes a JSON list from a JavaScript front end by an HTTP Post operation , and returns a sorted/sliced version of that list . As the input list gets longer , the sorting operation takes a much longer time to perform ( obviously ) , so on suitably long input lists , I hit the 60 second HTTP request timeout , and the application fails.I would like to start using the deferred library to perform the sort task , but I 'm not clear on how to store/retrieve the data after I perform that task . Here is my current code : Ideally I would like to replace the entire try/except block with a call to the deferred task library : But I 'm unclear on how I would get the result back from that operation . I 'm thinking I would have to store it in the Datastore , and then retrieve it , but how would my JavaScript front end know when the operation is complete ? I 've read the documentation on Google 's site , but I 'm still hazy on how to accomplish this task.How I Solved ItUsing the basic outline in the accepted answer , here 's how I solved this problem : The Javascript frontend then polls queryLineups URL for a fixed amount of time and stops polling if either the time limit expires , or it receives data back . I hope this is helpful for anyone else attempting to solve a similar problem . I have a bit more work to do to make it fail gracefully if things get squirrelly , but this works and just needs refinement ."
I 'm trying to get the max of a vector given a boolean value.With Numpy : But with Theano : Why does this happen ? Is this a subtle nuance that i 'm missing ?
"Try this in an interactive python shell.The above returns False , why ?"
"To understand my question , I should first point out that R datatables are n't just R dataframes with syntaxic sugar , there are important behavioral differences : column assignation/modification by reference in datatables avoids the copying of the whole object in memory ( see the example in this quora answer ) as it is the case in dataframes . I 've found on multiple occasions that the speed and memory differences that arise from data.table 's behavior is a crucial element that allows one to work with some big datasets while it would n't be possible with data.frame 's behavior.Therefore , what I 'm wondering is : in Python , how do Pandas ' dataframes behave in this regard ? Bonus question : if Pandas ' dataframes are closer to R dataframes than to R datatables , and have the same down side ( a full copy of the object when assigning/modifying column ) , is there a Python equivalent to R 's data.table package ? EDIT per comment request : Code examples : R dataframes : R datatables : In Pandas :"
"I have a simple Python script that recursively checks to see if a range of n numbers are factors of a number x . If any of the numbers are not factors I return False , otherwise when the n==1 I would like return True . However I keep returning NoneType and would appreciate suggestions on how to fix this ."
I 'm trying to build and install my python library using new PyGradle plugin from LinkedIn . I immediately run into problems because the the demo repo is very limitedI would like to be able to pull dependencies from the official pypi.python.org repo but to my surprise there 's no mention of it in the documentation I was able to dig out from the PyGradle 's GitHub site . Is it possible at all and if not what can I do ? P.S . Based on the pygradle documentation there is no direct way of using PyPi repo since it utilizes Ivy to do the actual dependence management . It would be great to get an `` official '' tutorial on how to setup local and Artifactory-based repos and import needed packages
"Today my CPSC professor assigned a quiz in python , the topic was recursion.The whole class got stuck on question number two , which is below . Nobody was able to even come close to a solution.Example code : You are only allowed to modify the underscores , such as my example below may demonstrate . My solution was so far from close , it really should n't even be considered : Does anyone know how this could be solved ? It seems like quite a challenging problem when we have only 15 minutes to solve it.Just for reference , he said he would drop the question considering nobody in the class - advanced comp sci class of about 10 carefully selected students - could solve it ."
"I want to implement a Keras custom layer without any input , just trainable weights.Here is the code so far : I am getting an error message : __call__ ( ) missing 1 required positional argument : 'inputs'Is there a workaround for building a custom layer without inputs in Keras ?"
How can I get goal from l ? I 'm playing with list comprehensions but it 's messy !
"I found that I can measure execution time on Windows with this command : and it works great . Unfortunately , when I try to run a script which takes some ( positional and optional ) arguments , I get an error message , withI get the error : Start-Process : A positional parameter can not be found that accepts argument 'file.txt'.Without Measure-Command everything works fine.What am I doing wrong ? How to measure execution time for scripts which take arguments ?"
"Suppose I have two data frame 'df_a ' & 'df_b ' , both have the same index structure and columns , but some of the inside data elements are different : And now I want to replace the element of df_a by element of df_b which have the same ( index , column ) coordinate , and attach df_b 's elements whose ( index , column ) coordinate beyond the scope of df_a . Just like add a patch 'df_b ' to 'df_a ' : How to write the 'patch ( df_a , df_b ) ' function ?"
"I have two , numpy arrays , the first , A , being one-dimensional , the second , B , is two-dimensional in the application I have in mind , but really could have any dimension . Every single index of B covers the same range as the single index of A.Now , I 'd like to sort A ( in descending order ) but would like to permute every dimension of B along with it . Mathematically speaking , if P is the permutation matrix that sorts A , I would like to transform B according to np.dot ( P , np.dot ( B , P.T ) ) . E.g . consider this example where sorting coincidentally corresponds to reversing the order : The application I have in mind is to obtain eigenvalues and eigenvectors of a nonsymmetric matrix using np.linalg.eig ( in contrast to eigh , eig does not guarantee any ordering of the eigenvalues ) , sort them by absolute value , and truncate the space . It would be beneficial to permute the components of the matrix holding the eigenvectors along with the eigenvalues and perform the truncation by slicing it ."
I want to check if two lists have the same type of items for every index . For example if I havethe check should be True for x and y.The check should be False for y and z because the types of the elements at the same positions are not equal .
"I am using the commoncrypto library on the iPhone to create a pair of RSA keys and I am trying to send the public key to a server ( Python ) so that I can use it to verify a signature sent from the phone . I 'm using the exact code from the CommonCrypto example using the method getPublicKeyBits ( ) which looks like this : ` - ( NSData ) getPublicKeyBits { OSStatus sanityCheck = noErr ; NSData publicKeyBits = nil ; NSData* publicTag = [ [ NSData alloc ] initWithBytes : publicKeyIdentifier length : sizeof ( publicKeyIdentifier ) ] ; CFDataRef cfresult = NULL ; } ` The problem is that I do n't know how , exactly , the key is being stored or how to pass it . I am using the getPublicKeyBits ( ) to get it in a NSData structure , and I 've imported a library to encode it in base64 . I 'm getting a key ( SHA1 , I 'd like to move to SHA256 , but that 's secondary to getting this working ) and the base64 version looks like other keys I 've found here and in other people solving RSA problems.I 've been trying to use the M2Crypto library in Python and when I try to verify I 'm getting the error `` RSA Error : No Start Line '' . Here 's the code I 'm using to receive the public key : And the code I 'm using to check the signature : I 'm really stumped as to what I 'm doing wrong but I feel like I 'm getting close . If my whole method is not the way you 'd do it , I 'd love to hear any other way to generate RSA keys on the phone , publish the public key to a server , and then verify a signature from the phone on that server.Thanks !"
"I 've got a library that takes in a very simple C image structure : I did n't create this library , nor this structure , so I ca n't change it . I 'm responsible for wrapping this library for python using SWIG . The Python wrapper needs to be able to take in a PIL Image and convert it into this structure . Here 's how I 'm doing it right now ( using a SWIG % inline % ) : And then on the python wrapper side here 's how things look right now : Amazingly this works , however as you may have guessed it 's incredibly slow when working with even moderately large images . I know with SWIG the proper way to do things is to use a typemap , however that would mean digging in to the C API of PIL , and I just did n't have time to do that at the moment.What are my options in terms of speed ? Are there quicker ways of marshaling the pixel data from a PIL image to this simple image structure ? Has someone already done this and my Google skills are just that bad ? Am I just boned and soon will need to learn the internals of PIL ? Thanks ."
"what does a [ 0:1 ] mean ? if it 's a pointer to the range of a , thus a [ 0:1 ] [ 0 ] = 1 should change the value of a.if it 's a copy of the range of a , thus a [ 0:1 ] = [ 1 ] should n't change the value of a.I think the result of two is inconsistent to each other . Could you please help me work out the problem ?"
"I 'm currently documenting a whole module with autodoc . However , I define several variables on the module level that contain long lists or dicts . They are included in the documentation together with the values , and the values are unformatted , so it looks like a 10-lines mess . What I want is for the docstring of those variables to be included , but for the values to be omitted or at least nicely formatted.I 've tried to exclude the variable from automodule directive and add it like that : This resulted in that only the variable name was included , whereas both the docstring and the value of longstuff were not present in the documentation.How can I keep the docstring and get rid of the value ( or have it nicely formatted ) at the same time ?"
Let 's say I have this dictionary : I get the key which has greatest value with this code : it returns ' c'But I want to select a random key from top 3 greatest values . According to this dictionary top 3 are : It should randomly select a key from them . How can I do that ?
When I run ./foo.py -h where foo.py is the following code it crashes with the error ValueError : too many values to unpackHere is the code.Is there a bug in my code ? Full traceback ( Python 2.7.3 ) :
"I have one list of data as follows : The list 'data ' has following elements : I have to check if an element intersect with any other elements . If intersects , they should put in one tuple ; and if not intersect they should put in different tuple . The expected result is : How to do it ? I tried it as :"
"When you try and nest several bookmarks with the same name , PyPdf2 does not take it into account.Below self-contained python code to test what I mean ( you need at have 3 pdf files named a , b and c in the working folder to test it out ) I would expect the resulting pdf to have three levels of nested bookmarksbut instead I get Any way to make sure this does not happen ? EDIT : I have removed the pagenum variable as I want those 3 bookmarks to point to the same page ."
"I 've been playing with the xkcd style feature in matplotlib . I have Matplotlib 1.4 , and the humor sans font is installed correctly , or works in msword at least . When I run the following example code , I get `` ? '' instead of `` - '' on labels any time there should be a negative number . Any idea what causes this ? Edit : At Bernie 's suggestion ( thanks Bernie ) I tried some different backends . Tried Qt , Tkinter , and inline , and none seem to work . I 'm using Ipython 2.3 btw.Also , reputation is now high enough to post an image . As you can see everything is as expected except those pesky question marks ."
"I am using Sublim Text 3 on Mac OS X El Capitan . What I need to do is to evaluate a Python file within Sublime Text 3.I have installed Package Control and then SublimREPL plugins.I have set up a 2 rows layout ( View > Layout > Rows : 2 ) in order to display a Python interpreter in the second part of the screen.I then launch the Python interpreter with Tools > Command Palette ... > SublimeREPL : Python.The interpreter starts correctly and I get this : I ca n't find how to start with Python 3.5 that I have downloaded manually ( thus installed in /usr/local/bin/ ) . I have tried to modify this file : /Library/Application Support/Sublime Text 3/Packages/SublimeREPL/Config/Python/Main.sublime-menu following this post instructions , but this did not change anything ( Python 2.7.10 still launched ) .Here is the content of my Main.sublime-menu : Still following this post advices , I modified the part of code below , but I ca n't find any exe file in folder /usr/local/bin/ : When I press Ctrl + , + f ( according to the doc ) , the interpreter still starts with Python 2.7.10 ."
This code ... Produces the following error ... But this code does not.The only difference I see is the last variable name is `` vb '' vs. `` v '' .I am leaning Python and am working on the OOP stuff now .
"I have recently started a new project in PyCharm , finally utilizing anaconda environments . However , after trying to make my first commit through PyCharm , it appears to be using my native python , not the environment set in PyCharm . I 've tried to restart PyCharm several times , restarted my computer , and reinstalled the virtual environment.Here is a copy of pre-commit hook : The linter is the following : ( which python has been added to highlight the issue ) I am running the commit through PyCharm - > VCS - > Commit . Inside PyCharm , the commit fails ( below this are a large amount of mypy errors , but note the environment ) However , if I run the commit from the terminal with $ git commit -m `` testing commit '' the commit works . It provides the following response : This is the correct virtual environment inside of the project , seen here : Am I setting something up incorrectly ? I vastly prefer PyCharm 's VCS and would prefer not to have to use git from the terminal ."
"I am using Plotly-Dash and need the font size of my text annotations to scale with the viewport width , as my graphs do . For various headers in my layout , I am able to directly set font-size : '1vw ' , however vw is not an accepted unit for setting the font-size for the style attribute of a dcc.Graph component . Here is the associated traceback : ValueError : Invalid element ( s ) received for the 'size ' property of scatter.textfont Invalid elements include : [ '1vw ' , '1vw ' , '1vw ' , '1vw ' , '1vw ' , '1vw ' , '1vw ' , '1vw ' , '1vw ' , '1vw ' ] I figure that if the dcc.Graph component can accept viewport units ( e.g . style = { height : 30vw , width : 30vw } ) and simply convert them to pixels browser-side , then I should be able to perform a similar conversion with the font-size.Is there a means on the Python side to retrieve the viewport width in pixels , so that I can perform my own scaling logic for the font size ? Here is a sample Dash application that demonstrates this behavior :"
"What is the recommeneded way to lock an entity ( e.g . User ( amount_earned ) ) to perform consistent read -- > update -- > set operations on Google Cloud Datastore.Using memcache , we usually do a memcache.add ( lock_key ) , to make sure there is only process/thread modifying a object.Since memcache 's add is atomic and returns false , if key was n't previously added , it is easy to simulate `` locking '' using memcache.How can one simulate similar locking semantic on Google Cloud datastore.Ideally"
"I 'm trying to test ARMA models , and working through the examples provided here : http : //www.statsmodels.org/dev/examples/notebooks/generated/tsa_arma_0.htmlI ca n't tell if there is a straightforward way to train a model on a training dataset then test it on a test dataset . It seems to me that you have to fit the model on an entire dataset . Then you can do in-sample predictions , which use the same dataset as you used to train the model . Or you can do an out of sample prediction , but that has to start at the end of your training dataset . What I would like to do instead is fit the model on a training dataset , then run the model over an entirely different dataset that was n't part of the training dataset and get a series of 1 step ahead predictions . To illustrate the issue , here is abbreviated code from the link above . You see that the model is fitting data for 1700-2008 then predicting 1990-2012 . The problem I have is that 1990-2008 were already part of the data that was used to fit the model , so I think I 'm predicting and training on the same data . I want to be able to get a series of 1 step predictions that do n't have look-ahead bias ."
"It seems very few people used it , but ... I did . Here you can read : Some undocumented classes in django.forms.widgets are removed : SubWidget RendererMixin , ChoiceFieldRenderer , RadioFieldRenderer , CheckboxFieldRenderer ChoiceInput , RadioChoiceInput , CheckboxChoiceInputMy source code is : I really dont know how to convert this to make it work with 1.11 and later : they say : Well . How ?"
I am trying to save ipaddress in a table . Created a custom type as follows : This seem to work fine but just wondering if I have to implement any other function to be complete . Eg : python_type . Is the following needed ? When will this be called and how do I call explicitly and test it works ?
I have a list of lists and a separator string like this : As result I want to have a list of strings combined with separator string from the strings in the sub lists : Order in result is irrelevant.How can I do this ?
"I was writing an answer to this question when noticed that my simple implementation did n't produce correct results . While hunting down the bug , I noticed the following : For whatever reason , gen 's next ( ) method is called one additioinal time.To illustrate this , I used the following :"
"How does one specify a memory view of a ndarray that contains strings ? char [ : ] , char* [ : ] , ... do not work.To illustrate , my problem is the definition of function abc ( ... ) : If a memoryview is not possible , can I implement direct array access myself ? I need to avoid the GIL for function abc ( ... ) .Edit 1 : In response to Bi Rico 's answer.My aim is to release the GIL for function abc ( ... ) and within it process the string elements of ndarray in_buffer with c string-functions . I.e . something like the following :"
"I am looking for the best way to combine a function with a dictionary that contains more items than the function 's inputsbasic **kwarg unpacking fails in this case : After some research I came up with the following approach : My question is : is this a good way of dealing with this problem , or is there a better practice or is there a mechanism in the language that I 'm missing perhaps ?"
"I have a python function which reads a line from a text file and writes it to another text file . It repeats this for every line in the file . Essentially : And so on.I can parallelise this process , using a queue to pass data , so it is more like : My question is - why does this work ( as in why do I get a speed up ? ) . Sounds like a daft question , but I was thinking - surely my hard disk can only do one thing at once ? So why is n't one process put on hold til the other is completed ? Things like this are hidden from the user when writing in a high level language..I 'd like to know whats going on at low level ?"
"I 'm trying to merge a line plot and a bar plot into one plot . The data source is a pandas dataframe.Here is a demo : Everything is good until now , you can see the line is above the bars . If I ask the barplot using the secondary yaxis on the right by changing the last line as : Then the bars will on top of the line which is not what I desired.Here are the two plots : I tried to plot the bars first and then plot the line and I also tried to use zorder to force the line above the bar but neither of them work.Any suggestions or help would be appreciated ."
"Suppose in Mathematica I define the following function : which outputs a list of prime numbers , such that if the input list has n at position i , then the output list will contain the nth prime number at position i . For example , Now , if for some reason ( debugging , etc ... ) I want to check what values are being fed into the Prime function . Because of the Sow command in the function , I can doFor more details on Sow/Reap , see the Wolfram Documentation . My question is , is there a natural Python equivalent of Mathematica 's Sow and Reap functionality ? In particular , is there a way to do this kind of thing without explicitly returning extra things from the python function you want to do it to , writing a second python function that is almost the same but returns something extra , or using global variables ?"
"There have been many MaltParser and/or NLTK related questions : Malt Parser throwing class not found exceptionHow to use malt parser in python nltkMaltParser Not Working in Python NLTKNLTK MaltParser wo n't parseDependency parser using NLTK and MaltParserDependency Parsing using MaltParser and NLTKParsing with MaltParser engmaltParse raw text with MaltParser in JavaNow , there 's a more stabilized version of MaltParser API in NLTK : https : //github.com/nltk/nltk/pull/944 but there are issues when it comes to parsing multiple sentences at the same time . Parsing one sentence at a time seems fine : But parsing a list of sentences does n't return a DependencyGraph object : Why is that using parse_sents ( ) do n't return an iterable of parse_one ? I could however , just get lazy and do : But this is not the solution I 'm looking for . My question is how to answer why does n't the parse_sent ( ) return an iterable of parse_one ( ) . and how could it be fixed in the NLTK code ? After @ NikitaAstrakhantsev answered , I 've tried it outputs a parse tree now but it seems to be confused and puts both sentences into one before parsing it . [ out ] : From the code it seems to be doing something weird : https : //github.com/nltk/nltk/blob/develop/nltk/parse/api.py # L45Why is it that the parser abstract class in NLTK is swooshing two sentences into one before parsing ? Am I calling the parse_sents ( ) incorrectly ? If so , what is the correct way to call parse_sents ( ) ?"
"I am using Python Caffe , and confused with net.layers [ layer_index ] .blobs and net.params [ layer_type ] . If I understand well , net.params contains all the network parameters . Take the LeNet for example , net.params [ 'conv1 ' ] represents the network coefficients for the 'conv1 ' layer . Then net.layer [ layer_index ] .blobs should represent the same . However , what I found is that they are not exactly the same . I use the following codes to test it : Any ideas on the difference between them ? Thanks ."
"I am having trouble getting the Flask test client to pass cookies . This code used to work and I presume something in my environment changed , which breaks this . I recently created a new Python 3.7 virtualenv and installed Flask 1.0.2.Running the example prints { } , but I expect it to print { `` abc '' : '' 123 '' , '' def '' : '' 456 '' } .If I run my app via flask run , sending headers with curl works :"
I need to convert a column of categorical variables in a Pandas data frame into a numerical value that corresponds to the index into an array of the unique categorical variables in the column ( long story ! ) and here 's a code snippet that accomplishes that : which converts the data frame : into the data frame : as desired . But my problem is that my dumb little for loop ( the only way I 've thought of to do this ) is slow as molasses when I try to run similar code on big data files . I was just curious as to whether anyone had any thoughts on whether there were any ways to do this more efficiently . Thanks in advance for any thoughts .
"I 'm using a framework called inginious and it 's using web.py to render its template . The rendering works on my localhost but not on my staging server . They both run python3 . I see that web.py enforces utf-8 on the encoding in Python2 only ( that 's out of my hands ) here is the stack traceMy html do include hebew chars , small example and I open it like so : and the line where the encoding fails is where the Hebrew chars are . I tried setting some environment variables in ~/.bashrc : under the user centosThe ingenious framework is installed as a pip under python3.5 site-packages . and it served by an apache server under the user apacheTried setting the environment variables in the code ( during the init of the app ) so that the apache WSGI will be aware of them I have edited the /etc/httpd/conf/httpd.conf using the setenv method : and restarted using sudo service httpd restart and still no luck . My question is , what is the best practice to solve this . I understand there are hacks for this , but I want to understand what is the underline cause as well as how to solve it . Thanks !"
Given a data frame like : Is it possible to edit index using something like replace ? Pseudo code : ( since df index doesnt have str attribute ) I need something like : The problem is that my dataframe is huge.Thanks in advance
"Python 3.6.1 , there are several ways of type hinting , in the doc string or annotation . How can I achieve this using annotation ? Say I have a class , which have a class method load to load data from somewhere , json or database for instance , and construct and return a instance of this class.I think this is quite straightforward , but python interpreter raised an error that Foo is not defined.I know the reason , because when python loads Foo 's load 's function signature , the Foo class 's definition is not finished , so the Foo is not defined yet.Is this a drawback of function annotation ? Can I find some way to achieve this goal , instead of using doc string to type hint , since I really like the clearness of function annotation ."
"Is this a feature or a bug ? Can someone explain to me this behavior of a numpy masked_array ? It seems to change the fill_value after applying the sum operation , which is confusing if you intend to use the filled result.Prints output :"
"Edit : I ended up answering the beef of my question so that I can have a working logging module . However , I still have a related question . See my answer below.I am trying to implement logging by always logging to the temporary directory of whatever os I am in . To do this I have written the following function.Here is the output : As you can see , it is now outputting double . However , the logging module is rather confusing , and I do not know of a way to find out if a log has already been instantiated , or if a log object already has a handler . Some help would be appreciated . Edit : to add a little more detail , I am planning on calling this in several modules , essentially trying to replace the `` logging.getLogger '' call ."
"After updating my Django from 1.7 to 1.9 , search engine , which is based on Haystack and Solr , stopped working . This is what I get : I have to say my database is not empy and my configuration is as follow : And this is my search_indexes.py :"
"Is there something internal in python that treats arguments passed to __getitem__ differently , and automatically converts start : stop : step constructs into slices ? Here 's a demonstration of what i meanIs it simply that the interpreter searches for instances of start : stop : step inside [ ] , and swaps them out for slice ( start , stop , step ) ? The documentation simply says : The bracket ( subscript ) notation uses slice objects internallyIs this one of the python internal bits that i ca n't alter the behaviour of ? Is it possible to make other functions take slice objects usign the start : stop : step shorthand ? **I 've seen the other question , Can python 's slice notation be used outside of brackets ? , but that just does it using a custom class , which i could easily do . What i want is a way to just use start : stop : step without having to wrap it in anything else.SIDE NOTE : It also apears that all arguments inside [ ... ] are packaged up into a tuple , somewhat as if it were doing [ *args ] - > __getitem__ ( args ) ."
"I am trying to train a very simple model which only have one convolution layer . But the input ( X ) , prediction output ( y_pred ) and true_output ( y_true ) are all complex number . When I call the function model.fit ( X , y_true ) There is the error TypeError : Gradients of complex tensors must set grad_ys ( y.dtype = tf.complex64 ) Does that means I have to write the back-propagation by hand ? What should I do to solve this problem ? thanks"
"I implemented my own __getattr__ ( ) to loosely handle any non-existent attributes.I just so happened to define this class in a Jupyter notebook to experiment with it interactively.IPython creates _ipython_canary_method_should_not_exist_ due to this __getattr__ implementation – and I 'd like to understand what this is and how to `` clean it up '' is possible.There is this issue opened about it , but it 's not clear to my why – if it checks for permissive handling within __getattr__ – it does n't check to see that _repr_html_ is implemented in the OP 's example ?"
"I 'm currently doing a maths course where my aim is to understand the concepts and process rather than crunch through problem sets as fast as possible . When solving equations , I 'd like to be able to poke at them myself rather than have them solved for me.Let 's say we have the very simple equation z + 1 = 4- if I were to solve this myself , I would obviously subtract 1 from both sides , but I ca n't figure out if sympy provides a simple way to do this . At the moment the best solution I can come up with is : Where the more obvious expression eq1 - 1 only subtracts from the left-hand side . How can I use sympy to work through equalities step-by-step like this ( i.e . without getting the solve ( ) method to just given me the answer ) ? Any pointers to the manipulations that are actually possible with sympy equalities would be appreciated ."
I created a boolean field . The boolean is showing but the label isn't.XML :
"I used to have some good working python that did auto-replies on the Tweepy stream listener , but due to changes in the Twitter API in August it no longer works.I am re-building it by getting my most recent mention every 10 seconds ( ideally it 'd be less as I want to do near-instant replies ) , and checking if it was in the last ten seconds ... if it was then the script assumes it 's a new tweet and replies.This could work on a loop every 12 seconds ; but any less and it hits the rate limit ( i.e . this method above at 10 secs will hit the rate limit eventually ) ... so , is there a better way of just retrieving the most recent mention in order to do a reply based on the mention ? I feel like I am probably doing it in a very inefficient way ( e.g . this method actually gets the last 20 mentions ! ! ) and the API probably has a better method that I could do more often without hitting rate limits ?"
"I previously implemented the original Bayesian Probabilistic Matrix Factorization ( BPMF ) model in pymc3 . See my previous question for reference , data source , and problem setup . Per the answer to that question from @ twiecki , I 've implemented a variation of the model using LKJCorr priors for the correlation matrices and uniform priors for the standard deviations . In the original model , the covariance matrices are drawn from Wishart distributions , but due to current limitations of pymc3 , the Wishart distribution can not be sampled from properly . This answer to a loosely related question provides a succinct explanation for the choice of LKJCorr priors . The new model is below.The goal with this reimplementation was to produce a model that could be estimated using the NUTS sampler . Unfortunately , I 'm still getting the same error at the last line : I 've made all the code for PMF , BPMF , and this modified BPMF available in this gist to make it simple to replicate the error . All you need to do is download the data ( also referenced in the gist ) ."
"I 've got a situation where I 'm catching a specific exception type , inspecting the exception 's message to check if it 's actually an exception I want to catch , and then re-raising the exception if not : This works fine , with one problem . In the case I re-raise the exception , that exception now occurs at the line I re-raised it ( i.e . at raise e ) , rather than at the location the exception originally occurred . This is n't ideal for debugging , where you want to know where the original exception happened.Thus my question : is there any way to re-raise or otherwise `` pass on '' an exception after catching it while maintaining the original exception location ? NOTE : In case you are wondering what the actual situation is : I 'm dynamically importing some modules using __import__ . I 'm catching ImportError to gracefully deal with the case that any of these modules do not exist . However , in the case that any of these modules themselves contain an import statement that raises ImportError , I want those `` real '' ( from the point of view of my application ) exceptions to be raised -- and at the original location as far as debugging tools are concerned ."
"I have a Newick tree that is built by comparing similarity ( euclidean distance ) of Position Weight Matrices ( PWMs or PSSMs ) of putative DNA regulatory motifs that are 4-9 bp long DNA sequences . An interactive version of the tree is up on iTol ( here ) , which you can freely play with - just press `` update tree '' after setting your parameters : My specific goal : to collapse the motifs ( tips/terminal nodes/leaves ) together if their average distances to the nearest parent clade is < X ( ETE2 Python package ) . This is biologically interesting since some of the gene regulatory DNA motifs may be homologous ( paralogues or orthologues ) with one another . This collapsing can be done via the iTol GUI linked above , e.g . if you choose X = 0.001 then some motifs become collapsed into triangles ( motif families ) . My question : Could anybody suggest an algorithm that would either output or help visualise which value of X is appropriate for `` maximizing the biological or statistical relevance '' of the collapsed motifs ? Ideally there would be some obvious step change in some property of the tree when plotted against X which suggests to the algorithm a sensible X . Are there any known algorithms/scripts/packages for this ? Perhaps the code will plot some statistic against the value of X ? I 've tried plotting X vs. mean cluster size ( matplotlib ) but I do n't see an obvious `` step increase '' to inform me which value of X to use : My code and data : A link to my Python script is [ here ] [ 8 ] , I have heavily commented it and it will generate the tree data and plot above for you ( use the arguments d_from , d_to and d_step to explore the distance cut-offs , X ) . You will need to install ete2 by simply executing these two bash commands if you have easy-install and Python :"
"Let 's say I have a simple class : If I use dir ( Foobar ) , I 'll get the following output : Even though it does not appear in the output of dir ( ) , I can access __name__ : and get Foobar.Why does Python behave that way ?"
"First off I was n't sure if I should post this as a Ubuntu question or here.But I 'm guessing it 's more of an Python question than a OS one . My Python application is running on top of Ubuntu on a 64 core AMD server . It pulls images from 5 GigE cameras over the network by calling out to a .so through ctypes and then processes them . I am seeing frequent pauses in my application causing frames from the cameras to be dropped by the external camera library . To debug this I 've used the popular psutil Python package with which I log out CPU stats every 0.2 seconds in a separate thread.I sleep for 0.2 seconds in that thread and when that sleep takes substantially longer I also see camera frames being dropped . I have seen pauses up to 17 seconds long ! Most of my processing is either in OpenCV or Numpy ( both of which release the GIL ) or in one part of the app a multiprocessing.Pool with 59 processes ( this it to get around the Python GIL ) .My debug logging shows very high 'system ' ( i.e . kernel ) CPU time on many of my process ' threads when the pauses happen.For example . I see CPU times as follows ( usually every 0.2 seconds ) and then suddenly a big jump ( 'Process ' numbers are in CPU utilization , i.e . 1 CPU fully used would be 1 , Linux top showing 123 % would be 1.2 ) : I do n't know why the high OS system usage is reported one line before matching high process system usage . The two match up since 26.4 of 64 cores = 41 % . At that point my application experienced an approximately 3.5 second pause ( as determined by my CPU info logging thread using OpenCV 's cv2.getTickCount ( ) and also the jump in time stamps in the Python logging output ) causing multiple camera frames to be dropped . When this happens I have also logged the CPU info for each thread of my process . For the example above 25 threads were running at a 'system ' CPU utilization of 0.9 and a few more at 0.6 , which matches the total for the process of 26.4 above . At that point there were about 183 threads running . This pause usually seems to happen close after the multiprocessing pool is used ( it 's used for short bursts ) but by no means happens every time the pool is used . Also , if I halve the amount of processing that needs to happen outside the pool then no camera skipping happens . Question : how can I determine why OS 'system ' / kernel time suddenly goes through the roof ? Why would that happen in a Python app ? And more importantly : any ideas why this is happening and how to avoid it ? Notes : This runs as root ( it has to for the camera library unfortunately ) from upstartWhen the cameras are turned off the app restarts ( using respawn in upstart ) and this happens multiple times a day so it 's not due to being long running , I have also seen this happen very soon after the process startsIt is the same code being run over and over , it 's not due to running a different branch of my codeCurrently has a nice of -2 , I have tried removing the nice with no affect Ubuntu 12.04.5 LTSPython 2.7Machine has 128GB of memory which I am no where near using"
"I created a table with name and image of the candidate , I inserted the values in the table . My question is how can I retrieve the stored image from the database and view in webpage ? Is that possible to retrieve all the stored images ? Any help with this would be much grateful Thank you.Here is my pyfinal_pic.html : and here is my another HTML where I am trying to retrieve the selected image from the database using select candidate_name , candidate_image from votetable2retrieve.html"
"My question is related with the new Python 's type hints . I 'm trying to add a type hint in an object 's method who has a parameter of the same type of the object , but PyCharm are marking me as error ( unresolved reference 'Foo ' ) . The problem is as follows : So the question is how to define the type of other_foo parameter properly . Maybe __class__ is correct ?"
"I 'm using django-filer for the first time , and it looks great , and work pretty well.But all my images are being uploaded to the 'Unsorted Uploads ' folder , and I ca n't figure out a way to put them in a specific one . This strikes me as a fundamental feature , and given that it allows you create folders , this must be possible , right ? But I ca n't find it in the docs , and a quick look through the source did n't help me.I have a basic setup like : And I want GiftImage.image and Story.image to go into separate folders , so as to make sorting/searching easier for the admin user.I have triedAll of these either give me a `` TypeError : init ( ) got an unexpected keyword argument ... '' or just do n't do what I was hoping.Cheers !"
How can I get the list of class functions from within __getattr__ function ? Python v2.7 if it matters.Trying to use dir within __getattr__ leads to infinite recursion.Here 's the output I want : Any other solution which gets me the output I want will work fine . I want to do fuzzy string matching in the case when a function does n't exist to try to suggest what the user might have meant .
"Given the following lists : I 'd like to repeat each element of [ a ] by the number of its corresponding position in [ b ] to produce this : i.e . 0 occurs 1 time , 5 occurs 2 times , and 1 occurs 1 time ."
"I 'm having trouble pickling a Cython class , but only when it 's defined inside a package . This problem was noted previously online , but they did n't state how it was resolved . There are two components here : the Cython pickling using a __reduce__ method and a package error.Cython Pickling SuccessI 'll first show how it works without the package part . This example works correctly . Cython FileMy Cython file is reudce.pyx : Setup FileThis may be compiled with a setup.py : by executing python setup.py build & & cp build/lib*/reduce.so .Test ScriptThe test script is called test_reduce.py and is : Executing python test_reduce.py works fine.Cython Pickling in Package FailureHowever , once the reduce.pyx is put into a package , there is an error . Package CreationTo reproduce this , first create a package called bar . Test ScriptChange the test_reduce.py file to be : Error MessageRunning python test_reduce.py gives the following error : There is a catch of errors which are all turned into a PicklingError in pickle.py After looking at that code , the specific error which is occuring is : Sanity TestTo check that there is not some kind of scope or other issue , if I run the steps which the pickle module should execute , everything works : So what 's going on here ? !"
"So I 've got 2 data-frames I 'd like to merge together.I 'm merging on 3 columns , 2 is an easy join.I want this to be using a third column , but it 's going to be a comparison , something like this : Not sure what the right syntax is here.If it was SQL , it would be easy for me.Any assistance ?"
"I want to find two values of x that intersect a certain value of y on a x-y plot of a resonance curve . However , as I have few data points I will need to interpolate to find these values of x.The curve I am looking at can be seen below : How do I find the two values of x that equal a y value ( shown in red ) ? I have tried np.interpolate by splitting the data into two arrays : first with gradient ( y ) > 0 and another with gradient ( y ) < 0 , however this yielded incorrect values . However , this approach is far from elegant and I seek a simple solution . Cheers in advance for any help.Additional information : Code used thus far : Also , the data analysed in this program is found in this dpaste : http : //dpaste.com/13AMJ92/"
"I 'm still new to python . I 'm working the framework for a larger project . This program makes you think of either a circle or square , then it ask four questions , then decides on an answer.I 'm on the last step of the framework , but ran into a problem . I get `` global name 'qas1 ' is not defined '' Line 50 in getQuestion question = 'qas ' Global name 'qas ' is not definedThis happend when I tried to pickle my tuples.Here is my loading program to create the pickle file that contains my tuples : The code worked okay , but when I tried to use the file that it created I ran into problems.It is a lot of code to look at , I know . When the program first loads the name qas1 it reconizes that it is a tuple , but when I try to pass the properties to 'question ' in getQuestion ( ) : it forgets what they were . Any ideals what the problem is ?"
"I have a tuple of tuples : I need it in the following format : How can I convert it ? I can do something like : But that seems not elegant , I suppose there 's a more straightforward solution ."
"I need to extend this question , which sums values of an array based on indices from a second array . Let A be the result array , B be the index array , and C the array to be summed over . Then A [ i ] = sum over C such that index ( B ) == i.Instead , my setup isI need A [ i , j ] = sum_ { k in 0 ... N } C [ j , k ] such that C [ k ] == i , i.e . a rowsum conditional on the indices of B matching i . Is there an efficient way to do this ? For my application N is around 10,000 and M is around 20 . This operation is called for every iteration in a minimization problem ... my current looping method is terribly slow.Thanks !"
"I am trying to implement a radio player using RPi . The goal is to setup a playlist and start playing once playlist is populated . The player and radio processes are expected to exit once stop code is executed.The radio process terminates nicely but the player process still remains on wait even after calling terminate . If the stop code is called again then player process terminatesThings tried : reordering wait commands ( player , radio ) / ( radio , player ) similarly reordering terminate commands using kill instead of terminate hangs the RPiPlayer Code : Player Stop code ( called from another thread ) : Alternative : Another alternative was to have a persistent sink for radio and on demand process for playerBut in this case calling player.terminate ( ) closes the player while playing last packet repeatedly on the radio process ( like a stuck record ) . This stuck record plays till I start a new player or terminate the radio ."
"I have a numpy array A , which has shape ( 10 , ) .I also have , as of this moment , a numpy array B with shape ( 10,3,5 ) . I want to do a multiplication between these two to get C such that C [ 0 , : , : ] =A [ 0 ] *B [ 0 , : , : ] , C [ 1 ] =A [ 1 ] *B [ 1 , : , : ] , etc.I do not want to work this out with loops , one reason being the aesthetics of the thing , the other being that this code needs to be very generic . I want the user to be able to input pretty much any B of any shape as long as the leading dimension is 10 . For instance , I want the user to be able to also put in a B of shape ( 10,4 ) .So : How can I implement this multiplication using numpy ? Thanks.ADDENDUM : Have been asked for example . Will go smaller . Let 's say A is the numpy array [ 1,2,3 ] and B is the numpy array [ [ 1,2 ] , [ 4,5 ] , [ 7,8 ] ] . I want the multiplication of the two to result in [ [ 1,2 ] , [ 8,10 ] , [ 21,24 ] ] . ..."
"Say I have a set S defined as a string , e.g . as follows : where A , B and C are finite sets , e.g . : If we analyze S step by step , we have : which gives us the final result : How can I compute the elements of S given its definition as a general boolean formula ? I do n't quite know how to start addressing this problem . On one hand I wonder if I need to use a full lexical parser . Also , after some reading I also found two concepts that seem that highly related , but do n't know how they would apply : Canonical Normal Forms De Morgan 's laws"
I am trying to program a robot to move . The robot moves based on where currently is . There are four places where it can be : I am struggling to come up with a good pythonic way to code this . I am thinking on the lines of defining 4 different next move rules and then having a bunch of if statements that choose the correct rulesHas someone done something similar ... Is there a better way
"I 'm having a problem with Python generators while working with the Openstack Swift client library.The problem at hand is that I am trying to retrieve a large string of data from a specific url ( about 7MB ) , chunk the string into smaller bits , and send a generator class back , with each iteration holding a chunked bit of the string . in the test suite , this is just a string that 's sent to a monkeypatched class of the swift client for processing.The code in the monkeypatched class looks like this : And in the test suite : After returning the generator object , it was called by a stream function in the storage class : And finally , in my test suite : The output ends up with this : I tried isolating this with a simple python script that follows the same flow as the code above , which passed without issues : Any help with this issue is greatly appreciated : )"
"If I do : I would expect the output the be the same , but it 's not : Why is this ?"
I have a problem with the implementation of a decorator applied at this metaclass decorator I wrote : This is the class in which I have used the metaclass : Everything seems to work great when I pass to the decorator metaclass a decorator implemented like this : But when I use a decorator implemented in this way : I got this error : Why ? Using the two decorators implementation with others function does n't show me problems . I think that the problem is tied to the fact that the methods to which I 'm trying to decorate are class methods . Am I missing something ?
"I have the following models : In my views.py , When I want to show the publisher page , I also want to show their books , so I usually do something like this : Then , after some processing I also do some work with the booksThis works great , but I have one problem . If there is a book added between the query and the for loop , the publisher.book_set.all ( ) wo n't have the newly added books because it was prefetched.Is there a way to update the publisher object ?"
I have mapreduce job defined in mapreduce.yaml : How to start it from cron ? Is there some url that can run it ?
"I 've got a python script that takes input on stdin . I 'd like to drop into IPython.embed ( ) , like this : I then invoke the script like this : The problem is that IPython uses stdin for the interactive console , so the first thing it sees are the remaining pipe data . Then , the pipe closes and the the terminal exits.Is there a way to debug a script that uses stdin with ipython ?"
"I 'm trying to build a function which I can use as a handler for an RxPy stream that I 'm mapping over . The function I have needs access to a variable outside the scope where that variable is defined which , to me , means that I need to use a closure of some kind . So I reached for functools.partial to close over the one variable and return a partial function that I can pass to as an observer to my stream.However , doing so results in the following : Here is some sample code which reproduces the problem : The problem seems to be that my partial function returns False when calling inspect.isfunction.How can I make my partial function pass this check ? Is there a way to easily convert a partial function into a `` real '' function type ?"
"I am trying to get the string lengths for different columns . Seems quite straightforward with : But I need to apply it to multiple columns . And then get the minimum on it . Something like : I know the above does n't work , but hopefully you get the idea . Column a , b , c all contain names and I want to retrieve the shortest name . Also because of huge data , I am avoiding creating other columns to save on size ."
"I 'm currently working on some code that shares some state between processes using a mmap object . The use case is a bunch of processes that have access to some read-only data in a shared mmap backed array . Part of this involves doing some arithmetic on the underlying memory representation , I 'm using ctypes much like in this question to get a the underlying memory address.I have a situation where want to be able to open this mmap file as read only by the processes that only read data . However when I do that I 'm not sure how to get the pointer address in that case . The following is as close to a minimal example of this problem as I can make : Running on Python3 this fails with this : While Python2 gives this : Given that I actually want to use a read only memory map how can I do this ? I 'll change to a writeable mmap if I have to , but I would rather not if there 's another way to do this , so any suggestions or workarounds would be appreciated ."
"How do you make an abstract SearchIndex class , similar to how Django lets you make abstract base models ? I have several SearchIndexes that I 'd like to give the same basic fields ( object_id , timestamp , importance , etc ) . Currently , I 'm duplicating all this code , so I 'm trying to create a `` BaseIndex '' and simply have all the real index classes inherit from this.I 'm tried : but this gives me the error : so I then tried : but these gives me error : How do I inherit from a custom SearchIndex subclass ?"
I would like to filter a numpy array ( or pandas DataFrame ) in a way that only continuous series of the same value with at least window_size length is kept and everything else set to 0.For example : should become when using a window size of 4I 've tried using rolling_apply and scipy.ndimage.filtes.gerneric_filter but due to the nature of rolling kernel functions I do n't think it is the right approach here ( and I am stuck with it at the moment ) .I insert my attempt here anyway :
"I want to quickly bzip2 compress several hundred gigabytes of datausing my 8 core , 16 GB ram workstation.Currently I am using a simple python script to compress a wholedirectory tree using bzip2 and an os.system call coupled to an os.walkcall.I see that the bzip2 only uses a single cpu while the other cpusremain relatively idle.I am a newbie in queue and threaded processes . But I am wondering howI can implement this such that I can have four bzip2 running threads ( actually I guess os.system threads ) , each using probably their owncpu , that deplete files from a queue as they bzip them.My single thread script is pasted here ."
"I am using a for loop to iterate through a config file and pass variables within to a unit test.The problem with this method is the failures can be similar and are printed out at the end of testing . So there is no way to tell which variable it was that failed , only that n variables failed the test . Is there a way to print a custom string upon failure which would include the failed variable ? Or perhaps there is even a better way to test the variables in my config file ?"
"How can I wrap a recursive function , recursive calls included ? For example , given foo and wrap : wrap ( foo ) ( x ) will only output `` f was called '' with the first call . Recursive calls still address foo ( ) .I do n't mind monkey patching , or poking around internals . I 'm not planning to add this code to the next nuclear warhead handling program , so even if it 's a bad idea , I 'd like to achieve the effect.Edit : for example , would patching foo.func_globals to override foo.__name__ work ? If it always does , any side-effects I should be minding ?"
"Suppose I have a project with a folder structure like so.The module helpers.py defines some exception and contains some method that raises that exception.On the other hand my main.py module defines its own exceptions and imports methods that may raise an exception from helpers.py.Now , I do not want users to have to do from project.__helpers.helpers import HelperException if they want to catch that exception . It would make more sense to be able to import the exception from the public module that is raising it.But I can not just move HelperException to main.py , which would create a circular import.What would be the best way to allow users to import all exceptions from main.py while those being raised in /__helpers ?"
"I am working on an algorithm . But I am not very clear on the haskell code provide by the author , so I need you guys ' help.The codes can split into two parts , I think.Here , very clearly , a type called LFT ( may be a tuple in Python ) and three function called extr unit comp be defined.However , the next part puzzled me a lot : I believe lfts is a generator but I am failed to understand how the loop performed in this code and I do not know much about the Haskell . Can you help me convert this one to Python or a pseudocode ?"
"With python 3.5.1. and a current installation of mypy using git , mypy flags error 1 & 2 , but it does not report 3What am I doing wrong , or is this a bug , or is this a known problem ?"
"I want to write a decorator that acts differently depending on whether it is applied to a function or to a method.I tried inspect.ismethod ( ) , inspect.ismethoddescriptor ( ) and inspect.isfunction ( ) but no luck . The problem is that a method actually is neither a bound nor an unbound method but an ordinary function as long as it is accessed from within the class body.What I really want to do is to delay the actions of the decorator to the point the class is actually instantiated because I need the methods to be callable in their instance scope . For this , I want to mark methods with an attribute and later search for these attributes when the .__new__ ( ) method of MyClass is called . The classes for which this decorator should work are required to inherit from a class that is under my control . You can use that fact for your solution.In the case of a normal function the delay is not necessary and the decorator should take action immediately . That is why I wand to differentiate these two cases ."
"I have a large numpy array : and a large-ish array of unique values in a particular order : How can I quickly ( no python dictionaries , no copies of A , no python loops ) replace the values in A so that become the indicies of the values in B ? : I feel reaaly dumb for not being able to do this off the top of my head , nor find it in the documentation . Easy points !"
"Edit : Really appreciate help in finding bug - but since it might prove hard to find/reproduce , any general debug help would be greatly appreciated too ! Help me help myself ! = ) Edit 2 : Narrowing it down , commenting out code.Edit 3 : Seems lxml might not be the culprit , thanks ! The full script is here . I need to go over it looking for references . What do they look like ? Edit 4 : Actually , the scripts stops ( goes 100 % ) in this , the parse_og part of it . So edit 3 is false - it must be lxml somehow.Edit 5 MAJOR EDIT : As suggested by David Robinson and TankorSmash below , I 've found a type of data content that will send lxml.etree.HTML ( data ) in a wild loop . ( I carelessly disregarded it , but find my sins redeemed as I 've paid a price to the tune of an extra two days of debug ! ; ) A working crashing script is here . ( Also opened a new question . ) Edit 6 : Turns out this is a bug with lxml version 2.7.8 and below ( at least ) . Updated to lxml 2.9.0 , and bug is gone . Thanks also to the fine folks over at this follow-up question.I do n't know how to debug this weird problem I 'm having.The below code runs fine for about five minutes , when the RAM is suddenly completely filled up ( from 200MB to 1700MB during the 100 % period - then when memory is full , it goes into blue wait state ) .It 's due to the code below , specifically the first two lines . That 's for sure . But what is going on ? What could possibly explain this behaviour ?"
I have two different numpy arrays given . First one is two-dimensional array which looks like ( first ten points ) : the second array is just one-dimensional which looks like ( first ten points ) : Values from the second ( one-dimension ) array could occur in first ( two-dimension ) one in the first column . F.e . 17.64705882I want to get an array from the two-dimension one where values of the first column match values in the second ( one-dimension ) array . How to do that ?
"How can one detect if an axis has a twin axis written on top of it ? For example , if given ax below , how can I discover that ax2 exists ?"
"I have a celery task on Heroku that connects to an external API and retrieves some data , stores in the database and repeats several hundred times . Very quickly ( after ~10 loops ) Heroku starts warning about high memory usage . Any ideas ? tasks.pymodels.pyHeroky logs"
"Using SymPy , I can create a contour plot manually using the following code ( there is n't yet a built-in contour plotting function Update : SymPy now has a contour plotting function ) : Currently , when SymPy calls contour ( ) , it does not appear to be saving the returned ContourSet ( Update : I have filed a issue to see if the ContourSet can be saved ) : In other examples where modifications are performed to the plot , such as adding inline labels using clabel ( ) , the ContourSet ( CS ) is needed : Going back to the SymPy example , my_plot._backend does provide access to the figure and axes ; what workarounds are possible to keep or obtain access to the ContourSet ?"
"I 'm a Python ( 3.1.2 ) /emacs ( 23.2 ) newbie teaching myself tkinter using the pythonware tutorial found here . Relevant code is pasted below the question.Question : when I click the Hello button ( which should call the say_hi function ) why does the inferior python shell ( i.e . the one I kicked off with C-c C-c ) wait to execute the say_hi print function until I either a ) click the Quit button or b ) close the root widget down ? When I try the same in IDLE , each click of the Hello button produces an immediate print in the IDLE python shell , even before I click Quit or close the root widget.Is there some quirk in the way emacs runs the Python shell ( vs . IDLE ) that causes this `` lagged '' behavior ? I 've noticed similar emacs lags vs . IDLE as I 've worked through Project Euler problems , but this is the clearest example I 've seen yet.FYI : I use python.el and have a relatively clean init.el ... ( setq python-python-command `` d : /bin/python31/python '' ) is the only line in my init.el.Thanks , Mike=== Begin Code==="
"In this problem , I 'm dealing with IPv6 network address spaces , so the length is 2^ ( 128-subnet ) .It appears that python ( at least on this machine ) , will cope with up to a 64 bit signed number as the return value from __len__ ( ) . So len ( IP ( '2001 : :/66 ' ) ) works , but len ( IP ( '2001 : :/65 ' ) ) fails.The IPy library in question is at https : //github.com/haypo/python-ipy.Any suggestions on how to handle this , or hint that it might be a limitation I 'm stuck with ?"
"My goal is to write a program in Haskell that takes the name of a json file and interprets the rest of the arguments as a path to navigate that json file by and print the value navigated to . The problem is because JSON can contain multiple value types , I do n't know how to make Haskell 's type system understand what I want . Here is the Haskell code with the `` navigate '' function I 'm not able to implement correctly : For reference , here is how the same program would have been written in Python : The program would be run like this :"
"I have a simple app , with two functions , one for listening to topic and other for web endpoint . I want to create server side event streaming ( SSE ) i.e text/event-stream , so that on client end I could listen to it using EventSource . I have the following code for now , where each function is doing its particular job : Now , I want in the index , something like this code , but using faust : I have tried this : But it gives me the following error : I event tried this : But I get following error : Could there be a way for this ? Thanks a lot in advance !"
"I need to implement an SVN pre-commit hook which executes a script that itself is stored in SVN.I can use the svn cat command to pipe that script to the Python interpreter , as follows : However , my_script.py itself requires data to be piped on STDIN.That data is not stored in a file ; it is stored on the network . I would prefer not to have to download the data to a temporary file , as normally I could pipe it to a Python program : I 'm not sure how to combine both of these pipes ."
"I have several files , say , a , b , c , I would like to something likebut with `` a , '' in the beginning lines of a . `` b , '' in the beginning of the lines of b and `` c , '' in the beginning of the lines of c.I can do this using python : but I would prefer to do it in the command line , combining some simple commands with the pipe | operator.Is there a easy way of accomplishing this ? Thanks ."
"I 'm using App Engine python to host an application and would love to use Appstats and a couple of other libraries that run as middleware . However , when I set up middleware through appengine_config.py ( as shown below ) it works on the dev server but not in production . Appstats AND gaesessions work like a charm in the dev server and do n't work at all in production . Here is my appengine_config.py , located in my root /src dir : Any ideas ? UPDATESo I 'm bringing this back up as I 've tried again to fix it to no avail . I 've boiled appengine_config.py down to : and app.yaml includesMy app uses basic webapp , bottom of every request-handling file includes : Deploying works fine . App has been going strong for over a year and sees lots of requests . myapp.appspot.com/_ah/stats comes up showing the GUI with a refresh button , no data , and the message `` No requests have been recorded yet '' etc . I 'm confused !"
"I find Hadley 's plyr package for R extremely helpful , its a great DSL for transforming data . The problem that is solves is so common , that I face it other use cases , when not manipulating data in R , but in other programming languages.Does anyone know if there exists an a module that does a similar thing for python ? Something like : It should n't be too difficult to implement , but would be great if it already existed . I 'd implement it , I 'd use itertools.groupby to group by cols , then apply the op function , then use itertools.chain to chain it all up . Is there a better solution ?"
"I have a number of Python generators , which I want to combine into a new generator . I can easily do this by a hand-written generator using a bunch of yield statements.On the other hand , the itertools module is made for things like this and to me it seems as if the pythonic way to create the generator I need is to plug together various iterators of that itertools module.However , in the problem at hand , it soon gets quite complicated ( the generator needs to maintain a sort of state -- - e.g . whether the first or later items are being processed -- - , the i-th output further depends on conditions on the i-th input items and the various input lists have to be processed differently before they are being joined to the generated list.As the composition of standard iterators that would solve my problem is -- - due to the one-dimensional nature of writing down source code -- - nearly incomprehensible , I wonder whether there are any advantages of using standard itertools generators versus hand-written generator functions ( in basic and in more advanced cases ) . Actually , I think that in 90 % of the cases , the hand-written versions are much easier to read -- - probably due to their more imperative style compared to the functional style of chaining iterators.EDITIn order to illustrate my problem , here is a ( toy ) example : Let a and b be two iterables of the same length ( the input data ) . The items of a consist of integers , the items of b are iterables themselves , whose individual items are strings . The output should correspond to the output of the following generator function : If I write down the same program in functional style using generator expressions and theitertools module , I end up with something like : EXAMPLEThis is possibly more elegant than my first solution but it looks like a write-once-do-not-understand-later piece of code . I am wondering whether this way of writing my generator has enough advantages that one should do so.P.S . : I guess part of my problem with the functional solution is that in order to minimize the amount of keywords in Python , some keywords like `` for '' , `` if '' and `` else '' have been recycled for use in expressions so that their placement in the expression takes getting used to ( the ordering in the generator expression z for x in a for y in x for z in y looks , at least to me , less natural than the ordering in the classic for loop : for x in a : for y in x : for z in y : yield z ) ."
"I 'm having trouble with filtering all but the last 1 element in each group of groupby object of pandas.DataFrame : As expected ( according to documentation ) g.head ( 1 ) returnswhereas g.head ( -1 ) returns empty DataFrameFrom the behavior of x.head ( -1 ) I 'd expect it to returni.e . dropping the last element of each group and then merging it back into the dataframe . If that 's just the bug in pandas , I 'd be grateful to anyone who suggests an alternative approach ."
"I would like to set up a maximum limit for an index within a Column definition or just through the Index constructor but I do n't seem to find a way to achieve it.Basically , I would like to simulate this MySQL behaviour : In SQLAlchemy I would have something like : but I ca n't find anything that would suggest the limit of the index can be customised . Something like : I guess since this option for the Column constructor is just an alias for the Index constructor , is there a custom param to include in the Index constructor to allow this setting ? Thanks !"
"I have a script which uses no randomisation that gives me different answers when I run it . I expect the answer to be the same , every time I run the script . The problem appears to only happen for certain ( ill-conditioned ) input data . The snippet comes from an algorithm to compute a specific type of controller for a linear system , and it mostly consists of doing linear algebra ( matrix inversions , Riccati equation , eigenvalues ) . Obviously , this is a major worry for me , as I now can not trust my code to give me the right results . I know the result can be wrong for poorly conditioned data , but I expect consistently wrong . Why is the answer not always the same on my Windows machine ? Why do the Linux & Windows machine not give the same results ? I 'm using Python 2.7.9 ( default , Dec 10 2014 , 12:24:55 ) [ MSC v.1500 32 bit ( Intel ) ] on win 32 , with Numpy version 1.8.2 and Scipy 0.14.0 . ( Windows 8 , 64bit ) .The code is below . I 've also tried running the code on two Linux machines , and there the script always gives the same answer ( but the machines gave differing answers ) . One was running Python 2.7.8 , with Numpy 1.8.2 and Scipy 0.14.0 . The second was running Python 2.7.3 with Numpy 1.6.1 and Scipy 0.12.0.I solve the Riccati equation three times , and then print the answers . I expect the same answer every time , instead I get the sequence ' 1.75305103767e-09 ; 3.25501787302e-07 ; 3.25501787302e-07'.My numpy config is as below print np.show_config ( ) ( edits to trim the question down )"
"I 've been working with Python for a while and I find the syntax for declaring methods as static to be peculiar.A regular method would be declared : A static method is declared : If you do n't add the static method line , the compiler complains about self missing.This is a very complex way of doing something very simple that in other languages simply use a keyword and a declaration grammar to . Can anyone tell me about the evolution of this syntax ? Is this merely because classes were added into the existing language ? Since I can move the staticmethod line to later in the class , it also suggests that the parser is working extra hard on bookkeeping.Note that I 'm aware of the decorator syntax that was added later , I 'm interested to know how the original syntax came about from a language design perspective . The only think I can think of is that the staticmethod application invokes an operation that transforms the function object into a static method ."
"I want to be able to accept all changes from a MS Word ( .docx ) document from Python , preferably using python-docx module.I know how to do in Perl ( see below for reference ) but would like to have native code in my Python program to do the same . IdeAny idea ?"
"I need to generate a sparse random matrix in Python with all values in the range [ -1,1 ] with uniform distribution . What is the most efficient way to do this ? I have a basic sparse random matrix : And this gives me values in [ 0,1 ] : It would be good to have an in-place solution or something that does n't require blowing it up to a full matrix since in practice I will be using very large dimensions . It surprises me there are not some quick parameters to set for sparse.rand itself ."
"I imported a CSV using Pandas and one column was read in with string entries . Examining the entries for this Series ( column ) , I see that they should actually be lists . For example : I would like to extract the list elements from the strings . So far , I 've tried the following chain : ( all on one line , of course ) .and this gives me back my desired list elements in one column . [ ' '' entry11 '' ' ] [ ' '' entry21 '' , `` entry22 '' ' ] [ ' '' entry31 '' , `` entry32 '' ' ] My question : Is there a more efficient way of doing this ? This seems like a lot of strain for something that should be a little easier ."
"Suppose I have a number of lists of pairs ( int , str ) , not necessarily of the same length . The only constraint here is that the lists are sorted in ascending order by their integer parts : What I would like to do is to emit the string elements in the order in which their corresponding integer elements occur i.e . in this case : I am wondering if there is an obvious ( nice + pythonic ) way to do this just using iterators of a , b and c ? I 've looked at itertools but ca n't immediately see how to use the functionality in this case . The lists a , b , c might be very large so I 'd like to do this without reading them into memory and then sorting ..."
"AllWindows 10 , 64bit , d/l Anaconda 2.5.0 with Python3 , 64bit and installed itAfter fresh installation i type conda list , and , among packages , I seeduplicates like Is it normal , and why some packages ( not all of them , just a few ) haveduplicates ( not quite , there is - vs _ ) both in conda and pip ? What will happen if I do pip uninstall jupyter-core ? What should be policy toward such packages ?"
"I have a list of signals ( representing consecutive mesures ) : I consider a signal valid only if equal to the previous n mesures.Eg . if we consider only 2 mesures for validation ( n=2 ) , the first time signals turns from 0 to 1 we consider it still 0 but the next mesure , if it 's 1 again then we consider it is still valid and make it 1 . Then we would need 2 mesures of 0 to turn it to 0 again , etc ... Here signals are 0 and 1 for simplification but in my application they can be other integers.Desired output : I was looking of a pythonesque one-liner way of doing it but ca n't seem to find the desired output . I tried something along the lines : Edit : I 'm open to numpy if it makes it easier as it 's already imported ."
"I need to generate a list for scipy.optimize.minimize 's boundry condition , it should look like this : I 'm wondering if there is any elegant way of doing it ? I tried : But this generatesHow can I remove the inner [ ] , to unravel the inner arrays into a single one ? Or is there any other good way of doing it ?"
"I have a large ( millions ) array of ID numbers ids , and I want to find the indices where another array of targets ( targets ) exist in the ids array . For example , ifthen I want the result : If I pre-sort the array of ids , then it 's easy to find matches using numpy.searchsorted , e.g.But how can I find the reverse mapping to 'unsort ' this result ? I.e . to map the sorted entries at [ 2,0 ] back to where they were before : [ 1,3 ] ."
"When I use get_current_user ( ) I need to check few things in Redis ( use tornado-redis ) asynchronously.I am doing the following : For example , I want to use authenticated_async decorator : But I have in console only 123.Whats wrong ? How to fix that ? Thanks ! UPDATEI have updated the code with yield result . In auth_cookie I have cfae7a25-2b8b-46a6-b5c4-0083a114c40e.Then I go to terminal : So , Must return But it returns one 123UPDATE 1WithIn console I have :"
"Say I have a sorted numpy array : and suppose I make a non trivial operation on it such that I have a new array which is the same as the old one but in another order : The question is : how do you get the indices of where each element of arr2 are placed in arr . In other terms , I want a method that takes both arrays and return an array the same length as arr2 but with the index of the element of arr . For example , the first element of the returned array would be the index of the first element of arr2 in arr.Does a function like this already exists in numpy ? EDIT : I tried : which returns what I want , but my question still holds : is there a more efficient way of doing this using numpy methods ? EDIT2 : It should also work if the length of arr2 is not the same as the length of the original array ( like if I removed some elements from it ) . Thus it is not finding and inverting a permutation but rather finding where elements are located at ."
"I 've currently constructed a plot using rectangle Patches to display a sequence of positions.EDIT : Code used to generate this ( built off of the RLPy library ) - Is there any way to overlay each of these patches with images ? Ideally , there would be a car image instead of a rectangle at each of the positions . I 've played around with AnnotationBbox and TransformedBbox , but both seem to be inflexible when dealing with rotations ."
Been trying to figure out how to get py2exe to handle errors more gracefully . There are basically 2 weird things happening:1 ) Popup message after shutting down the program = > want to suppress ( not show up ) this popup Use try/except = > does n't workhttp : //osdir.com/ml/python.py2exe/2006-09/msg00016.htmlNot sure where to put this code2 ) Log file getting created in c : \Program Files\AppName\AppName.exe.log ( sometimes has permission errors writing to this folder ) = > redirect log to c : \ProgramDataUse sys.stdout and sys.stderr = > does n't workhttp : //www.dreamincode.net/forums/topic/234318-py2exe-do-not-show-errors-occurred-prompt-to-user/Not sure where to put this codeI 'm thinking that I may just be putting the code in the wrong spot and the py2exe bootstrap code is firing AFTER I 've set these up but I 'm not sure . I 've tried putting this code right before the error log is generate but it still goes to where py2exe is bootstrapping them to ( the StdErr objects ) The structure of my program is as followsmain.py
"I am using Pandas 0.8.1 , and at the moment I ca n't change the version . If a newer version will help the problem below , please note it in a comment rather than an answer . Also , this is for a research replication project , so even though re-running a regression after appending only one new data point might be silly ( if the data set is large ) , I still have to do it . Thanks ! In Pandas , there is a rolling option for the window_type argument to pandas.ols but it seems implicit that this requires some choice of a window size or use of the whole data sample as default . I 'm looking to instead use all the data in a cumulative fashion.I am trying to run a regression on a pandas.DataFrame that is sorted by date . For each index i , I want to run a regression using the data available from the minimum date up through the date at index i . So the window effectively grows by one on every iteration , all data is cumulatively used from the earliest observation , and no data is ever dropped out of the window.I have written a function ( below ) that works with apply to perform this , but it is unacceptably slow . Instead , is there a way to use pandas.ols to directly perform this sort of cumulative regression ? Here are some more specifics about my data . I have a pandas.DataFrame containing a column of identifier , a column of dates , a column of left-hand-side values , and a column of right-hand-side values . I want to use groupby to group based on the identifier , and then perform a cumulative regression for every time period consisting of the left-hand and right-hand-side variables.Here is the function I am able to use with apply on the identifier-grouped object :"
"I need to parse expressions based on following rules : An expression can contain a filter object represented as name : valueAn expression can contain a string expressionAn expression can contain Booleans OR , ANDEverything inside can be quotedSo a typical expression looks likefilter1:45 hello world filter:5454filter1:45 'hello world ' filter:5454hello world'hello world ' OR filter:43Here 's what I 've tried so far : With this grammar , I can parse strings likefilter2:32 filter1:3243From what I understood I can provide csl function with a list of objects , and the grammar needs to be in that order . However what if I want to parse an object likefilter34:43 hello filter32:3232ORfilter34:43 OR filter32:3232How can I say that there are multiple types of objects ( filters , expressions , booleans ) in an expression ? Is that possible with peg ?"
"I am a bit new to Python , but an experienced programmer . I am writing a Python 2.7 script which should be started by a Linux server at boot . The purpose is to send , via various mediums , notification ( s ) when a user connects to the server.My problem has to do with the actual listening . How can I make the module see when a user connects ( via SSH , or whatever ) to the server ? Some quick pseudocode : I 'd like to include some details in the notification , like username , IP , connection time , last connection , a cute custom connection message , whatever . Any protips on how to do those things best is appreciated as well , but I 'm sure I can figure this out elsewhere.Thanks in advance for any guidance !"
"Most of the Numpy 's function will enable multithreading by default.for example , I work on a 8-cores intel cpu workstation , if I run a scriptthe linux top will show 800 % cpu usage during running like Which means numpy automatically detects that my workstation has 8 cores , and np.sqrt automatically use all 8 cores to accelerate computation.However , I found a weird bug . If I run a scriptthe cpu usage is 100 % ! ! . It means that if you plus two pandas DataFrame before running any numpy function , the auto multithreading feature of numpy is gone without any warning ! This is absolutely not reasonable , why would Pandas dataFrame calculation affect Numpy threading setting ? Is it a bug ? How to work around this ? PS : I dig further using Linux perf tool.running first script showsWhile running second script showsSo both script involves libmkl_vml_avx2.so , while the first script involves additional libiomp5.so which seems to be related to openMP.And since vml means intel vector math library , so according to vml doc I guess at least below functions are all automatically multithreaded"
"I 'm getting an error when I query MySQL on my Django app hosted on elastic beanstalk . The error says : OperationalError at /admin/login ( 1045 , `` Access denied for user 'adminDB ' @ '172.30.23.5 ' ( using password : YES ) '' ) Here is my .config file : Here is my the databases section on settings.py : I created the eb environment through the eb create command and then proceeded to create an RDS database on the eb console on their website . Is there anything else I need to do to connect Django to MySQL ? Something to do with security groups or something ? Thanks !"
"I want to be able to visualize my data points per days of the week , per weeks in a year and per months . I was able to visualize my data per year . But when I adjust the code for Monthly and weekly , the x-axis remains as per year . I have 8 years of hospital records . My data is organized into 2 columns . Column number 1 is my `` dates '' column starting from 2010-03-10 and ending at 2017-12-31 . Column number 2 is my value column . This column lists if I 've had a patient come in for treatment or not . The values in column 2 are 0 or x numbers . For example 0 meaning I 've had no patients x meaning I 've had x number of patients . When I try to graphically represent this data , it only counts the largest x number I 've had per week.Graph resultSo I get the correct graphical figure . But when I change the ( df.resample ( ' Y ' ) .sum ( ) to ( 'M ' ) from ( ' Y ' ) for monthly I get a graph that displays a yearly X-axis and values . How can I change this to get monthly X-axis and Weekly X-axis ?"
"Edit : I 'm looking for solution for this question now also with other programming languages.Based on the other question I asked , I have a dataset like this ( for R users , dput for this below ) which represents user computer sessions : There may be several concurrent ( overlapping based on time ) sessions for the same username from the the same computer . How can I remove those rows so that only one sessionsis left for this data ? Original data set has approx . 500 000 rows.The expected output is ( rows 2 , 15 removed ) Here is the dataset :"
"Why does the literal evaluation of 5 * 7 fail , while 5 + 7 does n't ? The documentation does n't explain this.I found that problem after answering this question on SO : Getting the result of a string ."
"I 'm learning Python and I noticed something strange with one of my scripts . Doing a little testing I discovered the problem stemmed from this behavior : Checking the documentation , this is , in fact , the design of the function : os.path.join ( path1 [ , path2 [ , ... ] ] ) Join one or more path components intelligently . If any component is an absolute path , all previous components ( on Windows , including the previous drive letter , if there was one ) are thrown away , and joining continues . ... My question is n't why my script failed , but rather why the function was designed this way . I mean , on Unix at least , a//b is a perfectly acceptable way to designate a path , if not elegant . Why was the function designed this way ? Is there any way to tell if one or more path elements have been discarded short of testing each path string with os.path.isabs ( ) ? Out of curiosity , I also checked the case where a path component ends in an os.sep character : That works as expected ."
"I have a 2D array t in numpy : It is indexed by two numbers : row and column index.What I want to do is to divide the array into rectangular cells of fixed size , 3×3 for example . I 'd like to avoid memory copying . The way I try to achieve this is creating a view onto t with correspondent shape and strides ( ( 3,3,3,3 ) and ( 216,24,72,8 ) respectively ) . This way the first two indexes of the view would mean the position of 3×3 cell in the larger grid and the last two would mean the position of element inside the cell . For example , t [ 0,1 , : , : ] would returnSo my question is — how to create the described view ? Am I missing a simpler method ? Can this be done elegantly with slicing syntax ?"
"gcloud app deploy keeps uploading all files in source directory , although I have explicitly excluded them using a .gcloudignore file . For example , the virtual environment folder env is uploaded , which is causing an error because the deployment contains more then 10,000 files then.I am working under Windows 10 with Python 3.7 and gcloud SDK version 251.0.0 . I tried both the beta and the normal version of gcloud app deploy.The .gcloudignore file contains just the following : I can see in the outputs with -- verbosity=info flag that it recognized the .gcloudignore file , but then it uploads the env folder to Cloud Storage . I would expect this folder to be skipped . Git works as expected ."
"So I have an array of the following form : Now I want to extract the first and the last element of each inside list into separate lists . So if I do : This works as I expect it to . However I was trying to merge these two into one call , something like : This however fails with an : ValueError : too many values to unpackSo could anyone explain why this is happening and if what I am trying to do is even possible.Regards , Bogdan"
"Ok , so this piece of code is from a practice question at my school . We are to mentally parse the code and check the answer.When I first parsed it , I got 4 . I copied the code and ran it through IDLE and got 8 . I ran the debugger and saw that the else : return is looping the if else statement until x == 0 and then it returns 1.I do not understand how return 1 is coming out to 8.I understand that it is calling foo ( x-1 ) inside the function foo ( x=5 ) which makes it check if else again and again until x == 0 then it returns 1 . How does return 1 end up printing 8 ?"
"I have installed ipython in my virtualenv , so python manage.py shell gives my ipython . However , I ca n't access imports from inside function definitions : This works fine if I start ipython directly . Why does n't it work from the Django shell ?"
"I am trying to install pyamg in my virtual environment . However , I am getting the following error . I am using mac OS ."
"Possible Duplicate : Python equivalent to perl -pe ? Is there a way to process each line of stdin with a given Python command without setting up things with boilerplate code ? With Perl , I can just do something like : can I do the same with Python ? Note : something similar is possible with many other tools , e.g . sed , awk , etc ..."
"I 'm trying to parse a little pseudo-code I 'm writing and having some trouble getting values for symbols . It parses successfully , but it wo n't return a value the same as it would with `` regular '' characters . Here 's an example : Why would n't there be a value ? Is there another way to get the exact operator that was used ?"
"I am currently using python trying to split a datetime column into 2 , one for Date and one for time and also have the column properly formatted.ORIGINAL DATASET*I have used 2 codes , one to format the column and the other that splits it . However , after formatting the column , missing time values were giving 00:00:00 value , here indicating a time for 12 midnight.See belowAFTER FORMATTINGCodes used : Is there away to do this without having the missing time value set at 00:00:00 ? Is it possible to have these missing values recorded as Nan while formatting the datetime ? Any thoughts on how I can achieve a formatted datetime showing the missing time values as NaN.WHAT I WOULD LIKE IT TO LOOK LIKE Hoping that there is a way to get this done ."
"I am trying to understand asyncio and port my undestanding of threading . I will take the example of two threads running indefinitely and a non-threaded loop ( all of them outputting to the console ) .The threading version is I now tried to port this to asyncio based on the documentation.Problem 1 : I do not understand how to add the non-threaded task as all examples I saw show an ongoing loop at the end of the program which governs the asyncio threads.I then wanted to have at least the two first threads ( a and b ) running in parallel ( and , worst case , add the third c as a thread as well , abandonning the idea of mixed thread and non-threded operations ) : Problem 2 : The output is a sequence of a , suggering that the b ( ) coroutine is not called at all . Is n't await supposed to start a ( ) and come back to the execution ( and then start b ( ) ) ?"
"In Python 3.6 , you can use f-strings like : I want to overload the method receiving the ' % A ' above . Can it be done ? For example , if I wanted to write a dumb wrapper around datetime , I might expect this overloading to look something like :"
"I 'm trying to use query on a MultiIndex column . It works on a MultiIndex row , but not the column . Is there a reason for this ? The documentation shows examples like the first one below , but it does n't indicate that it wo n't work for a MultiIndex column.I know there are other ways to do this , but I 'm specifically trying to do it with the query function"
"In PyCharm , when I try to plot something using its interactive console , such as : It opens a window and crashes . I have to stop the console and start a new one . It works fine when I run anything like that in an ipython console in my terminal , the error happens only in Pycharm , it seems.On the other hand , if import matplotlib with import matplotlib.pyplot as plt it works fine : But if I do both , it crashes too ( even calling the plot function using plt.plot ) : Furthermore , when I run it all in one command , it works the first time . But if I try to plot another time , it crashes : So it is something related with using the matplotlib library with the import using * and with running in the interactive console after the first time it was imported . I know the wildcard import is not recommended , but sometimes it is useful to do it for a sake of testing things faster and being less verbose . Looking for this warning online , I have only found thesehttps : //github.com/matplotlib/matplotlib/issues/13296But my case does n't seem to be related to multiprocessing . And even if pycharm is doing something behind the scenes , I wonder why it has changed , as I had no problems with this like a month ago ; Suppress warning `` QApplication was not created in main ( ) thread '' and other posts related to C++ , which is not my case ; WARNING : QApplication was not created in main ( ) thread - > related to pycharm , but has an additional error different than mineWhich did n't help much . Anyone knows what is happening and how to solve it ? SPECS : PyCharm 2019.1.2 ( Professional Edition ) Build # PY-191.7141.48 , built on May 7 , 2019JRE : 11.0.2+9-b159.56 amd64JVM : OpenJDK 64-Bit Server VM by JetBrains s.r.oLinux 4.15.0-50-genericconda 4.6.14 , with Python 3.7.3Qt5"
"I was browsing through some code , and I noticed a line that caught my attention . The code is similar to the example belowThis looks like any other class that I have seen , however a str is being passed in as default value for self . If I print out self , it behaves as normalThis has been bugging me and I can not figure out why this would be used . Is there any reason to why a str instance would be passed in as a default value for self ?"
"I have a Matrix say 3 x 3 , I want to display it in the Generated sphinx documentation of Python docs.Currently the above matrix is not printed as it is in the generated sphinx docs ."
"I have an emergent bug that I 've got to track down tomorrow . I know a previous hg revision which was good so I 'm thinking about using hg bisect.However , I 'm on Windows and do n't want to get into DOS scripting.Ideally , I 'd be able to write a Python unit test and have hg bisect use that . This is my first attempt.bisector.pyPerhaps I could then run : Is there a better way of doing it ? Thanks for any advice ."
"I want reproducible results for the CNNs I train . Hence I set the seed in my script : The docs of set_random_seed and np.random.seed do not report any special behaviour for a seed of 0.When I run the same script twice on the same machine within a couple of minutes and without making updates , I expected to get the same results . However , this is not the case : Run 1 : Run 2 : How can I make the network produce reproducible results ? System"
"I 've recently begun packaging my first project with SetupTools , and have mostly been successful.Unfortunately , I 've run into a confusing situation - my project depends on a single-file module which is n't available on PyPI . I 've been able to configure setup.py to depend on that module easily , using the dependency_links option , and everything works ... so long as I 'm using setup.py to install it . If I try to use pip to install the project egg , it fails while trying to install the module , assuming that it must be a pre-made egg archive . In comparison , setup.py detects that it 's a simple source file and generates an egg from that.My aim is to have my project available on PyPI , so it 's important that it be installable using just pip ; so my question is ... am I doing something wrong ? My understanding was that setuptools is essentially a means to an end , that end being pip and PyPI , so it seems very strange to me that the two tools should behave so differently.The relevant part of setup.py and output from each tool follows : Output from setup.py install : Output from pip install :"
"I am using the Python requests library to implement retry logic . Here is a simple script I made to reproduce the problem that I am having . In the case where we run out of retries , I would like to be able to log at least one of the responses from the server to help debugging . However , it is not clear to me how to access that information . Of course , I could implement retries in some other way to accomplish my goal , but it seemed like this was n't that much of an edge case and I would be surprised to find out that requests did not support my use case.I have looked at the requests.exceptions.RetryError , the requests.packages.urllib3.exceptions.MaxRetryError that it wraps , and the requests.packages.urllib3.exceptions.ResponseError that that wraps all to no avail.Am I missing something ? $ python test.py"
"I have two end-point arrays that look like this : I am looking for the most efficient way to generate an output that looks like this : one way to do it is this : This solution is slow and I am certain that it can still be vastly improved by entirely replacing the zip and list comprehension combo with some functions entirely in Numpy , it is just that I do n't know how . Can you guys show me the most efficient way to do it ? Thank you guys in advanceCode to generate these two arrays :"
"A n parameter may be given to tkinter.mainloop function , I was not able to find any documentation about itWhat is the purpose of this n parameter ?"
"I 'm implementing in terms of SQLAlchemy a structure that has the mathematical characteristic of Partially Ordered Set , in which I need to be able to add and remove edges one at a time.In my current , best design , I use two adjacency lists , one being the assignment list ( approximately edges in the Hass Diagram ) , since I need to preserve which pairs of nodes are explicitly set as ordered , and the other adjacency list is the transitive closure of the first , so that I can efficiently query if one node is ordered with respect to another . Right now , I recompute the transitive closure each time an edge is added to or removed from the assignment adjacency list.It looks something like this : where floyd_warshall ( ) is an implementation of the algorithm by the same name.This is leading me to two problems . The first is that It does n't seem to be very efficient , but I 'm not sure of what sort of algorithm I could use instead . The second is more about the practicality of having to explicitly call Node.recompute_ancestry ( ) each time an assignment occurs , and only after the assignments are flushed into the session and with the proper connections . If I want to see the changes reflected in the ORM , I 'd have to flush the session again . It would be much easier , I think , If I could express the recompute ancestry operation in terms of the orm ."
"If I use enumerate while iterating through a very large list of graph clusters , I want to make sure I 'm not unnecessarily creating any copies of this list in memory.I 've been trying to confirm that it will not create any copies , but would like to know for sure ."
I am a newbee with Kivy GUI framework and I have a few questions related to kvlang:1.How can I add my custom widget class to root in kv file ? ( example ) PS : I use here clear_widgets ( ) then I try to add my custom widget but I get error after I click on button.How can I add HelloWorldForm widget class using add_widget method 2.How can I use add_widget and clear_widgets methods in python code ? ( for example ) main.kvmain.py3.How can I access kvlang properties in python ? For example i want to take the text from a button . How can I achieve that ?
"I have a problem with returning a tuple of two variable v , wt where v has shape= ( 20,20 ) and wt has shape= ( 1 , ) . wt is a variable that is a weight value . I want to return the tuple ( v , wt ) inside a map_fnmy code look somewhat close to thisthe fn will return a tuple but the error output I get is :"
"I 'm currently trying to annotate and count some dates , based on the number of times they appear.If I print this in a for loop like , I would get back the following in Json.This is kinda the right direction , however , it 's counting to the second.. I just need to days , so that the event that happened on 2018 , 10 , 5 would be count : 2 for example.Can anyone lead me into the right direction ? Additionally , whats the most `` django '' way of converting the dates into something more json / api friendly ? My ideal json return would be something like Thanks !"
"Can someone help me unpack what exactly is going on under the hood here ? I 'm on 64-bit Python 2.7 . For the first , I would assume that since there is only a precision of 15 for float that it 's just round-off error . The true floating-point answer might be something likeAnd the decimal just gets lopped of . But the second result makes me question this understanding and ca n't 1 be represented exactly ? Any thoughts ? [ Edit : Just to clarify . I 'm not in any way suggesting that the answers are `` wrong . '' Clearly , they 're right , because , well they are . I 'm just trying to understand why . ]"
"Is there a data structure in Python that resembles a blocking dictionary ? This data structure must fulfill these requirements : it must be randomly accessible and allow any element to be modified/deleted ( not just the first or last ) it must have a blocking get ( ) and put ( ) it must be thread-safeI would have used a queue but , although blocking and thread-safe , it 's not randomly accessible . A dict is not blocking either ( as far as my Python knowledge goes ) .As an example , think of one producer thread adding key-value pairs to such a data-structure ( updating values for existing keys if already present - this is where a queue wo n't cut it ) , and a worker blocking on get ( ) and consuming these key-value pairs as they become available.Many many thanks ! edit : Let 's assume the producer polls a CI server and gets project-status pairs . It generates the differences in project statuses and puts them in the aforementioned data structure . The worker picks up these project-status updates and displays them one by one as an animation on the screen ."
"I was just experimenting in Python with different syntax for passing in a generator as an argument to a function , and I realized that although I 've been doing this , this works as well : This is tested on Python 2.6.6 on Linux . What 's going on under the hood ? Is it just syntactic sugar ? After all , usually an unwrapped generator is indecipherable to the interpreter :"
Pyflakes does not deal very well with the following code : Using vim and syntastic which uses pyflakes I get the following error : So I get warnings about @ nodes.setter because I redefine nodes.How do I disable this useless warning since this code is correct ? Or which python checker deals with this code correctly ? UpdateI ran into some problems when I refactored my code because properties and functions have different inheritance behavior . Accessing properties of a base class is different . see : How to call a property of the base class if this property is being overwritten in the derived class ? .Python derived class and base class attributes ? so I now tend to avoid this syntax and use proper functions instead .
"I 've run into a weird problem when starting an interactive python console in the background . After resuming the interpreter from the background , it does not display any of the text I type ( i.e . it just shows the > > > prompt , though it will interpret whatever I write . Pressing [ enter ] created another > > > prompt on the same line ) .An easy way to reproduce the problem is just to type : This problem does does not occur if you start the program in the foreground , put it in the background , and return it to the foreground : If you 're wondering why you might want to start an interactive interpreter in the background , consider the following scenario : I have a simulation that takes a long time to run , but after it 's done , I want to interact with the results . Thus , I started the program : The easy solution is just to start it in the foreground , move it to the background , and then later bring it to the foreground , but I 'm just wondering why this happens ."
I am using Google oauth2client and the code from sample is : All works in Python interactive or IDE but if I am trying to use the code from Jupiter Notebook I got an exception . Inside Jupiter Noteboo I am trying simple :
"I 'm creating a GUIclass that uses Frame ( ) as its base class . In my GUIclass ’ s init method I want to create a Frame widgetRight now I have : But I 've seen this elsewhere for the third line : I 'm new to programming , python and definitely inheritance and I wanted to know if I understand the difference between the two correctly . I did a lot of researching and reading , I promise , but I could n't quite find anything that made it completely clear : In the first situation I do n't call the init method as I created a Frame object ( frame ) and when an object is created its init method is called implicitly by python . In the second scenario , one is calling the init method on the class ( which I believe is totally legit ? ) because a Frame object was n't created , so therefore would n't do it automatically . Is that right ? I 've also seen : which really threw me off . Is this just someone doing something redundant or is there a reason for this ? Thank you for your help , I want to take it slow for now and make sure I fully understand any and every line of code I write as I go rather than writing and running a whole program I half understand ."
"How can I programmatically access the default argument values of a method in Python ? For example , in the followinghow can I access the string 'Foo ' inside test ?"
I currently have a system of classes that register callbacks and then call them when certain conditions are met . However I am running into some problems storing the function object.A : I am not sure why it requires a Foo instance when the function is not a member of Foo . B : Why does the first version not work while the second version does ? What functionality differs when adding it to a list instead .
"Ok , so I have several , multi-dimensional numpy arrays of sympy objects ( expressions ) . For example : and so on.What I would like to do is multiply several of these arrays using einsum , since I already have the syntax for that from a numerical calculation I was doing earlier . The problem is , when I try to do something likeI get a type error : Sure , so a quick search on Google shows me einsum probably ca n't do this , but no reason as to why . In particular , calling the numpy.dot ( ) and numpy.tensordot ( ) functions on those arrays works like a charm . I could use tensordot to do what I need , but my brain hurts when I think about having to replace fifty or so Einsten summations like the one above ( where the order of the indeces is very important ) with nested tensordot calls . Even more nightmarish is the though of having to debug that code and hunt for that one misplaced index swap.Long story short , does anyone know why tensordot works with objects but einsum will not ? Any suggestions towards a workaround ? If not , any suggestions as to how I would go about writing my own wrapper to nested tensordot calls that is somewhat similar to the einsum notation ( numbers instead of letters are fine ) ?"
In PyGtk I always used this to create a ListStore with an Image ( using it with an IconView for displaying files ) : But I ca n't figure out how to do this with Python 3 and PyGObject .
"We 're using Rauth to connect to various OAuth 1 APIs . It works fine for a single request , but trying to do 2 or more requests against the given session results in 401 not authorized errors from the APIs . Twitter API example : This happens on both Twitter and LinkedIn APIs . How do we execute multiple requests against a single OAuth1Session object ? VERSIONS : rauth==0.5.4requests==1.1.0 UPDATE : Strangely , if the params argument is not included then multiple requests can be made- but once params are included , even if it is an empty dict , we get 401s.Example 1 : Example 2 :"
"I 'm trying to solve the Hackerrank problem Non-Divisible Subset stated below : I attempted the following solution ( which works for the sample test case ) : What I 'm trying to do is identify the 'offending ' pairs and removing the element of a which occurs most frequently , until there are no 'offending ' pairs left.However , I 'm getting `` RunTime Error '' for most of the test cases : Presumably the algorithm above works in this simple case where only one number ( the number 2 ) needs to be removed , but fails in more complicated test cases . Can anyone see what is wrong with it ? UPDATEFollowing poke 's suggestion to test k = 2 and a = [ 1 , 2 , 3 ] , I made the following modifications : The resulting a is [ 2 , 3 ] and contains two elements as expected . I 've also checked that it still works for the original example . However , I am still getting a `` Segmentation Fault '' on some of the test cases : According to https : //www.hackerrank.com/challenges/pairs/forum/comments/9154 , segmentation faults typically occur because of invalid memory access ( array indices which do n't exist , etc. ) . I still have n't managed to find any other test cases , though , where the algorithm fails . Any ideas ?"
"So , I 'm making a game in Python 3.4 . In the game I need to keep track of a map . It is a map of joined rooms , starting at ( 0,0 ) and continuing in every direction , generated in a filtered-random way ( only correct matches for the next position are used for a random list select ) .I have several types of rooms , which have a name , and a list of doors : For the map at the moment I keep a dict of positions and the type of room : I have loop that generates 9.000.000 rooms , to test the memory usage.I get around 600 and 800Mb when I run it.I was wondering if there is a way to optimize that.I tried with instead of doing I would do but this does n't have a real change in usage.Now I was wondering if I could - and should - keep a list of all the types , and for every type keep the positions on which it occurs . I do not know however if it will make a difference with the way python manages its variables.This is pretty much a thought-experiment , but if anything useful comes from it I will probably implement it ."
Is it possible to draw a graph of a Sesame RDF database using RDFLIB ? This is what I tried : This is the error : I think the only trick is specifying a proper URL to cause Sesame to return a .rdf xml layout . Author of question : reposted to http : //answers.semanticweb.com/questions/9414/python-using-rdflib-to-graph-a-sesame-database ( see answer there )
"I 'm working with my Django app . For some reason an element of a list is being assigned incorrectly.I 'm trying to set a break where I think the error is occurring . ( line 20 ) I 'm invoking pdb with this line of code : However , inside the code , I ca n't seem to set a Break . What am I doing wrong ?"
Is it possible to have a class attribute targeting another attribute from the same object and have a function to update the value of the target ? Expected outcome :
"How to reverse re.escape ? This blog from 2007 says there is no reverse function , but is that still true , ten years later ? Python 2 's decode ( 'string_escape ' ) does n't work on all escaped chars ( such as space ) .Python 3 : Some suggest unicode_escape or codec.escape_decode or ast.literal_eval but no luck with spaces.So is this really the only thing that works ?"
"( If someone can suggest a better title , by all means go ahead and edit ) .Given a list list1 whose exact length is unknown but for which it is known will always be less than or equal to 5 , I 'm looking to populate a separate empty list list2 , of fixed length 5 , with the values in list1 , padding out with empty strings if the size of list2 is less than 5.e.g . if list1 = [ 1,2,3 ] then list2 should be [ 1,2,3 , '' , '' ] and so on.So : What 's the neatest way of achieving this ( determining how many empty strings to add ) ?"
"What is the reason those two divisions give different results ? I am very confused because with some numbers it gives the same results and with some it doesn't.Tested on windows x64 , python 3.5 and 3.6 x64 , numpy 1.13.1.EDIT : This was a numpy bug which has since been fixed ( https : //github.com/numpy/numpy/pull/9469 ) ."
"How can we do recursive access of nested dicts , in the general case ? I was going down some horrible path of reduce , d.__getitem__ and d.__setitem__ but felt there must surely be a more elegant way ..."
"I 'm re-formatting a Python script using notepad++ , but some lines are not indented by 4 ( or 8 , 12 , 16 , etc . ) spaces.So I need to match consecutive leading white-spaces ( i.e . indentation at beginning of each line ) , which are NOT in multiple of 4 , i.e . spaces in number of 1 , 2 , 3 , 5 , 6 , 7 , 9 , 10 , 11 , etc.e.g.I was able to match spaces in multiple of 4 using something like : then I tried to match the opposite of this with something like : but it only matches empty line or line start with a character , not what I want.I 'm a complete newbie to regex , but I 've searched for hours with no luck . I know I could always match with all the non-multiple-of-4 numbers listed , but I was hoping someone could help and provide a less-cumbersome method.Thanks.Update 1using regex ( @ user2864740 ) or ( @ alpha bravo ) matches non-multiple-of-4 indents , as well as empty line ( s ) with 4 ( 8 , 16 , etc . ) spaces and the first character of the first non-empty line following them.e.g . ( on regex101.com ) How to avoid matching these situations described in the example above ?"
"I 'm trying out Sanic and ran the Hello World app except I added a sleep in the request handler : However , when I run this , it still blocks on each request : In two separate terminals : I thought the idea of Sanic is being able to process all requests asynchronously and not blocking until one completes to process the next one . Am I missing something here ?"
Short QuestionIs it possible to call a module as retrieved from the python dir ( ) function ? BackgroundI am working on building a custom test runner and would like be able to choose which modules to run based on a string filter . See my examples below for ideal usage.module_a.py module_b.py Output
"I have a pandas sorted data frame ( based on time ) like this : Which turns out like this : I 'd like to aggregate the data ( averaging ) without a for loop . However , the way I am going to group the observations is not straight forward ! Looking at Value1 , I want to group them as non-zero values together . For example , indicies 1,2,3 would be in one group . Incidies 7,8,9 in one group and another one would be 12,13,14 . The rows where value1==0 , should be avoided and the zeros just act as a separation between groups . Eventually I 'd like to get something like this : Currently , I am thinking that I should somehow assign numbers 1,2 and 3 to a new column and then aggregate them based on that . I am not sure how to make that column without a for loop though ! Please notice that Value1 and Value2 are not necessarily the same ."
"I 'm trying to optimize some code using hotshot , which I expected to be great since it 's the high-performance profiler and all . But for some reason , I 'm getting wildly inaccurate results from it . On my most recent profiling run , hotshot reported the cumulative time of the top-level function I called as 7.946 seconds . But without even timing it I can tell it 's taking much longer than this . Timing the run time myself simply using time.time ( ) gives me a runtime of 42.465 seconds . I 'm guessing this is because hotshot does n't count system IO time or something and my program is batch-processing a lot of binary files ? However , using some more time.time ( ) blocks I narrowed most of the extra time usage ( which hotshot fails to notice ) down to a lower-level processing function that does n't do any IO . hotshot reported the total and cumulative times that this function took as 4.414 and 6.185 seconds , respectively . However , again using time.time ( ) statements I found that its cumulative time was over 30 seconds . The cumulative time spent in one relatively simply block of code was 7.32 seconds , longer than hotshot said was spent in the entire function . The block looks like this : That 's it . This block was run over 9 million times , so maybe I should n't be surprised that this much time was spent in it . But there is plainly no IO being done here . Why is hotshot so underestimating the time this function ( and the whole program ) is taking ? And before you ask , no , I 'm not using multiple threads or anything like that ."
"So i 'm green as grass and learning programming from How to think like a computer scientist : Learn python 3 . I 'm able to answer the question ( see below ) but fear i 'm missing the lesson . Write a function ( called insert_at_end ) that will pass ( return the bold given the two arguments before ) for all three : The book gives this hint : '' These exercises illustrate nicely that the sequence abstraction is general , ( because slicing , indexing , and concatenation are so general ) , so it is possible to write general functions that work over all sequence types . `` .This version does n't have solutions on-line ( that i could find ) but in I found someone 's answers to a previous version of the text ( for python 2.7 ) and they did it this way : Which seems to be solving the question by distinguishing between lists and strings ... going against the hint . So how about it Is there a way to answer the question ( and about 10 more similar ones ) without distinguishing ? i.e not using `` type ( ) ''"
"I 'm trying to convert a Perl script to python , and it uses quite a few different packs . I 've been able to figure out the lettering differences in the `` templates '' for each one , but I 'm having an issue with understanding how to handle Perl 's lack of length declaration.example : I do n't see an analog for this `` * '' feature in struct.pack , on Python . Any ideas on how to convert this to Python ?"
"In many cases unit-tests are significantly slowed down by the use of python 's logging package . Assuming logging is n't essential to the test , how would you cleanly override logging per-test , so that log commands would be effectively skipped.Assume the use of multiple loggers such as in :"
"I come from a PHP background and I 've been trying to learn Python but I 'm having a lot of trouble when it comes to debugging as I 'm not yet sure how to go about this in Django or Python in general.I 'm used to being able to print_r or var_dump everything in PHP . I could do it in the controller , In a service layer or the even the model and data would show up in my web browser.I ca n't do this in Django . Depending on what I 'm doing , attempting to just perform a print on an object from my view will bring the page down or output something to my console that does n't really help me . Here 's an example : The above will take down my page completely with a notice saying : There are some instances while working with CBV 's where I noticed I could get the data to just dump somewhere such as the console . But it would n't be anything that would help me . For instance if I was trying to to take a look at the contents of response from above , it would just show up like so : A var_dump would have allowed me to actually see inside of it.So I 'm guessing I 'm going about this all wrong . Do people just dump data when they 're debugging in Python ? If they do , how do you perform this , and does it show up in the web browser or the console ? If not , how do I go handle basic troubleshooting in Django ? Example scenarios : I want to see the contents of a list or dictionaryI want to see the raw sql query being performed by the ORMI want to see if a function is being executed by slipping some text inside to be outputted on the front end"
"I 'm trying to use pyodbc to connect to a SQL Server ( MS SQL Server through FreeTDS ) in a portable application ; since it 's supposed to be standalone , I would like to avoid having to explicitly install the driver on the system , just bringing the ODBC driver dll along the application.This page suggests that it 's possible to specify the driver dll directly in the connection string Specify the DRIVER= parameter in the szConnStrIn argument to the SQLDriverConnect function . For example : where ospath is the operating system subdirectory of your Adaptive Server Anywhere installation directory . Trying it through pyodbc+libtdsodbc.so on Linux , it does work fine ; however , trying the same on Windows ( pyodbc+tdsodbc.dll ) I always get pyodbc.Error : ( 'IM002 ' , ' [ IM002 ] [ Microsoft ] [ ODBC Driver Manager ] Data source name not found and no default driver specified ( 0 ) ( SQLDriverConnect ) ' ) ( my libtdsodbc.so seems to be fine , though , since , if I install it as a `` regular '' driver and refer it with its name it connects fine ) Checking the documentation of SQLDriverConnect and related pages there 's no mention of the DRIVER= option used straight with the dll path.So , is n't `` straight-to-driver-dll '' connection not supported on Windows ? Are there any alternatives , especially with Python , to connect straight to the driver dll , bypassing the ODBC driver manager ?"
"I have following column names in a list : I have one array which consists of array indexes I want to subset the list based on array indexesDesired output should beI have tried something like this in pythonBut , it does not work ."
"I understand that one should not use mutable default parameter value in Python ( with some exceptions ) because this value is evaluated and stored only once when the function is defined , and not each time the function is later called.My understanding of that is this ( using the example below ; please excuse my imprecise language as I 'm just a beginner in Python programming , who 's stuck at the Function chapter of my textbook because of this ) :1 ) The function f is defined , and x ( local variable in f ) assumes the default variable of [ 1 , 2 , 3 ] ( even before the function is called ) 2 ) When f ( ) is called , x is still [ 1 , 2 , 3 ] due to no argument passed to it , and x continues having its default value3 ) x is modified in place with append , becomes [ 1 , 2 , 3 , 4 ] , and is printed as suchHowever , this is where my confusion arises . I 'd assume that:4 ) When f ends , x is destroyed ( in the stack or whatever you 'd call it ) and is no longer associated with the list object [ 1 , 2 , 3 , 4 ] **5 ) The list object [ 1 , 2 , 3 , 4 ] is reclaimed since there 's no variable that refers to it anymoreTherefore,6 ) When f ( ) is called the second time , I 'd expect Python to output an error since x now no longer has a value associated with it . In other words , how can Python reuse the default value from the last evaluation when it 's been reclaimed/destroyed ? Appreciate all your help and explanation ! ** this understanding I got from Ned Batchelder 's page on variable name assignment ( see below )"
"Say we have a function that translates the morse symbols : . - > -.- - > ... -If we apply this function twice , we get e.g : . - > - . - > ... -- .Given an input string and a number of repetitions , want to know the length of the final string . ( Problem 1 from the Flemish Programming Contest VPW , taken from these slides which provide a solution in Haskell ) .For the given inputfileWe expect the solutionSince I do n't know Haskell , this is my recursive solution in Python that I came up with : which works for the first three inputs but dies with a memory error for the last input.How would you rewrite this in a more efficient way ? EditRewrite based on the comments given : Comments on style and further improvements still welcome"
"Not so much a question but something puzzling me.I have a column of dates that looks something like this : I 'd like to convert it the NaTs to a static value . ( Assume I imported pandas as pd and numpy as np ) .If I do : All is well , I get : But if I do : I get : This operation converts the original , non-null dates to integers . I thought there might be a mix-up of data types , so I did this : And still get : Please note ( and do n't ask ) : Yes , I have a better solution for replacing nulls . This question is not about replacing nulls ( as the title indicates that it is not ) but how numpy where is handling dates . I ask because I will have more complex conditions to select dates to replace in the future , and thought numpy where would do the job.Any ideas ?"
"I have an image containing text but with non straight lines drawn on it . I want to remove those lines without affecting/removing anything from the text.For that I used Hough probabilistic transform : The result was not as good as I expected : The lines were not entirely detected ( only some segments , the straight segments , of the lines were detected ) .I did some adjustments on cv2.Canny and cv2.HoughLinesP parameters , but it did n't work too . I also tried cv2.createLineSegmentDetector ( Not available in the latest version of opencv due to license issue , so I had to downgrade opencv to version 4.0.0.21 ) : The result was a bit better , but did n't detect the entire lines . Any idea how to make the lines detection more efficient ?"
"I 'm making a very straightforward plotting wxApp.I 've installed Python 2.7.9 and wxPython 3.0.2 Here is my code : However , whenever I test it , it throws me this error : I 'm passing all parameters that the documentation of wx says.What am I doing wrong ?"
"I just did a time test on loading a data array from a csv , creating a database on Postgres and writing the table to it in python and R.I was surprised that the times were very similar : The python code first : ( as an e.g ) and the R code ( which is much more legible ) I was very surprised at how close the times for the two were . ( I 've read an awful lot on R being slow and Python being extremely fast ) For pythonand for RI wondered if it had something to with the fact that I 'm INSERTing a row at at time into the python version of the table.Hence the main question - is there an equivalent in python for the dbWriteTable block in the R code and would it speed things up ? A second ancillary question would be is there anything obviously wrong with the code that might be slowing things down . Happy to provide sample csv 's if that would help.Not looking to start a flame war on R v Python , would just like to know how I can make my code faster.Thanks"
"My 8 year old niece had a lesson on Morse code in school yesterday , and her assignment was to convert various phrases to Morse code . One of the phrases included her age , and instead of writing -- -.. , she wrote 3-2. because ( in her words ) , `` it 's less writing that way . '' This rudimentary `` compression algorithm '' sparked my curiosity , so I wrote a bit of code to implement it.However , we made a few changes along the way . I pointed out to her that if you wrote just ... .. -- -- - , there is n't any way to tell if the writer meant 50 or eeeeettttt . In reality , there is a pause in between each letter of each word and every word so this is n't a problem , but our scheme did n't have that . I pulled out some graph paper and suggested padding the Morse code for each symbol with another symbol to facilitate coding and remove ambiguity from the scheme . My nice suggested using + because `` no one ever writes those in sentences . '' ( Ouch , I recently graduated with a math degree , but fair enough . ) Since some of us do write with + , and we all use hyphens and periods/dots , which would conflict with our standard definition of Morse code , these symbols are replaced with p , h , and d , respectively . Of course , this brings us to the problem of what to do with symbols that are n't defined in our extended Morse code . My niece wanted to simply ignore them , so that 's what we did . For the sake of case sensitive preservation of the textual message , upper case letters are n't lower cased in the code ; they 're just carried through as is and padded with +.Summary of the algorithm : Morse codes are right-padded to 5 characters with +We extended Morse code to substitute p for + , d for . , and h for -.Symbols that are n't defined in our `` extended '' Morse code are passed through intact.Runs of symbols are replaced with unless only one consecutive character occurs , in which case the number is omitted.Potential pitfalls : My padding scheme probably reduces the effectiveness of the compression.Compression might be improved by using blocks larger than 5 charactersIf either my niece or I knew anything about compression algorithms , we could probably use that makes them successful.This obviously is n't suitable for production , but since there are numerous efficient compression algorithms for such purposes , I 'm ignoring that problem for the time being. ? ? ? Example : In our algorithm , `` Hello , World '' translates toand compresses toHere is the Python code I threw together : What are some simple methods that would a ) improve our algorithm , and b ) be relatively easy to explain to my 8-year old niece ? While the last point is clearly subjective , I 'm nevertheless trying to indulge her curiosity as much as possible.I welcome any improvements to the code as well since it 's not structured terribly well ( I 'm fairly certain it 's structured quite poorly , actually , but it 's quick and dirty ) , although that 's strictly for my benefit since I have n't gotten my niece to use Python ( YET ) .UpdateHere is an updated version of the code , which attempts to incorporate both user1884905 's modifications to the algorithm and Karl 's improvements to the code itself ."
"I was wondering how to generate permalinks from the following markup , using python markdown library : The desired output would be something likeAnswer : Thanks @ BlaXpirit ( see answer ) Use headerid python markdown extension and input the following : This generates the following output : Then use some css styling to get the common output , something like :"
"https : //plotly.com/python/line-and-scatter/ has many scatter plot examples , but not a single one showing you how to set all the points ' colours within px.scatter : I 've tried adding colour = 'red ' etc does n't work . These examples only show you how to colour by some other variable.In principle I could add another feature and set it all the same but that seems a bizzare way of accomplishing the task ... ."
"Short versionA specific setup requires me to create local variables in __init__.py that shall mask modules from the same package.E.g . the variable y ( in the local context of __init__.py ) shall hide the module y.py . The statement import x.y shall yield the local variable instead of loading the module.If you do not want to read about the specific setup , scroll down to the question ; it is understandable without the details.Detailed descriptionI have implemented a set of Python 2.7 packages , each of which may require individual configuration settings . For convenience , I was planning to provide configuration defaults per package that can be locally overwritten by whoever uses one of the packages . ( The rationale for this is to distribute default settings when deploying an app to a machine running a specific environment ( a server , workstation , laptop , etc . ) , but at the same time to allow overriding configurations without messing up the local repository or resetting local adaptions on code updates . ) The directory structure example is : I 'd like to access the settings stored under config/ like regular module imports , e.g. : ... but have it implicitly switch to the overriding file , if it exists.My solution approachIn order to somehow automate the process , I am dynamically creating local variables pointing to the correct configuration file imports . Using the imp package , I can import a module and specifically naming it at the same time . ( I.e . at runtime , you can not distinguish whether < pkg > _sample.py or < pkg > .py was loaded to serve the configuration . ) I finally ended up with this : This actually creates a local reference to the required source files ( omitting < pkg > _sample.py when < pkg > .py is existing in config/.I can use it from other modules/scripts if using from config import package_a as cfg_a.The questionEssentially , this question may fall back to the well-known import x.y vs from x import y-thing . But there is a difference here.I know that import x.y requires y to be a module . Is there any possibility to hide a module in its package 's __init__.py and to provide a local variable instad on import ? from x import y yields the local variable y from x 's __init__.pyimport x.y always imports the module , even if a local variable y exists in __init__.py.I can not force everyone to always use the former import statement , people like to use the latter one in their code.Any advise here ? Edited : Fixed title . Sorry.The solutionThanks @ martijn-pieters for pointing out sys.modules.Actually , my approach would have worked perfectly without explicitly adding the new import to sys.modules , as I just failed at properly naming the new imports : This solves the issue , as it does not register the new submodule with its canonical name ( here : package_a ) but registers it as a submodule of my config package.Thanks a lot !"
"I 'm trying to write a Python script to let me log in to my fantasy football account at https : //fantasy.premierleague.com/ , but something is not quite right with my log in . When I login through my browser and check the details using Chrome developer tools , I find that the Request URL is https : //users.premierleague.com/accounts/login/ and the form data sent is : There are also a number of Request headers : So I 've written a short Python script using the request library to try to log in and navigate to a page as follows : On printing out the content of my post request , I get a HTML response code 500 error with : If I remove the 'host ' from my head dict , I get a HTML response code 405 error with : I 've tried including various combinations of the Request headers in my head dict and nothing seems to work ."
I have a model in django that have foreignkey with User model.So When any user account delete from database then in News table entries also deleted according to user . So I want that When any user account deleted then I need to set his/her full name in user field using 'on_delete=models.SET ( ? ? ? ) ' Example : If I have user that first_name = 'Neeraj ' and last_name='Kumar ' When I want to delete that user account then in News table I want to save his name .
"Looking at the default `` Hello world '' script on Flask 's website : I 'm very new to programming , so I do n't understand how this script can work - the hello ( ) function is n't called anywhere , so does Flask simply display the output of the first function found ? What if I wanted to display outputs from two or three functions on the page ?"
"I prefer to write my application without even thinking about a graphical user interface . Once the application code is working properly , I like to glue a GUI layer on top of it - with a clean interface between the two.I first tried to make the GUI run in a different process from the application . But I soon regretted that experiment . It is far from trivial to setup a communication link between two processes . So I decided that for now , multiple threads are fine ( although the Python Global Interpreter Lock makes them run on a single core ) .The MainThread is completely in the hands of the Qt GUI . Apparently this is standard practice . So let us suppose that the overall structure of the software should look like this ( note that qtThread is synonymous to MainThread ) : My application code is running in the appThread - cleanly separated from the GUI . But at some point , there has to be interaction.I have read many articles about how to organize this , but many sources contradict each other . Even the official Qt application is wrong according to many people ( the Official documentation encourages to subclass QThread ) . The most enlightening articles I could find are these : http : //ilearnstuff.blogspot.be/2012/08/when-qthread-isnt-thread.htmlhttp : //ilearnstuff.blogspot.be/2012/09/qthread-best-practices-when-qthread.htmlEven after considering all that , I still remain in doubt about several things.Question 1 . What is the most proper way to start the appThread ? What is the most proper way to start the appThread ? Correct me if I am wrong , but I believe that there are two choices : Choice 1 : Start a standard Python threadPython provides the threading library that one can import to spawn new threads : This choice looks the cleanest to me . You can truly write your appThread code without even thinking about a GUI . After all , you 're using the standard Python threading library . There is no Qt stuff in there.But I can not find clear documentation about setting up a communication link between the appThread and the MainThread . More about that issue in the second question..Choice 2 : Start a QThread threadThis choice looks not so clean , because you have to mess with Qt stuff to write your application code . Anyway , it looks like a viable option , because the communication link between both threads - appThread and MainThread - is probably better supported.There are myriads of ways to start a QThread thread . The official Qt documentation encouraged to subclass QThread and reimplement the run ( ) method . But I read that this practice is in fact very bad . Refer for more info to the two links I 've posted at the beginning of my question.Question 2 . What is the best communication link between both threads ? What is the best communication link between both threads ? Obviously the answer to this question depends very much on the choice made in Question 1 . I can imagine that linking a standard Python thread to the GUI differs very much from linking a QThread.I will leave it up to you to make suggestions , but a few mechanisms that pop up in my mind are : Queues : using standard Python queues or Qt Queues ? Signal/Slot mechanism : ... Sockets : should work , but looks a bit cumbersomePipes : ... Temporary files : cumbersomeNotes : Please mention if your answer applies to Python 2.x or 3.x . Also keep in mind that confusion may arise quickly when speaking about threads , queues and the like . Please mention if you refer to a standard Python thread or a QThread , a standard Python queue or a QQueue , ..."
"Possible Duplicate : Python : What is the best way to check if a list is empty ? I edited this because I need more help . How do you check if c is blank ? Is c is simply equal to [ ] ? I 've tried == [ ] and `` [ ] '' and getting the length and == `` '' , but nothing seems to work ."
"I 'm currently using tika to extract the text from pdf files . I found a very fast method within the tika module . This method is called unpack . This is my code : However , once in a while ( not always ! ) I get this warning : After retrying the code starts to work . However , I do n't understand the warning and also it takes time to retry.Anyone has an idea why I get this warning ? This is the github page : https : //github.com/chrismattmann/tika-python"
"Why - or why not - is it good practice to use getters and setters specifically in Python OOP ? My textbook states the following : The getValue ( ) method , called a getter or an accessor , returns the value of the value instance variable.Why write this kind of function ? Why not simply use the instance variable ? We ’ ll address this in the FAQ ’ s at the end of this chapter.However , there is no FAQ at the end of the chapter , and so it is never explained as to why getters are used in Python OOP.I 've tried reading other places , but I have not found a good explanation anywhere . Most answers on SO are about Java , and I 've read that it is not relevant to Python ... Can someone please help me understand why it is good practice to use them ? Or if not , why not ?"
"I am working on a project that requires that I create a 2D interface rendered `` on top '' or a 3D world . On some other forums , I read that you could use `` GluOrtho2D ( ) '' for the job , and switch back to GluPerspective ( ) once you were done . The only thing is , my test code I wrote for it only displays the 3D world , not the 2D quad . When I disable the 3D rendering code , however , the quad appears where it was supposed to be.I trimmed down the code to openGL only statements , which I wrote down below . The code is written in Python using the Pyglet library.The scene initialization code : The frame rendering code . The call to builder.buildMap ( ) creates the 3D scene , and the glBegin-glEnd pair draws a 2D quad : the stage.set3DMode function : and the stage.set3DMode function : I really hope someone can point out my mistake ! And thank you for helping me : )"
"I have a series that contains NaN and True as a value . I want another series to generate a sequence of number , such that whenever NaN comes put that series value as 0 and In between of Two NaN rows I need to perform cumcount.i.e. , Input : OutputHow to perform this in pandas ?"
"This seems simple ( and is trivial to write a three-line loop for ) , but how can I use numpy slicing make a list of the index locations of the upper diagonal of a numpy array ? I.e.Given a 4x4 array , I 'd like the index locations at the X 's : Giving :"
"I would like to start a selenium browser with a particular setup ( privoxy , Tor , randon user agent ... ) in a function and then call this function in my code . I have created a python script mybrowser.py with this inside : Then I import it into another script myscraping.py with this inside : The browser is working - I can access the page and retrieve it with .page_source . But the IP does n't change between the first and the second print . If I move the content of the function inside myscraping.py ( and remove the import + function call ) then the IP change . Why ? Is it a problem with returning the browser ? How can I fix this ? Actually , the situation is a bit more complex . When I connect to https : //check.torproject.org before and after the call to mybrowser.set_new_ip ( ) and the wait of 12 sec ( cf the lines below ) , the IP given by the webpage changes between the first and the second call . So my Ip is changed ( according to Tor ) but neither https : //httpbin.org/ip nor icanhazip.com detects the change in the IP . So the IP that are printed are like that : Privoxy configuration : in C : \Program Files ( x86 ) \Privoxy\config.txt , I have uncommented this line ( 9050 is the port Tor uses ) : Tor configuration : in torcc , I have this :"
"Inspired by another question here , I would like to retrieve the Python interpreter 's full command line in a portable way . That is , I want to get the original argv of the interpreter , not the sys.argv which excludes options to the interpreter itself ( like -m , -O , etc . ) .sys.flags tells us which boolean options were set , but it does n't tell us about -m arguments , and the set of flags is bound to change over time , creating a maintenance burden.On Linux you can use procfs to retrieve the original command line , but this is not portable ( and it 's sort of gross ) :"
"I have a csv file which goes like this : Now , if I pass the list of the columns I want in place of labels column ( does n't contain all the words in the label columns ) I want to obtain the dataframe as : I looked into get_dummies , but that didnt help"
"Several operators allow to pull data but I never managed to use the results.For example : https : //github.com/apache/incubator-airflow/blob/master/airflow/contrib/operators/bigquery_get_data.pyThis operator can be called as follow : Yet , get_data is of type DAG but line 116 says `` return table_data '' .To be clear , the operator works and retrieve the data , I just do n't understand how to use the data retrieve/where it is located.How do I get the data using `` get_data '' above ?"
"I have a python script . Script have selenium with Chrome and go to a website , take data and put in CSV file.This is a very long work.I put the script on the server . And run . All work.But I need script work in the background.And I seePress Enter.Then it runs and immediately writes errors to the file , that Chrome did not start or there was no click.I want to remind you that if you start without nohup , then everything will work.What am I doing wrong ? How to run a script ? Thank you very much ."
"I am using the Librosa library for pitch and onset detection . Specifically , I am using onset_detect and piptrack.This is my code : When running this on guitar audio samples recorded in a studio , therefore samples without noise ( like this ) , I get very good results in both functions . The onset times are correct and the frequencies are almost always correct ( with some octave errors sometimes ) .However , a big problem arises when I try to record my own guitar sounds with my cheap microphone . I get audio files with noise , such as this . The onset_detect algorithm gets confused and thinks that noise contains onset times . Therefore , I get very bad results . I get many onset times even if my audio file consists of one note.Here are two waveforms . The first is of a guitar sample of a B3 note recorded in a studio , whereas the second is my recording of an E2 note.The result of the first is correctly B3 ( the one onset time was detected ) .The result of the second is an array of 7 elements , which means that 7 onset times were detected , instead of 1 ! One of those elements is the correct onset time , other elements are just random peaks in the noise part.Another example is this audio file containing the notes B3 , C4 , D4 , E4 : As you can see , the noise is clear and my high-pass filter has not helped ( this is the waveform after applying the filter ) .I assume this is a matter of noise , as the difference between those files lies there . If yes , what could I do to reduce it ? I have tried using a high-pass filter but there is no change ."
"I am trying to get user details of persons who has put likes , comments on Facebook posts . I am using python facebook-sdk package . Code is as follows.From the above , I am getting a highly nested json . Here I will put only a json string for one post in the fb.Now I need to get this data into a dataframe as follows ( no need to get all ) .Any Help will be Highly Appreciated ."
"I 've created a function that generates a list of alphabets incrementing continuously . A , B , C ... , Z . After Z , it goes to AA , AB , AC ... AZ . This pattern repeats . This is similar to MS Excel 's column names . At the moment , this function generates a finite list of alphabets . I can then iterate over it in conjunction with some finite list , e.g . 0-10 . See my code below . What I 'd like is to create a generator that will give me an infinitely long list of incrementing alphabets ."
This is my data frameI need to separate numbers from time and put them in two new columns . The output is like this : This is my code : But it does not work . Any suggestion ? I also need to create another column based on the values of time column . So the new dataset is like this : Any suggestion ?
"I 've observed at least 3 types related to functions in Python 3 : I 'm wondering what 's the difference between 'function ' , 'method ' and 'bound method ' ? Is 'method ' a type equivalent to 'unbound method ' in Python 2 ?"
"I 'm trying to move some files around on my filesystem . I 'd like to use Python 3 's Pathlib to do so , in particular , Path.rename . Say I want to move Path ( '/a/b/c/d ' ) to Path ( '/w/x/y/z ' ) . givesI can fix this withBut that 's less elegant than old school os , which has a method called renames which does this for you . Is there a way to do this in Pathlib ?"
When using the typical 3D plot like so : flake8 reports the expected error : I know it can be avoided using the # NOQA comment . But is there a different way to formulate the projection in the figure so that the Axes3D object is used ?
"I 'm writing a Python C extension , and in the examples listed here , there is a snippet of code : According to the documentation for parsing a string with PyArg_ParseTuple , `` You must not provide storage for the string itself ; a pointer to an existing string is stored into the character pointer variable whose address you pass . '' So how does Python know when the memory pointed to by `` command '' can be freed ? How is a memory leak not occurring ?"
"Current pandas version : 0.22I have a SparseDataFrame.Right now , the fill values are 0 . However , I 'd like to change the fill_values to np.nan . My first instinct was to call replace : But this gives Which does n't really help me understand what I 'm doing wrong.I know I can do But is there a better way ? Or is my fundamental understanding of Sparse dataframes flawed ?"
"I have come across a small problem . Say I have two lists : I then have a list of lists : I then need to iterate through list_A and list_B and effectively use them as co-ordinates . For example I take the firs number from list A and B which would be ' 0 ' and ' 2 ' , I then use them as co-ordinates : print matrix [ 0 ] [ 2 ] I then need to do the same for the 2nd number in list A and B and the 3rd number in list A and B and so forth for however long List A and B how would be . How do this in a loop ?"
"I get a dict to init a class person . there is one field in person : 'name ' . 'name ' field is optional , meaning that if the dict do n't have the 'name ' item , then there 's no 'name ' value of person . I use getter methods to get instance attribute , but it will throw a error if there 's no 'name ' value . I do n't know is there any good programming style to improve my code ? Because python create instance field at run time , I do n't know how to use getter like java.AttributeError : Person instance has no attribute 'name '"
"I 'm starting to learn Python and PyQt . Currently I am fighting with a very basic problem about connecting signals and Slots , with Dialog forms generated form QDesigner.I want to connect a pushbutton from a QDialog . The code does not generate an error . The Dialog is show , as expected . But with clicking on the pushbuttons nothing happens . Alternatively I have tried included the code form Ui_Dialog directly in my target class Testdialog . Then the connection was working . It seems like i made an error in inheriting the properties from Ui_Dialog to Testdialog and/or in the way I want to execute the Dialog.My main program looks like : When I click on the bushbuttons nothing happens . It seems like the connection is not working . What did i do wrong ? How tho fix it ? What else should be improved ? The form UI_Test.py is nothing special , since it is automatically generated with QtDesigner and pyuic . So basically it should be ok ( although I do n't understand every detail about the code ) . In order to provide to a running example , here is the code :"
"I am getting started with Django , and I 'm trying to make a modular template , but I do n't know how . Now , I have the following files:1- base.html ( which provides basic layout for all the website ) :2- index.html ( main db read ) Finally , i wanted to add a third file , called menu.html which would contain the site menu . I wanted to add it in the base.html file . I 've been thinking about doing it in the following way , but i does n't work : Thanks so much for your help !"
"sometimes I have need to write class with static methods , however with possibility to initialized it and keep state ( object ) sth like : what I have now is : please do not write about differences between staticmethod and classmethod . I am interested if such decorator exists ( in more or less standard libs ) and moreover if above is suitable for PEP ."
"I 'm still very confused about how asyncio works , so I was trying to set a simple example but could n't achieve it.The following example is a web server ( Quart ) that receives a request to generate a large PDF , the server then returns a response before start processing the PDF , then starts processing it and will send the download link to an email later.How would I go about this ? in the above example I do n't want the 5 seconds to be waited before the return.I 'm not even sure if asyncio is what I need.And I 'm afraid that blocking the server app after the response has returned is not a thing that should be done , but not sure either.Also the pdf library is synchronous , but I guess that 's a problem for another day ..."
"I have lists inside a dictionary : The Number_of_lists is a variable set by the user . Without knowing beforehand the value set by the user , i would like to finally have a merged list of all dictionary lists . For example if Number_of_lists=3 and the corresponding lists are My_list [ 1 ] = [ ( 1,2,3 ) ] , My_list [ 2 ] = [ ( 4,5,6 ) ] , My_list [ 3 ] = [ ( 7,8,9 ) ] the result would be : where : All_my_lists= [ ( 1,2,3 ) , ( 4,5,6 ) , ( 7,8,9 ) ] .So what i 'm trying to do is automate the above procedure for all possible : Number_of_lists=n # where n can be any positive integerI 'm a bit lost up to now trying to use an iterator to add the lists up and always fail . I 'm a python beginner and this is a hobby of mine , so if you answer please explain everything in your answer i 'm doing this to learn , i 'm not asking from you to do my homework : ) EDIT @ codebox ( look at the comments below ) correctly pointed out that My_List as displayed in my code is in fact a dictionary and not a list . Be careful if you use any of the code ."
"( I am testing my abilities to write short but effective questions so let me know how I do here ) I am trying to train/test a TensorFlow recurrent neural network , specifically an LSTM , with some trials of time-series data in the following ndarray format : The the 1d portion of this 3darray holds the a time step and all feature values that were observed at that time step . The 2d block contains all 1d arrays ( time steps ) that were observed in one trial . The 3d block contains all 2d blocks ( trials ) recorded for the time-series dataset . For each trial , the time step frequency is constant and the window interval is the same across all trials ( 0 to 50 seconds , 0 to 50 seconds , etc . ) .For example , I am given data for Formula 1 race cars such as torque , speed , acceleration , rotational velocity , etc . Over a certain time interval recording time steps every 0.5 seconds , I form 1d arrays with each time step versus the recorded features recorded at that time step . Then I form a 2D array around all time steps corresponding to one Formula 1 race car 's run on the track . I create a final 3D array holding all F1 cars and their time-series data . I want to train and test a model to detect anomalies in the F1 common trajectories on the course for new cars . I am currently aware that the TensorFlow models support 2d arrays for training and testing . I was wondering what procedures I would have to go through in order the be able to train and test the model on all of the independent trials ( 2d ) contained in this 3darray . In addition , I will be adding more trials in the future . So what are the proper procedures to go through in order to constantly be updating my model with the new data/trials to strengthen my LSTM . Here is the model I was trying to initially replicate for a different purpose other than human activity : https : //github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition . Another more feasible model would be this which I would much rather look at for anomaly detection in the time-series data : https : //arxiv.org/abs/1607.00148 . I want to build a anomaly detection model that given the set of non-anomalous time-series training data , we can detect anomalies in the test data where parts of the data over time is defined as `` out of family . ''"
"Take this example : How and why does Python do this without an explicit print statement ? If I do the same thing in an IPython cell , only the last such value is actually printed on stdout in this way : Why does this happen ?"
"The normal way sitemap is used in Django is : and then in the model of School we define : In such implementation I have one About link for one school in sitemap.xmlThe problem is that my school has multiple pages : About , Teachers , Pupils and others and I would like all of the to be rendered is sitemap.xml What is the best approach to do it ?"
"According to this website , deep belief network is just stacking multiple RBMs together , using the output of previous RBM as the input of next RBM.In the scikit-learn documentation , there is one example of using RBM to classify MNIST dataset . They put a RBM and a LogisticRegression in a pipeline to achieve better accuracy.Therefore I wonder if I can add multiple RBM into that pipeline to create a Deep Belief Networks as shown in the following code.However , I discover that the more RBM I add to the pipeline , the less the accuracy is.1 RBM in pipeline -- > 95 % 2 RBMs in pipeline -- > 93 % 3 RBMs in pipeline -- > 89 % The training curve below shows that 100 iterations is just right for convergent . More iterations will cause over-fitting and the likelihood will go down again.Batch size = 10Batch size = 256 or aboveI have noticed one interesting thing . If I use a higher batch size , the performance of the network deteriorates a lot . When the batch size is above 256 , the accuracy drops to only less than 10 % . The training curve somehow does n't make sense to me , with first and second RBMs do n't learn much , but the third RBM suddenly learns quickly . It looks like 89 % is somehow the bottleneck for a network with 3 RBMs.I wonder if I am doing anything wrong here . Is my understanding of deep belief network correct ?"
"Model I-V.Method : Perform an integral , as a function of E , which outputs Current for each Voltage value used . This is repeated for an array of v_values . The equation can be found below.Although the limits in this equation range from -inf to inf , the limits must be restricted so that ( E+eV ) ^2-\Delta^2 > 0 and E^2-\Delta^2 > 0 , to avoid poles . ( \Delta_1 = \Delta_2 ) . Therefore there are currently two integrals , with limits from -inf to -gap-e*v and gap to inf . However , I keep returning a math range error although I believe I have excluded the troublesome E values by using the limits stated above . Pastie of errors : http : //pastie.org/private/o3ugxtxai8zbktyxtxuvgApologies for the vagueness of this question . But , can anybody see obvious mistakes or code misuse ? My attempt :"
"I have a timeseries data . Generating dataI want to create a zig-zag line connecting between the local maxima and local minima , that satisfies the condition that on the y-axis , |highest - lowest value| of each zig-zag line must exceed a percentage ( say 20 % ) of the distance of the previous zig-zag line , AND a pre-stated value k ( say 1.2 ) I can find the local extrema using this code : but I do n't know how to apply the threshold condition to it . Please advise me on how to apply such condition.Since the data could contain million timestamps , an efficient calculation is highly recommendedFor clearer description : Example output , from my data : My desired output ( something similar to this , the zigzag only connect the significant segments )"
"I have this code here : Which works , but I am not particularly fond of this line : In my opinion this would look cleaner if we could use a ternary operator : But that of course is n't there . What do you suggest ?"
"I am trying to find out operation applied on list . I have list/array name predictions and and executing following set of instruction.This code snippet is from a Udacity Machine Learning assignment that uses Numpy . It was used in the following manner : As pointed out by @ MosesKoledoye and various others , it is actually a Numpy array . ( Numpy is a Python library ) What does this line do ?"
"I 'm new to programming . I recently read that : Your program should have almost all functionality encapsulated in either functions or class methodsThis makes it seems as though I should not have both functions and methods . I have also read that methods should be short and simple.Well , I 've recently made a small program that downloads images from blogs . I used classes and the OOP approach because I need to inherit certain things.However , because the methods should be short and do one thing , my program can not do much.My question is , if I 'm trying to use a pure OOP approach , how is it possible to avoid writing functions ? My script follows basically this pattern : I also have other classes for different website APIs , and also a Downloader class that actually downloads the files to disk and to the proper directory . The issue is , right now all I have are these isolated classes and methods.I thought about creating a main function that can also use other functions , but again , I do n't think this is correct OOP.How can I actually get work done without writing functions ? ( The textbooks I 've read have said functions should n't be used in pure OOP , if I am using methods . )"
"I need to change Python 's encoding from cp1252 to UTF-8 . Using python 3.7.1 , Atom , and Atom script package for terminal . I have read about https : //www.python.org/dev/peps/pep-0540/ ( a solution to this ? I dont know how to implement or if useful ) I can not find a sound resolution . Currently it can not handle '\u2705 ' or others . When checking the Python file directory I found ... Python\Python37\lib\encodings\cp1252.py I expect my terminal to handle the characters and display them when using print ( )"
"In the python typing documentation it is written : Alternatively , annotate your generator as having a return type of either Iterable [ YieldType ] or Iterator [ YieldType ] : I wrote a very easy example of printing an infinite stream . I have a generator function which is passed to another function and then called.With mypy i get two errors : error : Iterator [ int ] not callableerror : Argument 1 to `` print_infinite_stream '' has incompatible type `` Callable [ [ int ] , Iterator [ int ] ] '' ; expected `` Iterator [ int ] '' I am confused why i am getting these errors as i worked according to the documentation and have the latest python ( 3.6.5 ) and mypy ( 0.590 ) installed . What is wrong here ?"
"I 'm looking for a library/module/package with which I could create and sign X.509 certificates , with the ability to conditionally add custom v3 extensions – which can be fairly complicated ; for example , this bletchful OpenSSL.cnf snippet used by Kerberos PKINIT , just to represent foo @ EXAMPLE.ORG : Out of everything I have found for languages I know ( that being Perl , Python , Ruby , PHP , Bash , and some C # ) , using openssl from command line with automatically generated .cnf files ... which is an ugly process . Is there a better way to do it ? ( Ruby 's 'openssl ' looked very nice at first , but then I got to PKINIT ... )"
"To make this clear , I will use an 4x4 grid to show where and how large I want my subplots . The counting goes like this : I want the top plot to be placed over 2 , 3 , 6 , and 7 . The two bottom plots are then 9 , 10 , 13 , 14 and 11 , 12 , 15 , 16 , respectively . In Matlab , you can use a subplot range , but I believe this only works for single row configurations.How can I do this in matplotlib ? Do I need a gridspec ? How would I use it then ? The examples are insufficient to understand how I should tackle this problem ."
"I have debugging a few similar solutions , but wondering if we could improve Trie Tree to partial match prefix ( in search method of class Trie , current search method only check if a full word is matched or not ) to even improve performance , which could return from a wrong path earlier ? I am not very confident for the idea , so seek for advice earlier.I post one of the similar solutions . Thanks.Given a 2D board and a list of words from the dictionary , find all words in the board.Each word must be constructed from letters of sequentially adjacent cell , where `` adjacent '' cells are those horizontally or vertically neighboring . The same letter cell may not be used more than once in a word.For example , Given words = [ `` oath '' , '' pea '' , '' eat '' , '' rain '' ] and board =Return [ `` eat '' , '' oath '' ]"
"I have spent some time learning Regular Expression , but I still do n't understand how the following trick works to match two words in different order.result ( https : //www.regex101.com/r/qW4rF4/1 ) I know the ( ? = . * ) part is called Positive Lookahead , but how does it work in this situation ? Any explanation ?"
"The example code for Google 's YouTube Data API is a piece of junk . It 's so complicated and tied to the oauth redirect flow that I ca n't use it . Trying to go raw with requests pip and not getting too far . I 've followed the instructions exactly ( as far as I can tell ) , with the following code : Obviously its not finished , but its dying on the metadata upload request with the following output : This error is not even listed in their `` Errors '' docs.What is wrong with my code ?"
I 'm comparing AES implementations in Python from the pycrypto and the cryptography.io library.The MWE prints : Why are the outputs different ?
"For every row in my dataframe , I need to create every combination of two values of column a from a three-day sliding window ending at that row . My dataframe is like this : Note that the time index is ragged ( inconsistent intervals between rows ) . The combinations should come out to be : I can do this easily enough without the window , just use itertools.combinations to generate every combination of two elements of column a with : but I need the windowed version for my application . My best bet so far is to use df.rolling . I can do simple things like summing the elements over a three day window with something like : but I ca n't seem to perform more complicated operations ( or return more complicated types than real numbers from an operation ) on the rolling window.QuestionHow do I use df.rolling to make combinations over my rolling window ? Or is there some other tool to do this ? AttemptsMy thought so far is that there is some way to use df.rolling and df.apply along with it.combinations to generate iterators for each window in my dataframe , and then plug that iterator into a new column of my dataframe . Something like : which gives a TypeError : TypeError : must be real number , not itertools.combinationsbecause df.rolling.apply requires that its argument return a single real value , not an object , nor a list.I also tried using it.combinations directly on the rolling window : which gives : KeyError : 'Column not found : 0'and if I select column a explicitly : I get : Exception : Column ( s ) a already selectedSo is there maybe a way to define a function that I can call with df.apply that plugs the iterator over my rolling window into a new column for each row of my dataframe ? Can I even operate on rows other than the current row in a function passed to apply ?"
"I am having trouble understanding the output of my function to implement multiple-ridge regression . I am doing this from scratch in Python for the closed form of the method . This closed form is shown below : I have a training set X that is 100 rows x 10 columns and a vector y that is 100x1.My attempt is as follows : For testing , I am running it with the first 5 lambda values.wList becomes 5 numpy.arrays each of length 10 ( I 'm assuming for the 10 coefficients ) .Here is the first of those 5 arrays : My question , and clarification : Should n't there be 11 coefficients , ( 1 for the y-intercept + 10 slopes ) ? How do I get the Minimum Square Error from this computation ? What comes next if I wanted to plot this line ? I think I am just really confused as to what I 'm looking at , since I 'm still working on my linear-algebra.Thanks !"
"Lets define simple class decorator function , which creates subclass and adds 'Dec ' to original class name only : Now apply it on a simple subclass definition : Now , if you try instantiate decorated MyClass , it will end up in an infinite loop : It seems , super ca n't handle this case and does not skip current class from inheritance chain.The question , how correctly use class decorator on classes using super ? Bonus question , how get final class from proxy-object created by super ? Ie . get object class from super ( Base , self ) .__init__ expression , as determined parent class defining called __init__ ."
"I have a ( 21 x 25 ) 2D array which contains two discrete values , `` 1 '' & `` 2 '' . The values are shown below : If I plot this using pcolor , the resulting figure looks like this : I want to draw the outlines of the grid squares where value == 2 : It seems like the outline is n't uniform ( the lines have different shades ) , and I ca n't set the grid line colors by changing edgecolor ."
"Recently , I have noticed an inconsistency while concatenating lists.So if I use the + operator , it does n't concatenates list with any object of different type . For example , But , if I use the += operator , it neglects the type of the object . For example , So , is it just the semantics of the language or some other reason ?"
"I have a subprocess that either quits with a returncode , or asks something and waits for user input.I would like to detect when the process asks the question and quit immediately . The fact that the process asks the question or not is enough for me to decide the state of the system.The problem is that I can not read the question because the child process probably does not flush standard output . So I can not rely on parsing subprocess.Popen ( ) .stdout : when trying to read it , well , it blocks because input is being read first.A bit like thisOf course , the actual subprocess is a third party binary , and I can not modify it easily to add the necessary flush calls , which would solve it.I could also try the Windows equivalent of unbuffer ( What is the equivalent of unbuffer program on Windows ? ) which is called winpty , which would ( maybe ) allow me to detect output and solve my current issue , but I 'd like to keep it simple and I 'd like to solve the standard input issue first ... I tried ... well , lots of things that do n't work , including trying to pass a fake file as stdin argument , which does n't work because subprocess takes the fileno of the file , and we can not feed it rubbish ... Using communicate with a string does n't work either , because you can not control when the string is read to be fed to the subprocess ( probably through a system pipe ) .Those questions were promising but either relied on standard output , or only apply to LinuxDetecting when a child process is waiting for inputHow can I know whether my subprocess is waiting for my input ? ( in python3 ) What I 'm currently doing is running the process with a timeout , and if the timeout is reached , I then decide that the program is blocked . But it costs the timeout waiting time . If I could decide as soon as stdin is read by the subprocess , that would be better.I 'd like to know if there 's a native python solution ( possibly using ctypes and windows extensions ) to detect read from stdin . But a native solution that does n't use Python but a non-Microsoft proprietary language could do ."
"When I run the following code , it only seems to be downloading the first little bit of the file and then exiting . Occassionally , I will get a 10054 error , but usually it just exits without getting the whole file . My internet connection is crappy wireless , and I often get broken downloads on larger files in firefox , but my browser has no problem getting a 200k image file . I 'm new to python , and programming in general , so I 'm wondering what nuance I 'm missing ."
"I 've found that I 'm using this pattern a lot : so I 've decided to put in a function in a file that has many such small utilities : The thing is , __file__ returns the current file and therefore the current folder , and I 've missed the whole point . I could do this ugly hack ( or just keep writing the pattern as is ) : and then the call to it will look like this : but I 'd prefer it if I had a way of getting the __file__ of the function that 's one level up in the stack . Is there any way of doing this ?"
"Function combn ( x , m ) in R generates all combination of the elements x taken m at a time . For instance , How can I have the same result in python ? I know scipy.misc.comb gives just the result not the list and I also read this article which seems different and needs a given list at first not just simply two integer numbers ."
"I just wondered , how to hide special methods in python* ? Especially I am using an interactive python interpreter with tab-completion , and I would like to display only the methods my modules expose ... thanks , / myyn /* ( at least from the user , who uses a python shell ) it looks like this now : and I wish it looked like :"
"I would like to allow a HyperLinkRelatedField to accept just an id instead of requiring a hyperlink to create a new instance of an object , but for get requests I would like to return the hyperlink not just an id but it appears to be either one or the other . Is this possible ? GET Request for Comment returns ( This is perfect ) : POST Request requires : I want to also be able to pass :"
"I am working on a fork of a python projet ( tryton ) which uses setuptools for packaging . I am trying to extend the server part of the project , and would like to be able to use the existing modules with my fork.Those modules are distributed with setuptools packaging , and are requiring the base project for installation.I need a way to make it so that my fork is considered an acceptable requirement for those modules.EDIT : Here is what I used in my setup.py : The modules I want to be able to install have those requirements : As it is , with my package installed , trying to install a module triggers the installation of the trytond package ."
I have a dataframe like this : and I want to get the cumulative number of unique values over time for the 'company ' column . So if a company appears at a later date they are not counted again.My expected output is : I 've tried : but this double counts if the same company appears on a different date .
I am going through this wonderful tutorial.I downloaded a collection called book : and imported texts : I can then run commands on these texts : How can I run these nltk commands on my own dataset ? Are these collections the same as the object book in python ?
"I 'm looking for an efficient python implementation of Somers 'D , for which I need to compute the number of concordant , discordant and tied pairs between two random variables X and Y . Two pairs ( X_i , Y_i ) , ( X_j , Y_j ) are concordant if the ranks of both elements agree ; that is , x_i > x_j and y_i > y_j or x_i < x_j and y_i < y_j . Two pairs are called discordant if the ranks of both elements do not agree : x_i > x_j and y_i < y_j or x_i < x_j and y_i > y_j . Two pairs are said to be tied in X ( Y ) when x_i = x_j y_i = y_j . Somers 'D is then computed as D = ( N_C - N_D ) / ( N_tot - N_Ty ) . ( See : https : //en.wikipedia.org/wiki/Somers % 27_D . ) I wrote a naive implementation using nested for-loops . Here , S contains my predictions and Y the realized outcomes.Obviously , this is gon na be very slow when ( Y , S ) have a lot of rows . I stumbled upon the use of bisect while searching the net for solutions : This is very efficient , but only works for binary variables Y . Now my question is : how can I implement the concordance_computer in an efficient way for ungrouped Y ?"
"I use the following code to load 24-bit binary data into a 16-bit numpy array : I imagine that it 's possible to improve the speed efficiency , but how ?"
"I 'm using python to work with large-ish ( approx 2000 x 2000 ) matrices , where each I , J point in the matrix represents a single pixel.The matrices themselves are sparse ( ie a substantial portion of them will have zero values ) , but when they are updated they tend to be increment operations , to a large number of adjacent pixels in a rectangular 'block ' , rather than random pixels here or there ( a property i do not currently use to my advantage.. ) . Afraid a bit new to matrix arithmetic , but I 've looked into a number of possible solutions , including the various flavours of scipy sparse matrices . So far co-ordinate ( COO ) matrices seem to be the most promising . So for instance where I want to increment one block shape , I 'd have to do something along the lines of : In the future , i 'd like to have a richer pixel value representation in anycase ( tuples to represent RGB etc ) , something that numpy array doesnt support out of the box ( or perhaps I need to use this ) . Ultimately i 'll have a number of these matrices that I would need to do simple arithmitic on , and i 'd need the code to be as efficient as possible -- and distributable , so i 'd need to be able to persist and exchange these objects in a small-ish representation without substantial penalties . I 'm wondering if this is the right way to go , or should I be looking rolling my own structures using dicts etc ?"
"Correcting one word spelling mistakes ( both non-word & real-word mistakes ) is easy : Where w is the incorrectly spelled word and c is the candidate we 're trying to match , such that the candidate is a one word token.But in Google , when you enter something like spelligncheck , it corrects the word into two different words . Now , P ( w|c ) is easy here , if i use levenshtein distance . But that means i ca n't have one word ( one token , rather ) candidates anymore . So this will increase the size of my dictionary exponentially.Moreover when I enter app le Google corrects it to apple ... So what is the best way of doing multiple word spelling correction , given a one-token dictionary ?"
"So I understand that if I do the followingBut what if following is my python scriptWhat does the above thing do ? Is it taken as comment ? Why is it not a syntax error ? Similarly , if I doHow are the above two scripts interpreted ? Thanks"
"Is there a way , without use of loops , to reindex a DataFrame using a dict ? Here is an example : I want to apply something efficient to df for obtaining : Speed is important , as the index in the actual DataFrame I am dealing with has a huge number of unique values . Thanks"
"So I am playing with docker for the first time on my mac . I used boot2docker through the standard tutorial and I am starting a prompt in an ubuntu image . When in docker I started my first experiment , to see if the performance would go down . From the command line I would use the python timeit module to quickly check some basic performance measures . Mac Python ResultsDocker Python ResultsIt seems strange that the docker ubuntu , that is running on top of my mac , is actually running python code faster than the python on mac . Is there any reason for why this might be ? EditsI can confirm that both python versions are running in 64 bit . Mac PythonUbuntu Python"
"I work on a service that will handle Alexa voice intents . I need to verify the signature of each request and I almost succeed . The only part that is not working is the validation of certificates chain.From the documentation I know that : This certificate chain is composed of , in order , ( 1 ) the Amazon signing certificate and ( 2 ) one or more additional certificates that create a chain of trust to a root certificate authority ( CA ) certificate.My code looks like this : I receive the following error : I do n't know what I did wrong , maybe there is someone who already implemented this and can drop a hint ."
"I have a response method that looks like this for my Lambda functions : When I test my endpoint with an OPTIONS request from Postman , I get a 500 internal server error . If I test it from the the API Gateway console , I get this additionally : I 'm not really sure what I 'm doing wrong . I think I am returning all the right headers . Any help is appreciated ."
"Does Python have augmented assignment statements corresponding to its boolean operators ? For example I can write this : or this : Is there something I can write in place of this : To avoid writing `` x '' twice ? Note that I 'm aware of statements using & = , but I was looking for a statement that would work when y is any type , not just when y is a boolean ."
"I 'm attempting to use the Python library for Pub/Sub , but I keep getting this error : TypeError : Incorrect padding . Some quick googling revealed this issue : https : //github.com/GoogleCloudPlatform/google-cloud-python/pull/2527However , this does n't resolve the issue - in fact , printing out the data revealed that the data was not even base64 encoded at all , and setting data = raw_data in the library resolved the issue.We 're sending the message from Java , here is the code we 're using : however , the same thing happens attempting to send a message through the console.Has something changed recently to mean that data is no longer base64 encoded ?"
"With Terminator , a user can define layouts within the configuration file . With these layouts , the user can set a command to be executed at start-up . So , for example , one could create a layout in which a terminal automatically executed ls like this ( note the bash command to avoid the terminal closing ) : Now , how can I make Terminator load a Python Virtual Environment instead ? Keeping , of course , the bash console active with the environment loaded.NoteThe trivial way : command = `` workon my_env ; bash '' or its source my_env/bin/activate equivalent ( without using virtualenvwrapper ) , wont work ."
"ExampleThese two timezones have different names but represent the same thing , howeverb==c returns false b.zone is different than c.zoneIs there any way to see that b is in reality equal to c ? The concrete problem is that I have to convert the timezone of a pandas data frame , but only if this zone is different than let 's say c. The original timezone might be b and in this case I do not want to convert as it would be a lost of time to convert b into c ( since they represent the same time zones at the end ... . ) Thanks for any help.Update : changed 'CET ' into 'Europe/Rome ' to make sure that the timezones are the same in the example , using the feedback from an answer"
I 'm trying to append data to a fits file using astropy.io.Here is an example of my code : The error I get is Could this be a problem with the astropy.io version I 'm using ? Is there an easier way to add extensions or columns to a fits file using astropy.io ? Any help would be appreciated .
"The documentation on nonlinsolve gives this example : but even in the live shell on their website , that throws an error : How can I use nonlinsolve to solve a system of equations numerically ? I know I can use ufuncify to convert the equations into a system that scipy.optimize.fsolve can solve , but I would rather avoid those couple of lines of boilerplate and just use SymPy directly.According to the SymPy documentation on solve , using solve is not recommended . For nonlinear systems of equations , the documentation recommends sympy.solvers.solveset.nonlinsolve , which is what I 'm trying to use here ."
"I 'm kind of new to Python and its MySQLdb connector.I 'm writing an API to return some data from a database using the RESTful approach . In PHP , I wrapped the Connection management part in a class , acting as an abstraction layer for MySQL queries.In Python : I define the connection early on in the script : con = mdb.connect ( 'localhost ' , 'user ' , 'passwd ' , 'dbname ' ) Then , in all subsequent methods : etc.I use mdb.cursors.DictCursor because I prefer to be able to access database columns in an associative array manner.Now the problems start popping up : in one function , I issue an insert query to create a 'group ' with unique 'groupid'.This 'group ' has a creator . Every user in the database holds a JSON array in the 'groups ' column of his/her row in the table . So when I create a new group , I want to assign the groupid to the user that created it.I update the user 's record using a similar function.I 've wrapped the 'insert ' and 'update ' parts in two separate function defs . The first time I run the script , everything works fine.The second time I run the script , the script runs endlessly ( I suspect due to some idle connection to the MySQL database ) .When I interrupt it using CTRL + C , I get one of the following errors : '' 'Cursor ' object has no attribute 'connection ' '' '' commands out of sync ; you ca n't run this command now '' or any other KeyboardInterrupt exception , as would be expected.It seems to me that these errors are caused by some erroneous way of handling connections and cursors in my code.I read it was good practice to use with con : so that the connection will automatically close itself after the query . I use 'with ' on 'con ' in each function , so the connection is closed , but I decided to define the connection globally , for any function to use it . This seems incompatible with the with con : context management . I suspect the cursor needs to be 'context managed ' in a similar way , but I do not know how to do this ( To my knowledge , PHP does n't use cursors for MySQL , so I have no experience using them ) .I now have the following questions : Why does it work the first time but not the second ? ( it will however , work again , once , after the CTRL + C interrupt ) .How should I go about using connections and cursors when using multiple functions ( that can be called upon in sequence ) ?"
"I 'm working on an iPython project with Pandas and Numpy . I 'm just learning too so this question is probably pretty basic . Lets say I have two columns of dataI want to transform this data of the form.Then I want to take a three column versionand turn it into I 'm very new to Pandas and Numpy , how would I do this ? What functions would I use ?"
"I would like to check if a value exists in every list . The following returns True as expected , but seems un-pythonic . What is the correct/more elegant way to do this ?"
I am working on an image processing script . I need to let the user specify how to remap some classes in an image via a text file . The syntax in this file should be simple and self-evident . What I thought of doing is to get the user to write the string version of a dictionary : and then transform it into a real dictionary ( this is the missing link ) : The remapping of the classes of the image would then be done like this : Is there a simple way to do this transformation from the `` string version '' to a real dictionary ? Can you think of an easier/alternative one-line syntax that the user could use ?
"Im writing an compiler in Python , using llvmlite to generate intermediate LLVM IR . Lexer and parser are finished , now im doing code generation . The compiler will be dynamic and weakly typed , so i will need to do somethings at runtime , like allocation . For this , i 've already implemented some functions in C , and now i want to call these functions using builder.call from llvmlite . I have not found documentation or examples of how to do this.This function its just an simple example , the real ones are much larger.C : Python : I could write the functions directly using llvmlite builders , but will be much quick , cleaner and easy do it in C. Any help are welcome !"
"Given an nxn array A of real positive numbers , I 'm trying to find the minimum of the maximum of the element-wise minimum of all combinations of three rows of the 2-d array . Using for-loops , that comes out to something like this : In the case for n = 100 , the output should be this : I have a feeling though that I could do this much faster using Numpy vectorization , and would certainly appreciate any help on doing this . Thanks ."
I want to replace negative values with nan for only certain columns . The simplest way could be : df could have many columns and I only want to do this to specific columns . Is there a way to do this in one line ? Seems like this should be easy but I have not been able to figure out .
"I am deploying a django project on apache2 using mod_wsgi , but the problem is that the server dont serve pages and it hangs for 10 minute before giving an error : This is my site-available/000-default.conf : settings.pywsgi.pyProject structureIn the arTfact_webSite/urls.pyIn the website/urls.pyam I doing something wrong here ?"
"I have an animation job that takes a long time and a lot of memory . I want to submit it to a TORQUE queue , but I ca n't use X on those machines . Since `` default '' matplotlib requires X , I need to import it like this : What 's passed to the use ( ) method is called a backend . The documentation on backends can be found here.Which backend should I use if I 'm using matplotlib.animate ( ) and want to save the animation as an mp4 or theora ?"
"I often use Python 's print statement to display data . Yes , I know about the ' % s % d ' % ( 'abc ' , 123 ) method , and the ' { } { } '.format ( 'abc ' , 123 ) method , and the ' '.join ( ( 'abc ' , str ( 123 ) ) ) method . I also know that the splat operator ( * ) can be used to expand an iterable into function arguments . However , I ca n't seem to do that with the print statement . Using a list : Using a tuple : Am I missing something ? Is this simply not possible ? What exactly are the things that follow print ? The documentation says that a comma-separated list of expressions follow the print keyword , but I am guessing this is not the same as a list data type . I did a lot of digging in SO and on the web and did not find a clear explanation for this.I am using Python 2.7.6 ."
"This is basically my function for writing words letter by letter , it works flawlessly on IDLE when testing it , however , when I run it normally ( and it opens up the command prompt ) , what I 'm trying to write stays invisible , and then suddenly displays the words at once . Did I type something wrong , or is it command prompt 's issue ? I 'm using windows 10 ."
"https : //www.tensorflow.org/versions/r1.6/api_docs/python/tf/gradientsIn the documentation for tf.gradients ( ys , xs ) it states that Constructs symbolic derivatives of sum of ys w.r.t . x in xsI am confused about the summing part , I have read elsewhere that this sums the derivatives dy/dx across the batch for every x in the batch . However , whenever I use this I fail to see this happening . Take the following simple example : This gives the following output : This is the output I would expect , simply the derivative dy/dx for every element in the batch . I do n't see any summing happening . I have seen in other examples that this operation is followed by dividing by the batch size to account for tf.gradients ( ) summing the gradients over the batch ( see here : https : //pemami4911.github.io/blog/2016/08/21/ddpg-rl.html ) . Why is this necessary ? I am using Tensorflow 1.6 and Python 3 ."
"The following program : fails with the following error : Why do I get the error ( on cPython 2.7 and 3.2 on Linux x64 ) , and why does it vanish if I uncomment the third line ?"
"I 'm trying to use OpenCV with PySide on a Mac with Mountain Lion . The Homebrew formula works fine , but the python binding files do not show up.The /usr/local/lib/python2.7/site-packages does not contain any file named cv . *.I did brew update and the output of brew doctor is below : Warning : `` config '' scripts exist outside your system or Homebrew directories . ./configure scripts often look for *-config scripts to determine if software packages are installed , and what additional flags to use when compiling and linking . Having additional scripts in your path can confuse software installed via Homebrew if the config script overrides a system or Homebrew provided script of the same name . We found the following `` config '' scripts : Is there anything I am missing to install OpenCV properly ? Thanks"
"I was looking forward to using the somewhat new typing.NamedTuple class , which allows the creation of named tuple classes using the usual Python class syntax ( including the ability to add docstrings and methods , provide default values , type hints , etc etc ) .However : the class at the bottom is producing the following error message : From this I gather just what it says : overriding __new__ is a still no-no . This is very disappointing.The `` old way '' of going about this would be to inherit from a named tuple class , but this requires what I consider to be some ugly boilerplate code : Is there some other alternate way I can cast the zero , width , and precision arguments below to int prior to the creation of the named tuple , but still using the same class creation syntax ? Or am I stuck using the old way ?"
"Title says it all , using a relatively vanilla Visual Studio 2015 Community Edition , I ca n't make a new Django project from the `` File > New Project ... '' menu.Thus far the only additions I 've made are installing the Python 2.7.10 runtime , as recommended my Microsoft , and then installed django via pip inside Visual Studio . But I keep getting the error below : Any tips ? For those wondering , if you type Crtl+C while a dialog has focus , it makes a pretty ASCII version of the dialog for pasting ."
"I have a list of the form 'Season Year ' : [ 'Fall 2014 ' , 'Spring 2015 ' , 'Fall 2008 ' , 'Spring 2008 ' ] And when sorted , the list should look like this : [ 'Spring 2015 ' , 'Fall 2014 ' , 'Spring 2008 ' , 'Fall 2008 ' ] Hence , the rules for sorting are : First sort by yearSort by season : 'Spring ' then 'Fall ' I currently have a function called lcmp that I use on my list like this : myList.sort ( lcmp ) .This works for sorting by year , but does not work for the season , even though I have specified that type of sorting . Why is this happening ?"
I 'm playing around with Map Reduce in MongoDB and python and I 've run into a strange limitation . I 'm just trying to count the number of `` book '' records . It works when there are less than 100 records but when it goes over 100 records the count resets for some reason.Here is my MR code and some sample outputs : MR output when record count is 99 : MR output when record count is 101 : Any ideas ?
"I am doing a News recommendation system and I need to build a table for users and news they read . my raw data just like this : and I use spark-SQL do explode and one hot encoding , After that , my data become : But one id have multiple rows , so I want to groupBy ( 'uuid ' ) and then add these vectors . But just use groupBy and then add will have error . How could I do that ?"
"Is the following code doing anything wrong with with statement and exception handling of python3 ? If no , what is the correct way of writing my expected output ? and the output isand I expect the output is : which I believe other people write similarly in the hope that the file would be closed when an exception was raised during the processing of the file.My python3 version is :"
"Why does while True : not affect CPU too much in some cases and in other does ? How does one prevent that ? I wrote a simple program and was worried thatwould consume all of computational resources ( or at least the cap given by OS ) for checking the if statement ( most of the time it 's false so it should loop quite rapidly ) so I initially put system.sleep ( sleeptime ) there to avoid that but I was surprised when I removed it it had no measurable effect on system resources . Why ? That brought me to the question how often does the system check the condition.Naively : If i do something likeon my machine I get 11 seconds and 50 % CPU load while running . Why in the second example I get 50 % load and nearly nothing on the first one ? Is it because of the i += 1 ? Can one profile the while loop ( how often it loops and how much resources it consumes ) without slowing it down or causing overhead and hence making the measured value irrelevant ? I am doing this on Python 3.6.2 ( v3.6.2:5fd33b5 , Jul 8 2017 , 04:57:36 ) [ MSC v.1900 64 bit ( AMD64 ) ] on win32"
"I have two list , one reference and one input listI want to sort Input list , in the order as that of Ref . If some element is missing in Input list , it can skip and go for the other element.Hence sorted Input list , based on Ref list will be like this"
"I am subscribing to topic `` /camera/depth/points '' and message PointCloud2 on a turtlebot ( deep learning version ) with ASUS Xtion PRO LIVE camera.I have used the python script below under the gazebo simulator environment and i can receive x , y , z and rgb values successfully.However , when i run it in the robot , the rgb values are missing.Is this a problem of my turtlebot version , or camera or is it that i have to specify somewhere that i want to receive PointCloud2 type= '' XYZRGB '' ? or is it a sync problem ? Any clues please thanks !"
"I have some tuple in python.And capacity limit , for example , is 5.I want to split tuple in subtuples limited by sum of them elements : For example : I am looking for a nice expressive solution of this task preferable in functional style of programming ( using itertools.dropwhile for example or something like that )"
"I am using the App Engine Bulk loader ( Python Runtime ) to bulk upload entities to the data store . The data that i am uploading is stored in a proprietary format , so i have implemented by own connector ( registerd it in bulkload_config.py ) to convert it to the intermediate python dictionary.To convert this neutral python dictionary to a datastore Entity , i use a custom post import function that i have defined in my YAML.Note : I am not using entity_instance , bulkload_state in my feature_post_import function . I am just creating new data store entities ( based on my input_dict ) , and returning them.Now , everything works great . However , the process of bulk loading data seems to take way too much time . For e.g . a GB ( ~ 1,000,000 entities ) of data takes ~ 20 hours . How can I improve the performance of the bulk load process . Am i missing something ? Some of the parameters that i use with appcfg.py are ( 10 threads with a batch size of 10 entities per thread ) .Linked a Google App Engine Python group post : http : //groups.google.com/group/google-appengine-python/browse_thread/thread/4c8def071a86c840Update : To test the performance of the Bulk Load process , I loaded entities of a 'Test ' Kind . Even though this entity has a very simple FloatProperty , it still took me the same amount of time to bulk load those entities . I am still going to try to vary the bulk loader parameters , rps_limit , bandwidth_limit and http_limit , to see if i can get any more throughput ."
"What is wrong with the following code ? The tf.assign op works just fine when applied to a slice of a tf.Variable if it happens outside of a loop . But , in this context , it gives the error below.results in :"
"In Python , I know that the value __hash__ returns for a given object is supposed to be the same for the lifetime of that object . But , out of curiosity , what happens if it is n't ? What sort of havoc would this cause ? I know __contains__ and __getitem__ would behave strangely , and dicts and sets would act odd because of that . You also might end up with `` orphaned '' values in the dict/set.What else could happen ? Could it crash the interpreter , or corrupt internal structures ?"
"Most commonly misspelled English words are within two or three typographic errors ( a combination of substitutions s , insertions i , or letter deletions d ) from their correct form . I.e . errors in the word pair absence - absense can be summarized as having 1 s , 0 i and 0 d.One can fuzzy match to find words and their misspellings using the to-replace-re regex python module.The following table summarizes attempts made to fuzzy segment a word of interest from some sentence : Regex1 finds the best word match in sentence allowing at most 2errorsRegex2 finds the best word match in sentence allowing atmost 2 errors while trying to operate only on ( I think ) whole wordsRegex3 finds the best word match in sentence allowing atmost 2 errors while operating only on ( I think ) whole words . I 'm wrong somehow.Regex4 finds the best word match in sentence allowing atmost 2 errors while ( I think ) looking for the end of the match to be a word boundaryHow would I write a regex expression that eliminates , if possible , false positive and false negative fuzzy matches on these word-sentence pairs ? A possible solution would be to only compare words ( strings of characters surrounded by white space or the beginning/end of a line ) in the sentence to the word of interest ( principal word ) . If there 's a fuzzy match ( e < =2 ) between the principal word and a word in the sentence , then return that full word ( and only that word ) from the sentence.CodeCopy the following dataframe to your clipboard : Now useTo load the table into your environment ."
Problem Statement : To classify a text document to which category it belongs and also to classify up to two levels of the category.Sample Training Set : Initial Attempt : I tried to create a classifier model which would try to classify the Category using Random forest method and it gave me 90 % overall.Code1 : Problem : I want to add two more level to the model they are Level1 and Level2 the reasons for adding them is when I ran classification for Level1 alone I got 96 % accuracy . I am stuck at splitting training and test dataset and to train a model which has three classifications.Is it possible to create a model with three classification or should I create three models ? How to split train and test data ? Edit1 : import string import codecs import pandas as pd import numpy as np
"I 'm using Boost Python to wrap some C++ functions that I 've created . One of my C++ functions contains 22 arguments . Boost complains when I try to compile my solution with this function , and I 'm trying to figure out if it is just because this function has too many arguments.Does anyone know if such a limit exists ? I 've copied the error I 'm getting below , not the code because I figure someone either knows the answer to this or not - and if there is no limit then I 'll just try to figure it out myself . Thanks very much in advance ! Here is a copy of the beginning of the error message I receive ... And eventually I get about a hundred copies of error messages very much resembling this one :"
"I have an error occurring in a very similar way to this SO question . The solution of simply installing rpy2 using conda does n't work.The key difference in my case is that rpy2 worked properly before I updated to Mac OSX 10.11 ( El Capitan ) . My Python version is Python 2.7.10 , conda : 3.18.4 , R : R version 3.2.2 ( 2015-08-14 ) -- `` Fire Safety and all were installed using the anaconda distribution.I get the following error : when trying to load the rpy2.ipython extension : I have a hunch it is a fix similar to this question dealing with loading the rJava R package ."
"Now that Django supports the DateRangeField , is there a 'Pythonic ' way to prevent records from having overlapping date ranges ? Hypothetical use caseOne hypothetical use case would be a booking system , where you do n't want people to book the same resource at the same time.Hypothetical example code"
"I have the fisher 's linear discriminant that i need to use it to reduce my examples A and B that are high dimensional matrices to simply 2D , that is exactly like LDA , each example has classes A and B , therefore if i was to have a third example they also have classes A and B , fourth , fifth and n examples would always have classes A and B , therefore i would like to separate them in a simple use of fisher 's linear discriminant . Im pretty much new to machine learning , so i dont know how to separate my classes , i 've been following the formula by eye and coding on the go . From what i was reading , i need to apply a linear transformation to my data so i can find a good threshold for it , but first i 'd need to find the maximization function . For such task , i managed to find Sw and Sb , but i do n't know how to go from there ... Where i also need to find the maximization function.That maximization function gives me an eigen value solution : What i have for each classes are matrices 5x2 of 2 examples . For instance : I tried finding Sw for the example above like this : As for Sb , i tried like this : The problem is , i have no idea what else to do with my Sw and Sb , i am completely lost . Basically , what i need to do is get from here to this : How for given Example A and Example B , do i separate a cluster only for classes As and only for classes b"
"Is there a better way to iterate to my dictionary data without using 3 nested for loops like what I am currently doing given this data below ? Btw , i am using python 2.6 . Here is the 3 for loops current code : EDITED : valid data values"
"I 'm trying to implement a django custom middleware that gives me access to the request object wherever I am in my project , based in the one suggested here . That article was written a long time ago , and django 1.5 does not have the library thread_support where it was back then . What alternative should I use to accomplish a thread safe local store to store the request object ? This is the code in the custom middleware : And , of course , it raises an exception : EDIT : I found out a working fix : Now I have a question : Does it leads to a memory leak ? what happens with _active ? is it cleaned when the request dies ? Anyway , There 's a valid answer already posted . I 'm going to accept it but any other ( better , if possible ) solution would be very welcome ! Thanks !"
"Let 's say I have the following pandas DataFrame : When I try to group on the team column and perform an operation I get a SettingWithCopyWarning : If I uncomment the line generating a copy of the sub-DataFrame , I do n't get the error . Is this generally best practice to avoid this warning or am I doing something wrong ? Note I do n't want to edit the original DataFrame df . Also I know this example can be done a better way but my use case is much more complex and requires grouping an original DataFrame and performing a series of operations based on a different DataFrame and the specs of that unique group ."
I am unable to read the column of another table which is joined . It throws AttributeErrorNow the query isNow I get an error AttributeError : 'Component ' object has no attribute 'group_id '
"I want to create a class that wraps an int and allows some things not normally allowed with int types . Here is my code : The output was.So , the first print statement ran nicely , and gave what I expected , but the second one gave an error . I think this is because the first one is using tInt 's add function because a appeared before + `` 5 '' and the second one used the string `` 5 '' 's add function first because it appeared first . I know this but I do n't really know how to either force a 's add function or allow the tInt class to be represented as a string/int/etc.. when a normal type appears before it in an operation ."
"Imagine I have a dictionary / hashtable of pairs of strings ( keys ) and their respective probabilities ( values ) : The hashes hashtable will look something like this : Imagine that this is the input hashtable that I 'll read from CSV file with the first and second column being the word pairs ( keys ) of the hashtable and the third column the probabilitiesIf I were to put the probabilities into some sort of numpy matrix , I would have to do this from the hashtable : Is there another way to get the prob into the |N| * |M| matrix from the hashtable without doing a nested loop through the m_vocab and n_vocab ? ( Note : I 'm creating random words and random probabilities here but imagine I have read the hash table from a file and it 's read into that hashtable structure ) Assume both scenarios , where : The hashtable is from a csv file ( @ bunji 's answer resolves this ) The hashtable comes from a pickled dictionary . Or that the hashtable was computed some other way before reaching the part where converting it into a matrix is necessary . It is important that the final matrix needs to be queryable , the following is n't desirable : The resulting matrix/dataframe should be queryable , i.e . is able to do something like :"
"I have Scrapy ( version 1.0.3 ) spider in which I extract both some data from web page and I also download file , like this ( simplified ) : in pipelines.py I just override FilePipeline to change the name of the file : in items.py I have : in settings.py I have : now in output csv file I get something like this : It looks like that empty lines ( having just comma ) represents downloaded file and I would like to know or get advice how to prevent such lines to be in output csv file . ( files are saved into folder ) .In Scrapy settings I found out about FEED_STORE_EMPTY ( which is by default false , i.e . it should not export empty feeds ) but this not relates to files I guess.I have feeling that this have to do something with pipelines but I ca n't figure out how to do it.any help would be appreciated"
"When the user saves a file I want a check to happen prior to saving . If the check fails then it does n't save . I got this working with mSceneMessage and kBeforeSaveCheck , but I do n't know how to customize the pop-up message when it fails . Is this possible ? Right now it displays File operation cancelled by user supplied callback ."
"I am starting to play around with creating polar plots in Matplotlib that do NOT encompass an entire circle - i.e . a `` wedge '' plot - by setting the thetamin and thetamax properties . This is something I was waiting for for a long time , and I am glad they have it done : ) However , I have noticed that the figure location inside the axes seem to change in a strange manner when using this feature ; depending on the wedge angular aperture , it can be difficult to fine tune the figure so it looks nice.Here 's an example : The labels are in awkward locations and over the tick labels , but can be moved closer or further away from the axes by adding an extra labelpad parameter to set_xlabel , set_ylabel commands , so it 's not a big issue.Unfortunately , I have the impression that the plot is adjusted to fit inside the existing axes dimensions , which in turn lead to a very awkward white space above and below the half circle plot ( which of course is the one I need to use ) .It sounds like something that should be reasonably easy to get rid of - I mean , the wedge plots are doing it automatically - but I ca n't seem to figure it out how to do it for the half circle . Can anyone shed a light on this ? EDIT : Apologies , my question was not very clear ; I want to create a half circle polar plot , but it seems that using set_thetamin ( ) you end up with large amounts of white space around the image ( especially above and below ) which I would rather have removed , if possible.It 's the kind of stuff that normally tight_layout ( ) takes care of , but it does n't seem to be doing the trick here . I tried manually changing the figure window size after plotting , but the white space simply scales with the changes . Below is a minimum working example ; I can get the xlabel closer to the image if I want to , but saved image file still contains tons of white space around it.Does anyone knows how to remove this white space ? EDIT 2 : Added background color to figures to better show the boundaries , as suggested in ImportanteOfBeingErnest 's answer ."
"For example , if you take the models : I know I do n't need to include id in these models , but I 've done so to make it clearer.In this example , sometimes you would want to simply return a list of Regions . Other times you 'd want to return a list of Regions with a list of each Region 's Companies under each Region.You would also , I imagine , want even more detail where you have a list of Regions , their Company children , and each Company 's Staff children.What 's the best way to handle these different levels of depth/detail as far as the rest framework views are concerned . How do people normally deal with this ? By this I mean , what kind of naming conventions would you use when you have say three views returning the same thing at the top level , with the only difference being how many levels of nesting they include ?"
How would I collapse run-on whitespace in python ?
"I must be really misunderstanding something with the GenericRelation field from Django 's content types framework.To create a minimal self contained example , I will use the polls example app from the tutorial . Add a generic foreign key field into the Choice model , and make a new Thing model : With a clean db , synced up tables , and create a few instances : So far so good - the relation works in both directions from an instance . But from a queryset , the reverse relation is very weird : Why the inverse relation gives me again the id from the thing ? I expected instead the primary key of the choice , equivalent to the following result : Those ORM queries generate SQL like this : The Django docs are generally excellent , but I ca n't understand why the second query or find any documentation of that behaviour - it seems to return data from the wrong table completely ?"
Let 's have 2 listsresult : Can not use zip ( ) because it shorten result to the smallest list . I need also a list at the output not an iterable .
"I want to serialize/deserialize md5 context . But I do n't know how to do it in Python.Pseudocode of what I want to do.There are C++ libraries for this . Is there one for Python ? Why does n't the md5 library support it ? Are there security concerns ? Thanks.Edited : I want to do this because for example , an HTTP server wants to accept streaming data in different HTTP requests . It would be convenient to serialize md5 context somehow between requests ."
"Suppose I have the following ( in Python 3 and SQLAlchemy ) : This feels like a 'non-idiomatic ' approach , because it seems unlikely to leverage the database to search for the given chapter . In the worst case , it seems like n calls to the db will be made to check for chapter titles , though my limited understanding of SQLAlchemy suggests this can be configured around . What I do n't know is if there is a way to initiate a query directly against only the relation of an object you 've already fetched ? If so , how does one do that ?"
"I have a DataFrame where multiple rows span each index . Taking the first index , for example , has such a structure : I would like to sort/order each column such that the NaNs are at the bottom of each column at that given index - a result which looks like this : A more explicit example might look like this : with the desired result to look like this : I have many thousands of rows in this dataframe , with each index containing up to a few hundred rows . My desired result will be very helpful when I to_csv the dataframe.I have attempted to use sort_values ( [ 'val1 ' , 'val2 ' , 'val3 ' ] ) on the whole data frame , but this results in the indices becoming disordered . I have tried to iterate through each index and sort in place , but this too does not restrict the NaN to the bottom of each indices ' column . I have also tried to fillna to another value , such as 0 , but I have not been successful here , either.While I am certainly using it wrong , the na_position parameter in sort_values does not produce the desired outcome , though it seems this is likely what want.Edit : The final df 's index is not required to be in numerical order as in my second example.By changing ignore_index to False in the single line of @ Leb 's third code block , toand by creating a temp df for all rows in a given index , I was able to make this work - not pretty , but it orders things how I need them . If someone ( certainly ) has a better way , please let me know ."
"I have a question , which I have a feeling might have already been asked before , but in a different form . Point me to the original if that 's the case please.Anyway , I am playing with Pandas extractall ( ) method , and I do n't quite like the fact it returns a DataFrame with MultiLevel index ( original index - > 'match ' index ) with all found elements listed under match 0 , match 1 , match 2 ... I would rather prefer if the output was a single indexed DataFrame , with multiple regex search results ( if applicable ) returned as a list in single cell . Is that possible at the moment ? Here 's a visualization of what I have in mind : Current output : Desired outputI ` ll be grateful for any suggestions ."
"I 'm trying to use the join function on a numpy array composed of only strings ( representing binary floats ) to get the joined string in order to use the numpy.fromstring function , but the join function does n't seem to work properly.Any idea why ? Which alternative function can I use to do that ? Here is a standalone example to show my problem : As you can see , using the join function on the list ( binary_list ) works properly , but on the equivalent numpy array ( binary_split_array ) it does n't : we can see the string returned is only 72 characters long instead of 80 ."
"I am using Kubuntu 16.04 with kde 5.6.4 . I have installed anaconda python 3.5 which includes ipython qtconsole.To launch ipython qtconsole , I have to type ipython qtconsole in terminal . Is there anyway I can create a launcher for it ? I know there a package , but it does n't link to the anaconda python 3.5 and I do n't want another separate python 3.5 ."
I have a list of generator functions like : What is the best way to cycle between these generators and remove the ones that are exhausted from the list ? The output should be :
"Recently , I have been following a tutorial where I came up with the following codehere , y_set is a vector having binary values 0 , 1 and X_set is an array with two columns.I am specifically not understanding how to interpret the following line of code"
"Here is a comparison I made . np.argsort was timed on a float32 ndarray consists of 1,000,000 elements.And here is a C++ program do the same procedure but on vectors referring to this answer.It prints Finished in 525.908 milliseconds . and it is far slower than the numpy version . So could anyone explain what makes np.argsort so fast ? Thanks.Edit1 : np.__version__ returns 1.15.0 which runs on Python 3.6.6 |Anaconda custom ( 64-bit ) and g++ -- version prints 8.2.0 . Operating system is Manjaro Linux.Edit2 : I treid to compile with -O2 and -O3 flags in g++ and I got result within 216.515 miliseconds and 205.017 miliseconds . That is an improve but still slower than numpy version . ( Referring to this question ) This was deleted because I mistakenly run the test with my laptop 's DC adapter unplugged , which would cause it slow down . In a fair competition , C-array and vector version perform equally ( take about 100ms ) .Edit3 : Another approach would be to replace vector with C like array : float numbers [ 1000000 ] ; . After that the running time is about 100ms ( +/-5ms ) . Full code here :"
"I 'm trying to do PyMongo aggregate - $ group averages of arrays , and I can not find any examples that matches my problem.Data exampleWanted resultsI have tried this approach but MongoDB is interpreting the arrays as Null ? I 've looked through the MongoDB documentation and I can not find or understand if there is any way to do this ?"
I 'm trying to run this simple Python code ( a.py ) : But it 's not working : What 's wrong ? Is it an encoding issue ?
"I have two dataframes , one with daily info starting in 1990 and one with daily info starting in 2000 . Both dataframes contain information ending in 2016.I need to compare columns in df1 and df2 which have the same name , which would n't usually be too complicated , but I need to compare them from the point at which there is data available in both dataframes for a given column ( e.g from df2 , 2000-01-02 in column ' A ' , 2000-01-04 in ' B ' ) . I need to return True if they are the same from that point on and False if they are different . I have started by merging , which gives me : I have figured out how to find the common date , but am stuck as to how to do the same/different comparison . Can anyone help me compare the columns from the point at which there is a common value ? A dictionary comes to mind as a useful output format , but would n't be essential : Many thanks ."
"I am trying to parse multiple pcap files using the pynids library , but can get to parse only the 1st file . I saw that there was a function nids_unregister_tcp in libnids , will that help ? I ca n't find that function in pynids though.Here 's the output :"
"I installed Kivy and Cython . I am using VirtualBox and Arch Linux with Gnome as my Display Manager . Upon creating the following app , I receive the following errorIf I run the command unset DISPLAY it works as intended , but without the option to minimize , maximize and close window that normally are available.Is there a way to fix this error so I wo n't need to type unset DISPLAY every time a new terminal window is open and have it look like a normal window ?"
"I was using tkinter on Mac . But when I used the following simple code , the computer will restart . What is the problem ? Thank you ! Mac : Mojave , version 10.14.6 tkinter : version 8.6 python : 3.7.3"
"I have dataset , the dataset have pairing duplication . Here 's my dataHere 's what I need , because one , two is equals two , one so I want ro remove the duplicate pair"
"I have a python script which does the following : i. which takes an input file of data ( usually nested JSON format ) ii . passes the data line by line to another function which manipulates the data into desired formatiii . and finally it writes the output into a file.Here is my current simple python line that does this ... This works , but with the python GIL limiting it to one core on the server , it 's painfully slow , especially with large amounts of data . The amount of data I normally deal with is around 4 gigs gzip compressed but occasionally I have to process data that is hundreds of gigs gzip compressed . It is not Big Data necessarily but still can not be processed all in memory and with Python 's GIL is very slow to process.While searching for a solution to optimize our data processing , I came across dask . While PySpark seemed to be the obvious solution to me at the time , the promises of dask and it 's simplicity won me over and I decided to give it a try.After a lot of research into dask and how to use it , I put together a very small script to replicate my current process . The script looks like this : This works and produces the same results as my original non-dask script but it still only uses one CPU on the server . So , it did n't help at all . In fact , it 's slower.What am I doing wrong ? Am I missing something ? I 'm still fairly new to dask so let me know if I 've overlooked something or if I should be doing something different altogether . Also , are there any alternatives to dask for using the full capacity of the server ( i.e . all CPUs ) for what I need to do ? Thanks , T"
I have data that shows the relationship for each employee with their managers ( Person : Manager ) -I am trying to show a hierarchy chart from the above data in a clean looking chart and if I can filter that data in the visualization itself that is a Bonus.The data that I get can contain sometimes 5 people or sometimes the number of records is more than 5000.I have tried these approaches but they are no where close to generating any graphs that are interactive . Code - Try 1 -Try 2 -Try 3 -And ran the test.py | dot -Tpng -otree.png
"I have a Pandas DataFrame as shown below - I 'm using Python Pandas.I want to convert it like below , by concatenating Header Names based on values ( 0 or 1 ) Expected Output"
Does anybody know any workaround for this ?
I 'm on Python 3.4 . I want to define an abstract base class that defines somes functions that need to be implemented by it 's subclassses . But Python does n't raise a NotImplementedError when the subclass does not implement the function ...
"Is there a way to use `` magic commands '' from IPython from an outside file ? For example if I have a file , `` rcode.py '' with the code : This gives me a SyntaxError for the first line when I run it using ipython rcode.py in the command line . However when I type these lines straight into the interactive shell with ipython it runs fine . Is this because you only do magic in the interactive shell ? Thanks !"
"I am classifying aerial imagery that is tiled into 256x256 tiles using Keras and TensorFlow . The model splits the training data ( i.e . the 256x256 image tiles making up the study area ) into 70 % training data and 30 % validation data . A sequential model is used followed by an image data generator . Lastly , a fit generator is used to fit the model to the data . The model is then saved to h5 format to be used to predict classes with other imagery in different study areas.When I run the model using the 70 % /30 % training/validation split , the predictions on the validation images work great with increasingly higher accuracies and steadily decreasing loss per epoch . Additionally , when I visualize the predictions ( i.e . probability arrays ) by joining the probability arrays to vector polygons representing the tile boundaries , the classified results look very good . My problem is when I use the saved h5 model to make predictions on new imagery -- the results are nonsensical and appear random for each tile . It is as if the probability arrays are being shuffled randomly such that when I join the results to the vector image boundary tiles , the results look totally random . How can I resolve this issue ? Here is relevant portions of the code used to train the model : And the following script uses the saved model from above to make predictions on test imagery from a different study area . Note , there is no 70/30 training/validation split in the final training run above -- I simply use 100 % of the tiles to train the model , which I then save and reuse in the following script :"
"In the process of using joblib to parallelize some model-fitting code involving Theano functions , I 've stumbled across some behavior that seems odd to me.Consider this very simplified example : I understand why the first case fails , since .get_Y ( ) is clearly an instancemethod of TheanoModel . What I do n't understand is why the second case works , since X , Y andtheano_get_Y ( ) are only declared within the __init__ ( ) method of TheanoModel . theano_get_Y ( ) ca n't be evaluated until the TheanoModel instance has been created . Surely , then , it should also be considered an instancemethod , and should therefore be unpickleable ? In fact , even still works if I explicitly declare X and Y to be attributes of the TheanoModel instance.Can anyone explain what 's going on here ? UpdateJust to illustrate why I think this behaviour is particularly weird , here are a few examples of some other callable member objects that do n't take self as the first argument : None of them are pickleable with the exception of the theano.function !"
"I 'm dealing with unicode data characters , and I wonder why some do not have any name in unicodedata ? Here is a sample code where you can check the < unknown > I thought that every characters inside unicode database were named , BTW there are all of the same category that is [ Cc ] Other , Control.Another question : how can I get the unicode code point value ? Is it ord ( unicodechar ) that does the trick ? I also put the file here ( as encoding is a weird thing ) , and because I think that my cut n ' paste with 'invisible ' character may be lossy ."
"I have a list and I want to find different pair in list.I implement a function -- > different ( ) Is there any other better way to do it ? I want to improve my function different.List size may be greater than 100,000 ."
"I have the following batch shape : And the following weight variable : But when I do tf.batch_matmul : I fail with the following error : ValueError : Shapes ( ? , ) and ( ) must have the same rankSo my question becomes : How do I do this ? How do I just have a weight_variable in 2D that gets multiplied by each individual picture ( 227x227 ) so that I have a ( 227x227 ) output ? ? The flat version of this operation completely exhausts the resources ... plus the gradient wo n't change the weights correctly in the flat form ... Alternatively : how do I split the incoming tensor along the batch dimension ( ? , ) so that I can run the tf.matmul function on each of the split tensors with my weight_variable ?"
"I have an array subclass , where some of the extra attributes are only valid for the object 's original shape . Is there a way to make sure that all array shape changing operations return a normal numpy array instead of an instance of my class ? I 've already written array_wrap , but this does n't seem to have any effect on operations like np.mean , np.sum or np.rollaxis . These all just return an instance of my class.I figure I have to do something in __new__ or __array_finalize__ , but I have n't a clue what.Update : After carefully reading the numpy documentation on subclassing ( http : //docs.scipy.org/doc/numpy/user/basics.subclassing.html ) , all array shape changing operations are doing the 'new from template ' operation . So the question becomes , how do you make the 'new from template ' operation return ndarray instances instead of instances of my class . As far as I can tell , __new__ is never called within these functions.Alternative : Assuming the above is not possible , how do I at least identify in __array_finalize__ the new from template operation ( as opposed to view casting ) ? This would at least let me dereference some attributes that are copied by reference . I could also set a flag or something telling the new instance that its shape is invalid ."
"I am learning PyQT programing , and when I try a simple test , I get Segmentation fault , here is my code pop.py : I started an Apache server at 127.0.0.1 for testing . And here is j.html : I start the pop.py , open a window , javascript popup alert dialog , I click the OK , then pop.py will quite and get `` Segmentation fault '' I tried PySide , get same result . If not JS alert in html , will be OK. Is this a bug of QT or I missed something ? I worked on Debian with python 2.6.6 , python-qt4 4.8.3 , libqtwebkit4 2.1.0I also tried Fedora 15 with PyQt4-4.8.3 , python 2.7.1 , same issueAny suggestion , clue for searching will be helpful . Thanks"
"I 'd like to implement a clipboard copy in a jupyter notebok.The jupyter notebook is running remotely , thus I can not use pandas.to_clipboard or pyperclip and I have to use javascriptThis is what I came up with : Note that the code does what it 's supposed to if I run it in my browser 's console.However , if I run it in jupyter with : Nothing works , Any ideas ?"
"As the question asks , why does n't the below code work : I am executing the above in pycharm via python 3.5.2 console.I initially thought it was a context issue but after reading the documentation , I have n't come closer to understanding why this error ocurs.Thanks in advance : ) EDIT : I understand that it works without exec ( ) by the way , I 'm curious why it wo n't work with exec ( as my circumstances required it ) - comprehensive answers welcome ."
"I am trying to generate undirected graphs in pygraphviz but have been unsuccessful . It seems that no matter what I do , the graph always turns out directed.I have no idea why something so trivial could not be working . What I am doing wrong ?"
"In pysqlite , violating a NOT NULL or a UNIQUE constraint likewise raise an IntegrityError.Unfortunately , this Exception type does not provide an error code , but only a message.So , let 's say I want to ignore unique-constraint violations , because I know this is safe on the given data , but Null values in the key columns should be reported.I 've come up with the following solution : However , parsing the error message seems wrong and might be unreliable.Is there a better way to do this , maybe even using con.executemany ( ) ?"
"I need to store functions in a dictionary , each function depending on its key , lets say , for a key 1 the lambda function associated is lambda s : s * A [ 1 ] . I tried with dict comprehension but it seems that the inline functions ends defined with the last value of the loop.After that all lambda functions created are declared with A [ 3 ] instead of A [ 0 ] , A [ 1 ] , A [ 2 ] and A [ 3 ] . What 's wrong with this code ?"
"I 've been at this for the better part of a day but have been coming up with the same Error 400 for quite some time . Basically , the application 's goal is to parse a book 's ISBN from the Amazon referral url and use it as the reference key to pull images from Amazon 's Product Advertising API . The webpage is written in Python 3.4 and Django 1.8 . I spent quite a while researching on here and settled for using python-amazon-simple-product-api since it would make parsing results from Amazon a little easier.Answers like : How to use Python Amazon Simple Product API to get price of a productMake it seem pretty simple , but I have n't quite gotten it to lookup a product successfully yet . Here 's a console printout of what my method usually does , with the correct ISBN already filled : Now I guess I 'm curious if this is some quirk with PythonAnywhere , or if I 've missed a configuration setting in Django ? As far as I can tell through AWS and the Amazon Associates page my keys are correct . I 'm not too worried about parsing at this point , just getting the object . I 've even tried bypassing the API and just using Bottlenose ( which the API extends ) but I get the same error 400 result.I 'm really new to Django and Amazon 's API , any assistance would be appreciated !"
"I 'm using Flask + gevent and want to access the flask.g application global inside the target function of a greenlet . I 'm using the copy_current_request_context decorator and have a situation pretty similar to example given in the docs : However , I get the following error : I think a new application context is pushed when the request context is copied ? I set a trace in the Flask code here and that seems to be the case . So the error is n't all that surprising because the flask.g object is application context scoped as of 0.10 ( see http : //flask.pocoo.org/docs/0.12/api/ # flask.Flask.app_ctx_globals_class ) .Obviously , I can just pass the user data into the target function as arguments : And this works just fine , but I was hoping to use flask.g if possible ."
"There are a few cases where the date is written as 'created ca . 1858-60 ' , where a human reader would understand it as 'created ca 1858-1860 . 'As such , imagine two integers representing years.I want to be able to get a+b == 1859 . I could parse them to strings , take the first two characters ( '18 ' ) , concatinate the shorter string and parse them back to numbers , sure , but..that seems a bit round-a-bound . What would be the Pythonic way to deal with this ?"
"Assuming , that I have a class like that : How can I create many instances of this class in parallel using asyncio in python 3.4.1 ?"
"I have some property methods in a Class and I want to clear the cache of this property at some point.Example : if I do self.prop.clear_cache ( ) , here the error message I get : clear_cache ( ) works for functions but not for property methods . Is there any way to do that ?"
"I have the following python code to create a charge in stripe.This succeeds ( I think ) , because it shows up in my dashboard with a charge_id.But then , immediately after in the code , the following fails : With error : No such charge : somelongstringhereHowever , somelongstringhere is indeed the ID under charge details on my stripe dashboard . So how come Stripe ca n't retrieve this charge ? Is this not the correct id ?"
"I just discovered Numpy structured arrays and I find them to be quite powerful . The natural question arises in my mind : How in the world do I create a Numpy structure scalar . Let me show you what I mean . Let 's say I want a structure containing some data : This gives me array ( 0.5 ) instead of 0.5 . On the other hand , if I do this : I get 0.5 , just like I want . Which means that ar [ 0 ] is n't an array , but a scalar . Is it possible to create a structured scalar in a way more elegant than the one I 've described ?"
"I am stumped by a problem seemingly related to asyncio + aiohttp whereby , when sending a large number of concurrent GET requests , over 85 % of the requests raise an aiohttp.client_exceptions.ClientConnectorError exception that ultimately stems fromwhile sending single GET requests or doing the underlying DNS resolution on the host/port does not raise this exception.While in my real code I 'm doing a good amount of customization such as using a custom TCPConnector instance , I can reproduce the issue using just the `` default '' aiohttp class instances & arguments , exactly as below.I 've followed the traceback and the root of the exception is related to DNS resolution . It comes from the _create_direct_connection method of aiohttp.TCPConnector , which calls ._resolve_host ( ) .I have also tried : Using ( and not using ) aiodnssudo killall -HUP mDNSResponderUsing family=socket.AF_INET as an argument to TCPConnector ( though I am fairly sure this is used by aiodns anyway ) . This uses 2 rather than the default int 0 to that paramWith ssl=True and ssl=FalseAll to no avail.Full code to reproduce is below . The input URLs are at https : //gist.github.com/bsolomon1124/fc625b624dd26ad9b5c39ccb9e230f5a.Printing each exception string from res looks like : What 's frustrating is that I can ping these hosts with no problem and even call the underlying ._resolve_host ( ) : Bash/shell : Python : My setup : Python 3.7.1aiohttp 3.5.4Occurs on Mac OSX High Sierra and Ubuntu 18.04Information on the exception itself : The exception is aiohttp.client_exceptions.ClientConnectorError , which wraps socket.gaierror as the underlying OSError.Since I have return_exceptions=True in asyncio.gather ( ) , I can get the exception instances themselves for inspection . Here is one example : Why do I not think this is a problem with DNS resolution at the OS level itself ? I can successfully ping the IP address of my ISP ’ s DNS Servers , which are given in ( Mac OSX ) System Preferences > Network > DNS :"
"I 'm trying to get map-reduce functionality with python using mongo-hadoop . Hadoop is working , hadoop streaming is working with python and the mongo-hadoop adaptor is working . However , the mongo-hadoop streaming examples with python are n't working . When trying to run the example in streaming/examples/treasury I get the following error : $ user @ host : ~/git/mongo-hadoop/streaming $ hadoop jar target/mongo-hadoop-streaming-assembly-1.0.1.jar -mapper examples/treasury/mapper.py -reducer examples/treasury/reducer.py -inputformat com.mongodb.hadoop.mapred.MongoInputFormat -outputformat com.mongodb.hadoop.mapred.MongoOutputFormat -inputURI mongodb : //127.0.0.1/mongo_hadoop.yield_historical.in -outputURI mongodb : //127.0.0.1/mongo_hadoop.yield_historical.streaming.outIf anyone could shed some light it would be a big help.Full info : As far as I can tell I needed to get the following four things to work : Install and test hadoopInstall and test hadoop streaming with pythonInstall and test mongo-hadoop Install and test mongo-hadoop streaming with pythonSo the short of it is I 've got everything working up to the fourth step . Using ( https : //github.com/danielpoe/cloudera ) i 've got cloudera 4 installedUsing chef recipe cloudera 4 has been installed and is up and running and testedUsing michael nolls blog tutorials , tested hadoop streaming with python successfullyUsing the docs at mongodb.org was able to run both treasury and ufo examples ( build cdh4 in build.sbt ) Have downloaded 1.5 hours worth of twitter data using the readme for the twitter example in streaming/examples and also tried the treasury example ."
"I came across a confusing problem when unit testing a module . The module is actually casting values and I want to compare this values.There is a difference in comparison with == and is ( partly , I 'm beware of the difference ) As expected till now , but here is my `` problem '' : Why ? At least the last one is really confusing to me . The internal representation of float ( 0 ) and float ( 0.0 ) should be equal . Comparison with == is working as expected ."
I was fiddling around with id recently and realized that ( c ? ) Python does something quite sensible : it ensures that small ints always have the same id . But then it occurred to me to wonder whether the same is true for the results of mathematical operations . Turns out it is : Seems like it starts failing at n=257 ... But sometimes it still works even with larger numbers : What 's going on here ? How does python do this ?
"I 'm new to python and html . I am trying to retrieve the number of comments from a page using requests and BeautifulSoup.In this example I am trying to get the number 226 . Here is the code as I can see it when I inspect the page in Chrome : When I request the text from the URL , I can find the code but there is no content between the span tags , no 226 . Here is my code : It returns this , same as the above but no 226 . I 'm at a loss as to why the value is n't appearing . Thank you in advance for any assistance ."
"Long story short : I want to plot Gaia astrometry data to TESS imagery in Python . How is it possible ? See below for elaborated version.I have 64x64 pixel TESS imagery of a star with Gaia ID 4687500098271761792 . Page 8 of the TESS Observatory Guide says 1 pixel is ~21 arcsec . Using the Gaia Archive , I search for this star ( below top features , click Search . ) and submit a query to see the stars within 1000 arcsec , roughly the radius we need . The name I use for the search is Gaia DR2 4687500098271761792 , as shown below : Submit Query , and I get a list of 500 stars with RA and DEC coordinates . Select CSV and Download results , I get the list of stars around 4687500098271761792 . This resulting file also can be found here . This is the input from Gaia we want to use.From the TESS , we have 4687500098271761792_med.fits , an image file . We plot it using : and get a nice pic : and a bunch warnings , most of which was kindly explained here ( warnings in the Q , explanation in the comments ) .Notice that it is indeed good that we are using the WCS projection . To check , let 's just plot the data in hdul.data without caring about the projection : The result : Almost the same as before , but now the labels of the axes are just pixel numbers , not RA and DEC , as would be preferable . The DEC and RA values in the first plot are around -72° and 16° respectively , which is good , given that the Gaia catalogue gave us stars in the proximity of 4687500098271761792 with roughly these coordinates . So the projection seems to be reasonably ok. Now let 's try to plot the Gaia stars above the imshow ( ) plot . We read in the CSV file we downloaded earlier and extract the RA and DEC values of the objects from it : Plot to check : Shape is not a circle as expected . This might be an indicator of future troubles.Lets try to transform these RA and DEC values to WCS , and plot them that way : The result is : The function all_world2pix is from here . The 1 parameter just sets where the origin is . The description of all_world2pix says : Here , origin is the coordinate in the upper left corner of the image . In FITS and Fortran standards , this is 1 . In Numpy and C standards this is 0.Nevertheless , the shape of the point distribution we get is not promising at all . Let 's put together the TESS and Gaia data : We get : which is not anywhere near what would be ideal . I expect to have an underlying imshow ( ) pic with many markers on it , and the markers should be where stars are on the TESS image . The Jupyter Notebook I worked in is available here.What steps am I missing , or what am I doing wrong ? Further DevelopmentsIn response to another question , keflavich kindly suggested to use a transform argument for plotting in world coordintes . Tried it with some example points ( the bended cross on the plot below ) . Also plotted the Gaia data on the plot without processing it , they ended up being concentrated at a very narrow space . Applied to them the transform method , got a seemingly very similar result than before . The code ( & also here ) : and the resulting plot : This bending cross is expected , since TESS is not aligned with constant latitude and longitude lines ( ie the arms of the cross do not have to be parallel with the sides of the TESS image , plotted with imshow ( ) ) . Now lets try to plot constant RA & DEC lines ( or to say , constant latitude and longitude lines ) to better understand why the datapoints from Gaia are misplaced . Expand the code above by a few lines : The result is encouraging : ( See notebook here . )"
"So I came across something very weird in python . I tried adding a reference to the list to itself . The code might help demonstrate what I am saying better than I can express . I am using IDLE editor ( interactive mode ) .So far the output is as expected . But when I do this . To me it seems that the output should be But it isApparently instead of creating a copy of the list , it puts a reference to the list in y. y [ 0 ] is l returns True . I ca n't seem to find a good explanation for this . Any ideas ?"
"This question is similar to Split ( explode ) pandas dataframe string entry to separate rows but includes a question about adding ranges.I have a DataFrame : And I 'd like the Options column to be split by the comma as well as rows added for a range.How can I go beyond using concat and split like the reference SO article says to accomplish this ? I need a way to add a range.That article uses the following code to split comma delineated values ( 1,2,3 ) : Thanks in advance for your suggestions ! Update 2/14 Sample data was updated to match my current case ."
I want to import subprocess module from py v3.3 to v2.7 to be able to use the timeout functionality.After reading few posts I tried thisBut it says : Then I found out that future does n't have any feature subprocess.So where and how should I import the subprocess from v3.3 ?
"I do n't care if I achieve this through vim , sed , awk , python etc . I tried in all , could not get it done.For an input like this : I want : Then I want to just load this up in Excel ( delimited by whitespace ) and still be able to look at the hierarchy-ness of the first column ! I tried many things , but end up losing the hierarchy information"
I am trying to wrap a Java class to be called from Python using thrift 's TFileTransport . I tried using two protocols TJSON and TBinary but I keep getting this exceptionThis is how my Python client looks : and this is my server : I have followed the example from this thread : Example on how to use TFileTransport in Thrift ( Client/Server ) and from this post : http : //theprogrammersguideto.com/thrift/blog/code/chapter-3-moving-bytes-with-transports/
I have a dictionary : I want to create the cartesian product of the two so that I get a tuple of each possible pair.It seems like there would be a simple way to do this so that it extends if I have three items . Kind of like a dynamic number of loops . Feels like I am missing an obvious way to do this ...
"I have a post , and I need to predict the final score as close as I can.Apparently using curve_fit should do the trick , although I am not really understanding how I should use it.I have two known values , that I collect 2 minutes after the post is posted.Those are the comment count , referred as n_comments , and the vote count , referred as n_votes.After an hour , I check the post again , and get the final_score ( sum of all votes ) value , which is what I want to predict.I 've looked at different examples online , but they all use multiple data points ( I have just 2 ) , also , my initial data point contains more information ( n_votes and n_comments ) as I 've found that without the other you can not accurately predict the score.To use curve_fit you need a function . Mine looks like this : And a sample datapoint looks like this : This is the broken mess of my attempt , and the result does n't look right at all.The plot should display the initial/final score and the current prediction . I have some doubts regarding the function too.. Initially this is what it looked like : But I replaced votes_per_minute with just votes . Considering that I collect this data after 2 minutes , and that I have a parameter there , I 'd say that it 's not too bad but I do n't know really.Again , who guarantees that this is the best function possible ? It would be nice to have the function discovered automatically , but I think this is ML territory ... EDIT : Regarding the measurements : I can get as many as I want ( every 15-30-60s ) , although they have to be collected while the post has = < 3 minutes of age ."
Here is my code . I need to add a percent symbol at the end of the print string . I ca n't get this figured out .
"Suppose I have this Python code : The question is , will there be any problem if I start iterating `` a '' in one thread and , at the same time , iterating `` b '' in another thread ? Clearly , a and b share some data ( the original iterable , + some additional stuff , internal buffers or something ) . So , will a.next ( ) and b.next ( ) do the appropriate locking when they access this shared data ?"
"I have difficulty aligning Japanese characters in python.Code : Result : If the string contains english and numbers only , the .format ( ) works fine , as shown on the right.The aligning goes wrong when encountering Japanese characters , as shown on the left.Interestingly , when aligning with { name : > 14s } : If `` name '' contains 4 JP characters , there would be 2 prefix spaces.If `` name '' contains 3 JP characters , there would be 5 prefix spaces.If `` name '' contains 2 JP characters , there would be 8 prefix spaces.If `` name '' contains 0 JP characters , there would be 14 prefix spaces.It seems like it treat 1 Japanese charater = 3 spaces in this case . { name : < 14s } { name : ^14s } { name : > 14s } all have the behavior mentioned above.I am using OSX 10.10.2 , terminal font is monaco.Maybe this has something to do with full-width/half-width characters.Is there anyway to align Japanese characters just like English characters ? Thanks you.Edit : Ignacio Vazquez-Abrams 's answer is indeed the correct way.Everyone who is dealing with unicode in Python should read the slide he pointed out.The `` \u3000 '' is the full-width space in CJK . See this page.Review the .Format Syntax would also help.I would also like to recommend this SO answer which helps me understanding how unicode works in Python.However , if the string contains both half-width and full-width characters , the alignment still goes wrong . A simple workaround is to use all full-width characters ."
"I 'm new to Python , and a bit rusty with my linear algebra , so perhaps this is a simple question . I 'm trying to implement a Taylor Series expansion on a Matrix to compute exp ( A ) , where A is just a simple 3x3 matrix . The formula , BTW for this expansion is sum ( A^n / n ! ) .My routine works alright up to n=9 , but at n=10 , the numbers in the Matrix suddenly become negative . This is the problem . A**9 matrix ( [ [ 250130371 , 506767656 , 688136342 ] , [ 159014912 , 322268681 , 437167840 ] , [ 382552652 , 775012944 , 1052574077 ] ] ) A**10 matrix ( [ [ -1655028929 , 1053671123 , -1327424345 ] , [ 1677887954 , -895075635 , 319718665 ] , [ -257240602 , -409489685 , -1776533068 ] ] ) Intuitively A^9 * A should produce larger numbers for each member of the matrix , but as you can see , A^10 is n't giving that result . Any ideas ? Thanks for any help you can give !"
"I have code which , in a simplified form , looks like this : When I run it the memory footprint keeps increasing over time more or less linearly . If , however , I remove the gen.engine nesting : memory usage remains constant.I 've managed to reproduce the issue with different versions of tornado 2 , on both Mac OS X and Linux . Any ideas what might be the cause of this problem ?"
i try to fix all pylint errors and pylint warnings in a project . but i keep getting an error when i set a metaclass ( https : //www.python.org/dev/peps/pep-3115/ ) .here is my example code : the error just says `` invalid syntax '' . i use pylint via the vim plugin syntastic ( https : //github.com/scrooloose/syntastic ) .my pylint version is ( pylint -- version ) : my syntastic plugin is up to date with github.any ideas ?
"I am new in Python . My task was quite simple -- I need a list of functions that I can use to do things in batch . So I toyed it with some examples likeSurprisingly , the call of gave me the result like [ 9 , 9 , 9 , 9 , 9 , 9 , 9 , 9 , 9 , 9 ] . It was not what I expected as I 'd like the variable i has different values in different functions.So My question is : Is the variable i in lambda global or local ? Does python has the same concept like 'closure ' in javascript ? I mean does each lambda here holds a reference to the i variable or they just hold a copy of the value of i in each ? What should I do if I 'd like the output to be [ 0 , 1 , ... ..9 ] in this case ?"
"I am starting to use cProfile to profile my python script.I have noticed something very weird.When I use time to measure the running time of my script it takes 4.3 seconds.When I use python -m cProfile script.py it takes 7.3 seconds.When running the profiler within the code : it takes 63 seconds ! ! I can understand why it might take a little more time when adding profiling , but why there is such a difference between using cProfile from outside or as part of the code ? Is there a reason why it takes so much time when I use profile.run ?"
"I have a list that I want to get the last few elements of in reverse order : I can reverse the list with lst [ : :-1 ] and get the whole list reversed . But I want the reversed list only up to a certain index . I can specify an index : and get : which is the original list up to the specified index , in reverse order . This works for any index > 0 . But if ind is 0 ( meaning the whole list should be returned , in reverse order ) , this causes a problem : returns : because my ending place is the same as my beginning ( -1 ) . My expected/desired output would be [ 4 , 3 , 2 , 1 ] I know an easy workaround would be to just put an if statement to catch the case where ind is 0 , or do it in two steps ( index then reverse ) , but it feels like this should be doable with the indexing system in Python . Am I wrong ?"
"Running Django Nonrel , with Google App Engine 2.6.0 and Python 2.7 , I 'm getting this exception when trying to load for the first time localhost and localhost/admin ( I expect it will happen with any page , though ) : Strangely enough , it only appears using Google Chrome . When using Firefox , it does n't print any exception ( or at least , I have n't been able to replicate this problem in Firefox after many tries ) .Does anyone know something about this problem ? .Thanks"
"The title might not be intuitive -- let me provide an example . Say I have df , created with : I can get the index location of each respective column minimum withNow , how could I get the location of the last occurrence of the column-wise maximum , up to the location of the minimum ? Visually , I want to find the location of the green max 's below : where the max 's after the minimum occurrence are ignored.I can do this with .apply , but can it be done with a mask/advanced indexing ? Desired result :"
"I need to write a python function or class with the following Input/OutputInput : The position of the X-rays source ( still not sure why it 's needed ) The position of the board ( still not sure why it 's needed ) A three dimensional CT-ScanOutput : A 2D X-ray Scan ( simulate an X-Ray Scan which is a scan that goes through the whole body ) A few important remarks to what I 'm trying to achieve : You don ’ t need additional information from the real world or any advanced knowledge.You can add any input parameter that you see fit.If your method produces artifacts , you are excepted to fix them.Please explain every step of your method.What I 've done until now : ( .py file added ) I 've read the .dicom files , which are located in `` Case2 '' folder.These .dicom files can be downloaded from my Google Drive : https : //drive.google.com/file/d/1lHoMJgj_8Dt62JaR2mMlK9FDnfkesH5F/view ? usp=sharingI 've sorted the files by their position.Finally , I 've created a 3D array , and added all the images to that array in order to plot the results ( you can see them in the added image ) - which are slice of the CT Scans . ( reference : https : //pydicom.github.io/pydicom/stable/auto_examples/image_processing/reslice.html # sphx-glr-auto-examples-image-processing-reslice-py ) Here 's the full code : The result is n't what I wanted because : These are slice of the CT scans . I need to simulate an X-Ray Scan which is a scan that goes through the whole body.Would love your help to simulate an X-Ray scan that goes through the body.I 've read that it could be done in the following way : `` A normal 2D X-ray image is a sum projection through the volume . Send parallel rays through the volume and add up the densities . '' Which I 'm not sure how it 's accomplished in code.References that may help : https : //pydicom.github.io/pydicom/stable/index.html"
"From the asyncio docs : asyncio.as_completed ( aws , * , loop=None , timeout=None ) Run awaitable objects in the aws set concurrently . Return an iterator of Future objects . Each Future object returned represents the earliest result from the set of the remaining awaitables.I would assume each of these Future objects has the methods described in asyncio.Future : .cancelled ( ) , .exception ( ) , and .result ( ) . But it appears that the yielded elements are just coroutines , not Future objects . What am I missing ? This seems to defeat the description of .as_completed ( ) . How is the coroutine `` completed '' if I need to await it ? Ultimately , the reason I care about this is because I 'd like to let exceptions bubble up as they do in asyncio.gather ( ... , return_exceptions=True ) . Consider adding on one bogus URL that will raise when session.request ( ) is called : What I would like to be able to do is something like this ( using the methods of a Future object , but these are n't Future objects at all , which is the problem ) :"
Is there a way in Python to write a class that will error unless it 's used with a with statement ? I can do it manually : have __enter__ set a flag and have every other method check for that flag . But is there a nicer way to do it ? Here 's code for the not-so-since way :
"I want to extract operators like : + , - , / , * and also ( , ) , _ from the stringEg.output : This does not work :"
"From what I understand , a for x in a_generator : foo ( x ) loop in Python is roughly equivalent to this : That suggests that something like this : would do two things : Not raise any exceptions , segfault , or anything like thatIterate over y until it reaches some x where should_inner_loop ( x ) returns truthy , then loop over it in the inner for until stop_inner_loop ( thing ) returns true . Then , the outer loop resumes where the inner one left off.From my admittedly not very good tests , it seems to perform as above . However , I could n't find anything in the spec guaranteeing that this behavior is constant across interpreters . Is there anywhere that says or implies that I can be sure it will always be like this ? Can it cause errors , or perform in some other way ? ( i.e . do something other than what 's described aboveN.B . The code equivalent above is taken from my own experience ; I do n't know if it 's actually accurate . That 's why I 'm asking ."
"Question are at the end of this post.First snippet : empty local variable dictionary.Output : Local variables : { } Second snippet : print inside inner ( ) function and creating local variable entry.Output : Third Snippet : del x from inside the inner function : Output : Questions : In Second Snippet , how print statement create local variable.If it creates local variable inside inner function why I am not able to delete it.Could someone please help me understanding this ."
"I 'm trying to annotate the values for a stacked horizontal bar graph created using pandas . Current code is belowThe problem is the annotation method I used gives the x starting points and not the values of each segment . I 'd like to be able to annotate values of each segment in the center of each segment for each of the bars . exampleedit : for clarity , what I would like to achieve is something like this where the values are centered horizontally ( and vertically ) for each segment : enter image description hereAny help would be appreciated ."
"I have a dataframe like this : and I want a new column that gives the category , like this : I know I could do it inefficiently using .loc : How do I combine cat and dog to both be animal ? This does n't work :"
I have been using Java + AspectJ extensively for my startup . I would love to switch to Scala but I have a common design pattern that I am not sure entirely the best way to implement in Scala.A tremendous amount of our application uses AspectJ pointcuts using annotations as the marker.This is very similar to Python 's decorator and blogged about it here.I have tried doing this technique in Scala but had problems with AspectJ + Scala.Even if I did get it to work it seems unScala like . I have seen some projects do some call-by-name closure magic ( I think thats what they are doing ) .Example of a replace of @ Transaction : I have to say though I prefer the annotation more as it seems more declarative.What is the Scala way of declaratively `` decorating '' code blocks ?
"I 've been trying to work out the basic skeleton for a language I 've been designing , and I 'm attempting to use Parsimonious to do the parsing for me . As of right now , I 've , declared the following grammar : When I try to output the resulting AST of a simple input string like `` { do-something some-argument } '' : Parsimonious decides to flat-out reject it , and then gives me this somewhat cryptic error : At first I thought this might be an issue related to my whitespace rule , _ , but after a few failed attempts at removing the whitespace rule in certain places , I was still coming up with the same error.I 've tried searching online , but all I 've found that seems to be remotely related , is this question , which did n't help me in any way.Am I doing something wrong with my grammar ? Am I not parsing the input in the correct way ? If anyone has a possible solution to this , it 'd be greatly appreciated ."
"In the July 2012 issue of `` Mensa Bulletin '' there is an article entitled `` The Digital Brain . '' In it the author relates the human brain to base64 computing . It is a rather interesting and fun article with a prompt at the end . Said prompt asks the reader to convert Cytosine Guanine Adenine Guanine Adenine Guanine to a base 10 number using the fact that Cytosine Cytosine Guanine Cytosine Adenine Guanine equals 2011 ( the first codon set mentioned is cgagag for short and the second is ccgcag for short . ) Basically you have to convert a base 64 number to base 10 using a table in the article that displays all of the possible codons in proper order with aug = 0 , uuu = 1 , uuc = 2 , ... , gga == 61 , ggg = 62 , uag = 63 . I decided to give this a go and settled on writing a python program to convert codon numbers to base 10 and base 10 numbers to codons . After writing a quick algorithm for both , I ran it . The program gave no errors and popped out codons for my numbers and vice versa . However , they were the wrong numbers ! I can not seem to see what is going wrong and would greatly appreciate any help.Without further ado , the code : EDIT 1 : The values I am getting are 1499 for ccgcag and 1978 for cgagag.EDIT 2 : base10ToCodonNum function fixed thanks to Ashwini Chaudhary ."
Here is way to get the last key of an OrderedDict in Python3.I need to get last value of an OrderedDict in Python3 without conversions to list .
"Using python3 and cartopy , having this code : I get this : Question : What should I write , in order to get a red `` A '' over Bulgaria ( or any other country , which I refer to in country.attributes [ 'SOVEREIGNT ' ] ) ? Currently the label is not shown at all and I am not sure how to change the font of the label . Thus , it seems that the following only changes the color , without adding the label :"
"Is there a way to limit function so that it would only have access to local variable and passed arguments ? For example , consider this codeNormally the output will beHowever , I want to limit my_fun to local scope so that print ( x ) would work but throw an error on print ( a ) . Is that possible ?"
I want to convert a RDD to a DataFrame and want to cache the results of the RDD : If you do n't use a cache function no job is generated.If you use cache only after the orderBy 1 jobs is generated for cache : If you use cache only after the parallelize no job is generated.Why does cache generate a job in this one case ? How can I avoid the job generation of cache ( caching the DataFrame and no RDD ) ? Edit : I investigated more into the problem and found that without the orderBy ( `` t '' ) no job is generated . Why ?
"In google app engine dev environment I can not get exif data . I followed guide from herehttps : //developers.google.com/appengine/docs/python/images/imageclassI have done following in the codeI only get None.the img object is fine as I can perform img.resize etc . I need to get Exif info.UPDATE : By doing this I was able to get metadata , Like explained in documentation I got very 'limited ' set more precisely thisApparently you get much bigger set in real environment , I have no idea why this cant be the feature of dev env though . It is quite frustrating . As long as I can get pyexiv2 level exif I am okay but if it is just using PIL that is not good enough . Currently PIL provides way little exif information ."
"In the Byte Pair Encoding algorithm , there 's a replacement step where it changes the character strings delimited by spaces to bigrams . I.e. , given a list of str tuples as such : And a string tuple : ( ' i ' , 's ' ) How do I process the list such that it iterates through all the tuple keys and and replace ( ' i ' , 's ' ) with ( 'is ' ) ? , i.e . the output Counter will look something like this : I 've tried this : but is there a more efficient way than looping through each word , then changing them to string to do a replace and splitting them again and then casting them back into tuples ? Would regex replacement be faster ? Is there a way to work with the list of tuples without dealing with strings ? I 've tried this and it seems like replacing the string with str.replace is not the problem . It 's really counting the bigrams and extracting them : This is tested on norvig.com/big.txt [ out ] : I 've already experimented with scikit-learn CountVectorizer and i did n't seem to be as fast as using zip , see Fast/Optimize N-gram implementations in pythonAlso , without them filter operation in the Counter step , it took even longer . The Counter operation is taking 3 seconds per iteration = ( How else can this operation be optimized ?"
"Given a list of 3-tuples , for example : [ ( 1,2,3 ) , ( 4,5,6 ) , ( 7,8,9 ) ] how would you compute all possible combinations and combinations of subsets ? In this case the result should look like this : all tuples with identical elements are regarded the samecombinations which derive from the same tuples are not allowed ( e.g . these should n't be in the solution : ( 1,2 ) , ( 4,6 ) or ( 7,8,9 ) )"
"Consider the dataframe dfWith the ' X ' group defined by column ' A ' , I want to sort [ 8 , 7 , 4 , 3 ] to the expected [ 3 , 4 , 7 , 8 ] . However , I want to leave those rows where they are ."
"My Problem : I 've created a series of nodes that each have a set of attribute objects associated with them . The attribute objects are initialized with descriptions and names for each attribute . I 'd like these attributes , and their descriptions , to show up in my sphinx documentation without having to maintain the name/description in two places , once in the doc string of the class and once in the initialization of the attribute.To illustrate the problem , consider the following code : I 'd like the result of Foo.attributes_string to show up in the doc string of Foo so that I get this : My Solution Attempts : First I thought `` Hey that 's easy ! I 'll just setup a class decorator ! `` : This failed miserably with the following error : So then I thought `` Well then I 'll just use a metaclass to set __doc__ before the class is created ! '' . When I went to implement this I immediately ran into a problem : How do you call a classmethod for a class that has n't been created yet ? I circumvented that problem with a very hacky workaround that makes me cringe : I create the class twice , once without modifying it so I can call it 's classmethod , and then again to create the class with the proper __doc__ : This totally works and gives me the result I was looking for : However , is n't it terribly inefficient to create the class twice ? Does it matter ? Is there a better way ? Is what I 'm trying to do really dumb ?"
"Python provides a flag ( re.X or re.VERBOSE ) to allow annotation of regular expressions : However , with automatic string concatenation , you could achieve basically the same thing : I do n't think that I 've really seen the latter form used , but ( IMHO ) it makes for an easier to read regex ( I do n't need to try to figure out which whitespace has been escaped , and what whitespace is being ignored ... etc . etc . ) and my comments get formatted by my text editor as comments . Is there a reason to prefer the former over the latter or visa-verse ? Or is this really a tomato-Tomato issue ?"
"Let 's say we have something like this : Does the Python interpreter evaluate fcn1 ( ) before fcn2 ( ) , or is the order undefined ?"
I need to find in which index in the large list where it match with the sub list.I expected True but it return False.The desired output is
"I 'm struggling with this problem since a week now so I guess it 's time to ask for some help . Long story short : I 'm building an application using Python 3.4 and PyQt5 and I 'm using cx_Freeze to create a standalone executable of my application . While on Windows and Mac OS everything goes fine I encountered this problem while executing my frozen application on a Ubuntu operating system on which Qt5 is not installed : The application runs fine on the machine I 'm using to build the frozen package ( where Qt5 is installed ) .I have googled a bit and it seems that this problem is quite common . I tried everything I could find in the following links : '' Failed to load platform plugin `` xcb '' `` while launching qt5 app on linux without qt installed ( Qt 5.4.1 ) This application failed to start because it could not find or load the Qt platform plugin `` xcb '' In the frozen application I do have the platforms directory in which there is libqxcb.so . Just for consistency I 'm posting the output of the ldd libqxcb.so which is the same on both the machines where Qt5 is installed , and the other one I 'm using for testing where I only have a fresh Ubuntu 14.04 install ( no Qt5 ) : The same applies to ldd Eddy ( where Eddy is the application executable name ) : On the machine where I 'm creating the frozen application I tried both Qt5 compiled from source and Qt5 installed using the offline installer . I used SIP 4.17 and PyQt5 5.5.1 compiled from sources and cx_Freeze 3.4.3.Any advice ? EDIT1 : I already tried moving libQt5XcbQpa.so.5 ( along with libQt5QCore.so.5 , etc.. ) into my build directory but it did n't help ."
"I 'm interested in learning how to Doctests and Unit tests in a more Agile / BDD way.I 've found a few tutorials that seem reasonable , but they are just thumbnails.What I would really like to see is the source code of some Django projects that were developed BDD style.The things I 'm unclear about are how do you handle request objects etc.I have a situation where I have deployed my app and I 'm getting completely different behavior in production that I did in development or even from the Python shell on the production server . I 'm hoping some Doctests will help me diagnose this and well as open the door for a more Agile process of writing the tests first.Specifically , here is the code I 'm trying to test : How do I create a Doctest that ensures that index returns 10 objects ? The Product queries seem to work fine from the shell on the production server . The actual server is not returning any products at all ."
"I am building an application with Python 2.7 using the Google App Engine framework.To test my application I have a several tests that are run through nosetests making use of the nosegae plugin . I run them with the following command : In the model layer of my application , I have the need to run several database operations that affect multiple entity groups inside the same transaction . I do this by making use of the run_in_transaction_options function of the db package : https : //developers.google.com/appengine/docs/python/datastore/functions # run_in_transactionUnfortunately , when running my test suites , I get the following error in those testcases that try to run such transaction : BadRequestError : transactions on multiple entity groups only allowed with the High Replication datastoreI can not find any flag in nosetests that makes it possible to enable the HRD.I am wondering if it is possible at all to run HRD from nosetests and if so , how it can be set up ?"
"Is it possible to tell Python 2.7 to only parse a function definition if a package exists ? I have a script that is run on multiple machines . There are some functions defined in the script that are very nice to have , but are n't required for the core operations the script performs . Some of the machines the script is run on do n't have the package that the function imports , ( and the package ca n't be installed on them ) . Currently I have to comment out the function definition before cloning the repo onto those machines . Another solution would be to maintain two different branches but that is even more tedious . Is there a solution that prevents us from having to constantly comment out code before pushing ? There are already solutions for when the function is called , such as this :"
"I have data that consists of 4 different time series , e.g . : Thing is , besides temporal dependency ( horizontal one ) , there also exists vertical dependency ( in columns , if we look at this example 'matrix ' ) .Output vectors would be these same time series , only shifted for one step.Is it possible to create LSTM network for each of time series ( so , 4 networks in my case , and also 4 outputs ) but also connect them vertically , i.e . create 2D LSTM ? If so , how would one achieve that in Tensorflow ? Is it also possible to make this kind of network deeper ( have additional LSTM layers appended to each of these 4 networks ) ? I hope I was clear enough with explanation ."
"I wrote a script to launch a number of processes ( simple unit tests ) to run in parallel . It will do N jobs with num_workers parallel processes at a time . My first implementation ran the processes in batches of num_workers and seemed to be working fine ( I used the false command here to test the behavior ) However , the tests do not take equal amounts of time so I was sometimes waiting for a slow test to finish . Thus I rewrote it to keep assigning tasks as they finishedHowever this produces wrong results for the last few processes . For instance , in the example above it produces 98/100 errors instead of 100 . I checked and this has nothing to do with concurrency ; the 2 latest jobs were returning with exit code 0 for some reason.Why does this happen ?"
"I have a simple dataframe df with a column of lists lists . I would like to generate an additional column based on lists.The df looks like : I would like df to look like this : Basically I want to 'sum'/append the rolling 2 lists . Note that row 1 , because I only have 1 list 1 , rolllists is that list . But in row 2 , I have 2 lists that I want appended . Then for row three , append df [ 2 ] .lists and df [ 3 ] .lists etc . I have worked on similar things before , reference this : Pandas Dataframe , Column of lists , Create column of sets of cumulative lists , and record by record differences.In addition , if we can get this part above , then I want to do this in a groupby ( so the example below would be 1 group for example , so for instance the df might look like this in the groupby ) : I have tried various things like df.lists.rolling ( 2 ) .sum ( ) and I get this error : in Pandas 0.24.1 and unfortunatley in Pandas 0.22.0 the command does n't error , but instead returns the exact same values as in lists . So Looks like newer versions of Pandas ca n't sum lists ? That 's a secondary issue.Love any help ! Have Fun !"
How can I remove only the leading zeroes from a numpy array without using for a loop ? I have written the following codeI was wondering if there is a more efficient solution .
"I 'm building a system that reads emails from a gmail account and fetches the subjects , using Python 's imaplib and email modules . Sometimes , emails received from a hotmail account have line breaks in their headers , for instance : If I try to decode that header , it does nothing : However , if I replace the line break and tab with a space , it works : Is this a bug in decode_header ? If not , I would like to know what other special cases like this I should be aware of ."
"I have a server-like Python program that receives packets , and processes them according to the type of packet . To do so , I 've spawned multiple processes using the multiprocessing module . I 've noticed that garbage collection causes some delay during operation and packets are n't processed within the desired time frame . I know how to disable garbage collection : However my question is how exactly does Python handle garbage collection when there are multiple processes or Threads involved ? Are there differences between garbage collection of processes or threads ? Do I need to alter the garbage collection for each processes/thread ? Or does single change in garbage collection in the parent process/thread take care of handling for all child processes/threads as well ? My current situation uses Python 2.7 however I would be interested to know if it 's the same for both Python 2 and Python 3 ."
"I 'm writing python in visual studio 2015 . I want to ask how to step into a library if I 'm using a function that is defined in the library.For example , I wrote the following code , And I want to step into the makeregression ( ) function at 4th line ( this function is defined in samples_generator.py , scikit-learn package ) to see what is happening inside of the function . When I press F11 ( shortcut for step into ) in visual studio , instead of moving into samples_generator.py , the arrow just moves to next line of my code.Is there a way to let me step into a function which is defined in a library ? Appreciate your help ."
"Classes have a defineable function __exit__ that allows implementation of a context manager.It takes the required arguments : but I can not find a definitive definition of what those arguments are and their types.Here 's my best guess of what they are and why , but I 'm not entirely sure : exc_typePython defines a TracebackException class that accepts an exc_type argument which is used contextually in the constructor within issubclass with SyntaxError , which infers that exc_type is indeed some sort of Exception , which SyntaxError inherits from.exc_valAlso , in that TracebackException class is an exc_value argument that matches up to our exc_val which seems to have various attributes like __cause__ , __context__ , and other attributes that are all defined in TracebackType itself . This makes me think that the parameter is itself an instance of TracebackException.exc_tbPython defines a walk_tb function that uses exc_tb as an argument ( manually traced from docs.python.org ) , and this object appears to have tb_frame , tb_lineno , and tb_next attributes which can be traced back to a TracebackType class in the typeshed library.Thoughts ?"
"I have created an HDFStore.The HDFStore contains a group df which is a table with 2 columns.The first column is a string and second column is DateTime ( which will be in sorted order ) .The Store has been created using the following method : Once the HDF5 file is created , i 'm querying the table using the following method : So , basically this is taking 13.2 seconds , then I added an index to this column using And then I again did the same query , this time I got the following : -From the above , it seems to me there is n't a significant improvement in the performance . So , my question is , what else can I do here to make my query faster , or is there something I 'm doing wrong ?"
"I 'm using urllib2 's urlopen function to try and get a JSON result from the StackOverflow api.The code I 'm using : The result I 'm getting : I 'm fairly new to urllib , but this does n't seem like the result I should be getting . I 've tried it in other places and I get what I expect ( the same as visiting the address with a browser gives me : a JSON object ) .Using urlopen on other sites ( e.g . `` http : //google.com '' ) works fine , and gives me actual html . I 've also tried using urllib and it gives the same result.I 'm pretty stuck , not even knowing where to look to solve this problem . Any ideas ?"
"Suppose I have a pandas.DataFrame called df . The columns of df represent different individuals and the index axis represents time , so the ( i , j ) entry is individual j 's observation for time period i , and we can assume all data are float type possibly with NaN values.In my case , I have about 14,000 columns and a few hundred rows.pandas.corr will give me back the 14,000-by-14,000 correlation matrix and it 's time performance is fine for my application.But I would also like to know , for each pair of individuals ( j_1 , j_2 ) , how many non-null observations went into the correlation calculation , so I can isolate correlation cells that suffer from poor data coverage.The best I 've been able to come up with is the following : The memory footprint and speed of this begin to be a bit problematic.Is there any faster way to get at the common observations with pandas ?"
"I placed a ClientConnectionError exception in a multiprocessing.Queue that was generated by asyncio . I did this to pass an exception generated in asyncio land back to a client in another thread/process.My assumption is that this exception occurred during the deserialization process reading the exception out of the queue . It looks pretty much impossible to reach otherwise.I figure it 's a long shot to ask , but does anyone know anything about this issue ? Python 3.6.8 , aiohttp.__version__ == 3.6.0Update : I managed to reproduce the issue ( credit to Samuel in comments for improving the minimal reproducible test case , and later xtreak at bugs.python.org for furthing distilling it to a pickle-only test case ) : References : https : //github.com/aio-libs/aiohttp/issues/4077https : //bugs.python.org/issue38254"
How are the sign bits determined when initializing an ndarray from empty memory ? These float values initialized from empty memory have lost their signs† . Why is that ? †Note : this result relies on implementation detail of memory re-use . The question asks what the implementation is doing .
"I have a list of transactions/tuples in Python with varying number or elements , like this : I would like to store this list in a tabular form ( preferably in a pd.DataFrame ) such as this : But if try to convert directly using pd.DataFrame , I get his instead : How can I convert this type of list into a binary table ?"
"I have a computation project with heavy use of log function ( for integers ) , billions of calls . I find the performance of numpy 's log is surprisingly slow.The following code takes 15 to 17 secs to complete : However , the math.log function takes much less time from 3 to 4 seconds.I also tested matlab and C # , which takes about 2 secs and just 0.3 secs respectively . matlabC # Is there any way in python that I can improve the performance of log function ?"
In Python i can slice array with `` jump-step '' . Example : Can Ruby do it ?
"Is there a methods to retrieve timezone names in another language ? In Python , if I do something like this : The result is in english , but what if I would like to have it in spanish or arabic ?"
"While playing with new f-strings in the recent Python 3.6 release , I 've noticed the following : We create a foo variable with value bar : Then , we declare a new variable , which is our f-string , and it should take foo to be formatted : Ok , all going fine and then we call baz to check its value : Let 's try to change the value of foo and call baz again : Should n't it be dynamic ? Why does this happen ? I thought the f-string would update if the value of foo changed , but this did n't happened . I do n't understand how this works ."
"I 'm creating a module called Qt ( Github link ) where I alias another module ( for example PyQt4 ) , so that when I import Qt I am in fact importing PyQt4 : Example Qt module : However , If I add print sys at the end of Qt.py , sys equals None : Why does sys `` break '' and can I avoid breaking it ? This is in Python 2.7.11 on OS X . @ Lol4t0 – Here 's an example where I 'm only using standard modules : So importing this will result in that `` None is printed :"
"I want to be able to pull out the type and count of letters from a piece of text where the letters could be in any order . There is some other parsing going on which I have working , but this bit has me stumped ! I could use search or scan and repeat for each possible letter , but is there a clean way of doing it ? This is as far as I got :"
"Is it possible to get all combinations of elements in case they are neighbours ? Here is the example : EDIT : I want to use it on strings , not only numbers . For example : [ Explain , it , to , me , please ] List : Result : There would n't be for example [ 0,2,3 ] etc . in the result because 0 and 2 are not neighbours in the sorted list above.I tried to use itertools.combinations but it gives all combinations ."
I 'm trying to use this guide to get access token.Here is my main file : Here 's the make_basic_auth_header ( ) function : But all I get in r.json ( ) is : I 'm frustrated - what am I doing wrong ?
"So heres my issue I am given a string like this onethen I have to separate it into something that will end up looking like thisSo first I thought lets try using split and maybe that will work so that was my first try and it just came out like thisAfter that failed I though to myself that I am going to have to split twice to get rid of the '00 ' and the '020511 ' , so I used the methodDoing that method I get this back ... But its not in the format that I wanted it in and its not coming out right , when I split it will just get rid of the values that I was like say '10 ' will come out as 1 because the program splits the 0 's and Im not entirely sure how to come up with a solution for this so any help is apreciated.. Thanks ."
"This is my dataframe : Now when I groupby , I want to update sets . If it was a list there was no problem . But the output of my command is : What should I do in groupby to update sets ? The output I 'm looking for is as below :"
"I have the following dataframe where I want to assign the bottom 1 % value to a new column . When I do this calculation with using the `` .loc '' notification , it takes around 10 seconds for using .loc assignment , where the alternative solution is only 2 seconds.Why is the .loc solution slower ? I understand using the .loc solution is safer , but if I want to assign data to all indices in the column , what can go wrong with the direct assignment ?"
"With scapy we can to this : What kind of operation does '/ ' really do ? What kind of object does it return ? If I type p in a python interpreter it returns thisWhat does '| ' mean in this case ? I was trying to change the field dst of IP after creating p without recreating the object entirely , but I did n't manage to do it as I do n't know what kind of object I am facing.Thank you ."
"I would like to get the 07h00 value every day , from a multiday DataFrame that has 24 hours of minute data in it each day . In this 10 000 row DataFrame spanning 7 days , how would I get the 7am value each day as efficiently as possible ? Assume I might have to do this for very large tick databases so I value speed and low memory usage highly.I know I can index with strings such as : But what I need is basically a wildcard style query for example"
"I have an application built using PySide2 which uses setColumnStretch for column stretching and setRowStretch for row stretching . It works well and good , but i am unable to understand how it is working . I refer to the qt docs , but it does n't helped me . I am stuck on the two values inside those parenthesis.For example : This produces the output as shown in the image below : But how ? What does these four values inside glay.addWidget ( widget , 2 , 0 , 1 , 3 ) do ? Please explain me all this with examples ."
"I am trying to find a way to create text to speech in python ( I am on windows 7 ) . I am using pyinstaller to compile this program . I have tried a large number of approaches , including using Google 's unofficial text to speech program accessed through the urllib2 module . This ends up creating an mp3 file . For details on the code , much of this code is from http : //glowingpython.blogspot.com/2012/11/text-to-speech-with-correct-intonation.html . I have then needed to play the mp3 file that this generates . I have used mplayer , ffmpeg , mp3play , audiere , pydub , and pygame all with the same results : no sound was played , yet no exceptions were raised . I have even used the same pygame code on a raspberry pi and successfully played an mp3 file . I have also tried converting it to a wav file , which has worked fine , only when I try to play it with pygame or winsound , the same thing happens . No sound , no exceptions . My current code uses winsound , playing a wav file that I can successfully play in the windows media player ( I can even open it in windows media player from python , using os.startfile ( ) ) . Here it is : I am also trying to use pygame mixer an music modules . For example : I have even played sounds from python successfully with the winsound and win32api Beep ( ) functions . However , this obviously can not play an mp3 or wav file . I have also tried a completely different text to speech engine , which plays the sound without an mp3 file in the mix , using pyttsx : This has also failed to create sound , or raise an exception . Because of this pattern , I have a feeling that this has something to do with the system , but it does n't seem like it is something obvious.Because this almost definitely has something to do with the hardware ( pygame.mixer has worked this way on different hardware , and I am sure it usually works on windows ) it may be important to know I am using a Toshiba laptop . Also , I am using python 2.7.Ideally , I would like to do this with pygame , because I have the most experience using it and there are some sound editing features I would like to access in pygame if at all possible.I also tried using 64 bit python ( I was using 32 bit python on 64 bit windows 7 ) . It still failed to work.I also tried playing an mp3 file inside a Ubuntu virtual box environment , but on the same device . It still did n't work . This is n't particularly surprising because virtualbox uses a lot of the resources ( like screen and wifi ) from the host operating system , hence it would n't necessarily play sounds any differently . Any way around this would be helpful . Some sounds play fine , just not specifically mp3 or wav files in python , so there is probably a solution ."
"While looking for some answers in a package source code ( colander to be specific ) I stumbled upon a string that I can not comprehend . Also my PyCharm frowns on it with 'statement seems to have no effect'.Here 's the code abstract : It seems to be extremely pythonic and I want to master it ! UPD . So , as I see it 's not pythonic at all - readability is harmed for the sake of shorthand ."
I have a list of sentences like : What i did isThe Output i got is The Output i want is : I am struggling to think how to achieve this .
"I upgraded to Mavericks , and am now unable to build my application after much troubleshooting.During linking , I receive the errorsI am using g++ from macports , specifically g++-mp-4.4 . Clang is not an option for my project , as my project depends upon OpenMP , which is currently incompatible with Clang.I have not been using the C++11 runtime , but the Boost.Python library from macports ( the one I am using during linking ) is linked against it . Is this the root of my problem ? Compiling part of my project with Clang succeeds ( and hurdles this linking error stage ) , yet as I mentioned , I required OpenMP.Notably , running : outputswhich have std : :__1 prefixes , which do not match what is expected in the undefined symbols errors ..."
"I have written a simple web application with embedded web server ( tornado ) , database ( sqlalchemy using sqlite for now ) , and the whole shabang . I would like to bundle it all up into a single self-contained directory with a single exe that can be run . The deployment scenario absolutely demands a one click install and run like this.I have absolutely failed trying to get py2exe or pyinstaller to bundle up my code . The problem has directly to do with the directory structure and layout , which is as follows . I do not want to change the directory layout to much . Can someone suggest how I can get this with either py2exe or pyinstaller or any other suitable tool ?"
"I 've trying to get a loop in python to run as fast as possible . So I 've dived into NumPy and Cython . Here 's the original Python code : It 's just a sum over k indices . Array sizes are nt=12 , nz=75 , ny=559 , nx=1442 , so ~725 million elements . That took 68 seconds . Now , I 've done it in cython asand that took 49 seconds . However , swapping the loop for only takes 0.29 seconds ! Unfortunately , I ca n't do this in my full code . Why is NumPy slicing so much faster than the Cython loop ? I thought NumPy was fast because it is Cython under the hood . So should n't they be of similar speed ? As you can see , I 've disabled boundary checks in cython , and I 've also compiled using `` fast math '' . However , this only gives a tiny speedup . Is there anyway to get a loop to be of similar speed as NumPy slicing , or is looping always slower than slicing ? Any help is greatly appreciated ! /Joakim"
I have a python dictionary as below : I want to convert it into a data frame as below : I tried with code below : The results is not what I desired.Could anybody know how to fix this ? Thanks !
"I using Open CV and skimage for document analysis of datasheets.I am trying to segment out the shade region separately .I am currently able to segment out the part and number as different clusters.Using felzenszwalb ( ) from skimage I segment the parts : But not able to connect them . Any idea to connect methodically and label out the corresponding segment with part and part number would of great help .Thanks in advance for your time – if I ’ ve missed out anything , over- or under-emphasised a specific point let me know in the comments ."
"Orientation : I have created the following functions to allow the user to change the turtle to an image of the his/her choosing and then stamp it to the canvas at any point : The image can also be manipulated to the user 's choosing by these other functions : Resize function – This function works either as a first or secondary function . First meaning that it is called initially , and secondary meaning it edits an already edited image . So , if ONLY called first , this function will take the image appended to the pictures deque , resize that , and output the edited image as a .gif image , which will be the new shape of the turtle . However , if called two times or more in a row , because of an issue where resizing the same picture more than once will result in a distorted image , I had to create another deque jiop which saves the original item from the pictures deque , and whenever this function is called more than once in a row , that original image is resized every time , instead of the same image each time . But , if ONLY called as a secondary function , then the function will simply take the current image from the edited deque , resize that image , and then set that as the turtle 's new shape : Flip , Rotate , and Mirror functions - These work rather simpler than the resize function above . If called initially , they each will take the image from the pictures deque , manipulate it , append that edited image to the edited deque , then change the turtle `` shape '' to that new image . However , if called second , they each will take the image from the edited deque , manipulate that , re-append the manipulated image back to the edited deque , then set that as the turtle 's new `` shape '' . These functions are shown below : This way ALL the editing functions work together on essentially the same fundamental image . The Issue : Now , consider that the user wants to take the turtle image and then resize it to the size , for instance , 800x400 , and stamp it to a specific spot on the canvas . After that , the user decides to move the turtle image to another spot on the canvas , flip the image , and then stamp the image there . There should now be two images right ? One stamped , and the other flipped ? However , with my program , for some reason , that is not the case . Instead , the stamped image disappears the moment the user flips the turtle image , even though there is no clear ( ) function to be found anywhere ( to show you what I mean , refer to the edit below ) . Apparently this issue ONLY occurs after the TurtleImageResize function is called.What is wrong in my TurtleImageResize function that is leading to this issue ? I had completely revamped the turtle shape 's image management process to what it is right now in hopes that it will fix this issue that I was also experiencing with my previous setup , but apparently , that is STILL not the case . Therefore , any help with this issue is greatly appreciated ! EDIT : Below is a minimal , complete , and verifiable way to reproduce the issue I am having ( MUST have PIL ( or Pillow ) and GhostScript installed in order for this to work ) : When/if you have both GhostScript and PIL ( or Pillow ) installed on your system , to reproduce my issue , please do the following ( All steps required except step # 4 ) : Click the Set Turtle Image button at bottom of window , select any image you want the turtle to be , then press Open . The turtle gets set to that image.Resize the Image to 800x400 ( or any other size you want ) by pressing the Resize turtle Image button at the bottom of the screen . Two dialogs will pop up in succession . Enter the width of 800 ( or your own width ) in the first dialog , and then enter the height of 400 ( or your own height ) in the second dialog , and after you finish , the image will change size according to the dimensions provided ( or set image back to the original dimension ( s ) depending on whether or not you pressed cancel ) .Select the Stamp button at the bottom of the window . The image is stamped onto the canvas , and the turtle moves forward 400 pixels `` behind '' the stamped image.OPTIONAL : Click anywhere on the canvas to take the turtle to that spot.Flip/mirror/rotate the image.As you can see , after doing all this , just as you flip/mirror/rotate the image , the stamped image just disappears . What is wrong with my TurtleImageResize function that is causing this to occur ? EDIT # 2 : Just in case this information is useful , I am running Python 3.5.1 on a Macintosh with OS version 10.11.2 ( El Capitan ) ."
"For example we have large list of objects like this : And we need to agregate lots of metrix about this obejct - min , max , sum , mean , stdev values of it propertirs . Currently i do it with code like this : Question : Is here is more `` pythonic '' way with better perfomance and memory usage ?"
"Let 's say I have the following two lists of tuplesI would like to design a merge operation that merges these two lists by checking for any intersections on the first element of the tuple , if there are intersections , add the second elements of each tuple in question ( merge the two ) . After the operation I would like to sort based upon the first element.I am also posting this because I think its a pretty common problem that has an obvious solution , but I feel that there could be very pythonic solutions to this question ; )"
"I have two arrays , for example with shape ( 3,2 ) and the other with shape ( 10,7 ) . I want all combinations of the two arrays such that I end up with a 9 column array . In other words , I want all combinations of each row of the first array with the rows of the second array.How can I do this ? I am not using meshgrid correctly as far as I can tell.Based on previous posts , I was under the impression that would work , but that gives me dimensions of ( 84,10 ) ."
"I want to replace elements in a numpy array using a list of old values and new values . See below for a code example ( replace_old is the requested method ) . The method must work for both int , float and string elements . How do I do that ?"
"Is it possible to initialise a ptr to NULL from the python side when dealing with SWIG module ? For example , say I have wrapped a struct track_t in a swig module m ( _m.so ) , I can create a pointer to the struct from python as follows : this will just malloc a track_t for me in the appropriate wrapper function.I would however like to be able to achieve the following : But in python rather than C , e.g . initialise a pointer to NULL from the python sideI need to do this as it is a requirement of the hash table C implementation I am using that new hash tables start with a NULL pointerI could write a helper function , say , track_t* create_null_track ( ) that returns a NULL pointer but thought there may be a simpler way ? EDIT : I can confirm that the helper function works but I would expect there to be a built in way of doing this as requiring a NULL pointer would seem a common requirementhelper function is as simple as : not sure that the return type information specified in the function is necessary so a more generic approach could be : perhaps ?"
"I often find myself writing code like : Okay - it 's generally not this simple [ and usually it really is with long lists , I just chose this ( see xkcd ) for fun ] , but I create a list , iterate over it doing things with those elements . While doing this , I will discover new things that I will need to iterate over , and I put them into a new list which I then iterate over.It appears to be possible to write : I know that it 's considered dangerous to modify a list while iterating over it , but in this case I want to iterate over the new elements.Are there dangers from doing this ? The only one I can think of is that the list may grow and take up a lot of memory ( in many of my cases I actually want to have the whole list at the end ) . Are there others I 'm ignoring ? Please note : Python : Adding element to list while iterating is related , but explains ways to create a copy of the list so that we can avoid iterating over the original . I 'm asking about whether there is anything wrong in my specific case where I actually want my iteration to be extended.edit : here is something closer to the real problem . Say we want to generate the `` k-core '' of a network . That is , delete all nodes with degree less than k. From remaining network delete all nodes with degree less than k. Repeat until none left to delete . The algorithm would find all less than k nodes to begin with , put them in a to_delete list . Then as nodes are deleted , if a neighbor 's degree becomes k-1 , add it to the list . This could be done by :"
"I use a simple Flask application with gunicorn 's gevent worker to serve server-sent events.To stream the content , i use : which streams events from redis : deployed with : But after its used for a while , i have 50 redis connections open , even when no one is connected to the server-sent events stream anymore.It seems , like the view does not terminate , because gunicorn is non-blocking and pubsub.listen ( ) is blocking.How can i fix this ? Should i limit the number of processes gunicorn may spawn , or should flask kill the view after some timeout ? If possible , it should stop the view/redis connections on inactivity , without disconnecting users , who are still connected to the SSE stream ."
"I 'd like to add two numpy arrays of different shapes , but without broadcasting , rather the `` missing '' values are treated as zeros . Probably easiest with an example likeorI do not know the shapes in advance.I 'm messing around with the output of np.shape for each , trying to find the smallest shape which holds both of them , embedding each in a zero-ed array of that shape and then adding them . But it seems rather a lot of work , is there an easier way ? Thanks in advance ! edit : by `` a lot of work '' I meant `` a lot of work for me '' rather than for the machine , I seek elegance rather than efficiency : my effort getting the smallest shape holding them both is not pretty : -/"
"I 'm trying to compute a simple dot product but leave nonzero values from the original matrix unchanged . A toy example : Desired outcome : The actual matrices in question , however , are sparse and look more like this : One simple way go would be just setting the values using mask index , like that : However , my original arrays are sparse and quite large , so changing them via index assignment is painfully slow . Conversion to lil matrix helps a bit , but again , conversion itself takes a lot of time.The other obvious approach , I guess , would be just resort to iteration and skip masked values , but I 'd like not to throw away the benefits of numpy/scipy-optimized array multiplication.Some clarifications : I 'm actually interested in some kind of special case , where B is always square , and therefore , A and C are of the same shape . So if there 's a solution that does n't work on arbitrary arrays but fits in my case , that 's fine.UPDATE : Some attempts : ANOTHER UPDATE : Could n't make anything more or less useful out of Cython , at least without going too far away from Python . The idea was to leave the dot product to scipy and just try to set those original values as fast as possible , something like this : This was a bit better then my pre-first `` naive '' implementation ( using .tolil ( ) ) , but following hpaulj 's approach , lil can be thrown out . Maybe replacing python dict with something like std : :map would help ."
"I really miss something basic about python 's logging module.In the following code , I create a logger object ( log ) and add to it two handlers . One with 'INFO ' level and one with 'WARNING ' level . Both of them are supposed to print to stdout . I expect that calling to log.info ( msg ) will result in one copy of msg in my stdout and calling to log.warn ( msg ) sould result in two copies of msg printed to my stdout . Here is the code : The output is really very strange to me . The .info call results in no visual effect.However , calling to warn results in two copies of msg printed to stdout ( which is OK ) , but also one copy printed to stderr ( why ? ) . This is the output of the above code . Note the formatting of the last line in this output . This line is printed to stderr.So my questions are : why does my call to info result in no output , despite the fact that h1 's level is set to INFO ? why does my call to warn results in additional output to stderr ?"
"I ran down a bug today that came about because I was using next ( ) to extract a value , and 'not found ' emits a StopIteration.Normally that would halt the program , but the function using next was being called inside an all ( ) iteration , so the all just terminated early and returned True.Is this an expected behavior ? Are there style guides that help avoid this kind of thing ? Simplified example :"
"I have set up a CloudSQL instance that I am attempting to use with my Django app on AppEngine . I 've confirmed that the server is set to use utf8mb4 character set via the CloudSQL console for my database : If I connect directly with the mysql cli , I can successfully insert and read emojis . However , if I insert the same emoji characters through the Django admin it 's just inserted as `` ? ? ? ? `` .I attempted to ensure the MySQLdb-python client is using utf8mb4 with : But this causes me to receive the following error on AppEngine : My app.yaml is using the `` latest '' MySQLdb library :"
"I often use virtualenv to keep right version of dependancies installed for a project.The problem is that when using that , virtualenv keep installing distribute==0.6.19I need each time to run first : Why is that and how can I make it install directly the right version of distribute ? Thank you ."
"For example , I have two numpy arrays , and I want to extract one element from each row of A , and that element is indexed by B , so I want the following results : I tried A [ : , B.ravel ( ) ] , but it 'll broadcast B , not what I want . Also looked into np.take , seems not the right solution to my problem.However , I could use np.choose by transposing A , but any other better solution ?"
"If I have : Is there a way to find the x binding that f2 will use ? I looked at inspect but could not tell if some of the frame stuff would apply . In other words , could I define a closed_vars ( ) below :"
"When including the linein a local .gitattributes file , git diff produces nice labels for the different diff hunks of Python files ( with the name of the function where the changes are , etc . ) .Is is possible to ask git to use this diff mode for all Python files across all git projects ? I tried to set a global ~/.gitattributes , but it is not used by local git repositories . Is there a more convenient method than initializing each new git project with a ln -s ~/.gitattributes ?"
"I have a large CSV file full of stock-related data formatted as such : Ticker Symbol , Date , [ some variables ... ] So each line starts of with the symbol ( like `` AMZN '' ) , then has the date , then has 12 variables related to price or volume on the selected date . There are about 10,000 different securities represented in this file and I have a line for each day that the stock has been publicly traded for each of them . The file is ordered first alphabetically by ticker symbol and second chronologically by date . The entire file is about 3.3 GB.The sort of task I want to solve would be to be able to extract the most recent n lines of data for a given ticker symbol with respect to the current date . I have code that does this , but based on my observations it seems to take , on average , around 8-10 seconds per retrieval ( all tests have been extracting 100 lines ) .I have functions I 'd like to run that require me to grab such chunks for hundreds or thousands of symbols , and I would really like to reduce the time . My code is inefficient , but I am not sure how to make it run faster.First , I have a function called getData : ( This code has a couple of helper functions , checkMatch and formatLineData , which I will show below . ) Then , there is another function called getDataColumn that gets the column I want with the correct number of days represented : ( changeRateTransform converts raw numbers into daily change rate numbers if True . ) The helper functions : Does anyone have any insight on what parts of my code run slow and how I can make this perform better ? I ca n't do the sort of analysis I want to do without speeding this up.EDIT : In response to the comments , I made some changes to the code in order to utilize the existing methods in the csv module : Performance was worse using the csv.reader class . I tested on two stocks , AMZN ( near top of file ) and ZNGA ( near bottom of file ) . With the original method , the run times were 0.99 seconds and 18.37 seconds , respectively . With the new method leveraging the csv module , the run times were 3.04 seconds and 64.94 seconds , respectively . Both return the correct results.My thought is that the time is being taken up more from finding the stock than from the parsing . If I try these methods on the first stock in the file , A , the methods both run in about 0.12 seconds ."
"I have a dict like below : I want to transform this dict into this form : How can I solve this ? So far I tried this solution , It works as I expected , but it looks very ineffective way to achieve this task . Is there a way to solve this problem ?"
"I have an ImageField for a `` People '' model..models.pyThe problem is , when I try to add a people object through Django Admin , and I select an image file , Django admin adds weird characters ( that looks like a hash value ) at the end of the image , which causes the image to not appear on the website..For example , this is my Django Admin page for adding people : When I click save and check my admin , this is what appears : As you can see , it added `` _PQSSbcg '' at the end of the image name for some reason.. Because of this , the website fails to display the image , because the template tries to find `` jose_atria.jpg '' ..Why is Django Admin adding this extra characters , and how do I get rid of this ? Thanks"
How can I revert back to the default function that python uses if there is no __str__ method ?
"I have a package that has a module that helps users generate specific plots using matplotlib . When I call these functions in my unit tests inside Travis , I get the following error : How do I fix that ? After generating the plot , my functions usually call pyplot.show ( ) , which opens a window that needs to be closed . When I call these functions in my unit tests inside Travis , they get hung up forever.How do I test that these plots are being generated using Travis CI ?"
"I have a non-standard CSV file that looks something like this : Using pd.read_csv ( ) leads to something that 's not all that useful , because the tuples are not parsed . There are a existing answers that address this ( 1 , 2 ) , but because these tuples have heterogeneous lengths , those answers are n't entirely useful for the problem I 'm having.What I 'd like to do is plot x vs y using the pandas plotting routines . The naive approach leads to an error because the tuples are stored as strings : The result I 'd hope for is something like this : Is there a straightforward way to create this plot directly from Pandas , by transforming the dataframe and using df.plot.scatter ( ) ( and preferably without using eval ( ) ) ?"
"Say I have the classSeeing as these methods are private , and I am accessing pname through name , how do I test with AssertRaises when AssertRaises only accepts a callable for its test ?"
My objective is simple but not sure if it 's possible . Reproducible example : Can you go from this : To this : Via a groupby ? I can get this far df.groupby ( [ 'score ' ] ) .agg ( { 'score ' : np.size } ) but ca n't work out how to create the new columns with the column values .
"I love the typing.NamedTuple in Python 3.6 . But there 's often the case where the namedtuple contains a non-hashable attribute and I want to use it as a dict key or set member . If it makes sense that a namedtuple class uses object identity ( id ( ) for __eq__ and __hash__ ) then adding those methods to the class works fine.However , I now have this pattern in my code in several places and I want to get rid of the boilerplate __eq__ and __hash__ method definitions . I know namedtuple 's are not regular classes and I have n't been able to figure out how to get this working.Here 's what I 've tried : Is there a way I do n't have to repeat these methods in each NamedTuple that I need 'object identity ' in ?"
"I wrote a little code to parse a XML file , and want to print it 's characters , but each character seems invoke characters ( ) callback function three times.code：xml file : and the output is like below , many blank lines.I think it should like : Why there are space line ? and I read the doc info : characters ( self , content ) Receive notification of character data . The Parser will call this method to report each chunk of character data . SAX parsers may return all contiguous character data in a single chunk , or they may split it into several chunks ; however , all of the characters in any single event must come from the same external entity so that the Locator provides useful information.so SAX will process one character area as several fragments ? and callback several times ?"
"I have a list of dicts : I want to update the value of each element in this list by the sum of all remainders . ( so 'one ' will get the value 2+3+4+5 ) .so that it will look like this : 'five ' is the last , so it will not update .Im not sure how to achieve this . Im thinking that you construct a function that will call itself recursivly something like : But Im not sure to do this list ( a [ 0 ] .values ( ) ) [ 0 ] is the `` best '' way . And this is also getting a KeyError : 0.Any ideas ?"
"I would like to be able to access data on a google sheet when running python code via cloud composer ; this is something I know how to do in several ways when running code locally , but moving to the cloud is proving challenging . In particular I wish to authenticate as the composer service account rather than stashing the contents of a client_secret.json file somewhere ( be that the source code or some cloud location ) . For essentially the same question but instead accessing google cloud platform services , this has been relatively easy ( even when running through composer ) thanks to the google-cloud_* libraries . For instance , I have verified that I can push data to bigquery : and the success or failure of this can be managed through sharing ( or not ) 'test dataset ' with the composer service account . Similarly , getting data from a cloud storage bucket works fine : and once again I have the ability to control access through IAM . However , for working with google sheets it seems I must resort to the Google APIs python client , and here I run into difficulties . Most documentation on this ( which seems to be a moving target ! ) assumes local code execution , starting with the creation and storage of a client_secret.json file example 1 , example 2 , which I understand locally but does n't make sense for a shared cloud environment with source control . So , a couple of approaches I 've tried instead : Trying to build credentials using discovery and oauth2Caveat : I know nothing about working with scopes to create credential objects via Http . But this seems closest to working : I get an HTTP403 error of 'Request had insufficient authentication scopes . 'However , I do n't know if that means I successfully presented myself as the service account , which was then deemed unsuitable for access ( so I need to mess around with permissions some more ) ; or did n't actually get that far ( and need to fix this credentials creation process ) .Getting a credential object with google.auth and passing to gspreadMy ( limited ) understanding is that oauth2client is being deprecated and google.auth is now the way to go . This yields credentials objects in a similarly simple way to my successful examples above for cloud platform services , that I hoped I could just pass to gspread : Sadly , gspread does n't work with these objects , because they do n't have the attributes it expects : AttributeError : 'Credentials ' object has no attribute 'access_token'This is presumably because gspread expects oauth2 credentials and those chucked out by google.auth are n't sufficiently compatible . The gspread docs also go down the 'just get a client_secret file ' ... but presumably if I can get the previous ( oauth/http-based ) approach to work , I could then use gspread for data retrieval . For now , though , a hybrid of these two approaches stumbles in the same way : a permission denied response due to insufficient authentication scopes.So , whether using google.auth , oauth2 ( assuming that 'll stick around for a while ) or some other cloud-friendly approach ( i.e . not one based on storing the secret key ) , how can I obtain suitable credentials in a cloud composer environment to make calls to the google sheets API ? Bonus marks for a way that is compatible with gspread ( and hence gspread_dataframe ) , but this is not essential . Also happy to hear that this is a PEBCAK error and I just need to configure IAM permissions differently for my current approach to work ."
Memory ( row major order ) : I guess the algorithm work like this in the following cases.Broadcasting Dimension is last dimension : Broadcasting dimension is 0th dimension : Question : How does numpy know which order of multiplication is the best . ( reading memory in order is better than reading memory all over the place . but how did numpy figure that out ? ) What would numpy do if the arrays have more than two dimensionWhat would numpy do if the broadcasting dimension is not the last dimension ? 2nd guess of what is going on :
"So after hours or reading post and looking at the documentation for tkinter I have found that on windows machines the color options for tkinter scrollbar will not work due to the scrollbar getting its theme from windows directly . My problem is the color of the default theme really clashes with my program and I am trying to find a solution that does not involve importing a different GUI package such as PyQt ( I do n't have access to pip at work so this is a problem to get new packages ) Aside from using a separate package can anyone point me towards some documentation on how to write my own sidebar for scrolling through the text widget . All I have found so far that is even close to what I want to be able to do is an answer on this question . ( Changing the apperance of a scrollbar in tkinter using ttk styles ) From what I can see the example is only changing the background of the scrollbar and with that I was still unable to use the example . I got an error on one of the lines used to configure the style.Not sure what to do with this error because I was just following the users example and I am not sure as to why it worked for them but not for me.What I have tried so far is : How I create my text box and the scrollbars to go with it.Following the documentation here My attempt below does not appear to do anything on windows machine . As I have read on other post this has to do with the scrollbar getting its theme natively from windows . I guess it all boils down to : Is it possible to create my own sidebar ( with colors I can change per theme ) without the need to import other python packages ? If so , where should I start or can someone please link me to the documentation as my searches always seam to lead me back to Tkinter scrollbar Information . As these config ( ) options do work for linux they do not work for windows ."
"I am trying to make a door swipe card system in Python for my Raspberry Pi . I broke the program into two : A Door Alarm and a Card Swipe Recording system . The two programs work individually but how do I combine the two programs into one python file ? I 've tried threading but it does n't seem to work . Below are the programs:1 . ) Door Alarm : If door is left open for a certain duration , an led will blink , then an alarm will ring2 . ) Card Swipe Recording System : When someone swipes their card , the led blinks and a picture is taken ( UPDATE ) Also , below is my attempt at threading :"
"Say I define the following variable using ctypes moduleand afterwards I try to find out the memory address of i using : or which , at the moment , yield different values . Why is that ?"
"I 'm looking for a more efficient way to reprioritize items in a priority queue . I have a ( quite naive ) priority queue implementation based on heapq . The relevant parts are like : And here is a simple co-routine to just demonstrate the reprioritize characteristics in my real application.With testingproducing : Now , my question is , does there exist any data-structure which would avoid calls to heapify ( . ) , when repriorizating the priority queue ? I 'm here willing to trade memory for speed , but it should be possible to implement it in pure Python ( obviously with much more better timings than my naive implementation ) .Update : In order to let you to understand more on the specific case , lets assume that no items are added to the queue after initial ( batch ) pushes and then every fetch ( pop ) from the queue will generate number of repriorizations roughly like this scheme:0* n , very seldom 0.05* n , typicallyn , very seldom where n is the current number of itemsin queue . Thus , in any round , there are more or less only relative few items to repriorizate . So I 'm hoping that there could exist a data-structure that would be able to exploit this pattern and therefore outperforming the cost of doing mandatory heapify ( . ) in every round ( in order to satisfy the heap invariant ) . Update 2 : So far it seems that the heapify ( . ) approach is quite efficient ( relatively speaking ) indeed . All the alternatives I have been able to figure out , needs to utilize heappush ( . ) and it seems to be more expensive what I originally anticipated . ( Anyway , if the state of issue remains like this , I 'm forced to find a better solution out of the python realm ) ."
"In the aiohttp 's doc reads : loop – event loop used for processing HTTP requests . If loop is None the constructor borrows it from connector if specified . asyncio.get_event_loop ( ) is used for getting default event loop otherwise . Deprecated since version 2.0.I googled but did n't get any description about why the loop parameter is deprecated.I often create ClientSession object like this : Now the loop parameter is depracted , but just call aiohttp.ClientSession ( ) without the loop will get a warning : Creating a client session outside of coroutineSo why the parameter is deprecated and how to use the session correctly ?"
"In Python 3 , modules can be namespace modules without an __init__.py ( as per PEP 420 ) or as a regular module ( i.e . ' [ modules ] packages as they are implemented in Python 3.2 and earlier ' - PEP 420 ) that have an __init__.py or are a single .py file.How can you tell the difference between a namespace module and an 'ordinary ' module ? ( I am using Python 3.5.3 ) e.g.Namespace module named mod prints out as : and ordinary modules print out as :"
"And that 's pretty obvious why it happens like this : when I type t = .. , I just 'relink ' t to other data in memory . But the questions are : how I can hack that and pass reference to the submatrix out of function ? and still be able to change this submatrix 's values ?"
"I 'm try to code Elastic-Net . It 's look likes : And I want to use this loss function into Keras : my loss function is : It 's mse+L1+L2and L1 and L2 isI use Calculate_L1 function to sum of the weight of dense1 & dense2 & dense3and Calculate_L2 do it again.When I train RB_model.compile ( loss = cost_function ( ) , optimizer= 'RMSprop ' ) the L1 and L2 variable did n't update every batch . So I try to use callback when batch_begin while using : How could I use callback in the batch_begin calculate L1 and L2 done , and pass L1 , L2 variable into loss funtion ?"
"I have a Flask app and am using Flask-Login to manage user authentication . The main route ( '/ ' ) requires login , however I would not like it to flash the error message defined for this specific route . I would like it to behave as it should flashing the error messages for other routes but not this one . Here is a snippet from my init.pyThe following is from my views.pyAs you can see I have the main route password protected , and because of the login_manager settings it is flashing the message `` You Must Login to Access This Page ! '' and of course I 'm displaying those errors on the login template . However , how can I ensure that this functions accordingly for every route EXCEPT the main ( '/ ' ) route ? I 've been through the documentation but I see nothing that would indicate it could be done.Can I simply overwrite the login_manager.login_message with None in that specific route ? If so , how would the login manager be appropriately called , overwritten , and then changed back ?"
"I am using pythons bz2 module to generate ( and compress ) a large jsonl file ( bzip2 compressed 17GB ) .However , when I later try to decompress it using pbzip2 it only seems to use one CPU-core for decompression , which is quite slow.When i compress it with pbzip2 it can leverage multiple cores on decompression . Is there a way to compress within python in the pbzip2-compatible format ?"
"USAGE CONTEXT ADDED AT ENDI often want to operate on an abstract object like a list . e.g.Now this appropriate if thing is a list , but will fail if thing is a dict for example . what is the pythonic why to ask `` do you behave like a list ? `` NOTE : this will work for all cases I can think of , but I do n't like defining a duck type negatively , as I expect there could be cases that it does not catch.really what I want is to ask . `` hey do you operate on integer indicies in the way I expect a list to do ? '' e.g.NOTE : I do not want to simply execute my operations inside of a large try/except , since they are destructive . it is not cool to try and fail here ... .USAGE CONTEXT -- A `` point-lists '' is a list-like-thing that contains dict-like-things as its elements. -- A `` matrix '' is a list-like-thing that contains list-like-things -- I have a library of functions that operate on point-lists and also in an analogous way on matrix like things . -- for example , From the users point of view destructive operations like the `` spreadsheet-like '' operations `` column-slice '' can operate on both matrix objects and also on point-list objects in an analogous way -- the resulting thing is like the original one , but only has the specified columns. -- since this particular operation is destructive it would not be cool to proceed as if an object were a matrix , only to find out part way thru the operation , it was really a point-list or none-of-the-above. -- I want my 'is_matrix ' and 'is_point_list ' tests to be performant , since they sometimes occur inside inner loops . So I would be satisfied with a test which only investigated element zero for example . -- I would prefer tests that do not involve construction of temporary objects , just to determine an object 's type , but maybe that is not the python way.in general I find the whole duck typing thing to be kinda messy , and fraught with bugs and slowness , but maybe I dont yet think like a true Pythonistahappy to drink more kool-aid ..."
"I 'm wondering what the `` best practice '' is for storing medium length strings to be used in a UI in Python/Django.Example : I have an error.html template that takes an error_description field . This is a few sentences explaining to the user what went wrong , and what they might do to address it . It may be different for different error pages , but remains fairly stable in the code ( there 's no reason that someone who ca n't push source code should be able to modify it ) , and can easily be held in memory , so I do n't think it 's the sort of thing that should be kept in the database.My current idea is that I should just create some kind of messages.py file that has a bunch of string constants like this : In general , is there some canonical way to store strings that are `` too flexible to be hard coded in '' , but `` too small and static for databases '' ( and do n't scale with your usage ) ? I 'm thinking of the sort of thing that would be in a strings.xml file in an Android project . Other possibilities I 'm juggling include a text file that views.py reads and stores as constants , actually just hardcoding them , and sticking them in template files.There 's a lot of ways to do this , and it 's not a very complicated thing , I just want to know which one is the most 'right'.Thanks ! And let me know if you need more info !"
"I 'm trying to get the first non null value from multiple pandas series in a dataframe.in this df I want to create a new column ' f ' , and set it equal to ' a ' if a is not null , ' b ' if b is not null etc . down to e. I could do a bunch of np.where statements which is inefficient.I looked into doing df.a or df.b or df.c etc.result should look like :"
"I am just starting use DEAP . Previously , I used GA based on Matlab that after crossover and mutation is to select the better individuals with specified size , and then updating the population . But it is quite hard for me to understand that why in DEAP the evaluate the individuals with an invalid fitness is needed after the crossover and mutation procedure : I tried that deleting those code , but it seems the algorithm will never convergence . And even I did't see those can updating the population/offspring , so what are those use for . Thanks in advance ! ! !"
"Why does this code give the error : RuntimeError : maximum recursion depth exceeded during compilation ? print_test never calls itself , hence I would think it is n't a recursive function.When I tested it , It worked in Python 2.7.7rc1 but gave the error in Python 3.3.5 . Pdb give a short call stack , unlike the tall one that normally exists when exceeding maximum recursion depth.I am wondering this out of curiosity , and realize this would not be best programming practices ."
"I 'm building a website using Flask and I 'm now in the process of adding some logging to it for which I found these docs . The basic example is as follows : after which you can log using app.logger.error ( 'An error occurred ' ) . This works fine , but apart from the fact that I do not see any advantage over the regular python logging module I also see a major downside : if I want to log outside of a request context ( when for example running some code with a cron job ) I get errors because I 'm using app outside of the request context.So my main question ; why would I use the Flask logger at all ? What is the reason that it was ever built ?"
I am extracting the MFCC features using two different libraries : The python_speech_features lib The BOB libHowever the output of the two is different and even the shapes are not the same . Is that normal ? or is there a parameter that I am missing ? The relevant section of my code is the following :
"I 'm trying to create a cf compliant netcdf file . I can get it about 98 % cf compliant with xarray but there is one issue that I am running into . When I do an ncdump on the file that I am creating , I see the following : The coordinates for my dataset are lat , lon , and time . When I convert to netcdf via ds.to_netcdf ( ) , all coordinate variables have fill values applied automatically because they are floats . Having a coordinate variable with a fill value applied violates cf standards ( http : //cfconventions.org/cf-conventions/v1.6.0/cf-conventions.html # attribute-appendix ) . I tried to change the encoding so these specific variables are not compressed : or by changing dtypes , but I 'm not having any luck . I 'd prefer not to reload the files using netCDF4 to remove the _FillValues . Is there a way around this that is built into xarray ?"
"Is it possible to query a column for its maximum possible size of data ( in bytes ) that can be stored in it ? For example , say I declare a column usingthen how can I query information about content ? Following the inspection approach suggested in this question : I get a max=None , whereas I would have expected this to be max=65535 considering field.type=BLOB . What am I doing wrong ?"
I am new to python . I am using dbscan code for clustering purpose with some changes.Now code is running fine but its very slow . So I found out that I have to remove 'for loop ' from my code.Here is a part of the code : distanceQuery function is using double for loop . Is there any way I can remove this ? Can I vectorize this double for loop ? As this is clustering code there are some steps which require appending . I have read that numpy array work different than python list when it comes to appending . Appending numpy arrays is inefficient.Edit : So this can be vectorize . But here is other part of code where appending is happening just after that I check for certain condition.Now if I vectorize neighbor_points also . I will have to tackle the problem of appending ? So each point will append into neighbour_points and then it will make a distanceQuery . And this process is also part of a iteration . So kind of two loops are here also.I just want to make sure that appending in numpy array wont be inefficient
I am writing a a high level interface for a C library for Python using Cython.I have an extension Type A that initializes the library with a pointer to a more complex C context structure c_context . The pointer is saved in A.A also has a def function which in turn creates another extension Type B initializing another C structure with a library function call . This structure is needed for the subsequent library calls made in B.B needs the c_context pointer from A which is wrapped by me within the extension type py_context in order to pass it to __cinit__ from B : Passing the wrapper with the correct C context works perfectly . Now I need to get the C struct out of py_context again and save it in B. I added cdef c_context get ( self ) to py_context.pxd/pyx.Calling py_context.get ( ) from Bs __cinit__ results in : AttributeError : py_context object has no attribute get . It seems like I do not get my head around when to call cdef functions in Cython . So my question is : What is the best way to extract the C struct from my wrapper class again ?
"I 'm still trying to figure out how to create packages , here is a file structure that demonstrates my problem : Within the __init__.py of the main_package let 's say I have : And within script1.py I have : This does n't work . When I try to import main_package I get an error in the following set of calls : import main_package.script1 - > from sub_package import modelApparently the from sub_package import model within script1.py does n't work.I tried the following : from main_package.sub_package import modelfrom . import sub_package.modelfrom .sub_package import modelAnd none of them work . Sorry if I 'm making a dumb mistake somewhere , but what would be the problem way to fix my issue ? UPDATE : Ok some people asked exactly how this happens , so I 'll post the actually error message I 'm getting and my actual structure and procedure . Sorry , the names are now changed up from the above.Here is my real package structure for the packages which cause an error : I run script.py in the terminal ( i.e ipython script.py [ args ] ) and get the following error message ( I erased everything that comes after the problem messages and replaced it with ... to make it clearer ) .Models definitely exists by the way ."
"What 's C # 's equivalence of the following Python 's min/max code : It seems that C # 's Enumerable.Min is very close . But according to its MSDN doc , it always returns the minimizing VALUE ( not the original object ) . Am I missing anything ? EDITPlease note - I 'm not inclined to achieve this by sorting first , since sorting ( O ( nlogn ) ) is computationally heavier than finding the minimum ( O ( n ) ) .Please also note - Dictionary is not a desired approach either . It can not handle cases where there are duplicate keys - ( 1 , `` cat '' ) and ( 1 , `` tiger '' ) . More importantly , dictionary can not handle cases where the items to be processed is a complex class . E.g. , finding minimum over a list of animal objects , using age as the key :"
"maybe this is a easy question , but is there a fast way to duplicate elements in a array ? It should work like this way for 3D : I tried it with 3 nested for-loops , but this was really slow ."
Lets say I have following HTML CodeNow If I use driver.find_elements_by_class_name ( `` something '' ) then I get all the classes present in the HTML code . But I want to get classes only after a specific word ( `` Today '' ) in HTML . How to exclude classes that appear before the specific word . Next divs and classes could be at any level .
"I have a little program which prints random lines from a text file . I want to save the the already chosen lines in a list or something else , so it do n't will repeat next time.Example text_database.txt This is a line This is an other line This is a test line That sucks This is an example to show that the output is random and the program repeats lines – it is not the direct output in the terminal : My code : What I tried : It does n't work and I need help . Thank you guys ."
"I am working on a Django application but this seems like it is just a python question , with nothing necessarily specific to Django . I 'm pretty new to python , and its hard to describe what I am trying to do , but easier to show so here goes : I have one class : which I subclass : and then I have another class : which I also sub-class : Note that the subclasses have the exact same code other than class names and do the exact same thing . I have been trying to figure what the best way to genericize this so I can keep it DRY and easily use it for other classes , and have considered decorators and/or multiple inheritance -- both of which are new concepts for me -- but I keep getting mixed up . Help is appreciated ! ( As a side note , feel free to point out any problems you see in my django code : ) )"
"I am currently trying to generate sphinx documentation for scripts which use the ArcGIS arcpy library . I am running into an issue when sphinx tries to run the scripts while generating the documentation , as arcpy scripts take input parameters from the arcgis gui . Since sphinx is calling the scripts without the gui , these parameters are empty and are causing Tracebacks such as : I get around this issue in unittests by setting a variable when the test begins which the script checks for and sets test values in the parameters , I am wondering if there is a similar workaround with sphinx ?"
I have following dataframe : And i need to do something like transform ( 'size ' ) with following sort and get N max values.To get something like this ( N=2 ) : Is there elegant way to do that in pandas 0.19.x ?
"I 'm about to start a program using Python which is mostly doing polling , it will constantly read from the serial port ( via PySerial ) and read from a file descriptor which will be changing from time to time . I started looking into the threading module but then I kept finding more and more suggestions of using the multiprocessing module instead.I 'm not well versed in Python , coming from a mostly C background . What are the technical advantages of a threaded approach in Python ? In C , threads share data vs. having to set up some IPC to communicate , that seems to be the same for Python ? My usecase : So I was thinking threads , since it will make sharing data got over serial easier , and they can have a shared handle to the serial port . Does this make sense , or am I thinking about this incorrectly from a Pythonic point of view ?"
"I have an array of vectors and compute the norm of their diffs vs the first one . When using python broadcasting , the calculation is significantly slower than doing it via a simple loop . Why ? I have Python 3.6.3 and Numpy 1.14.2To run the example in google colab : https : //drive.google.com/file/d/1GKzpLGSqz9eScHYFAuT8wJt4UIZ3ZTru/view ? usp=sharing"
This is scrapy 's default Dupefilter class method request_seenWhile implementing a custom dupefilter . i can not retrieve the spider object from this class unlike other scrapy middlewareIs there any way i can know which spider object this is ? so i can customize it via a spider on spider basis ? Also i can not just implement a middleware which reads urls and puts it into a list & checks duplicates instead of a custom dupefilter . This is because i need to pause/resume crawls and need scrapy to store the request fingerprint by default using the JOBDIR setting
"Is there an easy way to pull out the distinct combinations of values in a dataframe ? I 've used pd.Series.unique ( ) for single columns , but what about multiple columns ? Example data : Ideally , I 'd like a separate Series object of tuples with the distinct values ."
"How exactly does Python evaluate class attributes ? I 've stumbled across an interesting quirk ( in Python 2.5.2 ) that I 'd like explained.I have a class with some attributes that are defined in terms of other , previously defined attributes . When I try using a generator object , Python throws an error , but if I use a plain ordinary list comprehension , there 's no problem.Here 's the pared-down example . Note that the only difference is that Brie uses a generator expression , while Cheddar uses a list comprehension . ( My actual case was more complicated , and I was creating a dict , but this is the minimum example I could find . ) My only guess is that the list comprehensions are computed at that line , but the generator expressions are computed after the end of the class , at which point the scope has changed . But I 'm not sure why the generator expression does n't act as a closure and store the reference to base in the scope at the line.Is there a reason for this , and if so , how should I be thinking of the evaluation mechanics of class attributes ?"
"The following line is practically readable like a sentence . It also seems very Pythonic to do it this way , but again I 'm knew to this language and just looking for style tips ."
"I 'm trying to add a git clean-filter in order to ignore outputs and execution_count from my IPython notebook files.I 've basically followed this article ( based on this SO answer ) and modified it slightly for my needs . Also , I 'm on Windows so the part about making the python script executable is n't needed as far as I know ( see Python FAQ ) .I want to bundle that to my repository so that other contributors get it too.I 've saved the ipynb_drop_output.py at the root of my repository and saved the gitconfig and gitattributes files at the same place so at the root I have : In .gitattributes : In .gitconfig : I 've tested the code of my ipynb_drop_output manually and it works like a charm . Yet git diff still shows me execution_count and outputs that changed . It appears the script is n't running at all.I 'm thinking it might be because of the clean = ipynb_drop_output.py section , but I 've tried every variation : not include the .py , include the full path `` C ... \ipynb_drop_output.py '' , with forward slashes too etc.My second theory is that git is just not looking at the .gitconfig file but I 'm unclear how to tell it to and/or how to check that it is actually looking at it . And I thought the point of git config -- file .gitconfig filter.clean_ipynb.clean ipynb_drop_output was to do just this ... How can I make it work on Windows please ?"
"I have been reading all over the place , including Does Python have a ternary conditional operator ? . It is supposed thatis better code thanBut no one ever explains why . Will someone please elaborate ? If it 's mere readability , then it 's really just a matter of preference : some people will like the one and some people will like the other . So my question is : is there some really technical advantage for going one way versus the other ."
"I am using Vue.js for the first time . I need to serialize the objects of django views.pyI have tried to display serialized json data in html its working fine there , Now , how to intialize json data in vue instance and to access in html using v-repeat attribute.https : //jsfiddle.net/kn9181/1yy84912/Please can any one help ? ? ?"
"I am tasked with calculating hamming distances between 1D binary arrays in two groups - a group of 3000 arrays and a group of 10000 arrays , and every array is 100 items ( bits ) long . So thats 3000x10000 HD calculations on 100 bit long objects.And all that must be done in at most a dozen minutesHere 's the best of what I came up withAnd it 's still going to take 1-1.5 hours for it to finish . How do I go about making this faster ?"
"I 'm just trying to understand how to deal with the reference counts when using the Python C API.I want to call a Python function in C++ , like this : The Python code in pythonScript.py is very simple : The documentation of `` PyObject_CallFunctionObjArgs '' says that you get a new reference as return value . So I would expect `` scriptResult '' to have a reference count of 1 . However the output is : Furthermore I would expect a memory leak if I would do this in a loop without decreasing the reference count . However this seems not to happen.Could someone help me understand ? Kind regards !"
"So I have 3 netcdf4 files ( each approx 90 MB ) , which I would like to concatenate using the package xarray . Each file has one variable ( dis ) represented at a 0.5 degree resolution ( lat , lon ) for 365 days ( time ) . My aim is to concatenate the three files such that we have a timeseries of 1095 days ( 3 years ) .Each file ( for years 2007 , 2008 , 2009 ) has:1 variable : dis3 coordinates : time , lat , lon ... as suchI get them imported and use the concat module to concatenate , I think successfully . In this case the module reads out 3 netcdf filenames from filestrFNew details of the new dataset are shown to now be : Seems fine to me . However , when I write this dataset back to a netcdf , the filesize has now exploded , with 1 year of data seemingly equivalent to 700 MB . For 2 concatenated files , ~1.5 GBFor 3 , , , , 2.2 GBFor 4 , , , , 2.9 GBI would have expected 3 x 90 MB = 270 MB - since we are scaling ( 3x ) in one dimension ( time ) . The variable , dis , and other dimensions lat and lon remain constant in size.Any ideas please for the huge upscale in size ? I have tested reading in and writing back out files without concatenation , and do this successfully with no increase in size ."
"I 'm trying to use a C library which uses a callback function ( callback_function ) to provide a pointer to a struct I 'd like to wrap ( glp_tree ) .What is the correct way to initialize an instance with a pointer not created in __cinit__ ? I ca n't find an example of this pattern in the cython documentation.I have some working code ( see below ) , which casts the pointer to an integer and back , but I 'm not sure this is good practice / sane.Passing the glp_tree object directly seems to work ( although it 's not what I want to do ) , but trying to pass the pointer results in a compiler error :"
"I have a ranking function that I apply to a large number of columns of several million rows which takes minutes to run . By removing all of the logic preparing the data for application of the .rank ( method , i.e. , by doing this : I managed to get this down to seconds . However , I need to retain my logic , and am struggling to restructure my code : ultimately , the largest bottleneck is my double use of lambda x : , but clearly other aspects are slowing things down ( see below ) . I have provided a sample data frame , together with my ranking functions below , i.e . an MCVE . Broadly , I think that my questions boil down to : ( i ) How can one replace the .apply ( lambda x usage in the code with a fast , vectorized equivalent ? ( ii ) How can one loop over multi-indexed , grouped , data frames and apply a function ? in my case , to each unique combination of the date_id and category columns . ( iii ) What else can I do to speed up my ranking logic ? the main overhead seems to be in .value_counts ( ) . This overlaps with ( i ) above ; perhaps one can do most of this logic on df , perhaps via construction of temporary columns , before sending for ranking . Similarly , can one rank the sub-dataframe in one call ? ( iv ) Why use pd.qcut ( ) rather than df.rank ( ) ? the latter is cythonized and seems to have more flexible handling of ties , but I can not see a comparison between the two , and pd.qcut ( ) seems most widely used.Sample input data is as follows : The two ranking functions are : And the code to call my ranking function and recombine with df is : I am trying to get this ranking logic as fast as I can , by removing both lambda x calls ; I can remove the logic in rank_fun so that only f ( x ) 's logic is applicable , but I also do n't know how to process multi-index dataframes in a vectorized fashion . An additional question would be on differences between pd.qcut ( and df.rank ( : it seems that both have different ways of dealing with ties , but the overheads seem similar , despite the fact that .rank ( is cythonized ; perhaps this is misleading , given the main overheads are due to my usage of lambda x.I ran % lprun on f ( x ) which gave me the following results , although the main overhead is the use of .apply ( lambda x rather than a vectorized approach : Line # Hits Time Per Hit % Time Line Contents"
"I have a url that I need to add an api key to as a parameter . The key has % and other characters in it and should not be url encoded , is there a way to do this with furl ? Heres my current code :"
I have a parent package that has 2 child packages . It looks like thisAll the __init__.py files are empty.The code in backend/connections.py and backend/conf.py is being used by modules in both packages api and scheduled.in register.py i have code likeNow when i do python register.pyi get this errorAlso when i changed from backend.conf import * to from ..conf import * or from .. import conf i get this errorWhat i understand by the above error is that python is not treating the above folders as packages . But i have __init__.py in all the folders . What is wrong ?
"Is there any computational difference between these two methods of checking equality between three objects ? I have two variables : x and y . Say I do this : Is that different from : What about if they are False ? And : Is there any difference in how they are calculated ? In addition , how does x == y == z work ? Thanks in advance !"
"Consider the following example : It returns : Why eval does not take into consideration the variables defined inside the function ? From the documentation , optionally you can pass a globals and a locals dictionary . What does it means ? Finally , how can I modify this small case to make it work ?"
"I have two Dataframes one large one with a lot of missing values and a second one with data to fill the missing data in the first one.Dataframe examples : so what I want to do is fill up df using the data from df2 also taking into account that B1 is not B2 when coming across a second instance in df2.See below the desired output : The NaNs in B1 , B2 and B3 for 1 and 2 have been filled with the data from df2 . 1 0 1 for index 1 and 1 0 0 for index 2 . See below my inefficient for loop implementation : This works , however when the dataset gets larger it can take a significant amount of time . So my question is if there is a way to do this faster ? I have heard vectorization could work , how would you implement this ? Are there any other ways to do this faster ?"
"I 've been trying to assign a value for every row of a dataframe and I have n't been able to do so ( I 'm new in pandas ) , so if anyone could help , I 'd be super grateful ! I 've got two dataframes . In the input dataframe , I have brands : And then , on the output dataset , I have objects : and what I would need to have is a dataframe with all the objects combined with all the brands : I 've been trying to do it with the apply function , iterating over the rows , but I end up overwriting the values so I write the last brand : This is my code : Could someone give me a hint on how to deal with this , please ? Thanks a lot in advance !"
"I 'm using ParallelPython to develop a performance-critical script . I 'd like to share one value between the 8 processes running on the system . Please excuse the trivial example but this illustrates my question.The pp docs do n't seem to describe a way to share data across processes . Is it possible ? If so , is there a standard locking mechanism ( like in the threading module ) to confirm that only one update is done at a time ? I understand I could keep a local min and compare the 4 in the main thread once returned , but by sharing the value I can do some better pruning of my BFS binary tree and potentially save a lot of loop iterations . Thanks-Jonathan"
Most plotting methods like plot ( ) and errorbar automatically change to the next colour in the color_palette when you plot multiple things on the same graph . For some reason this is not the case for fill_between ( ) . I know that I could hard-code this but it is done in a loop which makes it annoying . Is there a good way to get around this ? Maybe just to get the current position in the palette and iterate to the next will be enough .
"I have a file format ( fastq format ) that encodes a string of integers as a string where each integer is represented by an ascii code with an offset . Unfortunately , there are two encodings in common use , one with an offset of 33 and the other with an offset of 64 . I typically have several 100 million strings of length 80-150 to convert from one offset to the other . The simplest code that I could come up with for doing this type of thing is : This works just fine , but it is not particularly fast . For 1 million strings , it takes about 4 seconds on my machine . If I change to using a couple of dicts to do the translation , I can get this down to about 2 seconds . If I blindly run under cython , I get it down to just under 1 second.It seems like at the C-level , this is simply a cast to int , subtract , and then cast to char . I have n't written this up , but I 'm guessing it is quite a bit faster . Any hints including how to better code a this in python or even a cython version to do this would be quite helpful.Thanks , Sean"
"As a Python newbie coming from the C++ background , the slicing operator in Python ( 3.4.x ) looks ridiculous to me . I just do n't get the design philosophy behind the `` special rule '' . Let me explain why I say it 's `` special '' .On the one hand , according to the Stack Overflow answer here , the slicing operator creates a ( deep ) copy of a list or part of the list , i.e . a new list . The link may be old ( earlier than python 3.4.x ) , but I just confirmed the behavior with the following simple experiment with python 3.4.2 : On the other hand , according to the official documentation here : Clearly , the slicing operator [ : ] does not do a deep copy here.From the observation it seems to suggest that the slicing operator produces different behavior when it 's on left/right side with respect to the assignment operator . I do not know any language in which an operator could produce similar behavior . After all , an operator is a function , just a syntactically special function , and a function 's behavior should be self-contained , purely determined by all of its inputs.So what can justify this `` special rule '' in Python design philosophy ? P.S . If my conclusion is not correct , there are really only two possibilities:1 , Python 's slicing 'operator ' is actually not an operator , so my assumption does not hold -- - then what is it ( the 'slicing operator ' [ : ] ) ? 2 , The difference in behavior is caused by some latent factor not observed . The slicing operator 's location ( left/right hand side ) with respect to the assignment operator accidentally co-exists with the observation of different behavior . They do not have causality relationship -- - then what is the latent factor that causes the difference in behavior ?"
"I 'm looking for a pythonic way to move up n directories from a given directory.Let 's say we have the example path /data/python_env/lib/python3.6/site-packages/matplotlib/mpl-data . If we were to move up n=2 directories we should end up at /data/python_env/lib/python3.6/site-packages.The following works to move up n directories : However , it 's not very readable and fails for paths on windows machines . In essence , it does n't feel a very pythonic solution.Is there a better , more pythonic solution , maybe using the os module ?"
"The goal is to maintain the relationship between two columns by setting to NaN all the values from one column in another column.Having the following data frame : Maintaining the relationship from column a to column b , where all NaN values are updated results in : One way that it is possible to achieve the desired behaviour is : Is there any other way to maintain such a relationship ?"
"I have a model with dynamic choices , and I would like to return an empty choice list if I can guarantee that the code is being run in the event of a django-admin.py migrate / makemigrations command to prevent it either creating or warning about useless choice changes.Code : So I would think if I can detect the run context in lazy_discover_foreign_id_choices then I can choose to output an empty choice list . I was thinking about testing sys.argv and __main__.__name__ but I 'm hoping there 's possibly a more reliable way or an API ?"
"In Python , given a list , I can sort it by a key function , e.g . : As you see , the list was sorted not alphanumerically but by the return value of get_value ( ) .Is there an equivalent in C++ ? std : :sort ( ) only allows me to provide a custom comparator ( equivalent of Python 's items.sort ( cmp= ... ) ) , not a key function . If not , is there any well-tested , efficient , publicly available implementation of the equivalent I can drop into my code ? Note that the Python version only calls the key function once per element , not twice per comparison ."
"I was wondering if anyone could shed light on this . We have multiple package libraries with the same root package e.g . a. I also have package a.b located in X and package a.c located in Y . Both X and Y are in my PYTHONPATH and When I do : I get an error : `` No module named b '' . After reading around it seems to me that once a.c is loaded python writes info about a as well , and when I come to do a.b because it has information about a already it never bothers to look in location X for a.b and throws an error that no module named b can be found.Moreover , I found that order with which X and Y are specified in the PYTHONPATH seem to affect the importing . For example , when I do But if I doIs that correct and if so , how can I work around this ? It 's convenient to have a common module root name and different sub packages reside in different projects etc . Of course , I am coming from Java point of view where you could do this kind of overlap ."
"I have a list of list of lists like thisAnd I want to construct a phylogenetic tree from them . I wrote a node class like so ( based partially on this code ) : And then I try to construct my tree like this : This works for the first match , but when I start with the second match it is appended at the end of the tree instead of starting again at the top . How would I do that ? I tried some variations on searching for the ID , but I ca n't get it to work ."
"Something peculiar I 've noticed is that any changes committed to the DB outside of the session ( such as ones made in MySQL 's Workbench ) are not recognised in the sqlAlchemy session . I have to close and open a new session for sqlAlchemy to recognise it.For example , a row I deleted manually is still fetched from sqlAlchemy.This is how I initialise the session : How can I get sqlAlchemy to recognise them ? My sqlAlchemy version is 0.9.4 and my MySQL version is 5.5.34 . We use only sqlAlchemy 's Core ( no ORM ) ."
"The following code uses the { } operator to combine two defaultdicts.But , as we see if we run this , the { } operator returns a dict type not a defaultdict type . Is there a way to cast a dict back to a defaultdict ?"
What explains the following behavior :
"I have a url like /posts/1 , where 1 refers to the id of the article in the db.However , what I would like to do is have a url with some sort of slugified title in it , like /posts/1/my_stack_overflow_question_is_bad . I can make a slugify property in the model : but how would I put that in the url ?"
"In the networkx python package , is there a way to find all node cuts of minimal size consisting of only nodes from one set in a bipartite graph ? For example , if the two sides of a bipartite graph are A and B , how might I go about finding all minimal node cuts consisting of nodes entirely from set B ? The following code I have works but it 's extremely slow : Note that this actually only checks if there is a minimal cut which , if removed , would disconnect two nodes in A only . If your solution does this ( instead of finding a cut that will separate any two nodes ) that 's fine too . Any ideas on how to do this more efficiently ?"
"My end goal is to query NVAPI for gpu usage and other statistics in python . See http : //developer.nvidia.com/nvapiHere is a copy of the header file available for download from the link above : http : //paste.pound-python.org/show/7337/At this point , I am just trying to familiarize myself with the api ... so what am I doing wrong ? I ca n't figure out how to call any of the functions listed in the header file ."
"I am new to python . This seems like a basic question to ask . But I really want to understand what is happening here Indexing first element from each dataframe Doubt1 : -Why this is happenening ? Why myseries_three [ 0 ] gives me a keyError ? what we meant by calling myseries_one [ 0 ] , myseries_one [ 0 ] or myseries_three [ 0 ] ? Does calling this way mean we are calling by rownames ? Doubt2 : -Is rownames and rownumber in Python works as different as rownames and rownumber in R ? Doubt3 : - If calling myseries_three [ 0 ] meant calling by rownames then how myseries_three [ 0:3 ] producing the output ? does myseries_three [ 0:4 ] mean we are calling by rownumber ? Please explain and guide . I am migrating from R to python . so its a bit confusing for me ."
"I would like to create a range ( e.g . ( 1 , 5 ) ) of numbers with some repetitions ( e.g . 4 ) : One way would be to write : Or similarly : However , there is a flatting step , which could be avoided.Is there a more pythonic or more compact version to generate such a sequence ?"
"I 'm trying to run three functions ( each can take up to 1 second to execute ) every second . I 'd then like to store the output from each function , and write them to separate files.At the moment I 'm using Timers for my delay handling . ( I could subclass Thread , but that 's getting a bit complicated for this simple script ) What 's the best way to handle the output from function_with_delay ? Append the result to a global list for each function ? Then I could put something like this at the end of my main function : Thoughts ? Edit : Added my own answer as a possibility"
"I have a server application and when requested by the client I schedule some work , likeI await fut later when it is requested explicitly . My use case requires that run_in_executor submit the work function immediately , and that behaves as expected in my environment ( Ubuntu 16.04 , Python 3.7.1 ) .Since my application depends on this behavior I wanted to verify that it is not something likely to change , so I checked several resources : The documentation seems kind of vague . awaitable seems like it may apply to the method or the return value - though the body of the text does say it returns an asyncio.Future explicitly.PEP 3156 that specifies asyncio - here it says nothing close to run_in_executor being a coroutine.In a few issues whether run_in_executor is a function that returns an awaitable or a coroutine itself seems to be considered an implementation detail . See 25675 and 32327.AbstractEventLoop.run_in_executor is specified as a coroutine , but the implementation in BaseEventLoop.run_in_executor is a plain function.1 and 2 mostly seem to indicate that the current behavior is correct , but 3 and 4 are concerning . This seems like a very important part of the interface because if the function itself is a coroutine then it will not begin executing ( therefore will not schedule the work ) until it is awaited.Is it safe to rely on the current behavior ? If so , is it reasonable to change the interface of AbstractEventLoop.run_in_executor to a plain function instead of a coroutine ?"
"I have a log-file where every line contains IP address , time of access , and the URL accessed . I want to count the accesses per hour.Time of access data looks like thisHow can I improve it so I do n't need to set up the variable and if statement for every hour ?"
"I 'm wondering if something like this is possible in python ( 3.2 , if that 's relevant ) .Where the behavior is : if the regex matches , the regex groups get assigned to a and bif there 's a mismatch there , it 'd throw an exceptionif the match is None , it would just bypass the context entirelyMy goal here is basically an extremely concise way of doing the contextual behavior.I tried making the following context manager : It actually works exactly as desired for the case when the regexes match , but , as you can see , it throws the ValueError if there 's no match . Is there any way I can get it to `` jump '' to the exit sequence ? Thanks ! !"
"a evaluates to [ ] .a evaluates to [ 1 , 2 ] .The first result is unexpected to me . What semantics are going on here ?"
"In the documents , it is said that we can set the parameter metric_freq to set the frequency . I have also tried the parameter verbose , the parameters are set asHowever , I still got the result as following , Which showed that the parameter metric_freq did n't work at all . So which parameter is used to set the frequency of output ? In R , we can use eval_freq , but there is no eval_freq parameter in python wrapper !"
"I have a recursive tuple of strings that looks like this : ( it 's actually a tuple of tuples of strings - it 's constructed recursively ) And I 'd like to flatten it into a list of strings , whereby foo [ 1 ] would contain `` text '' , foo [ 2 ] `` othertext '' and so forth.How do I do this in Python ? The duplicate is about a 2D list of lists , but here I 'm dealing with a recursive tuple ."
"I am implementing a program to detect lines in images from a camera . The problem is that when the photo is blurry , my line detection algorithm misses a few lines . Is there a way to increase the accuracy of the cv.HoughLines ( ) function without editing the parameters ? Example input image : Desired image : My current implementation :"
"Given a list of points , how can I get their indices in a KDTree ? I could do something like : Does it make sense to do it that way ?"
"I wrote a spider using scrapy , one that makes a whole bunch of HtmlXPathSelector Requests to separate sites . It creates a row of data in a .csv file after each request is ( asynchronously ) satisfied . It 's impossible to see which request is satisfied last , because the request is repeated if no data was extracted yet ( occasionally it misses the data a few times ) . Even though I start with a neat list , the output is jumbled because the rows are written immediately after data is extracted.Now I 'd like to sort that list based on one column , but after every request is done . Can the 'spider_closed ' signal be used to trigger a real function ? As below , I tried connecting the signal with dispatcher , but this function seems to only print out things , rather than work with variables or even call other functions ."
"Having an asynchronous generator I would expect to be able to iterate through it asynchronously . However , I am missing something or messing something up or both as I end up with a regular synchronous for loop in the end : This will take about 12 seconds to run and return this : Though I expected it to take about 4 seconds to run and return something like this :"
"Does numpy allocate new matrices for every operation you perform on a matrix ? For example : And slice operations : How about chained operations ? Numpy 's a fantastic library , but I just want to know what happens under the hood . My intuition says that slice operations modify the memory view in place , but I do n't know about assignments ."
"I followed the advice of the django docs , and use logging like this : With my current configuration , the output looks like this : Unfortunately some libraries which I ca n't modify use logging like this : The output unfortunately looks like this : I want to see this : Difference : root.third_party == > other_lib.some_file.third_partyI want to see the long version ( not root ) if code uses logging.info ( ) instead of logger.info ( ) UpdateThis is not a duplicate of Elegant setup of Python logging in Django , since the solution of it is : Start of quoteIn each module , I define a logger usingEnd of quote.No , I wo n't modify third-party-code which uses logging.info ( ) instead of logger.info ( ) .Follow Up QuestionAvoid logger=logging.getLogger ( __name__ ) without loosing way to filter logs"
"Let 's say I have a generator function like this : Example output might be : ( ' a ' , 1 ) , ( ' a ' , 2 ) , ( ' a ' , 3 ) , ( ' a ' , 4 ) , ( ' a ' , 5 ) , ( ' a ' , 6 ) , ( ' a ' , 7 ) , ( ' a ' , 8 ) , ( ' b ' , 9 ) , ( ' c ' , 10 ) , ( ' c ' , 11 ) , ( ' c ' , 12 ) , ( ' c ' , 13 ) I would like to break this into three groups : Group A , Group B , and Group C. And I would like a generator for each group . Then I 'd pass the generator and the group letter into a subfunction . An example of the subfunction : The desired output would be : How can I do this without changing big_gen ( ) or printer ( ) , and avoid storing the entire group in memory at once ? ( In real life , the groups are huge )"
"I am using both python and java to run the Stanford NER tagger but I am seeing the difference in the results.For example , when I input the sentence `` Involved in all aspects of data modeling using ERwin as the primary software for this . `` , JAVA Result : Python Result : Python nltk wrapper ca n't catch `` ERwin '' as PERSON.What 's interesting here is both Python and Java uses the same trained data ( english.all.3class.caseless.distsim.crf.ser.gz ) released in 2015-04-20.My ultimate goal is to make python work in the same way Java does.I 'm looking at StanfordNERTagger in nltk.tag to see if there 's anything I can modify . Below is the wrapper code : Or , if it 's because of using different Classifier ( In java code , it seems to use AbstractSequenceClassifier , on the other hand , python nltk wrapper uses the CRFClassifier . ) is there a way that I can use AbstractSequenceClassifier in python wrapper ?"
"I 'm currently transitioning from Java to Python and have taken on the task of trying to create a calculator that can carry out symbolic operations on infix-notated mathematical expressions ( without using custom modules like Sympy ) . Currently , it 's built to accept strings that are space delimited and can only carry out the ( , ) , + , - , * , and / operators . Unfortunately , I ca n't figure out the basic algorithm for simplifying symbolic expressions.For example , given the string ' 2 * ( ( 9 / 6 ) + 6 * x ) ' , my program should carry out the following steps:2 * ( 1.5 + 6 * x ) 3 + 12 * xBut I ca n't get the program to ignore the x when distributing the 2 . In addition , how can I handle ' x * 6 / x ' so it returns ' 6 ' after simplification ? EDIT : To clarify , by `` symbolic '' I meant that it will leave letters like `` A '' and `` f '' in the output while carrying out the remaining calculations.EDIT 2 : I ( mostly ) finished the code . I 'm posting it here if anyone stumbles on this post in the future , or if any of you were curious ."
"Ok , I have a big dataframe such as : Let 's do n't get lost here . The column hour represents the hours of the day , from 6 to 6 hours . Column values is well , exactly that , here the values are as an example , not the actual ones.If you look closely to the hour column , you can see that there are hours missing . For instance , there is a gap between rows 7 and 8 ( the value of hour 0 is missing ) . There are also bigger gaps , such as in between rows 10 and 11 ( hours 00 and 06 ) .What do I need ? I would like to check when an hour ( and of course ) a value is missing , and complete the dataframe inserting a row there with the corresponding hour and a np.nan as value.What have I thought ? I think this would be easily solved using modular arithmetic , in this case with mod 24 , such as when 18 + 6 = 24 = 0 mod 24 . So initializing the counter to zero and adding 6 with the caveat that the counter is defined in modular arithmetic mod 24 you can verify if each hour is the corresponding hour , and if not , insert a new row with the corresponding hour and with np.nan as value.I do n't know how to do the implementation of modular arithmetic in python to iterate a dataframe column.Thank you very much ."
"I 'm trying to start my kivy app 's service on bootup.I 'm sure that my service is ok because it works when I start my app . But on bootup I have a problem.I 've read this article and tried to make it : It works but starts the app but not the service . So I 've studied some questions on StackOverflow and changed my code for this : ... and got an error : Can you please explain me what 's wrong and what should I do to start the service ? Thanks ! UPDATEDBy request of @ Juggernaut I add my service code : It works when I run app because app calls the service : UPDATED ( AndroidManifest ) Here is some strings from my AndroidManifest.xml.I have the RECEIVE_BOOT_COMPLETED permission : < uses-permission android : name= '' android.permission.RECEIVE_BOOT_COMPLETED '' / > I have the receiver : < receiver android : name= '' .MyBroadcastReceiver '' android : enabled= '' true '' > < intent-filter > < action android : name= '' android.intent.action.BOOT_COMPLETED '' / > < /intent-filter > < /receiver > I have the service registered : < service android : name= '' net.saband.myapp.ServiceMyservice '' android : process= '' : service_myservice '' / > By advice of @ mariachi I 've tried to change android : enabled= '' true '' to android : enabled= '' false '' in the receiver and add android : exported= '' false '' to the service . In this case when the device starts happens nothing : no errors , no service ."
"I have a string in the format : I want to get this : I did this : It works for the most part , but it fails when the text part has the ' @ ' . Eg , when : it fails . The @ names are there in the beginning and there can be text after @ names , which may possibly contain @ .Clearly I can append initally with a space and find out first word without ' @ ' . But that does n't seem an elegant solution.What is a pythonic way of solving this ?"
"Let 's say I have a I have to do a My question is , does the 'if word in dict ' call the dict.keys ( ) function every time and hence a lot slower than if I added another variable at the top that is dict_keys = dict.keys ( ) ? The result of what I am talking about would be this.Thanks"
"I 'm working on a script that takes a few minutes to run , and would like to provide some output to the user about its progress . Unfortunately i 'm exceedingly lazy . what I 'd like to do is write a function without the logging , and then apply a decorator to it which steps through the function and prints each line before executing that line . Basically what I 'm looking for is a loggingdecorator such that : Here 's what I 've tried so far : Unfortunately , this prints a bit too much ; it follows function calls and prints them out , line by line as well ( well , this does n't actually print the source line , existing answers using inspect , combined with stuff on the frame object in the trace function would do it ) , but I 'm a bit stumped as to how the logging_tracer unless the function in question is actually decorated ."
"I know that I can get min or max values with : out of a numpy matrix/vector . The indices for those vales are returned by : So e.g . when I have a 5x5 matrix : I could get the max value via : ... but what is the most efficient way to get the max or min n-elements ? So let 's say out of a I want to have the 5 highest and 5 lowest elements . This should return me [ 30 , 31 , 32 , 33 , 34 ] for the 5 highest values respectively [ 20 , 21 , 22 , 23 , 24 ] for their indices . Likewise [ 10 , 11 , 12 , 13 , 14 ] for the 5 lowest values and [ 0 , 1 , 2 , 3 , 4 ] for the indices of the 5 lowest elements.What would be an efficient , reasonable solution for this ? My first idea was flattening and sorting the array and taking the last and first 5 values . Afterwards I search through the original 2D matrix for the indices of those values . Although this procedure works flattening + sorting is n't very efficient ... does anyone know a faster solution ? Additionally I would like to have the indices of the original 2D array and not the flattening one . So instead of 24 returned by np.argmax ( a ) I would like to have ( 4 , 4 ) ."
"While experimenting with different value types for Enum members , I discovered some odd behavior when the values are mutable.If I define the values of an Enum as different lists , the members still behave similarly to when the Enum values are typical immutable types like str or int , even though I can change the values of the members in place so that the values of the two Enum members are the same : However , if I define the values to be identical lists , each member 's value seems to be the same object , and thus any mutation of one member 's value affects all members : Why does Enum behave this way ? Is it the intended behavior or is it a bug ? NOTE : I 'm not planning on actually using Enums this way , I was simply experimenting with using non-standard values for Enum members"
"I have the following code : file1.pyfile2.pyand the output from python file1.py is the following : First of all why the code gets executed twice in this case ? I can understand that the import is recursive but why the code of the script get executed again ? Secondly , the second time it is not getting caught by the same except block even though it is the same type as the first time , which also I can not find an explanation for.Finally , I am trying to find a workaround for this problem without moving anything to a new file but I it does not seem to be any . Is it possible to get over this problem ? EditFor the second question I realized it is because the code is inside the module level ."
"Assume that I have a function which converts Python data-types to Postgres data-types like this : I could type-hint this as : But then code like the following would fail to type-check even though it is correct : Complete example ( working code , but errors in type-hinting ) : I 'm aware that I could split up the mapping into two functions , but the aim is to have a generic type-mapping function.I was also thinking about working with classes and polymorphism , but then how would I type-hint the topmost class methods ? Because their output type would depend on the concrete instance type ."
"Catching Python 's OverflowError after some dumb calculation , I checked the error 's args and saw it 's a tuple containing an integer as its first coordinate . I assume this is some kind of error number ( errno ) . However , I could not find any documentation or reference for it.Example : Do you know what 34 means in this context ? Do you know other possible error numbers for this exception ?"
"ContextI am reading a file from Google Storage in Beam using a process that looks something like this : Where LoadFileDoFn loads the file and creates a Python list of objects from it , which ParDo then returns as a PCollection.I know I could probably implement a custom source to achieve something similar , but this answer and Beam 's own documentation indicate that this approach with pseudo-dataset to read via ParDo is not uncommon and custom sources may be overkill.It also works - I get a PCollection with the correct number of elements , which I can process as I like ! However..Autoscaling problemsThe resulting PCollection does not autoscale at all on Cloud Dataflow . I first have to transform it via : I know this answer I also linked above explains pretty much this process - but it does n't give any insight as to why this is necessary . As far as I can see at Beam 's very high level of abstraction , I have a PCollection with N elements before the shuffle and a similar PCollection after the shuffle . Why does one scale , but the other not ? The documentation is not very helpful in this case ( or in general , but that 's another matter ) . What hidden attribute does the first PCollection have that prevents it from being distributed to multiple workers that the other does n't have ?"
"I am trying to create an optimal shift schedule where employees are assigned to shift times . The output should aim to spend the least amount of money . The tricky part is I need to account for specific constraints . These being : The staff_availability df contains the employees to choose from [ 'Person ' ] , the available min - max hours they can work [ 'MinHours ' ] - [ 'MaxHours ' ] , how much they get paid [ 'HourlyWage ' ] , and availability , expressed as hours [ 'Availability_Hr ' ] and 15min segments [ 'Availability_15min_Seg ' ] .Note : Available employees do n't have to be assigned shifts if not required . They 're just available to do so.The staffing_requirements df contains the time of day [ 'Time ' ] and the staff required [ 'People ' ] during those periods.The script returns a df 'availability_per_member ' that displays how many employees are available at each point in time . So 1 indicates available to be scheduled and 0 indicates not available . It then aims to allocate shift times , while accounting for the constraints using pulp.I am getting an output but the shift times are n't applied to employees consecutively.I am not meeting the 4th constraint in that employees can only work one shift a dayBelow is an output for the first two hours ( 8 15 min time slots ) . The issue is the shifts are n't consecutive . The employees scheduled for the first 8 time slots are mainly different . I 'd have 5 people starting within the first 2 hours . Employees should only work one shift per day ."
"I have some Python code that I want to debug with perf . For that purpose I want to use subprocess . The following command returns instruction-related information of a process until the command is exited via Ctrl^C.Now , I want to run this inside a Python code in background , until some point where I want to be able to terminate its operation and print the commands output . To show what I mean : Now , I want to determine what to do at the line of 'print x ' to terminate the process and check the output.Any idea/help is appreciated.Cheers and thanks in advance ,"
"I recently upgraded my GAE Python app to Python 2.7 . Since then , I periodically get the following error with the dev server and the dev server serves up a blank page : Some notes : This does n't happen on the production server.On the dev server , my app will work for a few minutes and then this error happens.If I stop and restart my app on the dev server , it will work again for a few minutes.I am using the latest version of gae-pytz and you can see that it fails in an import there.The [ ... ] that I removed are similar to the stuff you see near the end.I do n't know why setuptools is being invoked at the end.I 'm using a Mac with Lion.I can use the dev server , but it is really annoying to stop and restart every few minutes . Any ideas how to fix this ?"
"I wonder if it is possible to store numpy slice notation in a python dictionary . Something like : It is possible to use native python slice syntax , e.g . slice ( 0,10,2 ) , but I have not been able to store more complex slices . For example , something that is multidimensional [ : ,:2 , : , :540 ] .My current work around is to store the values as tuples and then unpack these into the necessary slices.Working in Python 2.x ."
"I am using the python lxml library to transform XML files to a new schema but I 've encountered problems parsing processing instructions from the XML body.The processing instruction elements are scattered throughout the XML , as in the following example ( they all begin with `` oasys '' and end with a unique code ) : I ca n't locate them through the lxml.etree.findall ( ) method , although etree.getchildren ( ) returns them : Is there an alternative to using getchildren ( ) to parse and remove processing instructions , especially considering that they 're nested at various levels throughout the XML ?"
"I am using Aerospike Python CLient to put and get some key-value pair : However after the put , my program terminates with SEGFAULT during Get command . Following is the stacktrace I have got from GDB : It looks like aerospike_key_get calls some subroutine in file src/main/aerospike/as_shm_cluster.c , which is absent , hence segmentation fault . I checked my $ LD_LIBRARY_PATH and it does contain directory containing aerospike.so.Why this particular file is missing ? Is it somewhere else other than aerospike.so ? Why segmentation fault . It should be gracefully handled in Python clientEdit :"
"I am using Scipy sparse matrix csr_matrix to be used as context vectors in word-context vectors . My csr_matrix is a ( 1 , 300 ) shape so it is a 1-dimensional vector.I need to use permutation ( circular right shift or circular left shift ) on the sparse vector ( for showing left context and right context ) .example : i have [ 1 , 2 , 3 , 4 ] and i want to create right and left permutations as follow : right permutation : [ 4 , 1 , 2 , 3 ] left permutation : [ 2 , 3 , 4 , 1 ] In csr matrices i ca n't access to column indices so i can not just change the column indices.Is there any efficient high performance solution for row permutations in csr_matrix or am i missing something ? runnable code : it means that i have a 300-column vector whose columns 100 , 47 , 150 all from row 0 are non-zero valued and their value is in data list respectively.now what i want is a permutation which means i want the columns array be changed into [ 101 , 48 , 151 ] for right permutation and [ 99 , 46 , 149 ] for left permutation.It should be noted that permutations are circular which means if column 299 has non-zero data , using a right permutation the data will be moved to column 0 ."
"For People With A Similar Question ( written after finding a solution ) : This problem , as you might notice according to the answers below , has a lot of different solutions . I only chose Evan 's because it was the easiest one for me implement into my own code . However , from what I tried , every other answer also worked . @ SalvadorDali linked this Kaggle page which was definitely interesting and I reccomend reading if you are interested . Prolog was also brought up as a possible solution , I 'm unfamiliar with it , but if you already know it -- it 's probably worth considering . Also , if you just want to get code to use there are working Javascript and Python examples below . However , each one had a different approach to the solution and I 'm not sure which is most effecient ( feel free to test it yourself ) .For further approaches/reading : http : //en.wikipedia.org/wiki/Breadth-first_searchProlog and ancestor relationshiphttps : //www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectorsSorry for the confusing title , I ca n't figure out a way to properly word my question -- any better ideas are welcome.Because I 'm having such a difficult time describing my question , I 'll try to explain my goal and code as much as needed : Note : my code here is Go , but I 'd be happy with answers in other languages as well , if you have any questions I 'll try to answer as quick as possibleBasically , I have an array of `` Word '' objects that look like this : This is an example of 4 words within the array : My challenge is writing a method to test for a relationship between 2 words . Of course , testing between 2 words like `` cat '' and `` kitten '' would be easy with the example above . I could just check `` Cat '' s list of synonyms and test to see if it contains `` kitten . '' With code like this : However , I ca n't figure out how to test for a more distant relationship.For example : I tried to do it recursively , but all my attempts do n't seem to work . Any example code ( My code is in Go , but Python , Java , or Javascript are also fine ) , pseudocode or just explanations would be really great ."
I need to multiply about 1e6 numbers that are of the order of 0.01 . The expected result is of order 1e-100000000 . Obviously the typical floating-point arithmetic can not handle this . Doing some research on the web I found the decimal library which seems to fix this problem . However it appears to have limitations that make it useless for my needs : Does anyone know any solution to this ?
"I have an autoencoder set up in Keras . I want to be able to weight the features of the input vector according to a predetermined 'precision ' vector . This continuous valued vector has the same length as the input , and each element lies in the range [ 0 , 1 ] , corresponding to the confidence in the corresponding input element , where 1 is completely confident and 0 is no confidence.I have a precision vector for every example.I have defined a loss that takes into account this precision vector . Here , reconstructions of low-confidence features are down-weighted.My issue is that the precision tensor y_prec depends on the batch . I want to be able to update y_prec according to the current batch so that each precision vector is correctly associated with its observation.I have the done the following : Here P is a numpy array containing all precision vectors with the indices corresponding to the examples . I initialize y_prec to have the correct shape for a batch size of 32 . I then define the following DataGenerator : Here I am aiming to update y_prec in the same function that generates the batch . This seems to be updating y_prec as expected . I then define my model architecture : And finally , I compile and run : Where digits.data is a numpy array of observations.However , this ends up defining separate graphs : I 've scoured SO for a solution to my problem but nothing I 've found works . Any help on how to do this properly is appreciated ."
"I 'm using gc module to debug a leak.It 's a gui program and I 've hooked this function to a button.I 've set set debug more to gc.SAVE_ALLand this is the output [ ( < type '_ctypes.Array ' > , ) , { '__module__ ' : 'ctypes._endian ' , '__dict__ ' : < attribute '__dict__ ' of 'c_int_Array_3 ' objects > , '__weakref__ ' : < attribute '__weakref__ ' of 'c_int_Array_3 ' objects > , '_length_ ' : 3 , '_type_ ' : < class 'ctypes.c_int ' > , '__doc__ ' : None } , < class 'ctypes._endian.c_int_Array_3 ' > , < attribute '__dict__ ' of 'c_int_Array_3 ' objects > , < attribute '__weakref__ ' of 'c_int_Array_3 ' objects > , ( < class 'ctypes._endian.c_int_Array_3 ' > , < type '_ctypes.Array ' > , < type '_ctypes._CData ' > , < type 'object ' > ) , ( < type '_ctypes.CFuncPtr ' > , ) , { '__module__ ' : 'ctypes ' , '__dict__ ' : < attribute '__dict__ ' of '_FuncPtr ' objects > , '__weakref__ ' : < attribute '__weakref__ ' of '_FuncPtr ' objects > , '_flags_ ' : 1 , '__doc__ ' : None , '_restype_ ' : < class 'ctypes.c_int ' > } , < class 'ctypes._FuncPtr ' > , < attribute '__dict__ ' of '_FuncPtr ' objects > , < attribute '__weakref__ ' of '_FuncPtr ' objects > , ( < class 'ctypes._FuncPtr ' > , < type '_ctypes.CFuncPtr ' > , < type '_ctypes._CData ' > , < type 'object ' > ) , { } , < cell at 0x10a24b0 : Resource object at 0x10e6a50 > , < cell at 0x10a2478 : dict object at 0x11a4440 > , < cell at 0x7f1703949f68 : function object at 0x10ec7d0 > , < cell at 0x10e2f30 : NoneType object at 0x826880 > , < cell at 0x10e2d70 : NoneType object at 0x826880 > , < cell at 0x10e2ef8 : str object at 0x7f1703dd5e10 > , < cell at 0x10e2de0 : dict object at 0x118aaa0 > , { 'Accept ' : 'application/json ' , 'User-Agent ' : 'couchdb-python 0.6 ' } , ( < cell at 0x10e2f30 : NoneType object at 0x826880 > , < cell at 0x10a24b0 : Resource object at 0x10e6a50 > , < cell at 0x10e2de0 : dict object at 0x118aaa0 > , < cell at 0x10a2478 : dict object at 0x11a4440 > , < cell at 0x10e2d70 : NoneType object at 0x826880 > , < cell at 0x10e2ef8 : str object at 0x7f1703dd5e10 > , < cell at 0x7f1703949f68 : function object at 0x10ec7d0 > ) , < function _make_request at 0x10ec7d0 > , ( 1 , ) , { } , < cell at 0x10e2bb0 : Resource object at 0x10e6a50 > , < cell at 0x10e2e88 : dict object at 0x119f360 > , < cell at 0x10f0130 : function object at 0x10ec578 > , < cell at 0x10f01d8 : NoneType object at 0x826880 > , < cell at 0x10f01a0 : NoneType object at 0x826880 > , < cell at 0x10f00f8 : str object at 0x7f170b05d810 > , < cell at 0x10f00c0 : dict object at 0x11969a0 > , { 'Accept ' : 'application/json ' , 'User-Agent ' : 'couchdb-python 0.6 ' } , ( < cell at 0x10f01d8 : NoneType object at 0x826880 > , < cell at 0x10e2bb0 : Resource object at 0x10e6a50 > , < cell at 0x10f00c0 : dict object at 0x11969a0 > , < cell at 0x10e2e88 : dict object at 0x119f360 > , < cell at 0x10f01a0 : NoneType object at 0x826880 > , < cell at 0x10f00f8 : str object at 0x7f170b05d810 > , < cell at 0x10f0130 : function object at 0x10ec578 > ) , < function _make_request at 0x10ec578 > , ( 1 , ) , { } , < cell at 0x10f0440 : Resource object at 0x10e6a50 > , < cell at 0x10f02b8 : dict object at 0x11b2d70 > , < cell at 0x10f0360 : function object at 0x10ec6e0 > , < cell at 0x10f0280 : NoneType object at 0x826880 > , < cell at 0x10f02f0 : str object at 0x10ca228 > , < cell at 0x10f0408 : str object at 0x7f170b05d810 > , < cell at 0x10f0050 : dict object at 0x11b6370 > , { 'Accept ' : 'application/json ' , 'User-Agent ' : 'couchdb-python 0.6 ' } , ( < cell at 0x10f0280 : NoneType object at 0x826880 > ] The gc.garbage list has a lot of items . Does this mean the objects in gc.garbage are leaking or have been collected or will be collected ?"
"I would like to do something similar to np.clip on PyTorch tensors on a 2D array . More specifically , I would like to clip each column in a specific range of value ( column-dependent ) . For example , in numpy , you could do : I found torch.clamp , but unfortunately it does not support multidimensional bounds ( only one scalar value for the entire tensor ) . Is there a `` neat '' way to extend that function to my case ? Thanks !"
"I am trying to write tests for aiohttp application . I am using pytest-aiohttp plugin . My intention is to initialize and run the application once before first test execution and tear down after all tests finished . pytest-aiohttp fixtures like 'loop ' , 'test_client ' are very helpful but they have scope='function ' which means I can not use them from my own fixture with scope='session ' . Is there a way to workaround this ? And if not then what would be a proper approach for achieving my goal without using built-in fixtures ? My code is as follows ( conftest.py ) My tests then use thisSo my fixture 'client ' runs for every test , which is what i want to avoid"
"Let 's say I have two path names : head and tail . They can overlap with any number of segments . If they do n't I 'd like to just join them normally . If they overlap , I 'd like to detect the common part and combine them accordingly . To be more specific : If there are repetitions in names I 'd like to find as long overlapping part as possible . ExampleIs there any ready-to-use library function for such case , or I have to implement one ?"
"I uploaded a package to pypi.org but my RST README is n't formatted correctly.This is what it looks like on GitHub : https : //github.com/NinjaSnail1080/akinator.pyThis is what it looks like on PyPi : https : //pypi.org/project/akinator.py/For some reason , on GitHub everything is formatted correctly , but on PyPi , it is n't formatted at all . Instead of showing the README formatted in RST , it 's just a complete mess of unformatted text.I tried running python3 -m twine check dist/* on my project and got this : For some reason , it says the long_description_content_type is missing , which is untrue . My setup.py file specifically says long_description_content_type= '' text/x-rst '' . It also mentions unexpected indentations in line 26 of the long description , but there are no indentations at all in that line.I have absolutely no idea what I did wrong here . Somehow , it looks perfectly fine on GitHub , but on PyPi , it 's just a mess . And the warnings that twine check gave me do n't make any sense ."
"A tutorial I have on Regex in python explains how to use the re module in python , I wanted to grab the URL out of an A tag so knowing Regex I wrote the correct expression and tested it in my regex testing app of choice and ensured it worked . When placed into python it failed.After much head scratching I found out the issue , it automatically expects your pattern to be at the start of the string . I have found a fix but I would like to know how to change : intoOkay , it 's a standard URL regex but I wanted to avoid any potential confusion about what I wanted to get rid of and possibly pretend to be funny ."
"I 'm trying to use the result of a class method several times without doing the heavy calculations required to obtain the result.I am seeing the following options . Which ones do you think is the right one , or more pythonic ? What are the advantages and disadvantages of each one ? Try/Except approachlru_cache approachDjango 's cache_property approach"
"I want to use bottlenecks for transfer learning using InceptionV3 in Keras.I 've used some of the tips on creating , loading and using bottlenecks fromhttps : //blog.keras.io/building-powerful-image-classification-models-using-very-little-data.htmlMy problem is that I do n't know how to use a bottleneck ( numpy array ) as input to an InceptionV3 with a new top layer.I get the following error : ValueError : Error when checking input : expected input_3 to have shape ( None , None , None , 3 ) but got array with shape ( 248 , 8 , 8 , 2048 ) 248 refers to the total number of images in this case.I know that this line is wrong , but I dont't know how to correct it : model = Model ( inputs=base_model.input , outputs=predictions ) What is the correct way to input the bottleneck into InceptionV3 ? Creating the InceptionV3 bottlenecks : Loading the bottlenecks : Starting training : Any help would be appreciated !"
"I am using pandas 0.17.0 and have a df similar to this one : with following dtypes : When I reindex my df to minute intervals all the columns int64 change to float64 . Also , if I try to resampledf3 = df.resample ( 'Min ' ) The int64 will turn into a float64 and for some reason I loose my object column.print ( df3.dtypes ) Since I want to interpolate the columns differently based on this distinction in an subsequent step ( after concatenating the df with another df ) , I need them to maintain their original dtype . My real df has far more columns of each type , for which reason I am looking for a solution that does not depend on calling the columns individually by their label . Is there a way to maintain their dtype throughout the reindexing ? Or is there a way how I can assign them their dtype afterwards ( they are the only columns consisiting only of integers besides NANs ) ? Can anybody help me ?"
"I have two sets of points in 2D A and B and I need to find the minimum distance for each point in A , to a point in B . So far I 've been using SciPy 's cdist with the code belowwhich works just fine for smallish values of N. But now the lengths of the sets have increased from N=10000 to N=35000 and I 'm running into aI know I can replace cdist with a for loop that keeps only the minimum distance ( and the index ) for each point in A to each point in B , as that is all I need . I do n't need the full AxB distance matrix . But I 've been using cdist precisely because it is fast.Is there a way to replace cdist with an implementation that is ( almost ? ) as fast , but that does not take that much memory ?"
I 've got an array of ( random ) floating point numbers . I want to round each value up to a limit of an arbitrary grid . See the following example : This results in : How can I avoid the for loop ? I 'm sure there 's some way using NumPy 's array magic that I do n't see right now .
"I have a problem to execute mapreduce python files on Hadoop by using Hadoop streaming.jar.I use : Windows 10 64bitPython 3.6 and my IDE is spyder 3.2.6 , Hadoop 2.3.0jdk1.8.0_161I can get answer while my maperducec code is written on java language , but my problem is when I want to mingle python libraries such as tensorflow or other useful machine learning libs on my data.Installing hadoop 2.3.0:1. hadoop-env export JAVA_HOME=C : \Java\jdk1.8.0_1612 . I created data - > dfs in hadoop folderFor environmentUser VariableHadoop_Home = D : \hadoopJava_Home = C : \Java\jdk1.8.0_161M2_HOME = C : \apache-maven-3.5.2\apache-maven-3.5.2-bin\Maven-3.5.2Platform = x64System Varibales : Edit Path as : My MapReduce Python code : D : \digit\wordcount-mapper.pyD : \digit\wordcount-reducer.pyWhen I run my command prompt as administrator : I checked : localhost:8088/ and http : //localhost:50070all is ok.Then when I enter : I have this error : I really donot know what is problem it tool my time a lot.Thank you in advanced for your help or any idea ?"
"In my app I find myself using stftime a lot , and mostly with 2 strings formats - ( `` % d/ % m/ % Y '' ) and ( `` % H : % M '' ) Instead of writing the string each time , I want to store those strings in some global var or something , so I can define the format strings in just one place in my app.What is the pythonic way of doing that ? Should I use a global dict , a class , a function , or maybe something else ? Maybe like this ? Or like this ? Thanks for the help"
"I want to be able to search a Seq object for a subsequnce Seq object accounting for ambiguity codes . For example , the following should be true : If ambiguity codes were taken into account , the answer should be But the answer i get is that no match is found , or Looking at the biopython source code , it doesnt appear that ambiguity codes are taken into account , as the subseqeunce is converted to a string using the private _get_seq_str_and_check_alphabet method , then the built in string method find ( ) is used . Of course if this is the case , the `` R '' ambiguity code will be taken as a literal `` R '' , not an A or G. I could figure out how to do this with a home made method , but it seems like something that should be taken care of in the biopython packages using its Seq objects . Is there something I am missing here.Is there a way to search for sub sequence membership accounting for ambiguity codes ?"
"Given a DataFrame : How can it be sorted according to the email 's domain name ( alphabetically , ascending ) , and then , within each domain group , according to the string before the `` @ '' ? The result of sorting the above should then be :"
"I 'm using os.walk with followlinks=True , but I hit a place where a symbolic link refers to it 's own directory , causing an infinite loop . The culprit in this case is /usr/bin/X11 which list listed as follow : Is there any way to avoid following links to either . or .. which I would assume , would cause similar problems ? I think I could check this with os.readlink then compare against the current path . Is there any other solution for this ?"
"My app relies on : Python 3Django 1.8WeasyprintSeleniumIt runs flawlessly on dev and production environment , but not while testing with selenium.Using weasyprint , I create a PDF from HTML , this library uses urllib to download CSS ( e.g . http : //localhost:8081/static/lib/bootstrap/css/bootstrap.min.css ) , but it hangs ( no errors , just stuck ) while opening these.If I enter this url directly in my browser while hanged , the CSS is displayed.Command used : Relevant part of the test : In my view : Here is the thread dump when stuck :"
"I can connect to Postgres via the command line no problem , however if I try to connect via Python using the psycopg2 module I get the error below . Interestingly I have just tried connecting with PSeqal.app and it crashes with the same error.Environment : MacOS : 10.13.3Xcode 9.2Python : 3.6.4Connection module : psycopg2Postgresql : 10.3Script : Here is my simple connection script which is trying to connect via Python to Postgresql : Error : Script output : $ python3 testdb.py Illegal instruction : 4Apple error ( partial ) : What I have tried so far : Upgraded Python to 3.6.4 from 3.5.1Rebuilt all dependent libraries including psycopg2 to make sure they are High Sierra compatible ( I hope ) Reinstalled Postgres from scratchUpgraded Xcode + toolsSo I 'm at a bit of a loss as to what to try next ? Has anyone else run into this ? Any pointers to where to look ? Anything that helps me get this working would be greatly appreciated thanks !"
"I was scrolling through the PEP index page and noticed that a PEP number was reserved by Warsaw : I looked it up out of curiosity and the only thing I found referencing this was the PEP index page and this commit from 2013 which did n't anwser my questionIn the commit , they explain that it could be for humour reason ( like 666 for example ) but I Do n't see why 801 . What is that number linked to ?"
"I have a list of values like this , Desired Output : Lets assume a window size of 4 , now my desired output : for each_element in the list , I want 4 values in-front and 4 values backward ignoring the current value . I was able to use this to get sliding window of values but this also not giving me the correct required output ."
"When I use keras 's binary_crossentropy as the loss function ( that calls tensorflow 's sigmoid_cross_entropy , it seems to produce loss values only between [ 0 , 1 ] . However , the equation itself implies that the range is from [ 0 , infinity ) . So is Tensorflow doing some sort of clipping that I 'm not catching ? Moreover , since it 's doing math_ops.add ( ) I 'd assume it 'd be for sure greater than 1 . Am I right to assume that loss range can definitely exceed 1 ?"
"I am surprised by the following behavior : What is going on here ? When I use float ( 'nan ' ) instead of np.nan , I get the behavior I expect : I am using python 2.7.3 and numpy 1.8.1.Edit : If I do : So , Counter or any python dict considers two objects X and Y not the same if : correct ?"
"I have a text file ( huge ) with all numbers separated with a combination of spaces and tabs between them and with comma for decimal and after decimal separation , while the first column is scientific formatted and the next ones are numbers but with commas . I just put the first row here as numbers : 0,0000000E00 -2,7599284 -1,3676726 -1,7231264 -1,0558825 -1,8871096 -3,0763804 -3,2206187 -3,2308111 -2,3147060 -3,9572818 -4,0232415 -4,2180738 the file is so huge that a notepad++ ca n't process it to convert the `` , '' to `` . '' So what I do is : I tried even to use digits , but that causes to divide the first column in two numbers : and eventually the error I get when using .replace command . what I would have prefered was to convert the commas to dots regardless of disturbing formats like scientific . I appreciate your help ValueError : could not convert string to float : ' 00000000E00 \t-29513521 \t-17002219 \t-22375536 \t-14994097 \t-24163610 \t-34076621 \t-31233623 \t-32341597 \t-24724552 \t-42434935 \t-43454237 \t-44885144 \n ' I also put how the input looks like in txt and how I need it in output ( in csv format ) input seems like this : first line : between 1st and 2nd column : 3 spaces + 1 Tabbetween rest of columns : 6 spaces + 1 Tabsecond line and on : between 1st and 2nd column : 2 spaces + 1 Tabbetween rest of columns : 6 spaces + 1 Tabthis is a screen shot of the txt input file : Attention : there is one space in the beginning of each lineand what I want as output is csv file with separated columns with `` ; ``"
So I am trying to implement Lowest common subsequence in Python and was trying this alternative to my previous solution . I tried using a dictionary instead of a 2-D matrix to memoize the results.It 's returningwhich I understand is because I am not returning anything so how can I do something like this.And I am trying to implement it without using any decorators .
I was reading the django article on form validatin here and i came across thismy question is what does the _ ( 'Invalid value ' ) does ?
"I 'm trying to reduce the number of decimals that I 'm getting after some calculations . The print ( ) where my problem arises looks like this : And it outputs this : Now I want to reduce the number of decimal places that are printed to 3 . I tried doing it with string formatting , like this : However , this code prints : What I actually want is this : How can I format res to only be displayed as scientific notation but with only 3 decimal places ?"
Alright so this is what I haveAnd when I try to load up some broken python codeThis gives me a SecurityException instead of letting me see the actual exception from the code.If I set the PermissionSet ( PermissionState.Unrestricted ) it works fine.Any ideas on what permissions I need in order to catch these blasted errors ?
I have list of lists and would like to create data frame with count of all unique elements . Here is my test data : I can do something like this using Counter with for loop as : But how can I have result of this loop summed up into new data frame ? Expected output as data frame :
"I have this simple code that helped me to measure how classes with __slots__ perform ( taken from here ) : If I run it via python2.7 - I would get something around 6 seconds - ok , it 's really faster ( and also more memory-efficient ) than without slots.But , if I run the code under PyPy ( using 2.2.1 - 64bit for Mac OS/X ) , it starts to use 100 % CPU and `` never '' returns ( waited for minutes - no result ) .What is going on ? Should I use __slots__ under PyPy ? Here 's what happens if I pass different number to timeit ( ) : Thanks in advance.Note that the same behavior is observed if I use namedtuples :"
"Is there an equivalent when using str.contains ? the following code is mistakenly listing `` Said Business School '' in the category because of 'Sa . ' If I could create a wordboundary it would solve the problem . Putting a space after messes this up . I am using pandas , which are the dfs . I know I can use regex , but just curious if i can use strings to make it faster"
I 'm using boost : :python to export some C++ functions that expect a void* . They interpret it as raw memory ( an array of bytes ) internally . Think read and write for some very special purpose device.How do I pass a Python bytearray to such a function ? I have tried using ctypes.c_void_p.from_buffer ( mybytearray ) but this does n't match the signature of the function.Here 's the minimal example : And at the python side :
Please note this is not a copy of the this issueI 'm simply trying to add to a list that is defined in a one_to_many relationship in my model as below . Here 's my model : And when I try to create via : with the init ( .. ) function being : I get the following stack trace :
"I am comparing the performance of the any ( ) built-in function with the actual implementation the docs suggest : I am looking for an element greater than 0 in the following list : This is the supposedly equivalent function : And these are the results of the performance tests : I would expect both of the to have the exact same performance , however any ( ) if two times slower . Why ?"
"I solved a quadratic equation using sympy : Based on some testing I figured out that only q_solve [ 0 ] makes physical sense . Will sympy always put ( b - sqrt ( b**2 - 4*a*c ) ) /2a in the first place ? I guess , it might change with an upgrade ?"
"So in my flask app , I have a form on the frontend that gets populated with several users . Each user is associated with a checkbox with the name 'selected_user ' . On submit , the form is posted through standard HTML form controls ( no javascript or manual ajax of any kind ) .In the backend , I can parse this usingand it returns a list of users as I expect ( a user here is itself a dictionary of unique keys and associated values ) .Printing out flask.request.form looks like so for example : My problem is , I can not seem to replicate this format in my unit tests for the life of me . Obviously , I could use some javascript to bundle the checked users on the frontend into an array or whatever and then duplicate that area much easier on the backend , and that may very well be what I end up doing , but that seems like an unnecessary hassle just to make this function testable when it already behaves perfectly in my application.Here is what I currently have tried in my test , which seems like it should be the correct answer , but it does not work : This gives me an error as follows : I 've also attempted sending these as query strings , sending data as json.dumps ( data ) , encoding each mock_user as a tuple like this : None of these approaches have worked for various other errors . What am I missing here ? Thanks ahead of time for any help ! Also , sorry if there are an obvious syntax errors as I rewrote some of this for SO instead of copy pasting ."
this bash script can catch all the environment variables which are set when data is passed through STDIN eg such as : script.shit there any way i can do same in python ? ? RESOLVED : this is the resultant python version..
"I have an example in C++ that I 'm trying to reproduce using h5py , but it is not working as expected . I 'm getting null padded strings with h5py where I expect null terminated strings.Here is my C++ driver ... main.cppWhich I build with the following SCons script.SConstructAnd here is my python script that I 'm trying to get to write out the same hdf5 file with.main.pyI 'm using python v2.7.11 , and I have tried this with h5py v2.5.0 , and v2.6.0 , with the following same results.As you can see from the output above , I am still ending up with null padded fixed length strings when using h5py , even though I am specifying I want null terminated fixed length strings.So how do I modify my python script to end up with null terminated fixed length strings in the dataset ? If it is a bug in h5py , are there any workarounds ? Thanks in advance for any help ."
"With matplotlib , I am making a figure containing two Axes objects ( i.e. , two sets of xy-axes ) . I want to connect two points -- - one picked from one of the axes and the other picked from the other axis -- - by an arrow or a line.I tried to do this by using annotate ( ) function and ConnectionPatch object , but in both ways , a part of the arrow was hidden by the 'frame ' of an axis . Please see the attached figure , in which I tried to connect the origins of the two axes by a ConnectionPatch object . I am also attaching the script used to generate the figure.Is there a way to 'bring forward ' the arrow ( or push the axis frame to the back ) ?"
"models.pyadmin.py-admin.jsNow I have imported the file in the admin.py , How do I trigger the jQuery function such that it works.update- the function works but , I need to reload the page to make the field appear and disappear . I want some thing equivalent to 'on-click ' event in HTML . I have no idea about Javascript ."
"Let 's consider 3 tables : booksAmerican authorsBritish authorsEach book has a foreign key to its author , which can either be in the American table , or the British one.How can I implement such foreign key condition in SQLAlchemy ? I 'd like to have a single column to handle the link.My approach so far was to create an abstract class Author , from which both AmericanAuthor and BritishAuthor inherit , and have the foreign key of Book point to the parent.It fails with the error : Which completely makes sense , considering author is abstract ..."
"List top_brands contains a list of brands , such asitems is a pandas.DataFrame and the structure is shown below . My task is to fill the brand_name from item_title if brand_name is missingMy code is as below . The problem is that it is too slow for a dataframe that contains 2 million records . Any way I can use pandas or numpy to handle the task ?"
"Trying to contribute some optimization for the parallelization in the pystruct module and in discussions trying to explain my thinking for why I wanted to instantiate pools as early in the execution as possible and keep them around as long as possible , reusing them , I realized I know that it works best to do this , but I do n't completely know why . I know that the claim , on *nix systems , is that a pool worker subprocess copies on write from all the globals in the parent process . This is definitely the case on the whole , but I think a caveat should be added that when one of those globals is a particularly dense data structure like a numpy or scipy matrix , it appears that whatever references get copied down into the worker are actually pretty sizeable even if the whole object is n't being copied , and so spawning new pools late in the execution can cause memory issues . I have found the best practice is to spawn a pool as early as possible , so that any data structures are small . I have known this for a while and engineered around it in applications at work but the best explanation I 've gotten is what I posted in the thread here : https : //github.com/pystruct/pystruct/pull/129 # issuecomment-68898032Looking at the python script below , essentially , you would expect free memory in the pool created step in the first run and the matrix created step in the second to be basically equal , as in both final pool terminated calls . But they never are , there is always ( unless something else is going on on the machine of course ) more free memory when you create the pool first . That effect increases with the complexity ( and size ) of the data structures in the global namespace at the time the pool is created ( I think ) . Does anyone have a good explanation for this ? I made this little picture with the bash loop and the R script also below to illustrate , showing the free memory overall after both pool and matrix are created , depending on the order : pool_memory_test.py : pool_memory_test.shpool_memory_test_plot.R : EDIT : fixing small bug in script caused by overzealous find/replace and rerunning fixedEDIT2 : `` -0 '' slicing ? You can do that ? : ) EDIT3 : better python script , bash looping and visualization , ok done with this rabbit hole for now : )"
"I have two lists of elements that look likea contains a set of data , and b defines some intervals , my aim is to create a list c with as many list as the intervals in b . Each list in c contains all the x elements in a for which x [ 0 ] is contained in the interval . Ex :"
"I am trying to make use of PEP 496 -- Environment Markers and PEP 508 -- Dependency specification for Python Software Packages by specifying dependencies that only make sense on specific OS.My setup.py looks like this : My minimal setup.cfg looks like this : However , when trying to install such a package with pip install -e foobar/ , it fails with : I guess it does not expect semicolon there . But how am I supposed to use environment markers then ?"
"CPython 3.6.4 : now : I thought partial just remembers part of parameters and then forwards them to the original function when called with the rest of the parameters ( so it 's nothing more than a shortcut ) , but it seems it makes some optimization . In my case the whole max2 function gets optimized by 15 % compared to the max1 , which is pretty nice.It would be great to know what the optimization is , so I could use it in a more efficient way . Docs are silent regarding any optimization . Not surprisingly , `` roughly equivalent to '' implementation ( given in docs ) , does not optimize at all :"
I have a set of LaTeX files . I would like to extract the `` abstract '' section for each one : I have tried the suggestion here : How to Parse LaTex fileAnd tried : Where data contains the text from the LaTeX file . But A is just an empty list . Any help would be greatly appreciated !
"I use a playbook runner like so : The results variable contains output like the following : Which is fine . But I also need the task level output like the dict shown below : I tried changing the verbosity , but that was not what I wanted.After digging around I managed to get the output to a log file like so : But I need access to it in my code ."
"I have discovered one thing that makes me crazy . If I specify the following list : then no errors will be displayed , and the same will happen if I use 02,03,04,05,06,07 , but in case I use 08 or 09 as the second item in the list , I get the following exception : Also the same behavior appears when I put these numbers ( 08,09 ) at the any location within list ( eg . [ 08,10,2011 ] ) , even if I try to assign 08 to a single int variable I get the same exception . Is there any reason why this happens ?"
"This is not actually a problem , it is more something curious about floating-point arithmetic on Python implementation.Could someone explain the following behavior ? It seems that 1 divided by a number near to zero is inf and if it is nearer to a ZeroDivisionError is thrown . It seems an odd behavior.The same output for python 2.x/3.x.EDIT : my main question here is why we get inf for some range and not ZeroDivisionError assuming that python seems to consider as zero 1e-309"
"Given the HTML code below I want output just the text of the h1 but not the `` Details about '' , which is the text of the span ( which is encapsulated by the h1 ) . My current output gives : I would like : Here is the HTML I am working withHere is my current code : Note : I do not want to just truncate the string because I would like this code to have some re-usability.What would be best is some code that crops out any text that is bounded by the span ."
"When setting up a Flask server , we can try to receive the file user uploaded by When I am looking at the Klein documentation , I 've seen http : //klein.readthedocs.io/en/latest/examples/staticfiles.html , however this seems like providing file from the webservice instead of receiving a file that 's been uploaded to the web service . If I want to let my Klein server able to receive an abc.jpg and save it in the file system , is there any documentation that can guide me towards that objective ?"
"I started out with having the same problem as in this question . As one of the answers suggests one can avoid that particular problem by making the app run without SSL . But as Facebook is set to enforce https for apps in just a few days ( October 1 2011 ) that seems like a solution that wo n't last . I first tried to enable ssl in app.run ( around line 149 in exampleapp.py . Like so : At first try it failed at start complaining about the missing OpenSSL module . Found a couple of suggestions on how to fix that out there on the net , and choose to do : Now no complaints at start : But then when trying to access the app : What to do about this ? Is it the wrong Python version or just me missunderstanding some other fundamental thing ?"
"I use Google Style Python Docstrings like in this Example for my Django 's project . When i create a class and using an attributes notation in docstring , Pycharm always say - `` Unresolved reference '' .I understand that PyCharm does n't see self for title and def __init__ ( ) function and write this error , but in Django I 've never seen using def __init__ ( ) for classes inherited from models.What should I do ? Is this my error or does PyCharm not see context in this case ? Should I use def __init__ ( ) or something else or write docsting in another way ?"
"I am writing dynamic filters in django for my database where I am using the below code where I have 2 variables ( p_type , s_type ) : but django returns error at below line I have dynamic radio button where the value of p_type matches with my database but even though I am receiving the following error : Is n't it doable with variable query ? Any other methods ? model :"
"I have written a class using ITK in CPP which reads all files in a directory and then averages them . I would like to use this class in a pipeline constructed using Python . I had previously tried to use Swig to wrap template code but according to the swig documenation , it does n't have template support and the type names need to explicitly specified . But when I use ITK in Python , the interface is very different to that I expect from Swig-generated template code ( the type name is not specified in the function/class name at all , which is contrary to what Swig documentation says ) .A small snippet from my code illustrating the usage of the class is shown below : The code for the class can be seen in the Git repository . I do n't have an issue with increasing the dependencies of my project by using Boost : :Python but I need a starting point to proceed . Any help would be extremely appreciated.Thanks.UPDATE : Expected usage in Python would be ,"
"Let 's say I have a list of 8 objects , numbered that 1-8.The objects are put into three boxes , 3 in one box , 3 in another box , 2 in the last box . By mathematics , there are 8C3*5C3=560 ways to do this . I want to loop through there 560 items.Is there any way in Python to do so ? The result should look like this : Note that [ '12 ' , '345 ' , '678 ' ] and [ '12 ' , '354',876 ' ] are considered the same for this purpose.I want to make a for-loop this list . Is there any way in Python to do so ? Here is the solution I get , but it seems ugly ."
I am using tf.python.ops.rnn_cell.GRUCellHow do I get the weights of this GRUCell . I need to see them for debugging .
"While creating a program that backs up my files , I found that os.path.basename ( ) was not working consistently . For example : returns 'backup_files'returns `` I want that second basename function to return 'studies ' but it returns an empty string . I ran os.path.split ( folder ) to see how it 's splitting the string and it turns out it 's considering the entire path to be the directory , i.e . ( '\\\\server\\studies ' , ' ' ) .I ca n't figure out how to get around it.. The weirdest thing is I ran the same line earlier and it worked , but it wo n't anymore ! Does it have something to do with the very first part being a shared folder on the network drive ?"
"Consider the following two functions , which essentially multiply every number in a small sequence with every number in a larger sequence to build up a 2D array , and then doubles all the values in the array . noloop ( ) uses direct multiplication of 2D numpy arrays and returns the result , whereas loop ( ) uses a for loop to iterate over arr1 and gradually build up an output array.I expected noloop to be much faster even for a small number of iterations , but for the array sizes above , loop is actually faster : And interestingly , if I remove *2 in both functions , noloop is faster , but only slightly : Is there a good explanation for these results , and is there a notably faster way to perform the same task ?"
"I am trying to mock a coroutine . As such , this mock 's __next__ ( ) and close ( ) are called . While mocking close ( ) works , I can not mock __next__ ( ) : What am I missing ? How to make sure my mock 's __next__ ( ) method is called ? For now , I am using the following : However , I very much rather use a standard mock ."
"I 'm trying to get the number of items on the desktop using win32gui in python 2.7.The following code : win32gui.SendMessage ( win32gui.GetDesktopWindow ( ) , LVM_GETITEMCOUNT ) returns zero and I have no idea why.I wrote win32api.GetLastError ( ) afterwards and it returned zero either.Thanks in advance.EDIT : I need to use this method because the final goal is to get the positions of the icons , and it 's done by a similar method . So I just wanted to make sure that I know how to use this method . Also , I think that it can give a different output than listing the content of desktop ( can it ? ) . And thirdly , my sources of how to get the positions suggested to do it this way - http : //www.codeproject.com/Articles/639486/Save-and-restore-icon-positions-on-desktop for example.EDIT2 : Full code for getting the count ( does n't work for me ) : Thanks again ! SOLUTION :"
"I 'm generating specifically formated input files for a program , and using a small python tkinter GUI frontend to do so . The old code made use of fortran format statements . Unless there 's already a direct conversion set of functions for python ( which I have n't found ) , I figured python 's formatting would do the job . In general it can , but I ca n't find a way to have repetition of a certain value : For example , in fortran : I know I could textually have 12 items in my format call ( e.g : ... { 0:5d } , { 1:5d } , { 2:5d } ... . ) , but I was wondering if there 's a way to have a simplified form like the above fortran example.Is there something I missed , or is this not possible and I must explicitly write out every item in the format ? -Cheers , Chris.editHere is a more clear example of what I 'm currently doing : I 'd like to be able to"
"I am trying to generate all possible keypad sequences ( 7 digit length only right now ) . For example if the mobile keypad looks like this : Some of the possible sequences can be : 123698 147896 125698 789632The requirement is that the each digit of number should be neighbor of previous digit.Here is how I am planning to start this : The information about the neighbor changes from keypad to keypad so we have to hardcode it like this : I will be traversing through all digits and will append one of the possible neighbors to it until required length is achieved.EDIT : Updated neighbors , no diagonals allowedEDIT 2 : Digits can be reused"
"When I create an AWS Lambda Layer , all the contents / modules of my zip file go to /opt/ when the AWS Lambda executes . This easily becomes cumbersome and frustrating because I have to use absolute imports on all my lambdas . Example : So I was wondering , is it possible to add these /opt/paths to the PATH environment variable by beforehand through serverless.yml ? In that way , I could just from dynamodb_layer.customer import CustomerManager , instead of that freakish ugliness ."
"I have installed Anaconda in order to set up different environments for python . I know that I can create different environments with different python versions with the shell , like so : How would I be able to create a new environment with Python 3.6 using the Anaconda navigator . For some reason it only shows Python version 3.7"
"Basically I have created a database , in normal full query , this is the code I 've used and the response generated.The queries generated are as below : This is logical as I 'm extracting everything from the table . However , when I try to use load_only to specifically select one column , in this case , the email column . The code I 've used is : Both commands give me the same results : Which is extremely weird because I should be getting just one column in the query . However , when I use this : it magically returns just one column for me . I needed to use load_only as I have dynamic tables that I want to reuse the same function , rather than maintaining many sets of functions . Can anyone advise what is the issue with the load_only command , or if I 'm doing something wrong ? Thank you ."
"I was only able to reproduce this on a Windows build of Python - to be precise : Python 2.7.15 ( v2.7.15 : ca079a3ea3 , Apr 30 2018 , 16:30:26 ) [ MSC v.1500 64 bit ( AMD64 ) ] on win32 . On my Arch Linux box installation of Python ( Python 2.7.15 ( default , May 1 2018 , 20:16:04 ) [ GCC 7.3.1 20180406 ] on linux2 ) the loop does not seem to terminate indicating that the a**2 = a * a invariant holds there.What is going on here ? I know that IEEE floats come with a plethora of misconceptions and idiosyncrasies ( this , for example , does not answer my question ) , but I fail to see what part of the specification or what kind of implementation of ** could possibly allow for this.To address the duplicate flagging : This is most likely not directly an IEEE floating point math problem and more of a implementation issue of the ** operator . Therefore , this is not a duplicate of questions which are only asking about floating point issues such as precision or associativity ."
"I have three applications , but I want them to use the same layout.html and css . Is there any way to achieve this ? EDIT : I put the static folder and layout.html etc in /common/ under the web2py root.Here 's what I did in the model : Then in the views : EDIT 2 : Regarding the comment below about compiling , I decided to put the 'common ' folder into '/applications/ ' and place the static folder ( css , images ) inside the 'common ' folder like a regular app . I then placed the layout.html into the root of 'common ' . Then from another app 's view , I used : Which referenced the layout.html from the common app . This layout.html file then referenced the files in the static folder within 'common ' using : As you would for a regular application ."
"I 'm trying to use NTLK grammar and parse algorithms as they seem pretty simple to use . Though , I ca n't find a way to match an alphanumeric string properly , something like : Is there an easy way to achieve this ?"
I was looking at the source code of a Blender add-on and I saw a new syntax : What is the meaning of ... ?
"In Ruby ( on Rails , actually ) , I have the try function : The function basically is a shortcut to avoid ifs to check if the position is present.Is there something similar in Python ?"
"I am trying to use the multiprocessing.Pool to implement a multithread application . To share some variables I am using a Queue as hinted here : This code is running perfectly on a Debian machine , but is not working at all on another Windows 10 device . It fails with the error I do not really know what exactly is causing the error . How can I solve the problem so that I can run the code on the Windows device as well ? EDIT : The problem is solved if I create the get_predediction_init function on the same level as the mainFunction . It has only failed when I defined it as an inner function . Sorry for the confusion in my post ."
"I have an existing solution to split a dataframe with one column into 2 columns.Recently , I got the following warning FutureWarning : Columnar iteration over characters will be deprecated in future releases.How to fix this warning ? I 'm using python 3.7"
"I 'm trying to create a simple markdown to latex converter , just to learn python and basic regex , but I 'm stuck trying to figure out why the below code does n't work : I want to convert something like : to : this is what I got ( from using my regex above ) : Why is the pattern only been matched once ? EDIT : I tried the following lookahead assertion : now it matches all the footnotes , however they 're not matched correctly.is giving meAny thoughts about it ?"
"The data are coordinates of objects in the sky , for example as follows : I want to do a 2D histogram in order to plot a map of the density of some point with ( l , b ) coordinates in the sky , using HEALPix pixellization on Mollweide projection . How can I do this using healpy ? The tutorial : http : //healpy.readthedocs.io/en/v1.9.0/tutorial.htmlsays how to plot a 1D array , or a fits file , but I do n't find how to do a 2d histogram using this pixellization.I also found this function , but it is not working , so I am stuck.I can do a plot of these points in Mollweide projection this way : Thank you very much in advance for your help ."
"I have two dataframesI wan na merge the two dataframes on ID and then create a multi-index to look like : How can I do it ? a direct pandas.merge will create suffixes_x , _y which I do not want ."
"Using a Korean Input Method Editor ( IME ) , it 's possible to type 버리 + 어 and it will automatically become 버려.Is there a way to programmatically do that in Python ? Is there a way to compute that 47532 + 50612 - > 47140 ? Here 's some more examples : 가보 + 아 - > 가봐끝나 + ㄹ - > 끝날"
"I am trying to create a subclass of a Pandas data structure to substitute , in my code , a subclass of a dict with a subclass of a Series , I do n't understand why this example code does n't workProbably it is not the best method to change the configuration of some objects , anyway I found this usefull in my code and most of all I want to understand why with dict instead of series it works as I expect.Thanks a lot !"
"I have columns in a data table that I need to join . One column consists of values and the other of corresponding error values , for example : What I 'd like to do is , for each row join the columns along with a +/- , but the clean unicode character ( U+00B1 ) . I 've never tried to use unicode characters in python before , so I 'm sorta stumped.If my .join ( ) looks likehow exactly do I let python know I want to use a unicode character ."
"I have a dataframe , where the left column is the left - most location of an object , and the right column is the right most location . I need to group the objects if they overlap , or they overlap objects that overlap ( recursively ) . So , for example , if this is my dataframe : so lines 0 and 3 overlap - thus they should be on the same group , and also line 1 is overlapping line 3 - thus it joins the group . So , for this example the output should be something like that : I thought of various directions , but did n't figure it out ( without an ugly for ) . Any help will be appreciated !"
Where is python getting the repr which is still yielding 'foo ' even after the original repr method has been overwritten ?
"When working with Pandas datetimes , I 'm trying to group data by the week and year . However , I have noticed some years where the last day of the year ends up grouped with the first week of the same year . For 2018 and 2019 , the first day of the year ends up grouped with the final day of the year ! Is this behavior expected ? Why would the final day of the year be week 1 ? I 've gotten the result I want with a basic if statement , but this week behavior seems like it could lead to issues because it 's unexpected.This does what I intended with the grouping :"
"In Python , what is the best way to generate some random number using a certain seed but without reseeding the global state ? In Java , you could write simply : and the standard Math.random ( ) would not be affected . In Python , the best solution that I can see is : Is this idiomatic Python ? It seems much less clean than the Java solution that does n't require `` restoring '' an old seed . I 'd love to know if there 's a better way to do this ."
"I 'm working with tif stacks and QImage appears to be skewing some images to a 45 degree angle . Matplotlib is able to display the images without a problem in both test cases ( links to two tif stacks are provided below ) so I do n't think I 've screwed up my array somewhere.Here 's a working example : ( NOTE : this example only shows the first image in the tif stack for simplicity ) Here 's a screenshot of the output : Qimage vs matplotlibHere 's a link to a tif stack that displays properly : https : //drive.google.com/uc ? export=download & id=0B9EG5AHWC9qzX3NrNTJRb2toV2cAnd here 's a link to a tif stack that becomes skewed when displayed : https : //drive.google.com/uc ? export=download & id=0B9EG5AHWC9qzbFB4TDU4c2x1OE0Any help understanding why QImage is skewing this image would be much appreciated . The only major difference between the two tif stacks is that the one that displays skewed has a padded black area ( zeros ) around the image which makes the array larger.UPDATE : I 've now discovered that if I crop the offending image to 1024x1024 or 512x512 or 1023x1024 QImage displays properly but cropping by 1024x1023 displays skewed . So it appears that the x ( horizontal ) length must be a power of 2 in order for QImage to handle it as expected . That 's a ridiculous limitation ! There must be something I 'm not understanding . Surely there 's a way for it to handle arbitrarily shaped arrays ... .I suppose , in principle , one could first apply a skew to the image and just let QImage deskew it back ... ( < == not a fan of this solution )"
"Ca n't get deleting videos to work using the Youtube Data API . I 'm using the Python Client Library.All of this seems straight from the docs , so I 'm really confused as to why it 's not working . Here 's my function : From the docs , this should return True if the video is successfully deleted . However , it returns None : And the video is not deleted . I know the credentials are good , because they are the same credentials I used to upload the video in the first place , and I know the id is good , because I got it directly from my channel in youtube . Any ideas ?"
"I am trying to reproduce the reactive extensions `` shared '' observable concept with Python generators.Say I have an API that gives me an infinite stream that I can use like this : I could use this generator multiple times like so : And the_infinite_stream ( ) will be called twice ( once for each generator ) .Now say that the_infinite_stream ( ) is an expensive operation . Is there a way to `` share '' the generator between multiple clients ? It seems like tee would do that , but I have to know in advance how many independent generators I want.The idea is that in other languages ( Java , Swift ) using the reactive extensions ( RxJava , RxSwift ) `` shared '' streams , I can conveniently duplicate the stream on the client side . I am wondering how to do that in Python . Note : I am using asyncio"
"I have the following code : My problem is that sum ( matrix [ i ] ) could be 0 in some cases , resulting in a ZeroDivisionError . But because matrix [ i ] [ i ] is also 0 in that case , I solved this as follows : The function divide ( x , y ) returns 1 if y == 0 and ( x / y ) if y > 0 . But I wonder if there is an easier way . Maybe I could use some ternary operator , but does that exist in Python ?"
I 'm trying OpenID support for Google App Engine on a small project i have on my machine but when i call : users.create_login_url ( federated_identity = provider_url ) i get this error : provider_url is https : //www.google.com/accounts/o8/idany clue ?
"Using the code below I can get a 2x2 graph with 4 plots . With brushes , I can select some data points . The question I have is how do get the selected data points as a JSON array or cvs . This code uses mlpd3 , but bokeh can do similar selections with brushes.. But there is no example of selecting the data points . I am trying to get selected data as object to continue processing with python . It would be nice to see the data in a cell . Bokeh has similar behavior in CustomJS for Selectionshttp : //docs.bokeh.org/en/latest/docs/user_guide/interaction/callbacks.html # userguide-interaction-jscallbacks-customjs-interactionsWhichever one is easier to extract the selected item -- would work.. If there is a Plotly solution , that would also work ."
"So I made a recursive lambda in Python for the Fibonacci sequence . I used recursion because it was easiest to implement with lambda.Because using recursion , the same Fibonacci values were calculated many times , I thought using a cache decorator would help , and I knew that functools.lru_cache was an easy option . I know you ca n't apply the decorator to the function using @ functools.lru_cache to a lambda like a normal function , but when I tried this : I got an error saying that functools.lru_cache would n't accept a function object as an argument.I checked the docs , and it looks like functools.lru_cache only accepts a maxsize and a typed argument , which default to 128 and False respectively.Is there a cleverer way to assign the decorator to the function rather than just defining the function without lambda and then applying the decorator ?"
"As I understand it , a deep copy of an ndarray should create a second iteration of the ndarray so that changing either array will not affect the contents of the other . However , in the following code , my original ndarray is changed : The array is a mixed type ( 5,2 ) array with a ( largenumber,2 ) subarray inside . I am only trying to change the subarray but I am wondering if the deep copy extends to that subarray as well . I have runIt might also be important to note that I am running this in a jupyter notebook . Although I ca n't imagine why it would change anything . You can recreate data with :"
"I want to use pytest to check if the argparse.ArgumentTypeError exception is raised for an incorrect argument : However , running pytest says During handling of the above exception , another exception occurred : and consequently the test fails . What am I doing wrong ?"
"There is a libx.so which export 2 functions , and a struct , I want to call create in Python and then save the Tag *res returned by create , later I will call use and pass the Tag *res saved before to use , here is it ( just to demonstrate ) : The above code might be wrong , just to demonstrate what I want to do.And my problem is that , how could I save the result returned by create ? Because it returns a pointer to a user-defined struct , and I do n't want to construct struct Tag 's counterpart in Python , would c_void_p do the trick ? UPDATEFrom @ David 's answer , I still do n't quite understand one thing : the pointer ( c_char_p ( `` a '' ) ) is only valid for the duration of the call to create . As soon as create returns then that pointer is no longer valid . And I assign c_char_p ( `` a '' ) to t- > name in create , when the call to create finishes , is t- > name a dangling pointer ? Because according to the quoted words , that pointer is no longer valid after create . Why c_char_p ( `` a '' ) is no longer valid ?"
"I want to search for a regex match in a larger string from a certain position onwards , and without using string slices.My background is that I want to search through a string iteratively for matches of various regex 's . A natural solution in Python would be keeping track of the current position within the string and using e.g . in a loop . But for really large strings ( ~ 1MB ) string slicing as in largeString [ pos : ] becomes expensive . I 'm looking for a way to get around that.Side note : Funnily , in a niche of the Python documentation , it talks about an optional pos parameter to the match function ( which would be exactly what I want ) , which is not to be found with the functions themselves : - ) ."
"My final purpose is to convert a running Python project to Jython interpreter because some java API 's are going to be added.Details : The latest Jython is 2.7The project I have is runnable with Python 3.5So I took the following approach : First thing was to convert my project to Python 2.7 utilizing the future module and pasteurize executable.This step was done successfully.Second thing is to convert the Python 2.7 project to a Jython 2.7 Project.Switching the interpreter at Eclipse mars , the following error was indicated : To workaround it , the solution from this post was utilized by passing the encoding argument -Dpython.console.encoding=UTF-8 to the java VM according to figure below : Thumbs were up when trying to run again . But unfortunately here , the error shown below just appeared . As I do n't plan to change any imported module if there 's no need to do so , I decided to ask help here.Running Jython through the command line results in a shorter error log : Does anybody have a clue of how to solve this error in the most elegant manner ? A bug was created at Jython.org with a critical severity because many people are utilizing already the latest Python modules and interpreter . And they might want to add Java functionality to their code . So they would basically have to take same path as I did . Backporting the project to Python 2.7 then to Jython 2.7 subsequentely.At python-future a feature request bug was recorded too ."
"Is anyone else getting the above ? I ca n't for the life of me figure out why it 's complaining about the google.api.core module . This was working for me last week , but since updating the google-api-python-client it 's now not , and I 'm struggling to get back to a working version.Pip freeze , as requested :"
"I have the following two threads : The targets are GObject.Mainloop ( ) methods.Afterwards my main program is in an infinite loop.My problem is that when the execution is terminated by CTRL-C , Keyboardexception is raised for both threads , but the main program does not terminate.Any ideas how could both the main program and the two threads be terminated by CTRL-C ?"
"I want to do something likeBut , I might have 15 different variables . Is there a simpler way likeThanks for any help"
"You all know this graph : There you have a day of the week and the month of a year with some kind of activity . Let 's say I have this Pandas dataframe : How to create such a Github-Activity-like graph with Matplotlib ? I think the contour plot with nearest is in the right direction , is n't it ?"
working on trying to get gevent-websocket working and it 's not connecting to my policy server for the flash specification . My policy.py is as follows : Yet with websocket I 'm getting :
I am trying to read a tab delimited text file into a dataframe . This is the how the file looks in Excel : Import into a df : Now it does n't see the first 3 columns as part of the column index ( df [ 0 ] = Transaction Type ) and all of the headers shift over to reflect this . I am trying to manipulate the text file and then import it to a mysql database as an end result .
"I am calculating triad census as follows for my undirected network.It works fine with small networks . However , now I have a bigger network with approximately 4000-8000 nodes . When I try to run my existing code with a network of 1000 nodes , it takes days to run . Is there a more efficient way of doing this ? My current network is mostly sparse . i.e . there are only few connections among the nodes . In that case , can I leave the unconnected nodes and do the computation first and later add the unconnceted nodes to the output ? I am also happy to get approximate answers without calculating every combination.Example of triad census : Triad census is dividing the triads ( 3 nodes ) in to the four categories shown in the below figure.For example consider the network below.The triad census of the four classes are ; I am happy to provide more details if needed.EDIT : I was able to resolve the memory error by commenting the line # print ( len ( list ( combinations ( G.nodes , 3 ) ) ) ) as suggested in the answer . However , my program is still slow and takes days to run even with a network of 1000 nodes . I am looking for a more efficient way of doing this in python.I am not limited to networkx and happy to accept answers using other libraries and languages as well.As always I am happy to provide more details as needed ."
"I 'm having trouble getting enums into my table from my alembic migration.MVCE here : https : //pastebin.com/ng3XcKLf ( SQLAlchemy 1.2 , psycopg2 2.7.3.2 and postgres 10.1 - line 15 needs to be modified with your postgres URI ) I read about issues with SQLAlchemy/Postgres and Arrays of Enums , but according to what I could find in issue tracker , that was resolved with 1.1 . Can someone point me in the right direction ? Variation 1 : Attempting to use an attribute of the postgres enum typeThis fails with : AttributeError : 'Enum ' object has no attribute 'resourceType'Variation 2 : attempting to use an attribute of the underlying python EnumThis fails with sqlalchemy.exc.ProgrammingError : ( psycopg2.ProgrammingError ) cols of type permissioncontexts [ ] but expression is of type text [ ] Variation 3 : Casting the string array into an enum arrayThis may or may not work - the python process balloons up to using 4GB of memory , and sits there until terminated.Variation 4 : Inserting empty arrayThis works , but , obviously , no value ."
"I 'm trying to implement a geofencing for a fleet of trucks . I have to associate a list of boundaries to a vehicle . On top of that one of the requirements is keep everything even once it is deleted for audit purposes . Therefore we have to implement soft delete on everything . This is where the problem lies . My many to many field does not conform to the soft delete manager , it includes both the active and the inactive records in the lookup dataset.As you see above I tried to make sure the default manager is a soft delete manager ( ie . filter for active records only ) and also try use limit limit_choices_to but that turn out to field the foreign model only not the `` through '' model I wanted . If you have any suggestions or recommendation I would love to hear from you.Thanks !"
"I was testing some code on the interpreter and I noticed some unexpected behavior for the sqlite3.Row class.My understanding was that print obj will always get the same result as print str ( obj ) , and typing obj into the interpreter will get the same result as print repr ( obj ) , however this is not the case for sqlite3.Row : I think sqlite3.Row must be a subclass of tuple , but I still do n't understand exactly what is going on behind the scenes that could cause this behavior . Can anyone explain this ? This was tested on Python 2.5.1 , not sure if the behavior is the same for other Python versions.Not sure whether or not this matters , but the row_factory attribute for my Connection was set to sqlite3.Row ."
"In python , I can do the following to get all the objects in a list with a specific property . In this example I grab the list of id fields of every obj in list objs where obj.id is greater than 100 : How would I do the same in perl ? I think I want to use map , but I do n't know how to conditionally map items from the origin set to the destination set ."
I 'm trying to enable a user to pass in a function name . For some reason it seems that argparse performs the type check/conversion BEFORE it checks the choices . Is this a bug ? Best thing to do ? This throws :
I have a found a Julia function that nicely does the job I need.How can I quickly integrate it to be able to call it from Python ? Suppose the function isWhat is the best and most elegent way to use it from Python ?
"One of my classes requires assignments to be completed in Python , and as an exercise , I 've been making sure my programs work in both Python 2 and Python 3 , using a script like this : One thing I 've been doing is making range work the same in both versions with this piece of code : Is this a bad idea ? EDIT : The reason for this is that xrange and range work differently in Python 2 and Python 3 , and I want my code to do the same thing in both . I could do it the other way around , but making Python 3 work like Python 2 seems stupid , since Python 3 is `` the future '' .Here 's an example of why just using range is n't good enough : I 'm obviously not using the list , but in Python 2 , this will use an insane amount of memory ."
"While playing around with compile ( ) , the marshal module , and exec . I 've encountered some confusing behavior . Consider simple.pyWhen I run this script using exec like thisit gives the expected output.However , when if I introduce compile ( ) , marshal.dump ( ) , and marshal.load ( ) like thisit prints the beginning of the expected output and then errors outWhy does it say that foo is not defined ? In order to understand , I tried using dir ( ) like thisand as expected , it shows that foo is defined.I 've also noticed that when I use dis.dis ( ) on the deserialized code object ( read via marshal.load ( ) ) , the only thing I see is the LOAD_NAME and CALL_FUNCTION for main ( ) , but when I do it with import like thisit gives me the entire disassembly as expected . I 've even looked at some of the code that python uses for compiling and although I think import uses some sort of lookup table for definitions , I 'm not sure what the difference is with compile ( ) that 's causing this behavior ."
"When I open Firefox , then run the command : the url opens in a new tab of Firefox ( same thing happens with Chromium as well ) . Is there some way to replicate this behavior in Python ? For example , calling : then calling : should not start two different processes , but send a message to the currently running program . For example , you could have info in one tabbed dialog box instead of 10 single windows.Adding bounty for anyone who can describe how Firefox/Chromium do this in a cross-platform way ."
"I 'm stumped . The date-cleaning functions I wrote work in Python 2.7.5 on my Mac but not in 2.7.6 on my Ubuntu server.Why does this not work in 2.7.6 on Ubuntu ? Edit : I tried using the timezone offset with the lowercase % z , but still get an error ( although a different one ) :"
"Following up the question from How to update the learning rate in a two layered multi-layered perceptron ? Given the XOR problem : And a simple two layered Multi-Layered Perceptron ( MLP ) with sigmoid activations between them and Mean Square Error ( MSE ) as the loss function/optimization criterionIf we train the model from scratch as such : We get a sharp dive in the loss from epoch 0 and then saturates quickly : But if we train a similar model with pytorch , the training curve has a gradual drop in losses before saturating : What is the difference between the MLP from scratch and the PyTorch code ? Why is it achieving convergence at different point ? Other than the weights initialization , np.random.rand ( ) in the code from scratch and the default torch initialization , I ca n't seem to see a difference in the model . Code for PyTorch :"
Source .
"I have made a function for deleting files : However , when passing a FIFO-filename ( without file-extension ) , this is not accepted by the os-module.Specifically I have a subprocess create a FIFO-file named 'Testpipe'.When calling : It results to False . The file is not in use/open or anything like that . Python runs under Linux.How can you correctly delete a file like that ?"
"When I try to use pyplot from matplotlib : It gives me AttributeError : 'module ' object has no attribute 'pyplot'It can be solved with : But what I am really confused with is , gives me < module 'numpy.random ' from '/Applications/Canopy.app/appdata/canopy-1.0.3.1262.macosx-x86_64/Canopy.app/Contents/lib/python2.7/site-packages/numpy/random/__init__.pyc ' > What is the difference between two cases ? pyplot can not be called in the first example , but random was in the second . I think it 's related with some kind of packages and modules . But I 'm not such a professional to the python , thus asking for an answer ."
"I 'm new to Python and trying to do a nested loop . I have a very large file ( 1.1 million rows ) , and I 'd like to use it to create a file that has each line along with the next N lines , for example with the next 3 lines : Right now I 'm just trying to get the loops working with rownumbers instead of the strings since it 's easier to visualize . I came up with this code , but it 's not behaving how I want it to : Instead of the output I want like above , it 's giving me this : As you can see the inner loop is iterating multiple times for each line in the outer loop . It seems like there should only be one iteration per line in the outer loop . What am I missing ? EDIT : My question was answered below , here is the exact code I ended up using :"
"I have some strange behaviour , at least for me , that is causing me some bugs in my project.I am using Django 1.9 and a third party django package ( django-jet ) that makes usage of field.related_model property in Django admin and sometimes it fails because it expects field.related_model returns a model instance and for some of my models is returning the model name.This is the property defined in Django code : Things that I tried : If Django 's related_model is a @ property instead a @ cached_property it works and returns the model instance.If I call field.remote_field.model instead of field.related_model in the line that is causing the bug it works and returns the model instance.Please , do you have any idea ? I can make the workaround but I would like to know why this behaviour.Thanks in advance !"
"The code below brings the text in the center of x , but i do n't know how to calculate the center for the y coordinate ... it is not ( imgH-h ) /2 ! ( The right y-coordinate is -80 ) screenshot of execution of code"
"Given a Pandas DataFrame with lists stored in several of the columns , is there a simple way to find the column name which contains the longest list for each row ? For example , with this data : I want to identify `` positive '' as the column with the longest list for row 1 and `` negative '' for rows 2 and 3.I thought I could use str.len ( ) to calculate the list lengths and idmax ( ) to get the column names , but ca n't figure out how to combine them ."
"I want to produce tidy data from an Excel file which looks like this , with three levels of `` merged '' headers : Pandas reads the file just fine , with multilevel headers : For repeatability , you can copy-paste this : I want to normalise this so that the level headings are in variable rows , but retain columns a , b and c as columns : Without the multi-level headers , I would do pandas.melt ( df , id_vars= [ ' a ' , ' b ' , ' c ' ] ) to get what I want . pandas.melt ( df ) gives me the three variable columns I want , but obviously does n't retain the a , b , and c columns ."
"I am currently accessing the parent directory of my file using Pathlib as follows : When I print it , and this gives me the following output : The main-folder has a .env file which I want to access and for that I want to join the parent path with the .env . Right now , I did : which works . But I would like to know , if there is a Pathlib alternate to os.path.join ( ) ? Something like :"
"I am trying to solve a problem , using python code , which requires me to add two integers without the use of '+ ' or '- ' operators . I have the following code which works perfectly for two positive numbers : This piece of code works perfectly if input is two positive integers or two negative integers but it fails when one number is positive and other is negative . It goes into infinite loop . Any idea as to why this might be happening ? EDIT : Here is the link discussing the code fix for this ."
"Given the following : I would expect the five tests to get run in parallel running with nosetests -- processes=8 and thus run in approximately one second — however , it takes just over five seconds to run : they appear to be running sequentially and not concurrently.According to the nose documentation , the multiprocess plugin has supported test generators ( as the nose documentation calls them ) since 1.1 : I 'm using nose 1.3.0 so it should be supported . Adding _multiprocess_can_split_ = True does make any difference , as one would expect , as fixtures are not used.How do I get these five tests to run concurrently ?"
My app logs unhandled exceptions . These tests pass OK : But I could n't get a test of unhandled exceptions logging working : What 's wrong here ? How can we actually trigger the app.excepthook from within a test ?
"Say I have an array like thisand I want to create , for each of the items in a , a `` cumsum of the next 4 items '' . That is , my expected output isi.e . a matrix that containsSince the cumsum operation can not be correctly done for the last 3 items , I expect a 0 there . I know how to do a single cumsum . In fact , the arrays arestacked horizontally . However , I do n't know how to do this in an efficient way . What would be the nice vectorized numpy way of doing this ? I 'm also open for scipy packages , as long as they dominate numpy in terms of efficiency or readability ."
"I would like to save array of enums.I have the following : product is an enum.In Django I defined it like this : However , when I write the following : I get the following error : NoteI saw this answer , but I do n't use sqlalchemy and would rather not use it if not needed.EDITEDI tried @ Roman Konoval suggestion below like this : and with : However , I still get the same error , I see that django is translating it to list of strings.if I write the following directly the the psql console : it works just fine"
"I am gathering data from X , Y and Z accelerometer sensors sampled at 200 Hz . The 3 axis are combined into a single signal called 'XYZ_Acc ' . I followed tutorials on how to transform time domain signal into frequency domain using scipy fftpack library.The code I 'm using is the below : Then I plot the frequency vs the magnitudeScreenshot : My difficulty now is how to extract features out of this data , such as Irregularity , Fundamental Frequency , Flux ... Can someone guide me into the right direction ? Update 06/01/2019 - adding more context to my question.I 'm relatively new in machine learning , so any feedback is appreciated . X , Y , Z are linear acceleration signals , sampled at 200 Hz from a smart phone . I 'm trying to detect road anomalies by analysing spectral and temporal statistics . Here 's a sample of the csv file which is being parsed into a pandas dataframe with the timestamp as the index.In answer to 'francis ' , two columns are then added via this code : 'XYZ_Acc_Mag ' is to be used to extract temporal statistics . 'XYZ_Acc ' is to be used to extract spectral statistics.Data 'XYZ_Acc_Mag ' is then re sampled in 0.5 second frequency and temporal stats such as mean , standard-deviation , etc have been extracted in a new dataframe . Pair plots reveal the anomaly shown at time 11:01:35 in the line plot above.Now back to my original question . I 'm re sampling data 'XYZ_Acc ' , also at 0.5 seconds , and obtaining the magnitude array 'fft_mag_values ' . The question is how do I extract temporal features such as Irregularity , Fundamental Frequency , Flux out of it ?"
"I have a setup.py file that checks if the user has setuptools and if he does n't , then I am forced to use distutils . The thing is that , to make sure the submodules are installed , I use setuptools ' find package : and then proceed from there.However , I 'm not sure how to do that with distutils . Is there an equivalent function or do I have to manually look for subdirs that have an __init__.py inside of them ? If that is the case , is it acceptable for me to require setuptools to install my package and just forget about distutils ? Cheers ."
"I run my uwsgi in the simplest way possible , as a process managed by upstart , no emperor mode or anything . This is what I get if application fails on startup for some reason : I have no use for this mode , and would like it to shut down completely , so my upstart job shows as stopped.How can I do this ?"
"I am creating a couple of multi dimensional arrays using NumPy and inititalising them based on the index as follows : The looping is a bit ugly and slow as well . Additionally , if I have a 3-dimensional object then I have to have another nested loop.I was wondering if there is a Pythonic way to generate these values as they are just based on the x , y and z indices . I tried to use the combinatorics bit from itertools , but I could not make it work ."
Is it possible to use Ocaml/Haskell algorithm of type inference to suggest better autocompletions for Python ? The idea is to propose autocompletion for example in the following cases : Are there any good starting points ?
"I am trying to use gridsearchCV with on my keras model , but seem to have ran into a error which i am not sure how to interpret . Here is the model and how i apply it . The model has multiple inputs , 33 in total.These inputs are given by a data_generator , which spits out a list of length 33 with numpy.arrays if shape ( batch_size , 1 , 40,8,3 ) . Could the problem be that it not able to handle list ? or why am i getting this error ? for batch_size = 100"
"Say I have something like this , which sends unhanded exceptions to logging.critical ( ) : It works : However if register_handler ( ) is called multiple times , multiple error_catcher 's are called in a chain , and the logging message appears several times..I can think of a few ways , but none of them are particularly good ( like checking if sys.excepthook is the error_catcher function , or using a `` have_registered '' attribute on the module to avoid double-registering ) Is there a recommended way of doing this ?"
I have a plot which is generated by matplotlib then I save it as .png and then I place it on a PPT file using the pptx module . I want to add the border of the pic in my PPT file can any one please help me for the code.. ? ?
"I have a pandas DataFrame looking like this : I would like to replace every line that does not have `` GE '' in the `` from '' or `` to '' column , by two lines , one having `` GE '' in the `` from '' column and one having `` GE '' in the `` to '' column.In the example above , I would replace the third line by the following two lines : GE VD 1500VS GE 1500I tried using `` apply '' but I ca n't figure out how to return a correct data frame . For exampleGives a strange result : Although my function seems correct : I can think of a way of doing it by filtering my dataframe in two sub dataframes , one with rows needing duplication and one with the others . Then duplicating the first dataframe , making the changes and collating all three DF together . But it 's ugly . Can anyone suggest a more elegant way ? In other words , can the applied function return a DataFrame , as in R we would do with ddply ? Thanks"
"I am working on an algorithm to automatically create character sheets for a roleplaying game . In the game , you have attributes which you put points into to increase them . However , at a certain value it takes 2 points to increase the value of the actual attribute by 1 . You start of with a certain number of points , and each attribute has a value of 1 by defaultI have a program that randomly assigns the points , however I am stuck as to how I then change these values ( that are in a dictionary ) to round down when necessary.For example , if I put 3 points in `` strength '' , thats fine , I get a `` strength '' value of 3 ( including tha base 1 ) . However , if I put 4 points in , I still should only have a value of 4 . It should take 5 points ( plus the base 1 ) in order to get a value of 5 . It then takes another 2 points to get a value of 6 , 3 points to get a value of 7 and 3 points to get a value of 8.The code I am currently using to assign the attibutes looks like this : What I want is just a simple function that takes the dictionary `` attributes '' as a parameter , and converts the number of points each attribute has to the proper value it should be.i.e . `` strength '' : 4 stays as `` strength '' : 4 , but `` wits '' : 6 '' goes down to `` wits '' : 5 '' , and `` intelligence : 9 goes down to `` intelligence : 7 '' .I 'm somewhat new to using dictionaries and so the ways I would normally approach this : Not efficient or pretty but still a solutuion . However , you ca n't just loop over indexes in a dictionary so I am not entirely sure how to go about something like this.General explanation or function would be appreciated ."
"If you try the following code segmentThe phase array will not be masked . The angle function will return values for the whole array , even for the masked out values . Am I doing something wrong here or is this the way it should be ? If so , why ?"
"I 'm using some python scripts to do statistics.One sort content of logs are like this I call it A logs : every A logs has the format of : another logs I call it B logs has the format of : I need to count how many records in the A logs are also in the B logs , and get the time gap of the two records with the same record id.My implementation was load all time and ID of B logs into a map , then iterate the A logs to check if it 's ID was exist in the map.The problem is it casts too much memory cause I have almost 100 million records in B logs.Any suggestion to improve the performance and memory usage ? Thanks ."
I am making a game using pygame and I wanted to add cut scenes to it . The pygame movie module however doesn ’ t work anymore so I had to resort to using moviepy . Moviepy isn ’ t that well documented from what I can see so I ’ m having some trouble figuring it out . I got it to work using this block of code but all I need now is to full screen it ( desired window screen is 640x400 ) . So how would I go about doing so ? Thank you in advance .
"Please look at the following lines of code and the results : where , collectn is a Mongodb collections object . Is there a setting or a way to query so that I obtain the same results for the above two queries ? They are essentially using the same dictionary ( in the sense of d1 == d2 being True ) . I am trying to do the following : before inserting a record into the database I check whether there already exists a record with the exact value combination that is being added . If so , then I do n't want to make a new record . But because of the above shown behavior it becomes possible to get that the record does not exist even when it does and a duplicate record is added to the database ( of course , with different _id but all other values are the same , and I would prefer not to have that ) .Thank you in advance for your help ."
"On reading through some code , I came across the below snippet which I am not able to understand . Would anyone be able to guide/provide hints/link or a basic explanation of line 3 belowmainly , what is the following doing ?"
"GOAL : spawn a few greenlet worker deal with the data pop from redis ( pop from redis and then put into queue ) RUNNING ENV : ubuntu 12.04PYTHON VER : 2.7GEVENT VER : 1.0 RC2REDIS VER:2.6.5REDIS-PY VER:2.7.1Try to pop data from redis , but it blocked..and no exception raised ... just freezeand remove spawn method , it will worked..i feel confuse what happened , plz help ! thk u !"
"The following code sends a request every 200ms and should handle the responses asynchronously whenever they come.Over HTTP it works as expected - a request is sent every 200ms and the response callback is called independently whenever a response arrives . Over HTTPS however , the requests get significantly delayed whenever a response arrives ( even though my response handler does no work ) . The response callback seems to be called twice for each request , once with a zero-length response ( edit : this is because of a redirect and seems unrelated to the blocking issue , thanks Padraic ) .What could be causing this blocking behaviour over HTTPS ? ( www.bbc.co.uk is just an example that 's geographically far from me , but it happens with all servers I 've tested ) .grequests_test.py $ ipython2 grequests_test.py 'http : //www.bbc.co.uk ' ( expected result ) ipython2 grequests_test.py 'https : //www.bbc.co.uk ' ( requests are sent late ) Notice that the first response appears to arrive long after the next request should have been sent but was n't . Why did n't the sleep return , and the next request get sent , before that first response arrived ?"
"I have a list of of tuples that represent different times I want to return the max from the list , after some searching I realized I could use the key in max to search by the AM or PM first.print ( max ( timeList , key = operator.itemgetter ( 2 ) ) ) When I run this however , I 'm getting the wrong max ( ' 4 ' , '12 ' , 'PM ' ) I thought about it , and not only does it not make sense , given that 8:23 should be max , but I also realized that 12:48 would probably return max since it 's a PM and also technically greater than 8 in my search.That being said , how might I get this max to find the latest possible time , given formatting of the list can not be changed ."
In C or C++ I know that there is something called undefined behaviourIn expression evaluation when some of the the expressions have side-effects . let 's say I want to calculate the following : but at some point g makes c = 5 . ( c is a glob variable ) What would be the behavior in python ? Would it be undefined as C ?
"I am writing a function to split numbers and some other things from text in python . The code looks something like this : Now , this code works perfectly fine in python3 , but it does not work under python2 and get an `` unmatched group '' error . The problem is , I need to support both versions , and I could not get it to work properly in python2 although I tried various other ways.I am curious what could be the root of this problem , and is there any workaround for it ?"
"Suppose I got a list [ 2 , 4 , 1 , 3 , 5 ] .I want to sort the list just from index 1 to the end , which gives me [ 2 , 1 , 3 , 4 , 5 ] How can I do it with python3 ? ( No extra spaces would be appreciated ) EDITED : I 've state slicing as the accepted answer . Python copies only object references , so the speed penalty wo n't be that huge compared to a real in-place sort . But if you require a very strict solution , you can refer to one of my favorite answer from @ Heap Overflow"
"I have a non-seekable file-like object . In particular it is a file of indeterminate size coming from an HTTP request.I am streaming this file to a call to an Amazon AWS SDK function which is writing the contents to Amazon S3 . This is working fine.However , at the same time as streaming it to S3 I want to also stream the data to another process . This other process may not need the entire stream and might stop listening at some point ; this is fine and should not affect the stream to S3.It 's important I do n't use too much memory , since some of these files could be enormous . I do n't want to write anything to disk for the same reason.I do n't mind if either sink is slowed down due to the other being slow , as long as S3 eventually gets the entire file , and the data goes to both sinks ( rather , to each one which still wants it ) .What 's the best way to go about this in Python ( 3 ) ? I know I ca n't just pass the same file object to both sinks , such asI think I could write a wrapper for the file-like object which passes any data read not only to the caller ( which would be the S3 sink ) but also to the other process . Something likeBut is there a better way to do it ? Perhaps something like"
"I would like to read through a file and capitalize the first letters in a string using Python , but some of the strings may contain numbers first . Specifically the file might look like this : I would like this to be : I have tried the following , but this only capitalizes if the letter is in the first position.Any suggestions ? : - )"
"I am confused using the new random.choices in Python 3.6.Here is the doc : -- Return a k sized list of elements chosen from the population with replacement . If the population is empty , raises IndexError.They give an example : weights= [ 10 , 5 , 30 , 5 ] and I do n't know what this means . Why do n't they sum to 100 ? If my population is [ 1 , 2 , 3 , 4 ] -- does this mean that a choice of '10 ' occurs with probability 0.1 ?"
"I have a list of elements in python . I do n't know the number of elements in the list . I would like to add indexes to the list.In Haskell , I could do the followingNow imagine that the string was of unknown size . This would still work in Haskell , and the integer list gives as many integers as necessary until the string runs out . How would one do the equivalent in Python ? I have tried this : And it works , but I 'm wondering if there is a shorter/cleaner way , since it is 4 lines of code and feels much ."
"If I have a src directory setup like this : Can you tell me the best way to import pkg1.util from main.py and from test.py ? Thanks ! ( If I need to have another __init__.py file in the root directory , let me know ? )"
"I tested both SublimeText 2 and 3 and both are buggy : If you test this code , you 'll notice all code right after the : will not be syntax highlighted properly.I found some links explaining how to add your own syntax highlighting rules but I did n't find how to modify those already implemented in a attempt to fix them.EDIT : Now knowing where to modify default syntax highlighting rules thanks to MattDMo , I tried to change line 385 of my fileto But it did n't work ."
"eg : What is a good hash function to generate equal hashes for both dicts ? The dictionaries willhave basic datatypes like int , list , dict , and strings , no other objects.It would be great if the hash is space optimized , the target set is around 5 million objects , hence collision chances are pretty less.I am not sure if json.dumps or other serializations pay respect to equality instead of structure of the members in the dictionary.eg . Basic hashing using str of dict does not work : json.dumps does not work either :"
"Consider the following test code , which compares a mock run result with an expected result . The value of the run result depends on a value of a parameterized fixture paramfixture , which provides two values , so there are two possible variants of the run result . Since they are all session fixtures we should expect the run_result fixture execute only two times.Now , please take a look at the test case test_run_result , which receives the run_result and expected_result fixtures to compare , and also receives the tolerance fixture , which is parameterized with two values . The test case checks if the difference between expected and resulted falls within the tolerance . Note that the run does not depend on the tolerance.For some reason , which I don ’ t understand Pytest executes the run_result ( ) fixture three times . Can you explain why ? This was tested using pytest vers . 2.9.1By the way , the run_result fixture would execute only two times if the test case were n't parameterized or were parameterized using a decoractor instead of a fixture i.e . : @ pytest.mark.parametrize ( 'tolerance ' , [ 1e-8 , 1e-11 ] ) .Pytest screenshot :"
"I am trying to use the pandas.DataFrame.query ( ) function as follows : The code works with positive numbers , but when negative numbers are passed to the string , as above , it returns the following error : Any suggestions on how to use negative numbers in DataFrame query ( ) expressions ? Thank you ! !"
"I have a dataframe like this I want to get where a , b , c are column names , and I get the values counting ' 1 ' in all columns when the filter is ' 1 ' in another column.For ample , when df.a == 1 , we count a = 2 , b =1 , c = 0 etc I made a loop to solveBut I think that there is a simpler solution , is n't it ?"
"I have an array x : and I want y : where the first row is x-1 , the second row is x , and the third row is x+1 . All even column indices are zero.I 'm doing : I was thinking there might be a one-liner to do this instead of 4 ."
"Before I have the audacity to file a bug report , I thought I 'd check my assumptions among wiser Pythonistas here . I encountered a baffling case today , so I whittled it down to a toy example , shown below : I thought function arguments were local in scope to the function , and were garbage-collected at the end of a function call , never to retain state between them . I have tested the above script on Python 2.5.2 and Python 2.6.1 , though , and my understanding does not the results . Argument a certainly retains state between most of these calls ; the most perplexing one being the final call to bleedscope , where it skips the state of the previous call and goes back to the state at the end of the second ( i.e. , [ 1 , 2 ] ) . [ I suggest running this in your favorite debugger to see for yourself . If you do n't have one , I suggest Winpdb as a solid FOSS standalone Python debugger . ] What 's going on here ?"
"I have a class method that I am trying to test that requires two patched methods , ConfigB.__init__ and listdir : I have the following test set up : Now the patched mock and return value works for ConfigB.ConfigB , but it does not work for os.listdir . When the print ( listdir ( ) ) method runs I get a list of file in the current directory , not a value of None as I specified in the patched return value . Any idea what is going wrong ?"
"I 'm using django-model-utils inheritance manager to query a data model with multi-table inheritance and it mostly works great ! What does n't work great is when I want to .select_subclasses ( ) but also filter for a specific subclass . For example : I would love to be able to just get ONLY all of the Cucumber objects related to a specific Salad without any Carrot objects . Unfortunately , the docs do n't appear to explain this . Is my best option to create a type field on the Vegetable class that I set when saving any Vegetable object that would then be available for `` regular '' filtering ? Thanks in advance !"
I had a bug where I was relying on methods being equal to each other when using is . It turns out that 's not the case : Why is that the case ? It works for regular functions :
"I am creating a full-frame ( no decorations ) window with code like this ( in python 3.2 using tkinter ) : When I try to open a file dialog or message box , they appear UNDER the full frame window . I can verify this by calling a withdraw ( ) on the main window before I open one of the dialogs . For example , On windows I do n't have a problem with this , only on fedora 14 and ubuntu 12.04 have I noticed it . ( I have n't tested on Mac ) . I 'm passing a parent to the dialogs but they do n't seem to be paying attention . Can someone help me understand what I 'm doing wrong ? Thanks ."
"In OpenCV 3.4.2 the option to return the number of votes ( accumulator value ) for each line returned by HoughLines ( ) was added . In python this seems to be supported as well as read in the python docstring of my OpenCV installation : `` Each line is represented by a 2 or 3 element vector ( ρ , θ ) or ( ρ , θ , votes ) . `` It is also included in the docs ( with some broken formatting ) .However I can find no way to return the 3 element option ( ρ , θ , votes ) in python.Here is code demonstrating the problem : outputsThe desired behavior is an extra column with the amount of votes each line received . With the vote values more advanced options than the standard thresholding can be applied , as such it has been often requested and asked about on SE ( here , here , here and here ) with sometimes the equivalent for HoughCircles ( ) . However both the questions and answers ( such as modifying source and recompiling ) are from before it was added officially , and therefore do not apply to the current situation ."
"The Python standard library and other libraries I use ( e.g . PyQt ) sometimes use exceptions for non-error conditions . Look at the following except of the function os.get_exec_path ( ) . It uses multiple try statements to catch exceptions that are thrown while trying to find some environment data.These exceptions do not signify an error and are thrown under normal conditions . When using exception breakpoints for one of these exceptions , the debugger will also break in these library functions.Is there a way in PyCharm or in Python in general to have the debugger not break on exceptions that are thrown and caught inside a library without any involvement of my code ?"
"I used django_openid_auth on my project and it worked quite alright for some time.But today , I tested the application and came across this exception : I 'm not sure what other info should I provide , but I do have my auth.py here : and also urls.py contains these two lines : Then again , upon request for logging in , the exception just pops out . And if that helps , the only thing changed is I started using django 1.6 for my site.could anyone advise ?"
"Suppose we try to access a non-existing attribute : Python ’ s AttributeError only has the attribute args with a string containing the finished error message : 'dict ' object has no attribute 'gte'Using the inspect and/or traceback modules with sys.last_traceback , is there a way to get hold of the actual dict object ? Edit : since the cat is out of the bag anyway , I ’ ll share my findings and code ( please don ’ t solve this and submit to PyPI , please ; ) ) The AttributeError is created here , which shows that there ’ s clearly no reference to the originating object attached.Here the code with the same placeholder function :"
"I saw some code where __del__ was being called explicitly on an object which I found curious , so I tried to play with it a bit to understand how it works.I tried the following code ( I understand using __del__ in and of itself is potentially bad , but I 'm just trying to edify myself here ) : and then tried ( in the IPython console ) the following : I see that __del__ is only called once even though the instance a of A has been kept alive by the instance f of Foo , and when I set the latter to None , I do n't see the destructor being called the second time.The Python docs here say : Note that it is possible ( though not recommended ! ) for the __del__ ( ) method to postpone destruction of the instance by creating a new reference to it . It may then be called at a later time when this new reference is deleted . It is not guaranteed that __del__ ( ) methods are called for objects that still exist when the interpreter exits.This seems to imply that the __del__method may be called again , though there is no guarantee it will . So my question is : is there some circumstance where __del__ will be called again ? ( I thought setting f to None above would do it , but it did n't ) . Any other nuances worth noting ?"
I am attempting to pass a bash environment variable back into my fabric function like this : -But it does n't seem to be able to retrieve the stdout results and assign it to my python project_home variable . What 's the correct way to do this right ?
"I 'm using libprotoc 3.2.0 with Python 3 in a Python package . If I try to the Protobuff Python files ( denoted with the extensions _pb2.py ) in to their own folder , with the .proto files , let 's say protobufs and then try to import them a python file as follows : I get import errors that b ca n't find a . If I output the _pb2.py files into the root of my package , I do n't have this problem . This is elaborated upon in this issue , but I 'm not sure if I 'm having the exact same issue or not . Is is possible to avoid outputting the _pb2.py files at the root of my package ? Non-Toy ExampleWhat I actually have is two protobuffs that reference each-other , as follows in the .proto file : In the Python file , this is translated to : But because the file is in a folder called protobufs in a Python package , the last line should be import protobufs.common_pb2 as common__pb2 and not import common_pb2 as common__pb2 , but libprotoc does n't know how to take the folder into account ."
"I am trying to use the AWS SAM CLI installed through Homebrew and I am seeing the following error when I try to use sam with any command : Looking at the .Python file referenced in the error , it is symlinked to a python folder that does n't actually exist : I do not have a 3.7 folder at that location , but I do have a 3.8 folder . That said , I am not sure what is the origin of this folder . My Python3 installation is from Homebrew and located in the Cellar as usual ( ../Cellar/python @ 3.8/3.8.3_1/bin/python3 ) and symlinked to /usr/local/bin/python3 . Not sure if that is relevant but I figure more info ca n't hurt.I tried symlinking the .Python file to the 3.8 version I do have at that location but it only produced other errors.Any idea how I can get this CLI working ?"
"I have a large dataframe ( 5000 x 12039 ) and I want to get the column name that matches a numpy array.For example , if I have the tableI want to do this : and have find_column ( x ) return m1lenmin"
"Using mock_open , I can capture the data from writes using the with [ ... ] as construct . However , testing that what I have is correct is a little tricky . For example , I can do this : But I want to do compare what I think should have been written to what was . In effect something like this : The problem I am facing with call is that different versions of json ( 2.0.9 vs 1.9 ) seem to print things differently . No , I can not just update to the latest json.The actual error I am getting is this : In effects , the calls are different but the end result is the same . The code I am testing is fairly simple : So , creating another method that passes the file pointer seems overkill ."
"I have a Numpy array and a list of indices , as well as an array with the values which need to go into these indices.The quickest way I know how to achieve this is : Notice I had to make a copy of a1 . What I 'm using this for is to wrap a function which takes an array as input , so I can give it to an optimizer which will vary some of those elements.So , ideally , I 'd like to have something which returns a modified copy of the original , in one line , that works like this : The reason for that is that I need to apply it like so : With a1 and ind constant , that would then give me a costfunction which is only a function of x.My current fallback solution is to define a small function myself : ... but this appears not very elegant to me.= > Is there a way to turn that into a lambda function ?"
"I 'm trying to maximize a utility function by finding the optimal N units a person would use . One of the constraints is that they have finite money , m. So I 'm trying to set up a constraint where array of length 3 , N times the prices , P also an array of length 3 , can not be greater than m. Like the example as follows : For this optimization , P is given based on a previous optimization . Now here 's my actual code : The function : The problem is I still get negative m ! So clearly I 'm not passing the constraint properly . I 'm guessing that it 's because P is n't used properly ? Output : What I 've tried : I 've also tried passing P in args , like so : But it tells me ` Lambda wants 2-arguments and received 4** Update : **using ( F , ) in 'args ' does not allow the program to run without raising an error , however the constraint still fails to hold up.Also , nan is returned after m is defined as a negative value , which of course throws the whole scipy optimization out of wack . ** Full project code : **"
I see that I can append to a list while iterating over itAm I allowed to make use of this behavior ? or is it discouraged ? I note that the same can not be said for set Error : size changed during iteration .
"I 've written a script in python 's scrapy to make a proxied requests using either of the newly generated proxies by get_proxies ( ) method . I used requests module to fetch the proxies in order to reuse them in the script . However , the problem is the proxy my script chooses to use may not be the good one always so sometimes it does n't fetch valid response . How can I let my script keep trying with different proxies until there is a valid response ? My script so far : PS My intension is to seek any solution the way I 've started here ."
"I am attempting to measure a section of code that I have `` parallelized '' using Python 's multiprocessing package , particularly , using the Process function.I have two functions that I want to run in parallel : function1 and function2 . function1 does not return a value , and function2 does . The return value from function2 is a fairly large class instance.Here is my existing code for parallelizing and getting the return value using a queue : So , here is the issue I am seeing . If I remove the call to result = q.get ( ) , the time it takes to execute the Wrapper function decreases significantly , as it is not returning the class from function2 , however I obviously do n't get the data I need out of the function . The run time increases significantly if I put it back in , thereby showing that parallelizing actually takes longer than sequentially executing these two functions.Here are some mean execution times for Wrapper , for reference : Sequential code ( i.e. , function1 ( timestep ) , res = function2 ( timestep , a1 , a2 , a3 , ... , None ) ) : 10 secondsParallelized code without using a Queue : 8 secondsParallelized code with the Queue : 60 secondsMy goal with this code is to show how parallelizing a section of code can improve the time required for execution in needlessly parallel functions . For reference , I am using the cProfile package , generating a profile of my code , and looking at the time required for Wrapper to run.I am starting to get frustrated with this whole process . It is intended to basically speed up parts of my program that I 've added to an existing , custom framework developed in-house , however I ca n't physically show that I 'm not adding too much overhead . If I look at overall execution time of the program , the parallelized code runs much faster . However , when I dig a bit deeper my parallelized code begins to appear to take longer . Now , my thought was that the Queue was doing some kind of deep copy operation , however I could n't find a reference to state that fact , so I assume that it is returning a shallow copy , which , to me , should n't require such overhead ."
"Is there a way to tidy-up the following code , rather than a series of nested try/except statements ?"
"I 'm testing a function that writes to a logfile ( it does n't matter that it writes to a logfile specifically , it could be doing anything , it 's just what gave rise to this question ) Something like this : And I can test it a bit like this : For debugging purposes , when a test fails , I want to be able to print out the new contens of the logs -- I can make a fixture a bit like this : Great - now I can check the log contents at any point in my tests : Hm - ah , but that second print is n't going to work , it 's after the test failure.What if my test fixture just always printed the logs out at the end of the test ? Then pytest 's stdout capture would show it to me when it fails , but not when it passes ! Oh , but that does n't work because pytests 's log capture is n't happening during test finalizers.So the question is : how can I make a test finalizer that can print stuff ? Here 's an ultra-minimal gist without the ( irrelevant ) writing-to-a-logfile stuff : https : //gist.github.com/hjwp/5154ec40a476a5c01ba6"
"I know you can do something like this in python : but how would I do something like this ? That is , I do n't want to substitute the 1 for another number . I want to simply omit it if conditional is false ."
"One of the great things of python is the ability to have introspection on methods and functions . As an example , to get the function signature of math.log you can ( in ipython ) run this : And see that x and optionally base are the parameters of this function.With the new gtk3 and the automaticall generated pygobject bindings , I can in all examples I tried only ever get ( *args , **kwargs ) as the parameters of every gtk method . Example : Label.set_text which requires a string : NOW THE QUESTION : is this ( the loss of method introspection for python bindings ) something that will change once more ( documentation ) effort has gone into pygobjects or is this something that is here to stay due to the way pygobjects works ?"
I have been reading about repr in Python . I was wondering what the application of the output of repr is . e.g.When would one be interested in ' < class __main__.A at 0x6f570 > ' or ' < __main__.A instance at 0x74d78 > ' ?
"There are plenty answers for how to access static variables from static methods ( like this one , and that one , and the great info on the subject here ) , however I 'm having trouble with the other direction : How can you use static methods to initialize static variables . E.g . : The last line gives an exception : NameError : name ' A ' is not defined . The same error happens if I use @ classmethod instead of @ staticmethod.Is it possible to access a static method from a class variable ?"
"I am having a problem similar to this user : when calling autocomplete on df.col. , I get no autocompletion even after evaluating a cell containing only df.col . For instance , I 'd like to see df.col.str.matc to autocomplete to df.col.str.match . What can I do to solve this ? Take as example the following dataframe : I do not want to try hinterland since I only want autocomplete upon pressing tab.Thanks a lot in advance !"
"I want to calculate x ln x with arbitrarily small positive x , or x = 0 without underflow or division by zero . How do I go about doing it ? I have googled `` python numpy xlnx OR xlogx '' with no meaningful result.Edit to add : It was another issue causing my problem . But what is the best way to calculate xlogx ? The straightforward method causes nans when x = 0 ."
"For example , define Which gives below display in Jupyter notebookMy question is that is it possible to add a column delimiter to the dataframe likeThank you for all the answers , currently I use below custom functionsand gives"
"I 'm new to SymPy and python and I was faced with a problem.I 'm trying to solve a system 'kunSys ' : With a list of variables 'lagVars ' : As you can see , my system contains both eqations and inequalities.Trying : Get : But it works fine when solving eqations and inequalities separately : But this is not a wanted result.I tried to use solveset ( ) but it does n't seem to work also.I googled a lot , but failed to find the answer.Question : How do I solve this system ?"
"I have a string in which I want to replace some variables , but in different steps , something like : But when I try to replace the first variable I get an Error : How can I accomplish this ? Edit : I want to create a new list : this is the desired output : But using .format instead of .replace ."
"I will refer to this explanation and this workaround : So what I am doing is : The problem is that , while the menu is notified that it must stop , and will do that soon , it can not do it now since it is stucked in a raw_input : So , since twisted is removing the default interrupt handler , raw_input is not stopped . I still need to press enter after ^C for it to stop.How can I force the raw_input to be stopped , without installing the default interrupt handler , which is a source of problems in a twisted context ( since twisted itself is not expecting to be interrupted ) I guess the problem is not related to raw_input only : any function taking an unbounded time ( or longer than a pre-set limit ) , should be interrupted somehow.Is there an accepted twisted pattern for this ? EDITThis is the full test code : Which I can run in two different ways . Normal way : Pressing ^C stops the program immediately : As expected.Twisted way : Pressing ^C will produce : But askUser is actually not interrupted : I still need to press enter for raw_input to finish ."
"In the Python docs , it states : Application developers should typically use the high-level asyncio functions , such as asyncio.run ( ) , and should rarely need to reference the loop object or call its methods . This section is intended mostly for authors of lower-level code , libraries , and frameworks , who need finer control over the event loop behavior.When using both async and a threadpoolexecutor , as show in the example code ( from the docs ) : Do I need to call loop.close ( ) , or would the asyncio.run ( ) close the loop for me ? Is using both asyncio and threadpoolexecutor together , one of those situations where you need finer control over the event loop ? Can using both , asyncio and threadpoolexecutor together be done without referencing the loop ?"
I am using APScheduler to run a python method in every 5 minutes . It 's working perfectly . I need to limit the scheduled job iteration to 100 . After the 100 iteration it should close the process . I have checked the reference document but I am unable to find any option which provide this feature . We have option to control number of job instance but not the max iteration . Does anyone has idea about that ? ORif I get the scheduled job iteration count then I can also remove the running job from code itself like below .
"I am having a fixture in conftest.pySimilarly there are many other fixures in the conftest.py with scope as session , moduleMy test cases looks like thistest.pyProblem is I want my fixture that is there in conftest.py to run before yield of fixture that is in my testcaseHow can i change order in the way fixture are executed"
"I 'm facing an issue with a Python script that connects to a mirrored MS SQL server DB . It 's throwing a segmentation fault when I try connecting to the DB for the second time . Both the app server and the DB instances are running on Google Compute Engine.Here 's some code replicating the issue : The first connection ( c1 ) succeeds , but the second connection ( c2 ) fails immediately with segfault . `` mydb '' is mirrored to a second server ( sql-server-02 ) .Using a non-mirrored DB , or disabling mirroring for this DB , makes it go away.We have tried upgrading several libs , and that did n't fix the issue.Versions : Microsoft SQL Server : 12.00.2000 ( latest ) Python : 2.7.6pyodbc : 3.0.10 ( latest ) unixODBC : 2.2.14p2-5ubuntu5 , 2.3.0 , 2.3.4 ( latest ) MS ODBC driver for RedHat : 11.0.1790.0 , 11.0.2270.0 ( latest ) To add here , Java code performing the same steps works fine.Any ideas ?"
"I 'm curious about one of the recommendations in the Google Python style guide concerning properties . In it , they give the following example : I have two questions about this : What is the benefit of using the three underscore `` indirection '' ___get_area and ___set_area as well as two underscore ones , over just using the two underscore ones directly ? Why use property ( ) as a method with this set of double and triple underscore methods , rather than doing something like :"
"I can remove the docstrings if I invoke cython manually , eg : but when I try to use the setup utility , I am unable to pass that -D parameter along . setup.pyThis produces a library that has all doc strings available , compared to when running with -D ."
"I 'm a noob with Python , but I 've written an auto-close function like this..I have three classes that have a Close ( ) method that this function can be used with . Is this the most Pythonic solution ? Should I be doing something in the classes themselves instead ?"
"I 'm a python beginner and I 'm in my first days playing with my own python scripts and projects such as django.I use Ubuntu and I set my PYTHONPATH asWhen I run into a python interpreterI can see [ `` , '/usr/bin ' , '/usr/local/lib/python2.6/dist-packages/ropemode-0.1_rc2-py2.6.egg ' , '/usr/local/lib/python2.6/dist-packages/rope-0.9.3-py2.6.egg ' , '/usr/local/lib/python2.6/dist-packages/ropevim-0.3_rc-py2.6.egg ' , '/usr/lib/python2.6 ' , '/home/jaume/Projects/mypython ' , '/usr/lib/python2.6/plat-linux2 ' , '/usr/lib/python2.6/lib-tk ' , '/usr/lib/python2.6/lib-old ' , '/usr/lib/python2.6/lib-dynload ' , '/usr/local/lib/python2.6/dist-packages ' , '/usr/lib/python2.6/dist-packages ' , '/usr/lib/python2.6/dist-packages/PIL ' , '/usr/lib/python2.6/dist-packages/gst-0.10 ' , '/usr/lib/pymodules/python2.6 ' , '/usr/lib/python2.6/dist-packages/gtk-2.0 ' , '/usr/lib/pymodules/python2.6/gtk-2.0 ' , '/usr/lib/python2.6/dist-packages/wx-2.8-gtk2-unicode ' , '/usr/lib/pymodules/python2.6/IPython/Extensions ' , u'/home/jaume/.ipython ' ] How does python load all the non-PYTHONPATH paths ? Is there any index ? Moreover , I have my django installed at /usr/lib/pymodules and , even it 's not in the sys.path list , it runs.Many thanks ."
"I 'm trying to parse a line with pyparsing . This line is composed of a number of ( key , values ) . What I 'd like to get is a list of ( key , values ) . A simple example : should result in something like : [ ( 'ids ' , '12 ' ) , ( 'fields ' , 'name ' ) ] A more complex example : should result in something like : [ ( 'ids ' , '12 , 13 , 14 ' ) , ( 'fields ' , 'name , title ' ) ] PS : the tuple inside the resulting list is just an example . It could be a dict or another list or whatever , it 's not that important.But whatever I 've tried up to now I get results like : [ ( 'ids ' , '12 fields ' ) ] Pyparsing is eating the next key , considering it 's also part of the value.Here is a sample code : Can someone help me ? Thanks ."
"For the following set of models ( Foo , Bar ) , you could impose a cross-validation rule like the one in the Bar.clean of the following code snippet up to django 1.7.The same snippet throws a RelatedObjectDoesNotExist error in django 1.8.3.What is the new and improved way of achieving the same result in django 1.8.3 ? ( I 've included the admin.py code just to show how these models are being used . ) models.pyadmin.py"
I wonder how to get out from loop like this : After task is completed all loops and computings are continued . They have to be broke but I do n't know how - single break statement after `` task completed '' will end only innermost for loop but it will be invoked again multiple times - so we gain nothing.In C I would do a=b=c=d=95 but in python that would n't work . Of course I can use while loop instead but then I have to use X+=1 statements and it would look awful.Any help ? About the loop : I use it to break a 4-char password using brute-force . It is n't a real purpose - used only for tests .
"This is pretty strange ( admitedly , this is my first attempt with python / sqlite ) , but I can seem to get all of the rows if I do a fetchAll ( ) , but other than that - no matter what I try , always ends up in the db only returning the first row - the second iteration stops because a null is returned . Wondering if there is something wrong with how I am coding this up in python ? The db seems ok.."
"I have a tableA view : And a template : It correctly gets the object pk and everything , but does n't send just the one single object to populate the table . I know it gets the object because object.id and object.nickname both display correctly . I know it is possible to display only a specific object because I have another app in the same project that displays only one object if you click the link that takes you to DetailView ( the template of which I borrowed to re-create with my Account models ) . But it will only display a table of ALL the objects.I can put up request data if necessary . I can promise you I have seen the object in the template context , and indeed it must be or object.id would not work . What 's the trick for django-tables2 ? Apparently I already did it once !"
"After asking a question about sending “ 304 Not Modified ” for images stored in the in the Google App Engine datastore , I now have a question about Cache-Control.My app now sends Last-Modified and Etag , but by default GAE alsto sends Cache-Control : no-cache . According to this page : The “ no-cache ” directive , according to the RFC , tells the browser that it should revalidate with the server before serving the page from the cache . [ ... ] In practice , IE and Firefox have started treating the no-cache directive as if it instructs the browser not to even cache the page.As I DO want browsers to cache the image , I 've added the following line to my code : According to the same page as before : The “ cache-control : public ” directive [ ... ] tells the browser and proxies [ ... ] that the page may be cached . This is good for non-sensitive pages , as caching improves performance.The question is if this could be harmful to the application in some way ? Would it be best to send Cache-Control : must-revalidate to `` force '' the browser to revalidate ( I suppose that is the behavior that was originally the reason behind sending Cache-Control : no-cache ) This directive insists that the browser must revalidate the page against the server before serving it from cache . Note that it implicitly lets the browser cache the page ."
"I 'm struggling on how to setup my environment for TDD with Google App Engine - Python . ( I 'm also pretty new to Python ) . My environment is as follows : Google App Engine 1.5.0IntelliJ 10.2IntelliJ setup to use Python 2.5.4 for this projectI am using IntelliJ with the Python plugin , so running unittests is as simple as hitting ctrl-shft-f10 . I 've also read the documentation on testbed and have successfully tested the datastore and memcache . However , where I am stuck is how do i unittest my RequestHandlers . I 've scanned a lot of articles on Google and most of them seem to be pre merging of gaetestbed into gae as testbed.In the following code sample , i would like to know how to write a unit test ( which is runnable in intellij ) which tests that a call to '/ ' returns - > Home PageFrom Nick Johnson 's answer below , I added a new folder named test and added a file to that folder called unit_test.py . To that file I added the following code ( Modified from Kris ' answer below ) : And it now works !"
"This is super handy for some problems : But what if I do n't know what order to expect ahead of time ? [ update ] Eg , say I have an input variable containing some unknown order of characters and it just so happens that ' b ' comes after ' i ' . I want to still be able to reference the groups for '.b . ' and '.i . ' without having to order my regex according to their order in the input var . So , I wish I could do something like this but I do n't know if it 's possible : [ end update ] I 've searched around and racked my brain a bunch but ca n't generate any good leads . Guessing this functionality would n't exist because probably the only way for re to do this is to scan the entire string once for each group ( which of course I could do in a loop instead ) but I thought I 'd see what the stackoverflow brain had to say about it.Thanks for your help , Josh"
Assume we have a simple Python dictionary : Which is the better way to copy this dictionary ? Is there a compelling reason to favour one approach over the other ?
"UPDATE : example now lists desired results ( boldfaced below ) I find myself writing lots of functions that search through some data , where I want to let the caller specify behaviours when matches are found : they might print something out or add it to one of their data structures , but it 's also highly desirable to be able to optionally return found data for further transmission , storage or processing.ExampleFirst client usage : results should yield 4/2 , 5/2 , then 6/2 ... i.e . 2 , 2 , 3.Second client usage : should print 1 2 3 4 5 6 ( separate lines ) but yield nothingBut , the yield does n't create a generator in `` results '' ( at least with python 2.6.6 which I 'm stuck with ) .What I 've triedI 've been hacking this up , often like this ... ... or sometimes , when the list of visitor parameters is a pain to type out too many times ... The Issues / QuestionThese `` solutions '' are not only clumsy - needing explicit built-in support from the `` find '' routine - they remove sentinel values from the set of results the visitor can yield back to the top-level caller ... Are there better alternatives in terms of concision , intuitiveness , flexibility , elegance etc ? Thanks !"
"I 'm doing some work whereby I have to load an manipulate CT images in a format called the Analyze 7.5 file format.Part of this manipulation - which takes absolutely ages with large images - is loading the raw binary data to a numpy array and reshaping it to the correct dimensions . Here is an example : I know numpy can do reshaping of arrays quickly , but I ca n't figure out the correct combination of transformations needed to replicate the effect of the nested loops . Is there a way to replicate that strange indexing with some combination of numpy.reshape/numpy.ravel etc ?"
"In my application , I have Users and Posts as models . Each post has a foreign key to a username . When I create a ModelView on top of my Posts model I can create posts as specific users in the admin interface as seen in the screenshot belowAfter I have added a post and click `` Save and Add Another '' , the `` User '' reverts back to `` user1 '' . How can I make the form remember the previous value `` user2 '' ? My reserach has led me to believe it can be done by modifying on_model_change and on_form_prefill , and saving the previous value in the flask session , but it seems to be overengineering such a simple task . There must be a simpler way.My code can be seen below"
"I am trying to create a Python plugin which will set the rating of the currently playing song in Rhythmbox 2.96 . It appears that Rhythmbox 2.96 does n't let you use the API ( Python modules ) to set the rating of a song anymore ; player related actions have been dropped in favor of MPRIS.I then tried looking at using dbus with MPRIS but MPRIS does n't have a spec for setting the rating of a song either . After a lot of digging , I found this sample in the Rhythmbox codebase and adapted it into a test script . It works , but the SetEntryProperties method is causing Rhythmbox to freeze for about 30 seconds . Here is the Python script . Instructions : Copy the code into a file called rate.pyStart rhythmbox from a terminal using In Rhythmbox , enable the Python Console from the pluginsStart the Python Console and runYou will see the print output in the terminal and Rhythmbox freezes for about 20-30 seconds.The exception that gets printed is : My knowledge of Python/dbus is limited , so I do n't understand why that error is occurring . I 'd appreciate any help with it.Also , if you know of a better way of setting the rating of a song in Rhythmbox through code , it would be welcome too ! I am using Ubuntu 12.04 , if it makes a difference ."
"Here is a demonstration of the issue I 'm having . I 'm fairly new to Python , so I 'm pretty sure I 'm overlooking something very obvious.How do I get around this issue so I can use an arbitrary list of indices specified as an argument at program start ?"
"I think many people have seen the python 's function which receives default parameters . For example : If we call this function using foo ( ) , the output will append integer ' 3 ' each time after the call.When this function is defined , a function object named 'foo ' is defined in the current environment , and also the default parameter values are evaluated at this time . Every time when the function is called without a parameter , the evaluated parameter value will be changed according to the code.My question is , where is this evaluated parameter exist ? Is it in the function object or it is in the method object when calling the function ? Since everything in python is a object , there must be some place to hold the name- > value binding of ' a ' -- > evaluated parameter . Am I over-thinking this problem ?"
"I would like to have a function get_permutation that , given a list l and an index i , returns a permutation of l such that the permutations are unique for all i bigger than 0 and lower than n ! ( where n = len ( l ) ) .I.e . get_permutation ( l , i ) ! = get_permutation ( l , j ) if i ! =j for all i , j s.t . 0 < = i and j < len ( l ) ! ) .Moreover , this function has to run in O ( n ) . For example , this function would comply the with the requirements , if it were n't for the exponential order : Does anyone has a solution for the above described problem ? EDIT : I want the permutation from the index NOT the index from the permutation"
"I´ve been doing some research , and I have found articles explaining how to use Django 's ( 1.8 ) cache busting , but they don´t explain my situation.I am using S3 ( and it works ) with the following setting in settings.py : In order to use cache busting the docs say I have to set : STATICFILES_STORAGE='django.contrib.staticfiles.storage.ManifestStaticFilesStorage ' I don´t know what setting to use in order to use both S3 and cache busting.Thanks !"
"I 'm trying to catch an exception thrown in the caller of a generator : This outputswhen I was intending for it to outputIn retrospect , I think this is because the caller has a different stack from the generator , so the exception is n't bubbled up to the generator . Is that correct ? Is there some other way to catch exceptions raised in the caller ? Aside : I can make it work using generator.throw ( ) , but that requires modifying the caller :"
"If I write a simple test ( which will be failing ) to check UTF-8 strings ( see example below ) and launch it using py.test , the output with errors will be ascii-encoded with \x.test : output : The question is : is there a way to make a output normal ( with py.test ) ? I mean without \x , like a E assert 'тест ' == 'тест1 ' ."
"I 'm struggling with this for one week . I 'm trying to run a python flask app that connect with a remote Oracle Database using instant client version 11.2.0.3.0.After a lot of problems , I ended using 3 buildpacks , two of them I need to customize and then I could install cx_Oracle in Heroku , but when I run the code I got the error : Well , this error is very well documented , so I just needed to do : But the problem is how to run apt-get in a Heroku App ? Using the third buildpack : The other buildpacks : After everything is configured , I runned a Heroku deploy and got the same error on execution . I could see in Heroku deploy log that heroku-buildpack-apt did its job but I got the same error in import cx_Oracle . Btw , just to be sure , I changed the forked python buildpack , that I 'm using , to do pip uninstall cx_Oracle at each deploy so I can have a freshly compiled version of it.At this point , the Great Internet was not able to help me anymore . Anywhere that I looked , I got the option to install libaio . I tried to search about using apt-get in Heroku App but everything points to heroku-buildpack-aptI think the problem could be cx_Oracle can not find the installed libaio and I setted a lot of Heroku App environment variables : EDIT : I forgot to mention this : When I run a heroku run ls -la /app/.apt/usr/lib/x86_64-linux-gnu where the libaio should be installed I got this : But when I run heroku run ls -l /lib/x86_64-linux-gnu/libaio.so.1.0.1 there is no file there . So the real problem is where are libaio installed ? Anyone can help me make this work ? Or there is another good substitution for cx_Oracle ? Thanks !"
"I have a list of addresses for many people ( 1-8 addresses each ) and I 'm trying to identify the number of unique addresses each person has . here is a sample address dataset for one personI 've got an address parser that separates the different portions of the address , the `` attn '' , house number , street name , PO Box , etc so that I can compare them individually ( code found here ) As you can see from the data above , addresses 1-3 are probably the same , and address 4 is different.I wrote the following similarity calculation method - there 's no magic to the weights , just what my intuition said should be most importantApplying this function to each of my addresses , you can see that addresses 1-3 correctly are completely similar , and address 4 is a bit different.To then cluster these , I thought affinity clustering might be the best way to go - the cluster count is variable , it works with distances , and can identify a prototypical example , which I could then use the `` best '' address to represent the cluster . However , I 'm getting some strange results - the affinityprop clusterer is producing 3 clusters for this data , instead of 2.Conversely , DBSCAN correctly clusters into twoLooking at this question , it seems that the issues is the clusterer is adding small random starting points , and counting the perfectly similar records as degeneracies.Is there a way around this or should I just give up with the affinity clustering and stick to DBSCAN ?"
"I 've got a date-ordered dataframe that can be grouped . What I am attempting to do is groupby a variable ( Person ) , determine the maximum ( weight ) for each group ( person ) , and then drop all rows that come after ( date ) the maximum . Here 's an example of the data : Here 's what I want the result to look like : I think its worth noting , there can be disjoint start dates and the maximum may appear at different times . My idea was to find the maximum for each group , obtain the MonthNo the maximum was in for that group , and then discard any rows with MonthNo greater Max Weight MonthNo . So far I 've been able to obtain the max by group , but can not get past doing a comparison based on that . Please let me know if I can edit/provide more information , have n't posted many questions here ! Thanks for the help , sorry if my formatting/question is n't clear ."
"I try to do simple async http client with asyncore : This code works fine and output is ( fast enought ) : But than i uncomment line with not exist host : The execution breaks , code hangs for some time , output part of data and hangs with no last data output : ... some hosts are lost here and long delay at start.Why this happen and how to fix this ? ps : My system ubuntu 11.10 + python 2.7.2"
"I apologize if this is published somewhere , but my cursory searching did n't find anything . While doing some Python programming I noticed that the following command : returns the empty string . But an equivalent command in sed : returns ab.It makes sense to me that the `` a* '' directive at the beginning of the python regex would match both a 's , causing `` ( ab ) * '' to match zero times , but I have no idea how sed comes up with ab . Does anybody know what the difference is between the two regex engines that causes this ? I believe they both match stars greedily by default , but it occurred to me that sed might match from the right rather than the left . Any insight would be greatly appreciated ."
The following code is used to execute doctests in a Google App Engine app . How would you do this for tests written as unit test asserts rather than as doctests ?
"Consider the following : NumPy 's numpy.int_ and numpy.float_ types are both in Python 's numeric abstract base class hierarchy , but it is strange to me that a np.int_ object is not an instance of the built-in int class , while a np.float_ object is an instance of the built-in float type.Why is this the case ?"
"I 'm making a pseudo transparent window in pygame with the intent of displaying varied info like a `` HUD '' The script uses PIL to grab an image of the desktop and use it as the background of the window.A simple version : This creates a Pygame window at the top of the screen.My problem is that the Pygame window blocks any mouse interaction with anything beneath it.Is there a way to allow mouse events to be ignored and go 'through ' the window , like for example clicking on a desktop icon , underneath a Pygame window ."
"I want to force object instantiation via class context manager . So make it impossible to instantiate directly.I implemented this solution , but technically user can still instantiate object.And context manager : Any better solution ?"
"I try to check if a variable is an instance of a number of any type ( int , float , Fraction , Decimal , etc . ) .I cam accross this question and its answer : How to properly use python 's isinstance ( ) to check if a variable is a number ? However , I would like to exclude complex numbers such as 1j.The class numbers.Real looked perfect but it returns False for Decimal numbers ... In contradiction , it works fine with Fraction ( 1 ) for example.The documentation describes some operations which should work with the number , I tested them without any error on a decimal instance.Decimal objects can not contains complex numbers moreover.So , why isinstance ( Decimal ( 1 ) , Real ) would return False ?"
"Recently , I came across the following oddity . Nesting { } -enclosed format fields seems to work in both Python 2.7 and 3.6 , but I ca n't find anything in the docs to say that should be so . For example , I get the following result on both 3.6 and 2.7 : Has anyone seen this before , and is it an intended feature ? Normally , I 'd dismiss this as an implementation quirk , and maybe report it as a bug . Two things , though : Python docs do n't always put all the information in the place I 'd look for it , and this is a Very Nice Feature to have ."
"I have a bunch of text files , all in the same format ( this is a snippet below , real file is longer ) : I have to extract all these variables and process them . I was going to write some code that goes through each line one at a time and extracts the relevant info through a series of split , strip etc . functions . This is a pretty common task people do with python , so i got to thinking there must be an easier method for this.Is there any module or method out there to allow something like : and then , to extract the vars from a another input file following the above format you would do the following : Note that while the same variable will appear in every file on the same line , they can be shifted a few characters to the right depending on how big the value preceding it on the same line is.The files are the output from emboss pepstats in case anyone was wondering.SOLUTIONThanks everyone for the quick replies . The solution here was to use findall function in the re module . Here is a simple example below : The ParseString function the successfully returns a list of strings which I can then process . As the files are always the same format , I was able to process all my files successfully . I only had two issues.1 ) As you can see above i 've had so escape all regex characters in my template file , which is n't that big of a deal.2 ) As I also mentioned above , this template is just a small snippit of the actual files I need to parse . When I tried this with my real data , re threw the following error : I worked around this by splitting my template string into 3 pieces , ran the ParseString function 3 times with the 3 different templates , and added the list results together.Thanks again !"
"So I have this metaclass that I want to use for automatic registration of new components , i.e . subclasses of some base component class . When registering a new component , its instance is expected to be passed to the register_component ( ) function that handles that.Metaclass code ( stripped-down version ) : The problem is that invoking new_class ( ) results in an error - but not for all classes . After some experimenting I realized that this only happens if a subclass calls super ( ) .__init__ ( ) in its own __init__ ( ) method.Sample component and the base class : What am I doing wrong here ? Reading this I found out that I probably should n't be doing instantiation in metaclass'es __new__ ( ) or __init__ ( ) , right ? Can this perhaps be circumvented somehow ? Also , some explanation in layman 's terms would be nice , I do n't know much of the internals of the CPython implementation.Thanks in advance ! ( FWIW , I use Python 3.3.6 , Ubuntu ) EDIT : I 'm adding the minimal example that was requested , you can run it directly and see the error in action yourself ."
"I am trying to create a class ( MySerial ) that instantiates a serial object so that I can write/read to a serial device ( UART ) . There is an instance method that is a decorator which wraps around a function that belongs to a completely different class ( App ) . So decorator is responsible for writing and reading to the serial buffer.If I create an instance of MySerial inside the App class , I ca n't use the decorator instance method that is created from MySerial.I have tried foregoing instance methods and using class methods as explained in this second answer , but I really need to instantiate MySerial , thus create an instance using __init__.How can this be accomplished ? Is it impossible ? Create a decorator that is an instance method.Use this decorator within another class"
"I have a python script that takes about 3 hours to run . I want to print some of the data it generates to a .csv file . I 'm on centos and I 'm running my script like so : I also tried : Nothing prints to output.csv , not even some test statements right at the beginning of the script . The next thing I tried was opening a file in my script - f = open ( 'output.csv ' , ' a ' ) and writing to it using f.write ( ) . Again , nothing shows up in the file ( although it does get created ) . How can I create my output.csv with the data that I want ? It seems to work normally when it 's not a background process but I 'd like it to run in the background . I 'm printing just like so : and in another version , I have something like this : I get exactly the same result in both cases . The file gets created , but nothing is actually written to it . Any help is appreciated . Thank you !"
"Want to convert the following numpy array ainto output bConsider each sub-array like thisThen merge 0 and 1 columns with ' ! ' and each row with ' . ' Then merge 0 and 2 column . And so on . Here 0,1 and 0,2 columns are merged using '* ' And ' , ' is used to merge the sub-arrays Just merging the strings using ' ! ' ' . ' '* ' ' , ' I tried the following code . Couldnt get the result though"
"I have a very large table ( currently 55 million rows , could be more ) , and I need to select subsets of it and perform very simple operations on those subsets , lots and lots of times . It seemed like pandas might be the best way to do this in python , but I 'm running into optimization problems.I 've tried to create a fake dataset that closely matches my real dataset ( although it 's ~5-10 times smaller ) . This is still big , takes a lot of memory , etc . There are four columns that I 'm querying on , and two that I 'm using for calculations.This selects ( for my random dataset ) 1848 rows : And it takes about .1-.15 seconds per query : So I thought I must be able to do better by sorting and indexing the table , right ? And I can take advantage of the fact that everything is an int , so I can do slices..And it takes a lot longer : Am I missing something here ? It seems like I could do something like numpy.searchsorted , but I ca n't think of how to do that on multiple columns . Is pandas the wrong choice ?"
Can someone explain the following behaviour : This is surprising to me that you can alter the constructor of a frozen set whereas it is not possible for the constructor of a mutable set .
"Background : I have a list of 44906 items : large = [ 1 , 60 , 17 , ... ] . I also have a personal computer with limited memory ( 8GB ) , running Ubuntu 14.04.4 LTS.The Goal : I need to find all the pair-wise combinations of large in a memory-efficient manner , without filling a list with all the combinations beforehand.The Problem & What I 've Tried So Far : When I use itertools.combinations ( large , 2 ) , and try to assign it to a list my memory fills up immediately , and I get very slow performance . The reason for this is that the number of pairwise combinations goes like n* ( n-1 ) /2 where n is the number of elements of the list.The number of combinations for n=44906 comes out to 44906*44905/2 = 1008251965 . A list with this many entries is much too large to store in memory . I would like to be able to design a function so that I can plug in a number i to find the ith pair-wise combination of numbers in this list , and a way to somehow dynamically compute this combination , without referring to a 1008251965 element list that 's impossible to store in memory.An Example of What I Am Trying To Do : Let 's say I have an array small = [ 1,2,3,4,5 ] In the configuration in which I have the code , itertools.combinations ( small , 2 ) will return a list of tuples as such : A call to a the function like this : ` find_pair ( 10 ) ' would return : , giving the 10th entry in the would-be array , but without calculating the entire combinatorial explosion beforehand.The thing is , I need to be able to drop in to the middle of the combinations , not starting from the beginning every time , which is what it seems like an iterator does : So , instead of having to execute next ( ) 10 times to get to the 10th combination , I would like to be able to retrieve the tuple returned by the 10th iteration with one call . The QuestionAre there any other combinatorial functions that behave this way designed to deal with huge data sets ? If not , is there a good way to implement a memory-saving algorithm that behaves this way ?"
"Long time user of this site but first time asking a question ! Thanks to all of the benevolent users who have been answering questions for ages : ) I have been using df.apply lately and ideally want to pass a dataframe into the args parameter to look something like so : df.apply ( testFunc , args= ( dfOther ) , axis = 1 ) My ultimate goal is to iterate over the dataframe I am passing in the args parameter and check logic against each row of the original dataframe , say df , and return some value from dfOther . So say I have a function like this : My current understanding is that args expects a Series object , and so if I actually run this we get the following error : However before I wrote testFunc which only passes in a single dataframe , I had actually written priorTestFunc , which looks like this ... And it works ! So to my dismay I have been coming into the habit of writing testFunc like so and it has been working as intended : I would really appreciate if someone could let me know why this would be the case and maybe errors that I will be prone to , or maybe another alternative for solving this kind of problem ! ! EDIT : As requested by the comment : My dfs generally look like the below.. They will have two matching columns and will be returning a value from the dfOther.at [ index , column ] I have considered pd.concat ( [ dfOther , df ] ) however I will be running an algorithm testing conditions on df and then updating it accordingly from specific values on dfOther ( which will also be updating ) and I would like df to be relatively neat , as opposed to making a multindex and throwing just about everything in it . Also I am aware df.iterrows is in general slow , but these dataframes will be about 500 rows at the max , so scalability is n't really a massive concern for me at the moment ."
"I 'm programming in python.I have the data of the following form : Segments of this data are associated with a score , for example : We can get a score for this data as follows : another possiblity is : So , the first combination gives the higher score.I wish to write an algorithm to establish for the data above the highest possible score . The members of data should no be repeated more than once . In other words : is not valid . How would you recommend I go about solving this ? Thanks , BarryEDIT : I should clarify that ( H , I ) ! = ( I , H ) and that ( I , H ) is not a subsegment for ABCDEFGHI , but is a subsegment of ABIHJ.Another thing I should mention is that scores is a very large set ( millions ) and the segment on which we 're calculating the score has an average length of around 10 . Furthermore , the way in which I calculate the score might change in the future . Maybe I 'd like to add the subsegments and take an average instead of multipling , who knows ... for that reason it might be better to seperate the code which calculates the possible combinations from the actual calculation of the score . At the moment , I 'm inclined to think that itertools.combinations might offer a good starting point ."
"I 'm trying to create a dataset that allows me to join actors and movies based on actor IDs and movie IDs from the Python IMDb API . Right now , I am trying to extract a list of movie IDs from an actor 's filmography and can not do it.For example , I know the ID for Rodney Dangerfield on IMDb is 0001098 . I can see his whole filmography with : which returns a large dictionary . I can see the most recent movie where he had an acting credit with : which returns : This object type is imdb.Movie.Movie . This object has a few key/value pairs : But there 's no ID key . And when I try to convert the IMDb object to a string with str ( results [ 'data ' ] [ 'filmography ' ] [ 0 ] [ 'actor ' ] [ 0 ] ) , I just get 'Angels with Angels'.Is there a way to get the Movie ID from the IMDb object ?"
"I have an array of doubles , roughly 200,000 rows by 100 columns , and I 'm looking for a fast algorithm to find the rows that contain sequences most similar to a given pattern ( the pattern can be anywhere from 10 to 100 elements ) . I 'm using python , so the brute force method ( code below : looping over each row and starting column index , and computing the Euclidean distance at each point ) takes around three minutes.The numpy.correlate function promises to solve this problem much faster ( running over the same dataset in less than 20 seconds ) . However , it simply computes a sliding dot product of the pattern over the full row , meaning that to compare similarity I 'd have to normalize the results first . Normalizing the cross-correlation requires computing the standard deviation of each slice of the data , which instantly negates the speed improvement of using numpy.correlate in the first place.Is it possible to compute normalized cross-correlation quickly in python ? Or will I have to resort to coding the brute force method in C ?"
"I have read this ( Is MATLAB faster than Python ? ) and I find it has lots of ifs.I have tried this little experiment on an old computer that still runs on Windows XP.In MATLAB R2010b I have copied and pasted the following code in the Command Window : The result was : Then I saved a py file with the following script : I pressed F5 and the result wasIn MATLAB it took 0.60 seconds ; in Python it took 49.78 seconds ( an eternity ! ! ) .So the question is : is there a simple way to make Python as fast as MATLAB ? Specifically : how do I change my py script so that it runs as fast as MATLAB ? UPDATEI have tried the same experiment in PyPy ( copying and pasting the same code as above ) : it did it in 1.0470001697540283 seconds on the same machine as before.I repeated the experiments with 1e9 loops.MATLAB results : PyPy results : I have also tried with a normal while loop , with similar results : Results : I am going to try NumPy in a little while ."
"I am trying to do what the user from this question is doing . I have it working for the separate IPython console but not the one integrated into PyCharm.To sum up , I want IPython to import some modules when I start it . I have gone to the C : \Users\Name\.ipython\profile_default\startup directory and I have made a startup.py file which contains After setting PYTHONSTARTUP to point to the file , the IPython console outside of PyCharm works as intended , but the one in PyCharm does not ."
"I 'm a beginner-intermediate self taught Python developer , In most of the projects I completed , I can see the following procedure repeats . I do n't have any outside home code experiences , I think the below code is not so professional as it is not reusable , and seems like it is not fitting all together in a container , but loosely coupled functions on different modules.How do I design this procedure as a class based design ?"
"I am trying to make a p2p app for sending just text messages . The way I am doing it is by having a server that keeps running as long as the application is running , and a client that connects to other node 's server to send messages.For testing purposes I am doing it with localhost , i.e . talking to myself.So i have following : However the problem is calling send_message ( last line ) after reactor.run seems to have no effect.The problem is I need to run the tcp client part ( connectTCP ) only when the user fills in a message , and sends it . So I am trying to do that with calling send_message . So how can I fix the above code to make that work ? From what I have read so far , using LoopingCall would be the way to go , but than I have to store new messages the client inputs into a variable , and constantly check that variable for a new message and than run send_message This would result in a delay between the user input and the function callback , regardless is this my best option ? Is there some other way to do it in this scenario ? Or am I lacking understanding of some crucial part of twisted 's architecture ? EDIT : As requested , here 's the GUI code , that takes the message input from the client : Thanks"
This question asks how to determine if every element in a list is the same . How would I go about determining if 95 % of the elements in a list are the same in a reasonably efficient way ? For example : This would need to be somewhat efficient because the lists may be very large .
I am working with an app using Django Rest Framework . models.pyserializers.pyviews.pyurl patternsThis is just a part of the whole system . I want to upload images using the RESTAPI and then upload it to amazon s3 and from there get the url and store it in the image_path field of the model Image . I have seen previous solutions for uploading files using REST ( like this ) but nothing worked for my case . Can someone suggest how can I do that ?
"I 'm using the SQLAlchemy recipe here to magically JSON encode/decode a column from the DB in my model like : I hit a snag when I wanted to create an extra `` raw_data '' field in my model to access the same underlying JSON data , but without encoding/decoding it : SQLAlchemy seems to get confused by the name collision and leave one column un-mapped . Is there any way I can convince SQLAlchemy to actually map both attributes to the same column ?"
"I am trying to create large graph via graph-tool library ( near 10^6 - 10^7 vertices ) and fill vertex property with vertex name or use names instead of vertex indexes . I have : list of names : set of edges , but instead of vertex indexes it consists of their names : Since add_edge_list ( ) allows to create vertices if they are no such vertix in the graph . I 'm trying to use it to fill an empty graph . It works ok , but when I was trying to get vertex by its name , I got an error that there are no vertex with such index.Here is the code of my program : The error message of print ( g.vertex ( '50 ' ) ) : I want to create graph : Using edge_list only ; Having quick access to a vertex by its name ; Optimal by time ( and RAM if possible ) .Is there any good way to do this ? EDIT : Current code :"
I 've been converting Ruby code to Python code and now I 'm stuck with this function that contains yield : I would like to call the function and tell it to print `` Hello '' three times because of the three yield statements . As the function does not take any arguments I get an error . Can you tell me the easiest way to get it working ? Thank you .
This is my view : Server Log : Output : I 'm using requests which gives me just the above line.I also tried testing some other urls where it worked ! What could be going wrong ?
"I am trying to upgrade PACKAGENAME using the following : I often see that no files are downloaded from the server and pip says all packages were installed successfully , but when I check the version installed it is not the newest version on the server.However , if I delete pip 's cache and run the above command again , it does download files from the server and install the newest version . Has anyone experienced this issue ? One workaround I found is to pass the -- ignore-installed argument to pip install , but this causes pip to download all packages from server even if the newest version is already installed ."
"I have a dataframe which has Dates and public holidays I have to create a conditional column named Public_Holiday_Week , which should return 1 , if that particular week has a public holiday And I want to see an output like this I tried using np.whereBut it applies 0 for other days of the week when it is not a public holiday.Do I have to apply rolling here ? Appreciate your help"
"How do I sort the above list of tuples by the second element according the the key list provided by 'order ' ? UPDATE on 11/18/13 : I found a much better approach to a variation of this question where the keys are certain to be unique , detailed in this question : Python : using a dict to speed sorting of a list of tuples.My above question does n't quite apply because the give list of tuples has two tuples with the key value of ' a ' ."
"Please do not mark this as a duplicate of how to call python and sklearn from matlab ? as the question is in my opinion not really answered.Since Matlab Release R2014b I think , it is possible to directly use python from matlab.In short words , you only have to place py in front of the python call.I my setup ( after giving matlab the python path with the command pyversion ( 'PATH_TO_PYTHON ' ) , that is running fine . I can even use dask multiprocessing . Quite cool . For example , executing py.dask.distributed.Client results inComing back to the question : I have sklearn installed and can use it from the referenced python Installation . It is working the same way as dask . But MATLAB R2017a is not able to find sklearn.A similiar call to the given above py.sklearn.cluster.dbscan results inIs there any python expert being able to explain ?"
"This may be trivial , but I 'm not sure I understand , I tried googling around but did not find a convincing answer.How do I understand this ? Why is an empty dict the same size as that of a non empty dict ?"
"Is there a way to make iPython automatically echo the result of an assignment statement ? For example , in MATLAB , ending an assignment statement without a semicolon prints the result of the assignment , and putting a semicolon at the end of the statement suppresses any output.I want to be able to do something similar in iPython . However , currently I have to use two separate statements if I want to see the assignment result :"
"I have a dataframe in this format : a and b are indexes , x is the value.I want to get rows 1 9 73 and 2 5 34 , in other words , the last row of that level.I 've been messing with .loc , .iloc , and .xs for an hour , but I ca n't get it to work . How do I do this ?"
"I have to parse a list of simple strings with a known structure but I 'm finding it unnecessarily clunky . I feel I 'm missing a trick , perhaps some simple regex that would make this trivial ? The string refers to some number of years/months in the future , I want to make this into decimal years.Generic format : `` aYbM '' Where a is the number of years , b is the number of months these can be ints and both are optional ( along with their identifier ) Test cases : My attempts so far have involved string splitting and been pretty cumbersome though they do produce the correct results :"
"Long story short , I have a substantial Python application that , among other things , does outcalls to `` losetup '' , `` mount '' , etc . on Linux . Essentially consuming system resources that must be released when complete.If my application crashes , I want to ensure these system resources are properly released.Does it make sense to do something like the following ? Is this something that is typically done ? Is there a better way ? Perhaps the destructor in a singleton class ?"
I want to get all rows where ( at least ) one of the columns in df [ mylist ] contains True.I 'm currently doing : where mylist is a list of strings relating to columns of df . But I would like to do this for any length of mylist.The only way I can think of to do this is to loop over mylist and create an new dataframe for each element of it and merge/concat or whatever them afterwards . But that does n't look very smart for me.Is there a better way ?
"When I am working with the PIL , I have to import a tons of PIL modules . I was experimenting with three ways to do this , but only the last one works despite all is being logical to me : Importing the complete PIL and calling it 's modules in the code : NOPEImporting everything from PIL : NOPEimporting some modules from PIL : OKWhat did I not get here ?"
"Consider the following Python3 program : I expected the output to be : But instead I got : My question is : is there anything in the Python language specification about associativity of the assignment operator , or is the behavior for the above example undefined ? All I was able to find is that expressions are evaluated form left to right , except that r-value is evaluated first in case of assignment , but that does n't help ."
"I 've been having a hard time trying to get a successful deployment of my Django Web App to AWS ' Elastic Beanstalk . I am able to deploy my app from the EB CLI on my local machine with no problem at all until I add a list of container_commands config file inside a .ebextensions folder.Here are the contents of my config file : I 've dug deep into the logs and found these messages in the cfn-init-cmd.log to be the most helpful : I 'm not sure why it ca n't find that command in this latest environment.I 've deployed this same app with this same config file to a prior beanstalk environment with no issues at all . The only difference now is that this new environment was launched within a VPC and is using the latest recommended platform.Old Beanstalk environment platform : Python 3.6 running on 64bit Amazon Linux/2.9.3New Beanstalk environment platform : Python 3.7 running on 64bit Amazon Linux 2/3.0.2I 've ran into other issues during this migration related to syntax updates with this latest platform . I 'm hoping this issue is also just a simple syntax issue , but I 've dug far and wide with no luck ... If someone could point out something obvious that I 'm missing here , I would greatly appreciate it ! Please let me know if I can provide some additional info !"
"IntroductionDisclaimer : I 'm very new to python packaging with distutils . So far I 've just stashed everything into modules , and packages manually and developed on top of that . I never wrote a setup.py file before.I have a Fortran module that I want to use in my python code with numpy . I figured the best way to do that would be f2py , since it is included in numpy . To automate the build process I want to use distutils and the corresponding numpy enhancement , which includes convenience functions for f2py wrappers.I do not understand how I should organize my files , and how to include my test suite.What I want is the possibility to use ./setup.py for building , installing , and testing , and developing.My directory structure looks as follows : And the setup.py file contains this : After running ./setup.py build I get.Which includes neither the __init__.py file , nor the tests.QuestionsIs it really necessary to add the path to every single source file of the extension ? ( I.e . volterra/integral.f90 ) Ca n't I give a parameter which says , look for stuff in volterra/ ? The top_path , and package_dir parameters did n't do the trick.Currently , the __init__.py file is not included in the build . Why is that ? How can I run my tests in this setup ? What 's the best workflow for doing development in such an environment ? I do n't want to install my package for every single change I do . How do you do development in the source directory when you need to compile some extension modules ?"
"I realise this question has been asked before , however this case is slightly different.I want to run a python imageboard ( using web.py ) , that will allow users to generate new images by submitting code . The code will be of the form of a single function that takes the x , y coordinates of a pixel and returns the r , g , b values , eg : Only a very small syntax is required , and it does n't necessarily have to be python . Using exec with limited scope seems to be too insecure , and using PyPy or a VM seems unnecessarily complex ( I 'm quite new to all this ) .Rather than sandboxing it , is there a pythonic way to execute the code in a much smaller language ? Either a subset of python ( parsing and whitelisting ? ) , or a math oriented language that I can embed ?"
"I am trying to connect to a GRPC server in a celery task . I have the following piece of codeWhen I run this snippet through the Python shell , I am able to establish the connection , and execute the GRPC methods . However , when I run this through the Celery task , I get the grpc.FutureTimeoutError , and the connection does not get established.The Celery worker lies on the same machine as the grpc server . I tried using the socket library to ping the GRPC server , and that worked ( It returned some junk response ) .I am using Python 2.7 , with grpcio==1.6.0 installed . The Celery version is 4.1.0 . Any pointers would be helpful ."
"I need to test a function in python that takes a list with any type of data , from integers to strings to any object a user makes up . Is there a way in hypothesis to generate a list with random objects ? I know that I could generate a list of random floats withAnd so on and so forth with integers , chars , etc . But how can I make it so a random list has more than one data type with it ?"
I 'm trying to write a function that can take two arguments and then add it to multiprocessing.Pool and parallelize it.I had some complications when I tried to write this simple function.It 's returning an error : I ca n't understand what 's wrong.Can anybody explain what this error means and how I can fix it ?
"I have been studying this example of stacking . In this case , each set of K-folds produces one column of data , and this is repeated for each classifier . I.e : the matrices for blending are : I need to stack predictions from a multiclass problem ( probs 15 different classes per sample ) . This will produce an n*15 matrix for each clf.Should these matrices just be concatenated horizontally ? Or should they be combined in some other way , before logistic regression is applied ? Thanks ."
"Really odd experience with this which took me over an hour to figure out . I have a cgi script written in python which takes form data and forwards it to an email , the problem was , if there was a colon in the string it would cause python to output nothing to the email.Does anyone know why this is ? For example : works , though : prints no output . My email function works essentially : where msg = outputJust wondering if the colon is a special character in python string output"
"There 's a C++ comparison to get union of lists from lists of lists : The fastest way to find union of setsAnd there 's several other python related questions but none suggest the fastest way to unionize the lists : Finding a union of lists of lists in PythonFlattening a shallow list in PythonFrom the answers , I 've gathered that there are at least 2 ways to do it : Note that I 'm casting the set to list afterwards because I need the order of the list to be fixed for further processing . After some comparison , it seems like list ( set ( chain ( *x ) ) ) is more stable and takes less time : [ out ] : Taking out the variable of casting sets to list : [ out ] : Here 's the full output when I try to print the intermediate timings ( without list casting ) : http : //pastebin.com/raw/y3i6dXZ8Why is it that list ( set ( chain ( *x ) ) ) takes less time than list ( set ( ) .union ( *x ) ) ? Is there another way of achieving the same union of lists ? Using numpy or pandas or sframe or something ? Is the alternative faster ?"
"Is this expected behaviour ? I thought to raise an issue with Spark , but this seems such a basic functionality , that it 's hard to imagine that there 's a bug here . What am I missing ? PythonPySpark"
"I have some code that uses ctypes to try to determine if the file pointed to by sys.stdout is actually stdout . I know that on any POSIX-compliant system , and even on Windows , it should be safe to assume this is true if sys.stdout.fileno ( ) == 1 , so my question is not how to do this in general.In my code ( which is already using ctypes for something unrelated to my question ) I carelessly had something like : This works perfectly fine on Linux , so I did n't really think about it much . It looked nicer and more readable than hard-coding 1 as the file descriptor . But I found a few days later that my code was n't working on OSX.It turns outs OSX 's libc does n't export any symbol called 'stdout ' . Instead its stdio.h has stdout defined as : If I change my code to c_void_p.in_dll ( libc , '__stdoutp ' ) my code works as expected , but of course that 's OSX-only . Windows , it turns out , has a similar issue ( at least if using MSVC ) .I will probably just change my code to use 1 , but my question still stands , out of curiosity , if there 's a cross-platform way to get the stdio pointer ( and likewise stdin and stderr ) without assuming that it 's using the POSIX-compliant descriptor ?"
"I have a huge number of records containing sequences ( 'ATCGTGTGCATCAGTTTCGA ... ' ) , up to 500 characters . I also have a list of smaller sequences , usually 10-20 characters . I would like to use the Levenshtein distance in order to find these smaller sequences inside the records allowing for small changes or indels ( L_distance < =2 ) .The problem is that I also want to get the start position of such smaller sequences , and apparently it only compares sequences of the same length.In this example I would like to obtain the position ( 7 ) and the distance ( 0 in this case ) .Is there an easy way to solve this problem , or do I have to break the larger sequences into smaller ones and then run the Levenshtein distance for all of them ? That might take too much time.Thanks.UPDATE # Naive implementation generating all substrings after looking for an exact match ."
"Consider the following code : My PyCharm IDE indicates a typing error for the last line : Is it a typing/type declaration error ? Is the PyCharm type checker failing ? If it 's a typing error , what should it be ?"
"I have been trying to train a ML classifier using Python and the scikit-learn toolkit.First I applied my own threshold ( e.g int ( len ( X ) *0.75 ) ) in splitting the dataset and got this result when printing my metrics : Then I used cross validation in order to have a more detailed view of the model 's accuracy using : scores = cross_validation.cross_val_score ( X , y , cv=10 ) and got the scores below : Cross_val_scores= [ 0.66666667 0.79166667 0.45833333 0.70833333 0.52173913 0.52173913 0.47826087 0.47826087 0.52173913 0.47826087 ] Accuracy : 0.56 ( Standard Deviation : +/- 0.22 ) , where Accuracy here equals mean ( scores ) .Can someone please advice me on how to interpret correctly those scores ? I understand how the dataset gets split when using cross validation in order to observe the model 's accuracy within the whole range of the dataset but I would like to know more.For instance is there a way to split it and achieve the highest accuracy possible ( e.g . 0.79166667 ) and if so how I could do that ? I imagine that happens because there is a split within my dataset that a model when trained using those data can produce a closer prediction , right ? Is there a way to reduce the relatively high standard deviation ? Thank you for your time ."
"I am using scipy 's wavfile library to read a wavfile.This will return the rate and RAW data of the given wav filename.What transformation on the data array do I need to do to go from RAW data to frequency ? I understand FFT is used to go to the frequency domain , but I would like to go to the time domain.Any help is appreciated ! : )"
"How do I quickly find , identify , and remove the last duplicate of all symmetric pairs in this data frame ? An example of symmetric pair is that ' ( 0 , 1 ) ' is equal to ' ( 1 , 0 ) ' . The latter should be removed . The algorithm must be fast , so it is recommended to use numpy . Converting to python object is not allowed ."
"I want my python script to change to a new directory using ~ as a shortcut for the home directory ( so I can use the script on several different computers with different home directory names ) : This , however generates an error . Python does n't seem to be able to recognize ~ : Why does this happen and is there a way around it ? I have python 3.4 on OsX Yosemite ."
"I 'm using Django and Python 3.7 . I want to have more efficient parsing so I was reading about SoupStrainer objects . I created a custom one to help me parse only the elements I need ... One of the conditions is I only want to parse `` span '' elements whose text matches a certain pattern . Hence theclause . However , this results in anerror when I try and run the above . What 's the proper way to write my strainer ?"
"I have a class that performs some simple data manipulation , I need three methods : set , add , sub : The problem is with the `` set '' method but defining it as a class method should not clash with the `` set ( ) '' builtin function.The Python Style Guide states that argument names of functions and methods should not shadow built-in functions , but it 's this the case for method names ? Obviously I could choose another method name , but the question is more generic and valid for other possible method names ( i.e . filter , sum , input ) ."
"I have a Pandas column that contains results from a survey , which are either free text or numbers from 1-5 . I am retrieving these from an API in JSON format and convert them into a DataFrame . Each row represents one question with the answer of a participant like this : The column that has the results stores all of them as string for now , so when exporting them to excel the numbers are stored as text . My goal is to have a separate column for the text answers and leave the field they were originally in empty , so that we have separate columns for the text results and the numeric results for calculation purposes.I am generating this df from lists like this : So the first thing I tried was to convert the numeric values in the column from string to numbers via this : df [ `` Answer '' ] = pd.to_numeric ( df [ 'Answer ' ] , errors='ignore ' ) Idea was that if it works I can simply do a for loop to check if a value in the answer column is string and then move that value into a new column.The issue is , that the errors command does not work as intended for me . When I leave it on ignore , nothing gets converted . When I change it to coerce , the numbers get converted from str to numeric , but the fields where there freetext answers are now empty in Excel ."
"I have a kivy application that can interact with other windows using the pywinauto module . The application works fine in Linux ( where pywinauto is n't used ) but in Windows I get the following error , the application wo n't even start up : The reason I think it 's an issue with pywinauto , is that I have the following lines and it works fine in Linux : Also I comment out the pywinauto import lines and it starts . It could be linked to this issue . I do n't really know what code to include , as it works in other operating systems ... . I 'm assuming pywinauto is changing something that stops kivy from working.My question is this : how can I have both the functionality of kivy and pywinauto in the same application ?"
Hi I am trying to import 'LDAPBindError ' from the ldap3 library and I am getting an import error . Can somebody please explain why this is not working ? I am using the following versionspip3.6 list |grep ldap3ldap3 ( 2.5 ) python3.6 -- versionPython 3.6.4
"I 'm using kafka-python to produce messages for a Kafka 2.2.1 cluster ( a managed cluster instance from AWS 's MSK service ) . I 'm able to retrieve the bootstrap servers and establish a network connection to them , but no message ever gets through . Instead after each message of the Type A I immediately receive one of type B ... and eventually a type C : What causes a broker node to accept a TCP connection from a hopeful producer , but then immediately close it again ? EditThe topic already exists , and kafka-topics.sh -- list displays it.I have the same problem with all clients I 've used : Kafka 's kafka-console-producer.sh , kafka-python , confluent-kafka , and kafkacatThe Kafka cluster is in the same VPC as all my other machines , and its security group allows any incoming and outgoing traffic within that VPC.However , it 's managed by Amazon 's Managed Streaming for Kafka ( MSK ) servive , which means I do n't have fine-grained control over the server installation settings ( or even know what they are ) . MSK just publishes the zookeeper and message broker URLs for clients to use.The producer runs as an AWS Lambda function , but the problem persists when I run it on a normal EC2 instance.Permissions are not the issue . I have assigned the lambda role all the AWS permissions it needs ( AWS is always very explicit about which operation required which missing permission ) .Connectivity is not the issue . I can reach the URLs of both the zookeepers and the message brokers with standard telnet . However , issuing commands to the zookeepers works , while issuing commands to the message brokers always eventually fails . Since Kafka uses a binary protocol over TCP , I 'm at a loss how to debug the problem further.EditAs suggested , I debugged this with ./kafkacat -b $ BROKERS -L -d brokerand got : So , is this a kind of mismatch between client and broker API versions ? How can I recover from this , bearing in mind that I have no control over the version or the configuration of the Kafka cluster that AWS provides ?"
"C # programmer trying to learn some Python . I am trying to run a CPU intensive calc while letting an IO bound async method quietly chug away in the background . In C # , I would typically set the awaitable going , then kick off the CPU intensive code , then await the IO task , then combine results.Here 's how I 'd do it in C # And here 's the equivalent code in pythonImportantly , please note that the CPU intensive task is represented by a blocking sleep that can not be awaited and the IO bound task is represented by a non-blocking sleep that is awaitable.This takes 2.5 seconds to run in C # and 4.5 seconds in Python . The difference is that C # runs the asynchronous method straight away whereas python only starts the method when it hits the await . Output below confirms this . How can I achieve the desired result . Code that would work in Jupyter Notebook would be appreciated if at all possible.Update 1Inspired by knh190 's answer , it seems that I can get most of the way there using asyncio.create_task ( ... ) . This achieves the desired result ( 2.5 secs ) : first , the asynchronous code is set running ; next , the blocking CPU code is run synchronously ; third the asynchronous code is awaited ; finally the results are combined . To get the asynchronous call to actually start running , I had to put an await asyncio.sleep ( 0 ) in , which feels like a horrible hack . Can we set the task running without doing this ? There must be a better way ..."
"I develop in python with glade and pygtk since 3 months , but even before I had time to get used to it , it was already obsolete.Using Archlinux , my system is constantly up to date , so I am forced to use gtk3 even if I found it a bit lacking of features compared to gtk2.So I decided to switch to pygobject . Unfortunately , the documentation is not complete.I successfully upgraded my glade file and my python code to the new system , but one error subsists.In one of my programs , I have a combobox with an entry.I use to call the method get_active_text ( ) to get the content of the entry , regardless if it was selected from the combobox or entered by the user.This method does not exist any more ( I suppose , because it gave me an error ) so I use this instead : As you can see the old code is commented.This code works , but I have an odd issue : I ca n't use the entry ! I am able to select the text from the combobox , but the entry is not usable . I can select , but I ca n't type in it.Is this a new behavior I need to activate somewhere ? With the gtk2 version of the program , I do n't have any problem.Here is the part in my glade file that describes the combobox entry : I created a liststore with one column of type gchararray containing the text.The cell is rendered by the GtkCellRenderer ( but the property `` text '' of the cellrenderer is not defined , because if I define it to 0 ( the gchararray ) , I get the text twice ! ) I thought adding an entrybuffer would help , but it does not change anything.EDIT : I found the solution : can_focus was false for the embedded entry.Now it works , without the need to an entrybuffer.I found the solution before posting this , but I post it in case other users have this issue too ."
"Objective : return a value from a function in the units ( or any trivial modification ) requested by the caller.Background : I am running Python 2.7 on a Raspberry Pi 3 , and use the function distance ( ) to get the distance a rotary encoder has turned . I need this distance in different units depending on where the function is called . How then , should this be written pythonically ( i.e . short , and easily maintained ) .First Attempt : My first attempt was to use a unit of meters in the function , and have a long elif tree to select the right units to return in.The nice thing about this approach is that it has a way to recognize a unit that is n't available.Second Attempt : My second attempt was to create a dictionary to contain various multipliers.Here , unrecognized units are caught with a KeyError.Relevance : I know of existing libraries like Pint , but am looking for a solution to this programming problem . When you have a function in Python , and you need to make slight modifications to the output in a reusable way . I have other functions such as speed ( ) that use 'm/s ' as a base unit , and need a similar units argument . From my experience , a well-structured program does not involve a paragraph of elif branches before every return statement . In this case , if I wanted to change how I calculate the units , I would have to carefully grep through my code , and make sure I change how the units are calculated at every instance . A proper solution would only require changing the calculation once.This is possibly too broad , but it is a pattern I keep running into ."
"I am attempting to write a test that checks if a variable holding the bound method of a class is the same as another reference to that method . Normally this is not a problem , but it does not appear to work when done within another method of the same class . Here is a minimal example : I am really using an assert instead of print , but that is neither here nor there since the end result is the same . The test is run as follows : The result is False even though I am expecting it to be True . The issue manifests itself in both Python 3.5 and Python 2.7 ( running under Anaconda ) .I understand that bound methods are closures that are acquired by doing something like TestClass.test_method.__get__ ( instance , type ( instance ) ) . However , I would expect that self.sample_method is already a reference to such a closure , so that self.sample_method and instance.sample_method represent the same reference.Part of what is confusing me here is the output of the real pytest test that I am running ( working on a PR for matplotlib ) : If I understand the output correctly , the actual comparison ( the first line ) is really comparing the same objects , but somehow turning up False . The only thing I can imagine at this point is that __get__ is in fact being called twice , but I know neither why/where/how , nor how to work around it ."
"I have an array of values , x . Given 'start ' and 'stop ' indices , I need to construct an array y using sub-arrays of x.Where I would like y to be : ( In practice the arrays I am using are much larger ) .One way to construct y is using a list comprehension , but the list needs to be flattened afterwards : I found that it is actually faster to use a for-loop : I was wondering if anyone knows of a way that I can optimize this ? Thank you very much ! EDITThe following tests all of the times : Which results in :"
"Here 's a simple reST snippet : and here 's a demonstration of it being rendered : with initial line : http : //rst.ninjs.org/ ? n=ff67380d732a33c7844f350c240804d0without initial line : http : //rst.ninjs.org/ ? n=550ea2c1b4233affdce1d158c5dc4d99I 'm rendering reST using the following Python : How do I get subheadings ( specifically , < h2 > tags ) without the initial line ? Do have provide two levels of hierarchy above the subheadings ? Naively providing a page header does n't help : http : //rst.ninjs.org/ ? n=e874f6eaad17c8ae7fd565f9ecb2212b"
"I need to convert a string in the format `` 1.234.345,00 '' to the float value 1234345.00.One way is to use repeated str.replace : However , this appears manual and non-generalised . This heavily upvoted answer suggests using the locale library . But my default locale does n't have the same conventions as my input string . I then discovered a way to extract the characters used in local conventions as a dictionary : Is there a way to update this dictionary , save as a custom locale and then be able to call this custom locale going forwards . Something like : If this is n't possible , how do we get a list of all available locale and their conventions ? Seems anti-pattern to `` deduce '' the correct configuration from the conventions ( not to mention inefficient / manual ) , so I was hoping for a generic solution such as above pseudo-code.Edit : Here 's my attempt at finding all locales where thousands_sep == ' . ' and decimal_point == ' , ' . In fact , more generally , to group locales by combinations of these parameters : Result :"
"Someone posted this interesting formulation , and I tried it out in a Python 3 console : While there is a lot to unpack here , what I do n't understand ( and the semantics of interesting character formulations seems particularly hard to search for ) is what the { ... } means in this context ? Changing the above a bit : It is this second output that really baffles me : I would have expected the { ... } to have been altered , but my nearest guess is that the , 5 implies a tuple where the first element is somehow undefined ? And that is what the { ... } means ? If so , this is a new category of type for me in Python , and I 'd like to have a name for it so I can learn more ."
"Let 's create two lists : Out : I created two independent lists , and the output shows two different memory addresses . This is not surprising . But now let 's do the same thing without the assignment : Out : And a second time : Out : I am not sure how to interpret this . Why do these two unnamed lists have the same memory address ?"
"I would like to repeat these example_1 example_2 with my dataset.But , the issue is I have my data set in a csv file . I 've tried to convert pandas data frame to dict , but it did n't work . So , the question is there a way for converting the pandas dataframe into Pandas-Ml ModelFrame ?"
"I have been using numpy.mgrid for a while and I am familiar with what it does . But ever since , I have been wondering what it actually is ... A typical use case and syntax for mgrid would be What bothers me is the syntax mgrid [ ... ] , which makes me believe that mgrid is not a function or a method . The docs say that mgrid is a nd_grid instance which returns a dense multi-dimensional “ meshgrid ” .What does it do in the background ? And why is it used like a function , but the syntax differs from that of a function ( I would expect something like mgrid ( ( 0,5 ) , ( 0,3 ) ) ) . I am really lost at trying to understand what mgrid is.Any insights are welcome !"
"I 'm trying to develop a Python based wrapper around the Xilinx ISE TCL shell xtclsh.exe . If it works , I 'll add support for other shells like PlanAhead or Vivado ... So what 's the big picture ? I have a list of VHDL source files , which form an IP core . I would like to open an existing ISE project , search for missing VHDL files and add them if necessary . Because IP cores have overlapping file dependencies , it 's possible that a project already contains some files , so I 'm only looking for missing files.The example user Python 3.x and subprocess with pipes . The xtclsh.exe is launched and commands are send line by line to the shell . The output is monitored for results . To ease the example , I redirected STDERR to STDOUT . A dummy output POC_BOUNDARY is inserted into the command stream , to indicate completed commands.The attached example code can be tested by setting up an example ISE project , which has some VHDL source files.My problem is that INFO , WARNING and ERROR messages are displayed , but the results from the TCL commands can not be read by the script.Manually executing search *.vhdl -type file in xtclsh.exe results in : Executing the script results in : Questions : Where does xtclsh write to ? How can I read the results from TCL commands ? Btw : The prompt sign % is also not visible to my script.Python code to reproduce the behavior :"
"Suppose I have following code : And let 's save it as foo.py . When I run : Have tried indent the docstring ( > > > a = `` ' ... ' '' . Have checked all indents - 4 spaces for each indent ) and changed single quote to double quote ( > > > a = `` '' '' ... . '' '' '' ) , the errors are different and the doctest just wo n't go successfully . Currently the only thing work is to join all lines to a extreme long string and separate with '\r\n'.Do I miss something ?"
I am trying to embed and send html table created with pandas .to_html.I 'm happy to either send the df directly to the email or from a file.So far I can embed images with the following : I have experimenting with this which attached the df but not embeds for some reason.Alternatively i can send the df data to an email but sends as unformatted text and ca n't seems to work out this approach with formatting ( i.e . ) table with simple border.More complete code for embedded image which i desire to adjust for embed html tables ... .and here 's my final code with some minor tweaks to msgText based on solution from all working great ! thanks
"In Django I have the following models.In the Supervisor model I have a many-to-many field without an explicitly defined through table . In the ForeignKey field of the Topic model I would like to refer to the automatically created intermediate model ( created by the many-to-many field in the Supervisor model ) , but I do n't know what is the name of the intermediate model ( therefore I wrote ' ? ? ? ' there , instead of the name ) .Django documentation tells that `` If you don ’ t specify an explicit through model , there is still an implicit through model class you can use to directly access the table created to hold the association . `` How can I use the automatically created implicit through model class in Django in a ForeignKey field ?"
"I am using a multiprocessing.Pool which calls a function in 1 or more subprocesses to produce a large chunk of data.The worker process creates a multiprocessing.shared_memory.SharedMemory object and uses the default name assigned by shared_memory.The worker returns the string name of the SharedMemory object to the main process.In the main process the SharedMemory object is linked to , consumed , and then unlinked & closed.At shutdown I 'm seeing warnings from resource_tracker : Since I unlinked the shared memory objects in my main process I 'm confused about what 's happening here . I suspect these messages are occurring in the subprocess ( in this example I tested with a process pool of size 1 ) .Here is a minimum reproducible example : I have found that running that example on my own laptop ( Linux Mint 19.3 ) it runs fine , however running it on two different server machines ( unknown OS configurations , but both different ) it does exhibit the problem . In all cases I 'm running the code from a docker container , so Python/software config is identical , the only difference is the Linux kernel/host OS.I notice this documentation that might be relevant : https : //docs.python.org/3.8/library/multiprocessing.html # contexts-and-start-methodsI also notice that the number of `` leaked shared_memory objects '' varies from run to run . Since I unlink in main process , then immediately exit , perhaps this resource_tracker ( which I think is a separate process ) has just not received an update before the main process exits . I do n't understand the role of the resource_tracker well enough to fully understand what I just proposed though.Related topics : https : //bugs.python.org/issue39959"
I use sphynx to generate documentation from static .rst files ( no docstrings extraction ) .I have two files in my repository : index.rstand intro.rstBelow is screenshot of pdf obtained by running sphinx-build -b latex . _build ; cd _build ; pdflatex * : Flabbergasting or what ?
"When querying a search index in the Python version of the GAE Search API , what is the best practice for searching for items where documents with words match the title are first returned , and then documents where words match the body ? For example given : If it is possible , how might one perform a search on an index of Documents of the above form with results returned in this priority , where the phrase being searched for is in the variable qs : Documents whose title matches the qs ; thenDocuments whose body match the qs words.It seems like the correct solution is to use a MatchScorer , but I may be off the mark on this as I have not used this search functionality before . It is not clear from the documentation how to use the MatchScorer , but I presume one subclasses it and overloads some function - but as this is not documented , and I have not delved into the code , I can not say for sure.Is there something here that I am missing , or is this the correct strategy ? Did I miss where this sort of thing is documented ? Just for clarity here is a more elaborate example of the desired outcome :"
"When running the following code , the result of dask.dataframe.head ( ) depends on npartitions : This yields the following result : However , when I set npartitions to 1 or 2 , I get the expected result : It seems to be important , that npartitions is lower than the length of the dataframe . Is this intended ?"
"Using Python 3.7.3 ( Anaconda ) on Windows , hitting tab results in the following traceback : Environment details : Windows 10 64-bitPython 3.7.3 , Anaconda distributionpyreadline == 2.1Tab key works fine in IPythonI am aware pyreadline is likely causing the issue . Tab not working in python 3.6 , working in 3.5 , 3.6 32 bit version on Windows suggests uninstalling pyreadline . However , when I try pip uninstall pyreadline I get this error : Trying this also did not help : Unable to install 'pyreadline ' using pip on Windows"
I have a dataframe as below.I want to convert it to directed networkx multigraph . I do & getBut i want to getI have found no parameter for directed & multigraph in this manual.I can save df as txt and use nx.read_edgelist ( ) but it 's not convinient
"I have a simple database as in the below picture : and the query looks like : Now I need to create an interactive chart which depends on inputs like a date picker and drop down list.As this is the sample GUI I created to figure chart throw this inputs as shown in the below picture : Now I am creating chart based on the below columns name as in the below picture : as this is the related query as the below : So now the X-axis should be the Date column and the y-axis related to those below columns is the KPI columns : So now we have a unique column contains unique values is called CellName , this cell name I want create a simple chart for this unique value based on date columnn and KPI column.So for example I want to show a line chart for a certain CellName = 2002334 for KPI LRRCConnReqAtt based on data from 27 of December to 9 of January . So I need a chart as the below picture and this is an example chart created in Excel.and this is my code : Note that I want to put more than one KPI in one chart with different plots ... .As the scale values in those KPI are little bit different so I tried to create a three types of objects with columns name scale values as the below codeand this is the error I found : Any assistance appreciated. ! ! ! !"
"`` Behavior of “ round ” function in Python '' observes that Python rounds floats like this : The accepted answer confirms this is caused by the binary representation of floats being inaccurate , which is all logical.Assuming that Ruby floats are just as inaccurate as Python 's , how come Ruby floats round like a human would ? Does Ruby cheat ?"
"Say I have a dictionary of kings with roman numerals in their names as the key , the roman numerals in integer form as the values.d = { 'Zemco III ' : 3 , 'Usamec XL ' : 40 , 'Usamec VII ' : 7 , 'Robert VIII ' : 8 , 'Usamec XLII ' : 42 , 'Mary XXIV ' : 24 , 'Robert III ' : 3 , 'Robert XV ' : 15 , 'Usamec XLIX ' : 49 } I would like to sort the list from oldest to youngest , that is Usamec XLII should come before Usamec XLIX . I would also like to sort the list alphabetically , that is the Usamec XLII should come before Zemco III.My approach was to sort by name first , then by roman numeral value as such : However , because the roman numerals are part of the key , my alphabetical sort does not work as intended . My question is , can I sort the dictionary by a part of the key , for example if my key is Zemco III , can I sort my items somehow with key.split ( ) [ 0 ] instead of the entire key ? Thanks !"
"I am doing a function optimization using an evolutionary algorithm ( CMAES ) . To run it faster I am using the multiprocessing module . The function I need to optimize takes large matrices as inputs ( input_A_Opt , and input_B_Opt ) in the code below . They are several GBs of size . When I run the function without multiprocessing , it works well . When I use multiprocessing there seems to be a problem with memory . If I run it with small inputs it works well , but when I run with the full input , I get the following error : And here 's a simplified version of the code ( again , if I run it with the input 10 times smaller , all works fine ) : Any suggestions on how to solve this issue ? am I not coding properly ( new to python ) or should I use other multiprocessing package like scoop ?"
"I 'm using Oct2Py in order to use some M-files in my Python code . Let 's say that I have this simple Matlab function : What happens if I call it in Octave is obviously : Now if I call it in Python , using oct2py : This returns : TypeError : 'int ' object is not iterableIt seems like octave.toto ( n ) only returns the first value , when I 'd expect two ... Can anyone explain to me what I should be doing ? Thanks"
"Is there a more elegant way of writing this function ? Here , what it does : It is taking average of even and odd indexes , and leaves last one as is if list has odd number of elements"
"Does anyone has any insight on organizing sqlalchemy based projects ? I have many tables and classes with foreign keys , and relations . What is everyone doing in terms of separating classes , tables , and mappers ? I am relatively new to the framework , so any help would be appreciated.Example : Including mappers inside classA , and classB works , but poses cross import issues when building relations.. Maybe I am missing something : )"
"I 'm trying to implement the django map widgethttps : //github.com/erdem/django-map-widgetsBut there is no map appearing and i have this in the browser consolein settings.py , i havein my model.pyUPDATE : my static files configuration in settings.pyafter running python manage.py collectstatic , static files are copied to another directory static_root - https : //imgur.com/a/TmhYr . notice that mapwidgets directory is not in the original project static directory . im running on development , i notice bootstrap static files are using the the file in static directory and not static_rootAm I missing something ? to I need to load anything in the template ?"
"I am a beginner in Python and can not cope with one of the moments of my project , so I would be glad you to help me : ) Lets 's imagine , I have a *.txt file with only one column which looks like : *Column with rows is added here just to simplify the explanation.I need to calculate the second column which takes as input data from the Column-1 and outputs the result of subtraction of this row value from the previous one ( if the row-1 ( Column-1 ) value is 0 , than it should be 0 in the row-1 ( Column-2 ) as well ) . It should be like : row-2 ( Column-2 ) = row-2 ( Column-1 ) - row-1 ( Column-1 ) row-3 ( Column-2 ) = row-3 ( Column-1 ) - row-2 ( Column-1 ) and so on . Let me show you how the output file shoud be like : For now I am only here with my programming : I wonder what to do next . Maybe , numpy is right for this task ? I am not strong in it ( basically , I do n't know it at all ) , should I learn it ? Anyway , I would be really thankful for you contribution ."
"The question is a bit complicated , and googling did n't really help . I will try to put in only relevant aspects of it.I have a large document in approximately the following format : Sample Input : I am trying to remove the section of the text according to the below : From either of : ABCDEFGHITo either of ( while retaining this word ) : PQRSTUVWXThe words that make up `` From '' can appear anywhere in a line ( Look at GHI ) . But for removal the entire line needs to be removed . ( The entire line containing GHI needs to be removed as in the sample output below ) Sample Output : The above example actually seemed easy for me until I ran it against very large input files ( 49KB ) What I have tried : The regular expression I am currently using is ( with case insensitive and multiline modifier ) : ProblemThe above regexp works wonderfully on small text files . But fails/crashes the engine on large files . I have tried it against the below : V8 ( Node.js ) : HangsRhino : HangsPython : HangsJava : StackoverflowError ( Stack trace posted at the end of this question ) IonMonkey ( Firefox ) : WORKS ! Actual Input : My original Input : http : //ideone.com/W4sZmBMy regular expression ( split across multiple lines for clarity ) : Question : Is my regular expression correct ? Can it be optimized further to avoid this problem ? In case it is correct , why do other engines hang infinitely ? A section of stack trace is below : Stack Trace : PS : I 'm adding several tags to this question since I have tried it on those environments and the experiment failed ."
I have a code that makes a rather simple query-skip-limit-sort.I 'm encountering a phenomena I 'm having hard time explaining.On `` small '' skip value - everything is fine.On `` high '' skip value ( > 18000 ) - I ca n't get a result with limit higher then 20 without getting the following error : The question is - why is this happening only with large skip count ? How can I solve this ? Running it on mongoShell ( even with DBQuery.shellBatchSize = 300 ) works.And it seems to be using the index db.my_collection.find ( { 'foo ' : false } ) .skip ( 19000 ) .limit ( 100 ) .sort ( { 'meta_data.created_at ' : -1 } ) .explain ( ) } More info : It seems that indeed sorting is done in memory - it exists in rejected plans.So what can be done ? And another question . Why is it happening only at a large skip count ? why does it matter ?
I have tried to get the edge of the mask image with the following code : What I get is this : But the edge is n't smooth for some reason.My plan was to use the edge image to crop the following picture : Does anyone know how I could make the edge image better and how I could use this to crop the normal image ? EDIT : @ Mark Setchell made a good point : If i could use the mask image directly to crop the image that would be great . Also : It is maybe possible to lay the normal image precisely on the mask image so that the black area on the mask would cover the blue-ish area on the normal picture.EDIT : @ Mark Setchell introduced the idea of multiplying the normale image with the mask image so what the background would result in 0 ( black ) and the rest would keep its color . Would it be a problem when my mask image is .png and my normal picture is .jpg when multiplying ? EDIT : I have written the following code to try to multiply two pictures : But I get the error : ValueError : images do not matchDoes anyone know how I could solve this ?
"BackgroundAs described here http : //www.ericharshbarger.org/dice/ # gofirst_4d12 , `` Go First '' Dice is a set of four dice , each with unique numbering , so that : Any roll of two or more dice will never result in a tie.Any die rolled against any other die in the set has an equal chance of `` win/lose '' against said die . Here is the numbering for the four dice mentioned : ( via ) QuestionI stink at math . I 'm stumped . Given the above information , I 'd like to be able to generate lists of integers ( `` dice '' ) given a number of dice . Such that , example output might look like so ( formatted , python console ) : The number of sides here is chosen just for example purposes , because it matches the other example given . The `` fairness '' of each die is really what I 'm looking for.I assure you this is n't homework . This is simply a determined geek , annoyed by a seemingly trivial puzzle that just wo n't leave me alone ... and for some reason , I ca n't seem to get it right . I 'm sure there 's some relatively trivial math , and a basic algorithm involved here , and that 's what I 'm looking for . What terminology should I search for , if this is obvious to you ? Because to me , it 's not . Ideally the solution would be in Python , but I can also read PHP , Javascript , some Ruby , quite well ."
"I 'm a newbie for MRO and having problem figuring out the logic for these outputs.Case 1 : Output : My question here is how super ( C ) is calling B.save ( ) .As per MRO : super ( C , self ) is not about the `` base class of C '' , but about the next class in the MRO list of C. But there is no B in MRO list of C.Case 2 : Output : Case 3 : Output : QuestionHow is the MRO affected if B is not inheriting from A , but object directly ? Can someone explain the reason behind this ?"
"I 've tried all possible solutions on several threads and I 'm still unable to fix the problem . I have the following code : models.pyforms.pyviews.pyjsMy address select its being populated based on customer selection using ajax call using select2 . After reading several threads I noticed that modelchoicefield expects a Address object so that 's why I 'm using the following code on my view before the form is being validated : post [ 'address ' ] = Address.objects.get ( id=post [ 'address ' ] ) but I 'm still getting the Select a valid choice . That choice is not one of the available choices . error I 'm using queryset=Address.objects.none ( ) , because I need an empty select"
"I want to transform the list [ `` A '' , '' B '' , '' A '' , '' A '' , '' B '' ] to the list [ `` AB '' , '' BA '' , '' AA '' , '' AB '' ] . I have tried to define a new list in which the first element is deleted and then add the strings of the lists together . After which I plan to delete the last element of the new list to get the result.But all I get is TypeError : 'list ' object can not be interpreted as an integerAny help is welcome.Edit : Thank you guys , all your solutions worked perfectly : )"
"I have a python script which I called via Yesterday it worked fine , but today I get the following error : I assume that it may have something to do with the new month starting , but I ca n't find a way to fix this ( strange ) problem . Some further information : The script runs on a raspberry pi . It uses the requests and the Adafruit_DHT library . I do n't do any time-regarding operations in the python script by myself . Could I kindly ask you to help me ? Many thanks in advance"
"Suppose I have a dataframe like this : what I wan na do is that when Knownvalue is between 0-10 , A is changed from 0 to 1 . And when Knownvalue is between 10-20 , B is changed from 0 to 1 , so on so forth.It should be like this after changing : Anyone know how to apply a method to change it ?"
I would like to keep the dtypes of the columns as int after the update for obvious reasons . Any ideas why this does n't work as expected ? The output looks like this : Thanks to anyone that has some advise
"In the documentation on data models it mentions the possibility to use a single item tuple as a singleton , due to it 's immutability.TuplesA tuple of one item ( a ‘ singleton ’ ) can be formed by affixing a commato an expression ... As far as I understand it in Python , a singleton functions similarly to a constant . It is a fixed value that maintains the same memory address so that you can test either equality or identity . For instance , None , True and False are all builtin singletons.However using a tuple defined this way seems impractical for awkward syntax , considering this kind of usage : Not to mention , it only functions as a singleton if you remember to index it.Meaning that you could easily do this : Given these limitations , is there any point to this singleton implementation ? The only advantage I see is that it 's simple to create . Other approaches I 've seen have been more complicated approaches ( using classes etc . ) but is there some other use for this I 'm not thinking of ?"
I found many queries on python function arguments but still left confused . Suppose I want to pass few arguments in some function . name & age arguments are positional arguments . sex is default argument followed by variable length of non keyword arguments which are few random capital alphabets . So now if I runIt gives me perfect output..But if I pass below arguments where I want to use default sex value instead of passing it . Output is not as expeceted.It should have taken the default value of sex i.e . 'M ' . I also tried to pass sex argument at the very last but it gave me syntax error . Is there any way to achieve what I want ?
"I 've read this article where the /^1 ? $ |^ ( 11+ ? ) \1+ $ / Perl regex is used to test if a number is prime or not.Process : If s matchs the regex , then it 's not prime . If it does n't , it 's prime.How would you translate that regex to Python 's re module ?"
"I have a set of many ( 10000+ ) items , from which have I have to choose exactly 20 items . I can only choose each item once . My items have profits , and costs , as well as several boolean properties ( such as colour ) . I 've read and worked through the tutorial at https : //developers.google.com/optimization/mip/integer_opt_cp and https : //developers.google.com/optimization/mip/integer_opt , but my constraints are slightly different than those presented there . Each item is represented as a tuple : as an exampleand the total set of items is a list of lists : My profits and costs are also lists : For each item chosen , it needs to have a minimum value , and a minimum of 5 items must have the property ( is_blue ) flag set to 1.I want to choose the 20 cheapest items with the highest value , such that 5 of them have the property flag set to 1.I 'm having trouble formulating this using google OR tools . I can get the set of items I 've chosen by : This works fine - it chooses the set of 20 items which maximise the profits subject to the cost constraint , but I 'm stuck on how to extend this to choosing items with the property ( is_blue ) set to true or false.Any help in formulating the constraints and objective would be really helpful . Thanks !"
"I have a weird problem . I can neither rename specific files , nor remove them . I get the FileNotFoundError.Similar questions have been asked before . The solution to this problem was using a full path and not just the filename . My script worked before using only the filenames , but using different files I get this error , even using the full path.It seems , that the filename is causing the error , but I can not resolve it.I get the following output : This file is existing if I use windows search in the folder . If I try to use the full path I also get an windows error not finding the file.I have also tried appending a unicode string u '' +filename to the strings , because it was suggested by an user.The pathlength is < 260 , so what is causing the problem ?"
"I am looking to parallelise numpy or pandas operations . For this I have been looking into pydata 's blaze . My understanding was that seemless parallelisation was its major selling point.Unfortunately I have been unable to find an operation that runs on more than one core . Is parallel processing in blaze available yet or currently only a stated aim ? Am I doing something wrong ? I am using blaze v0.6.5.Example of one function I was hoping to parallelise : ( deduplication of a pytables column too large to fit in memory ) Edit 1I have had problems following Phillip 's examples : My environment : But note , blaze seems to report a wrong version : With other data sources blaze seems to work :"
"I have a large project where at various places problematic implicit Unicode conversions ( coersions ) were used in the form of e.g . : ( Possibly other forms as well . ) Now I would like to track down those usages , especially those in actively used code.It would be great if I could easily replace the unicode constructor with a wrapper which checks whether the input is of type str and the encoding/errors parameters are set to the default values and then notifies me ( prints traceback or such ) ./edit : While not directly related to what I am looking for I came across this gloriously horrible hack for how to make the decode exception go away altogether ( the decode one only , i.e . str to unicode , but not the other way around , see https : //mail.python.org/pipermail/python-list/2012-July/627506.html ) .I do n't plan on using it but it might be interesting for those battling problems with invalid Unicode input and looking for a quick fix ( but please think about the side effects ) : ( An internet search for codecs.register_error ( `` strict '' revealed that apparently it 's used in some real projects . ) /edit # 2 : For explicit conversions I made a snippet with the help of a SO post on monkeypatching : This only affects explicit conversions using the unicode ( ) constructor directly so it 's not something I need./edit # 3 : The thread `` Extension method for python built-in types ! '' makes me think that it might actually not be easily possible ( in CPython at least ) ./edit # 4 : It 's nice to see many good answers here , too bad I can only give out the bounty once.In the meantime I came across a somewhat similar question , at least in the sense of what the person tried to achieve : Can I turn off implicit Python unicode conversions to find my mixed-strings bugs ? Please note though that throwing an exception would not have been OK in my case . Here I was looking for something which might point me to the different locations of problematic code ( e.g . by printing smth . ) but not something which might exit the program or change its behavior ( because this way I can prioritize what to fix ) .On another note , the people working on the Mypy project ( which include Guido van Rossum ) might also come up with something similar helpful in the future , see the discussions at https : //github.com/python/mypy/issues/1141 and more recently https : //github.com/python/typing/issues/208./edit # 5I also came across the following but did n't have yet the time to test it : https : //pypi.python.org/pypi/unicode-nazi"
"I 'm currently writing a very simple game using python and pygame . It has stuff that moves . And to make this stuff move smoothly I arranged the main game loop as said in Fix Your Timestep , with interpolation.Here is how I handle interpolation now.The lerp_xy ( ) function and data typesThe data types are of course temporary , but I still expect they will have the x and y attributes in the future . update : lerper.alpha is updated each frame.Ship is added as a single object - it 's the player ship . Stars are added as a list . Bullets are added as a dict { id , Bullet } , since bullets are added and removed all the time , I have to track which bullet is which , interpolate if both are present and do something if it just has been added or deleted.Anyway this code right here is crap . It grew as I added features , and now I want to rewrite it to be more generic , so it can continue growing and not become an unpythonic stinky pile of poo.Now I 'm still pretty new to Python , though I feel pretty comfortable with list comprehensions , generators and coroutines already.What I have the least experience with is the OO side of Python , and designing an architecture of something bigger than a 10-line hacky disposable script.The question is not a question as in something I do n't know and ca n't do anything about it . I 'm sure I 'm capable of rewriting this pretty simple code that will work in some way close to what I want.What I do want to know , is the way experienced Python programmers will solve this simple problem in a pythonic way , so I ( and of course others ) could learn what is considered an elegant way of handling such a case among Python developers.So , what I approximately want to achieve , in pseudocode : However do n't let that pseudocode stop you if you have a better solution in mind = ) So . Make all game objects as separate/inherited class ? Force them all to have id ? Add them all as list/dict lerper.add ( [ ship ] ) ? Make a special container class inheriting from dict/whatever ? What do you consider an elegant , pythonic way of solving this ? How would you do it ?"
"I am using the officer and rvg packages to get plots from R into MS PowerPoint as editable vector graphics . Reproducible example below.I am looking for a way to implement an equivalent solution with python , preferably using matplotlib . The critical part is not the creation of slides from the IDE but rather the editable vector graphics part , i.e . plots should end up in PowerPoint as grouped objects comprised of a range of simple powerpoint geometries such as lines , squares , and text fields.R example : The resulting chart is completely editable :"
I have a dataframe and i want it to select a few columns and convert it into Dictionary in the a certain mannerDataframe : and here 's the output I wantI have tried thisbut this does not include Pid in the list of valuesCan someone explain why ?
"I have been able to used the correlated update construct shown in the docs here to update one column in a table.For example : Which produces SQL like : Is it possible to use the Declaritive or Query Api to update multiple columns for a selection ? I am trying to emulate something like the following in PostgreSQL : EDIT : It seems that formatting this a single statement may not be possible with the current SQLAlchemy api.However , a workaround using two selects seems possible using the solution from here ."
"So , this should be a comment to this thread , but it 's apparently closed , so here it goes . I 've been playing around quite successfully with matplotlib and numpy and mencoder , as has been suggested around here . I have since adopted Voki Codder buffer to stdin solution , which speeds the whole process up considerably . The thing is , I could n't find any documentation on the -format= '' bgra '' part of the command . This means that the bytes are from right to left blue green red alpha , right . Do they have to be uint32 , or something else . The problem is I 'm plotting colormaps of floats , so I 'm trying to convert them to grayscale , but I 'm getting lots of weird patterns which make me strongly believe I 'm doing something wrong . I wrote this function to convert from floats to uint32 within a range . But the result is not why I expected , am I doing something terribly stupid ?"
"I have an array/set with unique positive integers , i.e.And an array containing multiple elements sampled from this previous array , such asI want to map the values of the array A to the position of which those values occur in unique.So far the best solution I found is through a mapping array : The above assigns to each element the index on the array , and thus , can be used later to map A through advanced indexing : Which already gives me the proper solution . However , if the unique numbers in unique are very sparse and large , this approach implies creating a very large table array just to store a few numbers for later mapping.Is there any better solution ? NOTE : both A and unique are sample arrays , not real arrays . So the question is not how to generate positional indexes , it is just how to efficiently map elements of A to indexes in unique , the pseudocode of what I 'd like to speedup in numpy is as follows , ( assuming unique is a list in the above pseudocode ) ."
"In python , I can so the next valid code : This allowed me use one slash ( '\ ' ) sign instead of two.Is there any equivalent in cpp ?"
So according to the Zen of Python ... Explicit is better than implicit ... Sparse is better than dense ... Readability counts ... but then again Flat is better than nested ... so then which is pythonic ? or I 'm just a Java programmer learning Python so I find this pythonic stuff interesting since there is no analog in the Java world AFAIK .
"I 'm new to Pyparsing ( and pretty new to Python ) . I have tried to reduce my problem down to the simplest form that will illustrate what 's going wrong ( to the point where I probably would n't need Pyparsing at all ! ) Suppose I 've got a string consisting of letters and numbers , such as `` b7 z4 a2 d e c3 '' . There 's always a letter , but the number is optional . I want to parse this into its individual elements , and then process them , but where there is a bare letter , with no number , it would be handy to change it so that it had the `` default '' number 1 after it . Then I could process every element in a consistent way . I thought I could do this with a setparseAction , as follows : Unfortunately , the t.append ( ) does n't do what I 'm expecting , which was to add a `` 1 '' to the list of parsed tokens . Instead , I get an error : TypeError : 'str ' object is not callable.I 'm probably just being really thick , here , but could one of you experts please set me straight.ThanksSteve"
"I 'm working on Ubuntu 14.04 with Python 3.4 ( Numpy 1.9.2 and PIL.Image 1.1.7 ) . Here 's what I do : Seems to me like Python runs out of memory all of a sudden . If that is the case - how can I allocate more memory to Python ? As I can see from htop my 32GB memory capacity is not even remotely depleated.You may download the TIFF image here.If I create an empty boolean array , set the pixels explicitely and then apply the summation - then it works : But this `` workaround '' is not very satisfactory as copying every pixel takes way too long - maybe there is a faster method ?"
When i run a python script in terminal it works just fine . I then proceed to run the exact same script via PHP 's shell_exec from localhost.The error it gives is : It does n't recognize the modules that are installed . Needless to say that it works just fine from windows on localhost .
"I want to format array of numbers with same width using f-strings . Numbers can be both positive or negative.Minimum working example The result is Due to the negative sign , the numbers are not printed off with the same width , which is annoying . How can I get the same width using f-strings ? One way that I can think of is converting number to strings and print string . But is there a better way than that ?"
Following PEP 526 I 'm wondering how to properly type hint an instance or class attribute that is decorated by a property . Do I type hint the underlying attribute or the name or the property ? Example of typing the attribute : Or typing the property : Or some other way ? Is it the same for instance and class variables ?
"I have written a Python module including a submodule written in C : the module itself is called foo and the C part is foo._bar . The structure looks like : foo/__init__.py imports _bar to augment it , and the useful stuff is exposed in the foo module . This works fine when it 's built , but obviously wo n't work in uncompiled form , since _bar does n't exist until it 's built.I 'd like to use Sphinx to document the project , and use the autodoc extension on the foo module . This means I need to build the project before I can build the documentation.Since I build with distutils , the built module ends up in some variably named dir build/lib.linux-ARCH-PYVERSION — which means I ca n't just hard-code the directory into a Sphinx ' conf.py.So how do I configure my distutils setup.py script to run the Sphinx builder over the built module ? For completeness , here 's the call to setup ( the 'fake ' things are custom builders that subclass build and build_ext ) :"
I 'm trying out the django aggregates . I 'm dynamically calling these aggregates and need to assign and alias them on the fly . I want the resulting alias to be the name of the original field.For example : I 'm expecting this : But getting this : Is there a way I can dynamically assign the alias in this case ?
"I am writing some code that takes binary data from Python , Pipes it to C++ , does some processing on the data , ( in this case calculating a mutual information metric ) and then pipes the results back to python . While testing I have found that everything works fine if the data I send is a set of 2 arrays with dimensions less than 1500 X 1500 , but if I send 2 arrays that are 2K X 2K I get back a lot of corrupted nonsense . I currently believe the algorithmic portion of the code is fine because it provides the expected answers during testing with small ( < =1500 X1500 ) arrays . That leads me to believe that this is an issue with either the stdin or stdout piping . That maybe I ’ m passing some intrinsic limit somewhere.The Python Code and C++ code are below.Python Code : C++ Code : Main function : and in case you need it the actual processing code : with arrays that return something that makes sense I get output bounded between 0 and 1 like this : ( 0.0 , 0.0 , 0.0 , 0.7160627908692593 , 0.6376472316395495 , 0.5728801401524277 , ... with the 2Kx2K or higher arrays I get nonesense like this ( even though the code clamps the values between 0 and 1 ) : ( -2.2491400820412374e+228 , -2.2491400820412374e+228 , -2.2491400820412374e+228 , -2.2491400820412374e+228 , -2.2491400820412374e+228 , ... I would like to know why this code is corrupting the data set after it is assigned between 0.0 and 1 , and whether or not it is a piping issue , a stdin/stdout issue , a buffer issue of some sort , or a coding issue I am simply not seeing.Update I tried passing the data in smaller chunks using the code that Chris suggested with no luck . also of note is that I added a catch for ferror on stdout and it never got tripped so I am pretty sure that the bytes are at least making it to stdout . Is it possible that something else is writing to stdout somehow ? maybe an extra byte making its way into stdout while my program is running ? I find this doubtful as the errors are appearing consistently on the 4th fwrite read in the 10th entry.Per Craig 's request here is the full C++ code ( the full Python Code is already posted ) : it is sitting in 3 files : main.cppMutualInformation.hMutualInformation.cppSOLVED By 65026502 's answer below solved my problem . I needed to explicitly tell Windows to use a binary mode for stdin / stdout . to do that I had to include 2 new header files in my main cpp file . add the following lines of code ( modified away from 6502 's POSIX versions because Visual Studio complained ) to the beginning of my main functionand then add these lines to my Python code :"
"I want to use django formset in my class based view.this is the view , and this is my form , and here is my 'RequestPassingFormMixin ' : but i am getting the following error , this error indicating the template where i have rendered the formset , so i am giving the template also ( only the part where i have rendered the forest ) , I am not used to using django formset , as far i guess , its not a formset error , or it might be , maybe I am missing something else here , but after a long findings , I ca n't figure it out ..."
"The Python Doc for Comparisons says : Comparisons can be chained arbitrarily , e.g. , x < y < = z is equivalent to x < y and y < = z , except that y is evaluated only once ( but in both cases z is not evaluated at all when x < y is found to be false ) .And these SO questions/answers shed some more light on such usage : Python comparison operators chaining/grouping left to right ? What does `` evaluated only once '' mean for chained comparisons in Python ? , in particular the currently-accepted answerSo something like ( contrived example ) : only asks for input once . This makes sense . And this : only asks for Val2 if Val1 is between 1 & 10 and only prints `` woo ! '' if Val2 is also between 10 and 20 ( proving they can be 'chained arbitrarily ' ) . This also makes sense.But I 'm still curious how this is actually implemented/interpreted at the lexer/parser/compiler ( or whatever ) level.Is the first example above basically implemented like this : where x really only exists ( and is actually essentially unnamed ) for those comparisons ? Or does it somehow make the comparison operator return both the boolean result and the evaluation of the right operand ( to be used for further comparison ) or something like that ? Extending analysis to the second example leads me to believe it 's using something like an unnamed intermediate result ( someone educate me if there 's a term for that ) as it does n't evaluate all the operands before doing the comparison ."
"I am using Apache Beam in Python with Google Cloud Dataflow ( 2.3.0 ) . When specifying the worker_machine_type parameter as e.g . n1-highmem-2 or custom-1-6656 , Dataflow runs the job but always uses the standard machine type n1-standard-1 for every worker.Does anyone have an idea if I am doing something wrong ? Other topics ( here and here ) show that this should be possible , so this might be a version issue.My code for specifying PipelineOptions ( note that all other options do work fine , so it should recognize the worker_machine_type parameter ) :"
"I am performing a large number of these calculations : A == A [ np.newaxis ] .Twhere A is a dense numpy array which frequently has common values.For benchmarking purposes we can use : When I perform this calculation , I run into memory issues . I believe this is because the output is n't in more efficient bitarray or np.packedbits format . A secondary concern is we are performing twice as many comparisons as necessary , since the resulting Boolean array is symmetric.The questions I have are : Is it possible to produce the Boolean numpy array output in a more memory efficient fashion without sacrificing speed ? The options I know about are bitarray and np.packedbits , but I only know how to apply these after the large Boolean array is created.Can we utilise the symmetry of our calculation to halve the number of comparisons processed , again without sacrificing speed ? I will need to be able to perform & and | operations on Boolean arrays output . I have tried bitarray , which is super-fast for these bitwise operations . But it is slow to pack np.ndarray - > bitarray and then unpack bitarray - > np.ndarray . [ Edited to provide clarification . ]"
"I have a list of values , and a dictionary . I want to ensure that each value in the list exists as a key in the dictionary . At the moment I 'm using two sets to figure out if any values do n't exist in the dictionaryIs there a more pythonic way to test this though ? It feels like a bit of a hack ?"
"This is a very silly question , but I 'm running some tasks and catching their errors by : I want the error message as a String because I 'm using a UI and I want to display the error in a window.The problem can be replicated as : However , if I try to catch the error and print its message ( which I was hoping to be something like `` MemoryError '' : The only output I get is `` Catched ! '' . This is so stupid , I was doing some work with UI 's and threading , and took me a while to realise that the problem was a memory error with no message at all.Is the MemoryError the only Exception that is translated to an empty string ? Because if it is the case I can check it . If not , how to get its message as a String ?"
"I need to accomplish the following related to privileges : I have 3 users : Each of the users has the following documents with associated access settings : User A has User B as a contact but is not a contact of User C ( User B and User C are contacts ) .Thus , User A would be able to view the following : I am interested to learn how these privileges would be handled . I am also seeking any documentation or articles that would help me hit the ground running ."
"This addresses `` a specific programming problem '' from On-TopicI am working on an interview question from Amazon Software InterviewThe question is `` Given a triangle of integers , find the path of the largest sum without skipping . `` My question is how would you represent a triangle of integers ? I looked this up on Triangle of Integers and saw that a triangle of integers looked something like What is the best way ( data structure ) to represent something like this ? My idea was having something like Is this the best way to represent this triangle integer structure ? I thought about using a 2 dimensional matrix structure but those have to have arrays of the same size ."
"I have an assignment where I need to approximate Pi in a computationally efficient manner . Here is my strategy : I use a unit circle , the angle bisector of an isoceles triangle , and the definition of sin . I drew a diagram : For example , if I want to use an hexagon ( 6 points/6 sides ) , I simply need to compute a : ( 0.5*sin ( 2*pi/2*x ) and multiply it by ( 2*x ) . Finally , since Pi = Circumference/Diameter , then my approximation of Pi = polygon perimeter ( since Diameter = 1 ) . Essentially : It works , and I think it 's as efficient as it gets , no ? Thank you for your time ! EDIT : to avoid circularity , I redid it using Archimedes algorithm using only the Pythagorean theroem : Code :"
"I have been using the following snippet to silence ( redirect output from ) C code called in my Python script : The idea is to redirect the fd associated with stdout and replace it with one associated with an open null device . Unfortunately , it does not work as expected under Python 3 : Under Python 2.7 and 3.3 with unbuffered output it does work . I am unsure what the underlying cause is , however . Even if stdout is buffered the call to sys.saved_stream.flush ( ) should end up calling fflush ( stdout ) at the C level ( flushing the output to the null device ) .What part of the Python 3 I/O model am I misunderstanding ?"
I have a html file like this in a subdirectory the_filesI want to extract the string in each tag and print it like this on terminalI want to ignore the first line of < div class='log ' > start < /div > .My code so farThis code produces result as follows Is there a way to fix this ? Thank you for your help .
I am following this procedure link to upload my mongodump to s3.bash scriptaws_s3.pyBut keep getting this error : Did a bit of my research and found out that its some kind of bug in boto.How to proceed further with this ?
"Say I have 2 numpy 2D arrays , mins , and maxs , that will always be the same dimension as one another . I 'd like to create a third array , results , that is the result of applying linspace to max and min value . Is there some `` numpy '' /vectorized way to do this ? Example non-vectorized code is below to show results I would like ."
"A colleague recently sent me a bash script that included some inline Python . The command he used was of the form : The command worked just fine , but I asked why he did n't use python -c. On further investigation , we were unable to translate this to a form that python -c would accept as it appears that Python disallows statements prior to a for loop even when delimited by a semicolon . The first example below shows that you can import , and print out the imported object . The second example shows that you can use a for loop and print from the for loop . The third example combines the first two , and results in a SyntaxError . And finally , the forth example shows the SyntaxError results when using an expression prior to the for loop : In hindsight , the SyntaxError is not that surprising given that the code is not valid when saved in a script , and that the grammar for statements combined with whitespace restrictions can impose this type of limitation . That being said , is there a way to allow a statement prior to a compound statement through python -c ?"
"ContextI am running scrapyd 1.1 + scrapy 0.24.6 with a single `` selenium-scrapy hybrid '' spider that crawls over many domains according to parameters . The development machine that host scrapyd 's instance ( s ? ) is an OSX Yosemite with 4 cores and this is my current configuration : Output when scrapyd starts : EDIT : Number of cores : ProblemI would like a setup to process 300 jobs simultaneously for a single spider but scrapyd is processing 1 to 4 at a time regardless of how many jobs are pending : EDIT : CPU usage is not overwhelming : TESTED ON UBUNTUI have also tested this scenario on a Ubuntu 14.04 VM , results are more or less the same : a maximum of 5 jobs running was reached while execution , no overwhelming CPU consumption , more or less the same time was taken to execute the same amount of tasks ."
"Let me start with the example code : On my local machine , and other devs ' machines , this works as expected and prints out : However , on our production server , we get : Which is 32-bit integer overflow , despite it being an int64.The production server is using the same versions of python , numpy , pandas , etc . It 's a 64-bit Windows Server 2012 OS and everything reports 64-bit ( e.g . python -- version , sys.maxsize , plastform.architecture ) .What could possibly be causing this ?"
"Sorry if this post is a bit confusing to read this is my first post on this site and this is a hard question to ask , I have tried my best . I have also tried googling and i can not find anything.I am trying to make my own command line like application in python and i would like to know how to split a string if a `` \ '' is not in front of a space and to delete the backslash.This is what i mean.When I split c with a space it keeps the backslash however it removes the space.This is how I want it to be like : If someone can help me that would be great ! Also if it needs Regular expressions could you please explain it more because I have never used them before.Edit : Now this has been solved i forgot to ask is there a way on how to detect if the backslash has been escaped or not too ?"
"I 'm attempting to integrate pyinstaller with updating feature for private repo ’ s . My question , is there a way to integrate pyupdater with free alternatives such as : bitbucket private repos ? Pyupdater tends to work for public repo ’ s but I can not workout how I can achieve this for private repo ’ s.Config file : Creating an ssh is easy enough : ssh-keygen -t rsa -C `` youremail @ example.com '' So…Main.py"
"I have a class Foo with a method isValid . Then I have a method bar ( ) that receives a Foo object and whose behavior depends on whether it is valid or not . For testing this , I wanted to pass some object to bar whose isValid method returns always False . For other reasons , I can not create an object of Foo at the time of testing , so I needed an object to fake it . What I first thought of was creating the most general object and adding the attribute isValid to it , for using it as a Foo . But that did n't quite work : I found out object does n't have a __dict__ , so you can not add attributes to it . At this point , the workaround I am using is creating a type on the fly for this purpose and then creating an object of that type : But this seems too long a shot . There must be some readily available general type that I could use for this purpose , but which ?"
I wrote the following code.This produces the following output.You can see that the newline is lost . I wanted the copyright notice to appear on the next line.How can I preserve the new lines in the version output message ? I still want argparse to compute how the output of python foo.py -h should be laid out with all the auto-wrapping it does . But I want the version output to be a multiline output with the newlines intact .
"I have a plain python ( non-Django ) project where I 'm trying to tie Raven into the logging setup.Under our current setup , we use a simple logging config : The output is then redirected to a log file ; this produces a nice , verbose log that we can look through when we need to.We now want to add Raven 's error logging , tying it into our current logging setup so that logging.error calls also result in a message being sent to the Sentry server . Using the following code : Errors are being successfully sent to Sentry , but I 'm now getting only a single line of file output : All other file output -- from logging.debug to logging.error -- is being suppressed.If I comment the setup_logging line , I get file output but no Sentry errors . What am I doing wrong ?"
"I have a dictionary S as : And an array D1_inv as : I need to obtain a product of all the items in S and D1_inv . For example , for S [ 1 ] : and for S [ 2 ] : Can somebody help me create a loop so that I can store all these products in a dict like S ?"
"I would like to invoke the code object again in the exit ( ) method if it raises an exception ( maybe several times , maybe with delay ) . I know it is very easy to do with a decorator , but my motivation is that sometimes I want to repeat just some fragment of code that I do n't want to extract to a separate function and decorate it . I 'm looking for something along these lines : It would used like so : and the expected output would be : I could find a way to get hold of the code object . None of the context manager attributes seem to reference it ( I guess it is not really needed , because it 's job is just to do stuff before and after ) .Is it possible to do it ?"
"SetupTwo tables : schools and students . The index ( or keys ) in SQLite will be id and time for the students table and school and time for the schools table . My dataset is about something different , but I think the school-student example is easier to understand.What do I need to do ? I have a bunch of functions that operate over pandas dataframes and create new columns that should then be inserted in either the schools or the students table ( depending on what I 'm constructing ) . A typical function does , in order : Queries columns from both SQL tablesUses pandas functions such as groupby , apply of custom functions , rolling_mean , etc . ( many of them not available on SQL , or difficult to write ) to construct a new column . The return type is either pd.Series or np.arrayAdds the new column to the appropriate dataframe ( schools or students ) These functions were written when I had a small database that fitted in memory so they are pure pandas.Here 's an example in pseudo-code : Because my dataset is really large , I 'm not able to call these functions in memory . Imagine that schools are located in different districts . Originally I only had one district , so I know these functions can work with data from each district separately.A workflow that I think would work is : Query relevant data for district iApply function to data for district i and produce new columns as np.array or pd.SeriesInsert this column at the appropriate table ( would fill data for district i of that columnsRepeat for districts from i = 1 to KAlthough my dataset is in SQLite ( and I 'd prefer it to stay that way ! ) I 'm open to migrating it to something else if the benefits are large.I realize there are different reasonable answers , but it would be great to hear something that has proved useful and simple for you . Thanks !"
"I have a script which i 'm using for copy purpose from one location to another location and the file beneath the directory structure are all .txt files.This script just evaluates the file size on the source and only copy if the file-size is not zero byte . However , I need to run this script in a cron after a certain intervals to copy the any incremented data.So , I need to know how to copy only the file content which are updated on the source file and then update the destination only for the new-contents and not just overwrite if its already present at destination.Code : I 'm looking if there is way to use shutil ( ) in way rsync works or if there is an alternative way to the code I have.In a nutshell I need to copy only files ones if it 's not already copied and then only copy the delta if source gets updated.Note : The Info_month = datetime.datetime.now ( ) .strftime ( `` % B '' ) is mandatory to keep as this determines the current directory by month name.Edit : Just having another raw idea if we can use filecmp with shutil.copyfile module to compare files and directories but i 'm not getting how to fit that into the code ."
"I 'm trying to do some symbolic matrix calculations with sympy . My goal is to obtain a symbolic representation of the result of some matrix computations . I 've run into some problems which I have boiled down to this simple example , in which I try to evaluate the result of a exponentiating a specified matrix and multiplying it by an arbitrary vector.In contrast , if I directly create the identity matrix and then multiply it by v , then there is no problem : One thing that I 've noted is that the two identity matrices are of different classes : I also found that calling the as_mutable ( ) method provided a work-around.Is it always necessary to put as_mutable ( ) calls throughout one 's linear algebra calculations ? I 'm guessing not , and that instead these errors suggest that I 'm using the wrong strategy to solve my problem , but I ca n't figure out what the right strategy would be . Does anyone have any pointers ? I have read the documentation page on Immutable Matrices but I could still use some help understanding how their differences with standard mutable matrices are important here , and why some operations ( e.g . sympy.exp ) convert between these different classes ."
Any quick way to achieve the below output pls ? Input : Output : Am again running into loops and a very ugly code to make it work . If there is an elegant way to achieve this pls ? Thank you !
"Given a torch 's nn.Module with a pre-forward hook , e.g . It 's possible to let an input tensor go through some manipulation before going to the actual forward ( ) function , e.g . How could we pass the arguments ( *args or **kwargs ) that the pre-forward hook needs but not accepted by the default forward ( ) function ? Without modification/overriding the forward ( ) function , this is not possible :"
"I have a integer that needs to be split up in to bins according to a probability distribution . For example , if I had N=100 objects going into [ 0.02 , 0.08 , 0.16 , 0.29 , 0.45 ] then you might get [ 1 , 10 , 20 , 25 , 44 ] .In reality , N and my number of bins are very large , so looping is n't really a viable option . Is there any way I can vectorize this operation to speed it up ?"
"My python application launches a subprocess that creates a pyglet window . When the pyglet window opens , it is in front of all other windows , and takes keyboard focus . I 'd like my pyglet window to open in the background , and not take focus . Is this possible ? Stripped-down version of the code I 'm using : I 'm using Windows 7 , in case that makes a difference.."
"I am trying to generate a learned timeseries with an LSTM RNN using Keras , so I want to predict a datapoint , and feed it back in as input to predict the next one and so on , so that I can actually generate the timeseries ( for example given 2000 datapoints , predict the next 2000 ) I am trying it like this , but the Test score RMSE is 1.28 and the prediction is basically a straight lineWhat am I doing wrong ?"
"By redefining the yield statement to be an expression in PEP 342 -- Coroutines via Enhanced Generators powerful new functionality was added to Python . David Beasley has an excellent presentation on Python coroutines A Curious Course on Coroutines and Concurrency.As the PEP states , A yield expression 's value is None whenever the generator is resumed by a normal next ( ) call . To instantiate the generator either next ( ) or send ( None ) must be called ( i.e . can not send non-None value initially ) .Is there any advantage in calling next ( ) vs send ( None ) ? next ( ) is a Built_in function , so perhaps that 's a factor , but there does n't seem to be any other differences . I 'm somewhat surprised , it would seem more Pythonic to add an optional variable to next than add a new function that does the same thing . Am I missing something ? Here is a simple coroutine to keep a running total of numbers entered , by sending them to the coroutine ."
I need to find a way to get a list of all of an SQLAlchemy models hybrid properties.For relationships on a Person instance I can do something like : Is there something like :
I am trying to find the exact digit of the expression . which I am unable to find it . Below is the snippet code I have tried.When I have tried with numpy I was getting like : But I couldnt get the exact digit . Can anyone help me out with this . It would be a great help for me.Thanks in Advance .
How can I get a task by name ? It should be possible with the REST API ( https : //developers.google.com/appengine/docs/python/taskqueue/rest/tasks/get ) . But I would prefer something like task_queue.get_task_by_name ( 'foobar ' ) . Any ideas ? Did I miss something ?
"I do n't have a clue why is this happening . I was messing with some lists , and I needed a for loop going from 0 to log ( n , 2 ) where n was the length of a list . But the code was amazingly slow , so after a bit a research I found that the problem is in the range generation . Sample code for demonstration : The outputThe number of tests is low ( I did n't want this to be running more than 10 minutes ) , but it already shows that range ( log ( n , 2 ) ) is orders of magnitude slower than the counterpart using just the logarithm of an integer . This is really surprising and I do n't have any clue on why is this happening . Maybe is a problem on my PC , maybe a Sage problem or a Python bug ( I did n't try the same on Python ) .Using xrange instead of range does n't help either . Also , if you get the number with .n ( ) , test 1 runs at the same speed of 2.Does anybody know what can be happening ? Thanks !"
"Searching for this topic I came across the following : How to represent integer infinity ? I agree with Martijn Peeters that adding a separate special infinity value for int may not be the best of ideas.However , this makes type hinting difficult . Assume the following code : However , the code behaves everywhere just the way as it should . And my type hinting is correct everywhere else.If I write the following instead : I can assign math.inf without a hitch . But now any other float is accepted as well.Is there a way to properly constrain the type-hint ? Or am I forced to use type : ignore each time I assign infinity ?"
"What is going on here ? Ignore the senselessness of the code . This is about the error message , `` iterable , not map '' . Maps are iterables , are they not ? And if I only replace None with str , the whole thing works fine : So now Python does n't have any issue with a map there after all.This happens in my Python 3.6.1 . My Python 3.5.2 instead raises the expected TypeError : 'NoneType ' object is not callable . And googling `` must be an iterable , not map '' finds no results at all . So apparently this is something introduced just recently.Is this just a Python bug ? Or is there some sense to this ? Update : Reported as bug now , as suggested ."
"I 'm having problems getting the NumPy C API to properly initialize . I think I 've isolated the problem to calling import_array from a different translation unit , but I do n't know why this should matter.Minimal working example : header1.hppfile1.cppfile2.cppCompiler commands : I 've tested this with Clang 3.6.0 , GCC 4.9.2 , Python 2.7 , and Python 3.4 ( with a suitably modified wrap_import_array because this is different between Python 2.x and 3.x ) . The various combinations all give the same result : if I do n't call loc_initialize , the program will segfault in the PyArray_DescrFromType call . I have NumPy version 1.8.2 . For reference , I 'm running this in Ubuntu 15.04.What baffles me most of all is this C++ NumPy wrapper appears to get away with calling import_array in a different translation unit.What am I missing ? Why must I call import_array from the same translation unit in order for it to actually take effect ? More importantly , how do I get it to work when I call import_array from a different translation unit like the Boost.NumPy wrapper does ?"
"I have two dictionaries . I want to merge these dictionaries such that the value for any key in the resultant dictionary is the minimum of the values for the key in the two dictionaries used to merge.Is there a cool one liner do so ? If not , what is the most elegant way of doing this ?"
"Here is my decorator code . I 'm getting UnboundLocalError for some reason but I could n't find it.So , I thought maybe it 's better to post here . I might be missing something.Thanks ."
How do I make sure that a method implementing an abstract method adheres to the python static type checks . Is there a way in pycharm to get an error if the return type is incorrect for the implemented method ? So for the above code I would want to get some sort of a hint that there is something wrong with my chihuahua
"I have a few files with generalized extensions , such as `` txt '' or no extension at all . I 'm trying to determine in a very quick manner whether the file is json or a csv . I thought of using the magic module , but it does n't work for what I 'm trying to do . For example : Is there a better way to determine if something is json or csv ? I 'm unable to load the entire file , and I want to determine it in a very quick manner . What would be a good solution here ?"
"I am using a reverse proxy to enable ssl on a custom domain . The proxy works fine on the public pages , however when the user attempts to access a login_required directory the login url shifts them over to my appspot domain after sign-in.Is there a way to keep users on the custom domain ? Here is my login handler : I 've tried overriding the destination url in the users.create_login_url call - the function still returns a login_url with the appspot domain as the 'continue ' parameter , like this : '' https : //appspot_domain/_ah/login_redir ? claimid=https : //www.google.com/accounts/o8/id & continue=https : //appspot_domain '' I attempted simply rewriting the returned login_url and replacing the 'continue ' parameter with my custom domain , but this resulted in a 404 error.Any thoughts ? Thanks in advance !"
"I 'm trying to write text to file , but I have other text I need to include besides the targeted string . When I 'm looping over the targeted strings it is printed with quotes as the quotes are needed for other text . How to remove quotes from a string that I 'm inserting each loop ? Output : Need to have stuff before 'random ' and after each loop string.Desired output : Need to have stuff before random and after each loop string ."
"I 'm unfamiliar with asyncore , and have very limited knowledge of asynchronous programming except for a few intro to twisted tutorials.I am most familiar with threads and use them in all my apps . One particular app uses a couchdb database as its interface . This involves longpolling the db looking for changes and updates . The module I use for couchdb is couchdbkit . It uses an asyncore loop to watch for these changes and send them to a callback . So , I figure from this callback is where I launch my worker threads . It seems a bit crude to mix asynchronous and threaded programming . I really like couchdbkit , but would rather not introduce issues into my program.So , my question is , is it safe to fire threads from an async callback ? Here 's some code ... Update : After looking over this more , I have an additional question for the couchdbkit gurus . There will potentially be hundreds of threads using the database . As you can see in my code sample , I am instantiating a couchdbkit.Database object per thread . I think this may be wasteful . So , is it ok for a single database object to be used globally among the threads ?"
"I have big gzip compressed files . I have written a piece of code to split those files into smaller ones . I can specifythe number of lines per file . The thing is I recently increased the number of line per splitto 16,000,000 , and when I process bigger files , the split wont happen . Sometimes a smaller file is successfully produced , sometimes one is produced but weighs only 40B or 50B , which is a failure . I tried to catch an exception for that by looking at those raised in the gzipcode . So my code looks like this : The thing is when the content is too high , related to the number of lines , I often get the `` UNEXPECTED ERROR '' message . So I have no idea which kind of error is thrown here.I finally found that the number of lines were the problem , and it appears python 's gzip fails at writing such amount of data in one file at once . Lowering the number of lines per split down to 4,000,000 works . However I would like to split the content and to write sequencially to a file to make sure that even high data content get to be written.So I would like to know how to find out the maximum number of characters that can be written for sure in one go in a file using gzip , without any failure . EDIT 1So I caugth all remaining exceptions ( I did not know that it was possible to simply catch Exception sorry ) : The error is about int size . I never thought I would have exceeded the int size one day : Ok so the max size of an int being 2,147,483,647 , my chunk of data is about 3,854,674,090 according to my log . This chunk is a string to which I applied the __len__ ( ) function.So as I planned to do , and as Antti Haapala suggested , I am about to read smaller chunks at a time in order to sequencially write them to smaller files ."
"I 'm using networkx and matplotlib to draw a graph of a network . I 've associated with each node a label in the form of a float ( up to two decimal points ) . I was hoping for the labels to be more visible in the graph . Is there any sort of workaround that will allow for better label visibility ? Updates : I found a similar question here , and have tried to apply the solution . The solution works pretty badly as it turned out . The code is as follows :"
"I have two list of album names , ordered by some score.How can I calculate the change of list order and get something like"
"Pretty confused about this because I 've verified correct logical output for small enough testcases ( N = 20 ) . I try doing N = 10,000 numbers and the program just hangs and I do n't understand why ... I 've implemented the algorithm as simply as I can . Also , calling sorted ( data ) on my N = 10k list seems to work almost instantly . So I 'm convinced my algorithm just gets stuck somewhere . Here is the code : So I 'm pretty lost as to why this could be happening . Thanks for any help ."
"I want to remove the whitespace at the end of 'Joe ' I tried the rstrip method , but it did n't workMy only solution was to concatenate the stringWhat am I missing ?"
"When I use .ix with a DataFrame , is there any way to force pandas to always return a DataFrame ? For example , if I run the following lines , then x will be a DataFrame , because 0 appears twice in the index , and y will be a Series , because 1 is unique in the index . I want y to be a DataFrame too ( because I 'm using iterrows ( ) , which is n't defined for Series , on the result ) .I can check the type of whatever .ix returns and convert it to a DataFrame if needed , but I thought there 'd be a better way.Note : As of Pandas v0.20 , .ix indexer is deprecated in favour of .iloc / .loc ."
"The Whole PointI am attempting to make a fairly basic website with Flask ( noob ! ) and am running into trouble with the user login system . I 've decided that I want to use Flask-Login , Flask-BrowserID ( Mozilla Persona ) and SQLAlchemy . I am going to have Persona be the part that takes care of storing user passwords and such , I am going to use Flask-Login once the user has been authenticated to keep track of their sessions and I am going to use SQLAlchemy to store everything in an sqlite3 db . I 've done a lot of bouncing around and I think I have almost finished these features , but I can not seem to get back a specific error.Update 1Based on the comment by davidism , I had to add db.Model to the User class . Unfortunately , that solved the first error , but now there is a new one to deal with . Traceback found below.The QuestionWhat gives ? I am obviously missing something , but I can not seem to find what that is.Resources I have been working withFlask-BrowserID : https : //github.com/garbados/flask-browserid/wikiFlask-Login : http : //flask-login.readthedocs.org/en/latest/Flask-SQLAlchemy : http : //pythonhosted.org/Flask-SQLAlchemy/quickstart.html # a-minimal-applicationIs it possible to store Python class objects in SQLite ? SQLAlchemy Docs : http : //docs.sqlalchemy.org/en/rel_0_9/orm/tutorial.htmlWORKING Flask-BrowserID : https : //github.com/garbados/flask-browserid/blob/master/tests/init.pyAdditional InformationHere is my main.py and index.html I am using with Flask and the Traceback I am getting : MAIN.pyINDEX.htmlTraceback"
"The Python 2.7.5 collections.defaultdict only seems to work when you pass default_factory as a positional argument -- it breaks when you pass it as a named parameter.If you run the following code you 'll see that default_dict_success ( ) runs fine , but default_dict_failure ( ) throws a KeyError.The output is Anyone know what 's going on ? EDIT : Originally I thought I was looking at some Python source that would 've suggested what I was trying to do was possible , but the commenters pointed out that I was mistaken , since this object is implemented in C and therefore there is no Python source for it . So it 's not quite as mysterious as I thought.That having been said , this is the first time I 've come across positional argument in Python that could n't also be passed by name . Does this type of thing happen anywhere else ? Is there a way to implement a function in pure Python ( as opposed to a C extension ) that enforces this type of behavior ?"
"I 've started a serious attempt to learn some Python as my first programming language with some basic knowledge on algorithms . Since everyone recommends that the best way to start is to find something useful to do , I 've decided to do a small script to manage my repositories.Basic things : - Enable/Disable YUM repositories - Change priority on current YUM repositories - Add/Remove repositoriesWhile parsing the file and replace/add/remove data is quite simple , I 'm struggling ( mainly with maybe lack of knowledge ) with a single thing with 'optparse ' ... I want to add to an option ( -l ) which lists the current available repositories ... I 've made a simple function that does this job ( not something very elaborate ) , but I 'm unable to 'connect ' it with the '-l ' on optparse . Anyone could provide examples/suggestions on how to make this ? The current script is something like this : Any suggestions to improve ( docs , examples ) are most welcome . The main goal once more , is that when I execute script.py -l it can run list_my_repos ( ) ."
"In Python , one could get elements that are exclusive to lst1 using : What would be the Matlab equivalent ?"
"I have a list1 like this , I want to get a new list2 like this ( joining first element with second element 's second entry ) ; As a first step , I am checking to join the first two elements in the tuple as follow , but I get this I was expecting this ; what I am doing wrong ? Is there any good way to do this ? a simple way.."
"I saved a data frame in pandas in an HDF5 file : The frame looks as follows : I am trying to load it in R : However , once loaded in R it looks as follows : I also tried : However it returns several values but no data frame : How can I load a data frame saved in pandas as an HDF5 file , in R ?"
"Given the following code : What is the best / simplest / most pythonic / coolest way to iterate over the list and print each value sequentially , beginning at start and wrapping to start-1 or the end if the random value were 0.Ex . start = 3 then output = 3,4,5,6,7,8,9,1,2I can think of some ugly ways ( try , except IndexError for example ) but looking for something better . Thanks ! EDIT : made it clearer that start is the index value to start at"
Hi guys suppose I have timeseries data.How to filter data that only occurs in 1 days different ? Suppose the data is What I want to do is something likethat would give meHow to do this in pandas ?
"I have a love/hate relationship with list comprehension . On the one hand I think they are neat and elegant . On the other hand I hate reading them . ( especially ones I did n't write ) I generally follow the rule of , make it readable until speed is required . So my question is really academic at this point.I want a list of stations from a table who 's strings often have extra spaces . I need those spaces stripped out . Sometimes those stations are blank and should not be included.Which translates to this list comprehension : This works well enough , but it occurs to me that I 'm doing strip twice . I guessed that .strip ( ) was not really needed twice and is generally slower than just assigning a variable.Turns out I was correct . Timeit shows between the two loop segments , the 2nd ( strip once ) is faster . No real surprise here . I am surprised that list comprehension is only marginally slower even though it 's doing a strip twice.My question : Is there a way to write a list comprehension that only does the strip once ? Results : Here are the timing results of the suggestions"
"I am making an app in python on mac osx 10.8.5 I converted the python script into an app by using py2app . But , within app , On Show Package Contents -- > Contents -- > Resources original code is present . I do n't want to show my code to others by distributing my app as security issue . I removed the ( .py ) code with ( .pyc ) code , in that case the app did n't work properly . Please suggest some way for it.I search on other questions also , but did n't get the desired result . My setup.py is"
"I am using Python with numpy , scipy and scikit-learn module.I 'd like to classify the arrays in very big sparse matrix . ( 100,000 * 100,000 ) The values in the matrix are equal to 0 or 1 . The only thing I have is the index of value = 1.which means How can I change the index array to the sparse array in scipy ? How can I classify those array quickly ? Thank you very much ."
"Say you want to optimize a ( byte ) string compare intensive algorithm implemented in Python . Since a central code path contains this sequence of statementsit would be great to optimize it to something likewhere ( the hypothetical ) bytes_compare ( ) ideally would just call the three-way comparison C function memcmp ( ) which is usually quite well optimized . This would reduce the number of string comparisons by half . A very feasible optimization unless the strings are ultra short.But how to get there with Python 3 ? PS : Python 3 has removed the three way comparison global function cmp ( ) and the magic method __cmp__ ( ) . And even with Python 2 , the bytes class does n't had a __cmp__ ( ) member.With the ctypes package it 's straight forward to call memcmp ( ) but the foreign function call overhead with ctypes is prohibitively high ."
"We 're setting up new branches at work which all will share the same libraries.The problem with this is if we update one library you might break all apps that are n't updated.Therefore we would like to version our libraries.The way I was planning on doing this is either like thisor maybe like this : The question is how this loader would work.The naive approach is to have each version in their own folder , the problem with this however is that if there is a common bug in all versions every version needs to be fixed separately.A slightly better approach ( for maintainability ) is to have all versions of a library in the same file and call some load function which creates links to functions.I do n't know how pretty this will be ( we might end up with monster file of several thousand lines , we could of course remove old , unused versions ) .To help with keeping the number of versions down I 'm planning on only incrementing the version number when I break compatibility , not when bug fixing or adding new stuff.Is there anything like this built into python or any other approach that would work ? Does anyone have experience of this sort of stuff ? I can add that things that are using the libs are test cases , we just want the tester to cd into the branch and run ./testit.py , nothing else.SolutionThe solution is based on Gringo Suave 's suggestion.Example usage :"
I have a header file like : I can wrap it in SWIG using std_vector.i and pyabc.i but that is quite inefficient ( there 's a jump between C++ and Python code for every access ) and given that these are literally just a bunch of bytes I ought to be able to wrap them with Python 's memoryview interface.How can I expose my std : :vector < uint8_t > as a Python memoryview ?
"in my scenario ( python 2.7 ) , i have : str ( var ) where var is a variable that sometimes requires thousands separators eg 1,500 but as you can see has been converted to a string ( for concatenation purposes ) . i would like to be able to print out that variable as a string with thousands separators . i have read solutions for adding formatting to a number eg : from https : //stackoverflow.com/a/1823189/1063287but this seems to be applicable just for a number , not for a number that has been converted to a string . thank you . edit : for more specific context , here is my implementation scenario : and i have several 'vars ' ."
"A ] Summary : Using get or insert functionality on one to many django models , checking whether a record or relationship exists prior to adding a new one.B ] Details:1 ] I have the following django model structure country ( one ) to City ( many ) 2 ] I am using ModelForm for displaying the form on the html page . The forms are passed as template values and the template values are then rendered on the html page3 ] When the post request comes in , the code extracts the country and city form data , checks for validity and saves it.C ] Issue/Item i need help with:1 ] I want to use the get_or_insert functionality with my country and city relationship2 ] Basically , i want to check whether there is an existing country record in the database for the country selected by the user,2.1 ] if the country exists , check if there is a city record for the selected country.2.1.1 ] if the city record exists , we are good , no need to add another record2.1.2 ] if the city record does n't exist , create the city record and associate the country with it2.2 ] if the country does n't exist , create the country record , create the city record and associate the city record with the country record . C ] Code Excerpts:1 ] Models and forms -- 2 ] code that prints the forms -- user_country_form and user_city_form are passed from python code as template values 3 ] HTML code excerpt which displays the template values on the html page3 ] Code excerpt for saving the data -- Thankyou for reading . [ Edit # 1 ] @ Torsten , post gave me a pointer to get_or_insert , I then came across StackOverflow post ( http : //stackoverflow.com/questions/4812832/get-or-create-matching-key-and-user-python-app-engine ) and implemented the key_name functionality to user get_or_insertCode for saving the record looks like follows : Request : Can someone please do a quick code review and let me know if i am doing something wrong or making some rookie mistakes ?"
I need to add a ppa to remote servers using a python script . The bash equivalent of what I want to do is : I 'm assuming it would look something like this : but I have n't been able to find much in the apt module source related to adding repositories .
"I have a database table : As you can see , a service number can be related to multiple meter numbers . Think of the service number as a unique identifier for a geographic location that does n't change.And I have a Django model : And I have a basic serializer : And the current response is : But what I really want is ( need the usage separated by meter number ) : My end goal is to display this data in a line chart using the ChartJS library in an Angular 7 app , and the data needs to be in this format : Is it possible to use a serializer to format the data as I 've shown above ? I 've tried different techniques based on various tutorials I 've read , but nothing seems to work and now I 'm just confused about how to handle it.Any insight is appreciated !"
"I am trying to write a python script scanning a folder and collect updated SQL script , and then automatically pull data for the SQL script . In the code , a while loop is scanning new SQL file , and send to data pull function . I am having trouble to understand how to make a dynamic queue with while loop , but also have multiprocess to run the tasks in the queue . The following code has a problem that the while loop iteration will work on a long job before it moves to next iteration and collects other jobs to fill the vacant processor.Update : Thanks to @ pbacterio for catching the bug , and now the error message is gone . After changing the code , the python code can take all the job scripts during one iteration , and distribute the scripts to four processors . However , it will get hang by a long job to go to next iteration , scanning and submitting the newly added job scripts . Any idea how to reconstruct the code ? I finally figured out the solution see answer below . It turned out what I was looking for is the_queue = Queue ( ) the_pool = Pool ( 4 , worker_main , ( the_queue , ) ) For those stumble on the similar idea , following is the whole architecture of this automation script converting a shared drive to a 'server for SQL pulling ' or any other job queue 'server ' . a . The python script auto_data_pull.py as shown in the answer . You need to add your own job function . b . A 'batch script ' with following : start C : \Anaconda2\python.exe C : \Users\bin\auto_data_pull.py c. Add a task triggered by start computer , run the 'batch script'That 's all . It works . Python Code : Reference : monitor file changes with python script : http : //timgolden.me.uk/python/win32_how_do_i/watch_directory_for_changes.htmlMultiprocessing : https : //docs.python.org/2/library/multiprocessing.html"
"In pandas dataframe , how to fill the value of a column conditionally to values in an other column being part of a list ? This is very similar to this SO question , but when I apply : I got an error : I suppose the in operator has been not overridden to work with vectors.."
"I am automating the creation of Gmail drafts in Python using the Gmail API . I am required to create HTML-formatted emails , but I also personally require creating a plaintext fallback , because it 's the Right Thing To Do . I thought I had all of the above working , until I tried making the plaintext fallback a little different from the HTML . It seems that Google takes it upon itself to create the plaintext fallback for me , rather than using the one I provide , so if my html body is < HTML > < BODY > HTML Body < /BODY > < /HTML > and my plaintext body is Plaintext body , the final plaintext body will be HTML Body , discarding the plaintext I provided.My question : Does anyone know a way to get the Gmail API to use the plaintext I provide , rather than auto-generating the fallback for me ? One relevant item I 've noticed : if I attach the HTML and Plaintext bodies in a different order , the reverse happens - GMail will automatically generate an HTML body based on my plaintext . So it seems like it 's only paying attention to the last attached body.Stripped down version of the code I 'm using : Update : Here are example gists of what I 'm sending to Gmail vs what gets sent from Gmail , in both the HTML-then-plaintext and the plaintext-then-HTML orders ( which generate different results )"
"I primarily develop my models in R and I am currently learning TensorFlow . I 'm going through a tutorial with the following codeFrom a layman 's perspective , why do I need to initialize and Variabalize in TensorFlow ? I know this may be a basic question but it 's something not dealt with in R ."
"I want to create a multi-index DataFrame by reading a textfile . Is it faster to create the multi-index and then allocate data to it from the text file using df.loc [ [ ] , [ ] ] , or concatenate rows to the DataFrame and set the index of the DataFrame at the end ? Or , is it faster to use a list or dict to store the data as it 's read from the file , and then create a DataFrame from them ? Is there a more pythonic or faster option ? Example text file : Output DataFrame : Update Jan 18 : This is linked to How to parse complex text files using Python ? I also wrote a blog article explaining how to parse complex files to beginners ."
I have no idea how __contains__ works for ndarrays . I could n't find the relevant documentation when I looked for it . How does it work ? And is it documented anywhere ?
"How could I send an e-mail from my Python script that is being run on `` Google App Engines '' to one of my mail boxes ? I am just a beginner and I have never tried sending a message from a Python script . I have found this script ( IN THIS TUTORIAL ) : Here is the same script as a quote : but I hardly understand how I could have this script run from `` Google App Engine '' : 1 ) Firstly , I do n't quite understand what e-mail address I need to place right after From : in this line : Can I just place here any e-mail address of any e-mail boxes that I have ? 2 ) Secondly , let 's say I want to send a message to this e-mail address of mine brilliant @ yahoo.com . Then the next line , I guess , must look this way : Is this right ? 3 ) Thirdly , let 's say , the message that I want to send will be this sentence : Cats can not fly ! Then , I guess , the line that starts with msg = must look this way : Is this correct ? 4 ) If I upload this script as an application to `` GAE '' , how often will it be sending this message to my mail box ? Will it send this message to me only once or it will be sending it to me every second all the time until I delete the application ? ( This is why I have n't tried uploading this script so far ) Thank You all in advance for Your time and patience ."
"I am using the pandas.ols function from version 0.7.3 . I am interested in doing a moving regression , such as : The inputs contain data for about 600 dates , of which 15 are NA values . But the output only contains regression results for about 120 dates . The issue is that whenever the window contains even one NA value , there is no output for that window . The problem disappears if I change window_type to expanding and I get about 500 output points as expected , but I do n't want to do an expanding regression.Can you tell me how to fix this ?"
When a novice ( like me ) asks for reading/processing a text file in python he often gets answers like : Now I would like to truncate everything in the file I 'm reading after a special line . After modifying the example above I use : and expect it to throw away everything after the first `` CC '' seen . Running this code on input.txt : the following is printed on the console ( as expected ) : but the file `` input.txt '' stays unchanged ! ? ! ? How can that be ? What I 'm doing wrong ? Edit : After the operation I want the file to contain :
"In module a.py Output : In the module , I can easily assign a function to a variable . When inside class try to assign module level function to class variable func = task it shows error , to remove this error I have to replace it with func = task.__call__ But when I assign to instance variable its work self.func_1 = task . My Question is : why I ca n't assign a module level function to a class variable without __call__ and when the same function I can assign to an instance variable is working ."
"I want to get 1/7 with better precision , but it got truncated . How can I get better precision when I convert a rational number ?"
"I 've been searching really hard for this , and i ca n't find it anywhere . So , here it goes : I 'm trying to build an generic class that takes a generic Model as parameter and creates a form for it . For this i 'm using Django 's ModelForm and CreateView classes . The main goal to this , is when i need to create a new form , i just declare a new URL passing the Model name.urls.pyviews.pyto_modelform is a function that i implemented that converts a model to a modelform , and it works.This gives me the following error : AttributeError at /createThis method is available only on the view class.Thank you in advance !"
"Currently I want to compare the speed of Python and C when they 're used to do string stuff . I think C should give better performance than Python will ; however , I got a total contrary result.Here 's the C program : and this is the Python one : and what I get : Does that make sense ? Or am I just making any stupid mistakes ?"
Consider that we have the following input Is there any python library that lets me parse the formula and convert it into a python function representation.eg.I am currently looking into something likeand then process this ast tree further . Do you know of any cleaner alternate solution ? Any help or insight is much appreciated !
"I 'm working in a memory constrained environment where I need to make archives of SQL dumps . If I use python 's built in tarfile module is the '.tar ' file held in memory or written to disk as it 's created ? For instance , in the following code , if huge_file.sql is 2GB will the tar variable take up 2GB in memory ?"
"I am running the following image : inside it ( as root ) , I installed : and then botoThen I wrote the following in /root/.boto : ( This file is the same that I have in my host machine ) The versions are : ( Exactly the same that I have in my host/local machine ) If I fired the python console : If I activate the logging : ( This example runs in my local machine ) I already read Boto Credential Error with Python on Windows Boto : Dynamically get aws_access_key_id and aws_secret_access_key in Python code from config ? and another questions . Nothing works.After that , I review the source code of boto.pyami and If I execute line by line got nothing.For example : Note that the last command returns None.The boto.config use ConfigParser , if I use that , it works : ( Note that if I use the relative path It did n't work ) Finally if I use : I did another tests : I instllaed awscli and it works without trouble , put the config file in /etc/boto.cfg and did n't work.I already spent all my weekend in this , any ideas ?"
I am using pandas and groupby to aggregate . I 'm doing the following : Is there a way to do the groupby where it will fill in the ac column so that it reads like the following ?
"Briefly : there is a similar question and the best answer suggests using numpy.bincount . I need the same thing , but for a matrix.I 've got two arrays : together they make indices that should be incremented : I want to get this matrix : The matrix will be small ( like , 5×5 ) , and the number of indices will be large ( somewhere near 10^3 or 10^5 ) .So , is there anything better ( faster ) than a for-loop ?"
"I 'm having some trouble to convert a pure Keras model to TensorFlow Estimator API on an unbalanced dataset.When using pure Keras API , the class_weight parameter is available at model.fit method , but when converting a Keras model to TensorFlow Estimator with tensorflow.python.keras.estimator.model_to_estimator there is no place to inform class_weights.How can overcome this ? I 'm using TF 1.12 on Ubuntu 18 , Cuda 9 , Cudnn 7Pure Keras model : Keras model to TensorFlow Estimator :"
"I have quite a large Python ( 3 ) script that I 'm trying to optimize.To my understanding when you use with open ( ... , ... ) as x you DO N'T need to use .close ( ) at the end of the `` with '' block ( it is automatically closed ) .I also know that you SHOULD add .close ( ) ( if you are not using with ) after you are done manipulating the file , like this : In an attempt to shove 3 lines ( above ) into 1 line I tried to change this : Into this : Unfortunately this did not work and I got this error : The same line works fine when I remove the .close ( ) like this : Why did n't open ( location , mode ) .write ( content ) .close ( ) work and is it safe to omit the .close ( ) function ?"
"Given a string dialogue such as below , I need to find the sentence that corresponds to each user . For the above dialogue , I would like to return tuples with three elements with : The name of the personThe sentence in lower case and The sentences within BracketsSomething like this : I am trying to use regex to achieve the above . So far I was able to get the names of the users with the below code . I am struggling to identify the sentence between two users ."
"When using try-except in a for loop context , the commands executed so far are obviously done withresults with However the same is not true for list comprehensionsAnd the result isIs the intermediate list , built before the exception occurred , kept anywhere ? Is it accessible ?"
"I have many python packages , wrote by my colleagues and I want to write a tool to check which third packages they rely on . Like this and the tool should work like this I have no idea how to do that , all I can come up implementation of this tool is read a Python file line by one line as string , and use regex to compare it.Could you give me some advises ? If you do n't know what I mean , please comment your question , I will fix it as soon as possible . Thanks !"
Django shell behaves ( at least for me ) unexpected when working with locale settings . Form validation of a comma separated decimal field works when calling from external script and fails on calling from django shell ( ipython ) .Starting a new Project I got the following files : local_forms/models.py : my_form.pytest_form.py : Calling ./test_form.py yields : Doing the same thing in django shell : python manage.py shellTo sum up : If i start the django shell ( which on my pc uses ipython ) the locale somehow does not work . Doing exactly the same in a script works perfectly.Can you please explain this behavior ?
"Say I have a simple class Foo , which comes from an external library , thus I can not change it directly : I want to create a subclass Bar and prevent x from being change from an instance of Bar , but still use the x in Bar 's methods.Here 's what I tried , and it will probably enlighten the basic idea , but unfortunately it does n't work : So basically I 've created some wrapper functions ( do_stuff ( ) ) around an attribute , and now I want to prevent the attribute from being changed directly , as it might mess up some functionality of the wrapper functions . Is this possible in a reasonable way ? Edited with a better example of what I want . I 'm not trying to prevent them from seeing the variable x , but instead changing it from outside of do_stuff ( )"
"I was given this as an exercise . I could of course sort a list by using sorted ( ) or other ways from Python Standard Library , but I ca n't in this case . I think I 'm only supposed to use reduce ( ) .The error I get : Which is expected , because my reduce function inserts a tuple into the int array , instead of 2 separate integers . And then the tuple gets compared to an int ... Is there a way to insert back 2 numbers into the list , and only run the function on every second number in the list ? Or a way to swap the numbers with using reduce ( ) ? Documentation says very little about the reduce function , so I am out of ideas right now.https : //docs.python.org/3/library/functools.html ? highlight=reduce # functools.reduce"
"I am successfully able to load a python script file and call a function within using boost : :python in a C++ app.In the boost python EmbeddingPython wiki there is a tip on how to load a python module.I can successfully use this to import a python module ( test.py ) Running the above code with a hello-world test.py script works fine : test.py : Output : Exposing a C++ class to python : However , I now want to expose a C++ class to that script.As per the boost : :python documentation , I expose this class as follows : As per the instructions in the above-linked wiki , I can then import my FooModule , and store it in my globals : This import is done prior to importing my test.py script , and this globals object is the one passed to bp : :exec when importing my script ( ie : Foo should be in the globals dict which bp : :exec exposes to my script when importing ) .However , for some reason my Foo module is not visible to test.pyQuestion : How can I expose my Foo class to the test.py python script I am loading ? Full working example : test.py : main.cpp : Output :"
"I need a function , which is capable of iterating over the collection , calling a supplied function with element of the collection as a parameter and returning the parameter or it 's index when received `` True '' from supplied function.It is somethong like this : So I am wondering whether there 's anything like that in standart python toolset ?"
"Possible Duplicate : How to clone a Python generator object ? Suppose I have a generator 'stuff_to_try ' , I can try them one by one but if I had a method that wanted to go through the generator and that method is recursive , I want to let each recursion get a fresh new generator that starts at the first yield rather than where the last recursion left off.Of course I can define stuff_to_try inside the recursive function but is there a better way ? Is there an equivalent of stuff_to_try.clone ( ) .reset ( ) or something ?"
"Say if I have four functions as below : How can I alternate foo and bar functions to run every , say 30 minutes ? Meaning : 1st 30mins - run foo , 2nd 30mins - run bar , 3rd 30mins - run foo , and so on . Each new run should 'kill ' the previous thread/func.I have a countdown timer threads , but not sure how to 'alternate ' the functions.EDIT : My solution with Nicholas Knight 's inputs ... Works fine ."
"The command : will clone the lastest version of cookiecutter-django , which is aimed for Django 1.9.There is a Stable section in the README pointing to some tags . One of them being https : //github.com/pydanny/cookiecutter-django/releases/tag/1.8.7.But if I try : I get an error : So , how to specify cookiecutter to use those stable released instead of the master branch ?"
I have a dataframe with course names for each year . I need to find the duration in months starting from year 2016.How do I add months in a new column starting from highest ( i.e . bottom like bfill ? ) The final data-frame will look like this ... Some of the answers do not consider that there can be multiple courses . Updating sample data ...
I am trying to implement a functionality in python where I want to send a file as an attachment to an email alertEverything works fine . i am getting the email alert with required subject but the only problem is that I get the same attachment twice in my email alert .
"I need to create a function to pass to curve_fit . In my case , the function is best defined as a piecewise function . I know that the following does n't work , but I 'm showing it since it makes the intent of the function clear : The problem , of course , is that X is a pandas Series , and the form ( X < = x1 ) evaluates to a series of booleans , so this fails with the message `` The truth value of a Series is ambiguous . `` It appears that np.piecewise ( ) is designed for exactly this situation : `` Wherever condlist [ i ] is True , funclist [ i ] ( x ) is used as the output value . '' So I tried this : But this fails at this call : with the message `` IndexError : too many indices for array '' . I 'm inclined to think it 's objecting to the fact that there are two elements in condlist and three elements in funclist , but the docs specifically state that the extra element in funclist is treated as the default.Any guidance ?"
"The purpose of this mathematical function is to compute a distance between two ( or more ) protein structures using dihedral angles : It is very useful in structural biology , for example . And I already code this function in python using numpy , but the goal is to have a faster implementation . As computation time reference , I use the euclidean distance function available in the scikit-learn package.Here the code I have for the moment:9.44 ms it 's very fast , but it 's very slow if you need to run it a million times . Now the question is , how to do that ? What is the next step ? Cython ? PyOpenCL ? I have some experience with PyOpenCL , however I never code something as elaborate as this one . I do n't know if it 's possible to compute the dihedral distances in one step on GPU as I do with numpy and how to proceed.Thank you for helping me ! EDIT : Thank you guys ! I am currently working on the full solution and once it 's finished I will put the code here . CYTHON VERSION : So I follow your link to create the cython version of the dihedral distances function . We gain some speed , not so much , but it is still slower than the numexpr version ( 17ms vs 9.44ms ) . So I tried to parallelize the function using prange and it is worse ( 37.1ms vs 17ms vs 9.4ms ) ! Do I miss something ?"
"I have a data frame like this and I 'm trying reshape my data frame using Pivot from Pandas in a way that I can keep some values from the original rows while making the duplicates row into columns and renaming them . Sometimes I have rows with 5 duplicatesI have been trying , but I do n't get it.InputOutput"
"Let 's say I have a dictionary : It has a method clear ( ) : ... which has a __hash__ attribute : ... which is callable : So why ca n't I hash it ? Note : I know that dict objects are unhashable – I 'm curious as to why this restriction extends to their methods , even though , as noted above , they appear to claim otherwise ?"
"I want to broadcast an array b to the shape it would take if it were in an arithmetic operation with another array a.For example , if a.shape = ( 3,3 ) and b was a scalar , I want to get an array whose shape is ( 3,3 ) and is filled with the scalar.One way to do this is like this : Although this works practically , I ca n't help but feel it looks a bit weird , and would n't be obvious to someone else looking at the code what I was trying to do.Is there any more elegant way to do this ? I 've looked at the documentation for np.broadcast , but it 's orders of magnitude slower ."
I want to create only one random prime number in any provided range . I have this code : But my program provides a set of prime numbers . I want to modify my code and create only one random number in any provided range but I ca n't find how to do it.Help please .
"I have following code : And the slider works like it should . It does n't slide when I uncomment that line with locale setting . That is probably due to the fact , that pl_PL locale uses comma for float separation . This may be a bug . How can I work around it so I can have the locale set properly ?"
"This is sort of a follow-up to Python regex - Replace single quotes and brackets thread.The task : Sample input strings : Desired outputs : The number of name [ 'something ' ] -like items is variable.The current solution : Currently , I 'm doing it through two separate re.sub ( ) calls : The question : Would it be possible to combine these two re.sub ( ) calls into a single one ? In other words , I want to replace something at the beginning of the string and then multiple similar things after , all of that in one go.I 've looked into regex module - it 's ability to capture repeated patterns looks very promising , tried using regex.subf ( ) but failed to make it work ."
"I 'm am trying to call the to_dict function on the following DataFrame : import pandas as pddata = { `` a '' : [ 1,2,3,4,5 ] , `` b '' : [ 90,80,40,60,30 ] } df = pd.DataFrame ( data ) df.reset_index ( ) .to_dict ( `` r '' ) However my problem occurs if I perform a float operation on the dataframe , which mutates the index into a float : ( df*1.0 ) .reset_index ( ) .to_dict ( `` r '' ) Can anyone explain the above behaviour or recommend a workaround , or verify whether or not this could be a pandas bug ? None of the other outtypes in the to_dict method mutates the index as shown above.I 've replicated this on both pandas 0.14 and 0.18 ( latest ) Many thanks !"
I ran aws configure to set my access key ID and secret access key . Those are now stored in ~/.aws/credentials which looks like : I 'm trying to access those keys for a script I 'm writing using configparser . This is my code : I run the script using python3 script.py . Any ideas of what 's going on here ? Seems like configparser just is n't reading/finding the file at all .
"I 'm trying to format the following as JSON ( in Python ) : Every JSON validator of course tells me that this is not valid JSON because of the trailing `` , '' before each } , so I want to remove that comma . I tried removing them with .replace ( ' '' Player '' , '' , ' '' Player '' ' ) but I do n't consider this as a good solution . This because I also have trailing `` , '' in for example a Alliance or Habitat string ( `` Habitat '' , & `` Alliance '' , ) Could anyone help me with finding a better solution to this problem ?"
"I have a university project in which we are asked to simulate a satellite approach to Mars using ODE 's and SciPy 's odeint function . I manage to simulate it in 2D by making a second-order ODE into two first-order ODE 's . However I am stuck in the time limitation because my code is using SI units therefore running in seconds and Python 's linspace limits does not even simulate one complete orbit.I tried converting the variables and constants to hours and kilometers but now the code keeps giving errors.I followed this method : http : //bulldog2.redlands.edu/facultyfolder/deweerd/tutorials/Tutorial-ODEs.pdfAnd the code is : I do n't know how to copy/paste the error code from PyLab so I took a PrintScreen of the error : Second error with t=linspace ( 0.01,24.0,100 ) and xinit=array ( [ 0.001,5251 ] ) : If anyone has any suggestions on how to improve the code I will be very grateful.Thank you very much !"
"I have a Django app that connects to Oracle . Every 10 refreshes or so , regardless of what page , it gives an error . First time after restart an ORA-03113 and every subsequent errors ORA-03135.I have checked to alert.log and listener.log on the database side and ca n't find anything about this session terminating in the log file . I 've edited site-packages/django/db/init.py and put an try/except around close_if_unusable_or_obsolete . The page does not give an error anymore , and in the logfile no ORA error but I still see the SIGPIPE error.My guess : Django thinks there is still a connection and tries to close ( close_if_unusable_or_obsolete ) it but on the Oracle side the connection does not exist . The first ORA-03113 does not result in a record in the listener.log , so it seems it does not even reach the Database server . After the modification in the init.py the result page is correct . So the query for building up the page is successful . It seems to be a connection for administrative purposes of some kind.I 'm lost , does anyone have an idea how to find this error ? uwsgi_error.logSQL*Net trace file contains : Incomming Error ORA-12151 : TNS : received bad packet type from network layerEDIT Response to TitusI do n't use TNSNAMES.ORA or SQLNET.ORA"
"I am trying to split financial documents to sentences . I have ~50.000 documents containing plain English text . The total file size is ~2.6 GB . I am using NLTK 's PunktSentenceTokenizer with the standard English pickle file . I additionally tweaked it with providing additional abbreviations but the results are still not accurate enough . Since NLTK PunktSentenceTokenizer bases on the unsupervised algorithm by Kiss & Strunk ( 2006 ) I am trying to train the sentence tokenizer based on my documents , based on training data format for nltk punkt.Unfortunately , when running the code , I got an error , that there is not sufficient memory . ( Mainly because I first concatenated all the files to one big file . ) Now my questions are : How can I train the algorithm batchwise and would that lead to a lower memory consumption ? Can I use the standard English pickle file and do further training with that already trained object ? I am using Python 3.6 ( Anaconda 5.2 ) on Windows 10 on a Core I7 2600K and 16GB RAM machine ."
"I have a problem in Python , for which I can not find any clean solution ... When calling some methods , I want to execute some code before the method execution and after . In order ( among many other things ) to automatically set and clean a context variable.In order to achieve this , I have declared the following metaclass : This works like a charm : But as soon as I want to subclass Parent , problems appear , when I call the parent method with super : I have thought of saving the context before setting it to a new value , but that only solves partially the problem ... Indeed , ( hang on , this is hard to explain ) , the parent method is called , the wrappers are executed , but they receive *args and **kwargs addressed to Parent.test , while self is a Child instance , so self attributes have irrelevant values if I want to challenge them with *args and **kwargs ( for example for automated validation purpose ) , example : So basically , to solve this problem I see two solutions : preventing the wrappers to be executed when the method was called with super ( Child , self ) having a self that is always of the `` right '' typeBoth solutions seem impossible to me ... Do somebody has an idea on how to solve this ? A suggestion ?"
"I 've looked and looked for a solution to this problem and am turning up nothing . I 'm generating rectangular FITS images through matplotlib and subsequently applying WCS coordinates to them using AstroPy ( or PyFITS ) . My images are in galactic latitude and longitude , so the header keywords appropriate for my maps should be GLON-CAR and GLAT-CAR ( for Cartesian projection ) . I 've looked at other maps that use this same map projection in SAO DS9 and the coordinates work great ... the grid is perfectly orthogonal as it should be . The FITS standard projections can be found here . But when I generate my maps , the coordinates are not at all Cartesian . Here 's a side-by-side comparison of my map ( left ) and another reference map of roughly the same region ( right ) . Both are listed GLON-CAR and GLAT-CAR in the FITS header , but mine is screwy when looked at in SAO DS9 ( note that the coordinate grid is something SAO DS9 generates based on the data in the FITS header , or at least stored somewhere in the FITS file ) : This is problematic , because the coordinate-assigning algorithm will assign incorrect coordinates to each pixel if the projection is wrong.Has anyone encountered this , or know what could be the problem ? I 've tried applying other projections ( just to see how they perform in SAO DS9 ) and they come out fine ... but my Cartesian and Mercator projections do not come out with the orthogonal grid like they should.I ca n't believe this would be a bug in AstroPy , but I ca n't find any other cause ... unless my arguments in the header are incorrectly formatted , but I still do n't see how that could cause the problem I 'm experiencing . Or would you recommend using something else ? ( I 've looked at matplotlib basemap but have had some trouble getting that to work on my computer ) .My header code is below : Thanks for any help you can provide ."
"I am trying to create a function to mix two lists in python , or I rather say put the element of list2 into list1 . It is required that in the output list , no more than two elements next two each other have the same valueFor example : Wrong output example : Here is my solution : The issue is getting the output with more than two of the same elements standing next two each other or the length of the output list is longer than combine length of two listsFor example one of my outputWhat am I doing wrong here ? Edit 1Clodion asked me so I put this up hereThe results can be random , as long as they satisfy the requirement in the question . The order of the elements does not matter as long as no more than two elements standing next to each other have the same valueEdit 2I am trying to mess around by create a iterable class , using Clodion solution . Here is the new code : The result turned out quite well , except for the code executing unlimited and I have to use KeyboardInterrupt to stop the code . What am I doing wrong in this case ? I would be grateful with a thorough explain since I am a newbie in Python and IteratorEdit 3Solved the iterator problem , easier than I think . It turned out that what returned in the iter is what the class return when calling in the for-loopSolution :"
"I 'm trying to replicate an N dimensional Delaunay triangulation that is performed by the Matlab delaunayn function in Python using the scipy.spatial.Delaunay function . However , while the Matlab function gives me the result I want and expect , scipy is giving me something different . I find this odd considering both are wrappers of the QHull library . I assume Matlab is implicitly setting different parameters in its call . The situation I 'm trying to replicate between the two of them is found in Matlab 's documentation.The set up is to have a cube with a point in the center as below . The blue lines I provided to help visualize the shape , but they serve no purpose or meaning for this problem.The triangulation I expect from this results in 12 simplices ( listed in the Matlab example ) and looks like the following.However this python equivalent produces `` extra '' simplices.The returned variable simp should be an M x N array where M is the number of simplices found ( should be 12 for my case ) and N is the number of points in the simplex . In this case , each simplex should be a tetrahedron meaning N is 4.What I 'm finding though is that M is actually 18 and that the extra 6 simplices are not tetrahedrons , but rather the 6 faces of the cube.What 's going on here ? How can I limit the returned simplices to only be tetrahedrons ? I used this simple case to demonstrate the problem so I 'd like a solution that is n't tailored to this problem.EDITThanks to an answer by Amro , I was able to figure this out and I can get a match in simplices between Matlab and Scipy . There were two factors in play . First , as pointed out , Matlab and Scipy use different QHull options . Second , QHull returns simplices with zero volume . Matlab removes these , Scipy does n't . That was obvious in the example above because all 6 extra simplices were the zero-volume coplanar faces of the cube . These can be removed , in N dimensions , with the following bit of code.I suppose the other conditions should be addressed , but I 'm guaranteed that my points contain no duplicates already , and the orientation condition appears to have no affect on the outputs that I can discern ."
"I have the following numpy random 2D array : I want to iterate over each grid cell in the array . If the value of the grid cell is > 0.6 , then I want to assign the excess to its immediate 8 neighboring cells ( in case of corner grid cell , the number of neighboring cells will be fewer ) .The excess should be redistributed as per one of 2 user-selected rules : Evenly amongst the 8 neighborsProportional to the value in each neighbor i.e . a neighbor with higher value gets higher Is there a way to do this in numpy without resorting to a for loop ?"
"I have a simple Flask-SQLAlchemy model , which I 'm writing a REST API for : Then I have the corresponding Marshmallow-SQLAlchemy schema : However , in my rest API , I need to be able to dump and load slightly different variants of this model : When dumping all the reports ( e.g . GET /reports ) , I want to dump all the above fields.When dumping a single report ( e.g . GET /reports/1 ) , I want to dump all this data , and also all associated relations , such as the associated Sample objects from the sample table ( one report has many Samples ) When creating a new report ( e.g . POST /reports ) , I want the user to provide all the report fields except report_id ( which will be generated ) , report_hash and uploaded_at ( which will be calculated on the spot ) , and also I want them to include all the associated Sample objects in their upload.How can I reasonably maintain 3 ( or more ) versions of this schema ? Should I : Have 3 separate ModelSchema subclasses ? e.g . AggregateReportSchema , SingleReportSchema , and UploadReportSchema ? Have one mega-ModelSchema that includes all fields I could ever want in this schema , and then I subtract fields from it on the fly using the exclude argument in the constructor ? e.g . ReportSchema ( exclude= [ ] ) ? Or should I use inheritance and define a class ReportBaseSchema ( ModelSchema ) , and the other schemas subclass this to add additional fields ( e.g . class UploadReportSchema ( ReportBaseSchema ) ) ? Something else ?"
"I am using Flask together with gevent-socketio : I 'm using a pretty standard setup to start the server : And a pretty standard hook for my SocketIO namespace : However , I 'm having problems where 'connect ' events are n't being fired on the client . After a little digging , I realized that even though I was getting 127.0.0.1 - - [ 2013-08-19 12:53:57 ] `` GET /socket.io/1/websocket/170191232666 HTTP/1.1 '' 101 - - messages in the output , I was n't getting the ws connect message ( while other print statements in the code were working fine ) . I commented out that endpoint , and sure enough it 's not even being called . That would explain why my namespace is n't being used . But why ? Am I registering my namespace wrong ? print app.url_map yields : So nothing out of the ordinary.Edit : The client code :"
It seems as if that when I have an abstract base class that inherits from gevent.Greenlet ( which inherits from the C extension module greenlet : https : //github.com/python-greenlet/greenlet ) then classes that implement it do not raise any of the abc errors about unimplemented methods.If I inherit from object it fails as expected : What is the right way to implement this functionality ?
I am just learning python and have written some code to set iptables using the python-iptables library . The problem I 'm running in to is that I had to rewrite a lot of the same lines of code over and over . I understand functions somewhat but not OOP . I 'm thinking there is a better OOP way of writing this code but I ca n't get my head around it . Any pointers will be greatly appreciated . The code is below.Notice how all the rules have similar lines of code . Is there a way to create a rule generator object or something like that to minimize rewriting that code ? I added the following function so and call it every time the script is run so that the rules get flushed .
"I defined an exception class SpamException in a module spam . Now I want to test a function spam_function , that raises this exception . So I wrote the following doctest.The test succeeds on Python 2.x , but on Python 3.x the test fails . The following test works on Python 3.x.The notable difference here is the inclusion of the module name in the exception name . So how can I write a doctest that works on both Python 2.x and 3.x ?"
If I have a class like below : And have 2 objects : How can I modify class Point to make id ( a ) == id ( b ) ?
"I am working with a Python app with Flask running on Bluemix . I know how to use Object Storage with the swiftclient module for creating a container and saving a file in it , but how do I dump a joblib or pickle file contained within it ? And how do I load it back in my Python program ? Here is the code to store a simple text file ."
"I have created tensorflow program in order to for the close prices of the forex . I have successfully created the predcitions but failed understand the way to forecast the values for the future . See the following is my prediction function : Here is the complete jupyter and datasets for test and train : My repository with code . Kindly , help me how I can forecast the close values for the future . Please do not share something related to predictions as I have tried . Kindly , let me know something that will forecast without any support just on the basis of training what I have given . I hope to hear soon ."
"I am running several tensorflow inferences using sess.run ( ) in a loop and it happens that some inferences are too heavy for my GPU.I get errors like : I would like to be able to catch these specific OutOfMemory errors but not other errors ( which may be due to a wrong input format or a corrupted graph . ) Obviously , a structure similar to : does not work since other kind of errors will lead to a call to the do_outOfMemory_specific_stuff function.Any idea how to catch these OutOfMemory errors ?"
"I have a malformed database . When I try to get records from any of two tables , it throws an exception : DatabaseError : database disk image is malformedI know that through commandline I can do this : Can I do something like this from within Python ?"
"I 'm using google 's API client to interact with the Gmail API . Assuming I have a draft 's immutable ID , I 'd like to send the associated draft.I tried : Here draft_id is the id of the draft that I 'd like to send , http is an instance of Http that works for other requests ( so it is properly authenticated ) .Trying the above , I get a TypeError : The documentation has a Java example , but no python example.Other variations that I 've tried : The last one actually gives a different error : which makes me feel like it might be moving in the correct direction ... ( note , I 've been able to use the IDs to send messages from the API explorer linked above , so I 'm confident that I 'm working with valid IDs ) What 's the right way to send this data ? I 'm not sure how relevant this is , but the discovery API 's representation of the GMAIL Api can be found at https : //www.googleapis.com/discovery/v1/apis/gmail/v1/restSpecifically , I think that the method I 'm working with is defined by the following JSON :"
"I am working with PySpark on a huge dataset , where I want to filter the data frame based on strings in another data frame . For example , I assume that domains and gooddomains are valid domain names.What I want to do is filter out the matching strings in dd that do not end with dd1 . So in the above example , I want to filter out row 1 and row 3 , to end up with My current solution ( as shown below ) can only account for domains up to 3 'words ' . If I were to add say , verygood.co.ac.uk in dd1 ( i.e . whitelist ) , then It will fail.I am using Spark 2.3.0 with Python 2.7.5 ."
"Flags in emoji are indicated by a pair of Regional Indicator Symbols . I would like to write a python regex to insert spaces between a string of emoji flags . For example , this string is two Brazilian flags : Which will render like this : I 'd like to insert spaces between any pair of regional indicator symbols . Something like this : Which would result in : But that code gives me an error : A hint ( I think ) at what 's going wrong is the following , which shows that \U0001F1E7 is turning into two `` characters '' in the regex : This results in : Sadly my understanding of unicode is too weak for me to make further progress.EDIT : I am using python 2.7.10 on a Mac ."
"I have a generator function generate which yields 5 random numbers one at a time . I need to be able to generate the numbers in two ways : Single generation , which means a single output of generate functionMultiple generation , which means multiple execution of generate and yielding all the results together as a single ( merged ) flowFor that I wrote another function get_resource , which calls generate either once or using itertools.chain to run the generators one after another , but transparently to the caller.My goal is to use get_resource function and produce the results in the same format ( one list of numbers ) , regardless of single/multiple generations.It prints : While I need it to print : I use python2.7"
"I 'm embedding Python 3.6 in my application , and I want to disable import command in the scripts to prevent users to import any python built-in libraries . I 'd like to use only the language itself and my own C++ defined modules.Do you know any way to achieve this ?"
"Is is possible to call a plpgsql function ( or any PostgreSQL function ) from a PL/Python function ? So , something like this : And then use it here"
"I am trying to identify the indices of the masked pixels when using maskoceansso I can then call only the land pixels in a code I have that is currently going through the whole globe , even though I do not care about the ocean pixels . I was trying different methods to do so , and noticed that my plots were looking really weird . Eventually , I realized that something was getting mixed up in my lat/lon indices , even though I am not actually touching them ! Here is the code : The first figure I get shows the expected map of Africa with oceans masked and the land values corresponding to the latitude ( until saturation of the colorbar at 34 , but that value was just taken as an example ) However , the second figure , which should plot the exact same thing as the first one , comes out all messed up , even though the loop in between the first and second figure does n't touch any of the parameters involved in plotting it : If I comment out the loop in between figure 1 and 2 , figure 2 looks just like figure 1 . Any idea about what is going on here ?"
"I 'm excited to see the latest version of the decorator python module ( 3.0 ) . It looks a lot cleaner ( e.g . the syntax is more sugary than ever ) than previous iterations.However , it seems to have lousy support ( e.g . `` sour '' syntax , to horribly stretch the metaphor ) for decorators that take arguments themselves . Does anyone have a good example for how you 'd cleanly do this using decorator 3.0 ?"
"I have data sets containing the date ( Julian day , column 1 ) , hour ( HHMM , column 2 ) and seconds ( column 3 ) in individual columns : I 'm reading the text file using Pandas as : Now I want to convert the date to something more convenient like YYYY-MM-DD HH : MM : SS ( The year is n't provided in the data set , but is fixed at 2001 ) . I tried combining the three columns into one using parse_dates : which converts the three columns into one string : I next tried to convert them using date_parser ; following this post using something like : The date_parser itself works , but I ca n't get this to combine with read_table , and I 'm pretty much stuck at this point . Is there an easy way to achieve the conversion ? The full minimal ( not-so ) working example :"
"I 've got a bad smell in my code . Perhaps I just need to let it air out for a bit , but right now it 's bugging me.I need to create three different input files to run three Radiative Transfer Modeling ( RTM ) applications , so that I can compare their outputs . This process will be repeated for thousands of sets of inputs , so I 'm automating it with a python script.I 'd like to store the input parameters as a generic python object that I can pass to three other functions , who will each translate that general object into the specific parameters needed to run the RTM software they are responsible . I think this makes sense , but feel free to criticize my approach.There are many possible input parameters for each piece of RTM software . Many of them over-lap . Most of them are kept at sensible defaults , but should be easily changed.I started with a simple dictThere are a lot of parameters , and they can be cleanly categorized into groups , so I thought of using dicts within the dict : I like that . But there are a lot of redundant properties . The solar azimuth and zenith angles , for example , can be found if the other is known , so why hard-code both ? So I started looking into python 's builtin property . That lets me do nifty things with the data if I store it as object attributes : But now I 've lost the structure I had from the second dict example.Note that some of the properties are less trivial than my solar_zenith_angle example , and might require access to other attributes outside of the group of attributes it is a part of . For example I can calculate solar_azimuth_angle if I know the day of year , time of day , latitude , and longitude.What I 'm looking for : A simple way to store configuration data whose values can all be accessed in a uniform way , are nicely structured , and may exist either as attributes ( real values ) or properties ( calculated from other attributes ) .A possibility that is kind of boring : Store everything in the dict of dicts I outlined earlier , and having other functions run over the object and calculate the calculatable values ? This does n't sound fun . Or clean . To me it sounds messy and frustrating.An ugly one that works : After a long time trying different strategies and mostly getting no where , I came up with one possible solution that seems to work : My classes : ( smells a bit func-y , er , funky . def-initely . ) How I can use them : ( works as I would like ) The behavior : ( output of execution of all of the above code , as expected ) Why I do n't like it : Storing configuration data in class definitions inside of the main object 's __init__ ( ) does n't feel elegant . Especially having to instantiate them immediately after definition like that . Ugh . I can deal with that for the parent class , sure , but doing it in a constructor ... Storing the same classes outside the main Configuration object does n't feel elegant either , since properties in the inner classes may depend on the attributes of Configuration ( or their siblings inside it ) .I could deal with defining the functions outside of everything , so inside having things likebut I ca n't figure out how to do something like ( when I try to be clever about it I always run into < property object at 0xXXXXX > ) So what is the right way to go about this ? Am I missing something basic or taking a very wrong approach ? Does anyone know a clever solution ? Help ! My python code is n't beautiful ! I must be doing something wrong !"
"Ok , so my current curve fitting code has a step that uses scipy.stats to determine the right distribution based on the data , Where data is a list of numeric values . This is working great so far for fitting unimodal distributions , confirmed in a script that randomly generates values from random distributions and uses curve_fit to redetermine the parameters.Now I would like to make the code able to handle bimodal distributions , like the example below : Is it possible to get a MLE for a pair of models from scipy.stats in order to determine if a particular pair of distributions are a good fit for the data ? , something likeand use those pairs to get an MLE value of that pair of distributions fitting the data ?"
Is there an elegant way to iterate through possible dice rolls with up to five dice ? I want to replace this hacky Python : Desired result :
"The period of the Mersenne Twister used in the module random is ( I am told ) 2**19937 - 1 . As a binary number , that is 19937 ' 1 's in a row ( if I 'm not mistaken ) . Python converts it to decimal pretty darned fast : I guess the second version is the one that requires conversion ? And it 's not just binary . This is also fast . ( Rather than show the numbers , I show the length of the decimal converted to a string ) : Timing : The question is : how is this actually done ? Am I just naive to be impressed ? I find the sight of the Python shell generating a number of 5000 or so places in an instant truly spectacular.Edit : Additional timings suggested by @ dalke and @ truppoSo it looks to me like result = 0 ; result += 2**19937 probably does force the conversion ."
"Today I ran a bunch of doctests using Python 2.6 on a Ubuntu 9.10 with nose : WTF ? I had tests in that files , why did n't that work ? I changed permission to 644 : Changing it back to 777 : Why is that ? Using 644 , I ca n't even edit my files !"
"The double-checked locking idiom is not reliable in some languages , and I want to know whether Python is one of them . More concretely , is the following code ... ... thread-safe in Python ? Are there scenarios/implementations where it is/is n't ? Why ?"
"I have a regular expression in which I 'm trying to extract every group of letters that is not immediately followed by a `` ( `` symbol . For example , the following regular expression operates on a mathematical formula that includes variable names ( x , y , and z ) and function names ( movav and movsum ) , both of which are composed entirely of letters but where only the function names are followed by an `` ( `` . I would like the expression to return the arraybut it instead returns the arrayI can see in theory why the regular expression would be returning the second result , but is there a way I can modify it to return just the array [ ' x ' , ' y ' , ' z ' ] ?"
"I have a wrapper class similar to this ( strongly simplified ) example : I can use it like this : I thought I could optimize and get rid of one function call by changing to this : However , I receive a TypeError : 'wrap ' object does not support indexingfor the print w [ 2 ] line with the latter version.The direct call to the method , i.e. , print w.__getitem__ ( 2 ) , works in both cases ... Why does the assignment version not allow indexing ?"
I have a data frame in pandas like this : I want to remove all leap days and my code isbut the AttributeError happened : Whats wrong with my code ?
"How can I convert : into : in python ( it does n't need to be sorted , just flattened ) ? I 've tried lots of things but ca n't find how to solve this ."
"Is there any way to get a reference to the returned generator object , inside the generator 's definition ? This would be akin to the self argument passed to the method inside the __next__ method of an iterator . After browsing Python 's documentation , I have not found anything similar to it . This question arose while I was exploring how of much of the following paper 's ideas I can implement in Python using generators as coroutines . Paper : http : //citeseerx.ist.psu.edu/viewdoc/summary ? doi=10.1.1.19.79The closest I could do was the following , using a decorator , which builds on David Beazley 's coroutine decorator , but it feels like a bit of a hack.EDIT : The following class , based on the answer below , can be used as a decorator to have the generator receive a reference to self as its first paramter . It has the added benefit that any generator decorated with it will have the type coroutine ."
"I have a python program that loads quite a bit of data before running . As such , I 'd like to be able to reload code without reloading data . With regular python , importlib.reload has been working fine . Here 's an example : setup.py : foo/bar.pyrunner.py : But this does n't seem to work . If I edit bar.pyx and run reload_bar I do n't see my changes . I also tried pyximport.build_module ( ) with no luck -- the module rebuilt but did n't reload . I 'm running in a `` normal '' python shell , not IPython if it makes a difference ."
"I am looking to achieve several features to sweeten up the output of my program . I want to set the background of a column of text to a certain color so that it is more clear that columns belong together . A picture is worth a thousand words : i want to convert this : to this : How can i do that using python ? Is there a framework that alows me to do this ? I converted this into latex , however i can not get the superscripted numbers to work . They need to be above the text , but not acting as a character , since there is a relation between the columns of the picure . Example : A is at position 1 . It has as Properties S and O , thats why it is on top of those two and that is why there is an indent of half a character . S has the superscript 170 , T has the superscript 185.particularly hard is also the part where the Str part is shifted by half a character.I would like to have an example for this to integrate into my program ."
"Start with a list of strings . Each string will have the same number of characters , but that number is not predetermined , nor is the unrelated total number of strings.Here is an example : My desired final output is something like : My desired output is what I am expecting to see after using zip ( ) in some way . My problem is that I can not find the right way to zip each element of one array together.Essentially , I am trying to get the columns out of a group of rows.What I have tried so far : Splitting the data into a two dimensional list : split_data = [ list ( row ) for row in rows ] which gives me : Trying to use zip ( ) on that with something like : zip ( split_data ) I continue to end up with this : Obviously , it is zipping each element with nothing , causing it to return the original data tupled with a blank . How can I get zip ( ) to consider each element of data or split_data as a list that should be zipped ? What I need is zip ( data [ 0 ] , data [ 1 ] , data [ 2 ] , ... )"
"I have been using 'dis ' module in order to re-write some compiled script ( .pyc ) . I understand the difference between JUMP_FORWARD and JUMP_ABSOLUTE . To my knowledge an IF statement will be closed by a JUMP_FORWARD : And the JUMP_ABSOLUTE will appear if the IF statement is at the end of another loop . For instance : From the Bytecode I am reading to write back the code , there is a JUMP_ABSOLUTE that surprises me : I would think the code is the following : but it provokes a JUMP_FORWARD and not a JUMP_ABSOLUTE . I know it is not a WHILE loop , nor a FOR statement because they both create a SETUP_LOOP line in the Bytecode.My question is : what am I missing ? why do I get a FORWARD instead of ABSOLUTE jump ? EDIT : The absolute jump to index 27 points to the beginning of a ( WHILE ? ) loop in which these two lines 121 and 122 belong to : There is an IF-statement before and another one after these lines . Here is the code before , with the same JUMP_ABSOLUTE closing the statement.The JUMP_FORWARD says `` go to the next line '' and the JUMP_ABSOLUTE says `` go back to the beginning of the WHILE loop '' . Problem is I do n't know how to replicate a code that would give the same bytecode as above.Thank you !"
"One advantage of enums in Java and other languages is that I do not have to care about specific values ( in contrast to constants/static final members ) . For example , in Java I can do the following : At least to my knowledge , in python I have to do the following : If I have an enum with hundreds of names , then this can get quite cumbersome , because I always have to keep track whether a value was already assigned to a name or not . Is it maybe possible to achieve the same result as in Java without resorting to generators or the like ?"
"I 'd like to do something like this : with the idea being that someone can doand also get some magical machine learning happening ; this last bit is not of particular interest to me , it was just the first thing that occurred to me as a way to exemplify the use of the static method in both an in-function context and from the caller 's perspective.Anyway , this does n't work , because is_silly_enough is not actually a function : it is an object whose __get__ method will return the original is_silly_enough function . This means that it only works in the `` normal '' way when it 's referenced as an object attribute . The object in question is created by the staticmethod ( ) function that the decorator puts in between SillyWalk 's is_silly_enough attribute and the function that 's originally defined with that name.This means that in order to use the default value of appraisal_method from within either SillyWalk.walk or its caller , we have to eithercall appraisal_method.__get__ ( instance , owner ) ( ... ) instead of just calling appraisal_method ( ... ) or assign it as the attribute of some object , then reference that object property as a method that we call as we would call appraisal_method.Given that neither of these solutions seem particularly Pythonic™ , I 'm wondering if there is perhaps a better way to get this sort of functionality . I essentially want a way to specify that a method should , by default , use a particular class or static method defined within the scope of the same class to carry out some portion of its daily routine.I 'd prefer not to use None , because I 'd like to allow None to convey the message that that particular function should not be called . I guess I could use some other value , like False or NotImplemented , but it seems a ) hackety b ) annoying to have to write an extra couple of lines of code , as well as otherwise-redundant documentation , for something that seems like it could be expressed quite succinctly as a default parameter.What 's the best way to do this ?"
"I am working on an image search project for which i have defined/extracted the key point features using my own algorithm . Initially i extracted only single feature and tried to match using cv2.FlannBasedMatcher ( ) and it worked fine which i have implemented as below : But now i have one more feature descriptor for each key point along with previous one but of different length.So now my feature descriptor has shape like this : Now since each point 's feature descriptor is a list two lists ( descriptors ) with different length that is ( 10 , 7 , ) so in this case i am getting error : setting an array element with a sequence.while converting feature descriptor to numpy array of float datatype : I understand the reason of this error is different length of lists , so i wonder What would be the right way to implement the same ?"
"The class collections.defaultdict takes a default factory , used to generate a default value.If the values contained in the dict-like object should default to False , the instance can be created as : What is the most pythonic way to achieve the same for a default value of True ? In other terms , is there a standard callable object returning True which is idiomatically used as the relative of bool ? Of course , the factory could be built as a lambda expression : but this might be reinventing the wheel ."
"Is there a simple way of escaping the magic characters used for variable substitution in a buildout configuration , such that the string is left alone . In other words , where I say : I do n't actually want it to expand $ { variable } but leave it as the literal value.In practice the specific problem I am encountering is not in the buildout configuration file itself , but in a template file processed by the recipe 'collective.recipe.template ' . This uses the same variable substitution engine from buildout that is used in the configuration files . Problem is that the file I want to use as a template already uses ' $ { variable } ' syntax for its own purposes in conjunction with the application configuration system which ultimately consumes the file.The only way I have found to get around the problem is to use something like : In the template input file then have : instead of : that it already had.What this is doing is cause a lookup of 'dollar ' attribute against the section using the template and replace it with ' $ '.Rather than have to do that , was sort of hoping that one could do : or perhaps even : and eliminate the need to have to have a dummy attribute to trick it into doing what I want.Looking at the source code for buildout , the way it matches variable substitution does n't seem to provide an escape mechanism.If there is indeed no way , then perhaps someone knows of an alternate templating recipe for buildout that can do variable expansion , but provides an escape mechanism for whatever way it indicates variables , such that one can avoid problems where there may be a clash between the templating systems expansion mechanism and literal data in the file being templated ."
"Result : I know it is not normal to use two `` = '' s and `` is '' s in the if sentence . But I would like to know how the Python interpreter intereprets the if statement.Is the expression lie is lie is lie interpreted simultaneously , or short-circuit way ?"
"When I get a runtime exception from the standard library , it 's almost always a problem in my code and not in the library code . Is there a way to truncate the exception stack trace so that it does n't show the guts of the library package ? For example , I would like to get this : and not this : update : added an answer with the code , thanks to the pointer from Alex ."
"When I colored an histogram it accept a list for the different colors , however , for hatching it accept only one value.This is the code : How can I have different hatch for the different series ?"
"I want to process a very large itertools.product object . The issue is something like this : I know the items 's length is 62**5 . If I want to process the elements of items whose indices range from 300000 to 600000 , how to achieve this ? I have tried to convert the itertools.product to python list , like this : but it seems the conversion has consumed a large amount of memory since I have been waiting for a long time for this convert , and finally gave it up.I have this demand because I want to do this thing in python gevent , so I want to slice the large itertool.product to small items for gevent spawn ."
"I 'm trying to map out the uses/causes of functions and variables in a python package at the function level . There are several modules where functions/variables are used in other functions , and I 'd like to create a dictionary that looks something like : The functions that I am referring to need to be defined in modules of the package.How would I start on this ? I know that I can iterate through the package __dict__ and test for functions defined in the package by doing : But after that I need to find the functions used within the current function . How can this be done ? Is there something already developed for this type of work ? I think that profiling libraries might have to do something similar to this ."
"Given a list of rotation angles ( lets say about the X axis ) : I can create an array of matrices matching these angles by doing so : This will work but it does n't take advantage of numpy 's strengths for dealing with large arrays ... So if my array of angles is in the millions , doing it this way wo n't be very fast.Is there a better ( faster ) way to do create an array of transform matrices from an array of inputs ?"
"Some R datasets can be loaded into a Pandas DataFrame or Panel quite easily : This appears to work as long as the dimension of the R dataset is < = 3 . Higher dimensional datasets print an error message : This error message originates in the rpy/common.py _convert_array function . Sure , it makes sense that Pandas can not directly shoe-horn a 4-dimensional matrix into a DataFrame or Panel , but is there some workaround to load datasets like Titanic into a DataFrame ( maybe with a hierarchical index ) ?"
"A short intoduction to the problem ... PostgreSQL has very neat array fields ( int array , string array ) and functions for them like UNNEST and ANY.These fields are supported by Django ( I am using djorm_pgarray for that ) , but functions are not natively supported.One could use .extra ( ) , but Django 1.8 introduced a new concept of database functions.Let me provide a most primitive example of what I am basicly doing with all these . A Dealer has a list of makes that it supports . A Vehicle has a make and is linked to a dealer . But it happens that Vehicle 's make does not match Dealer 's make list , that is inevitable.Having a database of dealers and makes , I want to count all vehicles for which the vehicle 's make and its dealer 's make list do match . That 's how I do it avoiding .extra ( ) .Resulting SQL : And it works , and much faster than a traditional M2M approach we could use in Django . BUT , for this task , UNNEST is not a very good solution : ANY is much faster . Let 's try it.It generates the following SQL : And it fails , because braces around ANY are bogus . If you remove them , it runs in the psql console with no problems , and fast.So my question.Is there any way to remove these braces ? I could not find anything about that in Django documentation.If not , - maybe there are other ways to rephrase this query ? P . S. I think that an extensive library of database functions for different backends would be very helpful for database-heavy Django apps . Of course , most of these will not be portable . But you typically do not often migrate such a project from one database backend to another . In our example , using array fields and PostGIS we are stuck to PostgreSQL and do not intend to move.Is anybody developing such a thing ? P . P. S. One might say that , in this case , we should be using a separate table for makes and intarray instead of string array , that is correct and will be done , but nature of problem does not change.UPDATE.TextArrayField is defined at djorm_pgarray . At the linked source file , you can see how it works.The value is list of text strings . In Python , it is represented as a list . Example : [ 'honda ' , 'mazda ' , 'anything else ' ] .Here is what is said about it in the database.And underlying PostgreSQL field type is text [ ] ."
"I create my connection to a server like this : Now , in some other method , where i have a handle on this protocol object I want to test if protocol is still connected , something like : Is there any way to do this . I looked at the twisted documentation and couldnt find a relevant method . Setting a flag in the connectionLost ( ) callback is an option , but I was wondering if I could avoid doing that ."
"I have a large dataframe > 5000000 rows that I am performing a rolling calculation on . I would like to do the same calculations but add in a weighted sum.However , I am getting a Length Mismatch expected axis has 5 elements error.I know I could do something like [ a *b for a , b in zip ( L , weight ) ] if I converted everything to a list but I would like to keep it in a dataframe if possible . Is there a way to multiply against different size frames or do I need to repeat the set of numbers the length of the dataset I 'm multiplying against ?"
"There are two version of my little tool : https : //pypi.python.org/pypi/tbzuploader/2017.11.0https : //pypi.python.org/pypi/tbzuploader/2017.12.0 Bug : The pypi page looks ugly.In the last update a change in README.rst cases a warning : Now the pypi page looks ugly : - ( I use this recipe to do CI , bumpversion , upload to pypi : https : //github.com/guettli/github-travis-bumpversion-pypiHow could I ensure that no broken README.rst gets released any more ? With other words I want to avoid that the pypi page looks ugly.Dear detail lovers : Please do n't look into the current particular error in the README.rst . That 's is not the question : - )"
"I 'm fairly new to Python , and this is a rather basic question.I 've got a list lst and have at some point computed an integer n with 0 < =n < =len ( lst ) that tells me that ( for the purpose at hand ) I should discard the final n elements of the list . The statement that comes to mind to achieve this is del lst [ -n : ] or maybe lst [ -n : ] = [ ] ( I 've found out that in either case adding 0 after the colon is not a good idea , as zero is not considered to be larger than -n in this setting ) . However both variants fail when n==0 , as the list gets emptied entirely instead . Should I really insert an explicit test for zero here , or stoop down to writingor is there something more elegant ( and reliable ) that can be done here ?"
"I am currently following the instructions laid out here for finding values , and it works . The only problem is my dataframe is quite big ( 5x3500 rows ) and I need to perform around ~2000 searches . Each one takes around 4 seconds , so obviously this adds up and has become a bit unsustainable on my end.Most concise way to select rows where any column contains a string in Pandas dataframe ? Is there a faster way to search for all rows containing a string value than this ?"
"The script below illustrates a capability of set and frozenset that I would like to understand , and , if possible replicate in a subclass of collections.MutableSet . ( BTW , this feature is not just a oddity of set and frozenset : it is actively verified in Python 's unit tests for these types . ) The script performs the following steps for each of several types/classes of set-like objects : create a dict d whose n keys are specially instrumented integers that keep track of how many times their __hash__ method is invoked ( d 's values are all None , but this is irrelevant ) ; compute ( and save for later ) the cumulative number of times the __hash__ method of d 's keys has been called so far ( i.e . during the creation of d ) ; create an object s of the current set-like type/class , using d as the argument to the constructor ( hence , d 's keys will become the contents of the resulting object , whereas d 's values will be ignored ) ; redo the calculation described in ( 2 ) ; output the results from calculations from ( 2 ) and ( 4 ) above.Here 's the output for the case where n is set to 10 for all types/classes ( I give the full code at the end of this post ) : The conclusion is clear : constructing a set or a frozenset from d does not require invoking the __hash__ method of d 's keys , hence the call counts remain unchanged after these constructors return . This is not the case , however , when instances of Set or myset are created from d. In each of these cases it appears that each of d 's keys ' __hash__ is invoked once . How can I modify myset ( see below ) so that running its constructor with d as its argument results in no calls to d 's keys ' hash methods ? Thanks ! Note that , the values of self.dictset are integers , and are pointedly not the same as the ( ignored ) iterable.values ( ) ( in those cases where iterable.values actually exists ) ! This is an attempt ( admittedly feeble ) to indicate that , even when iterable is a dict ( which need not be the case ) and its values are ignored , in the real code that this example is standing in for , the values of self.dictset are always significant . This means that any solution based on using self.dictset.update ( iterable ) still has to solve the problem of assigning the proper values to its keys , and once again one faces the problem of iterating over these keys without invoking their __hash__ methods . ( In addition , solutions based on self.dictset.update ( iterable ) also have to solve the problem of properly handling the case when iterable is not a suitable argument for self.dictset.update , although this problem is not insurmountable . ) Edits : 1 ) clarified the significance of myset.dictset 's values ; 2 ) renamed myset.__bomb__ to myset.__bomb ."
"I would like to run the following Django command ( to dump the contents of my database into a text file ) : I 'm using the Django framework within PyDev as an Eclipse plugin . Therefore , in order to run the above command in PyDev I go to `` Custom command '' and insert dumpdata my_app > data.json . However , doing so results in an error because of the > character : Error : Unknown application : > How can I use > ( redirection ) in PyDev ?"
"I 'm relatively new to python and I 'm confused about the performance of two relatively simple blocks of code . The first function generates a prime factorization of a number n given a list of primes . The second generates a list of all factors of n. I would have though prime_factor would be faster than factors ( for the same n ) , but this is not the case . I 'm not looking for better algorithms , but rather I would like to understand why prime_factor is so much slower than factors.Using the timeit module , { i : factors ( i ) for i in range ( 1 , 10000 ) } takes 2.5 seconds { i : prime_factor ( i , primes ) for i in range ( 1 , 10000 ) } takes 17 secondsThis is surprising to me . factors checks every number from 1 to sqrt ( n ) , while prime_factor only checks primes . I would appreciate any help in understanding the performance characteristics of these two functions.ThanksEdit : ( response to roliu ) Here is my code to generate a list of primes from 2 to up_to :"
"If there are duplicate values in a DataFrame pandas already provides functions to replace or drop duplicates . In many experimental datasets on the other hand one might have 'near ' duplicates . How can one replace these near duplicate values with , e.g . their mean ? The example data looks as follows : I tried to hack together something to bin together near duplicates but this is using for loops and seems like a hack against pandas : Which does the grouping and averaging : Is there a better way to achieve this ?"
"I am trying to run an extremely simple CGI server on VMS done in python . The problem I have is that it serves out static content properly and it tries to execute the CGI-s ( it is in the right place , and Ihave used those CGIs with Apache so that part is definitely not the issue ) but it hangs somewhere . It is something I do n't know about VMS.Any pointer to the right direction would be appreciated . : ) Update : Simplified , I need to execute a program on VMS and get the results of that program somehow . Any reference to executing subprocesses and getting their results is enough for me ."
"I have a Rust library that needs to be imported into Python via the ctypes module . My goal is to use Rust functions that take Vec < T > / i32 as arguments and return those types , from Python . Currently , I can pass integers to the Rust functions , and have them return lists / integers . Here is the current code : Python : Rust : What I need help with is passing a Python list as an argument to a Rust function . My best guess is to pass a ctypes.ARRAY to the function rather than a list , but I am not sure how to go about converting a Python list to that type.Note : I tried the Rust code from this related question but it says `` linking with ` gcc ` failed : exit code : 1 '' and `` bad reloc address '' when I try to compile it ."
"The logging docs say that calling the logging.disable ( lvl ) method can `` temporarily throttle logging output down across the whole application , '' but I 'm having trouble finding the `` temporarily . '' Take , for example , the following script : So far , I have n't been able to find the Something here that will re-enable the logging system as a whole and allow the second warning to get through . Is there a reverse to disable ( ) ?"
"i have a python chatserver that uses twisted and autobahn websockets for connection.this is the serveran independent html/js client can connect and send chat messages . but i want to implement an open id authentication ( performed by the server ) , before opening the websocket . this is the onload function : as i 'm new to python/twisted i do n't know how to do this and examples mostly show only websocket chatroom without authentification . how can i implement openid properly ? as it also requires redirection , which would break the ws connection ."
"I have nested dictionary like this : In my case , i need to sum all values based on its similar keys in every year , from 2010 till 2012..So the result i expected should be like this :"
Here is sample code of the request / response pattern with zeroMQ in python . I would like to know if there is way to process requests from multiple clients concurrently ?
I have two lists : I want to get the combinations produced between the elements of list b and the elements of list a but treating elements of a as pairs ( or triples etc . etc . ) as the example below which gives len ( b ) ** len ( a ) number of combinations.I have tried to use itertools.product ( as described here ) but this will give only the 6 possible combinations .
I want to use MySQLdb to create a parameterized query such as : But what ends up being send to the DBMS is : Is it possible to create a parameterized query like this ? Or do I have to loop myself and build up a result set ? Note : executemany ( ... ) wo n't work for this - it 'll only return the last result : Final solution adapted from Gareth 's clever answer :
"Appreciate any help on this task ! I 'm trying to have this dash app take slider input value to change a variable through a function and then change the marker colour variable only.The code is written in python and uses plotly-dash and plotly and as well as pandas numpy and mapbox.The top part of the code is getting the data into the right format . It has traffic data which is being processed to product a heatmap that shows congestion over time on a map . The dataframe DF is for volume of traffic , and the dataframe HF was created so the slider would work ( i added a column numbered 0 to number of columns to use with the slider ) - the function datatime should choose the volumes based on the time and the detector ID.I originally created this functionality with javascript as shown here - https : //jsbin.com/detejef/edit ? html , js , outputI 've been working at this code for awhile . Very close to finally getting a prototype but have this one snag - the variable of time does n't update properly and reupdate the map with the detector changes ... I just need marker dictionary sub function colour to change with the slider value changing in conjunction with the functions I 've created . The function works by itself.This is an update to the code.This is the app section with only the useful parts included . App functions/ callbacks Example data ( anonamized ) The key issue I have determined is that HF is not being pulled into the function after the initial call . I am not sure why - it should work just as the time value on the slider changes . The function itself clearly works though - it is definitely that HF is not being brought into def update_map ."
"Python itemgetter does n't return tuples of length 0 or 1 . For example : Is there any good reason that the itemgetter is written this way ? And not ( 1 , ) for the second last ( ) and for the last ? Is there some other built-in option if I always want to return a tuple/list given the keys ?"
"ok so I am feeling a little stupid for not knowing this , but a coworker asked so I am asking here : I have written a python algorithm that solves his problem . given x > 0 add all numbers together from 1 to x.first what is this type of equation is this and what is the correct way to get this answer as it is clearly easier using some other method ?"
"I was using Django 's .only ( ) construct to query only the columns I need.ex.Trainer.objects.filter ( id__in=ids ) .only ( 'id ' , 'lives ' ) Here is a quick example I wrote to reproduce this : I needed only id and name fields , that 's certain.And what was the reason for maximum recursion depth exceeded ?"
"I am writing a python script that takes two arguments , and some options : The -q ( quiet ) and -d ( debug ) options change the verbosity level , and the -h option is the help option automatically created by argparse.I would like the -l ( list ) option to behave similarly to the -h option in that it will not require that the ( otherwise mandatory ) arguments are present and list some useful information ( different from the -h option ) . In practice , this means that the script could be called in the following three ways : Two possible ways forward would be to : Make the arguments optional ( with nargs= ' ? ' ) and add code to verify that there are two arguments in all cases where there -l og -h options are not given.Write a custom action class ( not sure about the details ) .But I hope there is a more straightforward way to inherit the `` this option is all you need '' behaviour from the help option.Solution ( based on samwyse 's answer ) : Based on the _HelpAction ( ) in argparse.py : and then , during parser setup :"
"I have this part of the code in my API which recently has become a somewhat bottleneck : Note that results is the queryset of all the people and offset is a param used for pagination . Here I can see , when I print connection.queries , that my database getting hit twice by .count ( ) and list ( results ) . The reason why .count ( ) has to be at the top because I need to the length of all the people ( Not 100 . ) Is there a way to get around this ?"
"I made a simple converter in Python to convert images to ASCII . Right now it uses various shades of dark characters , so it works but it is hard to make out at low resolutions : for example , the Google logo comes out as : This can barely be made out . Is there a way that I could compare each section to a subset of Unicode characters and return the most similar , so it could return for example something like :"
"Suppose I want to get the reference to some global / internal c++ object , one method is to declare function with boost : :python : :return_value_policy < reference_existing_object > ( ) .Both GetGlobalObjectA and GetGlobalObjectB return the reference to the original c++ object without create a new copy ; But how to make GetGlobalObjectByID return a ref to the existing c++ object ?"
I am currently using the book 'Programming in D ' for learning D. I tried to solve a problem of summing up the squares of numbers from 1 to 10000000 . I first made a functional approach to solve the problem with the map and reduce but as the numbers get bigger I have to cast the numbers to bigint to get the correct output.The above takes 7s to finish when compiled with dmd -O . I profiled the program and most of the time is wasted on BigInt calls . Though the square of the number can fit into a long I have to typecast them to bigint so that reduce function sums and returns the appropriate sum . The python program takes only 3 seconds to finish it . When num = 100000000 D program gets to 1 minute and 13 seconds to finish . Is there a way to optimize the calls to bigint . The products can themselves be long but they have to be typecasted as bigint objects so that they give right results from reduce operations . I tried pushing the square of the numbers into a bigint array but its also slower . I tried to typecast all the numbers as Bigint But its also slower . I read the answers at How to optimize this short factorial function in scala ? ( Creating 50000 BigInts ) . Is it a problem with the implementation of multiplication for bigger integers in D too ? Is there a way to optimize the function calls to BigInt ? python code : The code was executed on a dual-core 64 bit linux laptop with 2 GB RAM.python : 2.7.4dmd : DMD64 D Compiler v2.066.1
"For a ConvNet it can be interesting to find the norm-bounded input that maximizes the activity of a single conv . filter as a way to visualize the filters . I 'd like to do this in the deep learning package Keras . This could be done using a black box optimization algorithm with the code from the FAQ.However , it would be a substantially easier optimization task if I had the gradient . How can I extract the gradient from the Theano expression and input it into a Python optimization library such as Scipy ?"
"I was reading another question on Stack Overflow ( Zen of Python ) , and I came across this line in Jaime Soriano 's answer : Entering the above in a Python shell prints : And so of course I was compelled to spend my entire morning trying to understand the above list ... comprehension ... thing . I hesitate to flatly declare it obfuscated , but only because I 've been programming for just a month and a half and so am unsure as to whether or not such constructions are commonplace in python.this.s contains an encoded version of the above printout : And this.d contains a dictionary with the cypher that decodes this.s : As far as I can tell , the flow of execution in Jaime 's code is like this:1. the loop c for c in this.s assigns a value to c2 . if the statement c in this.d evaluates to True , the `` and '' statement executes whatever happens to be to its immediate right , in this case this.d [ c ] .3. if the statement c in this.d evaluates to False ( which never happens in Jaime 's code ) , the `` or '' statement executes whatever happens to be to its immediate right , in this case the loop c for c in this.s.Am I correct about that flow ? Even if I am correct about the order of execution , this still leaves me with a ton of questions . Why is < 1 > the first thing to execute , even though the code for it comes last on the line after several conditional statements ? In other words , why does the for loop begin to execute and assign value , but then only actually return a value at a later point in the code execution , if at all ? Also , for bonus points , what 's with the weird line in the Zen file about the Dutch ? Edit : Though it shames me to say it now , until three seconds ago I assumed Guido van Rossum was Italian . After reading his Wikipedia article , I at least grasp , if not fully understand , why that line is in there ."
"I 'm debianizing a Python package , which has a bit weird dependencies . It either : Depends on python2.7Depends on python2.6 and python-ordereddict ( my self-built package for ordereddict on PyPI ) For example , in my setup.py I have : I have n't found anything in Debian packaging documentation on this matter . Just out of the blue I 've tried writingBut , no surprisingly , it is a wrong syntax that did n't work : I 'm using dh_python2 and $ { python : Depends } provides quite unreasonable list likeWith such dependency list , it 'll require python-ordereddict for python2.7 , that does not exist . And obviously I ca n't patch python2.7-minimal to say Provides : python-ordereddict ( like it 's done with python-argparse ) .Any suggestions on how to correctly package such library , please ?"
"[ edit ] This is not a pure duplicate of the PySide emit signal causes python to crash question . This question relates specifically to a ( now ) known bug in PySide preventing None from being passed across threads . The other question relates to hooking up signals to a spinner box . I 've updated the title of this question to better reflect the problem I was facing . [ /edit ] I 've banged my head against a situation where PySide behaves subtly different from PyQt . Well , I say subtly but actually PySide crashes Python whereas PyQt works as I expect.I 'm completely new to PySide and still fairly new to PyQt so maybe I 'm making some basic mistake , but damned if I can figure it out ... really hoping one of you fine folks can give some pointers ! The full app is a batch processing tool and much too cumbersome to describe here , but I 've stripped the problem down to its bare essentials in the code-sample below : Running this displays a window with a `` do it '' button . Clicking it crashes Python if running under PySide . Uncomment the ImportError on line 4 to see PyQt* correctly run the Dummy function . Or uncomment the return statement on line 20 to see PySide correctly run.I do n't understand why emitting None makes Python/PySide fail so badly ? The goal is to offload the processing ( whatever Dummy does ) to another thread , keeping the main GUI thread responsive . Again this has worked fine with PyQt but clearly not so much with PySide.Any and all advice will be super appreciated.This is under :"
How do I check if a sympy expression evaluates to nan ? I simply need to do something like this :
"I 'm creating some initial tests as I play with django-revisions . I 'd like to be able to test that some of my api and view code correctly saves revisions . However , I ca n't get even a basic test to save a deleted version.This fails when checking the length of the deleted QuerySet with : My hypothesis is that I need to create the initial revisions of my model ( do the equivalent of ./manage.py createinitialrevisions ) . If this is the issue , how do I create the initial revisions in my test ? If that is n't the issue , what else can I try ?"
I 'm trying to use python3 's new library asyncio .but I 'm getting this error dont know why . any help would be appreciated .
"In the docs , it says ( emphasis mine ) : Advanced indexing is triggered when the selection object , obj , is a non-tuple sequence object , an ndarray ( of data type integer or bool ) , or a tuple with at least one sequence object or ndarray ( of data type integer or bool ) . There are two types of advanced indexing : integer and Boolean . < snip > Also recognize that x [ [ 1,2,3 ] ] will trigger advanced indexing , whereas x [ [ 1,2 , slice ( None ) ] ] will trigger basic slicing.I know why x [ ( 1 , 2 , slice ( None ) ) ] triggers basic slicing . But why does x [ [ 1,2 , slice ( None ) ] ] trigger basic slicing , when [ 1,2 , slice ( None ) ] meets the condition of being a non-tuple sequence ? On a related note , why does the following occur ?"
"I have a pd series , when I do , how can I get the same result by using formatted string ?"
"I 'm really confused on why this is happening to me : Could someone shine a light ? As you can see , this is a plain input – what I thought would be a float literal . I 've tried to take away the ipython complexity , and run in isolated mode , and but still : Is my python3.7 broken , is my understanding broken , or what am I looking at ? I ruled out encoding problems by putting shown code into a file and hexdumping that . It 's clean ascii , as it should be , with a 0x2e for a dot , and a 0x0a as a line ending : OS : Fedora 29 on an x86_64 with 16 GB of RAM ."
"I have a nested dictionary structure with tuple keys . Here 's what an entry looks like when I pretty-print the dictionary using pprint : It 's pretty nifty , but I 'd like to customize it further by cutting down some extra digits from the floats . I was thinking that it 'd be possible to achieve by subclassing pprint.PrettyPrint but I do n't know how that would be done.Thanks ."
Going through some tutorials on matplotlib animations and encountered this problem . I am using the matplotlib.animation funcanimation as follows : This generates the following output:0012345and so on . It is calling the first argument twice . How can I prevent this ?
"I 'm trying to parallelize a for loop to speed-up my code , since the loop processing operations are all independent . Following online tutorials , it seems the standard multiprocessing library in Python is a good start , and I 've got this working for basic examples.However , for my actual use case , I find that parallel processing ( using a dual core machine ) is actually a little ( < 5 % ) slower , when run on Windows . Running the same code on Linux , however , results in a parallel processing speed-up of ~25 % , compared to serial execution.From the docs , I believe this may relate to Window 's lack of fork ( ) function , which means the process needs to be initialised fresh each time . However , I do n't fully understand this and wonder if anyone can confirm this please ? Particularly , -- > Does this mean that all code in the calling python file gets run for each parallel process on Windows , even initialising classes and importing packages ? -- > If so , can this be avoided by somehow passing a copy ( e.g . using deepcopy ) of the class into the new processes ? -- > Are there any tips / other strategies for efficient parallelisation of code design for both unix and windows.My exact code is long and uses many files , so I have created a pseucode-style example structure which hopefully shows the issue.EDIT : I do not believe the question is a duplicate of the one suggested , since this relates to a difference in Windows and Linunx , which is not mentioned at all in the suggested duplicate question ."
"I 'm trying to use KernelPCA for reducing the dimensionality of a dataset to 2D ( both for visualization purposes and for further data analysis ) .I experimented computing KernelPCA using a RBF kernel at various values of Gamma , but the result is unstable : ( each frame is a slightly different value of Gamma , where Gamma is varying continuously from 0 to 1 ) Looks like it is not deterministic.Is there a way to stabilize it/make it deterministic ? Code used to generate transformed data :"
I am running a @ nb.njit function within which I am trying to put an integer within a string array.I am getting the following error : Which function am I supposed to use to do the conversion from float to string within numba ?
"My question is a very simple one.Does a for loop evaluates the argument it uses every time ? Such as : Does python create a list of 300 items for every iteration of this loop ? If it is , is this a way to avoid it ? Same goes for code examples like this.Is the reverse process applied every single time , or the length calculated at every iteration ? ( I ask this for both python2 and python3 ) If not , how does Python evaluate the changes on the iterable while iterating over it ?"
"I 'm trying to simulate clicking `` publish to web '' - > `` start publishing now '' in Google docs using the Python version of the Google Drive API . Based on my vague understanding of the documentation , I believe this should work : However , this appears to have no effect on my document . I 'd like to be able to programmatically create a Google spreadsheet that is immediately world-accessible ."
"I read that it is considered bad practice to create a variable in the class namespace and then change its value in the class constructor . ( One of my sources : SoftwareEngineering SE : Is it a good practice to declare instance variables as None in a class in Python . ) Consider the following code : To force its subclass to declare the variable necessary_var , the class mixin uses the metaclass subclass_validator.And the only way I know to makes it work on app.py side , is to initialized necessary_var as a class variable.I am missing something or is it the only way to do so ?"
"I would like to have some callback run whenever a particular module is imported . For example ( using a fake @ imp.when_imported function that does not really exist ) : This feature was designed in PEP 369 : Post import hooks but was withdrawn with the reason : This PEP has been withdrawn by its author , as much of the detailed design is no longer valid following the migration to importlib in Python 3.3.But importlib has no clear solution . How does one use importlib to implement a post-import hook ?"
"Assuming that not all the categories have at least a product , how i can get all the categories that have at least one product associated ? Is there a way to do this with Django querysets ?"
"My question is about how to serve multiple urls.py ( like urls1.py , urls2.py and so on ) files in a single Django project . I am using Win7 x64 , django 1.4.1 , python 2.7.3 and as a server django dev-server tool.I have decided to use a method which i found by google from http : //effbot.org/zone/django-multihost.htm I have created a multihost.py file and put in to the django middleware folder : With the following code : Also in my project setting.py file i have added a mapping dictionary like the link above shows : I did not yet implemented the error handling like described by the link above . My hosts file includes the follwoing : Project structure is the following : effbot django project folder : There is no templates folder , because i dont serve html items from files , they are coming from databsse ( i doubt that my problem is in this ) . Now the problem is : when i go for the adress in my browser with django dev-server launched i get code 301 from the server . And browser shows `` can not display page '' message . Could you please explain me how to use mentioned method ? I 'm new to django and have n't done any real projects yet . Just have read the docs and launched a couple of sites at home to learn how it works . I expect that urlconfs will be called in dependance from incoming The target is to serve different urlconfs for mysite1.com and mysite2.comin a single django project.I think this should to work some how . Thank you for any feedback . EDIT : After some research attempts i found that i plugged my multyhost.py incorrectly in settings.Fixed now . But the same result still . Also i found out that my django dev-server tool is not reflecting anyhow that it handles any requests from the browser ( IE9 ) except when i do `` http : //127.0.0.1 '' .May be i have to try some production server for my task , like nginx ?"
Count the longest sequence of heads and tails in 200 coin flips.I did this - is there a niftier way to do it in python ? ( without being too obfuscated ) see this for what prompted my playing
"I have this Document in mongo engine : And I have this data into the DocumentOk if I want to push to mongo list using mongoengine , i do this : That works pretty well , if a query the database again i get thisBut When I try to pull from same list using pull in mongo engine : I get this error : mongoengine.errors.OperationError : Update failed ( Can not apply $ pull to a non-array value ) comparising the sentences : How can I pull from this list ? I appreciate any help"
"I have a json object which is very deep . In other words I have a dictionary , containing dictionaries containing dictionaries and so on many times . So , one can imagine it as a huge tree in which some nodes are very far from the root node.Now I would like to cut this tree so that I have in it only nodes that are separated not more than N steps from the root . Is there a simple way to do it ? For example if I have : And I want to keep only nodes that are 2 steps from the root , I should get : So , I just replace the far standing dictionaries by single values ."
"What 's the benefit of marking my_fixture as a pytest fixture ? It seems like the fixture 's benefit is the same with my_fixture just being a normal function , removing the decorator . I see the benefit only here where my_fixture is run simply by providing it as a parameter to test_my_fixture : This should print :"
"I am using ANTLR4 with Python and I am currently using the following code for parsing : However , I would like to change this code to parse directly from a given string instead of a given path . Thus changing the first line to something similar toHow do I do this ?"
"How to import or otherwise bind a pytest fixture for interactive use , without using breakpoints ? For example , I want to explore some behaviours of the tmpdir_factory fixture in the REPL . In the case of tmpdir I already know it 's just a py.path.local instance , but I 'm interested in the general question for user defined fixtures too . edit : Another acceptable interface : edit : an MCVE to show whether context has exited or not :"
I 've been looking around for answers but I ca n't seem to see why in my code I ca n't get the projected contours to show up `` behind '' the surface.I have been using the one of the Filled Contour Plots from this page as template : http : //matplotlib.org/mpl_toolkits/mplot3d/tutorial.html # scatter-plotsAt the moment the best I could get was to change the alpha value of the contours . I also tried plotting the surface after the contours but that did n't change anything.Any advice would be most welcome !
"I have the following code : FileForMixin defined as follows : FileFormMixin is provided by https : //github.com/mbraak/django-file-form , betterforms by https : //github.com/carljm/django-form-utils.The problem is , FileFormMixin 's __init__ is never getting called . How can I fix that ? I really need all of them . Right now it 's executing only FilterForm and NewEntityForm constructors.UPDATESo , I looked at all mentioned classes __init__ 's , and they 're calling super ( ) ! FileFormMixin : BetterForm : More of that , printing class ' mro as @ elwin-arens proposed , gives following output : filter form __init__NewEntityForm.__mro__ ( < class 'myapp.forms.NewEntityForm ' > , < class 'myapp.forms.FilterForm ' > , < class 'form_utils.forms.BetterForm ' > , < class 'django.forms.widgets.NewBase ' > , < class 'form_utils.forms.BetterBaseForm ' > , < class 'django.forms.forms.Form ' > , < class 'django.forms.forms.BaseForm ' > , < class 'django_file_form.forms.FileFormMixin ' > , < class 'object ' > ) newsiteform __init__But __init__ for FileFormMixin is executed only if I call it explicitly as @ tom-karzes advised"
"I am trying to parse a html page with the given format : When iterating over the div tags I want to figure out whether the current div tag is under img tag with id 'first ' , 'second ' or 'third ' . Is there a way to do that ? I have the list of img blocks and div blocks :"
"I 'm using Python since some times and I am discovering the `` pythonic '' way to code.I am using a lot of tuples in my code , most of them are polar or Cartesian positions.I found myself writing this : instead of this : to get rid of the double parenthesis I found hard to read.It seems that python is automatically doing the type conversion from list to tuple , as my code works properly.But is it a good way to code ? Do you have any presentation tip I could use to write readable code ? Thank you in advance for your surely enlightening answers ."
"I 'm working with a list of dict objects that looks like this ( the order of the objects differs ) : What I want to do is remove duplicate names , keeping only the one of each name that has the highest 'score ' . The results from the above list would be : I 'm not sure which pattern to use here ( aside from a seemingly idiotic loop that keeps checking if the current dict 's 'name ' is in the list already and then checking if its 'score ' is higher than the existing one 's 'score ' ."
"I 've been trying to do the following : What I tried to accomplish here is to have the same documentation for this private method as the documentation of the method simulate , except with a short introduction . This would allow me to avoid copy-pasting , keep a shorter file and not have to update the documentation for two functions every time.But it does n't work . Does anyone know of a reason why , or whether there is a solution ?"
"I use anaconda ( python3.6 ) , tensorflow 1.12.0 to learn object detection on windows 10.I used this command to train : cd E : \test\models-master\research\object_detection python model_main.py -- pipeline_config_path=training/ssd_mobilenet_v1_coco.config -- model_dir=training/ -- num_train_steps=10000When the model will finish training , there was such a mistake : The content in ssd_mobilenet_v1_coco.config : The generated ckpt files in my training directory look like this：I did n't find anything wrong.So where is wrong ?"
"I understand how mathematically-equivalent arithmentic operations can result in different results due to numerical errors ( e.g . summing floats in different orders ) .However , it surprises me that adding zeros to sum can change the result . I thought that this always holds for floats , no matter what : x + 0 . == x.Here 's an example . I expected all the lines to be exactly zero . Can anybody please explain why this happens ? It seems not to happen for smaller values of M and Z.I also made sure a.dtype==b.dtype.Here is one more example , which also demonstrates python 's builtin sum behaves as expected : I 'm using numpy V1.9.2 ."
"Thank you for your time.I am writing some code that is checking for correlation between multiple sets of data . It works great when I am using the original data ( which I am honestly unsure of which format it is in at that point ) , but after I run the data through some equations using the Decimal module , the data set will not show up when tested for correlation.I feel really stupid and new lol , I am sure it 's a very easy fix.Here is a small program I wrote to demonstrate what I mean.The data for both lists A & B as well as H & F are exactly the same , with the only difference of A & B being decimal formated numbers , where as H & F are not.When the program is run , A & B returns : and H & J returns : How do I make it so I can utilize the data after I 've ran it through my equations ? Sorry for the stupid question and thank you for your time . I hope you are all well , happy holidays !"
"In Python , you can assign an arbitrary attribute from outside the defining class : The underlying mechanism here is __dict__ attribute that maintains a dictionary of all attributes.We were all told not to expose our inner workings to the client code , but attaching new data does n't have to do with encapsulation at all , right ? Is this idiom common for Python code ? Just What I Mean…Each Tweet has standard fields , like id , text , owner.When returning tweet list for a user , you want to display if a tweet is “ favorited ” by this user.Obviously , to obtain is_favorite you need to query many-to-many relationship for this user.Would it be OK to pre-fill Tweet objects with is_favorite corresponding to current user ? Sure I could expose a method is_favorite_for ( user ) but I 'm hitting Django template language limitations that does n't allow to call methods with arguments from inside the template . Also , I believe a template should not be calling methods at all.I know this will work fine , but I wonder if doing something like that in an open source project would get other developers to look on me with contempt . Sidenote : I come from C # /.NET background where dynamic types were introduced very recently and are n't adapted widely except for some niche areas ( interoperability , IoC frameworks , REST client frameworks , etc ) ."
"I have two foreign keys in an entity refering to another entity.Here is how it looksand However , it still shows me an error saying there are multiple foreign key paths linking the tables . Specify the 'foreign_keys ' argument , providing a list of those columns which should be counted as containing a foreign key reference to the parent tableThe above workaround is what I get from some other posts . I have checked and changed many times , and still no luck . I wonder if it 's already correct or there is something I miss . Need help"
"I am a bit confused about how rounding in Python works.Could someone please explain why Python behaves like this ? Example : And same for : Edit : So in general , there is a possibility that Python rounds down instead of rounding up . So am I to understand that the only `` abnormal '' thing that can happen is that Python rounds down ? Or may it also get rounded up `` abnormally '' due to how it is stored ? ( I have n't found a case where Python rounded up when I expected it to round down )"
"I 'm fighting a memory leak in a Python project and spent much time on it already . I have deduced the problem to a small example . Now seems like I know the solution , but I ca n't understand why.The idea is that I store some values in the dict d and memorize used keys in a list to be able to clean the dict from time to time.This variation of the program confidently eats memory never returning it back . If I use alternative method to „ clear ” used_keys that is commented in the example , all is fine : memory consumption stays at constant level.Why ? Tested on CPython and many linuxes ."
"> > See EDIT below < < I am working on processing data from a special pixelated CCD camera over serial , using FTDI D2xx drivers via pyUSB.The camera can operate at high bandwidth to the PC , up to 80 frames/sec . I would love that speed , but know that it is n't feasible with Python , due to it being a scripted language , but would like to know how close I can get - whether it be some optimizations that I missed in my code , threading , or using some other approach . I immediately think that breaking-out the most time consuming loops and putting them in C code , but I do n't have much experience with C code and not sure the best way to get Python to interact inline with it , if that 's possible . I have complex algorithms heavily developed in Python with SciPy/Numpy , which are already optimized and have acceptable performance , so I would need a way to just speed-up the acquisition of the data to feed-back to Python , if that 's the best approach.The difficulty , and the reason I used Python , and not some other language , is due to the need to be able to easily run it cross-platform ( I develop in Windows , but am putting the code on an embedded Linux board , making a stand-alone system ) . If you suggest that I use another code , like C , how would I be able to work cross-platform ? I have never worked with compiling a lower-level language like C between Windows and Linux , so I would want to be sure of that process - I would have to compile it for each system , right ? What do you suggest ? Here are my functions , with current execution times : ReadStream : 'RXcount ' is 114733 for a device read , formatting from string to byte equivalentReturns a list of bytes ( 0-255 ) , representing binary valuesCurrent execution time : 0.037 secProcessRawData : To reshape the byte list into an array that matches the pixel orientationsResults in a 3584x32 array , after trimming off some un-needed bytes.Data is unique in that every block of 14 rows represents 14-bits of one row of pixels on the device ( with 32 bytes across @ 8 bits/byte = 256 bits across ) , which is 256x256 pixels . The processed array has 32 columns of bytes because each byte , in binary , represents 8 pixels ( 32 bytes * 8 bits = 256 pixels ) . Still working on how to do that one ... I have already posted a question for that previouslyCurrent execution time : 0.01 sec ... not bad , it 's just NumpyFinally , GetFrame : The device has a mode where it just outputs whether a pixel detected anything or not , using the lowest bit of the array ( every 14th row ) - Get that data and convert to int for each pixelResults in 256x256 array , after processing every 14th row , which are bytes to be read as binary ( 32 bytes across ... 32 bytes * 8 bits = 256 pixels across ) Current execution time : 0.04 secGoal : I would like to target a total execution time of ~0.02 secs/frame by whatever suggestions you make ( currently it 's 0.25 secs/frame with the GetFrame function being the weakest ) . The device I/O is not the limiting factor , as that outputs a data packet every 0.0125 secs . If I get the execution time down , then can I just run the acquisition and processing in parallel with some threading ? Let me know what you suggest as the best path forward - Thank you for the help ! EDIT , thanks to @ Jaime : Functions are now : ... time 0.013 sec ... time 0.000007 sec ! ... time 0.00006 sec ! So , with pure Python , I am now able to acquire the data at the desired frame rate ! After a few tweaks to the D2xx USB buffers and latency timing , I just clocked it at 47.6 FPS ! Last step is if there is any way to make this run in parallel with my processing algorithms ? Need some way to pass the result of GetFrame to another loop running in parallel ."
"I would like to drop rows within my dataframe based on if a piece of a string is duplicated within that string . For example , if the string is jkl-ghi-jkl , I would drop this row because jkl is repeated twice . I figured that creating a list and checking the list for duplicates would be the ideal approach.My dataframe for this example consist of 1 column and two data points : My first step I take is to apply a split to my data , and split of `` - '' Which is yields the output : My second step I take is to convert my output into lists : Which yields : My last step I wish to accomplish is to compare a full list with a distinct list of unique values : Which yields the error : I am aware that my .tolist ( ) creates a list of 2 series . Is there a way to convert these series into a list in order to test for duplicates ? I wish to use this piece of code : with a drop in order to drop all rows with a duplicated value within each cell . Is this the correct way of approaching , or is there a simpler way ? My end output should look like : Because string jkl-ghi-jkl-mno gets dropped due to `` jkl '' repeating twice"
"I have a celery worker that is using the gevent pool which does HTTP requests and adds another celery task with page source.I 'm using Django , RabbitMQ as a broker , Redis as a celery result backend , Celery 4.1.0.The task has ignore_result=True but I 'm getting this error pretty often ConcurrentObjectUseError : This socket is already used by another greenlet : < bound method Waiter.switch of < gevent.hub.Waiter ... > I see it is related to the Redis connection.I ca n't figure out how to solve this.This is more or less the logic of the task.I also tried using a semaphore when calling process_task.apply_async but it did n't work.This is the stack trace :"
"I have a 2D list : I want to find the most frequent element in the 2D list . In the above example , the most common string is 'Mohit'.I know I can use brute force using two for loops and a dictionary to do this , but is there a more efficient way using numpy or any other library ? The nested lists could be of different lengthsCan someone also add the time of their methods ? To find the fasted method . Also the caveats at which it might not be very efficient.EditThese are the timings of different methods on my system : Mayank Porwal 's method runs the fastest on my system ."
"I have a DataFrame : I want to insert a row above any occurrence of ' b ' , that is a duplicate of that row but with ' b ' changed to ' c ' , so I end up with this : For the life of me , I ca n't figure out how to do this ."
"I created this toy problem that reflects my much bigger problem : I want to do it as fast as possible , so using numpy 's functions to calculate ans should be the best approach , since this operation is heavy and my matrices are quite big.I saw this post , but the shapes are different and I can not understand which axes I should use for this problem . However , I 'm certain that tensordot should have the answer . Any suggestions ? EDIT : I accepted @ ajcr 's answer , but please read my own answer as well , it may help others ..."
"MotivationMotivated by this problem - the OP was using urlopen ( ) and accidentally passed a sys.argv list instead of a string as a url . This error message was thrown : AttributeError : 'list ' object has no attribute 'timeout'Because of the way urlopen was written , the error message itself and the traceback is not very informative and may be difficult to understand especially for a Python newcomer : ProblemHere is the shortened code I 'm working with : I 'm trying to think whether this kind of an error can be caught statically with either smart IDEs like PyCharm , static code analysis tools like flake8 or pylint , or with language features like type annotations . But , I 'm failing to detect the problem : it is probably too specific for flake8 and pylint to catch - they do n't warn about the problemPyCharm does not warn about sys.argv being passed into urlopen , even though , if you `` jump to source '' of sys.argv it is defined as : if I annotate the function parameter as a string and pass sys.argv , no warnings as well : QuestionIs it possible to catch this problem statically ( without actually executing the code ) ?"
"given an array of integers likeI need to mask elements that repeat more than N times . To clarify : the primary goal is to retrieve the boolean mask array , to use it later on for binning calculations.I came up with a rather complicated solutiongiving e.g.Is there a nicer way to do this ? EDIT , # 2 Thanks a lot for the answers ! Here 's a slim version of MSeifert 's benchmark plot . Thanks for pointing me to simple_benchmark . Showing only the 4 fastest options : ConclusionThe idea proposed by Florian H , modified by Paul Panzer seems to be a great way of solving this problem as it is pretty straight forward and numpy-only . If you 're fine with using numba however , MSeifert 's solution outperforms the other . I chose to accept MSeifert 's answer as solution as it is the more general answer : It correctly handles arbitrary arrays with ( non-unique ) blocks of consecutive repeating elements . In case numba is a no-go , Divakar 's answer is also worth a look !"
"I am writing code for an addon to XBMC that copies an image provided in a bytearray to a slice of a mmap object . Using Kern 's line profiler , the bottleneck in my code is when I copy the bytearray into the mmap object at the appropriate location . In essence : I can not change the data types of either the image or mmap . XBMC provides the image as a bytearray and the mmap interface was designed by a developer who wo n't change the implementation . It is NOT being used to write to a file , however - it was his way of getting the data out of XBMC and into his C++ application . I recognize that he could write an interface using ctypes that might handle this better , but he is not interested in further development . The python implementation in XBMC is 2.7.I looked at the possibility of using ctypes ( in a self-contained way withing python ) myself with memmove , but ca n't quite figure out how to convert the bytearray and mmap slice into c structures that can be used with memmove and do n't know if that would be any faster . Any advice on a fast way to move these bytes between these two data types ?"
"So , for input : I want output : So , basically , I have to abbreviate all words of length greater than or equal to 4 in the following format : first_letter + length_of_all_letters_in_between + last_letterI try to do this : But it does not work . In JS , I would easily do : How do I do the same in Python ? EDIT : I can not afford to lose any punctuation present in original string ."
"Running nosetests -s forprints out the following.The test instances are created before tests are run , while setup runs right before the test . For the general case , __init__ ( ) and setup ( ) accomplish the same thing , but is there a downside for using __init__ ( ) instead of setup ( ) ? Or using both ?"
I 'm having some difficulty with this problem . I need to remove all data that 's contained in squiggly brackets.Like such : Becomes : Here 's my first try ( I know it 's terrible ) : It seems to work but runs out of memory quite fast . Is there any better way to do this ( hopefully with regex ) ? EDIT : I was not clear so I 'll give another example . I need to allow for multiple top level brackets.Like such : Becomes :
"I am looking for a way to use the ExtendedInterpolation functionality found in the configparser lib when loading in a ini file to Logging.config.FileConfig.http : //docs.python.org/3/library/configparser # configparser.ExtendedInterpolationSo if I have a ini file that looks like this : As you can see I am following the extended interpolation syntax by using the $ { ... } notation to reference a value in a different section . When calling the file like so logging.config.fileConfig ( filepath ) , the eval'ing within the module always fails . In paticular on the eval'ing of the args option in the [ handler_dklogHandler ] section.Is there a way to get around this ? Thanks ! Note : Using Python 3.2"
"I have to parse some numbers from file names that have no common logic . I want to use the python way of `` try and thou shall be forgiven '' , or try-except structure . Now I have to add more than two cases . What is the correct way of doing this ? I am now thinking either nested try 's or try-except-pass , try-except-pass , ... Which one would be better or something else ? Factory method perhaps ( how ? ) ? This has to be easily expandable in the future as there will be much more cases.Below is what I want ( does not work because only one exeption per try can exist ) : Edit : Wow , thanks for the answers , but no pattern matching please . My bad , put `` some common logic '' at the beginning ( now changed to `` no common logic '' , sorry about that ) . In the cases above patterns are pretty similar ... let me add something completely different to make the point ."
I 'd like to filter out ( mostly one-line ) comments from ( mostly valid ) JavaScript using python 's re module . For example : I 'm now trying this for more than half an hour without any success . Can anyone please help me ? EDIT 1 : EDIT 2 :
"Questions with similar titles are about Python lists or NumPy . This is about the array.array class part of the standard Python library , see https : //docs.python.org/2/library/array.htmlThe fasted approach I came up with ( for integer types ) is to use array.fromfile with /dev/zero . This isabout 27 times faster than array.array ( ' L ' , [ 0 ] * size ) , which temporarily requires more than twice the memory than for the final array , about 4.7 times faster than arrar.array ( ' L ' , [ 0 ] ) * sizeand over 200 times faster than using a custom iterable object ( to avoid creating a large temporary list ) .However , /dev/zero may be unavailable on some platforms . Is there a better way to do this without NumPy , non-standard modules or my own c-extension ? Demonstrator code :"
"for intents of highlighting the issue lets follow this tutorial.theano has 3 ways to compute the sigmoid of a tensor , namely sigmoid , ultra_fast_sigmoid and hard_sidmoid . It seems using the latter two breaks the gradient descent algorithm.The conventional sigmoid converges as it should , but the the others have strange inconsistent behaviours . ultra_fast_sigmoid , just throws a straight error when trying to compute the gradient 'Method not defined ( 'grad ' , ultra_fast_sigmoid ) ' , whilst hard_sigmoid compiles fine , but fails to converge on the solution . Does anyone know the source of this behaviour ? It s not highlighted in the documentation that this should happen and it seems counter intuitive.code : i changed the following lines from the code to make the output shorter for this post ( they differ from the tutorial , but are already contained in the code above ) : sigmoidultra_fast_sigmoidhard_sigmoid"
"Got the following piece of pyspark code : In test code , the data frame is mocked , so I am trying to set the return_value for this call like this : But this fails with the following : Also tried mock_df.sample ( ) .filter ( ) .count.return_value = 250 , which gives the same error.How do I mock the filter i.e . F.col ( 'env ' ) .isNull ( ) | ( F.col ( 'env ' ) == 'Unknown ' ) correctly ?"
"I 'd like to find all the tuesdays and wednesdays ( as datetime object ) between 2015-11-02 and 2015-12-14 . This works : [ datetime.datetime ( 2015 , 11 , 3 , 0 , 0 ) , datetime.datetime ( 2015 , 11 , 4 , 0 , 0 ) , datetime.datetime ( 2015 , 11 , 10 , 0 , 0 ) , datetime.datetime ( 2015 , 11 , 11 , 0 , 0 ) , datetime.datetime ( 2015 , 11 , 17 , 0 , 0 ) , datetime.datetime ( 2015 , 11 , 18 , 0 , 0 ) , datetime.datetime ( 2015 , 11 , 24 , 0 , 0 ) , datetime.datetime ( 2015 , 11 , 25 , 0 , 0 ) , datetime.datetime ( 2015 , 12 , 1 , 0 , 0 ) , datetime.datetime ( 2015 , 12 , 2 , 0 , 0 ) , datetime.datetime ( 2015 , 12 , 8 , 0 , 0 ) , datetime.datetime ( 2015 , 12 , 9 , 0 , 0 ) ] Is there a more pythonic way to do it ?"
"There must be a way to do this query through the ORM , but I 'm not seeing it.The SetupHere 's what I 'm modelling : one Tenant can occupy multiple rooms and one User can own multiple rooms . So Rooms have an FK to Tenant and an FK to User . Rooms are also maintained by a ( possibly distinct ) User.That is , I have these ( simplified ) models : The ProblemGiven a Tenant , I want the Users owning a room which they occupy.The relevant SQL query would be : Getting any individual value off the related User objects can be done with , for example , my_tenant.rooms.values_list ( 'owner__email ' , flat=True ) , but getting a full queryset of Users is tripping me up.Normally one way to solve it would be to set up a ManyToMany field on my Tenant model pointing at User with TenantRoom as the 'through ' model . That wo n't work in this case , though , because the TenantRoom model has a second ( unrelated ) ForeignKey to User ( see `` restictions '' ) . Plus it seems like needless clutter on the Tenant model.Doing my_tenant.rooms.values_list ( 'user ' , flat=True ) gets me close , but returns a ValuesListQuerySet of user IDs rather than a queryset of the actual User objects . The QuestionSo : is there a way to get a queryset of the actual model instances , through the ORM , using just one query ? EditIf there is , in fact , no way to do this directly in one query through the ORM , what is the best ( some combination of most performant , most idiomatic , most readable , etc . ) way to accomplish what I 'm looking for ? Here are the options I see : SubselectSubselect through Python ( see Performance considerations for reasoning behind this ) Raw SQL : Others ... ?"
"Why are slice objects in python not hashable : They seem to be immutable : Context , I 'd like to make a dictionary that maps python ints or slice objects to some values , something like this : As a workaround I need to special case slices : This is n't a big deal , I 'd just like to know if there is some reasoning behind it ."
"UPDATEUnfortunately , due to my oversight , I had an older version of MKL ( 11.1 ) linked against numpy . Newer version of MKL ( 11.3.1 ) gives same performance in C and when called from python . What was obscuring things , was even if linking the compiled shared libraries explicitly with the newer MKL , and pointing through LD_* variables to them , and then in python doing import numpy , was somehow making python call old MKL libraries . Only by replacing in python lib folder all libmkl_*.so with newer MKL I was able to match performance in python and C calls.Background / library info.Matrix multiplication was done via sgemm ( single-precision ) and dgemm ( double-precision ) Intel 's MKL library calls , via numpy.dot function . The actual call of the library functions can be verified with e.g . oprof . Using here 2x18 core CPU E5-2699 v3 , hence a total of 36 physical cores.KMP_AFFINITY=scatter . Running on linux.TL ; DR1 ) Why is numpy.dot , even though it is calling the same MKL library functions , twice slower at best compared to C compiled code ? 2 ) Why via numpy.dot you get performance decreasing with increasing number of cores , whereas the same effect is not observed in C code ( calling the same library functions ) .The problemI 've observed that doing matrix multiplication of single/double precision floats in numpy.dot , as well as calling cblas_sgemm/dgemm directly from a compiled C shared library give noticeably worse performance compared to calling same MKL cblas_sgemm/dgemm functions from inside pure C code.Doing exactly the same as above , but with double precision A , B and C , you get:3 cores : 20s , 6 cores : 10s , 12 cores : 5s , 18 cores : 4.3s , 24 cores : 3s , 30 cores : 2.8s , 36 cores : 2.8s.The topping up of speed for single precision floating points seem to be associated with cache misses.For 28 core run , here is the output of perf.For single precision : And double precision : C shared library , compiled withPython wrapper function , calling the above compiled library : However , explicit calls from a C-compiled binary calling MKL 's cblas_sgemm / cblas_dgemm , with arrays allocated through malloc in C , gives almost 2x better performance compared to the python code , i.e . the numpy.dot call . Also , the effect of performance degradation with increasing number of cores is NOT observed . The best performance was 900 ms for single-precision matrix multiplication and was achieved when using all 36 physical cores via mkl_set_num_cores and running the C code with numactl -- interleave=all.Perhaps any fancy tools or advice for profiling/inspecting/understanding this situation further ? Any reading material is much appreciated as well.UPDATEFollowing @ Hristo Iliev advice , running numactl -- interleave=all ./ipython did not change the timings ( within noise ) , but improves the pure C binary runtimes ."
"I am trying to find an appropriate value for p & q to decode a hidden text of an image BMP file.The relevant code is as follows : I am struggling with the code to find the appropriate value of p and q . And I need to do it by maintaining the following steps : I need to write a function called est_bit_proportion which takes a Python array as an argument and computes the proportion of ones in it . And I am going to apply this to the existing message_bit_array for a particular choice of values for p and q.A code which will try different values of p and q , namely all combinations of p = 1 , 2 , 3 and q = 1 , 2 , 3 . There are thus nine combinations which I am going to try . For each combination of p and q , you are going to run function est_bit_proportion to find the proportion of ones . If the proportion of ones for some combination is less than 1 and greater than 0.5 , I have the answer . Save those values of p and q and continue with the loop.So far I have tried with below-mentioned codeBut it 's not working properly . The output is looking like below : But output should look like below : I am new in python and trying to learn it . It would be really great if someone helps me to find the appropriate way to solve it by maintaining the steps 1 to 4 ."
How to set transparent edge labels in NetworkX graph ? Currently there is a white background to each label which cuts my edges and overlaps other labels
"Recently , I went to a job interview for a Python developer position . The following code was one of the questions . I just had to write the output.The output is : I 'm trying to understand why the first list , list1 , has the ' a ' value . EDITI checked all the links and found out its a python `` gotcha '' for begginers , but want to thank the answers , can´t choose both so i´m going with the first one ."
"It looks like I was not the only one trying to avoid javascript altogether for a solution with OAuth 2.0 serverside . People could do everything but they could n't logout : Facebook Oauth LogoutOauth Logout using facebook graph apiFacebook OAuth2 Logout does not remove fb_ cookieThe official documentation for OAuth 2.0 with Facebook says : You can log a user out of their Facebook session by directing them to the following URL : https : //www.facebook.com/logout.php ? next=YOUR_URL & access_token=ACCESS_TOKEN YOUR_URL must be a URL in your site domain , as defined in the Developer App.I wanted to do everything serverside and I found that the suggested way to link leaves the cookie so that the logout link does n't work : https : //www.facebook.com/logout.php ? next=http : // { { host } } & access_token= { { current_user.access_token } } It does redirect but it does n't log the user out of my website . It seemed like a Heisenbug since this was changing to me and there was not much documentation . I anyway seemed to be able to achieve the functionality with a handler that manipulates the cookie so that in effect the user is logged out : So mapping the handler to /auth/logout and setting this to the link effectively logs the user out of my site ( but I 'm not sure about logging the user out of facebook , hopefully and untested ) Some other parts of my code is handling the OAuth tokens and cookie lookups for the Oauth communication : I did n't make a loginhandler since login basically is the code above at my root request handler . My user class is as follows : I mocked together two basic providersAnd I use the variable current_user for the facebook user and the variable user for the google user and the variable fbuser for a user who is logging in and therefore has no cookie match ."
"According to Tim Peters , `` There should be one -- and preferably only one -- obvious way to do it . '' In Python , there appears to be three ways to print information : Question : Are there best-practice policies that state when each of these three different methods of printing should be used in a program ?"
"I am working under ubuntu on a python3.4 script where I take in parameter a file ( encoded to UTF-8 ) , generated under Windows . I have to go through the file line by line ( separated by \r\n ) knowing that the `` lines '' contain some '\n ' that I want to keep.My problem is that Python transforms the file 's `` \r\n '' to `` \n '' when opening . I 've tried to open with different modes ( `` r '' , `` rt '' , `` rU '' ) .The only solution I found is to work in binary mode and not text mode , opening with the `` rb '' mode.Is there a way to do it without working in binary mode or a proper way to do it ? EDIT : Solution :"
"From time to time I find myself writing recursive generators in Python . Here is a recent example : The details of the algorithm are not important . I include it as a complete , real-world illustration to give the question some context.My question is about the following construct : Here , comb ( ) is the recursive instantiation of the generator.Every time I have to spell out the for : yield loop , it makes me cringe . Is this really the way to write recursive generators in Python , or are there superior ( more idiomatic , more performant , etc ) alternatives ?"
"I need to get all descendants point of links represented with side_a - side_b ( in one dataframe ) until reach for each side_a their end_point ( in other dataframe ) . So : The point is to get all points for each side_a value until reach end_point from df2 for that value . If it has two end_point values ( like `` k '' does ) that it should be two lists . I have some code but it 's not written with this approach , it drops all rows from df1 if df1 [ 'side_a ' ] == df2 [ 'end_points ' ] and that causes certain problems . But if someone wants me to post the code I will , of course . The desired output would be something like this : And one more thing , if there is the same both side , that point does n't need to be listed at all , I can append it later , whatever it 's easier . So , the problem with this code is that works if I drop rows where side_a is equal to end_point from df2 . I do n't know how to implement condition that if catch the df2 in side_b column , then stop , do n't go further . Any help or hint is welcomed here , truly . Thanks in advance ."
"The aim is to convert these regexes in C++ boost to Python re regexes : I could rewrite these regexes one by one but there 're quite a lot more that the example above , so my question is with regards to how to convert C++ boost regexest to Python and what is the difference between boost regexes and python re regexes ? Is the C++ boost : :u32regex the same as re regexes in python ? If not , what is the difference ? ( Links to the docs would be much appreciated = ) ) For instance : in boost , there 's boost : :u32regex_match , is that the same as re.match ? in boost , there 's boost : :u32regex_search , how is it different to re.searchthere 's also boost : :format_perl and boost : :match_default and boost : :smatch , what are their equivalence in python re ?"
"I were making a simple game in pygame and I realized I need a bunch of markers , counts and all sorts of global staff . So I decided to define a class and use it like this : and in my game loop I just give one variable to all my functions : I know this method is working and quite convenient ( for me ) , but I wonder how 's this going to affect speed of my game , may be I 'm doing something horrible ? Many thanks for answering ."
"In python 2.7 , by using I can now have print ( 1/2 ) showing 0.5.However is it possible to have this automatically imported at python startup ? I tried to use the sitecustomize.py special module but the inport is only valid inside the module and not in the shell.As I 'm sure people will ask why I need that : teaching Python to teenagers I noticed that the integer division was not easy for them so we decided to switch to Python 3 . However one of the requirement of the course was to be able to plot function and Matplotlib is pretty good but only valid for Python 2.7.So my idea was to use a custom 2.7 installation ... not perfect but I do n't have a better idea to have both Matplotlib and the new `` natural '' division `` 1/2=0.5 '' .Any advice or maybe a Matplotlib alternative that is working on python 3.2 ?"
"Is there any way we can target a specific metric to optimize using inbuilt tensorflow optimizers ? If not , how to achieve this ? For eg . If I want to focus only on maximizing F-score of my classifier specifically , is it possible to do so in tensorflow ? I am trying to optimize my classifier specifically on the basis of getting a better F-score . Despite using the decaying learning_rate and 300 training steps I am getting inconsistent results . While checking the metrics in the logs , I found the behavior of precision , recall and accuracy to be very erratic . Despite increasing the number of training steps , there was no significant improvement . So I thought that if i could make the optimizer focus more on improving the F-score as a whole I might get better results . Hence the question . Is there something that I am missing ?"
"Lets say I have three lists and I need to iterate through them and do some stuff to the contents . The three lists are streaks_0 , streaks_1 , and streaks_2 . For each list , I need to use different values specific to each list . For example , streak_0_num0s will not work in the streaks_1 for loop . Is there a way to make these three for loops into one or at least a way to clean this up ?"
How are routes suppose to be handled in flask when using an app factory ? Given a package blog that contains everything needed for the app and a management script that creates the app then how are you suppose to reference the app in the routes ? manage.pyblog/__init__.pyblog/routes.pyThe problem is the app is created outside the package so how are the routes suppose to be handled with a setup like this ?
"I have a list as follows.I would like sum up the last column grouped by the other columns.The result is like thiswhich is still a list.In real practice , I would always like to sum up the last column grouped by many other columns . Is there a way I can do this in Python ? Much appreciated ."
"I want to define a mix-in of a namedtuple and a base class with defines and abstract method : From what I understand reading the docs and other examples I have seen , c.do ( ) should raise an error , as class C does not implement do ( ) . However , when I run it ... it works : I must be overlooking something ."
"In the doctests of my module I would like to reference my module with the full namespace , for example : And I would like to avoid cluttering the doctests by writing : in each of the doctests.if I run doctest.testmod , I know I can use the globs keyword to provide this , while if I run nose , I can use the setup function.Is there another standard way that could work with both ?"
"I 'm checking out this code provided by python_apt but it appears to be a bit outdated : https : //github.com/jolicloud/python-apt/blob/master/doc/examples/inst.pyAll I 'm attempting to do here is follow the progress of the commit ( ) method ; currently when we call commit ( ) and pass in fprogress and iprogress , I can see on the console that all packages in the pkg_list are downloaded correctly , the problem appears to come after this . Program continues execution and it does n't trigger dpkg_status_change ( ) as I believe it should ? I have no way of knowing if the installation of multiple packages was a success ?"
"Why plotly package of python can not display figure in RMarkdown but matplotlib can ? For example : The R code can work well , but the python code can not dispay the figure , what is wrong with the code ?"
"Coming from Java , I 'm struggling a bit getting down inheritance , abstract classes , static methods and similar concepts of OO programming in Python.I have an implementation of an expression tree class , given ( simplified ) byThe method to_expr ( ) works fine when called on instances of Leaf , UnaryOpNode and BinaryOpNode , but raises a TypeError when called on an instance of VariadicOpNode : What am I doing wrong in that specific class that super ( ) is suddenly not working ? In Java the static method would get inherited so I would n't even need the super call , but in Python this does not seem to be the case ."
Matplotlib axes have Major and Minor ticks . How do I add a third level of tick below Minor ? For exampleproduces the effect I want using twinned x-axes . Is there a better way ?
"Clarification : As per some of the comments , I should clarify that this is intended as a simple framework to allow execution of programs that are naturally parallel ( so-called embarrassingly parallel programs ) . It is n't , and never will be , a solution for tasks which require communication or synchronisation between processes.I 've been looking for a simple process-based parallel programming environment in Python that can execute a function on multiple CPUs on a cluster , with the major criterion being that it needs to be able to execute unmodified Python code . The closest I found was Parallel Python , but pp does some pretty funky things , which can cause the code to not be executed in the correct context ( with the appropriate modules imported etc ) . I finally got tired of searching , so I decided to write my own . What I came up with is actually quite simple . The problem is , I 'm not sure if what I 've come up with is simple because I 've failed to think of a lot of things . Here 's what my program does : I have a job server which hands out jobs to nodes in the cluster.The jobs are handed out to servers listening on nodes by passing a dictionary that looks like this : moduleName and funcName are mandatory , and the others are optional.A node server takes this dictionary and does : On getting the return value , the server then sends it back to the job server which puts it into a thread-safe queue.When the last job returns , the job server writes the output to a file and quits.I 'm sure there are niggles that need to be worked out , but is there anything obvious wrong with this approach ? On first glance , it seems robust , requiring only that the nodes have access to the filesystem ( s ) containing the .py file and the dependencies . Using __import__ has the advantage that the code in the module is automatically run , and so the function should execute in the correct context . Any suggestions or criticism would be greatly appreciated . EDIT : I should mention that I 've got the code-execution bit working , but the server and job server have yet to be written ."
"I am using web2py to connect to a db with an 'at ' sign in the password , eg ' P @ sswd ' .This gets interpreted as a connection to host 'sswd @ localhost ' using password ' P'.I have tried the obvious URL escaping technique but this failed : Is there a resource that explains the escaping conventions used in these URL style connection strings ?"
"I 'm attempting to use signals/slots with large integers ranging from 0 - 2^32-1 . I 've discovered something a little weird -- once I emit > 7FFFFFFF boundary , I get OverflowError exceptions thrown after the slot is run . I might expect this kind of overflow if I or QT were explicitly using a signed 32 bit integer in another language like C or C++ -- as we all know 0x80000000 wraps back to -2^31 in 2s complement notation . In python though , its just 2^32 without wrapping . My assumption when writing the code though was that this is python and that the built-in int can grow very large ( maybe arbitrarilly so ? ) and that I do n't explicitly need to define something as 32 or 64 bit or signed/unsigned . It would all just work.The code below demonstrates what I 'm seeing ( Python 2.7.2 ( 64 bit ) , Pyside 1.1.0 , Windows 7 ) The exact output is : Why does Qt seem to treat the signals/slots of type integer as if its dealing with signed 32 bit integers and not python built-in ints ? If this is a restriction of Qt , what can I do to mark the int as unsigned or make sure QT can deal with integers > 0x7FFFFFFF ?"
"I 'm assembling a GUI using PyGObject . This Python code works in context . I get a toolbar button with the stock `` Open '' icon.But according to this resource , new_from_stock ( ) is deprecated : Deprecated since version 3.10 : Use Gtk.ToolButton.new ( ) together with Gtk.Image.new_from_icon_name ( ) instead.Okay then . So after digging further , this is what I came up with for a replacement : But this is the result : What is the correct way to do this that is still supported by the current GTK library ?"
Confused by the grouped one . How does it make difference ?
"I 'm working on a python project which reads in an URL encoded overlapping list of strings . Each string is 15 characters long and overlaps with its sequential string by at least 3 characters and at most 15 characters ( identical ) .The goal of the program is to go from a list of overlapping strings - either ordered or unordered - to a compressed URL encoded string . My current method fails at duplicate segments in the overlapping strings . For example , my program is incorrectly combining : to output : when correct output is : I am using simple python , not biopython or sequence aligners , though perhaps I should be ? Would greatly appreciate any advice on the matter or suggestions of a nice way to do this in python ! Thanks !"
"So this is two questions about what I 'm assuming is the same basic underlying confusion on my part . I hope that 's ok.Here some code : The output of this code isPlease note two things : The call to p [ -1 : ] does not result in a call to new_array.__getitem__ . This is true if p [ -1 : ] is replaced by things like p [ 0 : ] , p [ 0 : -1 ] , etc ... but statements like p [ -1 ] and p [ slice ( -1 , None , None ) ] do result in a call to new_array.__getitem__ . It 's also true for statements like p [ -1 : ] + p [ -1 : ] or s = p [ -1 ] but is n't true for statements like print s. You can see this by looking at `` blocks '' given above.The variable foo is correctly updated during calls to new_array.__getitem__ ( see blocks 5 and 6 ) but is not correct once evaluation of new_array.__getitem__ is complete ( see , again , blocks 5 and 6 ) . I should also add that replacing the line return super ( new_array , self ) .__getitem__ ( key ) with return new_array ( np.array ( self.view ( np.ndarray ) [ key ] ) , self.foo ) does not work either . The following blocks are the only differences in output.Which now contains excessive calls to new_array.__array_finalize__ , but has nochange in the `` problem '' with the variable foo.It was my expectation that a call like p [ -1 : ] to a new_array object with p.foo = 0 would result in this statement p.foo == 1 returning True . Clearly that is n't the case , even if foo was being correctly updated during calls to __getitem__ , since a statement like p [ -1 : ] results in a large number of calls to __getitem__ ( once the delayed evaluation is taken into account ) . Moreover the calls p [ -1 : ] and p [ slice ( -1 , None , None ) ] would result in different values of foo ( if the counting thing was working correctly ) . In the former case foo would have had 5 added to it , while in the later case foo would have had 1 added to it.The QuestionWhile the delayed evaluation of slices of numpy arrays is n't going to cause problems during evaluation of my code , it has been a huge pain for debugging some code of mine using pdb . Basically statements appear to evaluate differently at run time and in the pdb . I figure that this is n't good . That is how I stumbled across this behaviour.My code uses the input to __getitem__ to evaluate what type of object should be returned . In some cases it returns a new instance of the same type , in other cases it returns a new instance of some other type and in still other cases it returns a numpy array , scalar or float ( depending on whatever the underlying numpy array thinks is right ) . I use the key passed to __getitem__ to determine what the correct object to return is . But I ca n't do this if the user has passed a slice , e.g . something like p [ -1 : ] , since the method just gets individual indices , e.g . as if the user wrote p [ 4 ] . So how do I do this if the key in __getitem__ of my numpy subclass does n't reflect if the user is requesting a slice , given by p [ -1 : ] , or just an entry , given by p [ 4 ] ? As a side point the numpy indexing documentation implies that slice objects , e.g . slice ( start , stop , step ) will be treated the same as statements like , start : stop : step . This makes me think that I 'm missing something very basic . The sentence that implies this occurs very early : Basic slicing occurs when obj is a slice object ( constructed by start : stop : step notation inside of brackets ) , an integer , or a tuple of slice objects and integers.I ca n't help but feel that this same basic mistake is also the reason why I think the self.foo += 1 line should be counting the number of times a user requests a slice , or an element of an instance of new_array ( rather than the number of elements `` in '' a slice ) . Are these two issues actually related and if so how ?"
"Given the code : Where df is a pandas.DataFrame , I get the following error out of smf.mixedlm : Why is this error occurring ? len ( df ) reports that there are 7296 rows , so there should be no issue indexing the 7214th , and the explicit re-indexing ensures that the indices span from zero to 7295.You may download df here to fiddle around with it if you 'd like ."
"I am prototyping stuff in an IPython notebook that is loading Python2 code from modules on my computer . I activated reloading magic commands to make it easier to go back-and-forth between my text editor and the notebook when I make a code change and re-run a cell to see its affect : I am working in Python 2.7.10 because I working with some legacy code that does n't compile for 3 . Part of my work is extending some classes in this legacy code and overloading some of their methods . But , I also need to call some of the original base methods to do important work . For example : When I call important_method ( ) with some NewClass instance the first time in my notebook ( meaning , after a kernel reset ) it runs fine . The loop is such that the super call is happening more than once ! No errorsBut , if I go and modify some code to my new method in my text editor and go back to the IPython cell and call it again I get the following error at the line in my overloaded important_method ( ) where the super call is made.Note : I tried just naming my new method a different name because I thought it was something to do with the overloading method calling itself again but that did n't help . Also , I want them to be the same name because this is an API method and I want users of the legacy code to be able to call the same methods they know from before.Any idea how to use reloading in IPython notebooks with these Python2 super calls ? Thanks !"
"I 've got a standard run of the mill Pylons Pyramid application , that uses SQLAlchemy for its database persistence.I have set up an SQLAlchemy-migrate repo and have it functioning , but I really want to have the ability to use paster to upgrade and downgrade the database , or at least some way of having the user ( after installing the egg ) upgrade/downgrading the database to the version required.I 've got it built-into my app now , so upon app startup it does the version upgrade , but I would rather go with something where the user explicitly has to upgrade the database so that they know exactly what is going on , and know to make backups beforehand.How would I go about that ? How do I add commands to paste ? The way users would set up the application is : To set it up the first time , to do the database upgrade or upgrade in general I would want : Or something along those lines ."
JavaScript does n't have __setitem__ and this example obviously does n't work.In python __setitem__ works like : Is it possible to implement __setitem__ behavior in JavaScript ? All tricky workarounds would be helpful .
TFDV generates schema as a Schema protocol buffer . However it seems that there is no helper function to write/read schema to/from a file.How can I save it/load it ?
"Using NumPy , I would like to produce a list of all lines and diagonals of an n-dimensional array with lengths of k.Take the case of the following three-dimensional array with lengths of three.For this case , I would like to obtain all of the following types of sequences . For any given case , I would like to obtain all of the possible sequences of each type . Examples of desired sequences are given in parentheses below , for each case.1D linesx axis ( 0 , 1 , 2 ) y axis ( 0 , 3 , 6 ) z axis ( 0 , 9 , 18 ) 2D diagonalsx/y axes ( 0 , 4 , 8 , 2 , 4 , 6 ) x/z axes ( 0 , 10 , 20 , 2 , 10 , 18 ) y/z axes ( 0 , 12 , 24 , 6 , 12 , 18 ) 3D diagonalsx/y/z axes ( 0 , 13 , 26 , 2 , 13 , 24 ) The solution should be generalized , so that it will generate all lines and diagonals for an array , regardless of the array 's number of dimensions or length ( which is constant across all dimensions ) ."
"I am having trouble deleting a file from S3 using Fineuploader and Django/boto . I am able to successfully upload files to S3 with Fineuploader , and retrieve and display the image url , but deleting has n't been successful . From looking at the boto debug logs , it looks like boto is not sending the token as part of the request to S3 and I think that may be my problem.First I have the boto debug output because I suspect someone more familiar with it can help after just looking at it , but I have my full setup afterwards ( which follows the example at https : //github.com/Widen/fine-uploader-server/blob/master/python/django-fine-uploader-s3/ as closely as possible ) terminal output on deletesettings.py : Obviously I have my actual bucket name there , as I said uploading is working so I do n't think the issue is in the settings.Fineuploader Instanceurls.pyviews.py"
"What is the equivalent of the following R code in Rpy2 in python ? It 's not clear what ann_colors is . When evaluated in R it gives : Is it an robject.ListVector ? I tried : but it 's not quite it because I 'm not sure how to tell the ListVector object that the name of this object is Var1 , i.e . something like list ( Var1 = Var1 ) .How can this be properly translated to rpy2 ?"
"How can I randomly assign weights from a power-law distribution to a network with very large number of nodes.I wroteI know I assign weights to edges by a dictionary of tuples ( node1 , node2 , weight ) using : But when I am just interested in getting a weighted network where weights are power-law distributed , is there another shorter way ?"
"I have a plotly-dash dashboard and I ca n't seem to rescale my secondary y-axis . Is there a way of doing this ? I 've tried messing with the domain parameter and the range parameter in the go.Layout.I need the volume bar chart to be scaled down and occupy maybe 10 % of the height of the plot so it does n't overlap with my candlesticks . Thank you very much.Any help is appreciated.I have tried messing with the commented parametersUPDATEWith this combination of layout parameters I managed to rescale the bars , but now there are two x-axis , been trying to figure out how to bring the middle x-axis down ."
I 'm trying to pass on additional information to fields of a Django form to be displayed in a template . I tried to override the constructor and add another property to the field like this : but in the template this : did n't print anything . Does anyone know how to add additional information to a field without rewriting/inheriting the forms field classes ?
"I am trying to deploy django channels powered notification application behind Nginx reverse proxy server to only serve the websocket communications , while having Nginx + uWSGI setup to serve the django application.The application works in my local machine seamlessly when run with either python manage.py runserver -- noasgi + daphne -p 8000 myproject.asgi : application or python manage.py runserver with daphne handling all the requests internally.Problem : All the websocket requests are routed to http protocol type instead of websocket protocol type and it returns WebSocket connection to 'ws : //ip_address/ws/ ' failed : Error during WebSocket handshake : Unexpected response code : 404Packages Installed : Environment : Nginx Configuration for upgrading the request : routing.pyDaphne Logs : Kindly let me know if anything in addition is required to help.P.S : I have deployed the same application in two servers ( both having the same configuration and environment as above ) and the result was same ."
I 've come cross this question . Code : The result I expect is : Is this an infinite assignment for python list ? What is behind the scene ? Thanks .
"I am concatenating all files in a directory together into one , however some files have a different number of entries - how to I place an NaN when there is n't a value for that key in the file ? For example : file1.csfile2 . csDesired OutputThis is what I 've tried : I also tried pd.concat ( data , axis=1 , ignore_index=False ) but this does n't add the NaN because it just concatenates the files based on the column name ."
"Can the following __future__ statements be removed from source code , without affecting its functionality if I am using python 3.7.1 ?"
"I would like to use Qt and PyOpenGL to do some real time plotting and learn a bit about OpenGL , but am having trouble even getting my initail test data to show up.The idea is to store the x-coordinates and y-coordinates in different buffers since the x-coordinates will rarely change while the y-coordintates will change nearly every render . Unfortunately , having them in different buffers is giving me problems . Right now I have no errors and nothing is showing up so I 'm not sure where to go.Here is what I have so far for the plotting class : I also put a full working version of the program here :"
"Let 's say I have a DataFrame : And I want to merge ( maybe not merge , but concatenate ) the columns where their name 's first letter are equal , such as a1 and a2 and others ... but as we see , there is a c column which is by itself without any other similar ones , therefore I want them to not throw errors , instead add NaNs to them.I want to merge in a way that it will change a wide DataFrame into a long DataFrame , basically like a wide to long modification.I already have a solution to the problem , but only thing is that it 's very inefficient , I would like a more efficient and faster solution ( unlike mine : P ) , I currently have a for loop and a try except ( ugh , sounds bad already ) code such as : I would like to obtain the same results with better code ."
"So , I 'm generating an array of spaces , which have the property that they can be either red or black . However , I want to prevent red from being enclosed by black . I have some examples to show exactly what I mean : If red is 0 and black is 1 , then this example contains four enclosures , all of which I want to avoid when I generate the array . The inputs I have are the size of the array and the number of 1s I can generate.How would I go about doing this ?"
I have some data mimicking the following structure : What I would like to do is to pivot this data by group to show the presence of the 'value ' values as follows : Is there any way I could achieve this with PySpark ? I have tried a combination of groupby/pivot/agg without any success .
"My pandas/numpy is rusty , and the code I have written feels inefficient . I 'm initializing a numpy array of zeros in Python3.x , length 1000 . For my purpose , these are simply integers : I also have the following DataFrame ( which is much smaller than my actual data ) The DataFrame has two columns , start and end . These values represent a range of values , i.e . start will always be a smaller integer than end . Above , we see the first row has the range 100-400 , next is 200-500 , and then 300-600 . My goal is to iterate through the pandas DataFrame row by row , and increment the numpy array array_of_zeros based on these index positions . So , if there is a row in the dataframe of 10 to 20 , I would like to increment the zero by +1 for the indices 10-20 . Here is the code which does what I would like : And it works ! My questions : this is very clumsy code ! I should n't be using so many for-loops with numpy arrays ! This solution will be very inefficient if the input dataframe is quite largeIs there a more efficient ( i.e . more numpy-based ) method to avoid this for-loop ? Perhaps there is a pandas-oriented solution ?"
"I am looking to write a Rust backend for my library , and I need to implement the equivalent of the following function in pyo3 : This should return the same object as the input , and the function getting the return value should hold a new reference to the input . If I were writing this in the C API I would write it as : In PyO3 , I find it quite confusing to navigate the differences between PyObject , PyObjectRef , & PyObject , Py < PyObject > , Py < & PyObject > .The most naive version of this function is : Among other things , the lifetimes of x and the return value are not the same , plus I see no opportunity for pyo3 to increase the reference count for x , and in fact the compiler seems to agree with me : There may be a way for me to manually increase the reference count using the _py parameter and use lifetime annotations to make the compiler happy , but my impression is that pyo3 intends to manage reference counts itself using object lifetimes.What is the proper way to write this function ? Should I be attempting to wrap it in a Py container ?"
"I 'm using Python 3 's type hinting syntax , and I 'm writing a small AWS application that uses SQS . I 'm trying to hint the type of the Queue . This is how I obtain the type of the Queue : And I write my type-hinted function like this : But I get an error : I 've looked through the package myself using dir ( ... ) . It seems that factory does not contain sqs , indeed . Thus , I have two questions : Why is type returning this nonexistent class ? How can I find the right type of this object ?"
"The documentation for Pickle specifically says : Instances of a new-style class C are created using : Attempting to take advantage of this , I created a singleton with no instance attributes or methods : ( This class is used in a caching layer to differentiate a no-result result from nothing in the cache . ) The singleton works great ( every call to ZeroResultSentinel ( ) results in the same instance in memory , and ZeroResultSentinel ( ) == ZeroResultSentinel ( ) evaluates to True ) . And I can pickle and unpickle the instance without errors . However , when I unpickle it , I get a different instance . So I placed a breakpoint within __new__ . I hit the breakpoint every time I call ZeroResultSentinel ( ) , but I do not hit a breakpoint when I unpickle a pickled ZeroResultSentinel . This is in direct contradiction to the documentation . So am I doing something wrong , or is the documentation incorrect ?"
"Background : A binary file is read on a Linux machine using the following Fortran code : File Details : Attempts at a solution : I am trying to parse this into an array in python using the following code ( purposely leaving out two attributes ) : When testing some records , the data does not seem to line up with some sample records from the file when using Fortran . I have also tried dtype=numpy.float32 as well with no success . It seems as though I am reading the file in correctly in terms of number of observations though . I was also using struct before I learned the file was created with Fortran . That was not workingThere are similar questions out here , some of which have answers that I have tried adapting with no luck.UPDATEI am a little bit closer after trying out this code : I get the correct data for the first day , but for the second day I get all zeros , the third day the max is lower than the min , and so on ."
"I understand how to display matplotlib plots on-screen using the pyplot interface ( I think ! ) . I started plotting in a multi-threaded program , and this started causing errors , so I am trying to switch to the object-oriented interface . I can make a simple plot and save to file usingBut how do I display this plot on the screen ? I have seen other code that uses can.draw ( ) but that has no effect.Also , please let me know if there is anything suboptimal about my code above - I have n't really got to grips with what all these figure , canvas and axes objects do yet ."
"I 'm using pygame ( 1.9.0rc3 , though this also happens in 1.8.1 ) to create a heatmap . To build the heatmap , I use a small , 24-bit 11x11px dot PNG image with a white background and a very low-opacity grey dot that stops exactly at the edges : Dot image http : //img442.imageshack.us/img442/465/dot.pngThe area around the dot is perfect white , # ffffff , as it should be . However , when I use pygame to blit the image multiple times to a new surface using BLEND_MULT , a grey square appears , as though the dot background was n't perfect white , which does n't make sense.The following code , plus included images , can reproduce this : When you run the code , you will get the following image : Resulting image after blending http : //img263.imageshack.us/img263/4568/result.pngIs there a reason this happens ? How can I work around it ?"
How do I check if a property is settable or deletable in Python ? The best I 've found so far is
"I am currently using sphinx to create a complete documentation about the python-project i am working on . The two ouput formats are HTML and Latex/PDF . Whenever i create a nested list , the parent element is rendered bold in both the html page and the PDF.For example : Renders : abbbbb1b2This is not the case in all the examples of nested lists that i found online.Is there any way to configure Sphinx to render the parent element not bold ? I am using Sphinx v1.2.3.Here is my conf.py :"
I had the following line in a loop occurring during the initialization of a singly linked list class : The intended semantics was what I can obtain with : What I found out using pdb is that previous gets reassigned to the new Node object . And the ( former ) previous 's pointer attribute is unchanged.I could n't find documentation about the expected behaviour of this kind of assignment .
"Is there any way to get the number of errors that occurred during a Splunk search with the splunklib.results module or any of the splunklib modules ? Below , is my code so far : I get the following error with the above code : 'OrderedDict ' object has no attribute 'messages '"
"This is using the web app framework , not Django . The following template code is giving me an TemplateSyntaxError : 'for ' statements with five words should end in 'reversed ' error when I try to render a dictionary . I do n't understand what 's causing this error . Could somebody shed some light on it for me ? I 'm rendering it using the following :"
"I am using sympy to generate some functions for numerical calculations . Therefore I lambdify an expression an vectorize it to use it with numpy arrays . Here is an example : The problem between the sympy version and a pure numpy version is the speed i.e . vs.So is there any way to get closer to the speed of the numpy version ? I think np.vectorize is pretty slow but somehow some part of my code does not work without it . Thank you for any suggestions.EDIT : So I found the reason why the vectorize function is necessary , i.e : this seems to work fine however : So for simple expression like 1 this function does n't return an array.Is there a way to fix this and is n't this some kind of bug or at least inconsistent design ?"
"Reproducible code for the dataset : I need to find `` For each player , Find the week where his energy was maximum and display all the categories , energy values for that week '' So what I did was:1.Set Player and Week as Index2.Iterate over the index to find the max value of energy and return its valueOutput Obtained : This obtained output is for the maximum energy in the entire dataset , I would want the maximum for each player with the all other categories and its energy for that week.Expected Output : I have tried using groupby method as suggested in the comments , The output obtained is :"
"Sorry , I do not know the protocol for re-asking a question if it does n't get an answer . This question was asked a few months ago here : Numpy sum between pairs of indices in 2d array I have a 2-d numpy array ( MxN ) and two more 1-d arrays ( Mx1 ) that represent starting and ending indices for each row of the 2-d array that I 'd like to sum over . I 'm looking for the most efficient way to do this in a large array ( preferably without having to use a loop , which is what I 'm currently doing ) . An example of what i 'd like to do is the following.My problem is similar to the following question , however , I do n't think the solution presented there would be very efficient . Numpy sum of values in subarrays between pairs of indices In that question , they want to find the sum of multiple subsets for the same row , so cumsum ( ) can be used . However , I will only be finding one sum per row , so I do n't think this would be the most efficient means of computing the sum ."
"I have a descriptor on a class , and its __set__ method does not get called . I have been looking long and hard on this for a few hours and have no answer for this . But what I noticed below is that when I assign 12 to MyTest.X , it erases the property descriptor for X , and replaces it with the value of 12 . So the print statement for the Get function gets called . That 's good.But the print statement for the __set__ function does NOT get called at all . Am I missing something ?"
"In the event of an IndexError , is there a way to tell which object on a line is 'out of range ' ? Consider this code : In the event that x or y is too large and IndexError gets caught , I would like to know which of a or b is out of range ( so I can perform different actions in the except block ) .Clearly I could compare x and y to len ( a ) and len ( b ) respectively , but I am curious if there is another way of doing it using IndexError ."
"I 'm trying to use the new socket support for Google App Engine in order to perform some DNS queries . I 'm using dnspython to perform the query , and the code works fine outside GAE . The code is the following : When I run in GAE I get the following error : Which is raised by dnspython when no answer is returned within the time limit . I 've raised the timelimit to 60 seconds , and DnsQuery is a task , but still getting the same error.Is there any limitation in Google App Engine socket implementation , which prevents the execution of DNS requests ?"
"Why is it that when an exhausted generator is called several times , StopIteration is raised every time , rather than just on the first attempt ? Are n't subsequent calls meaningless , and indicate a likely bug in the caller 's code ? This also results in this behavior when someone accidentally uses an expired generator : If second and later attempts to call an exhausted generator raised a different exception , it would have been easier to catch this type of bugs.Perhaps there 's an important use case for calling exhausted generators multiple times and getting StopIteration ?"
"I use 64 bit Windows 10.I downloaded glpk-4.64 for Windows . I unzipped the file to my documents . I added the w64 path in this folder to Windows ' environment variables . I check if the glpk works in command line by executing this : So I think glpk is installed properly.Then I install cvxopt using this in command line : Then it installs cvxopt.When I do : I get this error : So , how do I fix this ?"
"Since python version 3.5 , you can use type hints to indicate what type of arguments a function is expecting . I find these type hints extremely valuable for documentation purposes , so I try to use them as much as I can . They also help the linter and therefore regularly save me from bugs introduced by code changes.For example , in my code I have a few functions which take a zero-argument function as an argument . E.g . : OrWhat I would like to do is make a type alias like so ( the code below is not valid python ) : And then I could shorten my types for the arguments above : I know I can make a type alias for a specific instance , e.g . : And I know of the existance of NewType in the typing module , but neither of these seem to generalize to higher order types ."
"I have a large dict src ( up to 1M items ) and I would like to take N ( typical values would be N=10K-20K ) items , store them in a new dict dst and leave only the remaining items in src . It does n't matter which N items are taken . I 'm looking for the fastest way to do it on Python 3.6 or 3.7.Fastest approach I 've found so far : Is there anything better ? Even a marginal gain would be good ."
"I was wondering what 's the most pythonic way to : Having a list of strings and a list of substrings remove the elements of string list that contains any of the substring list.Desired output : I have tried to implement similar questions such as this , but I 'm still struggling making the removal and extend that particular implementation to a list . So far I have done this : This works but probably it 's not the more cleaner way and more importantly it does not support a list , if I want remove hello.txt or yellow.txt I would have to use a or . Thanks ."
"I have a large python application which is running on a Django service . I need to turn off permission tests for certain operations so I created this context manager : Various parts of the application can then overide the tests using the context manager : Within the do stuff , the above context manager may be used multiple times in different functions . The use of the counter keeps this under control and it seems to work fine ... until threads get involved.Once there are threads involved , the global context manager gets re-used and as a result , tests may be incorrectly over-ridden.Here is a simple test case - this works fine if the thread.start_new_thread ( do_id , ( ) ) line is replaced with a simple do_it but fails spectacularly as shown : How can I make this context manager thread safe so that in each Python thread , it is consistently used even though it is re-entrant ?"
"I 'd like to run the doctests from this file , and it 's not clear to me out to accomplish it : README.md : ( aside : can anyone help me get this to highlight as markdown ? )"
"Let 's say we have a DataFrame with multiple levels of column headers.I want to select a list of columns from a named level.Method 1 : ( deprecated in favor of df.loc ) The problem with this is that I have to specify the level with 0 . It fails if I use the name of the level.Method 2 : The problem with this is that I can only specify a single value . It fails if I use [ ' A ' , ' B ' ] .Method 3 : This works , but is n't as concise as the previous two methods ! : ) Question : How can I get method 1 or 2 to work ? Or , is there a more pythonic way ? The MWE : Related questions : pandas dataframe select columns in multiindexGiving a column multiple indexes/headers"
"If I have to execute the following 3 commands , how do I group them so I only have to call one ? I can use fabric to put those in a single function say deploy which might accept a minion name then run through master , but I 'm wondering if saltstack has something built-in ?"
"I am trying to parse arbitrary documents download from the wild web , and yes , I have no control of their content.Since Beautiful Soup wo n't choke if you give it bad markup ... I wonder why does it giving me those hick-ups when sometimes , part of the doc is malformed , and whether there is a way to make it resume to next readable portion of the doc , regardless of this error.The line where the error occurred is the 3rd one : CLI full output is :"
"A few months ago I remember reading about a useful little web service somebody put together for API testing purposes . Basically , you just did a call like this , e.g . : And the server would respond with a 405 / Method Not Allowed.It was basically just a handy little utility you could use during development if you just wanted to interact with a live URL that behaved the way you specified.My google-fu is weak today and I can not for the life of me remember what it was called . I 'm sure I could whip something like this up myself in about as long as its taken me to type this question , but if anybody remembers what I 'm talking about , perhaps you can share ? Many thanks ... Edit : Posted something I whipped up real quick , but I 'd still be interested in the answer if someone knows what I 'm referring to ..."
I 'm using hypothesis to test a function that takes two lists of equal length as input . This gives me the error message : FailedHealthCheck : It looks like your strategy is filtering out a lot of data . Health check found 50 filtered examples but only 4 good ones.The assumption that len ( x ) == len ( y ) is filtering out too many inputs . So I would like to generate a random positive number and use that as the length of both x and y . Is there a way this can be done ?
"After a very thorough read of the Python 's decimal module documentation , I still find myself puzzled by what happens when I divide a decimal.In Python 2.4.6 ( makes sense ) : In Python 2.5.6 , Python 2.6.7 , and Python 2.7.2 ( puzzling ) : More confusing yet , that result does n't even appear to be valid : The result is the same using decimal.Decimal ( 1000 ) / decimal.Decimal ( 10 ) , so it 's not an issue with using an int as the divisor.Part of the issue is clearly around precision : But there should be ample precision in decimal.Decimal ( `` 1000.000 '' ) to divide safely by 10 and get an answer that 's at least in the right ballpark.The fact that this behavior is unchanged through three major revisions of Python says to me that it is not a bug.What am I doing wrong ? What am I missing ? How can I divide a decimal ( short of using Python 2.4 ) ?"
I have a question about styling in PEP 8 ( or cutting the number of characters in each line to be smaller ) .Consider that I have a book with a bunch of different attributes and I want to concatenate them into some String.How can I shorten this line to ensure its readability ? Any ideas will help . PEP 8 is just 1 idea .
"Trying to think of a one-liner to achieve the following ( summing all the values of a key ) : One-liner version without using any imports like itertools , collections etc.Is it possible in Python to use a reference to the object currently being comprehended ? If not , is there any way to achieve this in a one-liner without using any imports i.e . using basic list/dict comprehension and inbuilt functions ."
"Suppose I have : Suppose __init__.py is an empty file , and a.py is just one line : Suppose b.py is : Now in both Python 2.7 and Python 3.x , running b.py gives the result ( 5 ) .However , if I delete the file __init__.py , b.py still works in Python 3.x , but in Python 2.7 , I get the error : Why does Python 2.7 exhibit different behaviour in this situation ?"
"I recently updated my gcloud libraries from 118.0.0 to 132.0.0 and immediately remote_api_shell no longer worked . I went through a number of permutations of re-logging in , to set the application-default credentials through gcloud , and to use a service account and environment variable . All permutations failed with the same error message : After back revving through 131.0.0 and 130.0.0 , I just went back to 118.0.0 , re-logged in and everything worked fine.I did not update the running application after updating gcloud , as I 'm in the middle of a release cycle at the moment , so that may have been the issue , but any help would be appreciated . Thanks !"
I find myself using python for a lot of file management scripts as the one below . While looking for examples on the net I am surprised about how little logging and exception handling is featured on the examples . Every time I write a new script my intention is not to end up as the one below but if it deals with files then no matter what my paranoia takes over and the end result is nothing like the examples I see on the net . As I am a newbie I would like to known if this is normal or not . If not then how do you deal with the unknowns and the fears of deleting valuable info ?
"If I want to have a type that can represent multiple possible types , Unions seem to be how I represent that : U can be an int or a str.I noticed though that TypeVars allow for optional var-arg arguments that also seem to do the same thing : Both T and U seem to only be allowed to take on the types str and int.What are the differences between these two ways , and when should each be preferred ?"
"For example , how do you do the following R data.table operation in pandas : I.e . group by projectPath and pipelineId , aggregate some of the columnsusing possibly custom functions , and then rename the resulting columns.Output should be a DataFrame with no hierarchical indexes , for example :"
"I have a problem when using a groupby/apply chain on a MultiIndex DataFrame : The resulting data frame contains the grouped level ( s ) twice ! Example data frame : In this example , I simply sum over the rows ( I actually have another function , but one that also accepts and returns a dataframe ) : And it gives me the following result : So now I have three index levels , with the one grouped by doubled . When I group by both levels , the whole multiindex in doubled : If I set as_index=False , I still get another index level , containing ascending numbers : Is this intended behavior ? How can I avoid that another index level is created ? Do I have to remove it by hand every time I do a groupby/apply operation ?"
"This is an interesting question that I was trying to work through the other day . Is it possible to force the significand or exponent of one float to be the same as another float in Python ? The question arises because I was trying to rescale some data so that the min and max match another data set . However , my rescaled data was slightly off ( after about 6 decimal places ) and it was enough to cause problems down the line.To give an idea , I have f1 and f2 ( type ( f1 ) == type ( f2 ) == numpy.ndarray ) . I want np.max ( f1 ) == np.max ( f2 ) and np.min ( f1 ) == np.min ( f2 ) . To achieve this , I do : The result ( just as an example ) would be : My initial thought is that forcing the exponent of the float would be the correct solution . I could n't find much on it , so I made a workaround for my need : now np.max ( f2 ) == np.max ( f1 ) However , is there a better way ? Did I do something wrong ? Is it possible to reshape a float to be similar to another float ( exponent or other means ) ? EDIT : as was suggested , I am now using : While my solution above will work ( for my application ) , I 'm interested to know if there 's a solution that can somehow force the float to have the same exponent and/or significand so that the numbers will become identical ."
"If I have a an N^3 array of triplets in a numpy array , how do I do a vector sum on all of the triplets in the array ? For some reason I just ca n't wrap my brain around the summation indices . Here is what I tried , but it does n't seem to work : I would expect that as N gets large , if the sum is working correctly I should converge to 0 , but I just keep getting bigger . The sum gives me a vector that is the correct shape ( 3x1 ) , but obviously I must be doing something wrong . I know this should be easy , but I 'm just not getting it.Thanks in advance !"
"I have to return the second smallest number in a python list using recursion , and no loops . What I have done is created a helper function that returns a tuple of the ( smallest , second smallest ) values in the list , and then I just take the tuple [ 1 ] in my second_smallest func.This works , but now I need to handle nested lists , so s_smallest ( [ 1,2 , [ 3,0 ] ] ) should return ( 0,1 ) . I tried doing this : to get the first smallest and second smallest values if it is a list , but I get an error saying builtins.TypeError : unorderable types : int ( ) > = list ( ) . How can I fix this problem to deal with nested lists ?"
"When using the following code to train my network : where data_train_estimator is defined as : How does dataset.shuffle ( 1000 ) actually work ? More specifically , Let 's say I have 20000 images , batch size = 100 , shuffle buffer size = 1000 , and I train the model for 5000 steps . 1 . For every 1000 steps , am I using 10 batches ( of size 100 ) , each independently taken from the same 1000 images in the shuffle buffer ? 2.1 Does the shuffle buffer work like a moving window ? 2.2 Or , does it randomly pick 1000 out of the 5000 images ( with or without replacement ) ? 3 . In the whole 5000 steps , how many different states has the shuffle buffer been in ?"
"I am attempting to downsample monthly data to weekly data and have a time series dataframe of months that looks like this : I 've attempted using a resample to weeks like this : which yields : I 've tried interpolation , but I ca n't get what I am looking for , which is a fill of the unsampled ( 0 's ) with an even split of the first value like this : Is there some method using resample , fills and interpolation that allows this ?"
"I have a 5000*5000 numpy array on which I want to calculate the Kurtosis for windows of size 25 . I tried putting scipys own kurtosis function in the generic_filter found in ndimage.filters like so : This never ends and I 'm not sure at all of it gives the correct answer . So my first question is if this is a correct way to use the generic_filter with a scipy function . If it happened to be correct , then it is too slow for it to be of any use to me . So my next question would be if there 's a faster way to achieve this ? For example , thinking about a standard deviation you can simply do something like : This is blazing fast and simply comes from the fact that $ \sigma^2 = E [ ( X -\mu ) ^2 ] = E [ X^2 ] - ( E [ X ] ) ^2 $ ."
"I 'm trying to use a third-party lib ( docutils ) on Google App Engine and have a problem with this code ( in docutils ) : I want the import to fail , as it will on the actual GAE server , but the problem is that it does n't fail on my development box ( ubuntu ) . How to make it fail , given that the import is not in my own code ?"
"I have the following pandas DataFrame , called main_frame : I 've been having trouble to explore the dataset on Scikit-learn and I 'm not sure if the problem is the pandas Dataset , the dates as index , the NaN's/Infs/Zeros ( which I do n't know how to solve ) , everything , something else I was n't able to track.I want to build a simple regression to predict the next target_var item based on the variables named `` Input '' ( 1,2,3.. ) . Note that there are a lot of zeros and NaN 's in the time series , and eventually we might find Inf 's as well ."
I need to ship a compiled version of a python script and be able to prove ( using a hash ) that the compiled file is indeed the same as the original one.What we use so far is a simple : The issue is that this is not reproducible ( not sure what are the fluctuating factors but 2 executions will not give us the same .pyc for the same python file ) and forces us to always ship the same compiled version instead of being able to just give the build script to anyone to produce a new compiled version.Is there a way to achieve that ? Thanks
"SPOILER : partially solved ( see at the end ) .Here is an example of code using Python embedded : I work under Linux openSUSE 42.2 with gcc 4.8.5 ( but I also have the same problem on openSUSE 13.2 with gcc 4.8.3 or RedHat 6.4 with gcc 4.4.7 ) .I compiled a static and a dynamic version of Python 2.7.9 ( but I also have the same problem with Python 2.7.13 ) .I compile my example linking to the static version of Python with the following command : If I execute my example with the static version of Python in argument , it works.If I execute it on the dynamic version of Python in argument , I get the following error ( it happens in Py_Initialize ( ) ) : I have no idea why it works with static version and it does n't with the dynamic one . How can I solve this kind of problem ? EDIT : my script installing Python is the following : EDIT : I identified a possible cause of the problem . If I remove the line export LD_RUN_PATH= $ INSTALLDIR/ $ x/lib in the installation of dynamic Python , my embedded code works . I printed sys.path through embedded code and it point to the right installation . BUT ... in this way I ca n't use the installation directly : it loads a wrong version found in the system ( when I print sys.path I see it points to /usr/ ... ) . Also , I do n't want to have to set environment variables to launch Python because I use several versions of Python on the same machine.EDIT : Keeping my default installation script of Python , I solve the problem by adding -rdynamic in the options when compiling the C++ example . But I do n't well understand what is this option , and which kind of disaster it can cause ..."
"Consider a python script error.pyInvokingyields the expected `` 3 '' . However , consider runner.pyyields 768 . It seems that somehow the result of python code has been leftshifted by 8 , but how these two situations are different is not clear . What 's going on ? This is occurring in python 2.5 and 2.6 ."
"I have three columns , A , B and C. I want to create a fourth column D that contains values of A or B , based on the value of C. For example : In the above example , column D takes the value of column A if the value of C is 1 and the value of column B if the value of C is 0 . Is there an elegant way to do it in Pandas ? Thank you for your help ."
"Normally , we would use this to read/write a file : And to do read one file and output to another , can i do it with just one with ? I 've been doing it as such : Is there something like the following , ( the follow code dont work though ) : is there any benefit to using one with and not multiple with ? is there some PEP where it discusses this ?"
"In the documentation for Django , it specifies that models.py is a good place to locate callback functions for signals ( post_save , pre_save , etc ) . Where should this code live ? You can put signal handling and registration code anywhere you like . However , you 'll need to make sure that the module it 's in gets imported early on so that the signal handling gets registered before any signals need to be sent . This makes your app 's models.py a good place to put registration of signal handlers . source : https : //docs.djangoproject.com/en/dev/topics/signals/However , I have a significant amount of business logic that relies on signals and it 's becoming challenging to view them in the same file as all my models . I would like to move them to another file , but I do n't know how or where I can reference them . So , given the following file structure , could you provide an example of how I can reference a secondary ( or tertiary and so on ) file that contains appropriate signals ?"
"I have a list of elements I want to sort , but I do n't want to sort all of them , only those with a particular state . For example , let 's say I have a list of peole : Some of them have a job , let 's say [ Anna , Lisa , Steve ] . I want to sort ( ascending ) these by the number of hours they work and move them to the beginning of the list , while keeping the rest in the exact same order . So let 's say the number of hours they work is the following : After the partial sort the list would look like this : Of course I could create two new lists out of the original one , sort the one I want to sort and the put them together again to achieve what I am after : But I wonder if there is a straigthforward , Pythonic way of doing this ."
"I 'm trying to get VREP vision sensor output processed with opencv via ROS api . I did manage to set up scene and get scripts running , but the problem is that I get somewhat like 4-5 fps even without actual processing ( currently I just push images directly to output ) . This issue does not seem to depend on image resolution , since both 1024*512 and 128*128 captures result in exactly the same fps . This also is not a matter of blocking calls , although I 'm posting single-theaded code , I do have quite complex multithreaded processing pipeline which performs rather well with actual cameras ( ~50 fps ) . Lua scripts on VREP 's side do not seem to be a problem also , since I 've tried to play with video retranslation examples provided by vrep , and they seem to achieve rather satisfying fps . So it seems like image streaming is a bottleneck . Here 's my sample script : I also have to mention that I run it with ros bridge , since I need processing done with python3 , which is supported by ROS2 only , and VREP seems to work only with ROS1 ( although I 'm just starting to work with these systems , so I 'm not confident in that case ) ."
"We have a list : And want to print it out , normally I would use something like : But you could also do : But unfortunately : Why does n't it just automatically convert the list it receives to strings ? When would you ever not need it to convert them to strings ? Is there some tiny edge case I 'm missing ?"
"The built-in classifier in textblob is pretty dumb . It 's trained on movie reviews , so I created a huge set of examples in my context ( 57,000 stories , categorized as positive or negative ) and then trained it using nltk . I tried using textblob to train it but it always failed : That would run for hours and end in a memory error . I looked at the source and found it was just using nltk and wrapping that , so I used that instead , and it worked.The structure for nltk training set needed to be a list of tuples , with the first part was a Counter of words in the text and frequency of appearance . The second part of tuple was 'pos ' or 'neg ' for sentiment.Then I pickled it.Now ... I took this pickled file , and re opened it to get the nltk.classifier 'nltk.classify.naivebayes.NaiveBayesClassifier ' > -- and tried to feed it into textblob . Instead of I tried : what now ? I looked at the source and both are classes , but not quite exactly the same ."
"Problem summary and questionI 'm trying to look at some of the data inside an object that can be enumerated over but not indexed . I 'm still newish to python , but I do n't understand how this is possible.If you can enumerate it , why ca n't you access the index through the same way enumerate does ? And if not , is there a way to access the items individually ? The actual exampleTake a select subset of the datasetI can iterate over foo with enumerate : which generates the expected output : But then , when I try to index into it foo [ 0 ] I get this error :"
"I am currently building a small web application to improve my skills , as part of this , I am trying to go with best practices across the board , testing , CI , well architected , clean code , all of that . Over the last few sessions of working on it , I have been struggling with a test on my root route where instead of returning a string via the route function , I am rendering a template , I have gotten it to work , but I do n't understanding why it works , and this bothers me.Primarily , it 's the use of the b , before my assertion string , I assume it is to do with the fact that what I am rendering is not a string , but a html representation , akin to the difference between return and print , but I am hazy and would appreciate for someone to school me.The line I am asking about is line 4 of the test_homepage_response function . And how it operates . Especially in regards to this error I was getting : The error being returned : My tests for the home route : My views file : Some of the resources I used figuring this out , which pointed me towards the use of the b : https : //github.com/mjhea0/flaskr-tdd # first-testWhat does the ' b ' character do in front of a string literal ? Appreciate this is a long one , I tried to provide as much context because I honestly feel pretty stupid with this question , but I did n't want to just continue on without knowing why the solution I 'm using is working.Thank you as always ."
"Python , being the dynamic language that it is , offer multiple ways to implement the same feature . These options may vary in readability , maintainability and performance . Even though the usual scripts that I write in Python are of a disposable nature , I now have a certain project that I am working on ( academic ) that must be readable , maintainable and perform reasonably well . Since I have n't done any serious coding in Python before , including any sort of profiling , I need help in deciding the balance between the three factors I mentioned above.Here 's a code snippet from one of the modules in a scientific package that I am working on . It is an n-ary Tree class with a very basic skeleton structure . This was written with inheritance and sub classing in mind.Note : in the code below a tree is the same thing as a node . Every tree is an instance of the same class Tree.The two functions below belongs to this class ( along with many others ) Both these functions are doing exactly the same thing - they are just two different names . So , why not change it to something like : My doubts are these : I 've read that function calls in Python are rather expensive ( creating new stacks for each call ) , but I do n't know if it is a good or bad thing if one function depends on another . How will it affect maintainability . This happens many times in my code , where I call one method from another method to avoid code duplication . Is it bad practice to do this ? Here 's another example of this code duplication scenario in the same class : I could instead go for :"
"Dev_appserver.py ( the local development server for Python google app engine ) spews tons of useless INFO messages . I would like to up this to WARN or ERROR . How can I do that ? I 've tried the following , but it has no effect ... Any ideas ?"
I am having a hard time understanding this code.I would like to extract HTML comments using BeautifulSoup and Python3.Given : I searched for solutions and most people said : Which in my case would result in : This is what I understand : isinstance asks if text is an instance of Comment and returns a boolean.I sort of understand lambda . Takes text as an argument and evaluates the isinstance expression.You can pass a function to find_allThis is what I do not understand : What is text in text= ? What is text in lambda text ? What argument from html is passed into lambda textsoup.text returns I do n't grok it . Why is lambda text passing < ! -- Python is awesome -- > as an argument ?
"I am creating symmetric matrices/arrays in Python with NumPy , using a standard method : Now let 's be clever : Wait , what ? The upper left and lower right segments are symmetrical . What if I chose a smaller array ? OK ... .And just to be sure ... Is this a bug , or am I about to learn something crazy about += and NumPy arrays ?"
"I have a base class extending unittest.TestCase , and I want to patch that base class , such that classes extending this base class will have the patches applied as well . Code Example : Patching the TestFunctions class directly works , but patching the BaseTest class does not change the functionality of some.core.function in TestFunctions ."
I have the following snippet : Which gives me KeyError : ' a ' whereas the following code works fine : What 's the difference here ?
"I want to assign a single value to a part of a list . Is there a better solution to this than one of the following ? Maybe most performant but somehow ugly : Concise ( but I do n't know if Python recognizes that no rearrangement of the list elements is necesssary ) : Same caveat like above : Not sure if using a library for such a trivial task is wise : The problem that I see with the assignment to the slice ( vs. the for loop ) is that Python maybe tries to prepare for a change in the length of `` l '' . After all , changing the list by inserting a shorter/longer slice involves copying all elements ( that is , all references ) of the list AFAIK . If Python does this in my case too ( although it is unnecessary ) , the operation becomes O ( n ) instead of O ( 1 ) ( assuming that I only always change a handful of elements ) ."
"Given a graphlab SFrame where there 's a column with dates , e.g . : Is there an easy way in graphlab / other python function to convert the Date column to Year|Month|Day ? In pandas , I can do this : Which is the fastest way to extract day , month and year from a given date ? But to convert an SFrame into Panda to split date and convert back into SFrame is quite a chore ."
"I 'm still kind of new to PyQt but I really have no idea why this is happening.I have a Mainwindow that I create like this : and when I wanted to override the close event with : I read I had to change the uic.loadUI call to : But when I do this , all of my actions start being set off three times . I 'm pretty sure I 'm setting up the signals and slots correctly as they were working before adding this . Any help ? The pyuic file :"
"Copy the following dataframe to your clipboard : Now useto load it into your environment . How does one slice this dataframe such that all the rows of a particular textId are returned if the score group of that textId includes at least one score that equals 1.0 , 2.0 and 3.0 ? Here , the desired operation 's result would exclude textId rows name1 since its score group is missing a 3.0 and exclude name3 since its score group is missing a 2.0 : Attemptsdf [ df.textId == `` textIdRowName '' & df.score == 1.0 & df.score == 2.0 & & df.score == 3.0 ] is n't right since the condition is n't actingon the textId group but only individual rows . If this could berewritten to match against textId groups then it could be placedin a for loop and fed the unique textIdRowName 's . Such a functionwould collect the names of the textId in a series ( saytextIdThatMatchScore123 ) that could then be used to slice the original dflike df [ df.textId.isin ( textIdThatMatchScore123 ) ] .Failing at groupby ."
"For an algorithm competition training ( not homework ) we were given this question from a past year . Posted it to this site because the other site required a login.This is the problem : http : //pastehtml.com/view/c5nhqhdcw.htmlImage did n't work so posted it here : It has to run in less than one second and I can only think about the slowest way to do it , this is what I tried : What I 'm doing at the moment is going through each location and then going through each inhabited house for that location to find the max income location . Pseudocode : This is too slow since it 's O ( LN ) and wo n't run in under a second for the largest test case . Can someone please simply tell me how to do it in the shortest run time ( code is n't required unless you want to ) since this has been bugging me for ages.EDIT : There must be a way of doing this in less than O ( L ) right ?"
I have a large number of python file and I would like to generate public API documentation for my project . All the functions that are part of the api I have decorated with a decorator . for example : There are other public methods in the same classes . The API is spread among several files each file defining classes with methods and some methods have this @ api decorator . How can I tell Sphinx to generate docs just for the public API ?
"I am working on a binary classification problem with Tensorflow BERT language model . Here is the link to google colab . After saving and loading the model is trained , I get error while doing the prediction.Saving the ModelPredicting on dummy textError batch_size param is present in estimator , but not in the loaded model params ."
"I am trying to optimize my code with Cython . It is doing a a power spectrum , not using FFT , because this is what we were told to do in class . I 've tried to write to code in Cython , but do not see any difference.Here is my codeThe time and data is loaded via numpy.loadtxt and sent to this function.When I dothe .html file is very yellow , so not very efficient . Especially the entire for-loop and the calulation of power and returning everything.I have tried to read the official guide to Cython , but as I have never coded in C it is somewhat hard to understand.All help is very preciated : )"
"I 'm consuming messages from a RabbitMQ channel , I wish I could consume n elements at a time . I think I could use a ProcessPoolExecutor ( or ThreadPoolExecutor ) .I just wonder if it 's possible to know if there 's a free executor in the pool.This is what I want to write : I need to write the function block_until_a_free_worker.This methods should be able to check if all the running workers are in use or not . In alternative I could use any blocking executor.submit option , if available.I tried a different approach and change the list of futures meanwhile they are completed.I tried to explicitly add and remove futures from a list and then waiting like this : It seems it 's not a solution.I could set a future.add_done_callback , and possibily count the running instances ... Any hint or ideas ? Thank you ."
"I 'm trying to build a blog as a portfolio sample using python3 and flask and flask_jwt_extended.I can create a single file like this and it will run : But when I try to use Blueprint , it will not register the JWTManagerHere is my user.py file : here is my app.py : Now when I try to run app.py , this it returns a 500 ( Internal Error ) and will log this to the log file : Could someone please tell me what to do ? I tried EVERYTHING for the last 3 days . its been 20 hours of debugging and it 's still not fixed"
"Yesterday , I asked this question and never really got an answer I was really happy with . I really would like to know how to generate a list of N unique random numbers using a functional language such as Ruby without having to be extremely imperative in style.Since I did n't see anything I really liked , I 've written the solution I was looking for in LINQ : Can you translate my LINQ to Ruby ? Python ? Any other functional programming language ? Note : Please try not to use too many loops and conditionals - otherwise the solution is trivial . Also , I 'd rather see a solution where you do n't have to generate an array much bigger than N so you can then just remove the duplicates and trim it down to N.I know I 'm being picky , but I 'd really like to see some elegant solutions to this problem.Thanks ! Edit : Why all the downvotes ? Originally my code sample had the Distinct ( ) after the Take ( ) which , as many pointed out , could leave me with an empty list . I 've changed the order in which those methods are called to reflect what I meant in the first place.Apology : I 've been told this post came across as rather snobbish . I was n't trying to imply that LINQ is better than Ruby/Python ; or that my solution is much better than everyone else 's . My intent is just to learn how to do this ( with certain constraints ) in Ruby . I 'm sorry if I came across as a jerk ."
"i have a construct like the following : here cap is a process which normally writes to a file ( specified by -f ) , but i can get it to spew data to the screen by supplying /dev/stdout as the output file . similarly , enc expects to read from a file-like object , and i am able to get it to read from pipe by supplying - as the input . so instead of using a named pipe in the os , i thought the special file may not be necessary , i can use an unnamed pipe like this.. ( note also the reversal of order of spawning ) . i like this better because a temporary file seems unnecessary , but i am a bit concerned about whether this construct will chain the processes in the way i expect . is the /dev/stdout that cap is talking to distinct from the actual stdout in the OS ? that is , with the input pipe - in enc will i get a clean channel of data between these two processes even if other processes are chatting away to /dev/stdout on the OS ? will there be any significant differences in behaviour with blocking/queuing ? i think in my first example the named pipe will be a buffered 4096 bytes , and will block at either end if cap/enc are n't writing/reading fast enough , but correct me if i 'm wrong . is any special order of spawning or termination required , or any other gotchas i should be aware of ?"
Say I have a network called `` mynet '' and I want to start a container with an IP address bound to 192.168.23.2.The code I 'm starting with is : What do I do from here ? I 'm effectively looking for the equivalent to the -- ip option from docker run .
"An example from the book Core Python Programming on the topic Delegation does n't seem to be working.. Or may be I did n't understand the topic clearly..Below is the code , in which the class CapOpen wraps a file object and defines a modified behaviour of file when opened in write mode . It should write all strings in UPPERCASE only.However when I try to open the file for reading , and iterate over it to print each line , I get the following exception : This is strange , because although I have n't explicitly defined iterator methods , I 'd expect the calls to be delegated via __getattr__ to the underlying file object . Here 's the code . Have I missed anything ? I am using Python 2.7 ."
I am trying to group cards of the same suit ( color ) and rank inside generators and store those generators inside a list comprehension.The solution I came up with does that except for the fact that all the generators contain exactly the same cards . Any idea why ? Here is the codeBased on this I would expect for example : But instead I getAnd all the generators in the list return the same results..
"I am writing a blogpost on Python list.clear ( ) method where I also want to mention about the time and space complexity of the underlying algorithm . I expected the time complexity to be O ( N ) , iterate over the elements and free the memory ? But , I found an article where it is mentioned that it is actually an O ( 1 ) operation . Then , I searched the source code of the method in CPython implementation and found a method which I believe is the actual internal implementation of list.clear ( ) , however , I am not really sure it is . Here 's the source code of the method : I could be wrong but it does look like O ( N ) to me . Also , I found a similar question here , but there 's no clear answer there . Just want to confirm the actual time and space complexity of list.clear ( ) , and maybe a little explanation supporting the answer . Any help appreciated . Thanks ."
I 'm writing a python script for a program that has exposed its C++ API using SWIG.A SWIG exposed function has an interface like this : JoxColor is a POD struct looking like this : I can easily create a single JoxColor in Python and invoke a call to writePixelsRect like this : Repeatedly calling writePixelsRect with a 1x1 pixel rectangle is very slow so I want to create an array of JoxColor from python so I can write bigger rectangles at the time . Is this possible with SWIG types ? Note that I do n't have access to the source code for the C++ library exposing JoxColor and writePixelsRect so I ca n't add a help function for this . I also do n't want to introduce new C++ code in the system since it would force the users of my python script to compile the C++ code on whatever platform they are running . I do have access to ctypes in the python environment so if I could somehow typecast a float array created in ctypes to the type of JoxColor* for SWIG it would work for me .
"I 'm reading Raymond Hettinger 's Python ’ s super ( ) considered super ! About a page in , there 's this example : Why is it necessary to call super ( ) in Shape here ? My understanding is that this calls object.__init__ ( **kwds ) since Shape implicitly inherits from object . Even without that statement , we 've alreadyestablished shapename already in the parent 's __init__ , established the child class 's color in an explicit method override , then invoked the parent 's __init__ with super ( ) in ColoredShape.As far as I can tell , dropping this line produces the same behavior & functionality : What is the purpose of super ( ) within Shape here ?"
"Very simple minimal example : I get this output when running it in the PyCharm 2019.2 debugger with Python 3.6 : At this point the debugger breaks the execution flow but no Traceback and Exception message is shown in the Debugger console . If I run the same in PyCharm 2018.1 it does show these right when the breakpoint is hit.When I hit , I get the desired output , but then I ca n't run code in the debugging context anymore because the process ends : My Breakpoint configuration :"
"I have two Databases defined , default which is a regular MySQL backend andredshift ( using a postgres backend ) . I would like to use RedShift as a read-only database that is just used for django-sql-explorer . Here is the router I have created in my_project/common/routers.py : And my settings.py references it like so : The problem occurs when invoking makemigrations , Django throws an error with the indication that it is trying to create django_* tables in RedShift ( and obviously failing because of the postgres type serial not being supported by RedShift : So my question is two-fold : Is it possible to completely disable Django Management for a database , but still use the ORM ? Barring Read-Only Replicas , why has Django not considered it an acceptable use case to support read-only databases ? Related Questions- Column 'django_migrations.id ' has unsupported type 'serial ' [ with Amazon Redshift ]"
"I am having a problem with a fairly simple app.It performs properly , but I would like it to perform a little slower.The idea is to randomly generate a name from a list , display it , then remove it fromthe list every time a button is clicked.To make it a little more interesting , I want the program to display several names beforepicking the last one . I use a simple for loop for this . However , the code executes so quickly , the only name that winds up displaying is the last one.using time.sleep ( ) merely delays the display of the last name . no other names are shown.here is my code :"
"My issue is the following : I am training to retrieve the information on this website https : //www.cetelem.es/.I want to do several things : Click on the two slide buttons to change the information.Retrieve the information following the change of the sliding buttonsPut a condition , only retrieve information when tin and tae change.I tried with the following code on google colab : error1If you have the solution , can you explain my mistake ? Because I 'm a real beginner in scraping ."
"I have an abstract base class representing an interface . Subclasses of this class store as properties other subclasses of this class.For example : The depth and layout of the hierarchy can not be known in advance , but will not be recursive.What can I put as __repr__ on AbstractBase that will allow me to see the proprties of each child class in a useful way ? My current approach is : For a base class ( with no properties which are subclasses of AbstractBase , this outputs something readable , eg : However , for classes with AbstractBase subclasses it breaks , as it 's hard to tell where a parent class starts and another ends ( given that further levels of nesting are n't given further indenting by the __repr__ above ) .I 'd be happy with something like the below , imagining cls1 and cls2 had a single int property var : Sadly , I do n't know how to achieve this ( or if it 's even possible ) . Any thoughts ?"
"I saw this question but it uses the ? ? operator as a null check , I want to use it as a bool true/false test.I have this code in Python : In C # I could write this as : Is there a similar way to do this in Python ?"
"What is the right way ( or I 'll settle for a good way ) to lay out a command line python application of moderate complexity ? I 've created a python project skeleton using paster , which gave me a few files to start with : I want to know , mainly , where should my program entry point go , and how can I get it installed on the path ? Does setuptools create it for me ? I 'm trying to find this in the HHGTP , but maybe I 'm just missing it ."
"My Python application running on a 64-core Linux box normally runs without a problem . Then after some random length of time ( around 0.5 to 1.5 days usually ) I suddenly start getting frequent pauses/lockups of over 10 seconds ! During these lockups the system CPU time ( i.e . time in the kernel ) can be over 90 % ( yes : 90 % of all 64 cores , not of just one CPU ) .My app is restarted often throughout the day . Restarting the app does not fix the problem . However , rebooting the machine does.Question 1 : What could cause 90 % system CPU time for 10 seconds ? All of the system CPU time is in my parent Python process , not in the child processes created through Python 's multiprocessing or other processes . So that means something of the order of 60+ threads spending 10+ seconds in the kernel . I am not even sure if this is a Python issue or a Linux kernel issue.Question 2 : That a reboot fixes the problem must be a big clue as to the cause . What Linux resources could be left exhausted on the system between my app restarting , but not between reboots , that could cause this problem to get stuck on ? What I 've tried so far to solve this / figure it outBelow I will mention multiprocessing a lot . That 's because the application runs in a cycle and multiprocessing is only used in one part of the cycle . The high CPU almost always happens immediately after all the multiprocessing calls finish . I 'm not sure if this is a hint at the cause or a red herring.My app runs a thread that uses psutil to log out the process and system CPU stats every 0.5 seconds . I have independently confirmed what it 's reporting with top.I 've converted my app from Python 2.7 to Python 3.4 because Python 3.2 got a new GIL implementation and 3.4 had the multiprocessing rewritten . While this improved things it did not solve the problem ( see my previous SO question which I 'm leaving because it 's still a useful answer , if not the total answer ) .I have replaced the OS . Originally it was Ubuntu 12 LTS , now it 's CentOS 7 . No difference.It turns out multithreading and multiprocessing clash in Python/Linux and are not recommended together , Python 3.4 now has forkserver and spawn multiprocessing contexts . I 've tried them , no difference.I 've checked /dev/shm to see if I 'm running out of shared memory ( which Python 3.4 uses to manage multiprocessing ) , nothinglsof output listing all resource hereIt 's difficult to test on other machines because I run a multiprocess Pool of 59 children and I do n't have any other 64 core machines just lying aroundI ca n't run it using threads rather than processes because it just ca n't run fast enough due to the GIL ( hence why I switched to multiprocessing in the first place ) I 've tried using strace on just one thread that is running slow ( it ca n't run across all threads because it slows the app far too much ) . Below is what I got which does n't tell me much.ltrace does not work because you ca n't use -p on a thread ID . Even just running it on the main thread ( no -f ) makes the app so slow that the problem does n't show up.The problem is not related to load . It will sometimes run fine at full load , and then later at half load , it 'll suddenly get this problem.Even if I reboot the machine nightly the problem comes back every couple of days.Environment / notes : Python 3.4.3 compiled from sourceCentOS 7 totally up to date . uname -a : Linux 3.10.0-229.4.2.el7.x86_64 # 1 SMP Wed May 13 10:06:09 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux ( although this kernel update was only applied today ) Machine has 128GB of memory and has plenty freeI use numpy linked to ATLAS . I 'm aware that OpenBLAS clashes with Python multiprocessing but ATLAS does not , and that clash is solved by Python 3.4 's forkserver and spawn which I 've tried.I use OpenCV which also does a lot of parallel workI use ctypes to access a C .so library provided by a camera manufacturerApp runs as root ( a requirement of a C library I link to ) The Python multiprocessing Pool is created in code guarded by if __name__ == `` __main__ '' : and in the main threadUpdated strace resultsA few times I 've managed to strace a thread that ran at 100 % 'system ' CPU . But only once have I gotten anything meaningful out of it . See below the call at 10:24:12.446614 that takes 1.4 seconds . Given it 's the same ID ( 0x7f05e4d1072c ) you see in most the other calls my guess would be this is Python 's GIL synchronisation . Does this guess make sense ? If so , then the question is why does the wait take 1.4 seconds ? Is someone not releasing the GIL ?"
"I 'm looking to compute the the Levenshtein-distance between sequences containing up to 6 values . The order of these values should not affect the distance.How would I implement this into the iterative or recursive algorithm ? Example : 'dog ' and 'god ' have the exact same letters , sorting the strings before hand will return the desired result . However this does n't work all the time : 'doge ' and 'gold ' have 3/4 matching letters and so should return a distance of 1.Here is my current recursive code : EDIT : Followup question : Adding exceptions to Levenshtein-Distance-like algorithm"
"I 've trained a network on a multi GPU and CPU setup , and saved the resulting model as a tensorflow SavedModel . I then have another script which can load the resulting model and run the required ops to make a prediction , i.e. , run inference on the model . This works on the same setup that the model was trained on.However , I need to deploy the model to run on a device with 1 CPU and no GPUs . When I try to run the same script , I get these errors : InvalidArgumentError ( see above for traceback ) : Restoring from checkpoint failed . This is most likely due to a mismatch between the current graph and the graph from the checkpoint . Please ensure that you have not altered the graph expected based on the checkpoint . Original error : Can not assign a device for operation default_policy_1/tower_1/Variable : node default_policy_1/tower_1/Variable ( defined at restore.py:56 ) was explicitly assigned to /device : GPU:0 but available devices are [ /job : localhost/replica:0/task:0/device : CPU:0 , /job : localhost/replica:0/task:0/device : XLA_CPU:0 ] . Make sure the device specification refers to a valid device . The requested device appears to be a GPU , but CUDA is not enabled . [ [ node default_policy_1/tower_1/Variable ( defined at restore.py:56 ) ] ] This looked promising but the code did not change my graph at all , 0 nodes were removed - Remove operation graph tensorflow to run on CPUIn general it does n't seem wise to just remove every operation that does n't run on CPU anywayI 've tried wrapping everything in a with tf.device ( 'CPU:0 ' ) block , as well as using config = tf.ConfigProto ( device_count= { 'GPU ' : 0 } ) but neither changed the error.Relevant code :"
"I have a dataframe like this : I want to sum the three largest integers from COUNTY_POP for each state . So far , I have : However when I add the .sum ( ) operation to the above code , I receive the following output.I 'm relatively new to Python and Pandas . If anyone could explain what causes this and how to correct it , I 'd really appreciate it !"
"I 'm using Sphinx to document a project that depends on wxPython , using the autodocs extension so that it will automatically generate pages from our docstrings . The autodocs extension automatically operates on every module you import , which is fine for our packages but is a problem when we import a large external library like wxPython . Thus , instead of letting it generate everything from wxPython I 'm using the unittest.mock library module ( previously the external package Mock ) . The most basic setup works fine for most parts of wxPython , but I 've run into a situation I ca n't see an easy way around ( likely because of my relative unfamiliarity with mock until this week ) .Currently , the end of my conf.py file has the following : For all the wxPython modules but wx.lib.newevent , this works perfectly . However , here I 'm using the newevent.NewCommandEvent ( ) function [ 1 ] to create an event for a particular scenario . In this case , I get a warning on the NewCommandEvent ( ) call with the note TypeError : 'Mock ' object is not iterable.While I can see how one would use patching to handle this for building out unit tests ( which I will be doing in the next month ! ) , I 'm having a hard time seeing how to integrate that at a simple level in my Sphinx configuration.Edit : I 've just tried using MagicMock ( ) as well ; this still produces an error at the same point , though it now produces ValueError : need more than 0 values to unpack . That seems like a step in the right direction , but I 'm still not sure how to handle this short of explicitly setting it up for this one module . Maybe that 's the best solution , though ? FootnotesYes , that 's a function , naming convention making it look like a class notwithstanding ; wxPython follows the C++ naming conventions which are used throughout the wxWidgets toolkit ."
"Why and how can this tuple be modified by appending after construction ? Why is this going to print out ( [ 1,2,3 ] , [ 4,5,6 ] , `` lol '' ) instead of ( [ 1,2,3 ] , [ 4,5,6 ] , `` lolol '' ) ?"
Because of the APPEND_SLASH = True setting all requests with `` /whatever/path '' will be redirected to `` /whatever/path/ '' .BUT urls definded within a i18n_patterns ( ) do n't redirect for some reasoneven the test works :
"I have written a module that dynamically adds periodic celery tasks based on a list of dictionaries in the projects settings ( imported via django.conf.settings ) .I do that using a function add_tasks that schedules a function to be called with a specific uuid which is given in the settings : Like suggested here I use the on_after_configure.connect signal to call the function in my celery.py : This setup works fine for both celery beat and celery worker but breaks my setup where I use uwsgi to serve my django application . Uwsgi runs smoothly until the first time when the view code sends a task using celery 's .delay ( ) method . At that point it seems like celery is initialized in uwsgi but blocks forever in the above code . If I run this manually from the commandline and then interrupt when it blocks , I get the following ( shortened ) stack trace : It seems like there is a problem with acquiring a mutex.Currently I am using a workaround to detect if sys.argv [ 0 ] contains uwsgi and then not add the periodic tasks , as only beat needs the tasks , but I would like to understand what is going wrong here to solve the problem more permanently.Could this problem have something to do with using uwsgi multi-threaded or multi-processed where one thread/process holds the mutex the other needs ? I 'd appreciate any hints that can help me solve the problem . Thank you.I am using : Django 1.11.7 and Celery 4.1.0Edit 1I have created a minimal setup for this problem : celery.py : tasks.py : Make sure that CELERY_TASK_ALWAYS_EAGER=False and that you have a working message queue.Run : Wait about 10 seconds before interrupting to see the above error ."
"I was trying to write a function that inputs a nested tuple and returns a tuple where all the elements are backwards , including those elements in other tuples ( basically mirrors it ) .So with this input : It should return : What I tried"
How would I do the following in numpy ? select all rows of an array containing note more than 50 % 0 values.select first n ( let 's say 2 ) rows from all rows satisfying 1.do something and place modified rows on same index of a zero array with equal shape of 1.The following results in an array where no new values are assigned :
"So having a list say b = [ b1 , b2 , b3 ] I want to be able to sort a list a in such a way that all bi 's that also exist in a have the same relative order as in b - leaving the rest of a 's elements alone . So b of course may be a tuple or any other ordered structure . What I came up withis clumsy and O ( n^2 )"
"I am practising for a programming competition in which I will have the choice for each problem whether to use Python or C++ , so I am open to a solution in either language – whatever language is best suited to this problem.The URL to the past problem I am stuck on is http : //progconz.elena.aut.ac.nz/attachments/article/74/10 % 20points % 20Problem % 20Set % 202012.pdf , problem F ( `` Maps '' ) .Basically it involves matching occurrences of a small piece of ASCII art within a big one . In C++ I can make a vector for each piece of ASCII art . The problem is how to match it when the smaller piece is multi-line.I have no idea how to go about it . I do n't want all the code written for me , just an idea of the logic needed for the problem.Thanks for any help.Here 's what I 've got so far : EDIT : From the comments below I 've got a solution in Python from J.F . Sebastian . It works but I do n't understand it all . I 've commented what I could but need help understanding the return statement in the count_pattern function ."
"I am learning how to embed Rust functions in Python , and everything works fine if my inputs are ints , but not list . If my lib.rs file is : I can use this as follows : However if I change my lib.rs to : I can no longer use it in Python ( this compiled fine ) : The reason , I though this could work is that Python 's list and Rust 's Vec are the both dynamic arrays , but apparently I am missing something here ... Why does my attempt not work ? What should I do to fix it ?"
I want put external image on my blog created with Pelican So i try : After running `` pelican -s pelicanconf.py '' I 've got error : And no image in my post.Questions is : How put external images in my blog .
"On a server , for disk optimization , we do not install a C compiler , and here is the problem : I want to install 'spur ' python package with buildout 2.3.1 , spur is using pycrypto which requires a C compiler.To avoid compilation , I installed on the server ( Suse 11SP2 ) the rpm python-pycrypto ( python-pycrypto-2.6-31.7.x86_64.rpm ) . I can check with 'pip freeze ' that pycrypto is installed : Nevertheless , when I start buildout , it still tries to build pycrypto with a C compiler that does not exist.So I added include-site-packages = true and allowed-eggs-from-site-packages = pycrypto in buildout.cfg , but they are not taken in account.I also tried to do a : but that did not help too.What can I do to avoid buildout compile a package that is already compiled and installed from rpm ? Here is the buildout.cfg and the logs :"
"I have a list of revenue charges that I need to round up or down to the nearest bin.For example , if the only products sold cost the following : and I have a few charges like this : I followed this example , which is definitely the closest solution : I thought leaving the right= parameter to the default for np.digitize would accomplish what I needed , but the results are still rounded up : In my particular case , I need to round up or down to the nearest bin . For example , the $ 12.50 charge is only $ 2.55 away from $ 9.95 , but the chosen option , $ 19.95 , is $ 7.45 away.EDITWhile one answer in the solution I linked to can round both up/down , it appears that method can return inconsistent results , hence this new post ."
"Ran a simple script in Numpy to solve a system of 3 equations : But when I ran it through the command prompt , I somehow got : although Wolfram Alpha says there 's no solution.Does anyone know why there seems to be this difference between the Numpy/WRA answers ?"
My aiohttp webserver uses a global variable that changes over time : Results in : UnboundLocalError : local variable 'shared_item ' referenced before assignmentHow would I properly use a shared variable shared_item ?
"Apparently PostgreSQL 8.4 and Ubuntu 10.04 can not handle the updated way to sort W and V for Swedish alphabet . That is , it 's still ordering them as the same letter like this ( old definition for Swedish ordering ) : WaVbWcVdit should be ( new definition for Swedish ordering ) : VbVdWaWcI need to order this correctly for a Python/Django website I 'm building . I have tried various ways to just order a list of tuples created from a Django QuerySet using *values_list* . But since it 's Swedish also å , ä and ö letters needs to be correctly ordered . Now I have either one or the other way both not both.. The examples gives : Now , What I want is a combination of these so that both V/W and å , ä , ö ordering are correct . To be more precise . I want Ordering One to respect locale . By then using the second item ( object id ) in each tuple I could fetch the correct object in Django . I 'm starting to doubt that this will be possible ? Would upgrading PostgreSQL to a newer version that handles collations better and then use raw SQL in Django be possible ?"
"Compared to decorators applied to a function , it 's not easy to understand the decorators applied to a class . What 's the use case of decorators to a class ? How to use it ?"
"I 'm trying to have a class act in every way like a tuple that 's an attribute of the class , so len ( instance ) would be the same as len ( instance.tup ) , instance [ 3 ] would return instance.tup [ 3 ] , etc . Here 's the class : I canBut if I try to : mt does in fact have a __len__ that I can call : ( I even renamed it __len__ ) . As near as I can tell , this should look just as if I did : But python wo n't let me len ( mt ) ( or mt [ 2 ] or mt [ 1:5 ] for that matter ) ."
I am wondering if there is a way to add bias node to each layer in Lasagne neural network toolkit ? I have been trying to find related information in documentation.This is the network I built but i do n't know how to add a bias node to each layer .
"The code below reproduces the problem I have encountered in the algorithm I 'm currently implementing : The problem is that after some number of iterations , things start to get gradually slower until one iteration takes multiple times more time than initially.A plot of the slowdownCPU usage by the Python process is stable around 17-18 % the whole time.I 'm using : Python 2.7.4 32-bit version ; Numpy 1.7.1 with MKL ; Windows 8 ."
"I often use Numpy from the command line and always have to remember to apply some recurrent settings , for example setting the output formatting : Is there a global Numpy config file that is automatically executed for a new Python shell or during import that can execute this ? If not , is there an elegant way to achieve this effect ?"
"I 'm trying to add the following YAML fileTo edit the host part , I got it working withBut I ca n't find a way to update the password for main and admin ( which are different values ) .I tried to play around with \n and [ [ : space : ] ] and got different flavours of : But never got it to work.Any help greatly appreciated ! Edit - Requirement : no external binaries/tools . Just good ol ' bash ."
What is the time complexity of deleting an element of collections.deque ? E.g . :
Test code : The output on my machine ( a fairly fast x86-64 Linux desktop with Python 3.6 ) is : I understand why the second line is a bit faster ( it ignores the Pandas index ) . But why is the eval ( ) approach using numexpr so slow ? Should n't it be faster than at least the first approach ? The documentation sure makes it seem like it would be : https : //pandas.pydata.org/pandas-docs/stable/enhancingperf.html
"If I want to use a file object just once , normally I would still use the with block or explicitly close the file object when I 'm done , because closing a file seems to be the right-thing-to-do : orHowever , I 've seen people use pass the call to open directly to a function without saving the output to any variable , making it impossible to close explicitly : So , is either of these true , Somehow the unnamed file object gets closedIt does n't matter , it 's fine not to close the fileor is this a bad practice ? Also , does the answer change if I allow the file to be read/write ? To be clear , dosomething could be something like np.array ( ) or for i in f.readlines ( )"
"Consider the following script : On my machine this prints outThe first case is clear ( as per IEEE-754 ) , but what 's going on in the second case ? Why are the two NaNs comparing equal ? Python 2.7.3 , Numpy 1.6.1 on Darwin ."
"I have some simple code that does the following.It iterates over all possible length n lists F with +-1 entries . For each one , it iterates over all possible length 2n lists S with +-1 entries , where the first half of $ S $ is simply a copy of the second half . The code computes the inner product of F with each sublist of S of length n. For each F , S it counts the inner products that are zero until the first non-zero inner product . Here is the code . The correct output for n=14 is Using pypy , this takes 1 min 18 seconds for n = 14 . Unfortunately , I would really like to run it for 16,18,20,22,24,26 . I do n't mind using numba or cython but I would like to stay close to python if at all possible . Any help speeding this up is very much appreciated.I 'll keep a record of the fastest solutions here . ( Please let me know if I miss an updated answer . ) n = 22 at 9m35.081s by Eisenstat ( C ) n = 18 at 1m16.344s by Eisenstat ( pypy ) n = 18 at 2m54.998s by Tupteq ( pypy ) n = 14 at 26s by Neil ( numpy ) n - 14 at 11m59.192s by kslote1 ( pypy )"
"I 've developed a simple application that generates test data series and I 've built it to be able to be repeatable , using a random seed . I noticed the following and wanted to know why this happens : Note how a single call to random ( ) uses up two values for randint ( ) . I 'm guessing this has something to do with the amount of random information required to generate a float vs. an int in the given range , but is there some way of keeping track of 'how many random values have been used so far ? ' , i.e . how far along in the sequence of semi-random values the system is ? I ended up writing my own function , always using a single call to random.random ( ) in its logic . So I 'm not asking for a solution , just some background / an explanation ."
"I 'm wondering whether there is a Pythonic way to compute the means and variances of Counters ? For example , I have four Counters sharing the same keys : My way to do that is : I could use the following code to find the mean / variance for each key : Is there an easier way to compute the mean / variance for each key compared to my way ?"
"I 'm trying to implement a `` subcommand '' system as an inheritable class in Python . My expected use case is something like : I implemented Command.subcommand as a static method and everything worked fine until I tried to add a subcommand to the parent class , which got me TypeError : 'staticmethod ' object is not callable . In hindsight , it 's obvious that this wo n't work : The only alternative I 've found so far is to declare the subcommand decorator outside the parent class , then inject it after the class definition is complete.However , as someone who never used Python before decorators were added , this feels very clunky to me . Is there a more elegant way to accomplish this ?"
I am trying to Deploy an Recommendation Engine as mentioned in quick start guide.I completed the steps up to build the engine . Now I want to train the Recommendation Engine . I did as mentioned in quick start guide . ( execute pio train ) . Then I got the lengthy error log and I could n't paste all here . So I am putting first few rows of the error.what can I do to overcome this isssue ?
"Based on my experiments I 'm guessing the answer to this is no . But perhaps it might be possible with some changes to the futures module . I would like to submit a worker that itself creates an executor and submits work . I want to return that second future to the main process . I have this MWE , which does not work because the f2 object likely becomes disassociated from its parent executor when it is sent over via multiprocessing . ( It does work if both executors are ThreadPoolExecutor , because the f2 object is never copied ) . My question is : Is there any safe way that I might send a future object across a multiprocessing Pipe and be able to receive the value when it is finished . It seems like I might need to set up another executor-like construct that listens for results over another Pipe ."
"There is already a multi key dict in python and also a multivalued dict . I needed a python dictionary which is both : example : Probability of d [ 'red ' ] ==d [ 'green ' ] is high and Probability of d [ 'red ' ] ! =d [ 'red ' ] is low but possiblethe single output value should be probabilistically determined ( fuzzy ) based on a rule from keys eg : in above case rule could be if keys have both `` red '' and `` blue '' then return `` baloon '' 80 % of time if only blue then return `` toy '' 15 % of time else `` car '' 5 % of time.The setitem method should be designed such that following is possible : Above assigns multiple values to the dictionary with a predicate function and corresponding probability . And instead of the assignment list above even a dictionary as assignment would be preferable : In the above baloon will be returned 80 % of time if `` red '' or green is present , return toy 15 % of time if blue present and return car 5 % of time without any condition.Are there any existing data structures which already satisfy the above requirements in python ? if no then how can multikeydict code be modified to meet the above requirements in python ? if using dictionary then there can be a configuration file or use of appropriate nested decorators which configures the above probabilistic predicate logics without having to hard code if \else statements .Note : Above is a useful automata for a rule based auto responder application hence do let me know if any similar rule based framework is available in python even if it does not use the dictionary structure ?"
How can one strip the left parts and right parts off strings up to a matching expression as in ksh ? For instance : ( see http : //www.well.ox.ac.uk/~johnb/comp/unix/ksh.html for ksh examples ) .I ca n't seem to figure out a simple way of doing this using re module or string module but I must be missing something .
I have the following requirements file : Installing the project from shell works perfect : but trying to install it from the requirements file raises this error : NB : the hashes in the requirement files a generated from pipenv lock -r
Consider this code : I would expect it to show True . Why does it show False ?
"I was surprised today to see that the following works with no exceptions ( in Python 2.7.3 at least ) : I would have thought that this should raise a NameError in the REPL , similar to how the following would : Anyone have any idea what 's going on here ?"
"I 'm currently writing a data collection service for multiple services . There are probably 5 different API Endpoints with differing hosts & port numbers . I wanted to create a settings file for this but thought that the .ini should be a better place , or so I thought ... My development.ini looks something like this : I tried to access the custom sections within a pyramid event like such : But that did n't work , because settings is actually a dict of the [ app : main ] attributes . Can someone teach me the way to access the sections the Pyramid way ? I read about another way , using ConfigParser , but I wanted to ask if there 's any other easier way in Pyramid first ."
"I was playing around with f-strings ( see PEP 498 ) , and I decided to check the speed of the f-string parse , ( e.g . f '' { 1 } '' ) in comparison with the usual str parse ( e.g str ( 1 ) ) . But to my surprise , when I checked the speed of both methods with the timeit function , I found outthat f-strings are faster.whereasor even the repr func , which in most of the cases is faster than str castI wonder why is that ? I thought that the f-strings called str internally , but now , I 'm a bit confused , any ideas ? Thanks in advance ! PD : Just if anyone is wondering :"
I try to use the Django-activity-stream package but I have a problem with the configuration ( http : //django-activity-stream.readthedocs.org/en/latest/configuration.html ) .The name of my project is proj and I want use this package on the main_app activity./proj/proj /main_appSo in main_app/apps.py I did this : But I have a error when I add this line in main_app/init.py : The error : My settings : Edit ( 2 ) : Now I have this in main_app/apps.py : from django.apps import AppConfig
"I have a situation in which I 'd like to maintain a mapping from one object to another for as long as the first object exists . My first thought was to use a WeakKeyDictionary.This works fine in most cases . However , in the case where some_other_object is ( or has a reference to ) m , the M instance will not be garbage collected . ( To see an example , replace some_other_object with m ) Of course , if I stored the mapping on the object itself , it would be garbage collected when I deleted it : Can I achieve the results of the second example using the weakref module ( i.e . without mutating m ) ? In other words , can I use the weakref module to map an object to itself ( or another object that has a strong reference to it ) and only keep the object in memory as long as there are other references to it ?"
"I use Jedi for Python autocompletion in Emacs , but it 's not a dependency of my code so I do n't want to put it in my requirements.txt . ( Other developers may not use a Jedi editor plugin , and it 's certainly not needed when I deploy to Heroku . ) But Jedi must be available from my virtualenv in order to function , i.e . if I can'tit does n't work.Is there a good way to install Jedi user-globally such that it is available in all of my virtualenvs ? I think what I want is toinstall Jedi into ~/.local/lib/python2.7/site-packages/ with pip 's -- user flag , then tocreate my virtualenv using the equivalent of the -- system-site-packages flag , but for user packages instead of system packages.My current workaround is to pip install jedi in each of my virtualenvs . Then when I add new dependencies I pip install foo , pip freeze > requirements.txt , then manually remove jedi and a few other things from the file before committing . Obviously , this is time-consuming and error-prone.Does anybody have a better solution ?"
I have a ttk Combobox I 'd like to unbind from the mousewheel so that scrolling with the wheel while the Combobox is active does n't change the value ( instead it scrolls the frame ) .I 've tried unbind as well as binding to and empty function but neither works . See below : Any help would be greatly appreciated.Thanks !
"In Javascript , I can assign a value like this so : var i = p || 4Obviously if p is n't defined it will resort to 4 . Is there a more elegant version of this operation in Python than a try : / except : combo ?"
"I like my json output to be nicely formatted , even for a REST API . It helps when debugging , etc . The extra overhead is minimal , especially when using gzipIs there anyway to tell the pyramid json renderer ( i.e , this thing ) to output formatted , pretty-printed output ?"
"I need to write few basic scripts for students in Python , like this one : But apparently I can not use accented characters while declaring variables . Is there any way out ? ( `` mia_età '' means `` my_age '' in Italian and I would like to avoid them to write grammar errors in their firts language while learning Python )"
"Suppose I have the unit vector , u.The unit vector is an eigenvector of a matrix with integer entries . I also know that the eigenvalues are integers . So , the unit vector will contain irrational decimals that , when squared , are decimal approximations of rational numbers.Is there any good way to find the smallest value k such that all entries of ku are integers ? In general , k will be the square root of an integer.A naive approach would be to square each entry in the vector and find the smallest ki that produces an integer . Then , k will be the square root the of LCM of all ki . I 'm hoping there is a better approach than this.Note that this is not a duplicate of this question ."
"I need to process lots of medium to large files ( a few hundred MB to GBs ) in a linewise manner , so I 'm interested in standard D approaches for iterating over lines . The foreach ( line ; file.byLine ( ) ) idiom seems to fit the bill and is pleasantly terse and readable , however performance seems to be less than ideal.For example , below are two trivial programs in Python and D for iterating over the lines of a file and counting the lines . For a ~470 MB file ( ~3.6M lines ) I get the following timings ( best out of 10 ) : D times : Python times ( after EDIT 2 , see below ) : Here 's the D version , compiled with dmd -O -release -inline -m64 : And now the corresponding Python version : EDIT 2 : I changed the Python code to use the more idiomatic for line in infile as suggested in the comments below , leading to an even greater speed-up for the Python version , which is now approaching the speed of the standard wc -l call to the Unix wc tool.Any advice or pointers to what I might be doing wrong in D , that is giving such poor performance ? EDIT : And for comparison , here 's a D version that throws the byLine ( ) idiom out the window and sucks all the data into memory at once , and then splits the data into lines post-hoc . This gives better performance but is still about 2x slower than they Python version.The timings for this last version are as follows :"
"I have a N x 2 dimensional numpy array . I would like to make a ( 2*N ) x 2 , where each column is repeated . I 'm curious if there is a more efficient way than what I 've written below to accomplish this task.The code above is slow by numpy standards , most likely because of the zip . Is there a numpy function that I can replace zip with ? Or a better way to do this altogether ?"
What is happening : What I want to happen : Does anyone know why the command is printed out before being executed and how to stop it from doing that ? I ca n't find any documentation about this . I 'm using Emacs 23 on Mac OS X with Python 2.7 .
"I have two blocks that call the same method with same variables . I want to call the method only once , but the result is then outsite the scope of the block tags . I 've tried calling this method in the parent template header.html and with a with tag , but nothing seems to work.This is the layout : navigation_tags.py"
"I ’ m working on improving the character encoding support for a Python IRC bot that retrieves the titles of pages whose URLs are mentioned in a channel.The current process I ’ m using is as follows : Requests : Beautiful Soup : title = soup.title.string.replace ( '\n ' , ' ' ) .replace ( ... ) etc.Specifying from_encoding=r.encoding is a good start , because it allows us to heed the charset from the Content-Type header when parsing the page.Where this falls on its face is with pages that specify a < meta http-equiv … charset=… '' > or < meta charset= '' … '' > instead ( or on top ) of a charset in their Content-Type header.The approaches I currently see from here are as follows : Use Unicode , Dammit unconditionally when parsing the page . This is the default , but it seems to be ineffective for any of the pages that I ’ ve been testing it with.Use ftfy unconditionally before or after parsing the page . I ’ m not fond of this option , because it basically relies on guesswork for a task for which we ( usually ) have perfect information.Write code to look for an appropriate < meta > tag , try to heed any encodings we find there , then fall back on Requests ’ .encoding , possibly in combination with the previous option . I find this option ideal , but I ’ d rather not write this code if it already exists.TL ; DR is there a Right Way™ to make Beautiful Soup correctly heed the character encoding of arbitrary HTML pages on the web , using a similar technique to what browsers use ?"
"Given a list of 50k websites urls , I 've been tasked to find out which of them are up/reachable . The idea is just to send a HEAD request to each URL and look at the status response . From what I hear an asynchronous approach is the way to go and for now I 'm using asyncio with aiohttp.I came up with the following code but the speed is pretty abysmal . 1000 URLs takes approximately 200 seconds on my 10mbit connection . I do n't know what speeds to expect but I 'm new to asynchronous programming in Python so I figured I 've stepped wrong somewhere . As you can see I 've tried increasing the number of allowed simultaneous connections to 1000 ( up from the default of 100 ) and the duration for which DNS resolves are kept in the cache ; neither to any great effect . The environment has Python 3.6 and aiohttp 3.5.4.Code review unrelated to the question is also appreciated ."
I wrote the following code : get_size ( ) returns the memory consumption of the objects using this code.I get the following prints : How come the objects consumed 20M if the process consumed only 8M more ? If I exit a method should n't the memory decreased back to 21 as the garbage collector will clear the consumed memory ?
"I am trying to use spectral clustering on an image . I first calculate the affinity matrix , then attempt to get the eigenvectors . However , on a 7056x7056 matrix eig ( ) call is taking too long . Any suggestions on how to improve this ? Perhaps I should use a different form of affinity ?"
My structure of file is file __init__.pyfile bar.pyI got this error .
Number of rows in my dataset is 500000+ . I need Hausdorff distance of every id between itself and others . and repeat it for the whole datasetI have a huge data set . Here is the small part : I want to calculate Haudorff Distance : Output is 0.05114626086039758Now I want to calculate this distance for the whole dataset . For all id_easys . Desired output is matrix with 0 on diagonal ( because distance between the aaa and aaa is 0 ) :
"Consider this : With enumerate this loop can be written as : If the_data = { 'john ' : 'football ' , 'mary ' : 'snooker ' , 'dhruv ' : 'hockey ' } Loop with key value pair assigned in loop : While using enumerate , the data becomes a tuple within the loop , hence needs one extra line of assignment after the loop declaration :"
"I have enabled sphinx.ext.intersphinx in my project and added the following configuration : I have the following in my index.rst : I would like the link to point to http : //pythonhosted.org/pyserial/ , the root URL in intersphinx_mapping , but I do n't know what ? ? ? should be.If I do : ref : ` pyserial ` or : ref : ` pyserial < pyserial > ` , I get WARNING : undefined label : pyserial ( if the link has no caption the label must precede a section header ) If I do : ref : ` pyserial < > ` I get WARNING : undefined label : ( if the link has no caption the label must precede a section header ) I can replace the : ref : with ` pyserial < http : //pythonhosted.org/pyserial/ > ` _ , but I would really like to reference the page via intersphinx , to avoid broken links down the line.I am using sphinx 1.6.3 on Python 3.6.2 in Anaconda . I am not overly hung on the library I am trying to link to . I suspect that the answer will not really be tied to a library.If it matters any , the regular references to the pyserial docs work just fine . For example : py : class : ` serial.Serial ` links to https : //pythonhosted.org/pyserial/pyserial_api.html # serial.Serial ."
"So , I have a series of actions to perform , based on 4 conditional variables - lets say x , y , z & t. Each of these variables have a possible True or False value . So , that is a total of 16 possible permutations . And I need to perform a different action for each permutation.What is the best way to do this rather than making a huge if-else construct.Lets see a simplified example . This is how my code would look if I try to contain all the different permutations into a large if-else construct.Is there any way to simplify this ? Obviously , my objective for each case is more complicated than just printing `` Case 1 '' ."
"I am trying to install a list of packages using pip.The code which I used is : This code works fine and if a package is not available , it gives an error : No matching distributions foundHowever , what I am trying to do is if an installation fails ( for eg : invalid package name ) , I want to print the package which failed.What can be done for that ? Any help would be appreciated , thank you ."
"I 've been following official Kivy PongApp tutorial ( link - whole program code at the bottom of site ) and I 've faced a problem I ca n't really understand.I 've defined the move function to change the position of the ball by a velocity vector on each frame . The code : However , when I 've written the code like this : It results in an error : ValueError : PongBall.pos value length is immutableWhy , should n't it be the same ?"
"Is there any way to understand what data type that a string holds ... The question is of little logic but see below casesMy Case : I get a mixed data in a list but they 're in string format.I 'm looking for a generic way to understand whats their data so that i can add respective comparison . Since they 're already converted to string format , I cant really call the list ( myList ) as mixed data ... but still is there a way ?"
"May someone please explain me why np.array ( [ 1e5 ] ) **2 is not the equivalent of np.array ( [ 100000 ] ) **2 ? Coming from Matlab , I found it confusing ! I found that this behaviour starts from 1e5 , as the below code is giving the right result :"
"I would like to use logger module for all of my print statements . I have looked at the first 50 top google links for this , and they seem to agree that doctest uses it 's own copy of the stdout.If print is used it works if logger is used it logs to the root console.Can someone please demonstrate a working example with a code snippet that will allow me to combine.Note running nose to test doctest will just append the log output at the end of the test , ( assuming you set the switches ) it does not treat them as a print statement ."
"I am new to python , django . What I am trying to do is that I have a model defined as products which has two columns : name and price.I am trying to sort them using the following code : This sorts the values as 0,20,30.The model code isWhat I am trying to achieve is to sort it as 20,30,0 with 0 being appended at the end.Is there any function with which I can achieve it ?"
"I am trying to get the number of symbols in a SymPy expression like : The number for instance should be 3 . All I came up with so far is However , this counts also the constant factors 1/4 and 1/2.Any Ideas ?"
"Here is what is run on pre_delete of model Document . This code gets royally ignored when put in a separate file ( signals.py ) as suggested best practice . When put in model file , it works fine.So what is the problem ? Should I import something more in there ? Or should I import this file somewhere ?"
"I have downloaded Pandas source and now trying to debug it.I modified Makefile : Also I have a very simple script : When I try to run it with python-dbg test1.py , this is what I get : What is wrong ?"
"I 've been trying out TensroFlow v2 beta and I 'm trying out the tf.keras models.When I compile a model and select the optimizer to be the string 'adam ' . The model can be trained properly : But when I try to use the default optimizer tf.keras.optimizers.Adam ( ) it ca n't be trained and outputs a nan loss at each iteration.Is n't the string 'adam ' supposed to be the default adam optimizer or am I missing something ? I 've tried several hyperparameters ( learning_rate , beta_1 , beta_2 , etc . ) , but none seem to work . This is a big deal because I might not want to use the default hyperparameters all the time.Can anyone explain this behaviour ?"
"When attempting to use SignedJwtAssertionCredentials ( ) with a google service account I have been receiving the following error on one Windows 2008 server machine , but not on a Windows 7 machine locally.I am reading the p12 key file as follows before passing it to SignedJwtAssertionCredentials ( ) ."
I have a list such asand a dataframe such asI want to make a group sum from the list above to become : How can I do this using python pandas ? Needless to say I am a newbie in pandas . Thanks .
"I am trying to access a Google sheet using a cloud function and the sheets API in python , but get a 403 permissions error . I have created a service account , exported the key ( included in the cloud function zip I am uploading ) and have given this service account the API scopes necessary to access sheets ( I 've tried both https : //www.googleapis.com/auth/spreadsheets and https : //www.googleapis.com/auth/drive ) . Here is the code : requirements.txt : If I explicitly share the Google sheet with the service account , the authentication does work and I can access the sheet data . The question is why does n't this work by applying the proper API scopes as the Google accounts admin ?"
"So i have made my own dict-based named-tuple class : The point of using a dict is so i can use the **splat operator , as well as so the whole thing can be json 'd as a dict . The double for loop in init is so I can immediately re-construct it from the dict after deserialization.The only thing I 'm missing is the multi-assignment thing you can do with tuples , likeIs there any way I can get this sort of behavior with my own custom datatype ? Is there any function that I can override or some class I can inherit from to get this behaviour ?"
I want to highlight the area between 2 lines in Altair . I tried to look if the solution is using mark_area ( ) but I ca n't find how to specify the low limit with data.my data ( few lines ) : my chart :
"My spider.py file is as so : And my parse def is as below : When I run my spider , below line gets printed as the header : AS you can see , this does not have the custom header I added to the Scrapy request . Can anybody help me with adding a custom header values for this request ? Thanks in advance ."
"Hi I 'm trying to derive a class from ndarray . I 'm sticking to the recipe found in docs but I get an error I do not understand , when I override a __getiem__ ( ) function . I 'm sure this is how it is supposed to work but I do not understand how to do it correctly . My class that basically adds a `` dshape '' property looks like : when I now try to do : the interpreter will fail with a maximum depth recursion , because he calls __getitem__ over and over again.can someone explain to me why and how one would implement a getitem function ? cheers , David"
"I 'm using Python 3 to output 2 progress bars in the console like this : Both bars grow concurrently in separate threads.The thread operations are fine and both progress bars are doing their job , but when I want to print them out they print on top of each other on one line in the console . I just got one line progress bar which alternates between showing these 2 progress bars.Is there any way these progress bars can grow on separate lines concurrently ?"
"I am trying to find a regular expression to comma separate a large number based on the south asian numbering system.A few examples:1,000,000 ( Arabic ) is 10,00,000 ( Indian/Hindu/South Asian ) 1,000,000,000 ( Arabic ) is 100,00,00,000 ( Indian/H/SA ) . The comma pattern repeats for every 7 digits . For example , 1,00,00,000,00,00,000.From the book Mastering Regular Expressions by Friedl , I have the following regular expression for Arabic numbering system : For Indian numbering system , I have come up with the following expression but it does n't work for numbers with more than 8 digits : Using the above pattern , I get 100000000,00,00,000.I am using the Python re module ( re.sub ( ) ) . Any ideas ?"
"In Python 2.6 it is possible to suppress warnings from the warnings module by usingVersions of Python before 2.6 do n't support with however , so I 'm wondering if there alternatives to the above that would work with pre-2.6 versions ?"
"This is what my dataframe looks like : This is what I 'm trying to do in Pandas ( notice the timestamps are grouped ) : I 'm trying to create bins of 10-minute time intervals so I can make a bar graph . And have the columns as the CAT values , so I can have a count of how many times each CAT occurs within that time bin.What I have so far can create the time bins : But my issue is I can not for the life of me figure out how to group by both the CATs and by time bins . My latest try was to use df.pivot ( columns= '' CAT '' ) before doing the groupby but it just gives me errors : Which gives me : ValueError : Buffer has wrong number of dimensions ( expected 1 , got 2 )"
"I have this very simple code : This is the error : Why ca n't I have an iterable of Int pairs as the return type ? Both - > Position and Iterable [ Any ] work , just not Iterable and Position together do n't ."
I have a text file which holds lots of files path file.txt : What I did with Regex to extract the date from path : it does not give what I want.My output should be like this : and then save it as list of lists : or save it as a Pandas series.is there any way with regex to get what I want ! ?
"I am experiencing some trouble using the Python 2.6.5 xml.etree.ElementTree library . In particular , if I setup a simple xml element like the followingi have no problems with the library when accessing the inner element nodes , e.g . : However , I am encountering a strange boolean interpretation of leaf element nodes , see : Note that in the last command , the element xml.find ( ' b/c ' ) , which is clearly non-None , evaluates to False . This is especially annoying since i can not use the idiom to check whether a leaf element exists . ( I have to check explicitly for 'xml.find ( ' b/c ' ) ! = None ' . ) Can someone please explain this ( for me unexpected ) behavior ?"
"I 'm reading in various datatypes from a mySQL database . The fifth column has type 'DATETIME ' in the database . I use that as the entry_date for a 'BloodTraitRecord ' Object.BloodTraitRecord class : DATETIME objects from the database look like this in the mySQL server : The code functions as expected unless the time in the database is midnight , like so : In that case , and that case only , I get this error when comparing the record 's entry_date.date ( ) to another datetime.date later in the code : Printing record.entry_date confirms that the time attribute is gone for this case : I have a way to fix this by checking the type of the object , and only calling the date attribute if the object is a datetime , but I 'm wondering if there is a better fix than this . I also do n't understand why python is immediately converting the MySQL DATETIME to a datetime.date when the DATETIME time is 00:00:00.Thanks for your help !"
Sample data : I want to get the sum of the 3 columns with the largest values in the row . These are different columns for every row ( Sum_max_3 ) .I have many many columns so I need to do this automatically for all of them .
"Some simple data to get us started : So , up until this point , I always thought that assign was the equivalent of mutate in the dplyr library . However , if I try to use a variable that I have created in an assign step in that same assign step , I get an error . Consider the following , which is acceptable in R : If I try the equivalent in pandas , I get an error : The only way I can think of to do this is to use two assign steps : Is there something that I 've missed ? Or is there another function that is more appropriate ?"
I 'm using selenium-python with PhantomJS . The code is pretty much like this : I use a celery task which runs this code periodically . The problem is that from time to time there are some stale phantomjs processes . When I look into celery logs the task is completed successfully without any errors but the phantomjs process is still running.Some extra info : I 'm using Python 3.2I 'm using Celery 3.1 with beatI 'm using Debian WheezyI compiled PhamtomJS from source and created a symbolic link like this : ln -s /opt/phantomjs/bin/phantomjs /usr/local/bin/Can someone suggest a way to debug and find out who 's fault is this ?
"I am using Flask , Python for my web application . The user will login and if the session time is more than 5 minutes then the app should come out and it should land on the login page.I tried some of the methods and I can see the session time out is happening but redirect to login page is not happening.I have used before_request for seesion time out.I have referrred this link Flask logout if sessions expires if no activity and redirect for login page but I dont see any changes from what I have tried before and this code . I can see lot of stackoverflow questions over there for this topic and I couldnt find the solution.I have tried this link alsExpire session in flask in ajax contextBut I am not sure what should I pass as a session and what qualifier I should return here ? if the previous method is correct can some one tell me what is the session and what qualifier I should return here ? What I need is after session log out the page should automatically land on the login screenWhat is happening is session log out is happening but it 's not redirecting to login pagecould some one help me in this ?"
"I 'm a longtime SAS user trying to get into Pandas . I 'd like to set a column 's value based on a variety of if conditions . I think I can do it using nested np.where commands but thought I 'd check if there 's a more elegant solution . For instance , if I set a left bound and right bound , and want to return a column of string values for if x is left , middle , or right of these boundaries , what is the best way to do it ? Basically if x < lbound return `` left '' , else if lbound < x < rbound return `` middle '' , else if x > rbound return `` right '' .Can check for one condition by using np.where : But not sure what to do it I want to check multiple if-else ifs in a single line.Output should be :"
"I 'm new to python and I 'm trying to make a simple Guess the number game , and I 'm stuck in an if statement in a while loop . here is the code.I 'm experiencing it at the Your guess it too high and the low one . I tried breaking it , but it simply stops the whole things"
"I have an array ar = [ 2,2,2,1,1,2,2,3,3,3,3 ] .For this array , I want to find the lengths of consecutive same numbers like : In R , this is obtained by using rle ( ) function . Is there any existing function in python which provides required output ?"
"I have a df ( sample is pasted here at the end ) .I am looking to find which tradePrice had the most tradeVolume per last 10 min , or any other rolling period . This is the pivot table done in xls which is based on the sample data attached.The result need to be a df something like this searching for the price with the most volume traded ) : In order to get 1 min . results I groupby & applied the following functionMy question is : How can I get the same solution but per rolling time period ( which can not be less than 1 min ) so the expected result ( which price was traded with the max volume ) for the last 10 minutes is : thanks in advance ! The sample data :"
"I have been really fascinated by all the interesting iterators in itertools , but one confusion I have had is the difference between these two functions and why chain.from_iterable exists.What is the difference between the two functions ?"
"I need to search for an item in a list around a given index with in a given radius . Currently I use this function to generate alternating offsets for the search : The code that does the search looks something like this : My question is , is there a more elegant way to generate the search indizes , maybe without defining a special function ?"
"As part of building a Data Warehouse , I have to query a source database table for about 75M rows.What I want to do with the 75M rows is some processing and then adding the result into another database . Now , this is quite a lot of data , and I 've had success with mainly two approaches:1 ) Exporting the query to a CSV file using the `` SELECT ... INTO '' capabilities of MySQL and using the fileinput module of python to read it , and2 ) connecting to the MySQL database using MySQLdb 's SScursor ( the default cursor puts the query in the memory , killing the python script ) and fetch the results in chunks of about 10k rows ( which is the chunk size I 've found to be the fastest ) .The first approach is a SQL query executed `` by hand '' ( takes about 6 minutes ) followed by a python script reading the csv-file and processing it . The reason I use fileinput to read the file is that fileinput does n't load the whole file into the memory from the beginning , and works well with larger files . Just traversing the file ( reading every line in the file and calling pass ) takes about 80 seconds , that is 1M rows/s.The second approach is a python script executing the same query ( also takes about 6 minutes , or slightly longer ) and then a while-loop fetching chunks of rows for as long as there is any left in the SScursor . Here , just reading the lines ( fetching one chunk after another and not doing anything else ) takes about 15 minutes , or approximately 85k rows/s.The two numbers ( rows/s ) above are perhaps not really comparable , but when benchmarking the two approaches in my application , the first one takes about 20 minutes ( of which about five is MySQL dumping into a CSV file ) , and the second one takes about 35 minutes ( of which about five minutes is the query being executed ) . This means that dumping and reading to/from a CSV file is about twice as fast as using an SScursor directly.This would be no problem , if it did not restrict the portability of my system : a `` SELECT ... INTO '' statement requires MySQL to have writing privileges , and I suspect that is is not as safe as using cursors . On the other hand , 15 minutes ( and growing , as the source database grows ) is not really something I can spare on every build.So , am I missing something ? Is there any known reason for SScursor to be so much slower than dumping/reading to/from a CSV file , such that fileinput is C optimized where SScursor is not ? Any ideas on how to proceed with this problem ? Anything to test ? I would belive that SScursor could be as fast as the first approach , but after reading all I can find about the matter , I 'm stumped.Now , to the code : Not that I think the query is of any problem ( it 's as fast as I can ask for and takes similar time in both approaches ) , but here it is for the sake of completeness : The primary code in the first approach is something like thisThe primary code in the second approach is : At last , I 'm on Ubuntu 13.04 with MySQL server 5.5.31 . I use Python 2.7.4 with MySQLdb 1.2.3 . Thank you for staying with me this long !"
"I have a picture were I want to change all white-ish pixels to grey , but only for a certain area of the image . Example picture , I just want to change the picture outside of the red rectangle , without changing the image within the red rectangle : I already have the general code , which was part of someone elses Stackoverflow question , that changes the colour of every white pixel instead of only just the one outside of an area ."
"I have the following structure for my Python package : Buf after performing python setup.py build , the innermost test directory is not getting copied : On the contrary , python setup.py sdist works correctly.So far I 've used the MANIFEST.in rules to include or exclude certain files , patterns and directories from the sdist . Is there a way to control what goes to the build directory ? Why some tests are getting there and some others are n't ? Reference to original issue and source code : https : //github.com/poliastro/poliastro/issues/129"
I 'm looking to split a string Series at different points depending on the length of certain substrings : My output should look like : I half-thought this might work : How can I slice my strings by the variable locations in split_locations ?
I would think that if i did the following code in pythonthen var would be a list with the values 0 - 9 in it.What gives ?
"I 'm using Python 2.7 along with a python-slackclient . I have an attachment structure like so : then , However , when this posts , it just posts the plaintext , with no formatting : { `` attachments '' : [ { `` title '' : `` Upgrade Grafana to 3.0 '' , `` color '' : `` # 7CD197 '' , `` text '' : `` I\u2019ve added the JIRA maillist so this emailwill create a ticket we can queue it up in support.\u00a0 Eric if youwouldn\u2019t mind just replying to this email with the additionalinfo ? \n\n\u00a0\n\n\u00a0\n\nSent : Thursday , August25 , 2016 11:41 AM\n '' , `` title_link '' : '' https : //jira.jr.com/browse/ops-164 '' , `` mrkdwn_in '' : [ `` text '' , '' pretext '' , `` fields '' ] , `` pretext '' : `` Detail summary for ops-164 '' , '' fallback '' : `` Upgrade Grafana to 3.0 , https : //jira.jr.com/browse/ops-164 '' } ] } What am I doing wrong ? I 've tried also doing attachments=self.msg in the Send ( ) call , but I get no output at all to my slack channel when doing that ."
"My question is : where do these patterns ( below ) originate ? I learned ( somewhere ) that Python has unique `` copies '' , if that 's the right word , for small integers . For example : When I look at the memory locations of ints , there is a simple pattern early : It 's not clear to me why this is chosen as the increment . Moreover , I ca n't explain this pattern at all above 256 : Any thoughts , clues , places to look ? Edit : and what 's special about 24 ? Update : The standard library has sys.getsizeof ( ) which returns 24 when I call it with 1 as argument . That 's a lot of bytes , but on a 64-bit machine , we have 8 bytes each for the type , the value and the ref count . Also , see here , and the C API reference here.Spent some time with the `` source '' in the link from Peter Hansen in comments . Could n't find the definition of an int ( beyond a declaration of *int_int ) , but I did find :"
"So I 'm pretty upset I ca n't figure something this seemingly trivial out as I 'm fairly well versed in Java , but anyways my professor for introduction to Python assigned us a lab where we have to create a pattern with letters based on row and column position . No loops or iterations , just conditional statements.For instance , this function : would yield : if run through his driver file with row and col both equaling 20.The one I 'm stuck with is creating a function for the pattern : Please do NOT spoonfeed me the answer , rather point me in the right direction.So far I know that the X 's for the left- > right diagonal can be identified when row==col . It 's the right- > left diagonal I 'm having issues with . Thanks a lot ."
"I 'm writing custom django commands under my apps management/commands directory . At the moment I have 6 different files in that directory . Each file has a different command that solves a unique need . However , there are some utilities that are common to all of them . What is the best way to abstract away this common code ? Below is an example : load_colorsload_shades"
"I am new to luigi , came across it while designing a pipeline for our ML efforts . Though it was n't fitted to my particular use case it had so many extra features I decided to make it fit.Basically what I was looking for was a way to be able to persist a custom built pipeline and thus have its results repeatable and easier to deploy , after reading most of the online tutorials I tried to implement my serialization using the existing luigi.cfg configuration and command line mechanisms and it might have sufficed for the tasks ' parameters but it provided no way of serializing the DAG connectivity of my pipeline , so I decided to have a WrapperTask which received a json config file that would then create all the task instances and connect all the input output channels of the luigi tasks ( do all the plumbing ) .I hereby enclose a small test program for your scrutiny : So , basically , as is stated in the question 's title , this focuses on the dynamic dependencies and generates a 513 node dependency DAG with p=1/35 connectivity probability , it also defines the All ( as in make all ) class as a WrapperTask that requires all nodes to be built for it to be considered done ( I have a version which only connects it to heads of connected DAG components but I did n't want to over complicate ) .Is there a more standard ( Luigic ) way of implementing this ? Especially note the not so pretty complication with the TaskNode init and set_required methods , I only did it this way because receiving parameters in the init method clashes somehow with the way luigi registers parameters . I also tried several other ways but this was basically the most decent one ( that worked ) If there is n't a standard way I 'd still love to hear any insights you have on the way I plan to go before I finish implementing the framework ."
"How can I reverse the sign in the quantity column whenever the side column has a sell ? All other values should not be changed . The following is simply not working , it has no effect ."
"I want to filter the nested field with is_active column as True in Marshmallow 3Consider following scenarioI have 3 tablesNow I 'm trying to print all the organization with its members who are active . ( There are some active and inactive members ) I have following schema Now in my action following is the codeFollowing is OutputI want output with member who has 'is_active ' : True as followsMarshmallow provides a decorator @ post_dump . Problem here is Query brings all data and then we filter it with decorator @ post_dump . But the flow should be like , while querying there should be some way to filter the data and not post query filtering ."
I 'm trying to solve this problem in hackerrank . At some point I have to check if a number divides n ( given input ) or not.This code works perfectly well except one test case ( not an issue ) : Now you can see that I 'm dividing the number only when i is a factor of n.For example if the numbers be i = 2 and n = 4 then n / 2 and n // 2 does n't make any difference right.But when I use the below code all test cases are getting failed : This is not the first time.Even for this problem I faced the same thing.For this problem I have to only divide by 2 so I used right shift operator to get rid of this.But here I ca n't do any thing since right shift ca n't help me.Why is this happening ? If the numbers are small I ca n't see any difference but as the number becomes larger it is somehow behaving differently.It is not even intuitive to use // when / fails . What is the reason for this ?
"I have googled this for very long time but with no results . I ` m beginner to Django so I do n't know all features it have . But this problem is very important for client : - ( Could you help me , please ? So , I have this model defined : Now , because of czech language , I need these written in admin list:0 výrobků1 výrobek2-4 výrobky5- výrobkůEverywhere else , I 'm using ungettext sucessfully . However , I do n't know , how to get count in Meta . I have found this as abstract ( but seems to be useless ) : Source is from django internationalization : counter is not available when marking strings for pluralizationMaybe , at the end would be fine to show language definition ( tried to add/remove % s from msgid ) : If you need more info for question , sure I will provide it.Thank you a lot in advance.UPDATEPlease , be sure , that I 'm using following in the .po file : Once more , anywhere else but admin models , IT IS working . It 's quetion not how to run multi pluralization in general , but how to change anything in admin ( e.g . new abstract model etc . ) to run it there ..."
"I need an algorithm which given a list L and a number N , returns a list of N smaller lists where the sublists are `` balanced '' . Examples : As you can see , the algorithm should `` prefer '' the first list in the output.I 've been trying for hours , but I ca n't figure out a nice and terse algorithm for it . This will be implemented in Python , by the way , but it 's really the algorithm that I 'm after here . This is not homework , this is for a website which will display contents in a list in three columns ( Django ) .I got the best answer from # python on freenode and it is as follows : Do n't ask me why it works though . : ) I 'll give the correct answer to the one with most votes though ."
"I 'm new to coding and am trying to write a simple code that will take a list , say [ 1,2,3 ] and cycle the elements n number of times . So if n=1 , I should get A= [ 3,1,2 ] . If n=2 , I should get A= [ 2,3,1 ] .The code I have written is : The problem is that no matter what the value of n is I get the same answer which is only cycled once . I think the problem is that the loop is cycling through the same B every time , so I need to store the new B as something else and then repeat the loop with new B . But I ca n't figure out how to do that . Any tips would be appreciated"
"When trying to install the python wrapper of kenlm from pip within an anaconda environment , I get the error : The pip command works outside a conda environment , but then kenlm is not active within the environment . I was also able to run this from an AWS ec2 instance running linux , so maybe it 's a Mac OSX issue . Any idea how can this be solved ?"
"I have a Python 2 project ( 'foo 0.1.7 ' ) that required Python 2.4 or later.Now I ported it to Python 3 ( 'foo 0.2.0 ' ) in a way that it still is compatible with Python 2 , but the requirements are now lifted to Python 2.6 or later.I know that there is a -- target-version=2.6 option for setup.py , that can be used with upload , but that seems not to be meant as ' 2.6 or higher'The setup command has an install_requires option , but this is for required packages , .not Python interpreter.I could do something like this in setup.py of 'foo 0.2.0 ' : but I would prefer if easy_install foo would resolve this in some way.So , how should I deploy this on PyPI ?"
"I am trying to understand the Python `` ctypes '' module . I have put together a trivial example that -- ideally -- wraps the statvfs ( ) function call . The code looks like this : Running this invariably returns : I have n't been able to find any examples of calling functions with complex types as parameters ( there are lots of examples of functions that return complex types ) , but after staring at the ctypes documentation for a day or so I think my calling syntax is correct ... and it is actually callling the statvfs ( ) call and getting back correct results.Am I misunderstanding the ctypes docs ? Or is something else going on here ? Thanks !"
"The following appears in my Python 2.6 code : Can I do much better ? What I particularly do n't like is that I 'm in effect specifying the same pair twice , once for the for loop and again for the generator expression . I 'm uncertain whether I 'd prefer : Is there a way to express this loop concisely ? universe happens to be a list , if it makes any difference . Order of iteration does n't matter ."
"I am trying to change the directory creation timestamp for a windows system using python . I have a directory that was copied over from another drive and the directory creation times are not preserved . This is what I am hoping to do , Step 1 : Read the source directory list and thier creation times with the following code , Step 2 : Iterate over the directory list and set the Directory Creation Time . For this I am relyting on Python For Windows Extensions . Code is shown below , I am getting an 'Access is Denied ' at the CreateFile line . I am not sure if this a valid way to setting creation time for the directory . Is this the right way set directory creation times"
"I have a large DataFrame that looks something like this : df = Basically I am trying to record how the sales of a product ( UPC ) reacted once the price changed for the following 7 days . I want to create a new column [ 'Reaction ' ] which records the sum of the unit sales from the day of price change , and 7 days forward . Keep in mind , sometimes a UPC has more than 2 price changes , so I want a different sum for each price change . So I want to see this : What is difficult is how the dates are set up in my data . Sometimes ( like for UPC 35 ) the dates do n't range past 7 days . So I would want it to default to the next nearest date , or however many dates there are ( if there are less than 7 days ) . Here 's what I 've tried : I set the date to a datetime and I 'm thinking of counting days by .days method . This is how I 'm thinking of setting a code up ( rough draft ) : Is there an easier way to do this , maybe without a for loop ?"
On python 3.6 and 3.7 ( django 2.2 ) I am getting django.core.exceptions.ImproperlyConfigured : SQLite 3.8.3 or later is required ( found 3.8.2 ) .On TravisCIWhat I DidI tried installing it with addons : The sources here got ignoredAnd with ppa in before_installerror : How can I get it to work ? Sourceshttps : //github.com/travis-ci/apt-package-safelist/issues/368 # issuecomment-198586476
"I 'm trying to package a new project and upload it to the TestPyPI server . Because of the introduction of the new PyPI.org , virtually all the instructions I 've found are out of date , and trying to upload to https : //testpypi.python.org/pypi now returns 401 Gone.The guide for migrating to the new PyPI.org says to use https : //test.pypi.org/legacy as the repository for TestPyPI . However , when inserting this URL into my .pypirc file and trying to upload my package , I receive the following error : Here are the contents of my ~/.pypirc file :"
"I had the following question in an interview and I believe I gave a working implementation but I was wondering if there was a better implementation that was quicker , or just a trick I missed.Given 3 unsigned 30-bit integers , return the number of 30-bit integers that when compared with any of the original numbers have the same position bits set to 1 . That is we enumerate all the 0sLet me give an example , but lets use 4bit for clarity.Given : It should return 8 since there are 8 4bit ints that are in the set . The set being : Now how I worked it out was to take each number and enumerate the set of possibilites and then count all the distinct values . How I enumerated the set was to start with the number , add one to it and then OR it with itself until I reached the mask . With the number itself being in the set and the mask ( being all set to 1 ) also in the set . So for example to enumerate the set of 1001 : So do that for each number and then count the uniques.This is it in python code ( but language does n't really matter as long as you can do bitwise operations , hence why this question is tagged for c/c++ as well ) : Now this works and gives the correct answer but I 'm wondering if I have missed a trick . Since the question was to only ask the count and not enumerate all the values perhaps there is a much quicker way . Either by combining the numbers first , or getting a count without enumeration . I have a feeling there is . Since numbers that contain lots of zeros the enumeration rises exponentially and it can take quite a while . If you have A B and C , the count of the set of numbers which has bits set to 1 where A or B or C has corresponding bits set to 1.Some people do n't understand the question ( did n't help that I did n't ask it correctly first of ) . Let 's use the given A B and C values above : A : B : C : Now combine those sets and count the distinct entries . That is the answer . Is there a way to do this without enumerating the values ? edit : Sorry for the mistake the question . Fixed now ."
"I set numpy random seed at the beginning of my program . During the program execution I run a function multiple times using multiprocessing.Process . The function uses numpy random functions to draw random numbers . The problem is that Process gets a copy of the current environment . Therefore , each process is running independently and they all start with the same random seed as the parent environment . So my question is how can I share the random state of numpy in the parent environment with the child process environment ? Just note that I want to use Process for my work and need to use a separate class and do import numpy in that class separately . I tried using multiprocessing.Manager to share the random state but it seems that things do not work as expected and I always get the same results . Also , it does not matter if I move the for loop inside drawNumpySamples or leave it in main.py ; I still can not get different numbers and the random state is always the same . Here 's a simplified version of my code : And in the main file : Note : I use Python 3.5 . I also posted an issue on Numpy 's GitHub page . Just sending the issue link here for future reference ."
"I 'm trying to set up CI for some PL/Python PostgreSQL procedures in Travis CI.I 've tried several ways:1 ) With the legacy infrastructure I 've tried to just assume , that PL/Python is already installed , but it had not succeed:2 ) Have tried to add sudo apt-get update & & sudo apt-get -y install postgresql-plpython-9.4 commands in the beginning . And it was also failed , because this command initiated replacement of PostgresSQL 9.4 , that comes already installed in the Travis environment . Travis build.3 ) Also tried to use container-based infrastructure with this lines in the config : No success too.What is the good way to test PL/Python procedure in Travis CI ?"
"ProblemUsing this answer to create a segmentation program , it is counting the objects incorrectly . I noticed that alone objects are being ignored or poor imaging acquisition.I counted 123 objects and the program returns 117 , as can be seen , bellow . The objects circled in red seem to be missing : Using the following image from a 720p webcam : CodeQuestionHow to count the missing objects ?"
"For instance , the number 24 has prime factorization 2^3*3^1 , and can be written in the following waysI may have missed one but you get the idea.I tried looking into the other thread How to find multiplicative partitions of any integer ? but could n't understand the answers in all honesty.I do n't need anyone to write code for me but rather I could really use some help creating an efficient algorithm for this ( probably something recursive ? ) . I am coding in Python ."
When using the python struct module on can specify a format string that declares how binary data should be interpreted : It is easily possible to calculate the amount of bytes needed to store an instance of that format : What would be the best way to retrieve the number of variables need to 'fill ' a format ? Basically this would tell in advance how big the 'values ' array should be to perform the pack ( ) in the above example.Is there such a thing ?
"I 've been working on my first robot for google wave recently , a vital part of what it does is to insert inline replies into a blip . I ca n't for the life of me figure out how to do this ! The API docs have a function InsertInlineBlip which sounded promising , however calling that does n't appear to do anything ! EDIT : :It seems that this is a known bug . However , the question still stands what is the correct way to insert an inline blip ? I 'm assuming something like this :"
"I have the following two DataFrames : I keep a running count of the `` above '' and `` below '' values in the history DataFrame like so : This works so long as there are no extra columns in the current DataFrame . However when I add an extra column : I get the following : I want this extra column to be ignored , since it 's not present in both DataFrames . The desired output is just :"
"Looking for a elegant way to convert a list of substrings and the text between them to key , value pairs in a dict . Example : This is solvable using str.find ( ) , etc , but I know there 's a better solution than what I 've hacked together ."
The above code is used to create all paths of certain length in a graph.map [ a ] represents the points you can reach from point a.How can I change it to simulate having an arbitrary number of loops ? This is like a cartesian product ( itertools.product ) where at each iterationyour choice for the next element is limited to those in map [ current_point ] .
"I am aware that when using MyModel.objects.create in Django , it is possible to pass in a dictionary with keys which correspond to the model fields in MyModel . This is explained in another question here : Can a dictionary be passed to django models on create ? However , I am trying to pass in a dictionary which has more keys than there are model fields - in other words , some of the keys are not used in the creation of the object . Is it possible to do this in some way ? For example : When I try this , I get an error telling me that `` 'key3 ' is an invalid keyword argument for this function '' . Am I passing the dictionary in incorrectly ? Is there a different way to pass it to the model that means the model does n't have to use all of the arguments ? Or will I simply have to specify each field manually like this :"
"The continuation of my PyParsing quest is parsing nested ternary expressions ( e.g . ( x == 1 ? true : ( y == 10 ? 100 : 200 ) ) ) . As such , I constructed the following expression . Which , to me , seems intuitive . However I get no matches : Initially I thought that the problem was with printables consuming everything ; I set the excludeChars to not match : ? ) ( , but that did n't help either . The alternative was to construct nested expressions , one each for the `` ( ? `` , `` ? : `` and `` : ) '' blocks . But this approach is very messy . Does anybody have any recommendations on parsing ternary expressions ? UPDATEUsing the answer from below , but modified to work with scanString : When using scanString however , it returns a lot of other matches too ( basically , anything matching atom ) ."
"From plotly doc : layout > xaxis > tickvals : Sets the values at which ticks on this axis appear . Only has an effect if tickmode is set to `` array '' . Used with ticktext . layout > xaxis > ticktext : Sets the text displayed at the ticks position via tickvals . Only has an effect if tickmode is set to `` array '' . Used with tickvals.Example : Result : Since length of x_dates [ : :2 ] is 50 , the ticknumber does n't match at all .How do I sovle it ? ?"
Hi . how can i avoid calling the function twice without using an extra variable .
"I would like to find the index of the largest value in a 1D numpy array that is not infinity . I 've tried argmax , but when there is an infinity value in my array it just return that index . The code that I came up with seems quite hacky and unsafe . Is there a better solution ?"
"Here 's a dataframe : I could retrieve a column which is basically a tuple of columns from the original df using df.apply : But if I want a list of values instead of a tuple of them , I ca n't do it , because it does n't give me what I expect : Instead , I need to do : Why ca n't I use df.apply ( list , 1 ) to get what I want ? AppendixTimings of some possible workarounds :"
"I created this simple project in Gitlab : https : //gitlab.com/PequeX/deletemeWith a Pipfile that installs only a couple of packages : And a very simple .gitlab-ci.yml file : There is also the autogenerated Pipfile.lock file . Which I will not paste here as it is not very interesting.Now , the problem is Gitlab seems to get blocked when calling pipenv sync : https : //gitlab.com/PequeX/deleteme/-/jobs/171273142There is no output printed on the log and no error . It just gets blocked forever . And then , eventually , it timeouts ( because Gitlab would not let you run a pipeline forever ) .What is wrong or how could I successfully run pipenv sync ? Note that I would like to keep using the same peque/python-devel image from Docker Hub , as I need to have multiple Python versions installed for my pipelines.UpdateTried unsetting the CI variable as @ Hernan Garcia suggests in his answer , with no luck : BranchError ( does not find package ) I also tried with pipenv shell as @ Hernan Garcia suggests in his comments , with no luck : BranchError ( pipenv shell crashes )"
"I 'm trying to use the DataFrame.hint ( ) method to add a Range Join hint to my join.I have two tables : minutes and events.The minutes table has the minute_start and minute_end columns that are time in seconds since a fixed moment in time . Naturally , their values are multiples of 60.The events table has similar event_start and event_end columns , only for events . Events can start and end at any second.For each event , I need to find all the minutes it overlaps with.I 'm trying this on Databricks ( runtime 5.1 , Python 3.5 ) : Without the hint call , the result is as expected : With the hint , I get the exception : When I tried passing the 60 in the hint as a number as opposed to a string , it complained that a parameter of a hint must be a string.I 'm not on Azure , but I expect the outcome would be the same.Has anyone had a similar issue and found a solution or knows where I 'm making a mistake ? UPDATE 1 ( Currently , I 'm trying it on Databricks Runtime 6.1 , Python 3.7.3 , Spark 2.4.4 ) I thought I missed that the parameters are expected as an iterable , so I tried again , with events.hint ( `` range_join '' , [ 60 ] ) . Same complaint about the argument not being a string : TypeError : all parameters should be str , got 60 of type < class 'int ' > .I 'm wondering if Databricks ' version of Spark is behind.This is in Spark source code on GitHub : so a list of ints should be allowed.What I 'm getting is all parameters should be str , but the GitHub version would say all parameters should be in ( basestring , list , float , int ) if I passed a parameter of a wrong type.UPDATE 2hint ( `` skew '' , `` col_name '' ) appears to be working ."
"I 'm trying to wrap a parallel sort written in c++ as a template , to use it with numpy arrays of any numeric type . I 'm trying to use Cython to do this . My problem is that I do n't know how to pass a pointer to the numpy array data ( of a correct type ) to a c++ template . I believe I should use fused dtypes for this , but I do n't quite understand how . The code in .pyx file is below In the past I did similar tasks with ugly for-loops over all possible dtypes , but I believe there should be a better way to do this ."
Here is my program of cythonCython compiling result : But I can use push_back ( ) to append values at the end of the vector or use int instead of Node*.What is wrong ?
"I 'm working on a dataset from a MOOC . I have a lot of python3 code snippets that I need to run and get the results from . To do this I 've written a python script that loops over each snippet . For each snippet I : Create new StringIO objectsSet sys.stdout and sys.stderr to my stringIO buffersExecute the code snippet in a threading.thread objectJoin the threadLog the results in the stringIO buffersRestore stdout and stderrThis works fine for `` correct '' code , but this has issues in other cases : When the code has an infinite loop , thread.join does n't kill the thread . The thread is a daemon thread , so it runs quietly in the background until my loop finishes.When the code has an infinite loop with a print ( ) , the thread starts overwriting my actual stdout when I set it back to the default ( away from the StringIO buffer ) . This pollutes my reporting.Here is my current code : To handle this case , I 've been trying to use multithreading.Process and/or contextlib.redirect_stdout to run the code in a process ( then I can call process.terminate ( ) ) , but I 'm not having any success capturing stdout/stderr.So my question is : How can I redirect or capture stdout/stderr from a process ? Alternatively , is there some other way I could go about trying to run and capture the output of arbitrary code ? ( And yes , I know this is a bad idea in general ; I 'm running it in a virtual machine just in case there is malicious code in there somewhere ) Python version is 3.5.3UpdateIt occurs to me that there is a little more flexibility in this situation . I have a function , preprocess ( code ) that accepts a the code submission as a string and alters it . Mostly I 've been using it to swap out the value of some variables using regular expressions.Here is an example implementation : I could use the preprocess function to help redirect STDOUT"
"I 'm using Cython and distutils . During one of my tests , a binary extension module is built . After this , one of my other tests will import the binary module and test it . On Travis-CI , the linker command changes the name . See in the screenshot below : the compiler correctly builds test01.o , but the linker incorrectly creates test01.cpython-34m.so : I want test01.so.The above screenshot is for Python 3.4 ; the problem does n't happen for Python 2.7 , leading me to suspect that the Python-specific distutils may be doing something differently.The distutils setup ( ) function , which is called from my main script , does nothing special : Here ext_modules is a sequence of Extension ( ) classes , imported from distutils.extension.I have gone throught the distutils docs for 2.7 and 3.4 and nothing jumps out at me as different . What could be causing the name change , and what options could I specify , either in setup ( ) or Extension ( ) instantiation to prevent the name change ? Edit : I have set up a local VM running Ubuntu 13.10 and can confirm that , as described above , Python 2.7 produces test01.so while Python 3.4 produces test01.cpython-34m.so . Therefore this issue has nothing to do with travis-ci and I am removing that tag and editing the title.Edit : Indeed it is a change to distutils , made in 3.2 . I 'm still figuring out what I can do to get my import statement to look right.Edit : OMG the helloworld example in the python docs produces helloworld.cpython-34m.so on Python 3.4.1 ( Anaconda ) on Ubuntu 13.10 . Python 's sysconfig module starts up with sysconfig.get_config_var ( 'SO ' ) == 'cpython-34m.so'.I feel a rant coming on ."
What is the reason for this weirdness in numpy 's all ?
I am trying to split a word into subwords - all possible permutations . I hope I covered all possbile ways to arrange 'Bang ' . I thought long about it but could not find a way.Permutations method does not return the entire word . I can split into 2 words but unable to split a word into 3 or more ( for a bigger word ) .Splitting into 2 words can be done using the below code which was suggested by one of the members .
"EDIT : See below for a suggested answer and how it 's not quite right yet.There are many similar questions to this one on Stack Overflow , but none exactly like it in Python . I 'm a programming novice , so please go easy.I have a tree of nested dictionaries , like this : I want to flatten all paths from top to bottom and end up with this : I made a lovely little recursive function that created the original structure in the first place , but I 'm having a hard time unrecursivizing it . This is as far as I got : …which returns this : …which is clearly somewhat close , but not quite right.I know that function is probably horribly un-Pythonic , but I 'm teaching myself programming , so I 'm not even trying to take advantage of possibly-existent language features that would let me elide over thinking through this stuff from scratch ( ” he said , posting to a Q & A site in the hope its members would help him elide a bit of thought ) .So : what am I doing wrong ? EDIT : Moshe below corrected a couple of problems : This is closer yet , but not quite right :"
"I have seen something like this a few times in the Pandas source : This implies that a NumPy ndarray called mat is type-casted to a Python object . * Upon further inspection , it seems like this is used because a compile error arises if it is n't . My question is : why is this type-cast required in the first place ? Here are a few examples . This answer suggest simply that tuple packing does n't work in Cython like it does in Python -- -but it does n't seem to be a tuple unpacking issue . ( It is a fine answer regardless , and I do n't mean to pick on it . ) Take the following script , shape.pyx . It will fail at compile time with `` Can not convert 'npy_intp * ' to Python object . `` But again , the issue does not seem to be tuple unpacking , per se . This will fail with the same error.Seemingly , no tuple unpacking is happening here . A tuple is the first arg to np.zeros.This also works ( perhaps the most confusing of all ) : Example : *Perhaps it has something to do with arr being a memoryview ? But that 's a shot in the dark.Another example is in the Cython docs : In this case , simply indexing arr.shape [ i ] prevents the error , which I find strange.This also works :"
"I have a numpy array A such thatNow I want to construct two slices B and C of A by selecting the indices 0 , .. , n-1 and 1 , ... , n respectively along the axis axis . Thusand B and C have the same size as A along the other axes . There must be no copying of data ."
"When I run the code , label1 stretches perfectly along the X axis while label2 does n't stretch at all along the Y axis . What am I missing ? Result"
"I have a class that has lookup dict in the class body , which I store all the instances and with some key . When I instantiate the instances I do n't hold them in any variable or external dict , I store them in this lookup dict.When somehow I instantiate an instance that is already in the dict , I reassign it to the one in the dict and update it , using it 's new and old value in some other function.I wonder if this is a good practice ? Or I should refactor and make it have a external dict that hold the instances.Is there any PEP guide to this kind of behavior ? Example : The databases where I get the ids and values is changing every time , so I ca n't know for sure if I 'll get the ones I have already instantiate , or the values will be different.The way I see , is that all the functionality of the class happens within itself , no need to have dictionaries laying around , making it unclear where they are interacting with any other part of the code.This is how I would normally do , without reassigning it :"
"Why ca n't I raise an Exception instance after I catch that Exception class ? Oddly enough , I run into this error when I run the script from a function but not when ran directly in the python shell.Expected error here :"
"Is there a way to perform a roll on an array , but instead of having a copy of the data having just a different visualisation of it ? An example might clarify : given b a rolled version of a ... ... if I perform an assignment on array b ... ... the content of a wo n't change ... ... contrarily , I would like to have : Thanks in advance for your time and expertise !"
"I 'm not very familiar with Python . So I have some problem while I code.It 's very normal to use the function name in the function block , for example : But when I try to do use the class name in the class block , things go wrong : Although the code below is okay : Then I debug these two codes using print globals ( ) statement . I found that the global variable dict in class block does n't contain class Foo while the global variable dict in __init__ function block contains it.So it seems that the class name binding is after the execution of class block and before the execution of function block.But I do n't like guesswork in foundation area of coding . Could anyone offer a better explanation or official material about this ?"
"I 'm using python with numpy , scipy and matplotlib for data evaluation . As results I obtain averages and fitting parameters with errorbars.I would like python to automatically pretty-print this data according to a given precision . For example : Suppose I got the result x = 0.012345 +/- 0.000123.Is there a way to automatically format this as 1.235 ( 12 ) x 10^-2 when a precision of 2 was specified . That is , counting the precision in the errorbar , rather than in the value.Does anyone know a package that provides such functionality , or would I have to implement this myself ? Is there a way to inject this into the python string formatting mechanism ? I.e . being able to write something like `` % .2N '' % ( 0.012345 , 0.0000123 ) .I already looked through the docs of numpy and scipy and googled around , but I could n't find anything . I think this would be a useful feature for everyone who deals with statistics.Thanks for your help ! EDIT : As requested by Nathan Whitehead I 'll give a few examples.The powers of ten are omitted for clarity.The number inside the parenthesis is a shorthand notation for the standard error . The last digit of the number before the parens and the last digit of the number inside the parens have to be at the same decimal power . For some reason I can not find a good explanation of this concept online . Only thing I got is this German Wikpedia article here . However , it is a quite common and very handy notation.EDIT2 : I implemented the shorthand notation thing myself : Output :"
"Is Sphinx , is there way to automatically link text like # 112 or r1023 to the corresponding tickets/changesets in Trac ? For eg : See TracLinks for more examples ."
"Since python 2.5 there is the ability to send ( ) , throw ( ) , close ( ) into a generator . Inside the defined generator one can 'catch ' the sent data by doing something like : What i am trying to play with is doing something like : Noticed that it is a legal generator which does something.. First thing i 'm trying to figure out is : Is there a good usage for such writing ? Also when doing something like : Why each second 'send ' does not do anything ?"
"I 'm trying to convert python math expressions to postfix notation using the AST python module . Here 's what I got so far : I 'm trying to understand how to convert complex infix math expressions to postfix expressions to be sent to a swig c wrapper , to do that I 'm trying to use the AST module . Could anyone advice here ?"
"I 'm making something like a task scheduler using generators as coroutines . In the below code , I need to execute the print cleanup deterministically.It seems from my interaction that releasing the object to the garbage collector causes the context manager to exit . But , I know better than to rely on the timing of a GC . Is it really the GC calling the __exit__ or is it another mechanism ? How can I strictly force print 'cleanup ' ?"
"I 've actually never encountered this error before : sqlalchemy.exc.InvalidRequestError : stale association proxy , parent object has gone out of scopeAfter doing some research , it looks like its because the parent object is being garbage collected while the association proxy is working . Fantastic.However , I 'm not sure where it 's happening.Relevant code :"
"I have a list of tuples which I create dynamically.The list appears as : Each tuple ( a , b ) of list represents the range of indexes from a certain table.The ranges ( a , b ) and ( b , d ) is same in my situation as ( a , d ) I want to merge the tuples where the 2nd element matches the first of any other.So , in the example above , I want to merge ( 8 , 10 ) , ( 10,13 ) to obtain ( 8,13 ) and remove ( 8 , 10 ) , ( 10,13 ) ( 19,25 ) and ( 25,30 ) merge should yield ( 19 , 30 ) I do n't have a clue where to start . The tuples are non overlapping.Edit : I have been trying to just avoid any kind of for loop as I have a pretty large list"
"EDIT : Ok , if the data are two dimensional as follows : Then , how to calculate the k means ( 3 values ) and make plot ? Can not the computed centroid values be plotted over the existing plot based on data here ? I want to make the similiar plot as done in the following linkhttp : //glowingpython.blogspot.jp/2012/04/k-means-clustering-with-scipy.htmlHowever , I could not understand . Any help would be highly appreciated ."
"I have a variable , x , that is of the shape ( 2,2,50,100 ) . I also have an array , y , that equals np.array ( [ 0,10,20 ] ) . A weird thing happens when I index x [ 0 , : , : , y ] . Why does the last one output ( 3,2,50 ) and not ( 2,50,3 ) ?"
"The question : I 'm trying to grasp the concept of monkey patching and at the same time make a function to produce the perfect time-series plot . How can I include the following matplotlib functionality in pandas pandas.DataFrame.plot ( ) ? complete code at the end of the questionThe details : I think the default settings in df.plot ( ) is pretty neat , especially if you 're running a Jupyter Notebook with a dark theme like chesterish from dunovank : And I 'd like to use it for as much of my data analysis workflow as possible , but I 'd really like to remove the frame ( or what 's called spines ) like this : Arguably , this is a perfect time-series plot . But df.plot ( ) does n't have a built-in argument for this . The closest thing seems to be grid = False , but that takes away the whole grid in the same run : What I 've triedI know I can wrap the spine snippet in a function along with df.plot ( ) so I end up with this : Snippet 1 : Output 1 : But is that the `` best '' way to do it with regards to flexibilty and readability for future amendments ? Or even the fastest with regards to execution time if we 're talking hundreds of plots ? I know how I can get the df.plot ( ) source , but everything from there leaves me baffled . So how do I include those settings in df.plot ? And perhaps the wrapped function approach is just as good as monkey patching ? Snippet with full code and sample data : To reproduce the example 100 % , paste this into a Jupyter Notebook cell with the chesterish theme activated :"
"I have a list and I want to generate a dictionary d taking out duplicates and excluding a single item , such that the first key has value 0 , the second has value 1 , and so on.I have written the following code : If a_list = [ ' a ' , ' b ' , ' < ' , ' c ' , ' b ' , 'd ' ] , after running the code d contains { ' a ' : 0 , ' b ' : 1 , ' c ' : 2 , 'd':3 } . Order is not important.Is there a more elegant way to obtain the same result ?"
"The following code does n't work , I assume because the locals ( ) variable inside the comprehension will refer to the nested block where comprehension is evaluated : I could use globals ( ) instead , and it seems to work , but that may come with some additional problems ( e.g. , if there was a variable from a surrounding scope that happens to have the same name ) .Is there anything that would make the dictionary using the variables precisely in the scope of function f ? Note : I am doing this because I have many variables that I 'd like to put in a dictionary later , but do n't want to complicate the code by writing dict_ [ ' a ' ] instead of a in the meantime ."
"I am thinking to make a progress bar with python in terminal . First , I have to get the width ( columns ) of terminal window . In python 2.7 , there is no standard library can do this on Windows . I know maybe I have to call Windows Console API manually.According to MSDN and Python Documentation , I wrote the following code : It works fine . I set about deleting some print in code . But after that , it does n't work ! GetLastError return 6 ( Invalid Handle ) . After times of trying , I found that there must be SOMETHING at the pointed position of the code such as print 'hello ' , import sys or sys.stdout.flush ( ) . At first , I guess that maybe it need time to do something . So I tried to put time.sleep ( 2 ) at that position , but it still does n't work.But , if I do use struct instead of ctypes.Structure , there 's no such problem.Is there any one can tell me why the irrelevant code made such a difference ?"
"After Thomas very helpfully fixed my issues making two parasite sub plots in this question , I 'm now trying to rotate the x axis labels on the subplots.Unfortunately , my modification to the example code here seems to have no effect on the x axis labels : gives un-rotated x axis labels in : Although I 've shown the plt.xticks ( rotation = 45 ) , I 've also tried other ways that work with `` conventional '' matplotlib plots without success . Does anyone know if there is a way to do this , or am I just dealing with too much of a niche case ? Maybe I should just figure out a way to live with using sub plots and no parasite axes ? Thanks a lot , Alex"
"I recently wrote a sudoku solver in C to practice programming . After completing it I decided to write an equivalent program in Python for a comparison between the languages and more practice and this is where the problem is . It seems to be that a global variable ( sudokupossibilities [ ] [ ] [ ] ) I declared outside the while loop is n't available within the loop . I 've tried adding print statements for debugging and it seems that it 's set correctly ( all ones ) outside of the while loop , but once it enters the loop , the values are mostly zeros , with a few ones . The only way I 've found to fix this is adding a statement after `` for k in range ( 9 ) : '' setting it to a one there - which makes the following statement obsolete and slowing the program . Ive included the source code for the Python version below and the C version after it . C version : I 'm using Python 3.1 and C99.I 'd also appreciate anything to do with the quality of my code ( although I know my programs are lacking in functions - I 've added them to the C version and plan to add them to the Python version after it 's working ) .Thanks.Edit : an unsolved puzzle below.0 1 0 9 0 0 0 8 70 0 0 2 0 0 0 0 60 0 0 0 0 3 2 1 00 0 1 0 4 5 0 0 00 0 2 1 0 8 9 0 00 0 0 3 2 0 6 0 00 9 3 8 0 0 0 0 07 0 0 0 0 1 0 0 05 8 0 0 0 6 0 9 0"
"I 'm using ( Py ) OpenGL to display 256 colors indexed images . I use a shader together with a 1D texture containing the palette . Here 's the Fragment shader code : To avoid rounding errors , all MAG and MIN filters are set to NEAREST.The way I was seeing the texture coordinates for the 1D texture was : color 0 lies in interval [ 0 ; 1/256 [ color 1 lies in interval [ 1/256 ; 2/256 [ ... color 255 lies in interval [ 255/256 ; 1 [ I converted myself integer indexes to float between 0 and 1 , to make sure what was happening , using the formula x_float = ( x_int + .4 ) /256 , that is x_float is inside the before mentioned intervals , a little before its center ( to avoid the result being rounded at the wrong side of the interval ) .But it does n't work . I made a checkboard of 256 celles , with colors indexes 0 to 255 and a palette of levels of grey ( from 0x000000 to 0xFFFFFF ) . The code is given below . I then made a screen snapshot and edited it in Paint.NET to look if colors were right , and noticed a jump at color 0xE0 : I get twice color 0xDF , and from this one , everything is shifted by one : last color is 0xFE instead of 0xFF.I suspect some kind of rounding error , but do n't see how ... Here 's the complete code :"
"Take a canonical Python function defined like : Calling this function from Python might look like : Does the Python C API provide some way to call this function from C space ? How can the above three lines be converted to C equivalent ? Note that in a similar question , using boost : :python , C++ calls into this keyword argument function with almost the same syntax as from Python , which is quite a trick !"
"One proviso : The syntax element at the heart of my Question is in the Python language ; however , this element appears frequently in the Matplotlib library , which is the only context i have seen it . So whether it 's a general Python syntax question or a library-specific one , i am not sure . What i do know is that i could not find anything on point -- either in the Python Language Reference or in the Matplotlib docs.Those who use and/or develop with the excellent Python plotting library , Matplotlib will recognize the syntax pattern below . ( What is the construction on the left-hand side of this expression ? And , what is the purpose for using it ? I am familiar with Python 's assignment unpacking , e.g. , I 'm also aware that in Python one-item tuples are sometimes represented as t , And either could be the answer to the first question above ; if so then i do n't yet understand the reason why only the first element of the value returned from the call to plot is needed here . ( note : `` l '' is a lower case `` ell '' ; i am using this letter because ls is the letter most often used here , probably because it is bound to an object that begins with the same letter-see below ) .Some additional context : The call to plot returns a list of line2D instances : So l is an object of type line2D.Once bound to the lines2D object , this `` variable '' is usually seen in Matplotlib code like so : This expression changes the color of the line that represents the data values inside the plot window ( the `` plot line '' ) Below is one more example ; it shows a common scenario for this `` variable-comma '' construction , which is embedding small toolkit/graphics-backend-independent widgets in a Matplotlib plot window , e.g. , to toggle on/off by checkbox , multiple data series appearing in the plot window.In the code below , a simple Matplotlib plot and a simple widget comprised of two checkboxes one for each data series are created . l0 and l1 are again bound to calls to plot ; both appear a couple of liens later when their get_visible and set_visible methods are called within a custom function passed in when *on_click* is called ."
"Recently i discovered that there is not-documented django.db.models.fields.Field.name option : There is mention of it in doc-way : Description above is related with # 683 ( [ patch ] Saving with custom db_column fails ) ticket.So if we look through whole django.db.models.fields.Field class , this seems as name option is setting attribute name , which make real name of variable invalid : Suppose we have our model : What django-admin shell tells us : Question is kinda broad , but i am also curious.Is this name option is a thing that only helped to develop django , or ordinary developers can also make use of it ? And if we can , what for ?"
"I 've came to use mingus to try to reproduce some notes in python . Based on what was answered here , I 've tried with : Using among others , one of this sf2 files . But then I got the error : fluidsynth : warning : Failed to set thread to high priority fluidsynth : warning : No preset found on channel 9 [ bank=128 prog=0 ] Researching a bit , this answer said : For General MIDI compatibility , the default sound font instrument assignments are bank 0 , program 0 ( `` Acoustic Grand Piano '' ) for channels 0–8 and 10–15 , and bank 128 , program 0 ( `` Default Drum Set '' ) for channel 9 . Apparently , your sound font does not have the latter . This does not matter if your MIDI file does not assume General MIDI compatible instruments and does not try to play drum sounds on channel 9.But aside of this , that may made things a bit clearer , I still do n't know how to solve it.Plus , if I use the same file that the answer of the very first link , then the error I get is this one ( and do n't know how to solve it neither ) : fluidsynth : warning : Failed to set thread to high priority fluidsynth : warning : Failed to pin the sample data to RAM ; swapping is possible.UpdateRunning the program with sudo permissions removes this errors , but it does n't sound . This way , the error I get is : QStandardPaths : XDG_RUNTIME_DIR not set , defaulting to '/tmp/runtime-root ' I thought it may be a problem with the selected driver mode in fluidsynth driver , but I 've tried with them all ( alsa , oss , and so forth ) with the same result . Just for the sake of completeness , I 'm running it within a VM , and other sounds inside it are correctly reproduced in my host speakers.However based on this answer , I guess it should be solved by passing to it the right sound environment variable . Which I do n't know . I 've tried with : Where the complete path to the file is needed since this needs sudo permissions and changes its working directory , and in the end the same error came : XDG_RUNTIME_DIR.Update 2If I run it with sudo -E option , the error is replaced by a new one : QStandardPaths : wrong ownership on runtime directory /run/user/1000 , 1000 instead of 0I 'm reading in some webs , that the 1000 user is supposed to be the default user instead of root.Therefore , I 've done a : Just to try , and then the app runs without fails , but it still does n't sound.Update 3Based on this example and this Q & A , I 've tried both to use time sleep with several times , and using raw_input as well after ( also before ) doing a play_note , but it still does n't sound ."
"I am using ply and have noticed a strange discrepancy between the token re match stored in t.lex.lexmatch , as compared with an sre_pattern defined in the usual way with the re module . The group ( x ) 's seem to be off by 1.I have defined a simple lexer to illustrate the behavior I am seeing : ( I get a warning about t_error but ignore it for now . ) Now I feed some input into the lexer and get a token : I get a LexToken ( CHAR , < _sre.SRE_Match object at 0x100fb1eb8 > ,1,0 ) . I want to look a the match object : So now I look at the groups : m.group ( ) = > ' h ' as I expect.m.group ( 0 ) = > ' h ' as I expect.m.group ( 1 ) = > ' h ' , yet I would expect it to not have such a group.Compare this to creating such a regular expression manually : This gives different groups : m2.group ( ) = ' h ' as I expect.m2.group ( 0 ) = ' h ' as I expect.m2.group ( 1 ) gives IndexError : no such group as I expect.Does anyone know why this discrepancy exists ?"
"Environment : Python : 3.6.6pyglet version : 1.3.2Code base : abstract_model.pyground3d.pyExplanation : I have models ( just flat rect for an example ) which should be placed on 3 dimensions . And these models should be animated , like picture_1 , after second picture_2 , ... etc.As I understood from my previous question using pyglet.sprite.Sprite ( ) in 3D batch is not a good idea.Question : How I can change pictures ( using TextureGroup or any other approaches ) on self.vertices ? Or which arroach/classes I use use to implement it . I ca n't find any examples for such ( as for my simple vision ) usual case as animation for some flat models in 3 dimensions.There are many example about rotating/moving/resizing of vertices , but how to build a correct question ( is animation aspect ) for getting answer in google - I do n't know.PS : If you , reader , have any usefull links on this subject ( for pyglet or just for OpenGL ) I would be very appreciated you share this link ( s ) in comment ."
"Aaron Swartz played an important role in shaping the internet during its earlier years . For those familiar with Aaron , you likely know he committed suicide after facing up to 35 years in prison for downloading a massive number of articles from jstor 's archive , a digital library of academic journals and books . The script he used to download the articles was released and is pictured below . ( Here 's a link Aaron 's documentary for those interested . ) This is the code : I guess this is a tribute of sorts . I have always been extremely affected by Aaron 's story and his passing . He was a brilliant pioneer of the internet , founding creative commons , the web feed format RSS , and Reddit , all before taking his own life at 26 years old . I want to understand as much as I can about the event that led to the death of a man who did many great things for the internet and its growing community of users . Context Jstor is a large library of academic publications . When Aaron downloaded articles from their archive in 2010 , JSTOR was freely available to MIT Students , but not freely available to the public . While we do n't know exactly what Aaron wanted to do with the information , it 's a safe bet he wanted to spread it to those without access . My QuestionI see he created a function Getblocks ( ) that used the urllib module to access Jstor 's digital archives , read the HTML of the web pages into a variable and split the contents of the page.It 's the command line section of the code that I do n't understand , from after he imported the sys module through the end of the if/else statement.He created a command line argument that allowed him to define..what ? What was he doing here ? If the length of the command line argument was < 1 and the else condition was invoked , what was his lambda function accomplishing here ? Any insight into the mechanics of Aaron 's large file seige would be greatly appreciated . Rest easy , Aaron . Additional Notes The legal documents related to the case can be found here . In these documents , there 's a link to the several conversations among Jstor 's employees after Aaron downloaded all the documents . In one email exchange , a Jstor employee describes how Aaron circumvented the `` sessions by IP '' rule to download . `` By clearing their cookies and starting new session they effectively dodge the abuse tools in Literatum ... .The # of sessions per IP rule did not fire because it is on a server by server basis and the user was load balanced across more than few servers . 8500 sessions would only need two servers to dodge the rule.We can ratchet the of sessions down but am requesting data to find an effective level that would have caught incident without disrupting normal users elsewhere With our MDC and number of servers there may be no sweet spot that accomplishes both . ''"
"My code is : I somehow need to draw the bounding_codes , class_IDs , and scores onto the image and output it via imshow.How can I accomplish this ?"
"Is there a way to show only the important directory paths when executing a python program ? Currently I get this : As I know in which directory my code is , the full directory makes the error message just worse readable . Can I tell python to show me the output more like this ? AnswerUsing the code from unutbu I added some lines for colors , in case someone is looking for an easy improvement of the interpreter output , just use this as a module and import it :"
"I am currently doing this problem from the book cracking the coding interview : 9.3 . A magic index in an array A [ 0 ... n-1 ] is defined to be an index such thatA [ i ] = i . Given a sorted array of distinct integers , write a method to find amagic index , if one exists , in array A.And here is my code : However , when i run my code . I am not getting the proper output . Is there any help or advice I could get ? Thanks in advance"
"Lets say I have two numpy arrays A and B : I would like to ( quickly ) count the number of elements that are unequal between the two arrays . In the case above , the answer would be 2.Is there nice way to do this ?"
"I 've written a script in python to get different links leading to different articles from a webpage . Upon running my script I can get them flawlessly . However , the problem I 'm facing is that the article links traverse multiple pages as they are of big numbers to fit within a single page . if I click on the next page button , the attached information i can see in the developer tools which in reality produce an ajax call through post request . As there are no links attached to that next page button , I ca n't find any way to go on to the next page and parse links from there . I 've tried with a post request with that formdata but it does n't seem to work . Where am I going wrong ? Link to the landing page containing articlesThis is the information I get using chrome dev tools when I click on the next page button : This is my script so far ( the get request is working flawlessly if uncommented , but for the first page ) : Btw , the url in the browser becomes `` https : //www.ncbi.nlm.nih.gov/pubmed '' when I click on the next page link.I do n't wish to go for any solution related to any browser simulator . Thanks in advance ."
"This might be a long shot , but here 's the error that i 'm getting : I have built a Flask application that takes addresses as input and performs some string formatting , manipulation , etc , then sends them to Bing Maps to geocode ( through the geopy external module ) .I 'm using this application to clean very large data sets . The application works for inputs of usually ~1,500 addresses ( inputted 1 per line ) . By that I mean that it will process the address and send it to Bing Maps to be geocoded and then returned . After around 1,500 addresses , the application becomes unresponsive . If this happens while i 'm at work , my proxy tells me that there is a tcp error . If i 'm on a non work computer it just does n't load the page . If I restart the application then it functions perfectly fine . Because of this i 'm forced to run my program with batches of about 1,000 addresses ( just to be safe because i 'm not sure yet of the exact number that the program crashes at ) .Does anyone have any idea what might be causing it ? I was thinking something along the lines of me hitting my Bing API key limit for the day ( which is 30,000 ) , but that ca n't be accurate as I rarely use more than 15,000 requests per day.My second thought was that maybe it 's because i 'm still using the standard flask server to run my application . Would switching to gunicorn or uWSGI solve this ? My third thought was maybe it was getting overloaded with the amount of requests . I tried to sleep the program for 15 seconds or so after the first 1,000 addresses but that did n't solve anything.If anyone needs further clarification please let me know.Here is my code for the backend of the Flask Application . I 'm getting the input from this function : Here is the cleanAddress function : It 's a bit cluttered right now , with all of the if statements to check for specific typos in the address , but I plan on moving a lot of this code into other functions in another file and just passing the address though those functions to clean it up a bit.UPDATE : I have switched to running the application using gunicorn , and this is solving the issue when i 'm accessing the application from my home network , however , I am still receiving the TCP error from my work proxy . I am not getting any error message in my console , the browser just displays the TCP error . I can tell that the tool is still working in the background , because I have a print statement in the loop telling me that each address is still being geocoded . Could this be something along the lines of my work network not liking that the page remains loading for a long period of time and then just displays the proxy error page ?"
"I am trying to scrape the name of a link in a popup in wikipedia . So when you hover a link in wikipedia , it brings up a little snippet from the intro to that link . I need to scrape that information but I am unsure where it would be in the source . When I inspect the element ( as it is popped up ) this is the html ( for this example I am hovering over the link `` Greek '' ) What I need to extract is the href which = `` /wiki/Ancient_Greek '' but this piece of html disappears when I am not hovering the link . Is there a way ( with BS4 and python ) to extract this information with the source html I am scraping ? EDIT : I ca n't afford to make additional calls to webpages because the project takes long to run as it is . If there is anyway to change how I am retrieving the source such that I can get the popup information that would be helpful . This project is giant and getting this popup information is crucial . any suggestions at all that do n't require a complete rebuild of the project is extremely appreciated -- I am using urllib to pull source ( with requests ) and bs4 to scrape through ."
"The first time , but not subsequent times , I run this code block I get an error . Can anyone help me understand why please ? There are no previous SO posts on this error at all ( at least in search results ) I am fairly new to coding . As far as I can tell it is a sequence error in my code . Namely , the first time the attribute is n't loaded ( yet ) , then the attribute gets loaded further down the code block , making it available when I run the code again . Problem is I ca n't work out what part of the code it is.Error Message"
"I 'm aware that Python threads can only execute bytecode one at a time , so why would the threading library provide locks ? I 'm assuming race conditions ca n't occur if only one thread is executing at a time . The library provides locks , conditions , and semaphores . Is the only purpose of this to synchronize execution ? Update : I performed a small experiment : Basically I should have started 100k threads and incremented by 1.The result returned was 99993.a ) How can the result not be 99999 if there 's a GIL syncing and avoiding race conditions ? b ) Is it even possible to start 100k OS threads ? Update 2 , after seeing answers : If the GIL does n't really provide a way to perform a simple operation like incrementing atomically , what 's the purpose of having it there ? It does n't help with nasty concurrency issues , so why was it put in place ? I 've heard use cases for C-extensions , can someone examplify this ?"
"I 'm working on a Python Django package whose front-end components employ a bit of CoffeeScript.Right now , I have a rather brain-dead external script that takes care of the CoffeeScript compilation . It simply runs a coffee compile command for every *.coffee file in a src/coffee/ directory and stores the output in src/static/js -- this similar to how python ./setup.py build_ext -- inplace stores a C extension 's build files in the development source tree.That works for now , but it 's pretty cheesy -- it forces a flat directory structure , and modifies the files in src/static ( which is the opposite of what `` static '' implies ) . I want to be maximally pythonic about things , so I looked into modifying distutils.ccompiler.CCompiler to run coffee as a subcommand of the setup.py `` build_ext '' subcommand -- I was envisioning the ability to do things like this : ... but I found distutils ' compiler API to be way too focussed on C compilation nuances that have no analog in this case e.g . preprocessing , linking , etc . I also looked at Cython 's code ( specifically at Cython 's CCompiler subclass , which preprocesses .pyx files into .c source ) but this looked similarly specialized , and not so appropriate for this case.Does anyone have a good solution for compiling CoffeeScript with a distutils setup.py script ? Or , barring that , a good alternative suggestion ?"
"When creating a model using Keras subclassing API , we write a custom model class and define a function named call ( self , x ) ( mostly to write the forward pass ) which expects an input . However , this method is never called and instead of passing the input to call , it is passed to the object of this class as model ( images ) .How are we able to call this model object and pass values when we have n't implemented Python special method , __call__ in the classUse tf.GradientTape to train the model : Should n't the input be passed like below :"
"GAE 1.5.5 looks to have some excellent , long-waited for features . However , they 're not working for me yet.I 've downloaded and installed GAE 1.5.5 , and am using a degenerate `` AAA '' app to test.Here 's roughly my app.yaml ( with various changes having been made for testing ) .app.yamlI 'm running this on Mac OS X Lion ( 10.7.1 ) .I hypothesize that I am not actually using the Python 2.7 runtime , in spite of the declaration in app.yaml to use it . I 'm not quite sure how to validate this theory , but the errors I 've encountered are consistent with it . These errors are reproduced below.Python PathWhen Google App Engine 's Python Path is not set , the app engine runsusing Python 2.6.6.To fix this I set Python Path to /usr/bin/python2.7 in the Google AppEngine preferences.WSGII get the following error : Where I 've tried AAA.app as : AAA.py : AAA/__init__.pyAAA/AAA.pyNote that I can continue to run CGI with app.yaml and AAA.py modified , mutatis mutandis . However doing so nevertheless results in those errors below : Jinja2When I run import jinja2 I get an ImportError.Django2Without : I end up with Django 0.96.TheoryGiven the following : http : //code.google.com/appengine/docs/python/tools/libraries27.htmlstates `` The use_library ( ) function provided by thegoogle.appengine.dist package is unavailable in Python 2.7 . `` use_library works for meuse_library is required because the `` libraries : { django : { ... , version : `` 1.2 '' } } declaration does not set the django version to 1.2Only Django 1.2 is included in the Python 2.7 runtime ( per thelibraries27.html link above ) I have to manually specify Python 2.7 in the Python Path in orderfor GAE to use Python 2.7WSGI does n't load the application properlyJinja2 can not be importedI believe I am not really using the Python 2.7 GAE runtime ( i.e . the app.yaml declaration is being ignored in the GAE 1.5.5 SDK ) .I hope the above is a helpful description , and I 'd be grateful for any thoughts as to what may be happening here – and potential solutions ."
"From the Python Language Reference ( v 3.1 , see here - http : //docs.python.org/py3k/reference/executionmodel.html # naming-and-binding ) : It is illegal to unbind a name referenced by an enclosing scope ; the compiler will report a SyntaxError.But when I run the following code : it works fine ; and when I change the order of calls : I get a NameError , not a SyntaxError . Apparently , I 'm not understanding the rule correctly . Can anyone explain it ? Thanks ."
"If it is only concatenation of strings as follows , it finish immediately.Constant processing timeInexplicably , assigning a concatenated string to another variable makes the process slower and slower.Processing time will be delayed.Both python2 and python3 give the same result ."
"Memory mapped file is an efficient way for using regex or doing manipulation on large binary files.In case I have a large text file ( ~1GB ) , is it possible to work with an encoding-aware mapped file ? Regex like [ \u1234-\u5678 ] wo n't work on bytes objects and converting the pattern to unicode will not work either ( as `` [ \u1234-\u5678 ] '' .encode ( `` utf-32 '' ) for example will not understand the range correctly ) .Searching might work if I convert the search pattern from str to bytes using .encode ( ) but it 's still somewhat limited and there should be a simpler way instead of decoding and encoding all day.I have tried wrapping it with io.TextIOWrapper inside an io.BufferedRandom but to no avail : Creating a wrapper ( using inheritance ) and setting the methods seekable , readable and writable to return True did not work either.Regarding encoding , a fixed length encoding like utf-32 , code-points or the lower BMP of utf-16 ( if it 's even possible referring just to that part ) might be assumed.Solution is welcome for any python version ."
"I have the following piece of code : I am debugging this code on Python interactive terminal and if I do : It returns 'None ' . But if I type the commands manually ( instead of a function ) like this : it works . Python does n't give any errors too . Why this return instatement is n't working ? BTW , verify_key is a UUID that I set up manually for debugging purposes ( and it 's a known value ) and this code is using sqlite3 databases to make a consult ."
"I have a mp4 video of 720x1280 , and I want it in different sizes like:0.66 % , 0.5 % and 0.33 % .For each of these sizes I use : I do this for each of the sizes but some of them work and some not . The 0.66 not works , just like the 0.33 . The 0.5 % works just fine.It creates the files for every size , but they are corrupt , and ca n't open them ( except 0.5 as I said , which works ok ) .Any clue on this ? Any better solution for resizing in Python ?"
"I managed to reproduce this on both Python 3.4 and 3.7.Consider : Clearly one would expect b.__hash__ to be defined here , since it is defined under Comparable which B is a subclass of.Lo and behold , it is defined , but evaluates to None . What gives ? The same behavior is reproduced if implementing __init__ as super ( ) .__init__ ( ) in Comparable and A ."
"I am trying to port some code from python2 to python3.I am having trouble when converting some code using date/time manipulations . Python2.7 Python3.6The result for python2 = ( 2016 , 4 , 19 , 17 , 10 , 31 ) whereas for python3 = ( 2016 , 4 , 19 , 22 , 40 , 31 ) .Why is this difference , and how should I overcome this ?"
"Fabric displays Disconnecting from username @ server ... done . for almost 2 minutes prior to showing a new command prompt whenever I issue a fab command.This problem exists when using Fabric commands issued to both an internal server and a Rackspace cloud server . Below I 've included the auth.log from the server , and I did n't see anything in the logs on my MacBook . Any thoughts as to what the problem is ? Server 's SSH auth.log with LogLevel VERBOSEServer ConfigurationOS : Ubuntu 9.10 and Ubuntu 6.10 ( tested 4 servers with those OSes ) OpenSSH : Ubuntu package version 1.5.1p1-6ubuntu2Client ConfigurationOS : Mac OS X 10.6.3Fabric ver 0.9Vritualenv ver 1.4.7pip ver 0.7Simple fabfile.py Used for TestingThe problem persists even when I just run fab -H server_ip host_type with the following fabfile.Thoughts on Cause of the IssueI 'm not certain how long this problem has persisted , but below are some things that have changed since I started to notice the slow server disconnect using Fabric.I recreated my virtualenv 's using virtualenv 1.4.7 , virtualenvwrapper 2.1 , and pip 0.7 . Not sure if this is related , but it is a thought since I run my fabfiles from within a virtualenv.I enabled OS X 's firewall . I disabled OS X 's firewall and the problem persisted , so this is not the issue ."
"I have a array like below and a pandas DataFrame like belowWhen I apply np.isreal to DataFrameWhen I do np.isreal for the numpy array.I must using the np.isreal in the wrong use case , But can you help me about why the result is different ?"
"I am developing a desktop app using Python 3 and GTK3.I need a button to change its background color when hover , which can be done with this : Until now , I defined a RGB color that matches my own theme , but as I have to release this code , I would like to change that color to one of the established theme colors.Searching the web , I found there 's a way to do it with GTK2 , but since version 3.8 of GTK , the gtk-color-scheme property has been deprecated . Is there any other way to respect the user theme colors without using this property ?"
"This answer of @ Dunes states , that due to pipeline-ing there is ( almost ) no difference between floating-point multiplication and division . However , from my expience with other languages I would expect the division to be slower.My small test looks as follows : For different commands and size=1e8 I get the following times on my machine : The most interesting part : dividing by 0.5 is almost twice as fast as dividing by 0.51 . One could assume , it is due to some smart optimization , e.g . replacing division by A+A . However the timings of A*2 and A+A are too far off to support this claim.In general , the division by floats with values ( 1/2 ) ^n is faster : It gets even more interesting , if we look at size=1e4 : Now , there is no difference between division by .5 and by .51 ! I tried it out for different numpy versions and different machines . On some machines ( e.g . Intel Xeon E5-2620 ) one can see this effect , but not on some other machines - and this does not depend on the numpy version.With the script of @ Ralph Versteegen ( see his great answer ! ) I get the following results : timings with i5-2620 ( Haswell , 2x6 cores , but a very old numpy version which does not use SIMD ) : timings with i7-5500U ( Broadwell , 2 cores , numpy 1.11.2 ) : The question is : What is the reason for higher cost of the division by 0.51 compared to division by 0.5 for some processors , if the array sizes are large ( > 10^6 ) . The @ nneonneo 's answer states , that for some intel processors there is an optimization when divided by powers of two , but this does not explain , why we can see the benefit of it only for large arrays.The original question was `` How can these different behaviors ( division by 0.5 vs. division by 0.51 ) be explained ? `` Here also , my original testing script , which produced the timings :"
"I 've noticed that Python lets me do this : It lets me do the same thing for lists , deques , etc . What are the semantics of < when applied to dictionaries in Python ? In general where can I find out the semantics of < for any given type of collection ? In most cases it seems not to be found in the documentation . For example : I ask because I have a list of tuples of the form ( int , dict ) . I want to sort this array based on the first element , but if the first elements are equal for two items then I do n't care about the second . I 'd like to know if myArray.sort ( ) will do something complicated involving recursing through the dicts in this case , or if it will just return an arbitrary value ."
"I 'm currently working with a remote Jupyter notebook ( through a docker image ) , and am having an issue with finding a folder that exists in the directory ( where I 'm running the notebook ) but does n't exist in the notebook tree . Command I 'm using to execute the notebook : Command I 'm using to access the notebook remotely : What 's weird is that if I navigate to the notebook 's working directory ( on the remote host / server ) and add a folder + files , the notebook will not reflect the changes ( i.e . mkdir new_folder in the working directory will not add new_folder to the notebook 's tree ) .Would anyone know why this could be the case , and if so , how to `` refresh '' / `` update '' the tree ? Thanks so much for all and any help !"
"I 've been playing with pyglet . It 's very nice . However , if I run my code , which is in an executable file ( call it game.py ) prefixed with the usualby doingthen it 's a bit clunky . But if I run it withor then its super-smooth.I 'm do n't care much why it runs slow without optimization ; pyglet 's documentation mentions that optimizing disables numerous asserts and also OpenGL 's error checking , and I 'm happy to leave it at that.My question is : how do people distributing Python code make sure the end users ( with zero interest in debugging or modifying the code ) run the optimized version of the code . Surely there 's some better way than just telling people to make sure they use optimization in the release notes ( which they probably wo n't read anyway ) ? On Linux I can easily provide a ./game script to run the file for end users : but that 's not very cross-platform.I have an idea I ought to be able to change the # ! line to orbut those do n't seem to work as expected , and I 'm not sure what they 'd do on Windows.Is there some way of controlling optimization from within the code I 'm unaware of ? Something like : What 's considered best-practice in this area by people shipping multi-platform python code ?"
I have a list : I need to create a new list by concatenating an original list which range goes from 1 to k. Example : I try : But I think it is very complicated.What is the fastest and most generic solution ?
"I have a problem with something and I 'm guessing it 's the code.The application is used to 'ping ' some custom made network devices to check if they 're alive . It pings them every 20 seconds with a special UDP packet and expects a response . If they fail to answer 3 consecutive pings the application sends a warning message to the staff.The application is running 24/7 and for a random number of times a day ( 2-5 mostly ) the application fails to receive UDP packets for an exact time of 10 minutes , after which everything goes back to normal . During those 10 minutes only 1 device seems to be replying , others seem dead . That I 've been able to deduce from the logs.I 've used wireshark to sniff the packets and I 've verified that ping packets are going both out AND in , so the network part seems to be working okay , all the way to the OS . The computers are running WinXPPro and some have no configured firewall whatsoever . I 'm having this issue on different computers , different windows installs and different networks.I 'm really at a loss as to what might be the problem here.I 'm attaching the relevant part of the code which does all the network . This is run in a separate thread from the rest of the application.I thank you in advance for whatever insight you might provide ."
"I have a very simple namespace package ( contents included below , as well as the directory layout ) . If I try to import namespace_repro.module , I got the following error : AttributeError : module 'namespace_repro ' has no attribute 'module ' . As far as I understand my package has a valid layout and the import should work . Interestingly , the error is present only in Python 3.6.8 and the import succeeds in Python 3.7.How to reproduce the problem ? I have a directory named import-error-repro with in it a setup.py ( see below ) , then a nested directory path src/namespace_repro/module , containing three files , __init__.py , x.py and y.py . Their contents : setup.pysrc/namespace_repro/module/__init__.py : src/namespace_repro/module/x.py : and finally src/namespace_repro/module/y.py : I created a brand new Python 3.6 conda environment by conda create -n namespace6 python=3.6 ipython , then I activated it and installed the package as pip install -e ./import-error-repro ( note that -e does n't matter , the problem is reproducible without it ) . After that , I tried import namespace_repro.module in ipython ( though the same happens in the official python interpreter ) . The result isThe strange thing is that the import system finds namespace_repro.module twice but fails at the third time ! Some other interesting behaviour : Directory layout"
I have a following code : If I run it I get : How to force exec stop without errors ? Probably I should replace return with something ? Also I want the interpreter work after exec call .
"In PHP you can create form elements with names like : or evenWhen the form is posted , category is automatically turned into a nice dictionary like : Is there a way to do that in Django ? request.POST.getlist is n't quite right , because it simply returns a list , not a dictionary . I need the keys too ."
"I am struggling to understand how doxygen works with namespaces in Python . A namespace of the name `` filename '' i.e . temp.py , is generated by default . I can also declare new namespaces with the \package or \namespace command.However , what I do n't understand is why the class below ( or any other definition ) always appears under the temp namespace ? Please help me understand how the namespace command works in doxygen.If you know how and why /namespace pr /package commands are used in doxygen , you can bypass the example below and directly answer it.Now , I am adding a new namespace by the name \new_package , added to the temp.py fileI have also created a file named \new_package.py and added the below lines in it : In the generated documentation , I get \class_demo1 under namespace \temp.However , the new namespace \new_package.py does n't display the class and def declared under it ."
"I am getting 11 or 12 of 15 correct in a Python course on edX.org each time I submit , but not getting much help from anyone in the discussions because no one can really post any code there ( not really helpful ) and there does n't seem to be any available support staff to speak with from the course , which I would pay for , so I am posting here . I was about to pay someone to tutor me but no one is available right now , and I am under some pressure to get this course done by December for my job . This is the assignment : Now write a program that calculates the minimum fixed monthly payment needed in order pay off a credit card balance within 12 months . By a fixed monthly payment , we mean a single number which does not change each month , but instead is a constant amount that will be paid each month . In this problem , we will not be dealing with a minimum monthly payment rate . The following variables contain values as described below : balance - the outstanding balance on the credit card annualInterestRate - annual interest rate as a decimal The program should print out one line : the lowest monthly payment that will pay off all debt in under 1 year , for example : Lowest Payment : 180 Assume that the interest is compounded monthly according to the balance at the end of the month ( after the payment for that month is made ) . The monthly payment must be a multiple of $ 10 and is the same for all months . Notice that it is possible for the balance to become negative using this payment scheme , which is okay . A summary of the required math is found below : Monthly interest rate = ( Annual interest rate ) / 12.0 Monthly unpaid balance = ( Previous balance ) - ( Minimum fixed monthly payment ) Updated balance each month = ( Monthly unpaid balance ) + ( Monthly interest rate x Monthly unpaid balance ) This is my code : Here are the test results : ( I have here all 15 , so you might be able to identify a pattern I ca n't see . The ones marked `` ERROR '' are the ones I got incorrect )"
"There are three integers x , y and z ( each of them > = 1 ) and a given upper bound integer n < 10^6 . Also , n = x + y + z and output = cos ( x ) + cos ( y ) + cos ( z ) .The exercise is to maximize output.I wrote a simple script for this , but the time complexity is O ( n^3 ) . Is there any way to simplify this ?"
"Problem Statement : A pandas dataframe column series , same_group needs to be created from booleans according to the values of two existing columns , row and col . The row needs to show True if both cells across a row have similar values ( intersecting values ) in a dictionary memberships , and False otherwise ( no intersecting values ) . How do I do this in a vectorized way ( not using apply ) ? Setup : The Desired Goal : How do I accomplish creating this new column based on a lookup on a dictionary ? Note that I 'm trying to find intersection , not equivalence . For example , row 4 should have a same_group of 1 , since a and y are both vowels ( despite that y is `` sometimes a vowel '' and thus belongs to groups consonant and vowel ) ."
"The typing module provides some handy features to enable better readability and more confidence in the correctness of the code typed.One of the best features is that you can write something like below to describe an input dictionary with specified element-types.Now I wonder , can this be `` extended '' to custom types ? Can custom types ( which act like containers ) be provided indices in a formal way to tell a potential type checker the contents must be of a specific type ? For example the collections.Counter class ? - Above constraint wo n't work when I really want a counter , since dictionaries do not provide addition operator , where counters do.I could do something like : But then I lose information on what the counter stores . - Is using a TypeVar the right approach here ? I 'm not clear if Typevar is supposed to work that way or not . EDIT : just to be clear , above code does n't work and errors on the TypeVar line ."
I have a function Now I would like to collect all the a but not b . Also I want the result aa to be a list instead of iterator . Right now I useIs this the best one can do in terms of space/time efficiency ?
"In Haskell , I can define a binary tree as follows : then I can some operations on it as follows : I know Python does n't has equivalent of data in Haskell . If it has , please tell.So , how does one define a binary tree in Python and how to implement the above two functions in it ?"
"I have a dataframe likeHow do I only fill the NULLs ( with 0 ) for each series up until the first non NULL value , leading to"
In my requirements.txt I 'd like to specify that I need greater than or equal to a specific version of a Python dependency . I could do this if I wanted the dependency Python package installed from PyPI : But what if I want to specify the GitHub URL to install from ? I know you can specify an exact tag : But can I specify a > = instead ?
I have a doctest where I test a float conversion : In Python < 2.7 this results in : In Python 2.7 the result isCan I make both these results acceptable in my doctest ?
"Looking at a random selection of well-known Python packages , why is there a general trend to not include a # ! /usr/bin/env python line at the top of setup.py ? I know that the usual recommended way of interacting with the file is something like : rather thanbut is there a good reason for this ? These packages do not include a shebang : pytest , lxml , six , virtualenv , pip But these do : requests , simplejson , setuptools"
"My Bottle apps have n't been very DRY , here 's a test-case : One way of solving this issue is to have a global error handler , and raise errors all over the place.Another is to use decorators , which also have overhead issues.Is there a better way of doing the validation side of each route ? - I 'm thinking of something like :"
"Python 's documentation says : If no expressions are present , raise re-raises the last exception that was active in the current scope . ( Python 3 : https : //docs.python.org/3/reference/simple_stmts.html # raise ; Python 2.7 : https : //docs.python.org/2.7/reference/simple_stmts.html # raise . ) However , the notion of `` last active '' seems to have changed . Witness the following code sample : which results in something I did n't expect with Python 2 : but has the expected ( by me ) result with Python 3 : andSo what does `` the last ... active '' mean ? Is there some documentation on this breaking change ? Or is this a Python 2 bug ? And more importantly : What is the best way to make this work in Python 2 ? ( Preferably such that the code will keep working in Python 3 . ) Note that if one changes the code tothen things start to work for Python 2 as well : I 'm considering to switch to that ..."
"I am trying to implement the function fast modular exponentiation ( b , k , m ) which computes : b ( 2k ) mod m using only around 2k modular multiplications.I tried this method : but I am still stuck in same problem which is if I try b = 2 , k = 1 , m = 10 , my code returns 22 . However , the correct answer is : 2^ ( 2^1 ) mod 10 = 2^2 mod 10 = 4and I can not find the reason why ."
"I have been trying to make an app in Python using Scrapy that has the following functionality : A rest api ( I had made that using flask ) listens to all requests to crawl/scrap and return the response after crawling . ( the crawling part is short enough , so the connection can be keep-alive till crawling gets completed . ) I am able to do this using the following code : Now the problem I am facing is after making the reactor stop ( which seems necessary to me since I do n't want to stuck to the reactor.run ( ) ) . I could n't accept the further request after first request . After first request gets completed , I got the following error : Which is obvious , since we can not restart the reactor.So my questions are:1 ) How could I provide support for the next requests to crawl ? 2 ) Is there any way to move to next line after reactor.run ( ) without stopping it ?"
"I 'm currently writing a project that requires third party code that uses a method that returns an iterator of itself , an example of how this would look in my code : Currently this simply clutters my code , and becomes hard to read after 3 levels . Ideally I 'd get it to do something like this : Is there a built in way to do this in Python ?"
I 'm trying to run doctest on a function that works with nulls . But doctest does n't seem to like the nulls ... I 'm seeing these errorsWhat can I do to allow nulls in test cases like this ?
"I need a little bit of help understanding the subtleties of the descriptor protocol in Python , as it relates specifically to the behavior of staticmethod objects . I 'll start with a trivial example , and then iteratively expand it , examining it 's behavior at each step : At this point , this behaves as expected , but what 's going on here is a bit subtle : When you call Stub.do_things ( ) , you are not invoking do_things directly . Instead , Stub.do_things refers to a staticmethod instance , which has wrapped the function we want up inside it 's own descriptor protocol such that you are actually invoking staticmethod.__get__ , which first returns the function that we want , and then gets called afterwards.So far so good . Next , I need to wrap the class in a decorator that will be used to customize class instantiation -- the decorator will determine whether to allow new instantiations or provide cached instances : Now , naturally this part as-is would be expected to break staticmethods , because the class is now hidden behind it 's decorator , ie , Stub not a class at all , but an instance of factory that is able to produce instances of Stub when you call it . Indeed : So far I understand what 's happening here . My goal is to restore the ability for staticmethods to function as you would expect them to , even though the class is wrapped . As luck would have it , the Python stdlib includes something called functools , which provides some tools just for this purpose , ie , making functions behave more like other functions that they wrap . So I change my decorator to look like this : Now , things start to get interesting : Wait ... . what ? functools copies the staticmethod over to the wrapping function , but it 's not callable ? Why not ? What did I miss here ? I was playing around with this for a bit and I actually came up with my own reimplementation of staticmethod that allows it to function in this situation , but I do n't really understand why it was necessary or if this is even the best solution to this problem . Here 's the complete example : Indeed it works exactly as expected : What approach would you take to make a staticmethod behave as expected inside a decorated class ? Is this the best way ? Why does n't the builtin staticmethod implement __call__ on it 's own in order for this to just work without any fuss ? Thanks ."
"I 'm trying to write a method to generate a sequence of Gaussian divisors of a Gaussian integer - a Gaussian integer is either a normal integer or a complex number g = a + bi where a and b are both integers , and a Gaussian divisor of a Gaussian integer g is a Gaussian integer d such that g / d is also a Gaussian integer.I 've got the following code.It seems to `` mostly '' work but for some inputs it is missing out some Gaussian divisors , e.g . for 2 I would expect the sequence to include the divisor -2 + 0j ( which is just -2 ) , but it is missing . I ca n't figure out why it is doing this or where there is gap in the logic ."
"Recently , I try to learn how to use Tensorflow on multiple GPU to accelerate training speed . I found an official tutorial about training classification model based on Cifar10 dataset . However , I found that this tutorial reads image by using the queue . Out of curiosity , how can I use multiple GPU by feeding value into Session ? It seems that it is hard for me to solve the problem that feeds different value from the same dataset to different GPU . Thank you , everybody ! The following code is about part of the official tutorial ."
"I keep thinking there should be a function for this , but I 've searched the likely places ( google , itertools docs , list methods , other SO questions ) , but nowhere found quite what I was looking for.Naive and working implementation : It works , but it does n't feel right . There should be a better way to do this ! EDIT : I ended up with using a slightly modified version of senderle 's final suggestion after reviewing the answers : It 's short and elegant , output is two iterators no matter the input ( strings , lists , iterators ) , and as a bonus , it even works with the following input : The other solutions , those that work at all with iterators , will run out of memory with this input . ( Note that this is just a bonus . Infinite iterators was something I had n't even considered when I wrote this question )"
"I would like to know if there 's a way to associate different labels to a class with NeoModel . If not , what module can allow me to do this ? My understanding is that when using the following class declaration , `` Person '' is a label.Let 's say that I 'd like to add a second label , `` Employed '' , `` Unemployed '' , `` Student '' .With Cypher I could use : CREATE ( p : Person : Student ) Is there anyway I can achieve the same with NeoModel ? Notes : From my research working with labels yields faster queries than with properties ( neo4j/cypher ) , which is why I would like employed/unemployed/student to be labels . Otherwise I would be fine adding `` occupation '' as a node property ."
"I am developing a CNN in keras to classify satellite imagery that has 10 spectral bands . I 'm getting decent accuracy with the network below ( ~60 % val accuracy across 15 classes ) but I want to better incorporate the relationships between spectral bands at a single pixel which can yield a lot of information on the pixel 's class . I see a lot of papers doing this but it is often called different things . For example : Cascaded cross-channel parametric poolingConv1DDepthwise Separable ConvolutionConv2D ( num_filters , ( 1 , 1 ) ) And I 'm not certain about the differences between these approaches ( if there are any ) and how I should implement this in my simple CNN below . I 'm also not clear if I should do this at the very beginning or towards the end . I 'm inclined to do it right at the start when the channels are still the raw spectral data rather than the feature maps ."
Is this supposed to return a boolean value ? I already knew that
"Assume A is the parent class of B and b is an instance of B . Then an overriden method of A can be called with super : super ( B , b ) .method ( ) .The docs state `` str ( object ) returns object.__str__ ( ) '' in its basic invocation.It should follow that str ( super ( B , b ) ) == super ( B , b ) .__str__ ( ) , but that 's not the case ( interactive version ) : So where did I go wrong ? Does the super mechanism not work for magic methods ? Does str not invoke __str__ in this case ? Is it related to this paragraph : Note that super ( ) is implemented as part of the binding process for explicit dotted attribute lookups such as super ( ) .__getitem__ ( name ) . It does so by implementing its own __getattribute__ ( ) method for searching classes in a predictable order that supports cooperative multiple inheritance . Accordingly , super ( ) is undefined for implicit lookups using statements or operators such as super ( ) [ name ] ."
"I need to change a call to a function inside another function during run-time.Consider the following code : Then ... One user has written code , myfunc ( ) , that makes a call to a globally defined function now ( ) and I can not edit it . But I want it to call the method of the Sim instance instead . So I would need to `` patch '' the myfunc ( ) function in run-time.How could I go about this ? One possible solution is to edit the bytecode as done here : http : //web.archive.org/web/20140306210310/http : //www.jonathon-vogel.com/posts/patching_function_bytecode_with_python but I 'm wondering if there is an easier way ."
"i 'm just testing out the csv component in python , and i am having some trouble with it . I have a fairly standard csv string , and the default options all seems to fit with my test , but the result should n't group 1 , 2 , 3 , 4 in a row and 5 , 6 , 7 , 8 in a row ? Thanks a lot for any enlightenment provided !"
"Here { { dispN } } is integers numbers will come..like 36 , 42 , etc . If i give tableDataSet0.36.items it 's working fine . But I need multiples in the place 36 ."
"Here is an abstract base class and a concrete subclass , which I would like to expose to Python via Cython : This is my first attempt at declaring the interface of the classes for Cython . To avoid naming conflicts between the cppclasses and the Python wrapper classes , I declare every Class as _Class , followed by its proper name `` Namespace : :Class '' . But now I get a syntax error when trying to express that _NeighborhoodDistance is a subclass of _NodeDistance . What did I do wrong ?"
Answering this question it turned out that df.groupby ( ... ) .agg ( set ) and df.groupby ( ... ) .agg ( lambda x : set ( x ) ) are producing different results.Data : Demo : I would expect the same behaviour here - do you know what I am missing ?
"I 've got a 2D numpy array , A containing an index into another array , B . What is a good way to get C from A and B using numpy ?"
Suppose we have the following mod.py : and the following use of it : I get an error : According to the documentation the documentation the with statement should execute as follows ( I believe it fails at step 2 and therefore truncate the list ) : The context expression ( the expression given in the with_item ) is evaluated to obtain a context manager . The context manager ’ s __exit__ ( ) is loaded for later use . The context manager ’ s __enter__ ( ) method is invoked . etc ... As I 've understood it there is no reason why __exit__ could not be found . Is there something I 've missed that makes a module not able to work as a context manager ?
"I 'm working with this WNBA dataset here . I 'm analyzing the Height variable , and below is a table showing frequency , cumulative percentage , and cumulative frequency for each height value recorded : From the table I can easily conclude that the first quartile ( the 25th percentile ) can not be larger than 175.However , when I use Series.describe ( ) , I 'm told that the 25th percentile is 176.5 . Why is that so ?"
"Seeking to random points in a video file with OpenCV seems to be much slower than in media players like Windows Media Player or VLC . I am trying to seek to different positions on a video file encoded in H264 ( or MPEG-4 AVC ( part10 ) ) using VideoCapture and the time taken to seek to the position seems to be proportional to the frame number queried . Here 's a small code example of what I 'm trying to do : The perceived times for when the images are displayed from above are proportional to the frame number . That is , frame number 200 and 400 , barely have any delay , 8000 some noticeable lag , but 200000 would take almost half a minute.Why is n't OpenCV able to seek as `` quickly '' as say Windows Media Player ? Could it be that OpenCV is not using the FFMPEG codecs correctly while seeking ? Would building OpenCV from sources with some alternate configuration for codecs help ? If so , could someone tell me what the configuration could be ? I have only tested this on Windows 7 and 10 PCs , with OpenCV binaries as is , with relevant FFMPEG DLLs in system path.Another observation : With OpenCV ( binaries ) versions greater than 2.4.9 ( Example 2.4.11 , 3.3.0 ) , the first seek works , but not the subsequent ones . That is , it can seek to frame 200 from above example , but not to 400 and the rest ; the video just jumps back to frame 0 . But since it works for me with 2.4.9 , I 'm happy for now ."
"I have a script in Python to compress big string : When I run this script , I got a error : Some information : zlib.version = ' 1.0'zlib.ZLIB_VERSION = ' 1.2.7'How to compress big data ( more than 2Gb ) in Python ?"
Say I have a list l and a thread t1 iterating over l for ever : and another thread t2 randomly modify or delete members in l.What happens after a deletion ? Does t1 detect that in the current loop ? UPDATE : By freeze I mean t1 get a copy of l. t2 can modify l forsureciting documentation or simple but persuading code snippet arewelcomed .
I 'm making a program that needs to recieve a connection hash from a server . When I use : I get this error : Is this my fault or the servers fault ? Here is some of the code leading up to s.recv ( ) I am using Python v 2.7EDIT : This is for Minecraft just so you know .
"I would like to write unit tests to test whether a dependency exists between two python packages . E.g . : a unit test to check that modules in package b do n't import anything from modules in package a . The only solution I have so far is to scan the files and check that there is n't a `` from a '' or `` import a '' in the source code . Are there other ways of doing this ? One of the requirements is that a/ and b/ must be in the same directory level.I would like to have this unit test because I want to be sure that I can use package b in other projects without package a , and also not have other developers write code that will make b dependent on a ."
"I have a hashable identifier for putting things in a dictionary : I have a node type that encapsulates identifer for purposes of hashing and equality : I put some nodes into a dictionary : Some time later , I have only an identifier : Is there any way to efficiently lookup the node that has been stored with this identifier in this dictionary ? Please note that this is a little trickier than it sounds ; I know that I can trivially use d [ my_id ] to retrieve the associated item 'Node 2 ' , but I want to efficiently return a reference to n2.I know that I could do it by looking at every element in d , but I 've tried that and it 's much too slow ( the dictionary has thousands of items in it and I do this a fair number of times ) .I know that internally dict is using the hash and eq operators for that identifier to store node n2 and its associated item , 'Node 2 ' . In fact , using my_id to lookup 'Node 2 ' actually needs to lookup n2 as an intermediate step , so this should definitely be possible . I am using this to store data in a graph . The nodes have a lot of additional data ( where I put value ) that is not used in the hash . I did n't create the graph package I 'm using ( networkX ) , but I can see the dictionary that stores my nodes . I could also keep an extra dictionary around of identifiers to nodes , but this would be a pain ( I 'd need to wrap the graph class and rewrite all add node , remove node , add nodes from list , remove nodes from list , add edge , etc . type functions to keep that dictionary up to date ) .This is quite the puzzle . Any help would be really appreciated !"
"Recently I read the `` Fluent python '' and understood how == operator works with python objects , using __eq__ ( ) method . But how it works with int instances in python2 ? in python3 all a.__eq__ ( b ) returns True"
"I 'm trying to override an object 's next function using the code below ( python 2.7 ) .When the object 's next method is called directly , the new function is invoked . However when I call the builtin next ( ) function on my object ( which , according to the docs , should call the instance 's next method ) , the ORIGINAL function is invoked.Can someone explain this behaviour ?"
"snakemake is a python-like replacement for make that is geared more towards workflows than compilation . It 's quite nice , but also quite new , and I can not seem to find a mode for it in Emacs . I just want something very simple : a very slight modification from fundamental-mode , so I in perusing the emacs manual , I started the following in init.el : like make , snakemake is strict about indents ( actual tab `` \t '' characters , not how emacs behaves by default when one types TAB ) . When I instead type `` C-q TAB '' it puts a real tab character in the buffer : this works , I tried it with a Snakefile in fundamental-mode and it runs perfectly . So to avoid typing `` C-q TAB '' each time I want a TAB in this mode , the first addition I would like to make to snake-mode is to rebind the TAB key to `` C-q TAB '' ( or something like this ) . So I perused the emacs manual and tried : but this and other alternatives are n't working ... maybe rebinding standard keys like the TAB key is not a recommended practice ? the other addition to the snake-mode I would like is for it to highlight syntax according to python ( but not have any python behaviour , e.g. , python indenting behaviour ) To conclude , just these 2 simple modifications to fundamental-mode in creating a `` snake-mode '' and a way to also invoke snake-mode if the filename is `` Snakefile '' was all I was looking for , but I 've already spent several hours perusing the emacs manual and doing some googling , and it seems I 'm not even close . This is so simple , and I 'm quite sure it is possible ; any advice ? ThanksMurray"
"Can anyone familiar with Python 's internals ( CPython , or other implementations ) explain why list addition is required to be homogenous : Why should n't the x+ '' foo '' above return the same value as the final value of x in the above transcript ? This question follows on from NPE 's question here : Is the behaviour of Python 's list += iterable documented anywhere ? Update : I know it is not required that heterogenous += work ( but it does ) and likewise , it is not required that heterogenous + be an error . This question is about why that latter choice was made . It is too much to say that the results of adding a sequence to a list are uncertain . If that were a sufficient objection , it would make sense to prevent heterogenous += . Update2 : In particular , python always delegates operator calls to the lefthand operand , so no issue `` what is the right thing to do '' arises '' : the left-hand object always governs ( unless it delegates to the right ) .Update3 : For anyone arguing that this is a design decision , please explain ( a ) why it is not documented ; or ( b ) where it is documented.Update4 : `` what should [ 1 ] + ( 2 , ) return ? '' It should return a result value equal with the value of a variable x initially holding [ 1 ] immediately after x+= ( 2 , ) . This result is well-defined ."
"I have a string that looks like : How can I count the number of runs in the string so that I get , 5 runs of T and 4 runs of H"
"If I have an arbitrary binary vector ( numpy array ) in Python , e.g.This would give me the binary array 00001100 . I could also have 00000000 or 00010100 etc . How to make such a script that when I give this binary vector as an input , the script gives the minimum right-rotated binary numpy array as output ? Few examples : etc . Any suggestions / good optimized Python implementations in mind ? = ) Thank you for any assistance . I need this for Local Binary Pattern implementation = )"
"Is there a way to configure travis-ci to make the Python versions dependent on a certain env var ? Please consider the following travis.yml config : Among Django 1.3 ( DJANGO=1.3.4 ) and 1.4 ( DJANGO=1.4.2 ) i also want to test against the latest development version of Django ( DJANGO=https : //github.com/django/django/zipball/master ) , which is basically Django 1.5.The problem i see is that travis-ci will automatically run the integration against all specified Python versions . Django 1.5 however does n't support Python 2.5 anymore . Is it possible to omit it for the Django development version so that i get integrations like this only : DJANGO=1.3.4 -- > python `` 2.5 '' , `` 2.6 '' , `` 2.7 '' DJANGO=1.4.2 -- > python `` 2.5 '' , `` 2.6 '' , `` 2.7 '' DJANGO=https : //github.com/django/django/zipball/master -- > python `` 2.6 '' , `` 2.7 '' UPDATE : Here 's a link to a live example based on Odi 's answer which i 've been using successfully for a few months : https : //github.com/deschler/django-modeltranslation/blob/master/.travis.yml"
I was trying to build kivy app to android and got this errorcommandlog : http : //paste.ubuntu.com/20850804/want any details ? request in the comments
"I am using the app factory pattern to set up my Flask application . My app uses the Flask-Babel extension , and that is set up in the factory as well . However , I want to access the extension in a blueprint in order to use it , The factory is in __init__.py.I want to add the following to main.py : Unfortunately , main.py does n't have access to the babel variable from the application factory . How should I go about solving this ?"
"I want to change a ManyToMany field in a django app.I have the following models : and I want : I am using south for db migrations . Unfortunately , in this case , south propse to delete the existing app_b_a table and to recreate a new app_c.Is there a way to say to south not to recreate the m2m table ?"
"What 's the intuition behind floating point indices ? Struggling to understand when we would use them instead of int indices ( it seems like you can have three types of indices : int64 , float64 , or object , e.g . s.index= [ ' a ' , ' b ' , ' c ' , 'd ' , ' e ' , ' f ' ] ) .From the code above , it also looks like Pandas really wants float indices to be in 64-bit , as these 64-bit floats are getting cast to 32-bit floats and then back to 64-bit floats , with the dtype of the index remaining 'float64'.How do people use float indicies ? Is the idea that you might have some statistical calculation over data and want to rank on the result of it , but those results may be floats ? And we want to force float64 to avoid losing resolution ?"
When using the argparse module in Python I am looking for a way to trap invalid options and report them better.The documentation at https : //docs.python.org/3/library/argparse.html # invalid-arguments provides an example : However it is quite easy to trip this up as bad options are not reported first . For example : will report : When I would prefer it to report the more useful : Is it possible to achieve this ? It seems to me a quite basic use case.When writing argument parsers directly I typically use a patternsuch that anything starting with a -- option prefix that is not a known option is rejected immediately . For example in bashI believe argparse uses regular expressions internally but I do n't think they are accessible via add_argument ( ) Is there any way to do the equivalent easily with argparse ?
"I have this setup : And here is the code for my two files , main.py and module.py respectively : main.pymodule.pySo , what I noticed is that this outputs the following ( notice how the module logger has no formatting ) : It only seems like only after I make an edit in main.py to change logger = logging.getLogger ( __ name __ ) to logger = logging.getLogger ( ) or add logger = logging.getLogger ( ) after def main ( ) : that it logs like this ( which is what I want ) : Why is that ? I thought that because main.py is importing module.py , it is naturally higher on the hierarchical scale so module.py would inherit the logger settings as defined in main.py . Do need to explicitly set the root logger ( with logger = logging.getLogger ( ) ) in main for the inheritance to work ? Did I not configure my folder structure correctly to make module.py 's logger inherit main.py 's logger settings , or is folder structure irrelevant ? The reason I ask is because I thought one should use logger = logging.getLogger ( __ name __ ) throughout ( even in main.py ) and then based on the import structure ( or folder structure ? ) , that would determine the hierarchy and loggers would inherit accordingly . And the reason I was making that assumption is because what if I was importing main.py into another program ? I guess my point is , I want to make logging as generic as possible such that I can import one module into another and it always inherits the parent 's logger settings . Is there a way to display the underlying hierarchy of all the modules for debugging/learning purposes ?"
"I am trying to implement the Strategy design pattern to create an interface for an underlying algorithm to be implemented in a modular fashion.Currently , as per code below , I have one top-level/parent abstract class ( ParentAbstractStrategy ) that defines the base interface for the strategy method.I also have a one-level-down from this abstract class ( ChildAbstractStrategy ) .The reason I have two abstract classes is because of the attributes they need to hold ; see the __init__ methods . ChildAbstractStrategy is a special case of ParentAbstractStrategy in that it stores an additional attribute : attr2.Otherwise its interface is identical , as seen by identical strategy method signatures.Sometimes , I want to be able to directly subclass ParentAbstractStrategy and implement the strategy method ( see ConcreteStrategyA ) , but other times I want to be able to subclass ChildAbstractStrategy , because the extra attribute is required ( see ConcreteStrategyB ) .An additional complication is that in some subclasses of either abstract class I want to be able to handle additional arguments in the strategy method . This is why I have added **kwargs to all signatures of the strategy method , so that I can pass in whatever additional arguments I want to a subclass , on a case-by-case basis.This creates the last problem : these extra arguments are not optional in the subclasses . E.g . in the strategy method of ConcreteStrategyB I want to be certain that the caller passed in a third argument.I 'm basically abusing **kwargs to provide what probably should be positional arguments ( since I ca n't give them sane defaults and need their existence to be enforced ) .This current solution of using **kwargs for `` method overloading '' in subclasses feels really messy , and I 'm not sure if this means there is a problem with the class inheritance scheme or interface design , or both.Is there a way that I can achieve these design goals in a cleaner fashion . It feels like I 'm missing something big picture here and maybe the class/interface design is bad . Maybe creating two disjoint abstract classes with different signatures for the strategy method ? Here 's an interpreter session demonstrating how it 's currently working :"
"Possible Duplicate : How to get the function name as string in Python ? I know that I can do that : which will return the name of the function as 'my_func'.But as I am into the function , is there a way to directly call it generically ? Something like : In which Python would understand that I want the upper part of my code hierarchy ?"
"In Numpy you can subset certain columns by giving a list or integer . For example : But how to do exclusion ? Where it return all other columns except 2 or [ 1,3,4 ] .The reason is that I want to make all other columns zeros except one or a list of selected columns , for example : I can generate a new zeros array with the same shape then just assign the specific column to the new variable . But I wonder if there is any more efficient wayThanks"
"I am applying transfer-learning on a pre-trained network with keras . I have image patches with a binary class label and would like to use CNN to predict a class label in the range [ 0 ; 1 ] for unseen image patches.network : ResNet50 pre-trained with imageNet to which I add 3 layersdata : 70305 training samples , 8000 validation samples , 66823 testing samples , all with a balanced number of both class labelsimages : 3 bands ( RGB ) and 224x224 pixelsset-up : 32 batches , size of conv . layer : 16result : after a few epochs , I already have an accuracy of almost 1 and a loss close to 0 , while on the validation data the accuracy remains at 0.5 and loss varies per epoch . In the end , the CNN predicts only one class for all unseen patches . problem : it seems like my network is overfitting . The following strategies could reduce overfitting : increase batch sizedecrease size of fully-connected layeradd drop-out layeradd data augmentationapply regularization by modifying the loss functionunfreeze more pre-trained layersuse different network architectureI have tried batch sizes up to 512 and changed the size of fully-connected layer without much success . Before just randomly testing the rest , I would like to ask how to investigate what goes wrong why in order to find out which of the above strategies has most potential.Below my code :"
"What is the difference between print , object , and repr ( ) ? Why is it printing in different formats ? See the output difference :"
I receive 2 different formats of datetime in string format . But While storing in PostgreSql Database it has to be stored in UTC format.Tried using timestamptz in PostgreSql but I guess it does n't seem to understand AMERICA/NEW_YORK so how can to convert and store in UTC format in DB-4:00 and AMERICA/NEW_YORK are just example but it can be anything else .
"I 'm writing a Python/Django application to do some stock analysis.I have two very simple models that look like this : This is the dummy data I have populated them with : I want to find all the stocks that made a yearly low within the past week.But to make this question simpler , just assume that I want to find all the stocks whose lowest point since '2017-05-04 ' occurred on or after '2018-04-30 ' . Below is the SQL I wrote to find it . It works.But I need help figuring out what Django Query to write to get the same results as this SQL . How can I do it ?"
"I 'm looking to take a pandas series and fill NaN with the average of the next numerical value where : average = next numerical value / ( # consecutive NaNs + 1 ) Here 's my code so far , I just ca n't figure out how to divide the filler column among the NaNs ( and the next numerical value as well ) in num : Current Output : Desired Output :"
"This is based on this question asked 2018-10.Consider the following code . Three simple functions to count non-zero elements in a NumPy 3D array ( 1000 × 1000 × 1000 ) .Runtimes on my machine ( Python 3.7. ? , Windows 10 , NumPy 1.16. ? ) : So , f_2 ( ) works faster than f_1 ( ) and f_3 ( ) . However , it 's not the case with data of smaller size . The question is - why so ? Is it NumPy , Python , or something else ?"
"If you type help ( vars ) , the following is produced : When I do the following : it displays this : How can I change it so that it shows up with ... between the parentheses like the built-in function vars ( ) ? ( That is , func ( ... ) ) Edit : It has been suggested to use a docstring , but that wo n't do what I want . Here is an example : result : You see , x , y is still being displayed instead of ..."
"With the pandas dataframe below , taken from a dict of dict : I 'd like to form a new pandas DataFrame with the results of a Pearson correlation between rows for every row , excluding Pearson correlations between the same rows ( correlating A with itself should just be NaN . This is spelled out as a dict of dicts here : where NaN is just numpy.nan . Is there a way to do this as an operation within pandas without iterating through a dict of dicts ? I have ~76million pairs , so a non-iterative approach would be great , if one exists ."
"I am working with Snakemake and I ca n't find a way to access to the current-rule 's name.For instance , is there a way to have an access like this : This can be very helpful when the check_inputs function is more or less the same for each rules.For sure , I made this and it works : However , I was wondering that if a more `` Snakemaker way '' to get the current-rule 's name exists to avoid writing / hardcoding the rule 's name each time.Any kind of help or suggestion will be highly appreciated. -- - EDIT1 -- -The rule name is accessible via { rules.myrule.name } only when the input and output statements are parsed by snakemake . So the use of { rules.myrule.name } is not possible in input/output definition.The idea is to have a quick access to the current rule 's name { rules.current } for instance , because { rules.myrule.name } is also repetitive ."
"I have string that looks like dictionary like this : I would like to convert it to an actual dictionary as instructed hereYet , I got an error : Traceback ( most recent call last ) : File `` '' , line 1 , in File `` /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/init.py '' , line 286 , in load return loads ( fp.read ( ) , AttributeError : 'str ' object has no attribute 'read'What is wrong to my code , and how I convert string like dictionary to actual dictionary ? Thanks ."
I want to use IsolationForest for finding outliers . I want to find the best parameters for model with GridSearchCV . The problem is that I always get the same error : It seems like its a problem because IsolationForest does not have score method.Is there a way to fix this ? Also is there a way to find a score for isolation forest ? This is my code :
"I 'm trying to fill a numpy array using multiprocessing , following this post . What I have works fine on my Mac , but when I port it to Ubuntu I get segmentation faults a lot of the time.I 've reduced the code to following minimal example : On Ubuntu 16.04 , with Python 3.6.5 and numpy 1.15.4 ( same versions as on my Mac ) , I get the outputNow , I can change the array dimensions somewhat and in some cases it 'll work ( e.g. , change the first 224 to 100 and it works ) . But mostly it seg faults . Can anyone offer any insight ? I see one post on a related topic from 2016 that no one responded to , and another one involving pointers which I 'm not using . PS- It does n't seem to make any difference whether I specify a as a multidimensional array or as a flattened array ( e.g . np.zeros ( 224*224*3 ) ) . It also does n't seem to make a difference if I change the data type ( e.g . float to int ) ; it fails the same.One further update : Even setting `` size=224 '' in the code from the original post causes seg faults on two different Ubuntu machines with different versions of numpy , but works fine on Mac ."
"I have a text file like this : I want to replace the IP address and write the file in-place . The tricky part is , the lines might be in a different order , and the command-line arguments might be written in a different order . So I need to find the line beginning with CommandLine and then replace the string between -h and the next -.So far , I am able to get the old IP address with the following code , but I do n't know how to replace it and write the file . I 'm a Python beginner ."
From a list of values I want to create a new list of values until they add up a value . I am new with Python but I believe it is best done with a while loop .
"I make extensive use of the ORM facuilities in sqlalchemy , so in many contexts , I already have data loaded from the database , and want to check conditions or perform calculations on the python objects that are already loaded ; I also want/need to do more batch oriented tasks that are better expressed by executing sql against the database ( and not loading data at all ) . I would like to use the same code to express the same calculations for both uses , so that I do n't have to bend over backwards for a database connection or write each computation twice ( once in regular python , again as queries ) and run the risk that they disagree.suppose I have : Is there a way to getwithout first persisting a_foo to a database and then executing the query ? I specifically want a way to express the clause so that it can also be used in the context of a database query , but still useful without one.One option would be to change the clauses to functions : but I find this to be less readable than the original way the clauses were expressed ."
"Based of this answer , I want to create a one line tree as part of another class like thus : I will need to allow the users of said class to add path elements to the tree and run some callback starting at the lowest tree level . My naive implementation raises a error when I run pytest : This code works iff the tree is defined as Why is my naive approach not working with the lambda expression ? ⚠ The Zen of Python states that Simple is better than complex.The one-line lambda makes the code complex where there is a simpler implementation . Therefore the one-line lambda should not be used in production code . However , I shall leave this question here for academic interest ."
"I recently stumbled upon a seemingly strange behavior regarding the order in which __eq__ methods are executed , if one side of the comparison is an object which inherits from the other.I 've tried this in Python 3.7.2 in an admittedly academic example . Usually , given an equality comparison a == b , I expect a.__eq__ to be called first , followed by b.__eq__ , if the first call returned NotImplemented . However , this does n't seem to be the case , if a and b are part of the same class hierarchy . Consider the following example : In case 1 , I expect a.__eq__ to be called twice and this is also what I get : However , in cases 2.1 and 2.2 , b.__eq__ is always executed first , no matter on which side of the comparison it stands : In cases 3.1 and 3.2 then , the left-hand side is again evaluated first , as I expected : It seems that , if the compared objects are related to each other , __eq__ of the object of the child class is always evaluated first . Is there a deeper reasoning behind this behavior ? If so , is this documented somewhere ? PEP 207 does n't mention this case , as far as I can see . Or am I maybe missing something obvious here ?"
"TL ; DRThis was indeed a bug in Motor 1.2.0 that was promptly fixed by A. Jesse Jiryu Davis and is available in version 1.2.1 or greater of the driver.Original QuestionI wrote a program to monitor changes to a MongoDB collection using its new Change Stream feature , on Python 3 . Here 's the MCVE : When I kill the program with CTRL+C , it raises three different exceptions.Is there a way to make that program close silently ? I 'm testing with Python 3.6.4 , Motor 1.2 and pymongo 3.6.0 on macOS Sierra ."
"I 'm trying to solve a problem with python+numpy in which I 've some functions of type that I need to convolve with another function . In order to optimize code , I performed the fft of f and g , I multiplied them and then I performed the inverse transformation to obtain the result.As a further optimization I realized that , thanks to the shift theorem , I could simply compute once the fft of f ( x , y , z ) and then multiply it by a phase factor that depends on to obtain the fft of . In particular , , where N is the length of both x and y.I tried to implement this simple formula with python+numpy , but it fails for some reason that is obscure for me at the moment , so I 'm asking the help of SO community in order to figure out what I 'm missing.I 'm providing also a simple example.I expect that by means of shifted/base I could obtain the corresponding values of the phase factor , but as could be seen , it can not be a phase factor , since its np.abs is > = 1 !"
"In the C programming language , I often have done the following : In Python , I have not found anything similar , since I am not allowed to set variables inside the evaluated expression . I usually end up with having to setup the evaluated expression twice ! In my attempts to find a better way , I 've found a way that only require to setup and the evaluated expression once , but this is getting uglier ... So far I 've settled with the following method for some of my cases , but this is far from optimal for the regular while loops ... : All my solutions to solve a simple basic programming problem has become too complex ... Do anyone have a suggestion how to do this the proper way ?"
"First : yes , i have taken a very long look at Norvig 's lispy . Second : I have reused part of his code.On to my code , and my question . I am writing a really non-idiomatic lisp interpreter in Python , and I 'm curious as to how I would write nested function definitions ( e.g . ( define square ( lambda ( x ) ( * x x ) ) ) then ( define SoS ( lambda ( x y ) ( + ( square x ) ( square y ) ) ) ) ) and currently that does n't work . I 'm a bit stuck . What can I do ? EDIT : Any tips about my coding style or improvements I could make would be greatly appreciated . Thank you !"
"This one is just obtuse enough that no typical answer is bubbling to the top.The nature of the tool I 'm working on dictates we use MongoDB to store 'settings ' for about 25 different tools . Each tool has it 's own settings schema , so each document is different , but they 're all stored in the same collection and edited on the same edit page which is drawn by json schema.Not knowing the schema of the dictionary , I 'm struggling to figure out how to iterate and sanitize the data , specifically removing passwords.Given the following dictionary , and knowing that other dicts may have different schemas , how could I traverse every single item in the dict and create a copy , identical except with any key == `` password '' removed ? So : Should result in :"
"I am getting into testing in python and I asked myself how to test this method.Using doctest or unittest , how is this best achieved ? I thought of passing get_response ( ) a test url and some test parameters , which exists in real world and to check if response.read ( ) returns the expected data . But somehow I feel , this is not the way it should be done . Any suggestions ? I would appreciate suggestions how to handle such cases in context of texting ."
"In my django project , I have been using django-taggit to add tagging capabilities to a model.The migration adding tags also lists the initial taggit migration as dependency : At a later point in time , I have removed taggit everywhere , including INSTALLED_APPS . The problem is that django can ’ t resolve that migration belonging to taggit , and raises an error.What is the preferred solution in this scenario ? I can think of a two-step strategy : keep taggit in INSTALLED_APPS until all servers running the project are up to datesquash migrations afterwards , so that the field does not show up any more , and only then remove taggit from INSTALLED_APPS"
Suppose I have a form like this : How do I use webtest 's form library to test submitting multiple values ?
"Given a list : What is the Pythonic way to return every nth block of m elements ? Note that this is different from just returning every nth element.Desired result of taking every 1st of 3 blocks of 3 elements ( take 3 , skip 6 , take 3 , skip 6 ... ) : I can get to this as : Is there a cleaner way ?"
"I want to execute a sympy lambda function in parallel.I do n't know : why it works in parallel although it is a lambda functionwhy it stops working when I try executing without the poolwhy it works if I uncomment the first return in lambdifyAnd apparently the markdown preprocessor needs a line of text above the code so this is the code : It prints : ( I cut the traceback ) If I uncomment , it works normally : I tested only on Windows and it works exactly the same with 'numexpr ' instead of 'numpy ' ."
"I 've set up a custom CNN with K-fold cross-validation in keras with tensorflow back-end . The model.compile ( ) function is called before starting the training , but the model.fit_generator ( ) function call results in an runtime error : `` You must compile your model before using it . `` I do data augmentation with ImageDataGenerator and use the fit_generator function for training.The only related issue I 've found up untill now had to do with the tensorflow eager execution feature which seems to not be enabled in Keras . Here is the code : model definition : ... here would be the data initialization ... setting up the ImageGenerator : model setup and training :"
"Supposing I have a very big text file consisting of many lines that I would like to reverse . And I do n't care of the final order . The input file contains Cyrillic symbols . I use multiprocessing to process on several cores.I wrote such program : This program fails with error : On the other hand , everything works fine if I set POOL_NUMBER = 1 . But it does n't make a sense if I want to gain the total performance.Why does that error happen ? And how can I fix it ? I use Python 3.5.2.I generated data using this script :"
"I 'm looking for an algorithm or even an algorithm space that deals with the problem of validating that short text ( email ) matches known templates . Coding will probably be python or perl , but that 's flexible.Here 's the problem : Servers with access to production data need to be able to send out email that will reach the Internet : Obviously some of the email contents will vary - the salutation ( `` John Smith '' ) , `` $ 123.45 on 2/4/13 '' , and the lines with transactions printed out . Other parts ( `` We received your last payment '' ) are very static . I want to be able to match the static portions of the text and quantify that the dynamic portions are within certain reasonable limits ( I might know that the most transaction lines to be printed is 5 , for example ) .Because I am concerned about data exfiltration , I want to make sure email that does n't match this template never goes out - I want to examine email and quarantine anything that does n't look like what I expect . So I need to automate this template matching and block any email messages that are far enough away from matching.So the question is , where do I look for a filtering mechanism ? Bayesian filtering tries to verify a sufficient similarity between a specific message and a non-specific corpus , which is kind of the opposite problem . Things like Perl 's Template module are a tight match - but for output , not for input or comparison . Simple 'diff ' type comparisons wo n't handle the limited dynamic info very well.How do I test to see if these outgoing email messages `` quack like a duck '' ?"
"I 'm learning Python and I just finished to learn about the basics of the class . This is the first OOP language I am learning so not everything is very perfectly clear at the moment . Anyway . I 'm wondering if it 's possible in Python ( 3.6 ) , to create a class for which we do n't know the attributes of a few objects ( or maybe only 2 or 3 among many possible ) . For example , let say you want to define a molecule by its properties so that you can use them in another program to model or predict things . The basic properties you can involve will be ( for example ) , Temperature , MolecularWeight and MolarVolume . However , you can also easily define your molecule with 20 different parameters . But , some of them may not exist for every molecule , or you may not need , at the moment , to use them . ( i.e . first version of a software , ... ) .The question : I would like to know if a syntax as following exists in Python 3.x , so that the class will be able to create exactly the number of parameters a molecule has . I think it will cost a lot of memory space to define variables which do n't exist for thousands of molecules ... My goal is to use a file .txt to record the properties of my molecules ( I have thousands of them ) with their properties sorted in a defined order so that when the class is used if for the molecule methanol we have 10 parameters with the first three as mention before , then the class will create the molecule `` methanol '' with the ten properties in order.Else , if it does n't exist , should I by default create a class with all parameters I have in mind and consider some of them as not useful depending on the situations ? Or use something much better that exists ? Thank you in advance"
"I wrote the following function : I want to add type annotations to the function : However , I want to explicitly define that the values inside the returned dictionary can not be None.Is there a way to say `` Any type , except NoneType '' or `` Every possible value but None '' ?"
"I have a list of names , e.g . [ 'Agrajag ' , 'Colin ' , 'Deep Thought ' , ... , 'Zaphod Beeblebrox ' , 'Zarquon ' ] . Now I want to partition this list into approximately equally sized sublists , so that the boundaries of the subgroups are at the first letter of the names , e.g A-F , G-L , M-P , Q-Z , not A-Fe , Fi-Mo , Mu-Pra , Pre-Z.I could only come up with a statically sized parition that does n't take size of the subgroups into account : Any better ideas ? P.S . : this may sound somewhat like homework , but it actually is for a webpage where members should be displayed in 5-10 tabs of equally sized groups ."
I 'd like to add a new functionality to an existing awaitable class by subclassing it.Let 's start with a very simple base class creating objects which asynchronously return 99 after a short sleep . The subclass should just add +1 to the result.I ca n't find the proper way to use super ( ) to refer to the base class .
"I have a data frame with datetimes and integersGives meHow can I find which N-minute group of consecutive dt gives me the maximum sum of val ? In this case , if N=3 , then the result should be : ( marked with stars above )"
"If we have the following data : How can I shift the data in a cyclical fashion so that the next step is : And then : etc . This should also shift the index values with the row . I know of pandas X.shift ( ) , but it was n't making the cyclical thing ."
"I tried to compile Python 2.7 from source . Here are my commands : And the output of which python is /my/local/dir/bin/python , which is correct.But when I ran python -- version I see Python 2.7.3 instead of Python 2.7.10.The system version of Python is 2.7.3 . Could it be the system version of Python somehow links itself against the local , compiled version ? Or am I doing something wrong ? Edit : The output of ./my/local/dir/bin/python -- version is also Python 2.7.3Edit 2 : Seems like if I get rid of the -- enable-shared flag it will produce the correct version of Python , but I need that flag for my other software to work ."
"If I make a class against a local namespace , how exactly does it work ? For instance : The particular lines I 'm curious about are these : I suppose the biggest thing that I 'm wondering is what function is being made and then called ? And is this function where the closures are attached to the class , or does that happen elsewhere ?"
"I am new to python when i try to print `` \20 % '' that iswhy is the shell printing '\x10 % ' that is , it is showingthe same is happening with join also when is doit showsI am using python 2.7.3"
I tried to decorate a classmethod with functools.lru_cache . My attempt failed : The error message comes from functools.lru_cache :
"I am having NVARCHAR type column in my database . I am unable to convert the content of this column to plain string in my code . ( I am using pyodbc for the database connection ) .The closest I have gone is via encoding it to utf-16 as : But the actual value that I need as per the value store in database is : I tried with encoding it to utf-8 , utf-16 , ascii , utf-32 but nothing seemed to work.Does anyone have the idea regarding what I am missing ? And how to get the desired result from the my_string.Edit : On converting it to utf-16-le , I am able to remove unwanted characters from start , but still one character is missing from endOn trying for some other columns , it is working . What might be the cause of this intermittent issue ?"
"In Python , I try to find the last position in an arbitrary string that does match a given pattern , which is specified as negative character set regex pattern . For example , with the string uiae1iuae200 , and the pattern of not being a number ( regex pattern in Python for this would be [ ^0-9 ] ) , I would need ' 8 ' ( the last ' e ' before the '200 ' ) as result . What is the most pythonic way to achieve this ? As it 's a little tricky to quickly find method documentation and the best suited method for something in the Python docs ( due to method docs being somewhere in the middle of the corresponding page , like re.search ( ) in the re page ) , the best way I quickly found myself is using re.search ( ) - but the current form simply must be a suboptimal way of doing it : I am not satisfied with this for two reasons : - a ) I need to reverse string before using it with [ : :-1 ] , and - b ) I also need to reverse the resulting position ( subtracting it from len ( string ) because of having reversed the string before . There needs to be better ways for this , likely even with the result of re.search ( ) .I am aware of re.search ( ... ) .end ( ) over .start ( ) , but re.search ( ) seems to split the results into groups , for which I did not quickly find a not-cumbersome way to apply it to the last matched group . Without specifying the group , .start ( ) , .end ( ) , etc , seem to always match the first group , which does not have the position information about the last match . However , selecting the group seems to at first require the return value to temporarily be saved in a variable ( which prevents neat one-liners ) , as I would need to access both the information about selecting the last group and then to select .end ( ) from this group . What 's your pythonic solution to this ? I would value being pythonic more than having the most optimized runtime.UpdateThe solution should be functional also in corner cases , like 123 ( no position that matches the regex ) , empty string , etc . It should not crash e.g . because of selecting the last index of an empty list . However , as even my ugly answer above in the question would need more than one line for this , I guess a one-liner might be impossible for this ( simply because one needs to check the return value of re.search ( ) or re.finditer ( ) before handling it ) . I 'll accept pythonic multi-line solutions to this answer for this reason ."
"I am trying to convert my Jupyter Notebook file ( .ipynb ) into an HTML file for easier reading . Every time I try to save the notebook I get a `` Notebook validation failed '' error : message when file is savedWhen I try to download as .html using File > Download as , I get a further more detailed error but I still can not make out what it means or what needs to be done to solve the problem and finally download as HTML : message when trying to download as HTMLThis is only happening on the specific notebook I am working on . I have tried to save and export other notebooks as HTML successfully.Any help is appreciated . Thanks"
"I just read the question Why is there no tuple comprehension in Python ? In the comments of the accepted answer , it is stated that there are no true `` tuple comprehensions '' . Instead , our current option is to use a generator expression and pass the resulting generator object to the tuple constructor : Alternatively , we can create a list using a list comprehension and then pass the list to the tuple constructor : Lastly and to the contrary of the accepted answer , a more recent answer stated that tuple comprehensions are indeed a thing ( since Python 3.5 ) using the following syntax : To me , it seems like the second example is also one where a generator object is created first . Is this correct ? Is there any difference between these expressions in terms of what goes on behind the scenes ? In terms of performance ? I assume the first and third could have latency issues while the second could have memory issues ( as is discussed in the linked comments ) . Comparing the first one and the last , which one is more pythonic ? Update : As expected , the list comprehension is indeed much faster . I do n't understand why the first one is faster than the third one however . Any thoughts ?"
"http : //code.google.com/p/python-hidden-markov/source/browse/trunk/Markov.pyContains a class HMM , which inherits from BayesianModel , which is a new-style class . Each has a __call__ method . HMM 's __call__ method is meant to invoke BayesianModel 's at line 227 : However , this fails with an exceptionis not callable.What am I doing wrong ?"
Why Python forbids this use of walrus operator ?
"I 'm using Gensim Doc2Vec model , trying to cluster portions of a customer support conversations . My goal is to give the support team an auto response suggestions.Figure 1 : shows a sample conversations where the user question is answered in the next conversation line , making it easy to extract the data : during the conversation `` hello '' and `` Our offices are located in NYC '' should be suggestedFigure 2 : describes a conversation where the questions and answers are not in syncduring the conversation `` hello '' and `` Our offices are located in NYC '' should be suggestedFigure 3 : describes a conversation where the context for the answer is built over time , and for classification purpose ( I 'm assuming ) some of the lines are redundant.during the conversation `` here is a link for the free trial account '' should be suggestedI have the following data per conversation line ( simplified ) : who wrote the line ( user or agent ) , text , time stampI 'm using the following code to train my model : Q : How should I structure my training data and what heuristics could be applied in order to extract it from the raw data ?"
"I have some trouble with the tabs from the ttk Notebook class in python 2.7.I can not see all the tabs I create.I made a minimal code to view the problem : All I can see isI tried to change the number of tabs and I noticed the size of the top tab bar changes and unless there 's only one single lonely tab , I can not see all of them , as you can see : What I tried that did n't do anything : Setting tabs widthMoving the .pack ( ) aroundAdding .pack ( ) to the tabsUsing ttk.Frame instead of tk.FrameGoogling for a similar problemWhat I tried that worked but is n't what I want : Not using tabs ( too many stuff to show ) Having only one tabI 'll appreciate any help , thanks !"
"I 've tried both with pycassa , cassandra.cluster and dse.cluster without making a connection.I feel like I 'm connecting to the wrong host , as I 'm writing the linux servers hostname and not specifying anything regarding the cassandra . Collegues have told me they only know of connecting to the server through cqlsh inline on the linux machine . That just sounds unconvinient.Specific configurations in cassandra.yamlWhat I 'm doing in pycassa : Error message : With dse.clusterError message :"
"I am trying to read and process a large file in chunks with Python . I am following this blog that proposes a very fast way of reading and processing large chunks of data spread over multiple processes . I have only slightly updated the existing code , i.e . using stat ( fin ) .st_size over os.path.getsize . In the example I also have n't implemented multiprocessing , as the issue also manifests itself in a single process . That makes it easier to debug.The issue that I am having with this code , is that it returns broken sentences . This makes sense : the pointers do not take line endings into account , and just return some given byte size . In practice , one would assume that you could solve this by leaving out the last item in the fetched batch of lines , as that would most probably be the broken line . Unfortunately that does not work reliably either.The code above will print 29 as the total of processed lines . When you do not return the last item of the batch , naively assuming that that is a broken line anyway , you 'll get 26 . The actual number of lines is 27 . The testing data can be found below.If you print out the created lines , you 'll see that , indeed , broken sentences occur . I find this odd . Should't f.readline ( ) ensure that the file is read until the next line ? In the output below , the empty line separates two batches . That means that you can not check a line with the next line in a batch , and remove it if it 's a substring - the broken sentence belongs to another batch than the full sentence.Is there a way to get rid of these broken sentences , without removing too much ? You can download a larger test file ( 100,000 lines ) here.After a lot of digging , it seems that actually some inaccessible buffer is responsible for the inconsistent behaviour of seek , as discussed here and here . I tried out the proposed solution to use iter ( f.readline , `` ) with seek but that still gives me inconsistent results . I have updated my code to return the file pointer after each batch of 1500 lines , but in reality the batches return will overlap.An example of overlapping batches is below . The first two and a half sentence of the last batch are duplicated from the last sentences of the batch before . I do n't know how to explain nor solve this.In addition to this , I have also tried removing the readline from the original code , and keeping track of a remaining , incomplete chunk . The incomplete chunk is then passed to the next chunk and added to its front . The issue that I am running into now , is that because the text is read in byte chunks , it can happen that a chunk ends without completely finishing a character 's bytes . This wille lead to decoding errors . Running the code above , will inevitably result in a UnicodeDecodeError ."
"I have an array of names , along with a corresponding array of data . From the array of names , there is also a smaller subset of names : I am trying to make a new array of data corresponding only to the names that appear in arr2 . This is what I have been able to come up with on my own : Is there a more efficient way of doing this that does n't involve a for loop ? I should mention that the arrays themselves are being input from an external file , and there are actually multiple arrays of data , so I ca n't really change anything about that ."
I used sympy to calculate some integral as follows . This will fail and throw excepiton like : It works fine if I just use an integer as power num
"Pandas seems to be missing a R-style matrix-level rolling window function ( rollapply ( ... , by.column = FALSE ) ) , providing only the vector based version . Thus I tried to follow this question and it works beautifully with the example which can be replicated , but it does n't work with pandas DataFrames even when using the ( seemingly identical ) underlying Numpy array.Artificial problem replication : mm and pp look identical : The numpy directly-derived matrix gives me what I want perfectly : That is , it gives me 3 strides of 3 rows each , in a 3d matrix , allowing me to perform computations on a submatrix moving down by one row at a time . But the pandas-derived version ( identical call with mm replaced by pp ) : is all weird like it 's transposed somehow . Is this to do with column/row major order stuff ? I need to do matrix sliding windows in Pandas , and this seems my best shot , especially because it is really fast . What 's going on here ? How do I get the underlying Pandas array to behave like Numpy ?"
"I 'm trying to write a flask extension that needs to persist some information between requests . This works fine when I run Werkzeug with a single process but when I run with multiple processes I get some odd behavior that I do n't understand . Take this simple application as an example : For the first two scenarios it behaves exactly as I would expect : the Counter object is initialized once and then it increments with every request to the '/ ' route . When I run it with the third scenario ( passing processes=2 ) then I get this as output : It seems that counter.value is returning to it 's state right after being initialized without actually being re-initialized . Could somebody shed some light on what Werkzeug is doing internally to make this happen ? I 'd also be very interested in learning if there is a way to make this behave as I would naively expect ( two processes , each with their own instance of Counter ) . Thanks !"
I think I need some suggestion here . Below is my code : EXPECTED OUTCOME : The program ( main thread ) should exit once all child threads have exited.ACTUAL RESULT : EXPECTED RESULT : QUESTION : Is there something wrong in my approach in term of pool/map usage ?
"definitionfactorize : Map each unique object into a unique integer . Typically , the range of integers mapped to is from zero to the n - 1 where n is the number of unique objects . Two variations are typical as well . Type 1 is where the numbering occurs in the order in which unique objects are identified . Type 2 is where unique objects are first sorted and then the same process as in Type 1 is applied . The SetupConsider the list of tuples tupsI want to factorize this intoI know there are many ways to do this . However , I want to do this as efficiently as possible.What I 've tried pandas.factorize and get an error ... Or numpy.unique and get incorrect result ... ​I could use either of these on the hashes of the tuplesYay ! That 's what I wanted ... so what 's the problem ? First ProblemLook at the performance drop from this techniqueSecond ProblemHashing is not guaranteed unique ! QuestionWhat is a super fast way to factorize a list of tuples ? Timingboth axes are in log space code"
"I am trying to use an unbounded generator in itertools.permutations but it does n't seem to be working . The return generator is never created because the function just runs forever . To understand what I mean , consider : How I imagine this working is that it generates all possible 4-length permutations of the first 4 natural numbers . Then it should generate all possible 4-length permutations of the first 5 natural numbers , with no repeats so 5 must be included in all of these . What happens though is that python is hung on creating all_permutations.Before I go off and create my own function from scratch , I am wondering if there is another library that might be able to do what I 'm looking for ? Also , should n't the built-in function here be able to handle this ? Is this perhaps a bug that should be worked out ? EDIT : For some iterations ..."
Normally in version 2.7 I could add comments in python by pressing ctrl+slash.Result : After installation of PyCharm 3.0.I was searching in Setings > Project Setting > Code Style > Python and could not find anything.Also I checked Settings > Keymap But seems to it is impossible to edit anything there.I tried also change keymap from Eclipse to different options but did n't help.Annotation about right use of # with space . PEP8How to do it ?
